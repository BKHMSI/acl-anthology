<?xml version='1.0' encoding='utf-8'?>
<collection id="D19">
  <volume id="1" ingest-date="2019-11-03">
    <meta>
      <booktitle>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</booktitle>
      <url hash="8c963278">D19-1</url>
      <editor><first>Kentaro</first><last>Inui</last></editor>
      <editor><first>Jing</first><last>Jiang</last></editor>
      <editor><first>Vincent</first><last>Ng</last></editor>
      <editor><first>Xiaojun</first><last>Wan</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Hong Kong, China</address>
      <month>November</month>
      <year>2019</year>
    </meta>
    <frontmatter>
      <url hash="1bebc1a9">D19-1000</url>
      <bibkey>emnlp-2019-2019</bibkey>
    </frontmatter>
    <paper id="2">
      <title>Attention is not not Explanation</title>
      <author><first>Sarah</first><last>Wiegreffe</last></author>
      <author><first>Yuval</first><last>Pinter</last></author>
      <pages>11–20</pages>
      <abstract>Attention mechanisms play a central role in <a href="https://en.wikipedia.org/wiki/Natural_language_processing">NLP systems</a>, especially within <a href="https://en.wikipedia.org/wiki/Recurrent_neural_network">recurrent neural network (RNN) models</a>. Recently, there has been increasing interest in whether or not the intermediate representations offered by these <a href="https://en.wikipedia.org/wiki/Modular_programming">modules</a> may be used to explain the reasoning for a model’s prediction, and consequently reach insights regarding the <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a>’s decision-making process. A recent paper claims that ‘Attention is not Explanation’ (Jain and Wallace, 2019). We challenge many of the assumptions underlying this work, arguing that such a claim depends on one’s definition of explanation, and that testing it needs to take into account all elements of the model. We propose four alternative tests to determine when / whether <a href="https://en.wikipedia.org/wiki/Attention">attention</a> can be used as explanation : a simple uniform-weights baseline ; a variance calibration based on multiple random seed runs ; a diagnostic framework using frozen weights from pretrained models ; and an end-to-end adversarial attention training protocol. Each allows for meaningful interpretation of <a href="https://en.wikipedia.org/wiki/Attentional_control">attention mechanisms</a> in <a href="https://en.wikipedia.org/wiki/Neural_circuit">RNN models</a>. We show that even when reliable adversarial distributions can be found, they do n’t perform well on the simple diagnostic, indicating that prior work does not disprove the usefulness of attention mechanisms for explainability.</abstract>
      <url hash="8c6097fd">D19-1002</url>
      <attachment hash="793b4e4e">D19-1002.Attachment.zip</attachment>
      <doi>10.18653/v1/D19-1002</doi>
      <bibkey>wiegreffe-pinter-2019-attention</bibkey>
      <pwccode url="https://github.com/sarahwie/attention" additional="true">sarahwie/attention</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mimic-iii">MIMIC-III</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="3">
      <title>Practical Obstacles to Deploying <a href="https://en.wikipedia.org/wiki/Active_learning">Active Learning</a></title>
      <author><first>David</first><last>Lowell</last></author>
      <author><first>Zachary C.</first><last>Lipton</last></author>
      <author><first>Byron C.</first><last>Wallace</last></author>
      <pages>21–30</pages>
      <abstract>Active learning (AL) is a widely-used <a href="https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets">training strategy</a> for maximizing predictive performance subject to a fixed annotation budget. In AL, one iteratively selects training examples for annotation, often those for which the current <a href="https://en.wikipedia.org/wiki/Statistical_model">model</a> is most uncertain (by some measure). The hope is that active sampling leads to better performance than would be achieved under independent and identically distributed (i.i.d.) random samples. While AL has shown promise in retrospective evaluations, these studies often ignore practical obstacles to its use. In this paper, we show that while AL may provide benefits when used with specific <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> and for particular domains, the benefits of current approaches do not generalize reliably across models and tasks. This is problematic because in practice, one does not have the opportunity to explore and compare alternative <a href="https://en.wikipedia.org/wiki/Strategy_(game_theory)">AL strategies</a>. Moreover, AL couples the training dataset with the <a href="https://en.wikipedia.org/wiki/Statistical_model">model</a> used to guide its acquisition. We find that subsequently training a successor model with an actively-acquired dataset does not consistently outperform training on i.i.d. sampled data. Our findings raise the question of whether the downsides inherent to AL are worth the modest and inconsistent performance gains it tends to afford.</abstract>
      <url hash="37e1ac60">D19-1003</url>
      <attachment hash="8d3ec095">D19-1003.Attachment.zip</attachment>
      <doi>10.18653/v1/D19-1003</doi>
      <bibkey>lowell-etal-2019-practical</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/conll-2003">CoNLL-2003</pwcdataset>
    </paper>
    <paper id="5">
      <title>Knowledge Enhanced Contextual Word Representations</title>
      <author><first>Matthew E.</first><last>Peters</last></author>
      <author><first>Mark</first><last>Neumann</last></author>
      <author><first>Robert</first><last>Logan</last></author>
      <author><first>Roy</first><last>Schwartz</last></author>
      <author><first>Vidur</first><last>Joshi</last></author>
      <author><first>Sameer</first><last>Singh</last></author>
      <author><first>Noah A.</first><last>Smith</last></author>
      <pages>43–54</pages>
      <abstract>Contextual word representations, typically trained on unstructured, unlabeled text, do not contain any explicit grounding to real world entities and are often unable to remember facts about those entities. We propose a general method to embed multiple knowledge bases (KBs) into large scale models, and thereby enhance their representations with structured, human-curated knowledge. For each KB, we first use an integrated entity linker to retrieve relevant entity embeddings, then update contextual word representations via a form of word-to-entity attention. In contrast to previous approaches, the <a href="https://en.wikipedia.org/wiki/Entity–relationship_model">entity linkers</a> and self-supervised language modeling objective are jointly trained end-to-end in a multitask setting that combines a small amount of entity linking supervision with a large amount of raw text. After integrating <a href="https://en.wikipedia.org/wiki/WordNet">WordNet</a> and a subset of <a href="https://en.wikipedia.org/wiki/Wikipedia">Wikipedia</a> into BERT, the knowledge enhanced BERT (KnowBert) demonstrates improved perplexity, ability to recall facts as measured in a probing task and downstream performance on relationship extraction, entity typing, and <a href="https://en.wikipedia.org/wiki/Word-sense_disambiguation">word sense disambiguation</a>. KnowBert’s runtime is comparable to BERT’s and <a href="https://en.wikipedia.org/wiki/Information_technology">it</a> scales to large KBs.</abstract>
      <url hash="81d8db15">D19-1005</url>
      <attachment hash="079c073e">D19-1005.Attachment.pdf</attachment>
      <doi>10.18653/v1/D19-1005</doi>
      <bibkey>peters-etal-2019-knowledge</bibkey>
      <pwccode url="https://github.com/allenai/kb" additional="false">allenai/kb</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/aida-conll-yago">AIDA CoNLL-YAGO</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/semeval-2010-task-8">SemEval-2010 Task 8</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/tacred">TACRED</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wic">WiC</pwcdataset>
    </paper>
    <paper id="6">
      <title>How Contextual are Contextualized Word Representations? Comparing the Geometry of BERT, ELMo, and GPT-2 Embeddings<fixed-case>C</fixed-case>omparing the Geometry of <fixed-case>BERT</fixed-case>, <fixed-case>ELM</fixed-case>o, and <fixed-case>GPT</fixed-case>-2 Embeddings</title>
      <author><first>Kawin</first><last>Ethayarajh</last></author>
      <pages>55–65</pages>
      <abstract>Replacing static word embeddings with contextualized word representations has yielded significant improvements on many NLP tasks. However, just how contextual are the contextualized representations produced by <a href="https://en.wikipedia.org/wiki/Conceptual_model">models</a> such as ELMo and BERT? Are there infinitely many context-specific representations for each word, or are words essentially assigned one of a finite number of word-sense representations? For one, we find that the contextualized representations of all words are not isotropic in any layer of the contextualizing model. While representations of the same word in different contexts still have a greater cosine similarity than those of two different words, this <a href="https://en.wikipedia.org/wiki/Self-similarity">self-similarity</a> is much lower in upper layers. This suggests that upper layers of contextualizing models produce more context-specific representations, much like how upper layers of LSTMs produce more task-specific representations. In all layers of ELMo, BERT, and GPT-2, on average, less than 5 % of the variance in a word’s contextualized representations can be explained by a static embedding for that word, providing some justification for the success of contextualized representations.</abstract>
      <url hash="dda51fa6">D19-1006</url>
      <doi>10.18653/v1/D19-1006</doi>
      <bibkey>ethayarajh-2019-contextual</bibkey>
    </paper>
    <paper id="8">
      <title>Correlations between Word Vector Sets</title>
      <author><first>Vitalii</first><last>Zhelezniak</last></author>
      <author><first>April</first><last>Shen</last></author>
      <author><first>Daniel</first><last>Busbridge</last></author>
      <author><first>Aleksandar</first><last>Savkov</last></author>
      <author><first>Nils</first><last>Hammerla</last></author>
      <pages>77–87</pages>
      <abstract>Similarity measures based purely on <a href="https://en.wikipedia.org/wiki/Word_embedding">word embeddings</a> are comfortably competing with much more sophisticated deep learning and expert-engineered systems on unsupervised semantic textual similarity (STS) tasks. In contrast to commonly used <a href="https://en.wikipedia.org/wiki/Glossary_of_graph_theory_terms">geometric approaches</a>, we treat a <a href="https://en.wikipedia.org/wiki/Glossary_of_graph_theory_terms">single word embedding</a> as e.g. 300 observations from a <a href="https://en.wikipedia.org/wiki/Scalar_(mathematics)">scalar random variable</a>. Using this paradigm, we first illustrate that similarities derived from elementary pooling operations and classic correlation coefficients yield excellent results on standard STS benchmarks, outperforming many recently proposed methods while being much faster and trivial to implement. Next, we demonstrate how to avoid pooling operations altogether and compare sets of <a href="https://en.wikipedia.org/wiki/Word_embedding">word embeddings</a> directly via correlation operators between reproducing kernel Hilbert spaces. Just like <a href="https://en.wikipedia.org/wiki/Cosine_similarity">cosine similarity</a> is used to compare individual word vectors, we introduce a novel application of the centered kernel alignment (CKA) as a natural generalisation of squared cosine similarity for sets of word vectors. Likewise, CKA is very easy to implement and enjoys very strong empirical results.</abstract>
      <url hash="780cf34a">D19-1008</url>
      <attachment hash="1064108b">D19-1008.Attachment.zip</attachment>
      <doi>10.18653/v1/D19-1008</doi>
      <bibkey>zhelezniak-etal-2019-correlations</bibkey>
      <pwccode url="https://github.com/Babylonpartners/corrsim" additional="false">Babylonpartners/corrsim</pwccode>
    </paper>
    <paper id="10">
      <title>Guided Dialog Policy Learning : Reward Estimation for Multi-Domain Task-Oriented Dialog</title>
      <author><first>Ryuichi</first><last>Takanobu</last></author>
      <author><first>Hanlin</first><last>Zhu</last></author>
      <author><first>Minlie</first><last>Huang</last></author>
      <pages>100–110</pages>
      <abstract>Dialog policy decides what and how a task-oriented dialog system will respond, and plays a vital role in delivering effective conversations. Many studies apply <a href="https://en.wikipedia.org/wiki/Reinforcement_learning">Reinforcement Learning</a> to learn a dialog policy with the reward function which requires elaborate design and pre-specified user goals. With the growing needs to handle complex goals across multiple domains, such manually designed reward functions are not affordable to deal with the complexity of real-world tasks. To this end, we propose Guided Dialog Policy Learning, a novel algorithm based on Adversarial Inverse Reinforcement Learning for joint reward estimation and policy optimization in multi-domain task-oriented dialog. The proposed approach estimates the <a href="https://en.wikipedia.org/wiki/Reward_system">reward signal</a> and infers the <a href="https://en.wikipedia.org/wiki/Goal">user goal</a> in the <a href="https://en.wikipedia.org/wiki/Dialogue">dialog sessions</a>. The reward estimator evaluates the state-action pairs so that it can guide the dialog policy at each dialog turn. Extensive experiments on a multi-domain dialog dataset show that the dialog policy guided by the learned reward function achieves remarkably higher task success than state-of-the-art baselines.</abstract>
      <url hash="7a56c2be">D19-1010</url>
      <attachment hash="b69647d9">D19-1010.Attachment.zip</attachment>
      <doi>10.18653/v1/D19-1010</doi>
      <bibkey>takanobu-etal-2019-guided</bibkey>
      <pwccode url="https://github.com/truthless11/GDPL" additional="false">truthless11/GDPL</pwccode>
    </paper>
    <paper id="11">
      <title>Multi-hop Selector Network for Multi-turn Response Selection in Retrieval-based Chatbots</title>
      <author><first>Chunyuan</first><last>Yuan</last></author>
      <author><first>Wei</first><last>Zhou</last></author>
      <author><first>Mingming</first><last>Li</last></author>
      <author><first>Shangwen</first><last>Lv</last></author>
      <author><first>Fuqing</first><last>Zhu</last></author>
      <author><first>Jizhong</first><last>Han</last></author>
      <author><first>Songlin</first><last>Hu</last></author>
      <pages>111–120</pages>
      <abstract>Multi-turn retrieval-based conversation is an important task for building intelligent dialogue systems. Existing works mainly focus on matching candidate responses with every context utterance on multiple levels of granularity, which ignore the side effect of using excessive context information. Context utterances provide abundant information for extracting more matching features, but it also brings <a href="https://en.wikipedia.org/wiki/Noise_(signal_processing)">noise signals</a> and <a href="https://en.wikipedia.org/wiki/Information_overload">unnecessary information</a>. In this paper, we will analyze the side effect of using too many context utterances and propose a multi-hop selector network (MSN) to alleviate the problem. Specifically, <a href="https://en.wikipedia.org/wiki/MSN">MSN</a> firstly utilizes a multi-hop selector to select the relevant utterances as context. Then, the <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> matches the filtered context with the candidate response and obtains a matching score. Experimental results show that <a href="https://en.wikipedia.org/wiki/MSN">MSN</a> outperforms some state-of-the-art methods on three public multi-turn dialogue datasets.</abstract>
      <url hash="e3e4086c">D19-1011</url>
      <doi>10.18653/v1/D19-1011</doi>
      <bibkey>yuan-etal-2019-multi</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/douban">Douban</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/e-commerce-1">E-commerce</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/rrs">RRS</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/ubuntu-dialogue-corpus">UDC</pwcdataset>
    </paper>
    <paper id="12">
      <title>MoEL : Mixture of Empathetic Listeners<fixed-case>M</fixed-case>o<fixed-case>EL</fixed-case>: Mixture of Empathetic Listeners</title>
      <author><first>Zhaojiang</first><last>Lin</last></author>
      <author><first>Andrea</first><last>Madotto</last></author>
      <author><first>Jamin</first><last>Shin</last></author>
      <author><first>Peng</first><last>Xu</last></author>
      <author><first>Pascale</first><last>Fung</last></author>
      <pages>121–132</pages>
      <abstract>Previous research on empathetic dialogue systems has mostly focused on generating responses given certain emotions. However, being empathetic not only requires the ability of generating emotional responses, but more importantly, requires the understanding of user emotions and replying appropriately. In this paper, we propose a novel end-to-end approach for modeling empathy in dialogue systems : Mixture of Empathetic Listeners (MoEL). Our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> first captures the <a href="https://en.wikipedia.org/wiki/Emotion">user emotions</a> and outputs an emotion distribution. Based on this, MoEL will softly combine the output states of the appropriate Listener(s), which are each optimized to react to certain emotions, and generate an empathetic response. Human evaluations on EMPATHETIC-DIALOGUES dataset confirm that MoEL outperforms multitask training baseline in terms of <a href="https://en.wikipedia.org/wiki/Empathy">empathy</a>, <a href="https://en.wikipedia.org/wiki/Relevance">relevance</a>, and <a href="https://en.wikipedia.org/wiki/Fluency">fluency</a>. Furthermore, the case study on generated responses of different Listeners shows high interpretability of our <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a>.</abstract>
      <url hash="5a78899d">D19-1012</url>
      <doi>10.18653/v1/D19-1012</doi>
      <bibkey>lin-etal-2019-moel</bibkey>
      <pwccode url="https://github.com/HLTCHKUST/MoEL" additional="true">HLTCHKUST/MoEL</pwccode>
    </paper>
    <paper id="13">
      <title>Entity-Consistent End-to-end Task-Oriented Dialogue System with KB Retriever<fixed-case>KB</fixed-case> Retriever</title>
      <author><first>Libo</first><last>Qin</last></author>
      <author><first>Yijia</first><last>Liu</last></author>
      <author><first>Wanxiang</first><last>Che</last></author>
      <author><first>Haoyang</first><last>Wen</last></author>
      <author><first>Yangming</first><last>Li</last></author>
      <author><first>Ting</first><last>Liu</last></author>
      <pages>133–142</pages>
      <abstract>Querying the knowledge base (KB) has long been a challenge in the end-to-end task-oriented dialogue system. Previous sequence-to-sequence (Seq2Seq) dialogue generation work treats the <a href="https://en.wikipedia.org/wiki/Kibibyte">KB query</a> as an attention over the entire <a href="https://en.wikipedia.org/wiki/Kibibyte">KB</a>, without the guarantee that the generated entities are consistent with each other. In this paper, we propose a novel <a href="https://en.wikipedia.org/wiki/Software_framework">framework</a> which queries the <a href="https://en.wikipedia.org/wiki/Kibibyte">KB</a> in two steps to improve the consistency of generated entities. In the first step, inspired by the observation that a response can usually be supported by a single <a href="https://en.wikipedia.org/wiki/Row_(database)">KB row</a>, we introduce a KB retrieval component which explicitly returns the most relevant <a href="https://en.wikipedia.org/wiki/Row_(database)">KB row</a> given a dialogue history. The retrieval result is further used to filter the irrelevant entities in a Seq2Seq response generation model to improve the consistency among the output entities. In the second step, we further perform the <a href="https://en.wikipedia.org/wiki/Attentional_control">attention mechanism</a> to address the most correlated KB column. Two methods are proposed to make the training feasible without labeled retrieval data, which include distant supervision and Gumbel-Softmax technique. Experiments on two publicly available task oriented dialog datasets show the effectiveness of our model by outperforming the baseline systems and producing entity-consistent responses.</abstract>
      <url hash="79ee4194">D19-1013</url>
      <doi>10.18653/v1/D19-1013</doi>
      <bibkey>qin-etal-2019-entity</bibkey>
      <pwccode url="https://github.com/yizhen20133868/Retriever-Dialogue" additional="false">yizhen20133868/Retriever-Dialogue</pwccode>
    </paper>
    <paper id="15">
      <title>DialogueGCN : A Graph Convolutional Neural Network for Emotion Recognition in Conversation<fixed-case>D</fixed-case>ialogue<fixed-case>GCN</fixed-case>: A Graph Convolutional Neural Network for Emotion Recognition in Conversation</title>
      <author><first>Deepanway</first><last>Ghosal</last></author>
      <author><first>Navonil</first><last>Majumder</last></author>
      <author><first>Soujanya</first><last>Poria</last></author>
      <author><first>Niyati</first><last>Chhaya</last></author>
      <author><first>Alexander</first><last>Gelbukh</last></author>
      <pages>154–164</pages>
      <abstract>Emotion recognition in conversation (ERC) has received much attention, lately, from researchers due to its potential widespread applications in diverse areas, such as <a href="https://en.wikipedia.org/wiki/Health_care">health-care</a>, education, and <a href="https://en.wikipedia.org/wiki/Human_resources">human resources</a>. In this paper, we present Dialogue Graph Convolutional Network (DialogueGCN), a graph neural network based approach to ERC. We leverage self and inter-speaker dependency of the interlocutors to model conversational context for <a href="https://en.wikipedia.org/wiki/Emotion_recognition">emotion recognition</a>. Through the <a href="https://en.wikipedia.org/wiki/Graph_(abstract_data_type)">graph network</a>, DialogueGCN addresses context propagation issues present in the current RNN-based methods. We empirically show that this method alleviates such issues, while outperforming the current state of the art on a number of benchmark emotion classification datasets.</abstract>
      <url hash="29eb7474">D19-1015</url>
      <doi>10.18653/v1/D19-1015</doi>
      <bibkey>ghosal-etal-2019-dialoguegcn</bibkey>
      <pwccode url="https://github.com/SenticNet/conv-emotion" additional="true">SenticNet/conv-emotion</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/iemocap">IEMOCAP</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/meld">MELD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/semaine">SEMAINE</pwcdataset>
    </paper>
    <paper id="18">
      <title>Justifying Recommendations using Distantly-Labeled Reviews and Fine-Grained Aspects</title>
      <author><first>Jianmo</first><last>Ni</last></author>
      <author><first>Jiacheng</first><last>Li</last></author>
      <author><first>Julian</first><last>McAuley</last></author>
      <pages>188–197</pages>
      <abstract>Several recent works have considered the problem of generating reviews (or ‘tips’) as a form of explanation as to why a recommendation might match a customer’s interests. While promising, we demonstrate that existing approaches struggle (in terms of both quality and content) to generate justifications that are relevant to users’ decision-making process. We seek to introduce new <a href="https://en.wikipedia.org/wiki/Data_set">datasets</a> and <a href="https://en.wikipedia.org/wiki/Methodology">methods</a> to address the recommendation justification task. In terms of data, we first propose an ‘extractive’ approach to identify review segments which justify users’ intentions ; this approach is then used to distantly label massive review corpora and construct large-scale personalized recommendation justification datasets. In terms of generation, we are able to design two personalized generation models with this data : (1) a reference-based Seq2Seq model with aspect-planning which can generate justifications covering different aspects, and (2) an aspect-conditional masked language model which can generate diverse justifications based on templates extracted from justification histories. We conduct experiments on two real-world datasets which show that our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> is capable of generating convincing and diverse justifications.</abstract>
      <url hash="96ff29df">D19-1018</url>
      <doi>10.18653/v1/D19-1018</doi>
      <bibkey>ni-etal-2019-justifying</bibkey>
    </paper>
    <paper id="19">
      <title>Using Customer Service Dialogues for Satisfaction Analysis with Context-Assisted Multiple Instance Learning</title>
      <author><first>Kaisong</first><last>Song</last></author>
      <author><first>Lidong</first><last>Bing</last></author>
      <author><first>Wei</first><last>Gao</last></author>
      <author><first>Jun</first><last>Lin</last></author>
      <author><first>Lujun</first><last>Zhao</last></author>
      <author><first>Jiancheng</first><last>Wang</last></author>
      <author><first>Changlong</first><last>Sun</last></author>
      <author><first>Xiaozhong</first><last>Liu</last></author>
      <author><first>Qiong</first><last>Zhang</last></author>
      <pages>198–207</pages>
      <abstract>Customers ask questions and customer service staffs answer their questions, which is the basic <a href="https://en.wikipedia.org/wiki/Service_model">service model</a> via multi-turn customer service (CS) dialogues on <a href="https://en.wikipedia.org/wiki/E-commerce">E-commerce platforms</a>. Existing studies fail to provide comprehensive service satisfaction analysis, namely satisfaction polarity classification (e.g., well satisfied, met and unsatisfied) and sentimental utterance identification (e.g., positive, neutral and negative). In this paper, we conduct a pilot study on the task of service satisfaction analysis (SSA) based on multi-turn CS dialogues. We propose an extensible Context-Assisted Multiple Instance Learning (CAMIL) model to predict the <a href="https://en.wikipedia.org/wiki/Sentimentality">sentiments</a> of all the customer utterances and then aggregate those <a href="https://en.wikipedia.org/wiki/Sentimentality">sentiments</a> into service satisfaction polarity. After that, we propose a novel Context Clue Matching Mechanism (CCMM) to enhance the representations of all customer utterances with their matched context clues, i.e., sentiment and reasoning clues. We construct two CS dialogue datasets from a top E-commerce platform. Extensive experimental results are presented and contrasted against a few previous <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> to demonstrate the efficacy of our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a>.</abstract>
      <url hash="bc928ee2">D19-1019</url>
      <doi>10.18653/v1/D19-1019</doi>
      <bibkey>song-etal-2019-using</bibkey>
    </paper>
    <paper id="22">
      <title>Improving <a href="https://en.wikipedia.org/wiki/Relation_extraction">Relation Extraction</a> with Knowledge-attention</title>
      <author><first>Pengfei</first><last>Li</last></author>
      <author><first>Kezhi</first><last>Mao</last></author>
      <author><first>Xuefeng</first><last>Yang</last></author>
      <author><first>Qi</first><last>Li</last></author>
      <pages>229–239</pages>
      <abstract>While <a href="https://en.wikipedia.org/wiki/Attentional_control">attention mechanisms</a> have been proven to be effective in many NLP tasks, majority of them are data-driven. We propose a novel knowledge-attention encoder which incorporates prior knowledge from external lexical resources into <a href="https://en.wikipedia.org/wiki/Deep_learning">deep neural networks</a> for relation extraction task. Furthermore, we present three effective ways of integrating knowledge-attention with self-attention to maximize the utilization of both knowledge and data. The proposed relation extraction system is end-to-end and fully attention-based. Experiment results show that the proposed knowledge-attention mechanism has complementary strengths with self-attention, and our integrated models outperform existing CNN, RNN, and self-attention based models. State-of-the-art performance is achieved on TACRED, a complex and large-scale relation extraction dataset.</abstract>
      <url hash="61db8449">D19-1022</url>
      <attachment hash="66b5ca49">D19-1022.Attachment.pdf</attachment>
      <doi>10.18653/v1/D19-1022</doi>
      <bibkey>li-etal-2019-improving</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/semeval-2010-task-8">SemEval-2010 Task 8</pwcdataset>
    </paper>
    <paper id="24">
      <title>Tackling Long-Tailed Relations and Uncommon Entities in Knowledge Graph Completion</title>
      <author><first>Zihao</first><last>Wang</last></author>
      <author><first>Kwunping</first><last>Lai</last></author>
      <author><first>Piji</first><last>Li</last></author>
      <author><first>Lidong</first><last>Bing</last></author>
      <author><first>Wai</first><last>Lam</last></author>
      <pages>250–260</pages>
      <abstract>For large-scale knowledge graphs (KGs), recent research has been focusing on the large proportion of infrequent relations which have been ignored by previous studies. For example few-shot learning paradigm for <a href="https://en.wikipedia.org/wiki/Binary_relation">relations</a> has been investigated. In this work, we further advocate that handling uncommon entities is inevitable when dealing with infrequent relations. Therefore, we propose a meta-learning framework that aims at handling infrequent relations with few-shot learning and uncommon entities by using textual descriptions. We design a novel <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a> to better extract key information from textual descriptions. Besides, we also develop a novel <a href="https://en.wikipedia.org/wiki/Generative_model">generative model</a> in our <a href="https://en.wikipedia.org/wiki/Conceptual_framework">framework</a> to enhance the performance by generating extra triplets during the training stage. Experiments are conducted on two datasets from real-world KGs, and the results show that our framework outperforms previous methods when dealing with infrequent relations and their accompanying uncommon entities.</abstract>
      <url hash="6fa02eac">D19-1024</url>
      <attachment hash="798bc8c5">D19-1024.Attachment.zip</attachment>
      <doi>10.18653/v1/D19-1024</doi>
      <bibkey>wang-etal-2019-tackling</bibkey>
    </paper>
    <paper id="25">
      <title>Low-Resource Name Tagging Learned with Weakly Labeled Data</title>
      <author><first>Yixin</first><last>Cao</last></author>
      <author><first>Zikun</first><last>Hu</last></author>
      <author><first>Tat-seng</first><last>Chua</last></author>
      <author><first>Zhiyuan</first><last>Liu</last></author>
      <author><first>Heng</first><last>Ji</last></author>
      <pages>261–270</pages>
      <abstract>Name tagging in low-resource languages or domains suffers from inadequate training data. Existing work heavily relies on additional information, while leaving those noisy annotations unexplored that extensively exist on the web. In this paper, we propose a novel neural model for name tagging solely based on weakly labeled (WL) data, so that it can be applied in any low-resource settings. To take the best advantage of all WL sentences, we split them into high-quality and noisy portions for two modules, respectively : (1) a classification module focusing on the large portion of noisy data can efficiently and robustly pretrain the tag classifier by capturing textual context semantics ; and (2) a costly sequence labeling module focusing on high-quality data utilizes Partial-CRFs with non-entity sampling to achieve global optimum. Two <a href="https://en.wikipedia.org/wiki/Module_(computer_science)">modules</a> are combined via <a href="https://en.wikipedia.org/wiki/Parameter_(computer_programming)">shared parameters</a>. Extensive experiments involving five low-resource languages and fine-grained food domain demonstrate our superior performance (6 % and 7.8 % F1 gains on average) as well as <a href="https://en.wikipedia.org/wiki/Efficiency">efficiency</a>.</abstract>
      <url hash="d94e7a17">D19-1025</url>
      <doi>10.18653/v1/D19-1025</doi>
      <bibkey>cao-etal-2019-low</bibkey>
      <pwccode url="https://github.com/zig-kwin-hu/Low-Resource-Name-Tagging" additional="false">zig-kwin-hu/Low-Resource-Name-Tagging</pwccode>
    </paper>
    <paper id="26">
      <title>Learning Dynamic Context Augmentation for Global Entity Linking</title>
      <author><first>Xiyuan</first><last>Yang</last></author>
      <author><first>Xiaotao</first><last>Gu</last></author>
      <author><first>Sheng</first><last>Lin</last></author>
      <author><first>Siliang</first><last>Tang</last></author>
      <author><first>Yueting</first><last>Zhuang</last></author>
      <author><first>Fei</first><last>Wu</last></author>
      <author><first>Zhigang</first><last>Chen</last></author>
      <author><first>Guoping</first><last>Hu</last></author>
      <author><first>Xiang</first><last>Ren</last></author>
      <pages>271–281</pages>
      <abstract>Despite of the recent success of collective entity linking (EL) methods, these global inference methods may yield sub-optimal results when the all-mention coherence assumption breaks, and often suffer from high computational cost at the inference stage, due to the complex search space. In this paper, we propose a simple yet effective solution, called Dynamic Context Augmentation (DCA), for collective EL, which requires only one pass through the mentions in a document. DCA sequentially accumulates context information to make efficient, collective inference, and can cope with different local EL models as a plug-and-enhance module. We explore both supervised and reinforcement learning strategies for learning the DCA model. Extensive experiments show the effectiveness of our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> with different learning settings, base models, <a href="https://en.wikipedia.org/wiki/Decision-making">decision orders</a> and <a href="https://en.wikipedia.org/wiki/Attentional_control">attention mechanisms</a>.</abstract>
      <url hash="55a08168">D19-1026</url>
      <attachment hash="82a47bc3">D19-1026.Attachment.zip</attachment>
      <doi>10.18653/v1/D19-1026</doi>
      <bibkey>yang-etal-2019-learning</bibkey>
      <pwccode url="https://github.com/YoungXiyuan/DCA" additional="true">YoungXiyuan/DCA</pwccode>
    </paper>
    <paper id="28">
      <title>Learning to Bootstrap for Entity Set Expansion</title>
      <author><first>Lingyong</first><last>Yan</last></author>
      <author><first>Xianpei</first><last>Han</last></author>
      <author><first>Le</first><last>Sun</last></author>
      <author><first>Ben</first><last>He</last></author>
      <pages>292–301</pages>
      <abstract>Bootstrapping for Entity Set Expansion (ESE) aims at iteratively acquiring new instances of a specific target category. Traditional bootstrapping methods often suffer from two problems : 1) delayed feedback, i.e., the pattern evaluation relies on both its direct extraction quality and extraction quality in later iterations. 2) sparse supervision, i.e., only few seed entities are used as the <a href="https://en.wikipedia.org/wiki/Supervisor">supervision</a>. To address the above two problems, we propose a novel bootstrapping method combining the Monte Carlo Tree Search (MCTS) algorithm with a deep similarity network, which can efficiently estimate delayed feedback for pattern evaluation and adaptively score entities given sparse supervision signals. Experimental results confirm the effectiveness of the proposed <a href="https://en.wikipedia.org/wiki/Methodology">method</a>.</abstract>
      <url hash="05f3372f">D19-1028</url>
      <doi>10.18653/v1/D19-1028</doi>
      <bibkey>yan-etal-2019-learning</bibkey>
    </paper>
    <paper id="29">
      <title>Multi-Input Multi-Output Sequence Labeling for Joint Extraction of Fact and Condition Tuples from Scientific Text</title>
      <author><first>Tianwen</first><last>Jiang</last></author>
      <author><first>Tong</first><last>Zhao</last></author>
      <author><first>Bing</first><last>Qin</last></author>
      <author><first>Ting</first><last>Liu</last></author>
      <author><first>Nitesh</first><last>Chawla</last></author>
      <author><first>Meng</first><last>Jiang</last></author>
      <pages>302–312</pages>
      <abstract>Condition is essential in <a href="https://en.wikipedia.org/wiki/Scientific_literature">scientific statement</a>. Without the conditions (e.g., equipment, environment) that were precisely specified, facts (e.g., observations) in the statements may no longer be valid. Existing ScienceIE methods, which aim at extracting factual tuples from scientific text, do not consider the conditions. In this work, we propose a new sequence labeling framework (as well as a new tag schema) to jointly extract the fact and condition tuples from statement sentences. The <a href="https://en.wikipedia.org/wiki/Software_framework">framework</a> has (1) a multi-output module to generate one or multiple tuples and (2) a multi-input module to feed in multiple types of signals as sequences. It improves <a href="https://en.wikipedia.org/wiki/F1_score">F1 score</a> relatively by 4.2 % on BioNLP2013 and by 6.2 % on a new bio-text dataset for tuple extraction.</abstract>
      <url hash="23c10558">D19-1029</url>
      <doi>10.18653/v1/D19-1029</doi>
      <bibkey>jiang-etal-2019-multi</bibkey>
    </paper>
    <paper id="30">
      <title>Cross-lingual Structure Transfer for Relation and Event Extraction</title>
      <author><first>Ananya</first><last>Subburathinam</last></author>
      <author><first>Di</first><last>Lu</last></author>
      <author><first>Heng</first><last>Ji</last></author>
      <author><first>Jonathan</first><last>May</last></author>
      <author><first>Shih-Fu</first><last>Chang</last></author>
      <author><first>Avirup</first><last>Sil</last></author>
      <author><first>Clare</first><last>Voss</last></author>
      <pages>313–325</pages>
      <abstract>The identification of complex semantic structures such as <a href="https://en.wikipedia.org/wiki/Event_(philosophy)">events</a> and <a href="https://en.wikipedia.org/wiki/Entity–relationship_model">entity relations</a>, already a challenging Information Extraction task, is doubly difficult from sources written in under-resourced and under-annotated languages. We investigate the suitability of cross-lingual structure transfer techniques for these <a href="https://en.wikipedia.org/wiki/Task_(project_management)">tasks</a>. We exploit relation- and event-relevant language-universal features, leveraging both symbolic (including part-of-speech and dependency path) and distributional (including type representation and contextualized representation) information. By representing all entity mentions, event triggers, and contexts into this complex and structured multilingual common space, using graph convolutional networks, we can train a relation or event extractor from source language annotations and apply it to the target language. Extensive experiments on cross-lingual relation and event transfer among <a href="https://en.wikipedia.org/wiki/English_language">English</a>, <a href="https://en.wikipedia.org/wiki/Chinese_language">Chinese</a>, and <a href="https://en.wikipedia.org/wiki/Arabic">Arabic</a> demonstrate that our approach achieves performance comparable to state-of-the-art <a href="https://en.wikipedia.org/wiki/Supervised_learning">supervised models</a> trained on up to 3,000 manually annotated mentions : up to 62.6 % F-score for Relation Extraction, and 63.1 % F-score for Event Argument Role Labeling. The event argument role labeling model transferred from <a href="https://en.wikipedia.org/wiki/English_language">English</a> to <a href="https://en.wikipedia.org/wiki/Chinese_language">Chinese</a> achieves similar performance as the model trained from <a href="https://en.wikipedia.org/wiki/Chinese_language">Chinese</a>. We thus find that language-universal symbolic and distributional representations are complementary for cross-lingual structure transfer.</abstract>
      <url hash="c427a095">D19-1030</url>
      <doi>10.18653/v1/D19-1030</doi>
      <bibkey>subburathinam-etal-2019-cross</bibkey>
    </paper>
    <paper id="32">
      <title>Doc2EDAG : An End-to-End Document-level Framework for Chinese Financial Event Extraction<fixed-case>D</fixed-case>oc2<fixed-case>EDAG</fixed-case>: An End-to-End Document-level Framework for <fixed-case>C</fixed-case>hinese Financial Event Extraction</title>
      <author><first>Shun</first><last>Zheng</last></author>
      <author><first>Wei</first><last>Cao</last></author>
      <author><first>Wei</first><last>Xu</last></author>
      <author><first>Jiang</first><last>Bian</last></author>
      <pages>337–346</pages>
      <abstract>Most existing event extraction (EE) methods merely extract event arguments within the sentence scope. However, such sentence-level EE methods struggle to handle soaring amounts of documents from emerging applications, such as <a href="https://en.wikipedia.org/wiki/Finance">finance</a>, <a href="https://en.wikipedia.org/wiki/Legislation">legislation</a>, <a href="https://en.wikipedia.org/wiki/Health">health</a>, etc., where event arguments always scatter across different sentences, and even multiple such event mentions frequently co-exist in the same document. To address these challenges, we propose a novel end-to-end model, Doc2EDAG, which can generate an entity-based directed acyclic graph to fulfill the document-level EE (DEE) effectively. Moreover, we reformalize a DEE task with the no-trigger-words design to ease the document-level event labeling. To demonstrate the effectiveness of Doc2EDAG, we build a large-scale real-world dataset consisting of Chinese financial announcements with the challenges mentioned above. Extensive experiments with comprehensive analyses illustrate the superiority of Doc2EDAG over state-of-the-art methods. Data and codes can be found at https://github.com/dolphin-zs/Doc2EDAG.</abstract>
      <url hash="cde1bc48">D19-1032</url>
      <attachment hash="b4831ae9">D19-1032.Attachment.zip</attachment>
      <doi>10.18653/v1/D19-1032</doi>
      <bibkey>zheng-etal-2019-doc2edag</bibkey>
      <pwccode url="https://github.com/dolphin-zs/Doc2EDAG" additional="true">dolphin-zs/Doc2EDAG</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/chfinann">ChFinAnn</pwcdataset>
    </paper>
    <paper id="34">
      <title>A Boundary-aware Neural Model for Nested Named Entity Recognition</title>
      <author><first>Changmeng</first><last>Zheng</last></author>
      <author><first>Yi</first><last>Cai</last></author>
      <author><first>Jingyun</first><last>Xu</last></author>
      <author><first>Ho-fung</first><last>Leung</last></author>
      <author><first>Guandong</first><last>Xu</last></author>
      <pages>357–366</pages>
      <abstract>In <a href="https://en.wikipedia.org/wiki/Natural_language_processing">natural language processing</a>, it is common that many entities contain other entities inside them. Most existing works on named entity recognition (NER) only deal with flat entities but ignore nested ones. We propose a boundary-aware neural model for nested NER which leverages entity boundaries to predict entity categorical labels. Our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> can locate entities precisely by detecting boundaries using sequence labeling models. Based on the detected boundaries, our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> utilizes the boundary-relevant regions to predict entity categorical labels, which can decrease computation cost and relieve error propagation problem in layered sequence labeling model. We introduce <a href="https://en.wikipedia.org/wiki/Multitask_learning">multitask learning</a> to capture the dependencies of entity boundaries and their categorical labels, which helps to improve the performance of identifying entities. We conduct our experiments on GENIA dataset and the experimental results demonstrate that our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> outperforms other state-of-the-art methods.</abstract>
      <url hash="6d7a79d5">D19-1034</url>
      <attachment hash="7cc1ed15">D19-1034.Attachment.zip</attachment>
      <doi>10.18653/v1/D19-1034</doi>
      <bibkey>zheng-etal-2019-boundary</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/genia">GENIA</pwcdataset>
    </paper>
    <paper id="35">
      <title>Learning the Extraction Order of Multiple Relational Facts in a Sentence with <a href="https://en.wikipedia.org/wiki/Reinforcement_learning">Reinforcement Learning</a></title>
      <author><first>Xiangrong</first><last>Zeng</last></author>
      <author><first>Shizhu</first><last>He</last></author>
      <author><first>Daojian</first><last>Zeng</last></author>
      <author><first>Kang</first><last>Liu</last></author>
      <author><first>Shengping</first><last>Liu</last></author>
      <author><first>Jun</first><last>Zhao</last></author>
      <pages>367–377</pages>
      <abstract>The multiple relation extraction task tries to extract all relational facts from a sentence. Existing works did n’t consider the extraction order of relational facts in a sentence. In this paper we argue that the extraction order is important in this task. To take the extraction order into consideration, we apply the <a href="https://en.wikipedia.org/wiki/Reinforcement_learning">reinforcement learning</a> into a sequence-to-sequence model. The proposed <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> could generate <a href="https://en.wikipedia.org/wiki/Relational_model">relational facts</a> freely. Widely conducted experiments on two public datasets demonstrate the efficacy of the proposed <a href="https://en.wikipedia.org/wiki/Methodology">method</a>.</abstract>
      <url hash="37b19be3">D19-1035</url>
      <doi>10.18653/v1/D19-1035</doi>
      <bibkey>zeng-etal-2019-learning</bibkey>
    </paper>
    <paper id="40">
      <title>EntEval : A Holistic Evaluation Benchmark for Entity Representations<fixed-case>E</fixed-case>nt<fixed-case>E</fixed-case>val: A Holistic Evaluation Benchmark for Entity Representations</title>
      <author><first>Mingda</first><last>Chen</last></author>
      <author><first>Zewei</first><last>Chu</last></author>
      <author><first>Yang</first><last>Chen</last></author>
      <author><first>Karl</first><last>Stratos</last></author>
      <author><first>Kevin</first><last>Gimpel</last></author>
      <pages>421–433</pages>
      <abstract>Rich entity representations are useful for a wide class of problems involving entities. Despite their importance, there is no standardized benchmark that evaluates the overall quality of <a href="https://en.wikipedia.org/wiki/Entity–relationship_model">entity representations</a>. In this work, we propose EntEval : a test suite of diverse tasks that require nontrivial understanding of entities including <a href="https://en.wikipedia.org/wiki/Entity–relationship_model">entity typing</a>, <a href="https://en.wikipedia.org/wiki/Entity–relationship_model">entity similarity</a>, <a href="https://en.wikipedia.org/wiki/Entity–relationship_model">entity relation prediction</a>, and <a href="https://en.wikipedia.org/wiki/Entity–relationship_model">entity disambiguation</a>. In addition, we develop <a href="https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets">training techniques</a> for learning better <a href="https://en.wikipedia.org/wiki/Entity–relationship_model">entity representations</a> by using natural hyperlink annotations in <a href="https://en.wikipedia.org/wiki/Wikipedia">Wikipedia</a>. We identify effective objectives for incorporating the contextual information in <a href="https://en.wikipedia.org/wiki/Hyperlink">hyperlinks</a> into state-of-the-art pretrained language models (Peters et al., 2018) and show that they improve strong baselines on multiple EntEval tasks.</abstract>
      <url hash="81990993">D19-1040</url>
      <doi>10.18653/v1/D19-1040</doi>
      <bibkey>chen-etal-2019-enteval</bibkey>
      <pwccode url="https://github.com/ZeweiChu/EntEval" additional="true">ZeweiChu/EntEval</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/fever">FEVER</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikisrs">WikiSRS</pwcdataset>
    </paper>
    <paper id="44">
      <title>Label-Specific Document Representation for Multi-Label Text Classification</title>
      <author><first>Lin</first><last>Xiao</last></author>
      <author><first>Xin</first><last>Huang</last></author>
      <author><first>Boli</first><last>Chen</last></author>
      <author><first>Liping</first><last>Jing</last></author>
      <pages>466–475</pages>
      <abstract>Multi-label text classification (MLTC) aims to tag most relevant labels for the given document. In this paper, we propose a Label-Specific Attention Network (LSAN) to learn a label-specific document representation. LSAN takes advantage of label semantic information to determine the semantic connection between labels and document for constructing label-specific document representation. Meanwhile, the self-attention mechanism is adopted to identify the label-specific document representation from document content information. In order to seamlessly integrate the above two parts, an adaptive fusion strategy is proposed, which can effectively output the comprehensive label-specific document representation to build multi-label text classifier. Extensive experimental results demonstrate that LSAN consistently outperforms the state-of-the-art methods on four different datasets, especially on the prediction of low-frequency labels. The code and hyper-parameter settings are released to facilitate other researchers.</abstract>
      <url hash="57d721d1">D19-1044</url>
      <doi>10.18653/v1/D19-1044</doi>
      <bibkey>xiao-etal-2019-label</bibkey>
      <pwccode url="https://github.com/EMNLP2019LSAN/LSAN" additional="false">EMNLP2019LSAN/LSAN</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/20000-utterances">20000 utterances</pwcdataset>
    </paper>
    <paper id="50">
      <title>Linking artificial and human neural representations of language</title>
      <author><first>Jon</first><last>Gauthier</last></author>
      <author><first>Roger</first><last>Levy</last></author>
      <pages>529–539</pages>
      <abstract>What information from an act of sentence understanding is robustly represented in the human brain? We investigate this question by comparing sentence encoding models on a brain decoding task, where the sentence that an experimental participant has seen must be predicted from the fMRI signal evoked by the sentence. We take a pre-trained BERT architecture as a baseline sentence encoding model and fine-tune it on a variety of natural language understanding (NLU) tasks, asking which lead to improvements in brain-decoding performance. We find that none of the sentence encoding tasks tested yield significant increases in brain decoding performance. Through further task ablations and representational analyses, we find that tasks which produce syntax-light representations yield significant improvements in brain decoding performance. Our results constrain the space of NLU models that could best account for human neural representations of language, but also suggest limits on the possibility of decoding fine-grained syntactic information from fMRI human neuroimaging.</abstract>
      <url hash="f312b0b6">D19-1050</url>
      <attachment hash="249074f9">D19-1050.Attachment.zip</attachment>
      <doi>10.18653/v1/D19-1050</doi>
      <bibkey>gauthier-levy-2019-linking</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="51">
      <title>Neural Text Summarization : A Critical Evaluation</title>
      <author><first>Wojciech</first><last>Kryscinski</last></author>
      <author><first>Nitish Shirish</first><last>Keskar</last></author>
      <author><first>Bryan</first><last>McCann</last></author>
      <author><first>Caiming</first><last>Xiong</last></author>
      <author><first>Richard</first><last>Socher</last></author>
      <pages>540–551</pages>
      <abstract>Text summarization aims at compressing long documents into a shorter form that conveys the most important parts of the original document. Despite increased interest in the community and notable research effort, progress on <a href="https://en.wikipedia.org/wiki/Benchmark_(computing)">benchmark datasets</a> has stagnated. We critically evaluate key ingredients of the current research setup : <a href="https://en.wikipedia.org/wiki/Data_set">datasets</a>, evaluation metrics, and <a href="https://en.wikipedia.org/wiki/Statistical_model">models</a>, and highlight three primary shortcomings : 1) automatically collected datasets leave the task underconstrained and may contain noise detrimental to training and evaluation, 2) current evaluation protocol is weakly correlated with human judgment and does not account for important characteristics such as factual correctness, 3) models overfit to layout biases of current datasets and offer limited diversity in their outputs.</abstract>
      <url hash="86e8a904">D19-1051</url>
      <attachment hash="5bb2d5e7">D19-1051.Attachment.zip</attachment>
      <doi>10.18653/v1/D19-1051</doi>
      <bibkey>kryscinski-etal-2019-neural</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/cnn-daily-mail-1">CNN/Daily Mail</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/newsroom">NEWSROOM</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikihow">WikiHow</pwcdataset>
    </paper>
    <paper id="53">
      <title>MoverScore : Text Generation Evaluating with <a href="https://en.wikipedia.org/wiki/Contextualization">Contextualized Embeddings</a> and <a href="https://en.wikipedia.org/wiki/Earth_mover_distance">Earth Mover Distance</a><fixed-case>M</fixed-case>over<fixed-case>S</fixed-case>core: Text Generation Evaluating with Contextualized Embeddings and Earth Mover Distance</title>
      <author><first>Wei</first><last>Zhao</last></author>
      <author><first>Maxime</first><last>Peyrard</last></author>
      <author id="fei-liu-utdallas"><first>Fei</first><last>Liu</last></author>
      <author><first>Yang</first><last>Gao</last></author>
      <author><first>Christian M.</first><last>Meyer</last></author>
      <author><first>Steffen</first><last>Eger</last></author>
      <pages>563–578</pages>
      <abstract>A robust evaluation metric has a profound impact on the development of text generation systems. A desirable <a href="https://en.wikipedia.org/wiki/Metric_(mathematics)">metric</a> compares system output against references based on their <a href="https://en.wikipedia.org/wiki/Semantics">semantics</a> rather than surface forms. In this paper we investigate strategies to encode system and reference texts to devise a <a href="https://en.wikipedia.org/wiki/Metric_(mathematics)">metric</a> that shows a high correlation with human judgment of text quality. We validate our new metric, namely MoverScore, on a number of text generation tasks including <a href="https://en.wikipedia.org/wiki/Automatic_summarization">summarization</a>, <a href="https://en.wikipedia.org/wiki/Machine_translation">machine translation</a>, <a href="https://en.wikipedia.org/wiki/Closed_captioning">image captioning</a>, and data-to-text generation, where the outputs are produced by a variety of neural and non-neural systems. Our findings suggest that <a href="https://en.wikipedia.org/wiki/Metric_(mathematics)">metrics</a> combining contextualized representations with a <a href="https://en.wikipedia.org/wiki/Metric_(mathematics)">distance measure</a> perform the best. Such <a href="https://en.wikipedia.org/wiki/Metric_(mathematics)">metrics</a> also demonstrate strong generalization capability across tasks. For ease-of-use we make our <a href="https://en.wikipedia.org/wiki/Metric_(mathematics)">metrics</a> available as <a href="https://en.wikipedia.org/wiki/Web_service">web service</a>.</abstract>
      <url hash="4ce5de0e">D19-1053</url>
      <attachment hash="38145069">D19-1053.Attachment.pdf</attachment>
      <doi>10.18653/v1/D19-1053</doi>
      <bibkey>zhao-etal-2019-moverscore</bibkey>
      <pwccode url="" additional="true" />
      <pwcdataset url="https://paperswithcode.com/dataset/coco">COCO</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
    </paper>
    <paper id="55">
      <title>Sentence-Level Content Planning and Style Specification for Neural Text Generation</title>
      <author><first>Xinyu</first><last>Hua</last></author>
      <author><first>Lu</first><last>Wang</last></author>
      <pages>591–602</pages>
      <abstract>Building effective text generation systems requires three critical components : content selection, text planning, and surface realization, and traditionally they are tackled as separate problems. Recent all-in-one style neural generation models have made impressive progress, yet they often produce outputs that are incoherent and unfaithful to the input. To address these issues, we present an end-to-end trained two-step generation model, where a sentence-level content planner first decides on the keyphrases to cover as well as a desired language style, followed by a surface realization decoder that generates relevant and coherent text. For experiments, we consider three tasks from domains with diverse topics and varying language styles : persuasive argument construction from <a href="https://en.wikipedia.org/wiki/Reddit">Reddit</a>, paragraph generation for normal and simple versions of <a href="https://en.wikipedia.org/wiki/Wikipedia">Wikipedia</a>, and abstract generation for scientific articles. Automatic evaluation shows that our <a href="https://en.wikipedia.org/wiki/System">system</a> can significantly outperform competitive comparisons. Human judges further rate our <a href="https://en.wikipedia.org/wiki/System">system</a> generated text as more fluent and correct, compared to the generations by its variants that do not consider <a href="https://en.wikipedia.org/wiki/Linguistic_prescription">language style</a>.</abstract>
      <url hash="6e6e9f61">D19-1055</url>
      <attachment hash="9efeb859">D19-1055.Attachment.pdf</attachment>
      <doi>10.18653/v1/D19-1055</doi>
      <bibkey>hua-wang-2019-sentence</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/agenda">AGENDA</pwcdataset>
    </paper>
    <paper id="56">
      <title>Translate and Label ! An Encoder-Decoder Approach for Cross-lingual Semantic Role Labeling</title>
      <author><first>Angel</first><last>Daza</last></author>
      <author><first>Anette</first><last>Frank</last></author>
      <pages>603–615</pages>
      <abstract>We propose a Cross-lingual Encoder-Decoder model that simultaneously translates and generates sentences with Semantic Role Labeling annotations in a resource-poor target language. Unlike annotation projection techniques, our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> does not need parallel data during inference time. Our approach can be applied in monolingual, multilingual and cross-lingual settings and is able to produce dependency-based and span-based SRL annotations. We benchmark the labeling performance of our model in different monolingual and multilingual settings using well-known SRL datasets. We then train our <a href="https://en.wikipedia.org/wiki/Statistical_model">model</a> in a cross-lingual setting to generate new SRL labeled data. Finally, we measure the effectiveness of our method by using the generated data to augment the training basis for resource-poor languages and perform manual evaluation to show that it produces high-quality sentences and assigns accurate semantic role annotations. Our proposed <a href="https://en.wikipedia.org/wiki/Software_architecture">architecture</a> offers a flexible method for leveraging SRL data in multiple languages.</abstract>
      <url hash="ac877210">D19-1056</url>
      <attachment hash="d0f3c9cb">D19-1056.Attachment.zip</attachment>
      <doi>10.18653/v1/D19-1056</doi>
      <bibkey>daza-frank-2019-translate</bibkey>
      <pwccode url="https://github.com/Heidelberg-NLP/SRL-S2S" additional="false">Heidelberg-NLP/SRL-S2S</pwccode>
    </paper>
    <paper id="58">
      <title>VerbAtlas : a Novel Large-Scale Verbal Semantic Resource and Its Application to <a href="https://en.wikipedia.org/wiki/Semantic_Role_Labeling">Semantic Role Labeling</a><fixed-case>V</fixed-case>erb<fixed-case>A</fixed-case>tlas: a Novel Large-Scale Verbal Semantic Resource and Its Application to Semantic Role Labeling</title>
      <author><first>Andrea</first><last>Di Fabio</last></author>
      <author><first>Simone</first><last>Conia</last></author>
      <author><first>Roberto</first><last>Navigli</last></author>
      <pages>627–637</pages>
      <abstract>We present VerbAtlas, a new, hand-crafted lexical-semantic resource whose goal is to bring together all verbal synsets from <a href="https://en.wikipedia.org/wiki/WordNet">WordNet</a> into semantically-coherent frames. The <a href="https://en.wikipedia.org/wiki/Framing_(social_sciences)">frames</a> define a common, prototypical argument structure while at the same time providing new concept-specific information. In contrast to <a href="https://en.wikipedia.org/wiki/PropBank">PropBank</a>, which defines enumerative semantic roles, VerbAtlas comes with an explicit, cross-frame set of semantic roles linked to selectional preferences expressed in terms of WordNet synsets, and is the first resource enriched with semantic information about implicit, shadow, and default arguments. We demonstrate the effectiveness of VerbAtlas in the task of dependency-based Semantic Role Labeling and show how its integration into a high-performance system leads to improvements on both the in-domain and out-of-domain test sets of CoNLL-2009. VerbAtlas is available at http://verbatlas.org.</abstract>
      <url hash="1bef0563">D19-1058</url>
      <doi>10.18653/v1/D19-1058</doi>
      <bibkey>di-fabio-etal-2019-verbatlas</bibkey>
    </paper>
    <paper id="59">
      <title>Parameter-free Sentence Embedding via <a href="https://en.wikipedia.org/wiki/Orthogonal_basis">Orthogonal Basis</a></title>
      <author><first>Ziyi</first><last>Yang</last></author>
      <author><first>Chenguang</first><last>Zhu</last></author>
      <author><first>Weizhu</first><last>Chen</last></author>
      <pages>638–648</pages>
      <abstract>We propose a simple and robust non-parameterized approach for building <a href="https://en.wikipedia.org/wiki/Sentence_(linguistics)">sentence representations</a>. Inspired by the <a href="https://en.wikipedia.org/wiki/Gram–Schmidt_process">Gram-Schmidt Process</a> in geometric theory, we build an <a href="https://en.wikipedia.org/wiki/Orthogonal_basis">orthogonal basis</a> of the <a href="https://en.wikipedia.org/wiki/Linear_subspace">subspace</a> spanned by a word and its surrounding context in a sentence. We model the <a href="https://en.wikipedia.org/wiki/Semantics">semantic meaning</a> of a word in a sentence based on two aspects. One is its relatedness to the word vector subspace already spanned by its contextual words. The other is the word’s novel semantic meaning which shall be introduced as a new <a href="https://en.wikipedia.org/wiki/Basis_(linear_algebra)">basis vector</a> perpendicular to this existing <a href="https://en.wikipedia.org/wiki/Linear_subspace">subspace</a>. Following this motivation, we develop an innovative method based on orthogonal basis to combine pre-trained word embeddings into sentence representations. This approach requires zero parameters, along with efficient <a href="https://en.wikipedia.org/wiki/Statistical_inference">inference</a> performance. We evaluate our approach on 11 downstream NLP tasks. Our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> shows superior performance compared with non-parameterized alternatives and it is competitive to other approaches relying on either large amounts of labelled data or prolonged training time.</abstract>
      <url hash="0b3f44e5">D19-1059</url>
      <attachment hash="4495e8e4">D19-1059.Attachment.pdf</attachment>
      <doi>10.18653/v1/D19-1059</doi>
      <bibkey>yang-etal-2019-parameter</bibkey>
      <pwccode url="https://github.com/ziyi-yang/GEM" additional="false">ziyi-yang/GEM</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/mpqa-opinion-corpus">MPQA Opinion Corpus</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="61">
      <title>Extracting Possessions from <a href="https://en.wikipedia.org/wiki/Social_media">Social Media</a> : Images Complement Language</title>
      <author><first>Dhivya</first><last>Chinnappa</last></author>
      <author><first>Srikala</first><last>Murugan</last></author>
      <author><first>Eduardo</first><last>Blanco</last></author>
      <pages>663–672</pages>
      <abstract>This paper describes a new <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> and experiments to determine whether authors of tweets possess the objects they tweet about. We work with 5,000 tweets and show that both <a href="https://en.wikipedia.org/wiki/Human">humans</a> and <a href="https://en.wikipedia.org/wiki/Neural_network">neural networks</a> benefit from <a href="https://en.wikipedia.org/wiki/Image">images</a> in addition to <a href="https://en.wikipedia.org/wiki/Text_(literary_theory)">text</a>. We also introduce a simple yet effective <a href="https://en.wikipedia.org/wiki/Strategy">strategy</a> to incorporate <a href="https://en.wikipedia.org/wiki/Visual_system">visual information</a> into any <a href="https://en.wikipedia.org/wiki/Neural_network">neural network</a> beyond weights from pretrained networks. Specifically, we consider the <a href="https://en.wikipedia.org/wiki/Tag_(metadata)">tags</a> identified in an <a href="https://en.wikipedia.org/wiki/Image">image</a> as an additional textual input, and leverage pretrained word embeddings as usually done with regular text. Experimental results show this novel <a href="https://en.wikipedia.org/wiki/Strategy">strategy</a> is beneficial.</abstract>
      <url hash="b28ccf63">D19-1061</url>
      <doi>10.18653/v1/D19-1061</doi>
      <bibkey>chinnappa-etal-2019-extracting</bibkey>
    </paper>
    <paper id="62">
      <title>Learning to Speak and Act in a Fantasy Text Adventure Game</title>
      <author><first>Jack</first><last>Urbanek</last></author>
      <author><first>Angela</first><last>Fan</last></author>
      <author><first>Siddharth</first><last>Karamcheti</last></author>
      <author><first>Saachi</first><last>Jain</last></author>
      <author><first>Samuel</first><last>Humeau</last></author>
      <author><first>Emily</first><last>Dinan</last></author>
      <author><first>Tim</first><last>Rocktäschel</last></author>
      <author><first>Douwe</first><last>Kiela</last></author>
      <author><first>Arthur</first><last>Szlam</last></author>
      <author><first>Jason</first><last>Weston</last></author>
      <pages>673–683</pages>
      <abstract>We introduce a large-scale crowdsourced text adventure game as a research platform for studying grounded dialogue. In it, agents can perceive, emote, and act whilst conducting dialogue with other agents. Models and humans can both act as characters within the <a href="https://en.wikipedia.org/wiki/Game">game</a>. We describe the results of training state-of-the-art generative and retrieval models in this setting. We show that in addition to using past dialogue, these <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> are able to effectively use the state of the underlying world to condition their predictions. In particular, we show that grounding on the details of the local environment, including location descriptions, and the objects (and their affordances) and characters (and their previous actions) present within it allows better predictions of agent behavior and <a href="https://en.wikipedia.org/wiki/Dialogue">dialogue</a>. We analyze the ingredients necessary for successful grounding in this <a href="https://en.wikipedia.org/wiki/Setting_(narrative)">setting</a>, and how each of these factors relate to agents that can talk and act successfully.</abstract>
      <url hash="4de9070e">D19-1062</url>
      <attachment hash="4170d6f2">D19-1062.Attachment.zip</attachment>
      <doi>10.18653/v1/D19-1062</doi>
      <bibkey>urbanek-etal-2019-learning</bibkey>
    </paper>
    <paper id="63">
      <title>Help, Anna ! Visual Navigation with Natural Multimodal Assistance via Retrospective Curiosity-Encouraging Imitation Learning</title>
      <author><first>Khanh</first><last>Nguyen</last></author>
      <author><first>Hal</first><last>Daumé III</last></author>
      <pages>684–695</pages>
      <abstract>Mobile agents that can leverage help from humans can potentially accomplish more complex tasks than they could entirely on their own. We develop <a href="https://en.wikipedia.org/wiki/Help_(TV_series)">Help</a>, Anna ! (HANNA), an interactive photo-realistic simulator in which an <a href="https://en.wikipedia.org/wiki/Intelligent_agent">agent</a> fulfills object-finding tasks by requesting and interpreting natural language-and-vision assistance. An agent solving tasks in a HANNA environment can leverage simulated human assistants, called ANNA (Automatic Natural Navigation Assistants), which, upon request, provide natural language and visual instructions to direct the agent towards the goals. To address the HANNA problem, we develop a memory-augmented neural agent that hierarchically models multiple levels of <a href="https://en.wikipedia.org/wiki/Decision-making">decision-making</a>, and an imitation learning algorithm that teaches the agent to avoid repeating past mistakes while simultaneously predicting its own chances of making future progress. Empirically, our approach is able to ask for help more effectively than competitive baselines and, thus, attains higher task success rate on both previously seen and previously unseen environments.</abstract>
      <url hash="41a30a7b">D19-1063</url>
      <attachment hash="c4ec72f3">D19-1063.Attachment.zip</attachment>
      <doi>10.18653/v1/D19-1063</doi>
      <bibkey>nguyen-daume-iii-2019-help</bibkey>
      <pwccode url="https://github.com/khanhptnk/hanna" additional="false">khanhptnk/hanna</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/matterport3d">Matterport3D</pwcdataset>
    </paper>
    <paper id="64">
      <title>Incorporating Visual Semantics into Sentence Representations within a Grounded Space</title>
      <author><first>Patrick</first><last>Bordes</last></author>
      <author><first>Eloi</first><last>Zablocki</last></author>
      <author><first>Laure</first><last>Soulier</last></author>
      <author><first>Benjamin</first><last>Piwowarski</last></author>
      <author><first>Patrick</first><last>Gallinari</last></author>
      <pages>696–707</pages>
      <abstract>Language grounding is an active field aiming at enriching textual representations with <a href="https://en.wikipedia.org/wiki/Visual_system">visual information</a>. Generally, textual and visual elements are embedded in the same <a href="https://en.wikipedia.org/wiki/Representation_space">representation space</a>, which implicitly assumes a one-to-one correspondence between modalities. This hypothesis does not hold when representing words, and becomes problematic when used to learn sentence representations   the focus of this paper   as a visual scene can be described by a wide variety of sentences. To overcome this limitation, we propose to transfer <a href="https://en.wikipedia.org/wiki/Visual_system">visual information</a> to textual representations by learning an intermediate representation space : the grounded space. We further propose two new complementary objectives ensuring that (1) sentences associated with the same visual content are close in the grounded space and (2) similarities between related elements are preserved across modalities. We show that this <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> outperforms the previous <a href="https://en.wikipedia.org/wiki/State_of_the_art">state-of-the-art</a> on classification and semantic relatedness tasks.</abstract>
      <url hash="f2b0946b">D19-1064</url>
      <doi>10.18653/v1/D19-1064</doi>
      <bibkey>bordes-etal-2019-incorporating</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/coco">COCO</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mpqa-opinion-corpus">MPQA Opinion Corpus</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sick">SICK</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/senteval">SentEval</pwcdataset>
    </paper>
    <paper id="65">
      <title>Neural Naturalist : Generating Fine-Grained Image Comparisons</title>
      <author><first>Maxwell</first><last>Forbes</last></author>
      <author><first>Christine</first><last>Kaeser-Chen</last></author>
      <author><first>Piyush</first><last>Sharma</last></author>
      <author><first>Serge</first><last>Belongie</last></author>
      <pages>708–717</pages>
      <abstract>We introduce the new Birds-to-Words dataset of 41k sentences describing fine-grained differences between photographs of birds. The language collected is highly detailed, while remaining understandable to the everyday observer (e.g., <a href="https://en.wikipedia.org/wiki/Heart_(symbol)">heart-shaped face</a>, squat body). Paragraph-length descriptions naturally adapt to varying levels of taxonomic and visual distancedrawn from a novel stratified sampling approachwith the appropriate level of detail. We propose a new model called Neural Naturalist that uses a joint image encoding and comparative module to generate comparative language, and evaluate the results with humans who must use the descriptions to distinguish real images. Our results indicate promising potential for neural models to explain differences in visual embedding space using <a href="https://en.wikipedia.org/wiki/Natural_language">natural language</a>, as well as a concrete path for <a href="https://en.wikipedia.org/wiki/Machine_learning">machine learning</a> to aid citizen scientists in their effort to preserve biodiversity.</abstract>
      <url hash="c669478c">D19-1065</url>
      <attachment hash="3512abef">D19-1065.Attachment.zip</attachment>
      <doi>10.18653/v1/D19-1065</doi>
      <bibkey>forbes-etal-2019-neural</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/cub-200-2011">CUB-200-2011</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/spot-the-diff">Spot-the-diff</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/inaturalist">iNaturalist</pwcdataset>
    </paper>
    <paper id="66">
      <title>Fine-Grained Evaluation for Entity Linking</title>
      <author><first>Henry</first><last>Rosales-Méndez</last></author>
      <author><first>Aidan</first><last>Hogan</last></author>
      <author><first>Barbara</first><last>Poblete</last></author>
      <pages>718–727</pages>
      <abstract>The Entity Linking (EL) task identifies entity mentions in a <a href="https://en.wikipedia.org/wiki/Text_corpus">text corpus</a> and associates them with an unambiguous identifier in a <a href="https://en.wikipedia.org/wiki/Knowledge_base">Knowledge Base</a>. While much work has been done on the topic, we first present the results of a survey that reveal a lack of consensus in the community regarding what forms of mentions in a text and what forms of links the EL task should consider. We argue that no one definition of the Entity Linking task fits all, and rather propose a fine-grained categorization of different types of entity mentions and links. We then re-annotate three EL benchmark datasets   ACE2004, KORE50, and VoxEL   with respect to these categories. We propose a fuzzy recall metric to address the lack of consensus and conclude with fine-grained evaluation results comparing a selection of online EL systems.</abstract>
      <url hash="0ffc4328">D19-1066</url>
      <doi>10.18653/v1/D19-1066</doi>
      <bibkey>rosales-mendez-etal-2019-fine</bibkey>
    </paper>
    <paper id="68">
      <title>Neural Cross-Lingual Event Detection with Minimal Parallel Resources</title>
      <author><first>Jian</first><last>Liu</last></author>
      <author><first>Yubo</first><last>Chen</last></author>
      <author><first>Kang</first><last>Liu</last></author>
      <author><first>Jun</first><last>Zhao</last></author>
      <pages>738–748</pages>
      <abstract>The scarcity in annotated data poses a great challenge for event detection (ED). Cross-lingual ED aims to tackle this challenge by transferring knowledge between different languages to boost performance. However, previous cross-lingual methods for ED demonstrated a heavy dependency on <a href="https://en.wikipedia.org/wiki/Parallel_computing">parallel resources</a>, which might limit their applicability. In this paper, we propose a new method for cross-lingual ED, demonstrating a minimal dependency on <a href="https://en.wikipedia.org/wiki/Parallel_computing">parallel resources</a>. Specifically, to construct a lexical mapping between different languages, we devise a context-dependent translation method ; to treat the word order difference problem, we propose a shared syntactic order event detector for multilingual co-training. The efficiency of our <a href="https://en.wikipedia.org/wiki/Methodology">method</a> is studied through extensive experiments on two standard datasets. Empirical results indicate that our method is effective in 1) performing cross-lingual transfer concerning different directions and 2) tackling the extremely annotation-poor scenario.</abstract>
      <url hash="61d285f8">D19-1068</url>
      <doi>10.18653/v1/D19-1068</doi>
      <bibkey>liu-etal-2019-neural</bibkey>
    </paper>
    <paper id="69">
      <title>KnowledgeNet : A Benchmark Dataset for Knowledge Base Population<fixed-case>K</fixed-case>nowledge<fixed-case>N</fixed-case>et: A Benchmark Dataset for Knowledge Base Population</title>
      <author><first>Filipe</first><last>Mesquita</last></author>
      <author><first>Matteo</first><last>Cannaviccio</last></author>
      <author><first>Jordan</first><last>Schmidek</last></author>
      <author><first>Paramita</first><last>Mirza</last></author>
      <author><first>Denilson</first><last>Barbosa</last></author>
      <pages>749–758</pages>
      <abstract>KnowledgeNet is a benchmark dataset for the task of automatically populating a knowledge base (Wikidata) with facts expressed in natural language text on the web. KnowledgeNet provides text exhaustively annotated with facts, thus enabling the holistic end-to-end evaluation of knowledge base population systems as a whole, unlike previous benchmarks that are more suitable for the evaluation of individual subcomponents (e.g., <a href="https://en.wikipedia.org/wiki/Entity–relationship_model">entity linking</a>, relation extraction). We discuss five baseline approaches, where the best approach achieves an F1 score of 0.50, significantly outperforming a traditional approach by 79 % (0.28). However, our best <a href="https://en.wikipedia.org/wiki/Baseline_(configuration_management)">baseline</a> is far from reaching <a href="https://en.wikipedia.org/wiki/Human_factors_and_ergonomics">human performance</a> (0.82), indicating our <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> is challenging. The KnowledgeNet dataset and baselines are available at https://github.com/diffbot/knowledge-net</abstract>
      <url hash="2c5b4a40">D19-1069</url>
      <doi>10.18653/v1/D19-1069</doi>
      <bibkey>mesquita-etal-2019-knowledgenet</bibkey>
      <pwccode url="https://github.com/diffbot/knowledge-net" additional="false">diffbot/knowledge-net</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/knowledgenet">KnowledgeNet</pwcdataset>
    </paper>
    <paper id="71">
      <title>Explicit Cross-lingual Pre-training for Unsupervised Machine Translation</title>
      <author><first>Shuo</first><last>Ren</last></author>
      <author><first>Yu</first><last>Wu</last></author>
      <author><first>Shujie</first><last>Liu</last></author>
      <author><first>Ming</first><last>Zhou</last></author>
      <author><first>Shuai</first><last>Ma</last></author>
      <pages>770–779</pages>
      <abstract>Pre-training has proven to be effective in unsupervised machine translation due to its ability to model deep context information in cross-lingual scenarios. However, the cross-lingual information obtained from shared BPE spaces is inexplicit and limited. In this paper, we propose a novel cross-lingual pre-training method for unsupervised machine translation by incorporating explicit cross-lingual training signals. Specifically, we first calculate cross-lingual n-gram embeddings and infer an n-gram translation table from them. With those n-gram translation pairs, we propose a new pre-training model called Cross-lingual Masked Language Model (CMLM), which randomly chooses source n-grams in the input text stream and predicts their translation candidates at each time step. Experiments show that our <a href="https://en.wikipedia.org/wiki/Methodology">method</a> can incorporate beneficial cross-lingual information into pre-trained models. Taking pre-trained CMLM models as the encoder and decoder, we significantly improve the performance of unsupervised machine translation.</abstract>
      <url hash="8a180f4a">D19-1071</url>
      <doi>10.18653/v1/D19-1071</doi>
      <bibkey>ren-etal-2019-explicit</bibkey>
    </paper>
    <paper id="72">
      <title>Latent Part-of-Speech Sequences for Neural Machine Translation</title>
      <author><first>Xuewen</first><last>Yang</last></author>
      <author><first>Yingru</first><last>Liu</last></author>
      <author><first>Dongliang</first><last>Xie</last></author>
      <author><first>Xin</first><last>Wang</last></author>
      <author><first>Niranjan</first><last>Balasubramanian</last></author>
      <pages>780–790</pages>
      <abstract>Learning target side syntactic structure has been shown to improve Neural Machine Translation (NMT). However, incorporating <a href="https://en.wikipedia.org/wiki/Syntax">syntax</a> through <a href="https://en.wikipedia.org/wiki/Latent_variable">latent variables</a> introduces additional complexity in <a href="https://en.wikipedia.org/wiki/Statistical_inference">inference</a>, as the models need to marginalize over the latent syntactic structures. To avoid this, <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> often resort to <a href="https://en.wikipedia.org/wiki/Greedy_search">greedy search</a> which only allows them to explore a limited portion of the latent space. In this work, we introduce a new <a href="https://en.wikipedia.org/wiki/Latent_variable_model">latent variable model</a>, LaSyn, that captures the co-dependence between <a href="https://en.wikipedia.org/wiki/Syntax">syntax</a> and <a href="https://en.wikipedia.org/wiki/Semantics">semantics</a>, while allowing for effective and efficient inference over the latent space. LaSyn decouples direct dependence between successive latent variables, which allows its decoder to exhaustively search through the latent syntactic choices, while keeping decoding speed proportional to the size of the latent variable vocabulary. We implement LaSyn by modifying a transformer-based NMT system and design a neural expectation maximization algorithm that we regularize with part-of-speech information as the latent sequences. Evaluations on four different MT tasks show that incorporating target side syntax with LaSyn improves both translation quality, and also provides an opportunity to improve diversity.</abstract>
      <url hash="a41045ed">D19-1072</url>
      <attachment hash="e3b18bc3">D19-1072.Attachment.pdf</attachment>
      <doi>10.18653/v1/D19-1072</doi>
      <bibkey>yang-etal-2019-latent</bibkey>
    </paper>
    <paper id="73">
      <title>Improving <a href="https://en.wikipedia.org/wiki/Back-translation">Back-Translation</a> with Uncertainty-based Confidence Estimation</title>
      <author><first>Shuo</first><last>Wang</last></author>
      <author id="yang-liu"><first>Yang</first><last>Liu</last></author>
      <author><first>Chao</first><last>Wang</last></author>
      <author><first>Huanbo</first><last>Luan</last></author>
      <author><first>Maosong</first><last>Sun</last></author>
      <pages>791–802</pages>
      <abstract>While <a href="https://en.wikipedia.org/wiki/Back-translation">back-translation</a> is simple and effective in exploiting abundant monolingual corpora to improve low-resource neural machine translation (NMT), the synthetic bilingual corpora generated by NMT models trained on limited authentic bilingual data are inevitably noisy. In this work, we propose to quantify the confidence of NMT model predictions based on model uncertainty. With word- and sentence-level confidence measures based on <a href="https://en.wikipedia.org/wiki/Uncertainty">uncertainty</a>, it is possible for <a href="https://en.wikipedia.org/wiki/Back-translation">back-translation</a> to better cope with <a href="https://en.wikipedia.org/wiki/Noise">noise</a> in synthetic bilingual corpora. Experiments on Chinese-English and English-German translation tasks show that uncertainty-based confidence estimation significantly improves the performance of <a href="https://en.wikipedia.org/wiki/Back-translation">back-translation</a>.</abstract>
      <url hash="f9ed95f8">D19-1073</url>
      <doi>10.18653/v1/D19-1073</doi>
      <bibkey>wang-etal-2019-improving-back</bibkey>
      <pwccode url="https://github.com/THUNLP-MT/UCE4BT" additional="false">THUNLP-MT/UCE4BT</pwccode>
    </paper>
    <paper id="76">
      <title>Supervised and Nonlinear Alignment of Two Embedding Spaces for Dictionary Induction in Low Resourced Languages</title>
      <author><first>Masud</first><last>Moshtaghi</last></author>
      <pages>823–832</pages>
      <abstract>Enabling cross-lingual NLP tasks by leveraging multilingual word embedding has recently attracted much attention. An important motivation is to support lower resourced languages, however, most efforts focus on demonstrating the effectiveness of the techniques using embeddings derived from similar languages to English with large parallel content. In this study, we first describe the general requirements for the success of these techniques and then present a noise tolerant piecewise linear technique to learn a non-linear mapping between two monolingual word embedding vector spaces. We evaluate our approach on inferring bilingual dictionaries. We show that our <a href="https://en.wikipedia.org/wiki/Scientific_technique">technique</a> outperforms the <a href="https://en.wikipedia.org/wiki/State_of_the_art">state-of-the-art</a> in lower resourced settings with an average of 3.7 % improvement of precision @10 across 14 mostly low resourced languages.</abstract>
      <url hash="cd945f8d">D19-1076</url>
      <doi>10.18653/v1/D19-1076</doi>
      <bibkey>moshtaghi-2019-supervised</bibkey>
    </paper>
    <paper id="79">
      <title>Multi-agent Learning for Neural Machine Translation</title>
      <author><first>Tianchi</first><last>Bi</last></author>
      <author><first>Hao</first><last>Xiong</last></author>
      <author><first>Zhongjun</first><last>He</last></author>
      <author><first>Hua</first><last>Wu</last></author>
      <author><first>Haifeng</first><last>Wang</last></author>
      <pages>856–865</pages>
      <abstract>Conventional Neural Machine Translation (NMT) models benefit from the training with an additional <a href="https://en.wikipedia.org/wiki/Intelligent_agent">agent</a>, e.g., dual learning, and bidirectional decoding with one agent decod- ing from left to right and the other decoding in the opposite direction. In this paper, we extend the <a href="https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets">training framework</a> to the multi-agent sce- nario by introducing diverse agents in an in- teractive updating process. At training time, each agent learns advanced knowledge from others, and they work together to improve translation quality. Experimental results on NIST Chinese-English, IWSLT 2014 German- English, WMT 2014 English-German and large-scale Chinese-English translation tasks indicate that our approach achieves absolute improvements over the strong baseline sys- tems and shows competitive performance on all tasks.</abstract>
      <url hash="87c351b0">D19-1079</url>
      <doi>10.18653/v1/D19-1079</doi>
      <bibkey>bi-etal-2019-multi</bibkey>
    </paper>
    <paper id="80">
      <title>Pivot-based Transfer Learning for <a href="https://en.wikipedia.org/wiki/Neural_machine_translation">Neural Machine Translation</a> between Non-English Languages<fixed-case>E</fixed-case>nglish Languages</title>
      <author><first>Yunsu</first><last>Kim</last></author>
      <author><first>Petre</first><last>Petrov</last></author>
      <author><first>Pavel</first><last>Petrushkov</last></author>
      <author><first>Shahram</first><last>Khadivi</last></author>
      <author><first>Hermann</first><last>Ney</last></author>
      <pages>866–876</pages>
      <abstract>We present effective pre-training strategies for <a href="https://en.wikipedia.org/wiki/Neural_machine_translation">neural machine translation (NMT)</a> using <a href="https://en.wikipedia.org/wiki/Parallel_text">parallel corpora</a> involving a <a href="https://en.wikipedia.org/wiki/Pivot_language">pivot language</a>, i.e., source-pivot and pivot-target, leading to a significant improvement in source-target translation. We propose three methods to increase the relation among source, pivot, and target languages in the pre-training : 1) step-wise training of a single model for different language pairs, 2) additional adapter component to smoothly connect pre-trained encoder and decoder, and 3) cross-lingual encoder training via autoencoding of the pivot language. Our <a href="https://en.wikipedia.org/wiki/Methodology">methods</a> greatly outperform multilingual models up to +2.6 % BLEU in WMT 2019 French-German and German-Czech tasks. We show that our improvements are valid also in zero-shot / zero-resource scenarios.</abstract>
      <url hash="fa27d683">D19-1080</url>
      <doi>10.18653/v1/D19-1080</doi>
      <bibkey>kim-etal-2019-pivot</bibkey>
    </paper>
    <paper id="84">
      <title>A Discriminative Neural Model for Cross-Lingual Word Alignment</title>
      <author><first>Elias</first><last>Stengel-Eskin</last></author>
      <author><first>Tzu-ray</first><last>Su</last></author>
      <author><first>Matt</first><last>Post</last></author>
      <author><first>Benjamin</first><last>Van Durme</last></author>
      <pages>910–920</pages>
      <abstract>We introduce a novel discriminative word alignment model, which we integrate into a Transformer-based machine translation model. In experiments based on a small number of labeled examples (1.7K5 K sentences) we evaluate its performance intrinsically on both English-Chinese and English-Arabic alignment, where we achieve major improvements over unsupervised baselines (1127 F1). We evaluate the model extrinsically on data projection for <a href="https://en.wikipedia.org/wiki/Chinese_language">Chinese NER</a>, showing that our alignments lead to higher performance when used to project NER tags from <a href="https://en.wikipedia.org/wiki/English_language">English</a> to <a href="https://en.wikipedia.org/wiki/Chinese_language">Chinese</a>. Finally, we perform an ablation analysis and an annotation experiment that jointly support the utility and feasibility of future manual alignment elicitation.</abstract>
      <url hash="3e4884a1">D19-1084</url>
      <doi>10.18653/v1/D19-1084</doi>
      <bibkey>stengel-eskin-etal-2019-discriminative</bibkey>
    </paper>
    <paper id="85">
      <title>One Model to Learn Both : Zero Pronoun Prediction and Translation</title>
      <author><first>Longyue</first><last>Wang</last></author>
      <author><first>Zhaopeng</first><last>Tu</last></author>
      <author><first>Xing</first><last>Wang</last></author>
      <author><first>Shuming</first><last>Shi</last></author>
      <pages>921–930</pages>
      <abstract>Zero pronouns (ZPs) are frequently omitted in <a href="https://en.wikipedia.org/wiki/Pro-drop_language">pro-drop languages</a>, but should be recalled in <a href="https://en.wikipedia.org/wiki/Pro-drop_language">non-pro-drop languages</a>. This discourse phenomenon poses a significant challenge for <a href="https://en.wikipedia.org/wiki/Machine_translation">machine translation (MT)</a> when translating texts from pro-drop to non-pro-drop languages. In this paper, we propose a unified and discourse-aware ZP translation approach for neural MT models. Specifically, we jointly learn to predict and translate ZPs in an end-to-end manner, allowing both components to interact with each other. In addition, we employ hierarchical neural networks to exploit discourse-level context, which is beneficial for ZP prediction and thus <a href="https://en.wikipedia.org/wiki/Translation_(biology)">translation</a>. Experimental results on both Chinese-English and Japanese-English data show that our approach significantly and accumulatively improves both <a href="https://en.wikipedia.org/wiki/Translation">translation</a> performance and ZP prediction accuracy over not only baseline but also previous works using external ZP prediction models. Extensive analyses confirm that the performance improvement comes from the alleviation of different kinds of errors especially caused by subjective ZPs.</abstract>
      <url hash="0ea80c1b">D19-1085</url>
      <doi>10.18653/v1/D19-1085</doi>
      <bibkey>wang-etal-2019-one</bibkey>
    </paper>
    <paper id="86">
      <title>Dynamic Past and Future for <a href="https://en.wikipedia.org/wiki/Neural_machine_translation">Neural Machine Translation</a></title>
      <author><first>Zaixiang</first><last>Zheng</last></author>
      <author><first>Shujian</first><last>Huang</last></author>
      <author><first>Zhaopeng</first><last>Tu</last></author>
      <author><first>Xin-Yu</first><last>Dai</last></author>
      <author><first>Jiajun</first><last>Chen</last></author>
      <pages>931–941</pages>
      <abstract>Previous studies have shown that neural machine translation (NMT) models can benefit from explicitly modeling translated () and untranslated () source contents as recurrent states (CITATION). However, this less interpretable recurrent process hinders its power to model the dynamic updating of and contents during decoding. In this paper, we propose to model the dynamic principles by explicitly separating source words into groups of translated and untranslated contents through parts-to-wholes assignment. The assignment is learned through a novel variant of routing-by-agreement mechanism (CITATION), namely Guided Dynamic Routing, where the translating status at each decoding step guides the routing process to assign each source word to its associated group (i.e., translated or untranslated content) represented by a capsule, enabling translation to be made from holistic context. Experiments show that our approach achieves substantial improvements over both <a href="https://en.wikipedia.org/wiki/Reverse_Polish_notation">Rnmt</a> and <a href="https://en.wikipedia.org/wiki/Reverse_Polish_notation">Transformer</a> by producing more adequate translations. Extensive analysis demonstrates that our method is highly interpretable, which is able to recognize the translated and untranslated contents as expected.<i>dynamic principles</i> by explicitly separating source words into groups of translated and untranslated contents through parts-to-wholes assignment. The assignment is learned through a novel variant of routing-by-agreement mechanism (CITATION), namely <i>Guided Dynamic Routing</i>, where the translating status at each decoding step <i>guides</i> the routing process to assign each source word to its associated group (i.e., translated or untranslated content) represented by a capsule, enabling translation to be made from holistic context. Experiments show that our approach achieves substantial improvements over both Rnmt and Transformer by producing more adequate translations. Extensive analysis demonstrates that our method is highly interpretable, which is able to recognize the translated and untranslated contents as expected.</abstract>
      <url hash="c8fde544">D19-1086</url>
      <attachment hash="045adc73">D19-1086.Attachment.pdf</attachment>
      <doi>10.18653/v1/D19-1086</doi>
      <bibkey>zheng-etal-2019-dynamic</bibkey>
      <pwccode url="https://github.com/zhengzx-nlp/dynamic-nmt" additional="false">zhengzx-nlp/dynamic-nmt</pwccode>
    </paper>
    <paper id="88">
      <title>Towards Understanding <a href="https://en.wikipedia.org/wiki/Neural_machine_translation">Neural Machine Translation</a> with Word Importance</title>
      <author><first>Shilin</first><last>He</last></author>
      <author><first>Zhaopeng</first><last>Tu</last></author>
      <author><first>Xing</first><last>Wang</last></author>
      <author><first>Longyue</first><last>Wang</last></author>
      <author><first>Michael</first><last>Lyu</last></author>
      <author><first>Shuming</first><last>Shi</last></author>
      <pages>953–962</pages>
      <abstract>Although <a href="https://en.wikipedia.org/wiki/Neural_machine_translation">neural machine translation (NMT)</a> has advanced the state-of-the-art on various language pairs, the interpretability of <a href="https://en.wikipedia.org/wiki/Neural_machine_translation">NMT</a> remains unsatisfactory. In this work, we propose to address this gap by focusing on understanding the input-output behavior of NMT models. Specifically, we measure the word importance by attributing the NMT output to every input word through a gradient-based method. We validate the approach on a couple of perturbation operations, language pairs, and model architectures, demonstrating its superiority on identifying input words with higher influence on <a href="https://en.wikipedia.org/wiki/Translation">translation</a> performance. Encouragingly, the calculated importance can serve as indicators of input words that are under-translated by NMT models. Furthermore, our analysis reveals that words of certain syntactic categories have higher importance while the categories vary across language pairs, which can inspire better design principles of NMT architectures for multi-lingual translation.</abstract>
      <url hash="4995cb40">D19-1088</url>
      <attachment hash="3ec16867">D19-1088.Attachment.pdf</attachment>
      <doi>10.18653/v1/D19-1088</doi>
      <bibkey>he-etal-2019-towards</bibkey>
    </paper>
    <paper id="89">
      <title>Multilingual Neural Machine Translation with Language Clustering</title>
      <author><first>Xu</first><last>Tan</last></author>
      <author><first>Jiale</first><last>Chen</last></author>
      <author><first>Di</first><last>He</last></author>
      <author><first>Yingce</first><last>Xia</last></author>
      <author><first>Tao</first><last>Qin</last></author>
      <author><first>Tie-Yan</first><last>Liu</last></author>
      <pages>963–973</pages>
      <abstract>Multilingual neural machine translation (NMT), which translates multiple languages using a single model, is of great practical importance due to its advantages in simplifying the training process, reducing online maintenance costs, and enhancing low-resource and zero-shot translation. Given there are thousands of languages in the world and some of them are very different, it is extremely burdensome to handle them all in a single <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a> or use a separate <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a> for each language pair. Therefore, given a fixed resource budget, e.g., the number of models, how to determine which languages should be supported by one <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a> is critical to multilingual NMT, which, unfortunately, has been ignored by previous work. In this work, we develop a <a href="https://en.wikipedia.org/wiki/Software_framework">framework</a> that clusters languages into different groups and trains one multilingual model for each <a href="https://en.wikipedia.org/wiki/Cluster_analysis">cluster</a>. We study two methods for language clustering : (1) using prior knowledge, where we cluster languages according to language family, and (2) using language embedding, in which we represent each language by an embedding vector and cluster them in the embedding space. In particular, we obtain the <a href="https://en.wikipedia.org/wiki/Embedding">embedding vectors</a> of all the languages by training a universal neural machine translation model. Our experiments on 23 languages show that the first clustering method is simple and easy to understand but leading to suboptimal translation accuracy, while the second method sufficiently captures the relationship among languages well and improves the translation accuracy for almost all the languages over baseline methods.</abstract>
      <url hash="2b56286e">D19-1089</url>
      <doi>10.18653/v1/D19-1089</doi>
      <bibkey>tan-etal-2019-multilingual</bibkey>
    </paper>
    <paper id="96">
      <title>A Lexicon-Based Graph Neural Network for Chinese NER<fixed-case>C</fixed-case>hinese <fixed-case>NER</fixed-case></title>
      <author><first>Tao</first><last>Gui</last></author>
      <author><first>Yicheng</first><last>Zou</last></author>
      <author><first>Qi</first><last>Zhang</last></author>
      <author><first>Minlong</first><last>Peng</last></author>
      <author><first>Jinlan</first><last>Fu</last></author>
      <author><first>Zhongyu</first><last>Wei</last></author>
      <author><first>Xuanjing</first><last>Huang</last></author>
      <pages>1040–1050</pages>
      <abstract>Recurrent neural networks (RNN) used for Chinese named entity recognition (NER) that sequentially track character and word information have achieved great success. However, the characteristic of chain structure and the lack of global semantics determine that RNN-based models are vulnerable to <a href="https://en.wikipedia.org/wiki/Ambiguity">word ambiguities</a>. In this work, we try to alleviate this problem by introducing a lexicon-based graph neural network with global semantics, in which lexicon knowledge is used to connect characters to capture the local composition, while a global relay node can capture global sentence semantics and long-range dependency. Based on the multiple graph-based interactions among characters, potential words, and the whole-sentence semantics, word ambiguities can be effectively tackled. Experiments on four NER datasets show that the proposed <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> achieves significant improvements against other baseline models.</abstract>
      <url hash="103f2ea3">D19-1096</url>
      <doi>10.18653/v1/D19-1096</doi>
      <bibkey>gui-etal-2019-lexicon</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/ontonotes-4-0">OntoNotes 4.0</pwcdataset>
    </paper>
    <paper id="101">
      <title>A Bayesian Approach for Sequence Tagging with Crowds<fixed-case>B</fixed-case>ayesian Approach for Sequence Tagging with Crowds</title>
      <author><first>Edwin</first><last>Simpson</last></author>
      <author><first>Iryna</first><last>Gurevych</last></author>
      <pages>1093–1104</pages>
      <abstract>Current methods for sequence tagging, a core task in <a href="https://en.wikipedia.org/wiki/Natural_language_processing">NLP</a>, are data hungry, which motivates the use of <a href="https://en.wikipedia.org/wiki/Crowdsourcing">crowdsourcing</a> as a cheap way to obtain labelled data. However, annotators are often unreliable and current aggregation methods can not capture common types of span annotation error. To address this, we propose a <a href="https://en.wikipedia.org/wiki/Bayesian_inference">Bayesian method</a> for aggregating sequence tags that reduces errors by modelling sequential dependencies between the annotations as well as the ground-truth labels. By taking a Bayesian approach, we account for uncertainty in the model due to both annotator errors and the lack of data for modelling annotators who complete few tasks. We evaluate our model on crowdsourced data for <a href="https://en.wikipedia.org/wiki/Named-entity_recognition">named entity recognition</a>, <a href="https://en.wikipedia.org/wiki/Information_extraction">information extraction</a> and <a href="https://en.wikipedia.org/wiki/Argument_mining">argument mining</a>, showing that our sequential model outperforms the previous state of the art, and that Bayesian approaches outperform non-Bayesian alternatives. We also find that our approach can reduce crowdsourcing costs through more effective <a href="https://en.wikipedia.org/wiki/Active_learning">active learning</a>, as it better captures uncertainty in the sequence labels when there are few annotations.</abstract>
      <url hash="508fa2c8">D19-1101</url>
      <doi>10.18653/v1/D19-1101</doi>
      <bibkey>simpson-gurevych-2019-bayesian</bibkey>
      <pwccode url="https://github.com/UKPLab/arxiv2018-bayesian-ensembles" additional="false">UKPLab/arxiv2018-bayesian-ensembles</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/conll-2003">CoNLL-2003</pwcdataset>
    </paper>
    <paper id="104">
      <title>Look-up and Adapt : A One-shot Semantic Parser</title>
      <author><first>Zhichu</first><last>Lu</last></author>
      <author><first>Forough</first><last>Arabshahi</last></author>
      <author><first>Igor</first><last>Labutov</last></author>
      <author><first>Tom</first><last>Mitchell</last></author>
      <pages>1129–1139</pages>
      <abstract>Computing devices have recently become capable of interacting with their end users via <a href="https://en.wikipedia.org/wiki/Natural_language_processing">natural language</a>. However, they can only operate within a limited supported domain of discourse and fail drastically when faced with an out-of-domain utterance, mainly due to the limitations of their <a href="https://en.wikipedia.org/wiki/Semantic_parser">semantic parser</a>. In this paper, we propose a <a href="https://en.wikipedia.org/wiki/Semantic_parser">semantic parser</a> that generalizes to out-of-domain examples by learning a general strategy for parsing an unseen utterance through adapting the logical forms of seen utterances, instead of learning to generate a logical form from scratch. Our <a href="https://en.wikipedia.org/wiki/Parsing">parser</a> maintains a <a href="https://en.wikipedia.org/wiki/Memory_(computing)">memory</a> consisting of a representative subset of the seen utterances paired with their logical forms. Given an unseen utterance, our <a href="https://en.wikipedia.org/wiki/Parsing">parser</a> works by looking up a similar utterance from the <a href="https://en.wikipedia.org/wiki/Memory">memory</a> and adapting its <a href="https://en.wikipedia.org/wiki/Logical_form">logical form</a> until it fits the unseen utterance. Moreover, we present a data generation strategy for constructing utterance-logical form pairs from different domains. Our results show an improvement of up to 68.8 % on one-shot parsing under two different evaluation settings compared to the <a href="https://en.wikipedia.org/wiki/Baseline_(configuration_management)">baselines</a>.</abstract>
      <url hash="cf449d43">D19-1104</url>
      <attachment hash="4f9fd566">D19-1104.Attachment.zip</attachment>
      <doi>10.18653/v1/D19-1104</doi>
      <bibkey>lu-etal-2019-look</bibkey>
      <pwccode url="https://github.com/zhichul/lookup-and-adapt-parser-data" additional="false">zhichul/lookup-and-adapt-parser-data</pwccode>
    </paper>
    <paper id="111">
      <title>Analytical Methods for Interpretable Ultradense Word Embeddings</title>
      <author><first>Philipp</first><last>Dufter</last></author>
      <author><first>Hinrich</first><last>Schütze</last></author>
      <pages>1185–1191</pages>
      <abstract>Word embeddings are useful for a wide variety of <a href="https://en.wikipedia.org/wiki/Task_(project_management)">tasks</a>, but they lack <a href="https://en.wikipedia.org/wiki/Interpretability">interpretability</a>. By rotating word spaces, interpretable dimensions can be identified while preserving the information contained in the <a href="https://en.wikipedia.org/wiki/Embedding">embeddings</a> without any loss. In this work, we investigate three methods for making word spaces interpretable by rotation : Densifier (Rothe et al., 2016), linear SVMs and DensRay, a new method we propose. In contrast to Densifier, DensRay can be computed in <a href="https://en.wikipedia.org/wiki/Closed-form_expression">closed form</a>, is hyperparameter-free and thus more robust than Densifier. We evaluate the three <a href="https://en.wikipedia.org/wiki/Methodology">methods</a> on lexicon induction and set-based word analogy. In addition we provide qualitative insights as to how interpretable word spaces can be used for removing <a href="https://en.wikipedia.org/wiki/Gender_bias">gender bias</a> from <a href="https://en.wikipedia.org/wiki/Embedding">embeddings</a>.</abstract>
      <url hash="018bb981">D19-1111</url>
      <attachment hash="dd8e8eeb">D19-1111.Attachment.pdf</attachment>
      <doi>10.18653/v1/D19-1111</doi>
      <bibkey>dufter-schutze-2019-analytical</bibkey>
      <pwccode url="https://github.com/pdufter/densray" additional="false">pdufter/densray</pwccode>
    </paper>
    <paper id="115">
      <title>Neural Linguistic Steganography</title>
      <author><first>Zachary</first><last>Ziegler</last></author>
      <author><first>Yuntian</first><last>Deng</last></author>
      <author><first>Alexander</first><last>Rush</last></author>
      <pages>1210–1215</pages>
      <abstract>Whereas traditional <a href="https://en.wikipedia.org/wiki/Cryptography">cryptography</a> encrypts a secret message into an unintelligible form, <a href="https://en.wikipedia.org/wiki/Steganography">steganography</a> conceals that communication is taking place by encoding a secret message into a cover signal. Language is a particularly pragmatic cover signal due to its benign occurrence and independence from any one medium. Traditionally, linguistic steganography systems encode <a href="https://en.wikipedia.org/wiki/Cipher">secret messages</a> in existing text via synonym substitution or word order rearrangements. Advances in neural language models enable previously impractical generation-based techniques. We propose a <a href="https://en.wikipedia.org/wiki/Steganography">steganography technique</a> based on <a href="https://en.wikipedia.org/wiki/Arithmetic_coding">arithmetic coding</a> with large-scale neural language models. We find that our approach can generate realistic looking cover sentences as evaluated by humans, while at the same time preserving <a href="https://en.wikipedia.org/wiki/Computer_security">security</a> by matching the cover message distribution with the language model distribution.</abstract>
      <url hash="8eca7995">D19-1115</url>
      <attachment hash="2bad6200">D19-1115.Attachment.zip</attachment>
      <doi>10.18653/v1/D19-1115</doi>
      <bibkey>ziegler-etal-2019-neural</bibkey>
      <pwccode url="https://github.com/harvardnlp/NeuralSteganography" additional="false">harvardnlp/NeuralSteganography</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/cnn-daily-mail-1">CNN/Daily Mail</pwcdataset>
    </paper>
    <paper id="118">
      <title>Rewarding Coreference Resolvers for Being Consistent with World Knowledge</title>
      <author><first>Rahul</first><last>Aralikatte</last></author>
      <author><first>Heather</first><last>Lent</last></author>
      <author><first>Ana Valeria</first><last>Gonzalez</last></author>
      <author><first>Daniel</first><last>Herschcovich</last></author>
      <author><first>Chen</first><last>Qiu</last></author>
      <author><first>Anders</first><last>Sandholm</last></author>
      <author><first>Michael</first><last>Ringaard</last></author>
      <author><first>Anders</first><last>Søgaard</last></author>
      <pages>1229–1235</pages>
      <abstract>Unresolved coreference is a bottleneck for <a href="https://en.wikipedia.org/wiki/Relation_extraction">relation extraction</a>, and high-quality coreference resolvers may produce an output that makes it a lot easier to extract knowledge triples. We show how to improve coreference resolvers by forwarding their input to a relation extraction system and reward the resolvers for producing triples that are found in <a href="https://en.wikipedia.org/wiki/Knowledge_base">knowledge bases</a>. Since relation extraction systems can rely on different forms of supervision and be biased in different ways, we obtain the best performance, improving over the state of the art, using multi-task reinforcement learning.</abstract>
      <url hash="698872a0">D19-1118</url>
      <doi>10.18653/v1/D19-1118</doi>
      <revision id="1" href="D19-1118v1" hash="a64cdba8" />
      <revision id="2" href="D19-1118v2" hash="698872a0">Corrected an error in Figure 2 that incorrectly portrayed the data collection process for training the reward models.</revision>
      <bibkey>aralikatte-etal-2019-rewarding</bibkey>
      <pwccode url="https://github.com/rahular/coref-rl" additional="false">rahular/coref-rl</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/dbpedia">DBpedia</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikicoref">WikiCoref</pwcdataset>
    </paper>
    <paper id="121">
      <title>Measure Country-Level Socio-Economic Indicators with Streaming News : An Empirical Study</title>
      <author><first>Bonan</first><last>Min</last></author>
      <author><first>Xiaoxi</first><last>Zhao</last></author>
      <pages>1249–1254</pages>
      <abstract>Socio-economic conditions are difficult to measure. For example, the U.S. Bureau of Labor Statistics needs to conduct large-scale household surveys regularly to track the <a href="https://en.wikipedia.org/wiki/Unemployment">unemployment rate</a>, an indicator widely used by economists and policymakers. We argue that events reported in <a href="https://en.wikipedia.org/wiki/Streaming_media">streaming news</a> can be used as <a href="https://en.wikipedia.org/wiki/Microscope">micro-sensors</a> for measuring <a href="https://en.wikipedia.org/wiki/Socioeconomics">socio-economic conditions</a>. Similar to collecting surveys and then counting answers, it is possible to measure a socio-economic indicator by counting related events. In this paper, we propose Event-Centric Indicator Measure (ECIM), a novel approach to measure socio-economic indicators with <a href="https://en.wikipedia.org/wiki/Event_(probability_theory)">events</a>. We empirically demonstrate strong correlation between ECIM values to several representative indicators in socio-economic research.</abstract>
      <url hash="41f6ee42">D19-1121</url>
      <doi>10.18653/v1/D19-1121</doi>
      <bibkey>min-zhao-2019-measure</bibkey>
    </paper>
    <paper id="122">
      <title>Towards Extracting <a href="https://en.wikipedia.org/wiki/Medical_family_history">Medical Family History</a> from <a href="https://en.wikipedia.org/wiki/Natural_language_processing">Natural Language Interactions</a> : A New Dataset and Baselines</title>
      <author><first>Mahmoud</first><last>Azab</last></author>
      <author><first>Stephane</first><last>Dadian</last></author>
      <author><first>Vivi</first><last>Nastase</last></author>
      <author><first>Larry</first><last>An</last></author>
      <author><first>Rada</first><last>Mihalcea</last></author>
      <pages>1255–1260</pages>
      <abstract>We introduce a new dataset consisting of <a href="https://en.wikipedia.org/wiki/Natural-language_understanding">natural language interactions</a> annotated with <a href="https://en.wikipedia.org/wiki/Family_history_(medicine)">medical family histories</a>, obtained during interactions with a <a href="https://en.wikipedia.org/wiki/Genetic_counseling">genetic counselor</a> and through <a href="https://en.wikipedia.org/wiki/Crowdsourcing">crowdsourcing</a>, following a questionnaire created by experts in the domain. We describe the data collection process and the annotations performed by medical professionals, including illness and personal attributes (name, age, gender, family relationships) for the patient and their family members. An initial system that performs argument identification and <a href="https://en.wikipedia.org/wiki/Relation_extraction">relation extraction</a> shows promising results   average <a href="https://en.wikipedia.org/wiki/F-score">F-score</a> of 0.87 on complex sentences on the targeted relations.</abstract>
      <url hash="1aae95b4">D19-1122</url>
      <doi>10.18653/v1/D19-1122</doi>
      <bibkey>azab-etal-2019-towards</bibkey>
    </paper>
    <paper id="124">
      <title>Dirichlet Latent Variable Hierarchical Recurrent Encoder-Decoder in Dialogue Generation<fixed-case>D</fixed-case>irichlet Latent Variable Hierarchical Recurrent Encoder-Decoder in Dialogue Generation</title>
      <author><first>Min</first><last>Zeng</last></author>
      <author><first>Yisen</first><last>Wang</last></author>
      <author><first>Yuan</first><last>Luo</last></author>
      <pages>1267–1272</pages>
      <abstract>Variational encoder-decoders have achieved well-recognized performance in the dialogue generation task. Existing works simply assume the Gaussian priors of the latent variable, which are incapable of representing complex latent variables effectively. To address the issues, we propose to use the <a href="https://en.wikipedia.org/wiki/Dirichlet_distribution">Dirichlet distribution</a> with flexible structures to characterize the <a href="https://en.wikipedia.org/wiki/Latent_variable">latent variables</a> in place of the traditional <a href="https://en.wikipedia.org/wiki/Normal_distribution">Gaussian distribution</a>, called Dirichlet Latent Variable Hierarchical Recurrent Encoder-Decoder model (Dir-VHRED). Based on which, we further find that there is redundancy among the dimensions of <a href="https://en.wikipedia.org/wiki/Latent_variable">latent variable</a>, and the lengths and sentence patterns of the responses can be strongly correlated to each dimension of the <a href="https://en.wikipedia.org/wiki/Latent_variable">latent variable</a>. Therefore, controllable responses can be generated through specifying the value of each dimension of the <a href="https://en.wikipedia.org/wiki/Latent_variable">latent variable</a>. Experimental results on benchmarks show that our proposed Dir-VHRED yields substantial improvements on negative log-likelihood, word-embedding-based and human evaluations.</abstract>
      <url hash="dcb5e308">D19-1124</url>
      <attachment hash="2b7d7353">D19-1124.Attachment.zip</attachment>
      <doi>10.18653/v1/D19-1124</doi>
      <bibkey>zeng-etal-2019-dirichlet</bibkey>
    </paper>
    <paper id="125">
      <title>Semi-Supervised Bootstrapping of Dialogue State Trackers for Task-Oriented Modelling</title>
      <author><first>Bo-Hsiang</first><last>Tseng</last></author>
      <author><first>Marek</first><last>Rei</last></author>
      <author><first>Paweł</first><last>Budzianowski</last></author>
      <author><first>Richard</first><last>Turner</last></author>
      <author><first>Bill</first><last>Byrne</last></author>
      <author><first>Anna</first><last>Korhonen</last></author>
      <pages>1273–1278</pages>
      <abstract>Dialogue systems benefit greatly from optimizing on detailed annotations, such as <a href="https://en.wikipedia.org/wiki/Transcription_(linguistics)">transcribed utterances</a>, internal dialogue state representations and dialogue act labels. However, collecting these <a href="https://en.wikipedia.org/wiki/Annotation">annotations</a> is expensive and time-consuming, holding back development in the area of dialogue modelling. In this paper, we investigate semi-supervised learning methods that are able to reduce the amount of required intermediate labelling. We find that by leveraging un-annotated data instead, the amount of turn-level annotations of dialogue state can be significantly reduced when building a neural dialogue system. Our analysis on the MultiWOZ corpus, covering a range of domains and topics, finds that annotations can be reduced by up to 30 % while maintaining equivalent system performance. We also describe and evaluate the first end-to-end dialogue model created for the MultiWOZ corpus.</abstract>
      <url hash="ca824247">D19-1125</url>
      <doi>10.18653/v1/D19-1125</doi>
      <bibkey>tseng-etal-2019-semi</bibkey>
    </paper>
    <paper id="126">
      <title>A Progressive Model to Enable Continual Learning for Semantic Slot Filling</title>
      <author><first>Yilin</first><last>Shen</last></author>
      <author><first>Xiangyu</first><last>Zeng</last></author>
      <author><first>Hongxia</first><last>Jin</last></author>
      <pages>1279–1284</pages>
      <abstract>Semantic slot filling is one of the major tasks in spoken language understanding (SLU). After a slot filling model is trained on precollected data, it is crucial to continually improve the <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> after deployment to learn users’ new expressions. As the data amount grows, it becomes infeasible to either store such huge data and repeatedly retrain the <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> on all data or fine tune the <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> only on new data without forgetting old expressions. In this paper, we introduce a novel progressive slot filling model, ProgModel. ProgModel consists of a novel context gate that transfers previously learned knowledge to a small size expanded component ; and meanwhile enables this new <a href="https://en.wikipedia.org/wiki/Component-based_software_engineering">component</a> to be fast trained to learn from new data. As such, ProgModel learns the new knowledge by only using new data at each time and meanwhile preserves the previously learned expressions. Our experiments show that ProgModel needs much less training time and smaller model size to outperform various model fine tuning competitors by up to 4.24 % and 3.03 % on two benchmark datasets.</abstract>
      <url hash="73c2a551">D19-1126</url>
      <doi>10.18653/v1/D19-1126</doi>
      <bibkey>shen-etal-2019-progressive</bibkey>
    </paper>
    <paper id="127">
      <title>CASA-NLU : Context-Aware Self-Attentive Natural Language Understanding for Task-Oriented Chatbots<fixed-case>CASA</fixed-case>-<fixed-case>NLU</fixed-case>: Context-Aware Self-Attentive Natural Language Understanding for Task-Oriented Chatbots</title>
      <author><first>Arshit</first><last>Gupta</last></author>
      <author><first>Peng</first><last>Zhang</last></author>
      <author><first>Garima</first><last>Lalwani</last></author>
      <author><first>Mona</first><last>Diab</last></author>
      <pages>1285–1290</pages>
      <abstract>Natural Language Understanding (NLU) is a core component of <a href="https://en.wikipedia.org/wiki/Dialog_(software)">dialog systems</a>. It typically involves two tasks-Intent Classification (IC) and Slot Labeling (SL), which are then followed by a dialogue management (DM) component. Such NLU systems cater to utterances in isolation, thus pushing the problem of <a href="https://en.wikipedia.org/wiki/Context_management">context management</a> to DM. However, <a href="https://en.wikipedia.org/wiki/Context_(language_use)">contextual information</a> is critical to the correct prediction of intents in a conversation. Prior work on contextual NLU has been limited in terms of the types of contextual signals used and the understanding of their impact on the <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a>. In this work, we propose a context-aware self-attentive NLU (CASA-NLU) model that uses multiple signals over a variable context window, such as previous intents, slots, dialog acts and utterances, in addition to the current user utterance. CASA-NLU outperforms a recurrent contextual NLU baseline on two conversational datasets, yielding a gain of up to 7 % on the IC task. Moreover, a non-contextual variant of CASA-NLU achieves state-of-the-art performance on standard public datasets-SNIPS and ATIS.</abstract>
      <url hash="97e14081">D19-1127</url>
      <attachment hash="bf1c27a8">D19-1127.Attachment.pdf</attachment>
      <doi>10.18653/v1/D19-1127</doi>
      <bibkey>gupta-etal-2019-casa</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/atis">ATIS</pwcdataset>
    </paper>
    <paper id="130">
      <title>Modeling Multi-Action Policy for Task-Oriented Dialogues</title>
      <author><first>Lei</first><last>Shu</last></author>
      <author><first>Hu</first><last>Xu</last></author>
      <author><first>Bing</first><last>Liu</last></author>
      <author><first>Piero</first><last>Molino</last></author>
      <pages>1304–1310</pages>
      <abstract>Dialogue management (DM) plays a key role in the quality of the interaction with the user in a task-oriented dialogue system. In most existing approaches, the agent predicts only one DM policy action per turn. This significantly limits the expressive power of the <a href="https://en.wikipedia.org/wiki/Intelligent_agent">conversational agent</a> and introduces unwanted turns of interactions that may challenge users’ patience. Longer conversations also lead to more errors and the <a href="https://en.wikipedia.org/wiki/System">system</a> needs to be more robust to handle them. In this paper, we compare the performance of several <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> on the task of predicting multiple acts for each turn. A novel policy model is proposed based on a recurrent cell called gated Continue-Act-Slots (gCAS) that overcomes the limitations of the existing models. Experimental results show that <a href="https://en.wikipedia.org/wiki/GCAS">gCAS</a> outperforms other approaches. The datasets and code are available at https://leishu02.github.io/.</abstract>
      <url hash="8901a528">D19-1130</url>
      <doi>10.18653/v1/D19-1130</doi>
      <bibkey>shu-etal-2019-modeling</bibkey>
    </paper>
    <paper id="131">
      <title>An Evaluation Dataset for Intent Classification and Out-of-Scope Prediction</title>
      <author><first>Stefan</first><last>Larson</last></author>
      <author><first>Anish</first><last>Mahendran</last></author>
      <author><first>Joseph J.</first><last>Peper</last></author>
      <author><first>Christopher</first><last>Clarke</last></author>
      <author><first>Andrew</first><last>Lee</last></author>
      <author><first>Parker</first><last>Hill</last></author>
      <author><first>Jonathan K.</first><last>Kummerfeld</last></author>
      <author><first>Kevin</first><last>Leach</last></author>
      <author><first>Michael A.</first><last>Laurenzano</last></author>
      <author><first>Lingjia</first><last>Tang</last></author>
      <author><first>Jason</first><last>Mars</last></author>
      <pages>1311–1316</pages>
      <abstract>Task-oriented dialog systems need to know when a query falls outside their range of supported intents, but current text classification corpora only define label sets that cover every example. We introduce a new <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> that includes queries that are out-of-scopei.e., queries that do not fall into any of the system’s supported intents. This poses a new challenge because models can not assume that every query at inference time belongs to a system-supported intent class. Our dataset also covers 150 intent classes over 10 domains, capturing the breadth that a production task-oriented agent must handle. We evaluate a range of benchmark classifiers on our <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> along with several different out-of-scope identification schemes. We find that while the <a href="https://en.wikipedia.org/wiki/Statistical_classification">classifiers</a> perform well on in-scope intent classification, they struggle to identify out-of-scope queries. Our dataset and evaluation fill an important gap in the field, offering a way of more rigorously and realistically benchmarking text classification in task-driven dialog systems.</abstract>
      <url hash="9e3b6fe4">D19-1131</url>
      <attachment hash="efd6e797">D19-1131.Attachment.zip</attachment>
      <doi>10.18653/v1/D19-1131</doi>
      <bibkey>larson-etal-2019-evaluation</bibkey>
      <pwccode url="https://github.com/clinc/oos-eval" additional="true">clinc/oos-eval</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/clinc150">CLINC150</pwcdataset>
    </paper>
    <paper id="132">
      <title>Automatically Learning Data Augmentation Policies for Dialogue Tasks</title>
      <author><first>Tong</first><last>Niu</last></author>
      <author><first>Mohit</first><last>Bansal</last></author>
      <pages>1317–1323</pages>
      <abstract>Automatic data augmentation (AutoAugment) (Cubuk et al., 2019) searches for optimal perturbation policies via a controller trained using performance rewards of a sampled policy on the target task, hence reducing data-level model bias. While being a powerful algorithm, their work has focused on computer vision tasks, where it is comparatively easy to apply imperceptible perturbations without changing an image’s semantic meaning. In our work, we adapt AutoAugment to automatically discover effective perturbation policies for natural language processing (NLP) tasks such as dialogue generation. We start with a pool of atomic operations that apply subtle semantic-preserving perturbations to the source inputs of a dialogue task (e.g., different POS-tag types of stopword dropout, grammatical errors, and paraphrasing). Next, we allow the <a href="https://en.wikipedia.org/wiki/Controller_(computing)">controller</a> to learn more complex augmentation policies by searching over the space of the various combinations of these atomic operations. Moreover, we also explore conditioning the controller on the source inputs of the target task, since certain strategies may not apply to inputs that do not contain that strategy’s required linguistic features. Empirically, we demonstrate that both our input-agnostic and input-aware controllers discover useful data augmentation policies, and achieve significant improvements over the previous state-of-the-art, including trained on manually-designed policies.</abstract>
      <url hash="ab9f0f56">D19-1132</url>
      <doi>10.18653/v1/D19-1132</doi>
      <bibkey>niu-bansal-2019-automatically</bibkey>
      <pwccode url="https://github.com/WolfNiu/AutoAugDialogue" additional="false">WolfNiu/AutoAugDialogue</pwccode>
    </paper>
    <paper id="134">
      <title>Multilingual word translation using auxiliary languages</title>
      <author><first>Hagai</first><last>Taitelbaum</last></author>
      <author><first>Gal</first><last>Chechik</last></author>
      <author><first>Jacob</first><last>Goldberger</last></author>
      <pages>1330–1335</pages>
      <abstract>Current multilingual word translation methods are focused on jointly learning <a href="https://en.wikipedia.org/wiki/Map_(mathematics)">mappings</a> from each language to a shared space. The actual <a href="https://en.wikipedia.org/wiki/Translation">translation</a>, however, is still performed as an isolated bilingual task. In this study we propose a multilingual translation procedure that uses all the learned <a href="https://en.wikipedia.org/wiki/Map_(mathematics)">mappings</a> to translate a word from one language to another. For each source word, we first search for the most relevant auxiliary languages. We then use the <a href="https://en.wikipedia.org/wiki/Translation">translations</a> to these languages to form an improved representation of the source word. Finally, this <a href="https://en.wikipedia.org/wiki/Representation_(systemics)">representation</a> is used for the actual <a href="https://en.wikipedia.org/wiki/Translation">translation</a> to the target language. Experiments on a standard multilingual word translation benchmark demonstrate that our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> outperforms state of the art results.</abstract>
      <url hash="4ca6a053">D19-1134</url>
      <doi>10.18653/v1/D19-1134</doi>
      <bibkey>taitelbaum-etal-2019-multilingual</bibkey>
    </paper>
    <paper id="135">
      <title>Towards Better Modeling <a href="https://en.wikipedia.org/wiki/Hierarchical_structure">Hierarchical Structure</a> for Self-Attention with Ordered Neurons</title>
      <author><first>Jie</first><last>Hao</last></author>
      <author><first>Xing</first><last>Wang</last></author>
      <author><first>Shuming</first><last>Shi</last></author>
      <author><first>Jinfeng</first><last>Zhang</last></author>
      <author><first>Zhaopeng</first><last>Tu</last></author>
      <pages>1336–1341</pages>
      <abstract>Recent studies have shown that a hybrid of self-attention networks (SANs) and recurrent neural networks RNNs outperforms both individual architectures, while not much is known about why the hybrid models work. With the belief that modeling hierarchical structure is an essential complementary between SANs and RNNs, we propose to further enhance the strength of hybrid models with an advanced variant of RNNs   Ordered Neurons LSTM (ON-LSTM), which introduces a syntax-oriented inductive bias to perform tree-like composition. Experimental results on the benchmark machine translation task show that the proposed approach outperforms both individual <a href="https://en.wikipedia.org/wiki/Computer_architecture">architectures</a> and a standard hybrid model. Further analyses on targeted linguistic evaluation and logical inference tasks demonstrate that the proposed approach indeed benefits from a better modeling of hierarchical structure.</abstract>
      <url hash="300d5f5c">D19-1135</url>
      <attachment hash="7802304f">D19-1135.Attachment.pdf</attachment>
      <doi>10.18653/v1/D19-1135</doi>
      <bibkey>hao-etal-2019-towards</bibkey>
    </paper>
    <paper id="137">
      <title>Simpler and Faster Learning of Adaptive Policies for <a href="https://en.wikipedia.org/wiki/Simultaneous_translation">Simultaneous Translation</a></title>
      <author><first>Baigong</first><last>Zheng</last></author>
      <author><first>Renjie</first><last>Zheng</last></author>
      <author><first>Mingbo</first><last>Ma</last></author>
      <author><first>Liang</first><last>Huang</last></author>
      <pages>1349–1354</pages>
      <abstract>Simultaneous translation is widely useful but remains challenging. Previous work falls into two main categories : (a) fixed-latency policies such as Ma et al. (2019) and (b) adaptive policies such as Gu et al. The former are simple and effective, but have to aggressively predict future content due to diverging source-target word order ; the latter do not anticipate, but suffer from unstable and inefficient training. To combine the merits of both approaches, we propose a simple supervised-learning framework to learn an adaptive policy from oracle READ / WRITE sequences generated from parallel text. At each step, such an oracle sequence chooses to WRITE the next target word if the available source sentence context provides enough information to do so, otherwise READ the next source word. Experiments on German = English show that our method, without retraining the underlying NMT model, can learn flexible policies with better BLEU scores and similar latencies compared to previous work.</abstract>
      <url hash="24ac8f7f">D19-1137</url>
      <doi>10.18653/v1/D19-1137</doi>
      <bibkey>zheng-etal-2019-simpler</bibkey>
    </paper>
    <paper id="138">
      <title>Adversarial Learning with Contextual Embeddings for Zero-resource Cross-lingual Classification and NER<fixed-case>NER</fixed-case></title>
      <author><first>Phillip</first><last>Keung</last></author>
      <author><first>Yichao</first><last>Lu</last></author>
      <author><first>Vikas</first><last>Bhardwaj</last></author>
      <pages>1355–1360</pages>
      <abstract>Contextual word embeddings (e.g. GPT, BERT, ELMo, etc.) have demonstrated state-of-the-art performance on various NLP tasks. Recent work with the multilingual version of BERT has shown that the <a href="https://en.wikipedia.org/wiki/Statistical_model">model</a> performs surprisingly well in cross-lingual settings, even when only labeled English data is used to finetune the <a href="https://en.wikipedia.org/wiki/Statistical_model">model</a>. We improve upon multilingual BERT’s zero-resource cross-lingual performance via <a href="https://en.wikipedia.org/wiki/Adversarial_learning">adversarial learning</a>. We report the magnitude of the improvement on the multilingual MLDoc text classification and CoNLL 2002/2003 named entity recognition tasks. Furthermore, we show that language-adversarial training encourages BERT to align the embeddings of English documents and their translations, which may be the cause of the observed performance gains.</abstract>
      <url hash="9a7eaaf8">D19-1138</url>
      <doi>10.18653/v1/D19-1138</doi>
      <bibkey>keung-etal-2019-adversarial</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/mldoc">MLDoc</pwcdataset>
    </paper>
    <paper id="143">
      <title>Handling Syntactic Divergence in Low-resource Machine Translation</title>
      <author><first>Chunting</first><last>Zhou</last></author>
      <author><first>Xuezhe</first><last>Ma</last></author>
      <author><first>Junjie</first><last>Hu</last></author>
      <author><first>Graham</first><last>Neubig</last></author>
      <pages>1388–1394</pages>
      <abstract>Despite impressive empirical successes of <a href="https://en.wikipedia.org/wiki/Neural_machine_translation">neural machine translation (NMT)</a> on standard <a href="https://en.wikipedia.org/wiki/Benchmark_(computing)">benchmarks</a>, limited parallel data impedes the application of <a href="https://en.wikipedia.org/wiki/Neural_machine_translation">NMT models</a> to many language pairs. Data augmentation methods such as <a href="https://en.wikipedia.org/wiki/Back-translation">back-translation</a> make it possible to use monolingual data to help alleviate these issues, but <a href="https://en.wikipedia.org/wiki/Back-translation">back-translation</a> itself fails in extreme low-resource scenarios, especially for syntactically divergent languages. In this paper, we propose a simple yet effective solution, whereby target-language sentences are re-ordered to match the order of the source and used as an additional source of training-time supervision. Experiments with simulated low-resource Japanese-to-English, and real low-resource Uyghur-to-English scenarios find significant improvements over other semi-supervised alternatives.</abstract>
      <url hash="8e529db6">D19-1143</url>
      <attachment hash="b8a58c48">D19-1143.Attachment.zip</attachment>
      <doi>10.18653/v1/D19-1143</doi>
      <bibkey>zhou-etal-2019-handling</bibkey>
      <pwccode url="https://github.com/violet-zct/pytorch-reorder-nmt" additional="false">violet-zct/pytorch-reorder-nmt</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/aspec">ASPEC</pwcdataset>
    </paper>
    <paper id="145">
      <title>Self-Attention with Structural Position Representations</title>
      <author><first>Xing</first><last>Wang</last></author>
      <author><first>Zhaopeng</first><last>Tu</last></author>
      <author><first>Longyue</first><last>Wang</last></author>
      <author><first>Shuming</first><last>Shi</last></author>
      <pages>1403–1409</pages>
      <abstract>Although self-attention networks (SANs) have advanced the state-of-the-art on various NLP tasks, one criticism of SANs is their ability of encoding positions of input words (Shaw et al., 2018). In this work, we propose to augment SANs with structural position representations to model the latent structure of the input sentence, which is complementary to the standard sequential positional representations. Specifically, we use dependency tree to represent the grammatical structure of a sentence, and propose two strategies to encode the positional relationships among words in the dependency tree. Experimental results on NIST Chinese-to-English and WMT14 English-to-German translation tasks show that the proposed approach consistently boosts performance over both the absolute and relative sequential position representations.</abstract>
      <url hash="0ead0d23">D19-1145</url>
      <doi>10.18653/v1/D19-1145</doi>
      <bibkey>wang-etal-2019-self</bibkey>
    </paper>
    <paper id="146">
      <title>Exploiting <a href="https://en.wikipedia.org/wiki/Multilingualism">Multilingualism</a> through Multistage Fine-Tuning for Low-Resource Neural Machine Translation</title>
      <author><first>Raj</first><last>Dabre</last></author>
      <author><first>Atsushi</first><last>Fujita</last></author>
      <author><first>Chenhui</first><last>Chu</last></author>
      <pages>1410–1416</pages>
      <abstract>This paper highlights the impressive utility of multi-parallel corpora for <a href="https://en.wikipedia.org/wiki/Transfer_learning">transfer learning</a> in a one-to-many low-resource neural machine translation (NMT) setting. We report on a systematic comparison of multistage fine-tuning configurations, consisting of (1) pre-training on an external large (209k440k) parallel corpus for <a href="https://en.wikipedia.org/wiki/English_language">English</a> and a helping target language, (2) mixed pre-training or fine-tuning on a mixture of the external and low-resource (18k) target parallel corpora, and (3) pure fine-tuning on the target parallel corpora. Our experiments confirm that multi-parallel corpora are extremely useful despite their scarcity and content-wise redundancy thus exhibiting the true power of <a href="https://en.wikipedia.org/wiki/Multilingualism">multilingualism</a>. Even when the helping target language is not one of the target languages of our concern, our multistage fine-tuning can give 39 BLEU score gains over a simple one-to-one model.</abstract>
      <url hash="5593c131">D19-1146</url>
      <doi>10.18653/v1/D19-1146</doi>
      <bibkey>dabre-etal-2019-exploiting</bibkey>
    </paper>
    <paper id="147">
      <title>Unsupervised Domain Adaptation for <a href="https://en.wikipedia.org/wiki/Neural_machine_translation">Neural Machine Translation</a> with Domain-Aware Feature Embeddings</title>
      <author><first>Zi-Yi</first><last>Dou</last></author>
      <author><first>Junjie</first><last>Hu</last></author>
      <author><first>Antonios</first><last>Anastasopoulos</last></author>
      <author><first>Graham</first><last>Neubig</last></author>
      <pages>1417–1422</pages>
      <abstract>The recent success of neural machine translation models relies on the availability of high quality, in-domain data. Domain adaptation is required when domain-specific data is scarce or nonexistent. Previous unsupervised domain adaptation strategies include training the <a href="https://en.wikipedia.org/wiki/Statistical_model">model</a> with in-domain copied monolingual or back-translated data. However, these methods use generic representations for text regardless of domain shift, which makes it infeasible for translation models to control outputs conditional on a specific domain. In this work, we propose an approach that adapts <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> with domain-aware feature embeddings, which are learned via an auxiliary language modeling task. Our approach allows the <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a> to assign domain-specific representations to words and output sentences in the desired domain. Our empirical results demonstrate the effectiveness of the proposed <a href="https://en.wikipedia.org/wiki/Strategy">strategy</a>, achieving consistent improvements in multiple experimental settings. In addition, we show that combining our method with <a href="https://en.wikipedia.org/wiki/Back_translation">back translation</a> can further improve the performance of the <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a>.</abstract>
      <url hash="f6b80475">D19-1147</url>
      <doi>10.18653/v1/D19-1147</doi>
      <bibkey>dou-etal-2019-unsupervised</bibkey>
      <pwccode url="https://github.com/zdou0830/DAFE" additional="false">zdou0830/DAFE</pwccode>
    </paper>
    <paper id="148">
      <title>A Regularization-based Framework for Bilingual Grammar Induction</title>
      <author><first>Yong</first><last>Jiang</last></author>
      <author><first>Wenjuan</first><last>Han</last></author>
      <author><first>Kewei</first><last>Tu</last></author>
      <pages>1423–1428</pages>
      <abstract>Grammar induction aims to discover <a href="https://en.wikipedia.org/wiki/Syntax">syntactic structures</a> from <a href="https://en.wikipedia.org/wiki/Sentence_(linguistics)">unannotated sentences</a>. In this paper, we propose a framework in which the learning process of the grammar model of one language is influenced by knowledge from the <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a> of another language. Unlike previous work on multilingual grammar induction, our approach does not rely on any external resource, such as <a href="https://en.wikipedia.org/wiki/Parallel_text">parallel corpora</a>, <a href="https://en.wikipedia.org/wiki/Word_alignment">word alignments</a> or <a href="https://en.wikipedia.org/wiki/Phylogenetic_tree">linguistic phylogenetic trees</a>. We propose three <a href="https://en.wikipedia.org/wiki/Regularization_(mathematics)">regularization methods</a> that encourage similarity between model parameters, dependency edge scores, and parse trees respectively. We deploy our methods on a state-of-the-art unsupervised discriminative parser and evaluate it on both transfer grammar induction and bilingual grammar induction. Empirical results on multiple languages show that our <a href="https://en.wikipedia.org/wiki/Methodology">methods</a> outperform strong baselines.</abstract>
      <url hash="274ca8b6">D19-1148</url>
      <doi>10.18653/v1/D19-1148</doi>
      <bibkey>jiang-etal-2019-regularization</bibkey>
    </paper>
    <paper id="150">
      <title>Korean Morphological Analysis with Tied Sequence-to-Sequence Multi-Task Model<fixed-case>K</fixed-case>orean Morphological Analysis with Tied Sequence-to-Sequence Multi-Task Model</title>
      <author><first>Hyun-Je</first><last>Song</last></author>
      <author><first>Seong-Bae</first><last>Park</last></author>
      <pages>1436–1441</pages>
      <abstract>Korean morphological analysis has been considered as a sequence of morpheme processing and POS tagging. Thus, a <a href="https://en.wikipedia.org/wiki/Pipeline_(software)">pipeline model</a> of the <a href="https://en.wikipedia.org/wiki/Task_(computing)">tasks</a> has been adopted widely by previous studies. However, the <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a> has a problem that it can not utilize interactions among the tasks. This paper formulates Korean morphological analysis as a combination of the tasks and presents a tied sequence-to-sequence multi-task model for training the two tasks simultaneously without any explicit <a href="https://en.wikipedia.org/wiki/Regularization_(linguistics)">regularization</a>. The experiments prove the proposed <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> achieves the state-of-the-art performance.</abstract>
      <url hash="5ffca15a">D19-1150</url>
      <doi>10.18653/v1/D19-1150</doi>
      <bibkey>song-park-2019-korean</bibkey>
    </paper>
    <paper id="152">
      <title>Improving Generative Visual Dialog by Answering Diverse Questions</title>
      <author><first>Vishvak</first><last>Murahari</last></author>
      <author><first>Prithvijit</first><last>Chattopadhyay</last></author>
      <author><first>Dhruv</first><last>Batra</last></author>
      <author><first>Devi</first><last>Parikh</last></author>
      <author><first>Abhishek</first><last>Das</last></author>
      <pages>1449–1454</pages>
      <abstract>Prior work on training generative Visual Dialog models with <a href="https://en.wikipedia.org/wiki/Reinforcement_learning">reinforcement learning</a> ((Das et al., ICCV 2017) has explored a Q-Bot-A-Bot image-guessing game and shown that this ‘self-talk’ approach can lead to improved performance at the downstream dialog-conditioned image-guessing task. However, this improvement saturates and starts degrading after a few rounds of <a href="https://en.wikipedia.org/wiki/Interaction">interaction</a>, and does not lead to a better Visual Dialog model. We find that this is due in part to repeated interactions between <a href="https://en.wikipedia.org/wiki/Q-Bot">Q-Bot</a> and A-BOT during <a href="https://en.wikipedia.org/wiki/Self-talk">self-talk</a>, which are not informative with respect to the <a href="https://en.wikipedia.org/wiki/Image">image</a>. To improve this, we devise a simple auxiliary objective that incentivizes <a href="https://en.wikipedia.org/wiki/Q-Bot">Q-Bot</a> to ask diverse questions, thus reducing repetitions and in turn enabling A-Bot to explore a larger <a href="https://en.wikipedia.org/wiki/State_space">state space</a> during <a href="https://en.wikipedia.org/wiki/Real-time_computing">RL</a> i.e. be exposed to more visual concepts to talk about, and varied questions to answer. We evaluate our approach via a host of <a href="https://en.wikipedia.org/wiki/Metric_(mathematics)">automatic metrics</a> and <a href="https://en.wikipedia.org/wiki/Humanities">human studies</a>, and demonstrate that it leads to better <a href="https://en.wikipedia.org/wiki/Dialogue">dialog</a>, i.e. dialog that is more diverse (i.e. less repetitive), consistent (i.e. has fewer conflicting exchanges), fluent (i.e., more human-like), and detailed, while still being comparably image-relevant as prior work and ablations.</abstract>
      <url hash="4af41ad0">D19-1152</url>
      <attachment hash="275ec017">D19-1152.Attachment.pdf</attachment>
      <doi>10.18653/v1/D19-1152</doi>
      <bibkey>murahari-etal-2019-improving</bibkey>
      <pwccode url="https://github.com/vmurahari3/visdial-diversity" additional="false">vmurahari3/visdial-diversity</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/visdial">VisDial</pwcdataset>
    </paper>
    <paper id="153">
      <title>Cross-lingual Transfer Learning with Data Selection for Large-Scale Spoken Language Understanding</title>
      <author><first>Quynh</first><last>Do</last></author>
      <author><first>Judith</first><last>Gaspers</last></author>
      <pages>1455–1460</pages>
      <abstract>A typical cross-lingual transfer learning approach boosting model performance on a language is to pre-train the <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> on all available <a href="https://en.wikipedia.org/wiki/Supervised_learning">supervised data</a> from another language. However, in large-scale systems this leads to high <a href="https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets">training times</a> and <a href="https://en.wikipedia.org/wiki/Computational_complexity_theory">computational requirements</a>. In addition, characteristic differences between the source and target languages raise a natural question of whether source data selection can improve the <a href="https://en.wikipedia.org/wiki/Knowledge_transfer">knowledge transfer</a>. In this paper, we address this question and propose a simple but effective language model based source-language data selection method for cross-lingual transfer learning in large-scale spoken language understanding. The experimental results show that with data selection i) source data and hence training speed is reduced significantly and ii) <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> performance is improved.</abstract>
      <url hash="1fd1fd44">D19-1153</url>
      <doi>10.18653/v1/D19-1153</doi>
      <bibkey>do-gaspers-2019-cross</bibkey>
    </paper>
    <paper id="154">
      <title>Multi-Head Attention with Diversity for Learning Grounded Multilingual Multimodal Representations</title>
      <author><first>Po-Yao</first><last>Huang</last></author>
      <author><first>Xiaojun</first><last>Chang</last></author>
      <author><first>Alexander</first><last>Hauptmann</last></author>
      <pages>1461–1467</pages>
      <abstract>With the aim of promoting and understanding the multilingual version of image search, we leverage visual object detection and propose a model with diverse multi-head attention to learn grounded multilingual multimodal representations. Specifically, our model attends to different types of textual semantics in two languages and visual objects for fine-grained alignments between sentences and images. We introduce a new <a href="https://en.wikipedia.org/wiki/Loss_function">objective function</a> which explicitly encourages attention diversity to learn an improved visual-semantic embedding space. We evaluate our model in the German-Image and English-Image matching tasks on the Multi30 K dataset, and in the Semantic Textual Similarity task with the English descriptions of visual content. Results show that our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> yields a significant performance gain over other methods in all of the three <a href="https://en.wikipedia.org/wiki/Task_(project_management)">tasks</a>.</abstract>
      <url hash="dc72cb55">D19-1154</url>
      <doi>10.18653/v1/D19-1154</doi>
      <bibkey>huang-etal-2019-multi</bibkey>
    </paper>
    <paper id="155">
      <title>Decoupled Box Proposal and Featurization with Ultrafine-Grained Semantic Labels Improve Image Captioning and Visual Question Answering</title>
      <author><first>Soravit</first><last>Changpinyo</last></author>
      <author><first>Bo</first><last>Pang</last></author>
      <author><first>Piyush</first><last>Sharma</last></author>
      <author><first>Radu</first><last>Soricut</last></author>
      <pages>1468–1474</pages>
      <abstract>Object detection plays an important role in current solutions to vision and language tasks like image captioning and visual question answering. However, popular models like Faster R-CNN rely on a costly process of annotating ground-truths for both the bounding boxes and their corresponding semantic labels, making it less amenable as a primitive task for <a href="https://en.wikipedia.org/wiki/Transfer_learning">transfer learning</a>. In this paper, we examine the effect of decoupling box proposal and featurization for <a href="https://en.wikipedia.org/wiki/Downstream_(networking)">down-stream tasks</a>. The key insight is that this allows us to leverage a large amount of labeled annotations that were previously unavailable for standard object detection benchmarks. Empirically, we demonstrate that this leads to effective <a href="https://en.wikipedia.org/wiki/Transfer_learning">transfer learning</a> and improved image captioning and visual question answering models, as measured on publicly-available benchmarks.</abstract>
      <url hash="fe3f6eeb">D19-1155</url>
      <attachment hash="56ac472d">D19-1155.Attachment.zip</attachment>
      <doi>10.18653/v1/D19-1155</doi>
      <bibkey>changpinyo-etal-2019-decoupled</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/coco">COCO</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/conceptual-captions">Conceptual Captions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/visual-genome">Visual Genome</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/visual-question-answering">Visual Question Answering</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/vizwiz">VizWiz</pwcdataset>
    </paper>
    <paper id="159">
      <title>Robust Navigation with Language Pretraining and Stochastic Sampling</title>
      <author><first>Xiujun</first><last>Li</last></author>
      <author><first>Chunyuan</first><last>Li</last></author>
      <author><first>Qiaolin</first><last>Xia</last></author>
      <author><first>Yonatan</first><last>Bisk</last></author>
      <author><first>Asli</first><last>Celikyilmaz</last></author>
      <author><first>Jianfeng</first><last>Gao</last></author>
      <author><first>Noah A.</first><last>Smith</last></author>
      <author><first>Yejin</first><last>Choi</last></author>
      <pages>1494–1499</pages>
      <abstract>Core to the vision-and-language navigation (VLN) challenge is building robust instruction representations and action decoding schemes, which can generalize well to previously unseen instructions and environments. In this paper, we report two simple but highly effective methods to address these challenges and lead to a new state-of-the-art performance. First, we adapt large-scale pretrained language models to learn text representations that generalize better to previously unseen instructions. Second, we propose a stochastic sampling scheme to reduce the considerable gap between the expert actions in training and sampled actions in test, so that the agent can learn to correct its own mistakes during long sequential action decoding. Combining the two techniques, we achieve a new state of the art on the Room-to-Room benchmark with 6 % absolute gain over the previous best result (47 %-53 %) on the Success Rate weighted by Path Length metric.</abstract>
      <url hash="83b8857d">D19-1159</url>
      <attachment hash="46334a8e">D19-1159.Attachment.pdf</attachment>
      <doi>10.18653/v1/D19-1159</doi>
      <bibkey>li-etal-2019-robust</bibkey>
      <pwccode url="https://github.com/xjli/r2r_vln" additional="false">xjli/r2r_vln</pwccode>
    </paper>
    <paper id="161">
      <title>Unsupervised Labeled Parsing with Deep Inside-Outside Recursive Autoencoders</title>
      <author><first>Andrew</first><last>Drozdov</last></author>
      <author><first>Patrick</first><last>Verga</last></author>
      <author><first>Yi-Pei</first><last>Chen</last></author>
      <author><first>Mohit</first><last>Iyyer</last></author>
      <author><first>Andrew</first><last>McCallum</last></author>
      <pages>1507–1512</pages>
      <abstract>Understanding text often requires identifying meaningful <a href="https://en.wikipedia.org/wiki/Constituent_(linguistics)">constituent spans</a> such as <a href="https://en.wikipedia.org/wiki/Noun_phrase">noun phrases</a> and <a href="https://en.wikipedia.org/wiki/Verb_phrase">verb phrases</a>. In this work, we show that we can effectively recover these types of labels using the learned phrase vectors from deep inside-outside recursive autoencoders (DIORA). Specifically, we cluster span representations to induce span labels. Additionally, we improve the <a href="https://en.wikipedia.org/wiki/Statistical_model">model</a>’s labeling accuracy by integrating latent code learning into the training procedure. We evaluate this approach empirically through unsupervised labeled constituency parsing. Our method outperforms ELMo and BERT on two versions of the Wall Street Journal (WSJ) dataset and is competitive to prior work that requires additional human annotations, improving over a previous state-of-the-art system that depends on ground-truth part-of-speech tags by 5 absolute F1 points (19 % relative error reduction).</abstract>
      <url hash="70091fbe">D19-1161</url>
      <attachment hash="7a3e8aff">D19-1161.Attachment.pdf</attachment>
      <doi>10.18653/v1/D19-1161</doi>
      <bibkey>drozdov-etal-2019-unsupervised</bibkey>
    </paper>
    <paper id="163">
      <title>Span-based Hierarchical Semantic Parsing for Task-Oriented Dialog</title>
      <author><first>Panupong</first><last>Pasupat</last></author>
      <author><first>Sonal</first><last>Gupta</last></author>
      <author><first>Karishma</first><last>Mandyam</last></author>
      <author><first>Rushin</first><last>Shah</last></author>
      <author><first>Mike</first><last>Lewis</last></author>
      <author><first>Luke</first><last>Zettlemoyer</last></author>
      <pages>1520–1526</pages>
      <abstract>We propose a <a href="https://en.wikipedia.org/wiki/Semantic_parser">semantic parser</a> for parsing compositional utterances into Task Oriented Parse (TOP), a <a href="https://en.wikipedia.org/wiki/Tree_(data_structure)">tree representation</a> that has intents and slots as labels of nesting tree nodes. Our <a href="https://en.wikipedia.org/wiki/Parsing">parser</a> is span-based : it scores labels of the <a href="https://en.wikipedia.org/wiki/Tree_(data_structure)">tree nodes</a> covering each token span independently, but then decodes a valid <a href="https://en.wikipedia.org/wiki/Tree_(data_structure)">tree</a> globally. In contrast to previous sequence decoding approaches and other span-based parsers, we (1) improve the training speed by removing the need to run the decoder at training time ; and (2) introduce edge scores, which model relations between parent and child labels, to mitigate the independence assumption between node labels and improve accuracy. Our best <a href="https://en.wikipedia.org/wiki/Parsing">parser</a> outperforms previous methods on the TOP dataset of mixed-domain task-oriented utterances in both <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> and training speed.</abstract>
      <url hash="1bacda3a">D19-1163</url>
      <attachment hash="cffa2418">D19-1163.Attachment.zip</attachment>
      <doi>10.18653/v1/D19-1163</doi>
      <bibkey>pasupat-etal-2019-span</bibkey>
    </paper>
    <paper id="166">
      <title>Controlling Text Complexity in <a href="https://en.wikipedia.org/wiki/Neural_machine_translation">Neural Machine Translation</a></title>
      <author><first>Sweta</first><last>Agrawal</last></author>
      <author><first>Marine</first><last>Carpuat</last></author>
      <pages>1549–1564</pages>
      <abstract>This work introduces a machine translation task where the output is aimed at audiences of different levels of target language proficiency. We collect a high quality dataset of <a href="https://en.wikipedia.org/wiki/Article_(publishing)">news articles</a> available in English and Spanish, written for diverse grade levels and propose a method to align segments across comparable bilingual articles. The resulting <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> makes it possible to train multi-task sequence to sequence models that can translate and simplify text jointly. We show that these multi-task models outperform <a href="https://en.wikipedia.org/wiki/Pipeline_(software)">pipeline approaches</a> that translate and simplify text independently.</abstract>
      <url hash="08f19f49">D19-1166</url>
      <attachment hash="1f97d7e0">D19-1166.Attachment.zip</attachment>
      <doi>10.18653/v1/D19-1166</doi>
      <bibkey>agrawal-carpuat-2019-controlling</bibkey>
      <pwccode url="https://github.com/sweta20/ComplexityControlledMT" additional="false">sweta20/ComplexityControlledMT</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/newsela">Newsela</pwcdataset>
    </paper>
    <paper id="169">
      <title>Cross-Lingual Machine Reading Comprehension</title>
      <author><first>Yiming</first><last>Cui</last></author>
      <author><first>Wanxiang</first><last>Che</last></author>
      <author><first>Ting</first><last>Liu</last></author>
      <author><first>Bing</first><last>Qin</last></author>
      <author><first>Shijin</first><last>Wang</last></author>
      <author><first>Guoping</first><last>Hu</last></author>
      <pages>1586–1595</pages>
      <abstract>Though the community has made great progress on Machine Reading Comprehension (MRC) task, most of the previous works are solving English-based MRC problems, and there are few efforts on other languages mainly due to the lack of large-scale training data. In this paper, we propose Cross-Lingual Machine Reading Comprehension (CLMRC) task for the languages other than English. Firstly, we present several back-translation approaches for CLMRC task which is straightforward to adopt. However, to exactly align the answer into source language is difficult and could introduce additional noise. In this context, we propose a novel model called Dual BERT, which takes advantage of the large-scale training data provided by rich-resource language (such as English) and learn the semantic relations between the passage and question in bilingual context, and then utilize the learned knowledge to improve <a href="https://en.wikipedia.org/wiki/Reading_comprehension">reading comprehension</a> performance of low-resource language. We conduct experiments on two Chinese machine reading comprehension datasets CMRC 2018 and DRCD. The results show consistent and significant improvements over various state-of-the-art systems by a large margin, which demonstrate the potentials in CLMRC task. Resources available : https://github.com/ymcui/Cross-Lingual-MRC</abstract>
      <url hash="fa1271e8">D19-1169</url>
      <doi>10.18653/v1/D19-1169</doi>
      <bibkey>cui-etal-2019-cross</bibkey>
      <pwccode url="https://github.com/ymcui/Cross-Lingual-MRC" additional="false">ymcui/Cross-Lingual-MRC</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/cmrc">CMRC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/cmrc-2018">CMRC 2018</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/drcd">DRCD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
    </paper>
    <paper id="171">
      <title>Neural Duplicate Question Detection without Labeled Training Data</title>
      <author><first>Andreas</first><last>Rücklé</last></author>
      <author><first>Nafise Sadat</first><last>Moosavi</last></author>
      <author><first>Iryna</first><last>Gurevych</last></author>
      <pages>1607–1617</pages>
      <abstract>Supervised training of neural models to duplicate question detection in community Question Answering (CQA) requires large amounts of labeled question pairs, which can be costly to obtain. To minimize this cost, recent works thus often used alternative methods, e.g., adversarial domain adaptation. In this work, we propose two novel methodsweak supervision using the title and body of a question, and the automatic generation of duplicate questionsand show that both can achieve improved performances even though they do not require any labeled data. We provide a comparison of popular training strategies and show that our proposed approaches are more effective in many cases because they can utilize larger amounts of data from the CQA forums. Finally, we show that weak supervision with question title and body information is also an effective method to train CQA answer selection models without direct answer supervision.</abstract>
      <url hash="eb93025f">D19-1171</url>
      <attachment hash="4f4d1b2c">D19-1171.Attachment.zip</attachment>
      <doi>10.18653/v1/D19-1171</doi>
      <bibkey>ruckle-etal-2019-neural</bibkey>
      <pwccode url="https://github.com/UKPLab/emnlp2019-duplicate_question_detection" additional="false">UKPLab/emnlp2019-duplicate_question_detection</pwccode>
    </paper>
    <paper id="173">
      <title>Multi-View Domain Adapted Sentence Embeddings for Low-Resource Unsupervised Duplicate Question Detection</title>
      <author><first>Nina</first><last>Poerner</last></author>
      <author><first>Hinrich</first><last>Schütze</last></author>
      <pages>1630–1641</pages>
      <abstract>We address the problem of Duplicate Question Detection (DQD) in low-resource domain-specific Community Question Answering forums. Our multi-view framework MV-DASE combines an ensemble of sentence encoders via Generalized Canonical Correlation Analysis, using unlabeled data only. In our experiments, the <a href="https://en.wikipedia.org/wiki/Statistical_ensemble_(mathematical_physics)">ensemble</a> includes generic and domain-specific averaged word embeddings, domain-finetuned BERT and the Universal Sentence Encoder. We evaluate MV-DASE on the CQADupStack corpus and on additional low-resource Stack Exchange forums. Combining the strengths of different encoders, we significantly outperform BM25, all single-view systems as well as a recent supervised domain-adversarial DQD method.</abstract>
      <url hash="adf81a15">D19-1173</url>
      <attachment hash="63eef917">D19-1173.Attachment.zip</attachment>
      <doi>10.18653/v1/D19-1173</doi>
      <bibkey>poerner-schutze-2019-multi</bibkey>
    </paper>
    <paper id="174">
      <title>Multi-label Categorization of Accounts of Sexism using a Neural Framework</title>
      <author><first>Pulkit</first><last>Parikh</last></author>
      <author><first>Harika</first><last>Abburi</last></author>
      <author><first>Pinkesh</first><last>Badjatiya</last></author>
      <author><first>Radhika</first><last>Krishnan</last></author>
      <author><first>Niyati</first><last>Chhaya</last></author>
      <author><first>Manish</first><last>Gupta</last></author>
      <author><first>Vasudeva</first><last>Varma</last></author>
      <pages>1642–1652</pages>
      <abstract>Sexism, an <a href="https://en.wikipedia.org/wiki/Injustice">injustice</a> that subjects women and girls to enormous suffering, manifests in blatant as well as subtle ways. In the wake of growing documentation of experiences of <a href="https://en.wikipedia.org/wiki/Sexism">sexism</a> on the web, the automatic categorization of accounts of sexism has the potential to assist social scientists and policy makers in utilizing such data to study and counter sexism better. The existing work on <a href="https://en.wikipedia.org/wiki/Sexism">sexism classification</a>, which is different from sexism detection, has certain limitations in terms of the categories of <a href="https://en.wikipedia.org/wiki/Sexism">sexism</a> used and/or whether they can co-occur. To the best of our knowledge, this is the first work on the multi-label classification of sexism of any kind(s), and we contribute the largest dataset for sexism categorization. We develop a neural solution for this multi-label classification that can combine sentence representations obtained using models such as BERT with distributional and linguistic word embeddings using a flexible, hierarchical architecture involving recurrent components and optional convolutional ones. Further, we leverage unlabeled accounts of sexism to infuse domain-specific elements into our <a href="https://en.wikipedia.org/wiki/Software_framework">framework</a>. The best proposed method outperforms several <a href="https://en.wikipedia.org/wiki/Deep_learning">deep learning</a> as well as traditional machine learning baselines by an appreciable margin.</abstract>
      <url hash="1ec120ee">D19-1174</url>
      <attachment hash="78350e74">D19-1174.Attachment.pdf</attachment>
      <doi>10.18653/v1/D19-1174</doi>
      <bibkey>parikh-etal-2019-multi</bibkey>
      <pwccode url="https://github.com/pulkitparikh/sexism_classification" additional="false">pulkitparikh/sexism_classification</pwccode>
    </paper>
    <paper id="175">
      <title>The Trumpiest Trump? Identifying a Subject’s Most Characteristic Tweets</title>
      <author><first>Charuta</first><last>Pethe</last></author>
      <author><first>Steve</first><last>Skiena</last></author>
      <pages>1653–1663</pages>
      <abstract>The sequence of documents produced by any given author varies in style and content, but some documents are more typical or representative of the source than others. We quantify the extent to which a given short text is characteristic of a specific person, using a <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> of tweets from fifteen celebrities. Such analysis is useful for generating excerpts of high-volume Twitter profiles, and understanding how <a href="https://en.wikipedia.org/wiki/Representation_(mathematics)">representativeness</a> relates to tweet popularity. We first consider the related task of binary author detection (is x the author of text T?), and report a <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">test accuracy</a> of 90.37 % for the best of five approaches to this <a href="https://en.wikipedia.org/wiki/Problem_solving">problem</a>. We then use these <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> to compute characterization scores among all of an author’s texts. A user study shows human evaluators agree with our characterization model for all 15 celebrities in our <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a>, each with p-value   0.05. We use these <a href="https://en.wikipedia.org/wiki/Classifier_(linguistics)">classifiers</a> to show surprisingly strong correlations between characterization scores and the popularity of the associated texts. Indeed, we demonstrate a statistically significant correlation between this score and tweet popularity (likes / replies / retweets) for 13 of the 15 celebrities in our study.</abstract>
      <url hash="37121add">D19-1175</url>
      <doi>10.18653/v1/D19-1175</doi>
      <bibkey>pethe-skiena-2019-trumpiest</bibkey>
    </paper>
    <paper id="176">
      <title>Finding Microaggressions in the Wild : A Case for Locating Elusive Phenomena in Social Media Posts</title>
      <author><first>Luke</first><last>Breitfeller</last></author>
      <author><first>Emily</first><last>Ahn</last></author>
      <author><first>David</first><last>Jurgens</last></author>
      <author><first>Yulia</first><last>Tsvetkov</last></author>
      <pages>1664–1674</pages>
      <abstract>Microaggressions are subtle, often veiled, manifestations of <a href="https://en.wikipedia.org/wiki/Bias">human biases</a>. These uncivil interactions can have a powerful negative impact on people by marginalizing minorities and disadvantaged groups. The linguistic subtlety of <a href="https://en.wikipedia.org/wiki/Microaggression">microaggressions</a> in <a href="https://en.wikipedia.org/wiki/Communication">communication</a> has made it difficult for researchers to analyze their exact nature, and to quantify and extract <a href="https://en.wikipedia.org/wiki/Microaggression">microaggressions</a> automatically. Specifically, the lack of a corpus of real-world microaggressions and objective criteria for annotating them have prevented researchers from addressing these problems at scale. In this paper, we devise a general but nuanced, computationally operationalizable typology of <a href="https://en.wikipedia.org/wiki/Microaggression">microaggressions</a> based on a small subset of data that we have. We then create two <a href="https://en.wikipedia.org/wiki/Data_set">datasets</a> : one with examples of diverse types of <a href="https://en.wikipedia.org/wiki/Microaggression">microaggressions</a> recollected by their targets, and another with gender-based microaggressions in public conversations on <a href="https://en.wikipedia.org/wiki/Social_media">social media</a>. We introduce a new, more objective, criterion for <a href="https://en.wikipedia.org/wiki/Annotation">annotation</a> and an active-learning based procedure that increases the likelihood of surfacing posts containing <a href="https://en.wikipedia.org/wiki/Microaggression">microaggressions</a>. Finally, we analyze the trends that emerge from these new <a href="https://en.wikipedia.org/wiki/Data_set">datasets</a>.</abstract>
      <url hash="5d74c404">D19-1176</url>
      <attachment hash="744d0a4b">D19-1176.Attachment.zip</attachment>
      <doi>10.18653/v1/D19-1176</doi>
      <bibkey>breitfeller-etal-2019-finding</bibkey>
    </paper>
    <paper id="178">
      <title>Learning Invariant Representations of Social Media Users</title>
      <author><first>Nicholas</first><last>Andrews</last></author>
      <author><first>Marcus</first><last>Bishop</last></author>
      <pages>1684–1695</pages>
      <abstract>The evolution of social media users’ behavior over time complicates user-level comparison tasks such as <a href="https://en.wikipedia.org/wiki/Verification_and_validation">verification</a>, <a href="https://en.wikipedia.org/wiki/Statistical_classification">classification</a>, <a href="https://en.wikipedia.org/wiki/Cluster_analysis">clustering</a>, and <a href="https://en.wikipedia.org/wiki/Ranking">ranking</a>. As a result, naive approaches may fail to generalize to new users or even to future observations of previously known users. In this paper, we propose a novel procedure to learn a <a href="https://en.wikipedia.org/wiki/Map_(mathematics)">mapping</a> from short episodes of user activity on <a href="https://en.wikipedia.org/wiki/Social_media">social media</a> to a <a href="https://en.wikipedia.org/wiki/Vector_space">vector space</a> in which the distance between points captures the similarity of the corresponding users’ invariant features. We fit the model by optimizing a surrogate metric learning objective over a large corpus of unlabeled social media content. Once learned, the <a href="https://en.wikipedia.org/wiki/Map_(mathematics)">mapping</a> may be applied to users not seen at training time and enables efficient comparisons of users in the resulting <a href="https://en.wikipedia.org/wiki/Vector_space">vector space</a>. We present a comprehensive evaluation to validate the benefits of the proposed approach using <a href="https://en.wikipedia.org/wiki/Data">data</a> from <a href="https://en.wikipedia.org/wiki/Reddit">Reddit</a>, <a href="https://en.wikipedia.org/wiki/Twitter">Twitter</a>, and <a href="https://en.wikipedia.org/wiki/Wikipedia">Wikipedia</a>.</abstract>
      <url hash="d5bad3c2">D19-1178</url>
      <doi>10.18653/v1/D19-1178</doi>
      <bibkey>andrews-bishop-2019-learning</bibkey>
      <pwccode url="https://github.com/noa/iur" additional="false">noa/iur</pwccode>
    </paper>
    <paper id="179">
      <title>(Male, Bachelor) and (Female, Ph. D) have different connotations : Parallelly Annotated Stylistic Language Dataset with Multiple Personas<fixed-case>P</fixed-case>h.<fixed-case>D</fixed-case>) have different connotations: Parallelly Annotated Stylistic Language Dataset with Multiple Personas</title>
      <author><first>Dongyeop</first><last>Kang</last></author>
      <author><first>Varun</first><last>Gangal</last></author>
      <author><first>Eduard</first><last>Hovy</last></author>
      <pages>1696–1706</pages>
      <abstract>Stylistic variation in text needs to be studied with different aspects including the writer’s personal traits, <a href="https://en.wikipedia.org/wiki/Interpersonal_relationship">interpersonal relations</a>, <a href="https://en.wikipedia.org/wiki/Rhetoric">rhetoric</a>, and more. Despite recent attempts on computational modeling of the variation, the lack of parallel corpora of style language makes it difficult to systematically control the stylistic change as well as evaluate such models. We release PASTEL, the parallel and annotated stylistic language dataset, that contains ~41 K parallel sentences (8.3 K parallel stories) annotated across different personas. Each persona has different styles in conjunction : gender, age, country, <a href="https://en.wikipedia.org/wiki/Politics">political view</a>, <a href="https://en.wikipedia.org/wiki/Education">education</a>, <a href="https://en.wikipedia.org/wiki/Ethnic_group">ethnic</a>, and time-of-writing. The <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> is collected from human annotators with solid control of input denotation : not only preserving original meaning between text, but promoting stylistic diversity to annotators. We test the <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> on two interesting applications of <a href="https://en.wikipedia.org/wiki/Style_language">style language</a>, where PASTEL helps design appropriate experiment and evaluation. First, in predicting a target style (e.g., male or female in gender) given a text, multiple styles of PASTEL make other external style variables controlled (or fixed), which is a more accurate experimental design. Second, a simple <a href="https://en.wikipedia.org/wiki/Supervised_learning">supervised model</a> with our parallel text outperforms the <a href="https://en.wikipedia.org/wiki/Unsupervised_learning">unsupervised models</a> using nonparallel text in style transfer. Our dataset is publicly available.</abstract>
      <url hash="e5460084">D19-1179</url>
      <attachment hash="5de6f77a">D19-1179.Attachment.pdf</attachment>
      <doi>10.18653/v1/D19-1179</doi>
      <bibkey>kang-etal-2019-male</bibkey>
      <pwccode url="https://github.com/dykang/PASTEL" additional="false">dykang/PASTEL</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/pastel">PASTEL</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/vist">VIST</pwcdataset>
    </paper>
    <paper id="180">
      <title>Movie Plot Analysis via Turning Point Identification</title>
      <author><first>Pinelopi</first><last>Papalampidi</last></author>
      <author><first>Frank</first><last>Keller</last></author>
      <author><first>Mirella</first><last>Lapata</last></author>
      <pages>1707–1717</pages>
      <abstract>According to screenwriting theory, turning points (e.g., change of plans, major setback, climax) are crucial narrative moments within a screenplay : they define the plot structure, determine its progression and segment the screenplay into thematic units (e.g., setup, complications, aftermath). We propose the task of turning point identification in <a href="https://en.wikipedia.org/wiki/Film">movies</a> as a means of analyzing their <a href="https://en.wikipedia.org/wiki/Narrative_structure">narrative structure</a>. We argue that turning points and the segmentation they provide can facilitate processing long, complex narratives, such as <a href="https://en.wikipedia.org/wiki/Screenplay">screenplays</a>, for summarization and <a href="https://en.wikipedia.org/wiki/Question_answering">question answering</a>. We introduce a dataset consisting of <a href="https://en.wikipedia.org/wiki/Screenplay">screenplays</a> and <a href="https://en.wikipedia.org/wiki/Plot_(narrative)">plot synopses</a> annotated with turning points and present an end-to-end neural network model that identifies turning points in <a href="https://en.wikipedia.org/wiki/Plot_(narrative)">plot synopses</a> and projects them onto scenes in <a href="https://en.wikipedia.org/wiki/Screenplay">screenplays</a>. Our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> outperforms strong baselines based on state-of-the-art <a href="https://en.wikipedia.org/wiki/Sentence_(linguistics)">sentence representations</a> and the expected position of turning points.</abstract>
      <url hash="5da946e7">D19-1180</url>
      <attachment hash="c61f64d5">D19-1180.Attachment.zip</attachment>
      <doi>10.18653/v1/D19-1180</doi>
      <bibkey>papalampidi-etal-2019-movie</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/tripod">TRIPOD</pwcdataset>
    </paper>
    <paper id="182">
      <title>Deep Ordinal Regression for Pledge Specificity Prediction</title>
      <author><first>Shivashankar</first><last>Subramanian</last></author>
      <author><first>Trevor</first><last>Cohn</last></author>
      <author><first>Timothy</first><last>Baldwin</last></author>
      <pages>1729–1740</pages>
      <abstract>Many pledges are made in the course of an <a href="https://en.wikipedia.org/wiki/Political_campaign">election campaign</a>, forming important corpora for political analysis of campaign strategy and governmental accountability. At present, there are no publicly available annotated datasets of pledges, and most political analyses rely on manual annotations. In this paper we collate a novel <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> of manifestos from eleven <a href="https://en.wikipedia.org/wiki/Elections_in_Australia">Australian federal election cycles</a>, with over 12,000 sentences annotated with <a href="https://en.wikipedia.org/wiki/Sensitivity_and_specificity">specificity</a> (e.g., rhetorical vs detailed pledge) on a <a href="https://en.wikipedia.org/wiki/Scale_(social_sciences)">fine-grained scale</a>. We propose deep ordinal regression approaches for specificity prediction, under both supervised and semi-supervised settings, and provide empirical results demonstrating the effectiveness of the proposed techniques over several baseline approaches. We analyze the utility of pledge specificity modeling across a spectrum of policy issues in performing ideology prediction, and further provide qualitative analysis in terms of capturing party-specific issue salience across election cycles.</abstract>
      <url hash="0689b7ce">D19-1182</url>
      <doi>10.18653/v1/D19-1182</doi>
      <bibkey>subramanian-etal-2019-deep</bibkey>
      <pwccode url="https://github.com/shivashankarrs/Pledge-Specificity" additional="false">shivashankarrs/Pledge-Specificity</pwccode>
    </paper>
    <paper id="185">
      <title>Are You for Real? Detecting Identity Fraud via Dialogue Interactions</title>
      <author><first>Weikang</first><last>Wang</last></author>
      <author><first>Jiajun</first><last>Zhang</last></author>
      <author><first>Qian</first><last>Li</last></author>
      <author><first>Chengqing</first><last>Zong</last></author>
      <author><first>Zhifei</first><last>Li</last></author>
      <pages>1762–1771</pages>
      <abstract>Identity fraud detection is of great importance in many real-world scenarios such as the <a href="https://en.wikipedia.org/wiki/Financial_services">financial industry</a>. However, few studies addressed this problem before. In this paper, we focus on identity fraud detection in <a href="https://en.wikipedia.org/wiki/Loan">loan applications</a> and propose to solve this problem with a novel interactive dialogue system which consists of two <a href="https://en.wikipedia.org/wiki/Modular_programming">modules</a>. One is the knowledge graph (KG) constructor organizing the <a href="https://en.wikipedia.org/wiki/Personal_data">personal information</a> for each loan applicant. The other is structured dialogue management that can dynamically generate a series of questions based on the personal KG to ask the applicants and determine their <a href="https://en.wikipedia.org/wiki/Identity_(social_science)">identity states</a>. We also present a heuristic user simulator based on <a href="https://en.wikipedia.org/wiki/Problem_analysis">problem analysis</a> to evaluate our <a href="https://en.wikipedia.org/wiki/Methodology">method</a>. Experiments have shown that the trainable dialogue system can effectively detect <a href="https://en.wikipedia.org/wiki/Fraud">fraudsters</a>, and achieve higher recognition accuracy compared with rule-based systems. Furthermore, our learned dialogue strategies are interpretable and flexible, which can help promote real-world applications.</abstract>
      <url hash="66badba9">D19-1185</url>
      <attachment hash="03bdc123">D19-1185.Attachment.zip</attachment>
      <doi>10.18653/v1/D19-1185</doi>
      <bibkey>wang-etal-2019-real</bibkey>
      <pwccode url="https://github.com/Leechikara/Dialogue-Based-Anti-Fraud" additional="false">Leechikara/Dialogue-Based-Anti-Fraud</pwccode>
    </paper>
    <paper id="186">
      <title>Hierarchy Response Learning for Neural Conversation Generation</title>
      <author><first>Bo</first><last>Zhang</last></author>
      <author><first>Xiaoming</first><last>Zhang</last></author>
      <pages>1772–1781</pages>
      <abstract>The neural encoder-decoder models have shown great promise in neural conversation generation. However, they can not perceive and express the intention effectively, and hence often generate dull and generic responses. Unlike past work that has focused on diversifying the output at word-level or discourse-level with a flat model to alleviate this problem, we propose a hierarchical generation model to capture the different levels of diversity using the conditional variational autoencoders. Specifically, a hierarchical response generation (HRG) framework is proposed to capture the conversation intention in a natural and coherent way. It has two modules, namely, an expression reconstruction model to capture the hierarchical correlation between expression and intention, and an expression attention model to effectively combine the expressions with contents. Finally, the training procedure of <a href="https://en.wikipedia.org/wiki/Homeostasis">HRG</a> is improved by introducing reconstruction loss. Experiment results show that our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> can generate the responses with more appropriate content and expression.</abstract>
      <url hash="65aa5c91">D19-1186</url>
      <doi>10.18653/v1/D19-1186</doi>
      <bibkey>zhang-zhang-2019-hierarchy</bibkey>
    </paper>
    <paper id="187">
      <title>Knowledge Aware Conversation Generation with Explainable Reasoning over Augmented Graphs</title>
      <author><first>Zhibin</first><last>Liu</last></author>
      <author><first>Zheng-Yu</first><last>Niu</last></author>
      <author><first>Hua</first><last>Wu</last></author>
      <author><first>Haifeng</first><last>Wang</last></author>
      <pages>1782–1792</pages>
      <abstract>Two types of <a href="https://en.wikipedia.org/wiki/Knowledge">knowledge</a>, triples from <a href="https://en.wikipedia.org/wiki/Knowledge">knowledge graphs</a> and texts from documents, have been studied for <a href="https://en.wikipedia.org/wiki/Knowledge">knowledge aware open domain conversation generation</a>, in which graph paths can narrow down vertex candidates for <a href="https://en.wikipedia.org/wiki/Knowledge">knowledge selection decision</a>, and texts can provide rich information for response generation. Fusion of a <a href="https://en.wikipedia.org/wiki/Knowledge_graph">knowledge graph</a> and texts might yield mutually reinforcing advantages, but there is less study on that. To address this challenge, we propose a knowledge aware chatting machine with three components, an augmented knowledge graph with both triples and texts, knowledge selector, and knowledge aware response generator. For knowledge selection on the <a href="https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)">graph</a>, we formulate it as a problem of multi-hop graph reasoning to effectively capture conversation flow, which is more explainable and flexible in comparison with previous works. To fully leverage long text information that differentiates our <a href="https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)">graph</a> from others, we improve a state of the art <a href="https://en.wikipedia.org/wiki/Automated_reasoning">reasoning algorithm</a> with machine reading comprehension technology. We demonstrate the effectiveness of our <a href="https://en.wikipedia.org/wiki/System">system</a> on two <a href="https://en.wikipedia.org/wiki/Data_set">datasets</a> in comparison with state-of-the-art models.</abstract>
      <url hash="40c90d6b">D19-1187</url>
      <doi>10.18653/v1/D19-1187</doi>
      <bibkey>liu-etal-2019-knowledge</bibkey>
      <pwccode url="https://github.com/PaddlePaddle/Research/tree/master/NLP/EMNLP2019-AKGCM" additional="false">PaddlePaddle/Research</pwccode>
    </paper>
    <paper id="189">
      <title>Towards Knowledge-Based Recommender Dialog System</title>
      <author><first>Qibin</first><last>Chen</last></author>
      <author><first>Junyang</first><last>Lin</last></author>
      <author><first>Yichang</first><last>Zhang</last></author>
      <author><first>Ming</first><last>Ding</last></author>
      <author><first>Yukuo</first><last>Cen</last></author>
      <author><first>Hongxia</first><last>Yang</last></author>
      <author><first>Jie</first><last>Tang</last></author>
      <pages>1803–1813</pages>
      <abstract>In this paper, we propose a novel end-to-end framework called KBRD, which stands for Knowledge-Based Recommender Dialog System. It integrates the <a href="https://en.wikipedia.org/wiki/Recommender_system">recommender system</a> and the dialog generation system. The dialog generation system can enhance the performance of the <a href="https://en.wikipedia.org/wiki/Recommender_system">recommendation system</a> by introducing information about users’ preferences, and the <a href="https://en.wikipedia.org/wiki/Recommender_system">recommender system</a> can improve that of the dialog generation system by providing recommendation-aware vocabulary bias. Experimental results demonstrate that our proposed <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> has significant advantages over the baselines in both the evaluation of dialog generation and <a href="https://en.wikipedia.org/wiki/Recommender_system">recommendation</a>. A series of analyses show that the two <a href="https://en.wikipedia.org/wiki/System">systems</a> can bring mutual benefits to each other, and the introduced <a href="https://en.wikipedia.org/wiki/Knowledge">knowledge</a> contributes to both their performances.</abstract>
      <url hash="c91ad60c">D19-1189</url>
      <doi>10.18653/v1/D19-1189</doi>
      <bibkey>chen-etal-2019-towards</bibkey>
      <pwccode url="https://github.com/THUDM/KBRD" additional="true">THUDM/KBRD</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/redial">ReDial</pwcdataset>
    </paper>
    <paper id="191">
      <title>Improving Open-Domain Dialogue Systems via Multi-Turn Incomplete Utterance Restoration</title>
      <author><first>Zhufeng</first><last>Pan</last></author>
      <author><first>Kun</first><last>Bai</last></author>
      <author><first>Yan</first><last>Wang</last></author>
      <author><first>Lianqiang</first><last>Zhou</last></author>
      <author><first>Xiaojiang</first><last>Liu</last></author>
      <pages>1824–1833</pages>
      <abstract>In multi-turn dialogue, utterances do not always take the full form of sentences. These incomplete utterances will greatly reduce the performance of open-domain dialogue systems. Restoring more incomplete utterances from context could potentially help the <a href="https://en.wikipedia.org/wiki/System">systems</a> generate more relevant responses. To facilitate the study of incomplete utterance restoration for open-domain dialogue systems, a large-scale multi-turn dataset Restoration-200 K is collected and manually labeled with the explicit relation between an utterance and its context. We also propose a pick-and-combine model to restore the incomplete utterance from its context. Experimental results demonstrate that the annotated dataset and the proposed approach significantly boost the response quality of both single-turn and multi-turn dialogue systems.</abstract>
      <url hash="ae311da1">D19-1191</url>
      <doi>10.18653/v1/D19-1191</doi>
      <bibkey>pan-etal-2019-improving</bibkey>
    </paper>
    <paper id="193">
      <title>Dually Interactive Matching Network for Personalized Response Selection in Retrieval-Based Chatbots</title>
      <author><first>Jia-Chen</first><last>Gu</last></author>
      <author><first>Zhen-Hua</first><last>Ling</last></author>
      <author><first>Xiaodan</first><last>Zhu</last></author>
      <author><first>Quan</first><last>Liu</last></author>
      <pages>1845–1854</pages>
      <abstract>This paper proposes a dually interactive matching network (DIM) for presenting the personalities of dialogue agents in retrieval-based chatbots. This model develops from the interactive matching network (IMN) which models the matching degree between a context composed of multiple utterances and a response candidate. Compared with previous <a href="https://en.wikipedia.org/wiki/Persona">persona fusion approach</a> which enhances the representation of a context by calculating its similarity with a given <a href="https://en.wikipedia.org/wiki/Persona">persona</a>, the DIM model adopts a dual matching architecture, which performs interactive matching between responses and contexts and between responses and personas respectively for ranking response candidates. Experimental results on PERSONA-CHAT dataset show that the DIM model outperforms its baseline model, i.e., <a href="https://en.wikipedia.org/wiki/IMN">IMN</a> with persona fusion, by a margin of 14.5 % and outperforms the present <a href="https://en.wikipedia.org/wiki/State-of-the-art">state-of-the-art model</a> by a margin of 27.7 % in terms of top-1 accuracy hits@1.</abstract>
      <url hash="4fa6255c">D19-1193</url>
      <doi>10.18653/v1/D19-1193</doi>
      <bibkey>gu-etal-2019-dually</bibkey>
      <pwccode url="https://github.com/JasonForJoy/DIM" additional="false">JasonForJoy/DIM</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/persona-chat-1">PERSONA-CHAT</pwcdataset>
    </paper>
    <paper id="195">
      <title>Retrieval-guided Dialogue Response Generation via a Matching-to-Generation Framework</title>
      <author><first>Deng</first><last>Cai</last></author>
      <author><first>Yan</first><last>Wang</last></author>
      <author><first>Wei</first><last>Bi</last></author>
      <author><first>Zhaopeng</first><last>Tu</last></author>
      <author><first>Xiaojiang</first><last>Liu</last></author>
      <author><first>Shuming</first><last>Shi</last></author>
      <pages>1866–1875</pages>
      <abstract>End-to-end sequence generation is a popular technique for developing open domain dialogue systems, though they suffer from the safe response problem. Researchers have attempted to tackle this problem by incorporating <a href="https://en.wikipedia.org/wiki/Generative_model">generative models</a> with the returns of <a href="https://en.wikipedia.org/wiki/Information_retrieval">retrieval systems</a>. Recently, a skeleton-then-response framework has been shown promising results for this <a href="https://en.wikipedia.org/wiki/Task_(computing)">task</a>. Nevertheless, how to precisely extract a <a href="https://en.wikipedia.org/wiki/Skeleton">skeleton</a> and how to effectively train a retrieval-guided response generator are still challenging. This paper presents a novel framework in which the skeleton extraction is made by an interpretable matching model and the following skeleton-guided response generation is accomplished by a separately trained generator. Extensive experiments demonstrate the effectiveness of our model designs.<i>safe response problem</i>. Researchers have attempted to tackle this problem by incorporating generative models with the returns of retrieval systems. Recently, a skeleton-then-response framework has been shown promising results for this task. Nevertheless, how to precisely extract a skeleton and how to effectively train a retrieval-guided response generator are still challenging. This paper presents a novel framework in which the skeleton extraction is made by an interpretable matching model and the following skeleton-guided response generation is accomplished by a separately trained generator. Extensive experiments demonstrate the effectiveness of our model designs.</abstract>
      <url hash="24314d17">D19-1195</url>
      <doi>10.18653/v1/D19-1195</doi>
      <bibkey>cai-etal-2019-retrieval</bibkey>
    </paper>
    <paper id="199">
      <title>Who Is Speaking to Whom? Learning to Identify Utterance Addressee in Multi-Party Conversations</title>
      <author><first>Ran</first><last>Le</last></author>
      <author><first>Wenpeng</first><last>Hu</last></author>
      <author><first>Mingyue</first><last>Shang</last></author>
      <author><first>Zhenjun</first><last>You</last></author>
      <author><first>Lidong</first><last>Bing</last></author>
      <author><first>Dongyan</first><last>Zhao</last></author>
      <author><first>Rui</first><last>Yan</last></author>
      <pages>1909–1919</pages>
      <abstract>Previous research on dialogue systems generally focuses on the conversation between two participants, yet multi-party conversations which involve more than two participants within one session bring up a more complicated but realistic scenario. In real multi- party conversations, we can observe who is speaking, but the addressee information is not always explicit. In this paper, we aim to tackle the challenge of identifying all the miss- ing addressees in a conversation session. To this end, we introduce a novel who-to-whom (W2W) model which models users and utterances in the session jointly in an interactive way. We conduct experiments on the benchmark Ubuntu Multi-Party Conversation Corpus and the experimental results demonstrate that our model outperforms baselines with consistent improvements.</abstract>
      <url hash="2ec254aa">D19-1199</url>
      <attachment hash="4b810d98">D19-1199.Attachment.pdf</attachment>
      <doi>10.18653/v1/D19-1199</doi>
      <bibkey>le-etal-2019-speaking</bibkey>
    </paper>
    <paper id="201">
      <title>Modeling Personalization in Continuous Space for Response Generation via Augmented Wasserstein Autoencoders<fixed-case>W</fixed-case>asserstein Autoencoders</title>
      <author><first>Zhangming</first><last>Chan</last></author>
      <author><first>Juntao</first><last>Li</last></author>
      <author><first>Xiaopeng</first><last>Yang</last></author>
      <author><first>Xiuying</first><last>Chen</last></author>
      <author><first>Wenpeng</first><last>Hu</last></author>
      <author><first>Dongyan</first><last>Zhao</last></author>
      <author><first>Rui</first><last>Yan</last></author>
      <pages>1931–1940</pages>
      <abstract>Variational autoencoders (VAEs) and Wasserstein autoencoders (WAEs) have achieved noticeable progress in open-domain response generation. Through introducing <a href="https://en.wikipedia.org/wiki/Latent_variable">latent variables</a> in <a href="https://en.wikipedia.org/wiki/Continuous_or_discrete_variable">continuous space</a>, these models are capable of capturing utterance-level semantics, e.g., topic, syntactic properties, and thus can generate informative and diversified responses. In this work, we improve the WAE for response generation. In addition to the utterance-level information, we also model user-level information in latent continue space. Specifically, we embed user-level and utterance-level information into two <a href="https://en.wikipedia.org/wiki/Multimodal_distribution">multimodal distributions</a>, and combine these two <a href="https://en.wikipedia.org/wiki/Multimodal_distribution">multimodal distributions</a> into a mixed distribution. This mixed distribution will be used as the prior distribution of WAE in our proposed model, named as PersonaWAE. Experimental results on a large-scale real-world dataset confirm the superiority of our model for generating informative and personalized responses, where both automatic and human evaluations outperform state-of-the-art models.</abstract>
      <url hash="112f87c9">D19-1201</url>
      <doi>10.18653/v1/D19-1201</doi>
      <bibkey>chan-etal-2019-modeling</bibkey>
    </paper>
    <paper id="203">
      <title>Recommendation as a Communication Game : Self-Supervised Bot-Play for Goal-oriented Dialogue</title>
      <author><first>Dongyeop</first><last>Kang</last></author>
      <author><first>Anusha</first><last>Balakrishnan</last></author>
      <author><first>Pararth</first><last>Shah</last></author>
      <author><first>Paul</first><last>Crook</last></author>
      <author><first>Y-Lan</first><last>Boureau</last></author>
      <author><first>Jason</first><last>Weston</last></author>
      <pages>1951–1961</pages>
      <abstract>Traditional recommendation systems produce static rather than interactive recommendations invariant to a user’s specific requests, clarifications, or current mood, and can suffer from the cold-start problem if their tastes are unknown. These issues can be alleviated by treating <a href="https://en.wikipedia.org/wiki/Recommender_system">recommendation</a> as an interactive dialogue task instead, where an expert recommender can sequentially ask about someone’s preferences, react to their requests, and recommend more appropriate items. In this work, we collect a goal-driven recommendation dialogue dataset (GoRecDial), which consists of 9,125 dialogue games and 81,260 conversation turns between pairs of human workers recommending movies to each other. The <a href="https://en.wikipedia.org/wiki/Task_(project_management)">task</a> is specifically designed as a <a href="https://en.wikipedia.org/wiki/Cooperative_game_theory">cooperative game</a> between two players working towards a <a href="https://en.wikipedia.org/wiki/Goal">quantifiable common goal</a>. We leverage the <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> to develop an end-to-end dialogue system that can simultaneously converse and recommend. Models are first trained to imitate the behavior of human players without considering the task goal itself (supervised training). We then finetune our <a href="https://en.wikipedia.org/wiki/Computer_simulation">models</a> on simulated bot-bot conversations between two paired pre-trained models (bot-play), in order to achieve the dialogue goal. Our experiments show that models finetuned with bot-play learn improved dialogue strategies, reach the dialogue goal more often when paired with a human, and are rated as more consistent by humans compared to <a href="https://en.wikipedia.org/wiki/Computer_simulation">models</a> trained without bot-play. The <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> and code are publicly available through the ParlAI framework.</abstract>
      <url hash="a1a4cc0f">D19-1203</url>
      <attachment hash="292b89fd">D19-1203.Attachment.pdf</attachment>
      <doi>10.18653/v1/D19-1203</doi>
      <bibkey>kang-etal-2019-recommendation</bibkey>
      <pwccode url="https://github.com/facebookresearch/ParlAI" additional="false">facebookresearch/ParlAI</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/redial">ReDial</pwcdataset>
    </paper>
    <paper id="206">
      <title>How to Build User Simulators to Train RL-based Dialog Systems<fixed-case>RL</fixed-case>-based Dialog Systems</title>
      <author><first>Weiyan</first><last>Shi</last></author>
      <author><first>Kun</first><last>Qian</last></author>
      <author><first>Xuewei</first><last>Wang</last></author>
      <author><first>Zhou</first><last>Yu</last></author>
      <pages>1990–2000</pages>
      <abstract>User simulators are essential for training reinforcement learning (RL) based dialog models. The performance of the <a href="https://en.wikipedia.org/wiki/Simulation">simulator</a> directly impacts the <a href="https://en.wikipedia.org/wiki/Non-linear_gameplay">RL policy</a>. However, building a good user simulator that models real user behaviors is challenging. We propose a method of standardizing user simulator building that can be used by the community to compare dialog system quality using the same set of user simulators fairly. We present implementations of six user simulators trained with different dialog planning and generation methods. We then calculate a set of automatic metrics to evaluate the quality of these <a href="https://en.wikipedia.org/wiki/Simulation">simulators</a> both directly and indirectly. We also ask human users to assess the simulators directly and indirectly by rating the simulated dialogs and interacting with the trained systems. This paper presents a comprehensive evaluation framework for user simulator study and provides a better understanding of the pros and cons of different user simulators, as well as their impacts on the trained systems.</abstract>
      <url hash="15198236">D19-1206</url>
      <attachment hash="ee9e8f97">D19-1206.Attachment.pdf</attachment>
      <doi>10.18653/v1/D19-1206</doi>
      <bibkey>shi-etal-2019-build</bibkey>
      <pwccode url="https://github.com/wyshi/user-simulator" additional="false">wyshi/user-simulator</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/multiwoz">MultiWOZ</pwcdataset>
    </paper>
    <paper id="207">
      <title>Low-Rank HOCA : Efficient High-Order Cross-Modal Attention for Video Captioning<fixed-case>HOCA</fixed-case>: Efficient High-Order Cross-Modal Attention for Video Captioning</title>
      <author><first>Tao</first><last>Jin</last></author>
      <author><first>Siyu</first><last>Huang</last></author>
      <author><first>Yingming</first><last>Li</last></author>
      <author><first>Zhongfei</first><last>Zhang</last></author>
      <pages>2001–2011</pages>
      <abstract>This paper addresses the challenging task of video captioning which aims to generate descriptions for <a href="https://en.wikipedia.org/wiki/Video">video data</a>. Recently, the attention-based encoder-decoder structures have been widely used in video captioning. In existing literature, the attention weights are often built from the information of an individual modality, while, the association relationships between multiple modalities are neglected. Motivated by this, we propose a video captioning model with High-Order Cross-Modal Attention (HOCA) where the attention weights are calculated based on the high-order correlation tensor to capture the frame-level cross-modal interaction of different modalities sufficiently. Furthermore, we novelly introduce Low-Rank HOCA which adopts <a href="https://en.wikipedia.org/wiki/Tensor_decomposition">tensor decomposition</a> to reduce the extremely large space requirement of HOCA, leading to a practical and efficient implementation in real-world applications. Experimental results on two benchmark datasets, MSVD and MSR-VTT, show that Low-rank HOCA establishes a new state-of-the-art.</abstract>
      <url hash="c8b4654d">D19-1207</url>
      <attachment hash="139eaccc">D19-1207.Attachment.zip</attachment>
      <doi>10.18653/v1/D19-1207</doi>
      <bibkey>jin-etal-2019-low</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/msvd">MSVD</pwcdataset>
    </paper>
    <paper id="210">
      <title>Unsupervised Discovery of Multimodal Links in Multi-image, Multi-sentence Documents</title>
      <author><first>Jack</first><last>Hessel</last></author>
      <author><first>Lillian</first><last>Lee</last></author>
      <author><first>David</first><last>Mimno</last></author>
      <pages>2034–2045</pages>
      <abstract>Images and text co-occur constantly on the <a href="https://en.wikipedia.org/wiki/World_Wide_Web">web</a>, but explicit links between images and <a href="https://en.wikipedia.org/wiki/Sentence_(linguistics)">sentences</a> (or other intra-document textual units) are often not present. We present <a href="https://en.wikipedia.org/wiki/Algorithm">algorithms</a> that discover image-sentence relationships without relying on explicit multimodal annotation in training. We experiment on seven datasets of varying difficulty, ranging from documents consisting of groups of images captioned post hoc by crowdworkers to naturally-occurring user-generated multimodal documents. We find that a structured training objective based on identifying whether collections of images and sentences co-occur in documents can suffice to predict links between specific sentences and specific images within the same document at test time.</abstract>
      <url hash="c1ad2482">D19-1210</url>
      <attachment hash="a6c3610d">D19-1210.Attachment.zip</attachment>
      <doi>10.18653/v1/D19-1210</doi>
      <bibkey>hessel-etal-2019-unsupervised</bibkey>
      <pwccode url="https://github.com/jmhessel/multi-retrieval" additional="true">jmhessel/multi-retrieval</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/coco">COCO</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/recipeqa">RecipeQA</pwcdataset>
    </paper>
    <paper id="211">
      <title>UR-FUNNY : A Multimodal Language Dataset for Understanding Humor<fixed-case>UR</fixed-case>-<fixed-case>FUNNY</fixed-case>: A Multimodal Language Dataset for Understanding Humor</title>
      <author><first>Md Kamrul</first><last>Hasan</last></author>
      <author><first>Wasifur</first><last>Rahman</last></author>
      <author><first>AmirAli</first><last>Bagher Zadeh</last></author>
      <author><first>Jianyuan</first><last>Zhong</last></author>
      <author><first>Md Iftekhar</first><last>Tanveer</last></author>
      <author><first>Louis-Philippe</first><last>Morency</last></author>
      <author><first>Mohammed (Ehsan)</first><last>Hoque</last></author>
      <pages>2046–2056</pages>
      <abstract>Humor is a unique and creative communicative behavior often displayed during <a href="https://en.wikipedia.org/wiki/Social_relation">social interactions</a>. It is produced in a multimodal manner, through the usage of words (text), gestures (visual) and prosodic cues (acoustic). Understanding <a href="https://en.wikipedia.org/wiki/Humour">humor</a> from these three modalities falls within boundaries of multimodal language ; a recent research trend in <a href="https://en.wikipedia.org/wiki/Natural_language_processing">natural language processing</a> that models <a href="https://en.wikipedia.org/wiki/Natural_language">natural language</a> as it happens in face-to-face communication. Although humor detection is an established research area in <a href="https://en.wikipedia.org/wiki/Neuro-linguistic_programming">NLP</a>, in a multimodal context it has been understudied. This paper presents a diverse multimodal dataset, called UR-FUNNY, to open the door to understanding multimodal language used in expressing humor. The <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> and accompanying studies, present a <a href="https://en.wikipedia.org/wiki/Software_framework">framework</a> in multimodal humor detection for the natural language processing community. UR-FUNNY is publicly available for research.</abstract>
      <url hash="e2d88155">D19-1211</url>
      <doi>10.18653/v1/D19-1211</doi>
      <bibkey>hasan-etal-2019-ur</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/ur-funny">UR-FUNNY</pwcdataset>
    </paper>
    <paper id="214">
      <title>A Stack-Propagation Framework with Token-Level Intent Detection for Spoken Language Understanding</title>
      <author><first>Libo</first><last>Qin</last></author>
      <author><first>Wanxiang</first><last>Che</last></author>
      <author><first>Yangming</first><last>Li</last></author>
      <author><first>Haoyang</first><last>Wen</last></author>
      <author><first>Ting</first><last>Liu</last></author>
      <pages>2078–2087</pages>
      <abstract>Intent detection and slot filling are two main tasks for building a spoken language understanding (SLU) system. The two <a href="https://en.wikipedia.org/wiki/Task_(project_management)">tasks</a> are closely tied and the slots often highly depend on the intent. In this paper, we propose a novel <a href="https://en.wikipedia.org/wiki/Conceptual_framework">framework</a> for SLU to better incorporate the intent information, which further guiding the slot filling. In our framework, we adopt a joint model with Stack-Propagation which can directly use the intent information as input for slot filling, thus to capture the intent semantic knowledge. In addition, to further alleviate the <a href="https://en.wikipedia.org/wiki/Error_propagation">error propagation</a>, we perform the token-level intent detection for the Stack-Propagation framework. Experiments on two publicly datasets show that our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> achieves the state-of-the-art performance and outperforms other previous methods by a large margin. Finally, we use the Bidirectional Encoder Representation from Transformer (BERT) model in our framework, which further boost our performance in SLU task.</abstract>
      <url hash="b5d8c7bd">D19-1214</url>
      <doi>10.18653/v1/D19-1214</doi>
      <bibkey>qin-etal-2019-stack</bibkey>
      <pwccode url="https://github.com/LeePleased/StackPropagation-SLU" additional="true">LeePleased/StackPropagation-SLU</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/atis">ATIS</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snips">SNIPS</pwcdataset>
    </paper>
    <paper id="215">
      <title>Talk2Car : Taking Control of Your Self-Driving Car<fixed-case>T</fixed-case>alk2<fixed-case>C</fixed-case>ar: Taking Control of Your Self-Driving Car</title>
      <author><first>Thierry</first><last>Deruyttere</last></author>
      <author><first>Simon</first><last>Vandenhende</last></author>
      <author><first>Dusan</first><last>Grujicic</last></author>
      <author><first>Luc</first><last>Van Gool</last></author>
      <author><first>Marie-Francine</first><last>Moens</last></author>
      <pages>2088–2098</pages>
      <abstract>A long-term goal of <a href="https://en.wikipedia.org/wiki/Artificial_intelligence">artificial intelligence</a> is to have an agent execute commands communicated through <a href="https://en.wikipedia.org/wiki/Natural_language_processing">natural language</a>. In many cases the <a href="https://en.wikipedia.org/wiki/Command_(computing)">commands</a> are grounded in a visual environment shared by the human who gives the command and the agent. Execution of the command then requires mapping the <a href="https://en.wikipedia.org/wiki/Command_(computing)">command</a> into the physical visual space, after which the appropriate action can be taken. In this paper we consider the former. Or more specifically, we consider the problem in an autonomous driving setting, where a passenger requests an action that can be associated with an object found in a street scene. Our work presents the Talk2Car dataset, which is the first object referral dataset that contains commands written in <a href="https://en.wikipedia.org/wiki/Natural_language_processing">natural language</a> for <a href="https://en.wikipedia.org/wiki/Self-driving_car">self-driving cars</a>. We provide a detailed comparison with related datasets such as ReferIt, RefCOCO, RefCOCO+, RefCOCOg, Cityscape-Ref and CLEVR-Ref. Additionally, we include a performance analysis using strong <a href="https://en.wikipedia.org/wiki/State-of-the-art">state-of-the-art models</a>. The results show that the proposed object referral task is a challenging one for which the models show promising results but still require additional research in <a href="https://en.wikipedia.org/wiki/Natural_language_processing">natural language processing</a>, <a href="https://en.wikipedia.org/wiki/Computer_vision">computer vision</a> and the intersection of these fields. The <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> can be found on our website : http://macchina-ai.eu/</abstract>
      <url hash="4e2cdcd9">D19-1215</url>
      <attachment hash="be28d61a">D19-1215.Attachment.pdf</attachment>
      <doi>10.18653/v1/D19-1215</doi>
      <bibkey>deruyttere-etal-2019-talk2car</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/talk2car">Talk2Car</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/coco">COCO</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/cityscapes">Cityscapes</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/refcoco">RefCOCO</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/nuscenes">nuScenes</pwcdataset>
    </paper>
    <paper id="216">
      <title>Fact-Checking Meets Fauxtography : Verifying Claims About Images</title>
      <author><first>Dimitrina</first><last>Zlatkova</last></author>
      <author><first>Preslav</first><last>Nakov</last></author>
      <author><first>Ivan</first><last>Koychev</last></author>
      <pages>2099–2108</pages>
      <abstract>The recent explosion of false claims in <a href="https://en.wikipedia.org/wiki/Social_media">social media</a> and on the <a href="https://en.wikipedia.org/wiki/World_Wide_Web">Web</a> in general has given rise to a lot of manual fact-checking initiatives. Unfortunately, the number of claims that need to be fact-checked is several orders of magnitude larger than what humans can handle manually. Thus, there has been a lot of research aiming at automating the process. Interestingly, previous work has largely ignored the growing number of claims about <a href="https://en.wikipedia.org/wiki/Image">images</a>. This is despite the fact that <a href="https://en.wikipedia.org/wiki/Visual_imagery">visual imagery</a> is more influential than <a href="https://en.wikipedia.org/wiki/Writing">text</a> and naturally appears alongside <a href="https://en.wikipedia.org/wiki/Fake_news">fake news</a>. Here we aim at bridging this gap. In particular, we create a new <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> for this problem, and we explore a variety of <a href="https://en.wikipedia.org/wiki/Feature_(machine_learning)">features</a> modeling the claim, the <a href="https://en.wikipedia.org/wiki/Image">image</a>, and the relationship between the claim and the image. The evaluation results show sizable improvements over the <a href="https://en.wikipedia.org/wiki/Baseline_(configuration_management)">baseline</a>. We release our <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a>, hoping to enable further research on fact-checking claims about <a href="https://en.wikipedia.org/wiki/Digital_image">images</a>.</abstract>
      <url hash="e6fe48c9">D19-1216</url>
      <doi>10.18653/v1/D19-1216</doi>
      <bibkey>zlatkova-etal-2019-fact</bibkey>
      <pwccode url="https://gitlab.com/didizlatkova/fake-image-detection" additional="false">didizlatkova/fake-image-detection</pwccode>
    </paper>
    <paper id="217">
      <title>Video Dialog via Progressive Inference and Cross-Transformer</title>
      <author><first>Weike</first><last>Jin</last></author>
      <author><first>Zhou</first><last>Zhao</last></author>
      <author><first>Mao</first><last>Gu</last></author>
      <author><first>Jun</first><last>Xiao</last></author>
      <author><first>Furu</first><last>Wei</last></author>
      <author><first>Yueting</first><last>Zhuang</last></author>
      <pages>2109–2118</pages>
      <abstract>Video dialog is a new and challenging task, which requires the agent to answer questions combining video information with dialog history. And different from single-turn video question answering, the additional dialog history is important for video dialog, which often includes <a href="https://en.wikipedia.org/wiki/Context_(language_use)">contextual information</a> for the question. Existing visual dialog methods mainly use <a href="https://en.wikipedia.org/wiki/Radio-frequency_identification">RNN</a> to encode the dialog history as a single vector representation, which might be rough and straightforward. Some more advanced methods utilize hierarchical structure, attention and memory mechanisms, which still lack an explicit reasoning process. In this paper, we introduce a novel progressive inference mechanism for video dialog, which progressively updates query information based on dialog history and video content until the agent think the information is sufficient and unambiguous. In order to tackle the multi-modal fusion problem, we propose a cross-transformer module, which could learn more fine-grained and comprehensive interactions both inside and between the modalities. And besides answer generation, we also consider question generation, which is more challenging but significant for a complete video dialog system. We evaluate our <a href="https://en.wikipedia.org/wiki/Methodology">method</a> on two large-scale datasets, and the extensive experiments show the effectiveness of our <a href="https://en.wikipedia.org/wiki/Methodology">method</a>.</abstract>
      <url hash="d9404426">D19-1217</url>
      <doi>10.18653/v1/D19-1217</doi>
      <bibkey>jin-etal-2019-video</bibkey>
    </paper>
    <paper id="219">
      <title>Fusion of Detected Objects in Text for Visual Question Answering</title>
      <author><first>Chris</first><last>Alberti</last></author>
      <author><first>Jeffrey</first><last>Ling</last></author>
      <author><first>Michael</first><last>Collins</last></author>
      <author><first>David</first><last>Reitter</last></author>
      <pages>2131–2140</pages>
      <abstract>To advance models of multimodal context, we introduce a simple yet powerful neural architecture for <a href="https://en.wikipedia.org/wiki/Data">data</a> that combines <a href="https://en.wikipedia.org/wiki/Computer_vision">vision</a> and <a href="https://en.wikipedia.org/wiki/Natural_language_processing">natural language</a>. The Bounding Boxes in Text Transformer (B2T2) also leverages referential information binding words to portions of the image in a single unified architecture. B2T2 is highly effective on the Visual Commonsense Reasoning benchmark, achieving a new state-of-the-art with a 25 % relative reduction in error rate compared to published baselines and obtaining the best performance to date on the public leaderboard (as of May 22, 2019). A detailed ablation analysis shows that the early integration of the visual features into the text analysis is key to the effectiveness of the new <a href="https://en.wikipedia.org/wiki/Architecture">architecture</a>. A reference implementation of our <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> is provided.</abstract>
      <url hash="4529a080">D19-1219</url>
      <doi>10.18653/v1/D19-1219</doi>
      <bibkey>alberti-etal-2019-fusion</bibkey>
      <pwccode url="https://github.com/google-research/language" additional="false">google-research/language</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/coco">COCO</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/conceptual-captions">Conceptual Captions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/vcr">VCR</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/visual-question-answering">Visual Question Answering</pwcdataset>
    </paper>
    <paper id="222">
      <title>To Annotate or Not? Predicting Performance Drop under Domain Shift</title>
      <author><first>Hady</first><last>Elsahar</last></author>
      <author><first>Matthias</first><last>Gallé</last></author>
      <pages>2163–2173</pages>
      <abstract>Performance drop due to domain-shift is an endemic problem for <a href="https://en.wikipedia.org/wiki/Natural_language_processing">NLP models</a> in production. This problem creates an urge to continuously annotate evaluation datasets to measure the expected drop in the <a href="https://en.wikipedia.org/wiki/Statistical_model">model</a> performance which can be prohibitively expensive and slow. In this paper, we study the problem of predicting the performance drop of modern NLP models under domain-shift, in the absence of any target domain labels. We investigate three families of methods (-divergence, reverse classification accuracy and confidence measures), show how they can be used to predict the performance drop and study their robustness to adversarial domain-shifts. Our results on sentiment classification and sequence labelling show that our method is able to predict performance drops with an <a href="https://en.wikipedia.org/wiki/Error_rate">error rate</a> as low as 2.15 % and 0.89 % for <a href="https://en.wikipedia.org/wiki/Sentiment_analysis">sentiment analysis</a> and <a href="https://en.wikipedia.org/wiki/Tag_(metadata)">POS tagging</a> respectively.<tex-math>\mathcal{H}</tex-math>-divergence, reverse classification accuracy and confidence measures), show how they can be used to predict the performance drop and study their robustness to adversarial domain-shifts. Our results on sentiment classification and sequence labelling show that our method is able to predict performance drops with an error rate as low as 2.15% and 0.89% for sentiment analysis and POS tagging respectively.</abstract>
      <url hash="a0113224">D19-1222</url>
      <attachment hash="8a1f0cf4">D19-1222.Attachment.pdf</attachment>
      <doi>10.18653/v1/D19-1222</doi>
      <bibkey>elsahar-galle-2019-annotate</bibkey>
    </paper>
    <paper id="225">
      <title>A Deep Factorization of Style and Structure in Fonts</title>
      <author><first>Nikita</first><last>Srivatsan</last></author>
      <author><first>Jonathan</first><last>Barron</last></author>
      <author><first>Dan</first><last>Klein</last></author>
      <author><first>Taylor</first><last>Berg-Kirkpatrick</last></author>
      <pages>2195–2205</pages>
      <abstract>We propose a deep factorization model for typographic analysis that disentangles content from style. Specifically, a variational inference procedure factors each training glyph into the combination of a character-specific content embedding and a latent font-specific style variable. The underlying <a href="https://en.wikipedia.org/wiki/Generative_model">generative model</a> combines these factors through an asymmetric transpose convolutional process to generate the image of the glyph itself. When trained on corpora of fonts, our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> learns a manifold over font styles that can be used to analyze or reconstruct new, unseen fonts. On the task of reconstructing missing glyphs from an unknown font given only a small number of observations, our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> outperforms both a strong nearest neighbors baseline and a state-of-the-art discriminative model from prior work.</abstract>
      <url hash="872e9d97">D19-1225</url>
      <doi>10.18653/v1/D19-1225</doi>
      <revision id="1" href="D19-1225v1" hash="301b4c96" />
      <revision id="2" href="D19-1225v2" hash="872e9d97" date="2020-07-07">Name of first author updated to "Nikita Srivatsan".</revision>
      <bibkey>srivatsan-etal-2019-deep</bibkey>
      <pwccode url="https://bitbucket.org/NikitaSrivatsan/DeepFactorizationFontsEMNLP19" additional="false">NikitaSrivatsan/DeepFactorizationFontsEMNLP19</pwccode>
    </paper>
    <paper id="226">
      <title>Cross-lingual Semantic Specialization via Lexical Relation Induction</title>
      <author><first>Edoardo Maria</first><last>Ponti</last></author>
      <author><first>Ivan</first><last>Vulić</last></author>
      <author><first>Goran</first><last>Glavaš</last></author>
      <author><first>Roi</first><last>Reichart</last></author>
      <author><first>Anna</first><last>Korhonen</last></author>
      <pages>2206–2217</pages>
      <abstract>Semantic specialization integrates structured linguistic knowledge from external resources (such as lexical relations in WordNet) into pretrained distributional vectors in the form of <a href="https://en.wikipedia.org/wiki/Constraint_(mathematics)">constraints</a>. However, this technique can not be leveraged in many languages, because their structured external resources are typically incomplete or non-existent. To bridge this gap, we propose a novel method that transfers specialization from a resource-rich source language (English) to virtually any target language. Our specialization transfer comprises two crucial steps : 1) Inducing noisy constraints in the target language through automatic word translation ; and 2) Filtering the noisy constraints via a state-of-the-art relation prediction model trained on the source language constraints. This allows us to specialize any set of <a href="https://en.wikipedia.org/wiki/Distribution_(mathematics)">distributional vectors</a> in the target language with the refined <a href="https://en.wikipedia.org/wiki/Constraint_(mathematics)">constraints</a>. We prove the effectiveness of our method through intrinsic word similarity evaluation in 8 languages, and with 3 downstream tasks in 5 languages : lexical simplification, dialog state tracking, and semantic textual similarity. The gains over the previous state-of-art specialization methods are substantial and consistent across languages. Our results also suggest that the transfer method is effective even for lexically distant source-target language pairs. Finally, as a by-product, our <a href="https://en.wikipedia.org/wiki/Method_(computer_programming)">method</a> produces lists of WordNet-style lexical relations in resource-poor languages.</abstract>
      <url hash="5be5fd24">D19-1226</url>
      <doi>10.18653/v1/D19-1226</doi>
      <bibkey>ponti-etal-2019-cross</bibkey>
    </paper>
    <paper id="227">
      <title>Modelling the interplay of <a href="https://en.wikipedia.org/wiki/Metaphor">metaphor</a> and <a href="https://en.wikipedia.org/wiki/Emotion">emotion</a> through <a href="https://en.wikipedia.org/wiki/Multitask_learning">multitask learning</a></title>
      <author><first>Verna</first><last>Dankers</last></author>
      <author><first>Marek</first><last>Rei</last></author>
      <author><first>Martha</first><last>Lewis</last></author>
      <author><first>Ekaterina</first><last>Shutova</last></author>
      <pages>2218–2229</pages>
      <abstract>Metaphors allow us to convey <a href="https://en.wikipedia.org/wiki/Emotion">emotion</a> by connecting <a href="https://en.wikipedia.org/wiki/Experience">physical experiences</a> and <a href="https://en.wikipedia.org/wiki/Abstraction">abstract concepts</a>. The results of previous research in <a href="https://en.wikipedia.org/wiki/Linguistics">linguistics</a> and <a href="https://en.wikipedia.org/wiki/Psychology">psychology</a> suggest that metaphorical phrases tend to be more emotionally evocative than their literal counterparts. In this paper, we investigate the relationship between <a href="https://en.wikipedia.org/wiki/Metaphor">metaphor</a> and <a href="https://en.wikipedia.org/wiki/Emotion">emotion</a> within a <a href="https://en.wikipedia.org/wiki/Software_framework">computational framework</a>, by proposing the first joint model of these <a href="https://en.wikipedia.org/wiki/Phenomenon">phenomena</a>. We experiment with several multitask learning architectures for this purpose, involving both hard and soft parameter sharing. Our results demonstrate that metaphor identification and emotion prediction mutually benefit from joint learning and our models advance the state of the art in both of these tasks.</abstract>
      <url hash="a884b431">D19-1227</url>
      <doi>10.18653/v1/D19-1227</doi>
      <bibkey>dankers-etal-2019-modelling</bibkey>
    </paper>
    <paper id="228">
      <title>How well do NLI models capture verb veridicality?<fixed-case>NLI</fixed-case> models capture verb veridicality?</title>
      <author><first>Alexis</first><last>Ross</last></author>
      <author><first>Ellie</first><last>Pavlick</last></author>
      <pages>2230–2240</pages>
      <abstract>In natural language inference (NLI), contexts are considered veridical if they allow us to infer that their underlying propositions make true claims about the real world. We investigate whether a state-of-the-art natural language inference model (BERT) learns to make correct inferences about <a href="https://en.wikipedia.org/wiki/Veridicality">veridicality</a> in verb-complement constructions. We introduce an NLI dataset for veridicality evaluation consisting of 1,500 sentence pairs, covering 137 unique verbs. We find that both human and model inferences generally follow theoretical patterns, but exhibit a systematic bias towards assuming that verbs are veridicala bias which is amplified in BERT. We further show that, encouragingly, BERT’s inferences are sensitive not only to the presence of individual verb types, but also to the syntactic role of the verb, the form of the <a href="https://en.wikipedia.org/wiki/Complement_clause">complement clause</a> (to- vs. that-complements), and <a href="https://en.wikipedia.org/wiki/Affirmation_and_negation">negation</a>.</abstract>
      <url hash="e6173c13">D19-1228</url>
      <attachment hash="e8b9e982">D19-1228.Attachment.zip</attachment>
      <doi>10.18653/v1/D19-1228</doi>
      <bibkey>ross-pavlick-2019-well</bibkey>
    </paper>
    <paper id="230">
      <title>Negative Focus Detection via Contextual Attention Mechanism</title>
      <author><first>Longxiang</first><last>Shen</last></author>
      <author><first>Bowei</first><last>Zou</last></author>
      <author><first>Yu</first><last>Hong</last></author>
      <author><first>Guodong</first><last>Zhou</last></author>
      <author><first>Qiaoming</first><last>Zhu</last></author>
      <author><first>AiTi</first><last>Aw</last></author>
      <pages>2251–2261</pages>
      <abstract>Negation is a universal but complicated linguistic phenomenon, which has received considerable attention from the NLP community over the last decade, since a negated statement often carries both an explicit negative focus and implicit positive meanings. For the sake of understanding a negated statement, it is critical to precisely detect the negative focus in context. However, how to capture <a href="https://en.wikipedia.org/wiki/Context_(language_use)">contextual information</a> for negative focus detection is still an open challenge. To well address this, we come up with an attention-based neural network to model <a href="https://en.wikipedia.org/wiki/Context_(language_use)">contextual information</a>. In particular, we introduce a framework which consists of a Bidirectional Long Short-Term Memory (BiLSTM) neural network and a Conditional Random Fields (CRF) layer to effectively encode the order information and the long-range context dependency in a sentence. Moreover, we design two types of attention mechanisms, word-level contextual attention and topic-level contextual attention, to take advantage of contextual information across sentences from both the word perspective and the topic perspective, respectively. Experimental results on the SEM’12 shared task corpus show that our approach achieves the best performance on negative focus detection, yielding an absolute improvement of 2.11 % over the <a href="https://en.wikipedia.org/wiki/State_of_the_art">state-of-the-art</a>. This demonstrates the great effectiveness of the two types of contextual attention mechanisms.</abstract>
      <url hash="b7226b2a">D19-1230</url>
      <doi>10.18653/v1/D19-1230</doi>
      <bibkey>shen-etal-2019-negative</bibkey>
    </paper>
    <paper id="231">
      <title>A Unified Neural Coherence Model</title>
      <author><first>Han Cheol</first><last>Moon</last></author>
      <author><first>Tasnim</first><last>Mohiuddin</last></author>
      <author><first>Shafiq</first><last>Joty</last></author>
      <author><first>Chi</first><last>Xu</last></author>
      <pages>2262–2272</pages>
      <abstract>Recently, neural approaches to coherence modeling have achieved state-of-the-art results in several evaluation tasks. However, we show that most of these <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> often fail on harder <a href="https://en.wikipedia.org/wiki/Task_(project_management)">tasks</a> with more realistic application scenarios. In particular, the existing models underperform on tasks that require the <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> to be sensitive to local contexts such as candidate ranking in <a href="https://en.wikipedia.org/wiki/Dialogue">conversational dialogue</a> and in <a href="https://en.wikipedia.org/wiki/Machine_translation">machine translation</a>. In this paper, we propose a unified coherence model that incorporates sentence grammar, inter-sentence coherence relations, and global coherence patterns into a common neural framework. With extensive experiments on local and global discrimination tasks, we demonstrate that our proposed <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> outperforms existing <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> by a good margin, and establish a new <a href="https://en.wikipedia.org/wiki/State_of_the_art">state-of-the-art</a>.</abstract>
      <url hash="e6c3fed0">D19-1231</url>
      <doi>10.18653/v1/D19-1231</doi>
      <bibkey>moon-etal-2019-unified</bibkey>
    </paper>
    <paper id="233">
      <title>Neural Generative Rhetorical Structure Parsing</title>
      <author><first>Amandla</first><last>Mabona</last></author>
      <author><first>Laura</first><last>Rimell</last></author>
      <author><first>Stephen</first><last>Clark</last></author>
      <author><first>Andreas</first><last>Vlachos</last></author>
      <pages>2284–2295</pages>
      <abstract>Rhetorical structure trees have been shown to be useful for several document-level tasks including <a href="https://en.wikipedia.org/wiki/Automatic_summarization">summarization</a> and <a href="https://en.wikipedia.org/wiki/Document_classification">document classification</a>. Previous approaches to RST parsing have used <a href="https://en.wikipedia.org/wiki/Discriminative_model">discriminative models</a> ; however, these are less sample efficient than <a href="https://en.wikipedia.org/wiki/Generative_model">generative models</a>, and RST parsing datasets are typically small. In this paper, we present the first <a href="https://en.wikipedia.org/wiki/Generative_model">generative model</a> for RST parsing. Our <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a> is a document-level RNN grammar (RNNG) with a <a href="https://en.wikipedia.org/wiki/Top-down_and_bottom-up_design">bottom-up traversal order</a>. We show that, for our parser’s traversal order, previous beam search algorithms for RNNGs have a left-branching bias which is ill-suited for RST parsing. We develop a novel beam search algorithm that keeps track of both structure-and word-generating actions without exhibit-ing this branching bias and results in absolute improvements of 6.8 and 2.9 on unlabelled and labelled F1 over previous <a href="https://en.wikipedia.org/wiki/Algorithm">algorithms</a>. Overall, our <a href="https://en.wikipedia.org/wiki/Generative_model">generative model</a> outperforms a <a href="https://en.wikipedia.org/wiki/Discriminative_model">discriminative model</a> with the same <a href="https://en.wikipedia.org/wiki/Feature_(machine_learning)">features</a> by 2.6 <a href="https://en.wikipedia.org/wiki/F-number">F1points</a> and achieves performance comparable to the state-of-the-art, outperforming all published parsers from a recent replication study that do not use additional training data</abstract>
      <url hash="7a28535b">D19-1233</url>
      <doi>10.18653/v1/D19-1233</doi>
      <bibkey>mabona-etal-2019-neural</bibkey>
    </paper>
    <paper id="234">
      <title>Weak Supervision for Learning Discourse Structure</title>
      <author><first>Sonia</first><last>Badene</last></author>
      <author><first>Kate</first><last>Thompson</last></author>
      <author><first>Jean-Pierre</first><last>Lorré</last></author>
      <author><first>Nicholas</first><last>Asher</last></author>
      <pages>2296–2305</pages>
      <abstract>This paper provides a detailed comparison of a data programming approach with (i) off-the-shelf, state-of-the-art deep learning architectures that optimize their representations (BERT) and (ii) handcrafted-feature approaches previously used in the discourse analysis literature. We compare these approaches on the task of learning discourse structure for <a href="https://en.wikipedia.org/wiki/Multi-party_system">multi-party dialogue</a>. The data programming paradigm offered by the Snorkel framework allows a user to label training data using expert-composed heuristics, which are then transformed via the generative step into probability distributions of the class labels given the data. We show that on our task the <a href="https://en.wikipedia.org/wiki/Generative_model">generative model</a> outperforms both deep learning architectures as well as more traditional ML approaches when learning discourse structureit even outperforms the combination of deep learning methods and hand-crafted features. We also implement several strategies for decoding our generative model output in order to improve our results. We conclude that weak supervision methods hold great promise as a means for creating and improving data sets for discourse structure.</abstract>
      <url hash="8c619117">D19-1234</url>
      <doi>10.18653/v1/D19-1234</doi>
      <bibkey>badene-etal-2019-weak</bibkey>
    </paper>
    <paper id="235">
      <title>Predicting Discourse Structure using Distant Supervision from Sentiment</title>
      <author><first>Patrick</first><last>Huber</last></author>
      <author><first>Giuseppe</first><last>Carenini</last></author>
      <pages>2306–2316</pages>
      <abstract>Discourse parsing could not yet take full advantage of the neural NLP revolution, mostly due to the lack of annotated datasets. We propose a novel approach that uses distant supervision on an auxiliary task (sentiment classification), to generate abundant data for RST-style discourse structure prediction. Our approach combines a neural variant of multiple-instance learning, using document-level supervision, with an optimal CKY-style tree generation algorithm. In a series of experiments, we train a discourse parser (for only structure prediction) on our automatically generated dataset and compare it with parsers trained on human-annotated corpora (news domain RST-DT and Instructional domain). Results indicate that while our <a href="https://en.wikipedia.org/wiki/Parsing">parser</a> does not yet match the performance of a <a href="https://en.wikipedia.org/wiki/Parsing">parser</a> trained and tested on the same dataset (intra-domain), it does perform remarkably well on the much more difficult and arguably more useful task of inter-domain discourse structure prediction, where the <a href="https://en.wikipedia.org/wiki/Parsing">parser</a> is trained on one domain and tested / applied on another one.</abstract>
      <url hash="0b4be029">D19-1235</url>
      <doi>10.18653/v1/D19-1235</doi>
      <bibkey>huber-carenini-2019-predicting</bibkey>
    </paper>
    <paper id="239">
      <title>Training Data Augmentation for Detecting Adverse Drug Reactions in User-Generated Content</title>
      <author><first>Sepideh</first><last>Mesbah</last></author>
      <author><first>Jie</first><last>Yang</last></author>
      <author><first>Robert-Jan</first><last>Sips</last></author>
      <author><first>Manuel</first><last>Valle Torre</last></author>
      <author><first>Christoph</first><last>Lofi</last></author>
      <author><first>Alessandro</first><last>Bozzon</last></author>
      <author><first>Geert-Jan</first><last>Houben</last></author>
      <pages>2349–2359</pages>
      <abstract>Social media provides a timely yet challenging data source for <a href="https://en.wikipedia.org/wiki/Adverse_drug_reaction">adverse drug reaction (ADR) detection</a>. Existing dictionary-based, semi-supervised learning approaches are intrinsically limited by the coverage and maintainability of laymen health vocabularies. In this paper, we introduce a data augmentation approach that leverages variational autoencoders to learn high-quality data distributions from a large unlabeled dataset, and subsequently, to automatically generate a large labeled training set from a small set of labeled samples. This allows for efficient social-media ADR detection with low training and re-training costs to adapt to the changes and emergence of informal medical laymen terms. An extensive evaluation performed on Twitter and Reddit data shows that our approach matches the performance of fully-supervised approaches while requiring only 25 % of training data.</abstract>
      <url hash="967a3e7b">D19-1239</url>
      <doi>10.18653/v1/D19-1239</doi>
      <bibkey>mesbah-etal-2019-training</bibkey>
    </paper>
    <paper id="242">
      <title>PullNet : Open Domain Question Answering with Iterative Retrieval on Knowledge Bases and Text<fixed-case>P</fixed-case>ull<fixed-case>N</fixed-case>et: Open Domain Question Answering with Iterative Retrieval on Knowledge Bases and Text</title>
      <author><first>Haitian</first><last>Sun</last></author>
      <author><first>Tania</first><last>Bedrax-Weiss</last></author>
      <author><first>William</first><last>Cohen</last></author>
      <pages>2380–2390</pages>
      <abstract>We consider open-domain question answering (QA) where answers are drawn from either a <a href="https://en.wikipedia.org/wiki/Text_corpus">corpus</a>, a knowledge base (KB), or a combination of both of these. We focus on a setting in which a <a href="https://en.wikipedia.org/wiki/Text_corpus">corpus</a> is supplemented with a large but incomplete KB, and on questions that require non-trivial (e.g., multi-hop) reasoning. We describe PullNet, an integrated framework for (1) learning what to retrieve and (2) reasoning with this heterogeneous information to find the best answer. PullNet uses an <a href="https://en.wikipedia.org/wiki/Iterative_and_incremental_development">iterative process</a> to construct a question-specific subgraph that contains information relevant to the question. In each iteration, a graph convolutional network (graph CNN) is used to identify subgraph nodes that should be expanded using retrieval (or pull) operations on the corpus and/or KB. After the <a href="https://en.wikipedia.org/wiki/Glossary_of_graph_theory_terms">subgraph</a> is complete, another graph CNN is used to extract the answer from the <a href="https://en.wikipedia.org/wiki/Glossary_of_graph_theory_terms">subgraph</a>. This retrieve-and-reason process allows us to answer multi-hop questions using large KBs and <a href="https://en.wikipedia.org/wiki/Corpus_linguistics">corpora</a>. PullNet is weakly supervised, requiring question-answer pairs but not gold inference paths. Experimentally PullNet improves over the prior state-of-the art, and in the setting where a <a href="https://en.wikipedia.org/wiki/Text_corpus">corpus</a> is used with incomplete KB these improvements are often dramatic. PullNet is also often superior to prior systems in a KB-only setting or a <a href="https://en.wikipedia.org/wiki/Text-based_user_interface">text-only setting</a>.<fixed-case>iterative</fixed-case> process to construct a question-specific subgraph that contains information relevant to the question. In each iteration, a graph convolutional network (graph CNN) is used to identify subgraph nodes that should be expanded using retrieval (or “pull”) operations on the corpus and/or KB. After the subgraph is complete, another graph CNN is used to extract the answer from the subgraph. This retrieve-and-reason process allows us to answer multi-hop questions using large KBs and corpora. PullNet is weakly supervised, requiring question-answer pairs but not gold inference paths. Experimentally PullNet improves over the prior state-of-the art, and in the setting where a corpus is used with incomplete KB these improvements are often dramatic. PullNet is also often superior to prior systems in a KB-only setting or a text-only setting.</abstract>
      <url hash="cd514d9e">D19-1242</url>
      <doi>10.18653/v1/D19-1242</doi>
      <bibkey>sun-etal-2019-pullnet</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/complexwebquestions">ComplexWebQuestions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/metaqa">MetaQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikimovies">WikiMovies</pwcdataset>
    </paper>
    <paper id="246">
      <title>A Non-commutative Bilinear Model for Answering Path Queries in Knowledge Graphs</title>
      <author><first>Katsuhiko</first><last>Hayashi</last></author>
      <author><first>Masashi</first><last>Shimbo</last></author>
      <pages>2422–2430</pages>
      <abstract>Bilinear diagonal models for knowledge graph embedding (KGE), such as DistMult and ComplEx, balance expressiveness and computational efficiency by representing relations as diagonal matrices. Although they perform well in predicting atomic relations, composite relations (relation paths) can not be modeled naturally by the product of relation matrices, as the product of diagonal matrices is commutative and hence invariant with the order of relations. In this paper, we propose a new bilinear KGE model, called BlockHolE, based on block circulant matrices. In BlockHolE, <a href="https://en.wikipedia.org/wiki/Matrix_(mathematics)">relation matrices</a> can be non-commutative, allowing composite relations to be modeled by <a href="https://en.wikipedia.org/wiki/Matrix_multiplication">matrix product</a>. The <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> is parameterized in a way that covers a spectrum ranging from diagonal to full relation matrices. A fast computation technique can be developed on the basis of the duality of the Fourier transform of circulant matrices.</abstract>
      <url hash="81bf74f2">D19-1246</url>
      <doi>10.18653/v1/D19-1246</doi>
      <bibkey>hayashi-shimbo-2019-non</bibkey>
    </paper>
    <paper id="247">
      <title>Generating Questions for Knowledge Bases via Incorporating Diversified Contexts and Answer-Aware Loss</title>
      <author><first>Cao</first><last>Liu</last></author>
      <author><first>Kang</first><last>Liu</last></author>
      <author><first>Shizhu</first><last>He</last></author>
      <author><first>Zaiqing</first><last>Nie</last></author>
      <author><first>Jun</first><last>Zhao</last></author>
      <pages>2431–2441</pages>
      <abstract>We tackle the task of question generation over <a href="https://en.wikipedia.org/wiki/Knowledge_base">knowledge bases</a>. Conventional methods for this task neglect two crucial research issues : 1) the given predicate needs to be expressed ; 2) the answer to the generated question needs to be definitive. In this paper, we strive toward the above two issues via incorporating diversified contexts and answer-aware loss. Specifically, we propose a neural encoder-decoder model with multi-level copy mechanisms to generate such questions. Furthermore, the answer aware loss is introduced to make generated questions corresponding to more definitive answers. Experiments demonstrate that our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> achieves state-of-the-art performance. Meanwhile, such generated question is able to express the given predicate and correspond to a definitive answer.</abstract>
      <url hash="9ad3ff59">D19-1247</url>
      <doi>10.18653/v1/D19-1247</doi>
      <bibkey>liu-etal-2019-generating</bibkey>
    </paper>
    <paper id="248">
      <title>Multi-Task Learning for Conversational Question Answering over a Large-Scale Knowledge Base</title>
      <author><first>Tao</first><last>Shen</last></author>
      <author><first>Xiubo</first><last>Geng</last></author>
      <author><first>Tao</first><last>Qin</last></author>
      <author><first>Daya</first><last>Guo</last></author>
      <author><first>Duyu</first><last>Tang</last></author>
      <author><first>Nan</first><last>Duan</last></author>
      <author><first>Guodong</first><last>Long</last></author>
      <author><first>Daxin</first><last>Jiang</last></author>
      <pages>2442–2451</pages>
      <abstract>We consider the problem of conversational question answering over a large-scale knowledge base. To handle huge entity vocabulary of a large-scale knowledge base, recent neural semantic parsing based approaches usually decompose the task into several subtasks and then solve them sequentially, which leads to following issues : 1) errors in earlier subtasks will be propagated and negatively affect downstream ones ; and 2) each subtask can not naturally share supervision signals with others. To tackle these issues, we propose an innovative multi-task learning framework where a pointer-equipped semantic parsing model is designed to resolve coreference in conversations, and naturally empower joint learning with a novel type-aware entity detection model. The proposed <a href="https://en.wikipedia.org/wiki/Software_framework">framework</a> thus enables shared supervisions and alleviates the effect of <a href="https://en.wikipedia.org/wiki/Error_propagation">error propagation</a>. Experiments on a large-scale conversational question answering dataset containing 1.6 M question answering pairs over 12.8 M entities show that the proposed <a href="https://en.wikipedia.org/wiki/Conceptual_framework">framework</a> improves overall F1 score from 67 % to 79 % compared with previous state-of-the-art work.</abstract>
      <url hash="b286747b">D19-1248</url>
      <attachment hash="c529a902">D19-1248.Attachment.pdf</attachment>
      <doi>10.18653/v1/D19-1248</doi>
      <bibkey>shen-etal-2019-multi</bibkey>
      <pwccode url="https://github.com/taoshen58/MaSP" additional="false">taoshen58/MaSP</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/csqa">CSQA</pwcdataset>
    </paper>
    <paper id="249">
      <title>BiPaR : A Bilingual Parallel Dataset for Multilingual and Cross-lingual Reading Comprehension on Novels<fixed-case>B</fixed-case>i<fixed-case>P</fixed-case>a<fixed-case>R</fixed-case>: A Bilingual Parallel Dataset for Multilingual and Cross-lingual Reading Comprehension on Novels</title>
      <author><first>Yimin</first><last>Jing</last></author>
      <author><first>Deyi</first><last>Xiong</last></author>
      <author><first>Zhen</first><last>Yan</last></author>
      <pages>2452–2462</pages>
      <abstract>This paper presents BiPaR, a bilingual parallel novel-style machine reading comprehension (MRC) dataset, developed to support multilingual and cross-lingual reading comprehension. The biggest difference between BiPaR and existing reading comprehension datasets is that each triple (Passage, Question, Answer) in BiPaR is written parallelly in two languages. We collect 3,667 bilingual parallel paragraphs from Chinese and English novels, from which we construct 14,668 parallel question-answer pairs via crowdsourced workers following a strict quality control procedure. We analyze BiPaR in depth and find that BiPaR offers good diversification in prefixes of questions, answer types and relationships between questions and passages. We also observe that answering questions of novels requires reading comprehension skills of <a href="https://en.wikipedia.org/wiki/Coreference_resolution">coreference resolution</a>, multi-sentence reasoning, and understanding of implicit causality, etc. With BiPaR, we build monolingual, multilingual, and cross-lingual MRC baseline models. Even for the relatively simple monolingual MRC on this dataset, experiments show that a strong BERT baseline is over 30 points behind human in terms of both EM and F1 score, indicating that BiPaR provides a challenging testbed for monolingual, multilingual and cross-lingual MRC on novels. The <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> is available at https://multinlp.github.io/BiPaR/.</abstract>
      <url hash="604c1179">D19-1249</url>
      <doi>10.18653/v1/D19-1249</doi>
      <bibkey>jing-etal-2019-bipar</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/bipar">BiPaR</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/coqa">CoQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/hotpotqa">HotpotQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
    </paper>
    <paper id="250">
      <title>Language Models as Knowledge Bases?</title>
      <author><first>Fabio</first><last>Petroni</last></author>
      <author><first>Tim</first><last>Rocktäschel</last></author>
      <author><first>Sebastian</first><last>Riedel</last></author>
      <author><first>Patrick</first><last>Lewis</last></author>
      <author><first>Anton</first><last>Bakhtin</last></author>
      <author><first>Yuxiang</first><last>Wu</last></author>
      <author><first>Alexander</first><last>Miller</last></author>
      <pages>2463–2473</pages>
      <abstract>Recent progress in pretraining language models on large textual corpora led to a surge of improvements for downstream NLP tasks. Whilst learning linguistic knowledge, these models may also be storing relational knowledge present in the training data, and may be able to answer queries structured as fill-in-the-blank cloze statements. Language models have many advantages over structured knowledge bases : they require no schema engineering, allow practitioners to query about an open class of relations, are easy to extend to more data, and require no human supervision to train. We present an in-depth analysis of the relational knowledge already present (without fine-tuning) in a wide range of state-of-the-art pretrained language models. We find that (i) without fine-tuning, BERT contains relational knowledge competitive with traditional NLP methods that have some access to oracle knowledge, (ii) BERT also does remarkably well on open-domain question answering against a supervised baseline, and (iii) certain types of factual knowledge are learned much more readily than others by standard language model pretraining approaches. The surprisingly strong ability of these <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> to recall factual knowledge without any <a href="https://en.wikipedia.org/wiki/Fine-tuning">fine-tuning</a> demonstrates their potential as <a href="https://en.wikipedia.org/wiki/Unsupervised_learning">unsupervised open-domain QA systems</a>. The code to reproduce our analysis is available at https://github.com/facebookresearch/LAMA.</abstract>
      <url hash="1226a776">D19-1250</url>
      <doi>10.18653/v1/D19-1250</doi>
      <bibkey>petroni-etal-2019-language</bibkey>
      <pwccode url="https://github.com/facebookresearch/LAMA" additional="false">facebookresearch/LAMA</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/lama">LAMA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/bookcorpus">BookCorpus</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/natural-questions">Natural Questions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/t-rex">T-REx</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikitext-103">WikiText-103</pwcdataset>
    </paper>
    <paper id="251">
      <title>NumNet : Machine Reading Comprehension with <a href="https://en.wikipedia.org/wiki/Numerical_analysis">Numerical Reasoning</a><fixed-case>N</fixed-case>um<fixed-case>N</fixed-case>et: Machine Reading Comprehension with Numerical Reasoning</title>
      <author><first>Qiu</first><last>Ran</last></author>
      <author><first>Yankai</first><last>Lin</last></author>
      <author><first>Peng</first><last>Li</last></author>
      <author><first>Jie</first><last>Zhou</last></author>
      <author><first>Zhiyuan</first><last>Liu</last></author>
      <pages>2474–2484</pages>
      <abstract>Numerical reasoning, such as <a href="https://en.wikipedia.org/wiki/Addition">addition</a>, <a href="https://en.wikipedia.org/wiki/Subtraction">subtraction</a>, <a href="https://en.wikipedia.org/wiki/Sorting">sorting</a> and <a href="https://en.wikipedia.org/wiki/Counting">counting</a> is a critical skill in human’s reading comprehension, which has not been well considered in existing machine reading comprehension (MRC) systems. To address this issue, we propose a numerical MRC model named as NumNet, which utilizes a numerically-aware graph neural network to consider the comparing information and performs numerical reasoning over numbers in the question and passage. Our system achieves an EM-score of 64.56 % on the DROP dataset, outperforming all existing <a href="https://en.wikipedia.org/wiki/Machine_learning">machine reading comprehension models</a> by considering the <a href="https://en.wikipedia.org/wiki/Numerical_analysis">numerical relations</a> among numbers.</abstract>
      <url hash="156bb29f">D19-1251</url>
      <attachment hash="9ba59542">D19-1251.Attachment.pdf</attachment>
      <doi>10.18653/v1/D19-1251</doi>
      <bibkey>ran-etal-2019-numnet</bibkey>
      <pwccode url="https://github.com/ranqiu92/NumNet" additional="true">ranqiu92/NumNet</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/drop">DROP</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/race">RACE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
    </paper>
    <paper id="253">
      <title>Addressing Semantic Drift in Question Generation for Semi-Supervised Question Answering</title>
      <author><first>Shiyue</first><last>Zhang</last></author>
      <author><first>Mohit</first><last>Bansal</last></author>
      <pages>2495–2509</pages>
      <abstract>Text-based Question Generation (QG) aims at generating natural and relevant questions that can be answered by a given answer in some context. Existing QG models suffer from a semantic drift problem, i.e., the <a href="https://en.wikipedia.org/wiki/Semantics">semantics</a> of the model-generated question drifts away from the given context and answer. In this paper, we first propose two semantics-enhanced rewards obtained from downstream question paraphrasing and question answering tasks to regularize the QG model to generate semantically valid questions. Second, since the traditional evaluation metrics (e.g., BLEU) often fall short in evaluating the quality of generated questions, we propose a QA-based evaluation method which measures the QG model’s ability to mimic human annotators in generating QA training data. Experiments show that our <a href="https://en.wikipedia.org/wiki/Methodology">method</a> achieves the new state-of-the-art performance w.r.t. traditional metrics, and also performs best on our QA-based evaluation metrics. Further, we investigate how to use our QG model to augment QA datasets and enable <a href="https://en.wikipedia.org/wiki/Semi-supervised_learning">semi-supervised QA</a>. We propose two ways to generate synthetic QA pairs : generate new questions from existing articles or collect QA pairs from new articles. We also propose two empirically effective strategies, a data filter and mixing mini-batch training, to properly use the QG-generated data for <a href="https://en.wikipedia.org/wiki/Quality_assurance">QA</a>. Experiments show that our method improves over both BiDAF and BERT QA baselines, even without introducing new articles.</abstract>
      <url hash="9c15afd3">D19-1253</url>
      <doi>10.18653/v1/D19-1253</doi>
      <bibkey>zhang-bansal-2019-addressing</bibkey>
      <pwccode url="https://github.com/ZhangShiyue/QGforQA" additional="false">ZhangShiyue/QGforQA</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
    </paper>
    <paper id="255">
      <title>Incorporating External Knowledge into <a href="https://en.wikipedia.org/wiki/Machine_reading">Machine Reading</a> for Generative Question Answering</title>
      <author><first>Bin</first><last>Bi</last></author>
      <author><first>Chen</first><last>Wu</last></author>
      <author><first>Ming</first><last>Yan</last></author>
      <author><first>Wei</first><last>Wang</last></author>
      <author><first>Jiangnan</first><last>Xia</last></author>
      <author><first>Chenliang</first><last>Li</last></author>
      <pages>2521–2530</pages>
      <abstract>Commonsense and background knowledge is required for a QA model to answer many nontrivial questions. Different from existing work on knowledge-aware QA, we focus on a more challenging task of leveraging external knowledge to generate answers in <a href="https://en.wikipedia.org/wiki/Natural_language">natural language</a> for a given question with context. In this paper, we propose a new neural model, Knowledge-Enriched Answer Generator (KEAG), which is able to compose a natural answer by exploiting and aggregating evidence from all four information sources available : question, passage, vocabulary and knowledge. During the process of <a href="https://en.wikipedia.org/wiki/Question_answering">answer generation</a>, KEAG adaptively determines when to utilize symbolic knowledge and which fact from the knowledge is useful. This allows the <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> to exploit external knowledge that is not explicitly stated in the given text, but that is relevant for generating an answer. The empirical study on public benchmark of answer generation demonstrates that KEAG improves answer quality over <a href="https://en.wikipedia.org/wiki/Conceptual_model">models</a> without knowledge and existing knowledge-aware models, confirming its effectiveness in leveraging <a href="https://en.wikipedia.org/wiki/Knowledge">knowledge</a>.</abstract>
      <url hash="efcce521">D19-1255</url>
      <doi>10.18653/v1/D19-1255</doi>
      <bibkey>bi-etal-2019-incorporating</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/conceptnet">ConceptNet</pwcdataset>
    </paper>
    <paper id="257">
      <title>Discourse-Aware Semantic Self-Attention for Narrative Reading Comprehension</title>
      <author><first>Todor</first><last>Mihaylov</last></author>
      <author><first>Anette</first><last>Frank</last></author>
      <pages>2541–2552</pages>
      <abstract>In this work, we propose to use linguistic annotations as a basis for a Discourse-Aware Semantic Self-Attention encoder that we employ for <a href="https://en.wikipedia.org/wiki/Reading_comprehension">reading comprehension</a> on narrative texts. We extract relations between discourse units, <a href="https://en.wikipedia.org/wiki/Event_(philosophy)">events</a>, and their arguments as well as coreferring mentions, using available annotation tools. Our empirical evaluation shows that the investigated structures improve the overall performance (up to +3.4 Rouge-L), especially intra-sentential and cross-sentential discourse relations, sentence-internal semantic role relations, and long-distance coreference relations. We show that dedicating self-attention heads to intra-sentential relations and relations connecting neighboring sentences is beneficial for finding answers to questions in longer contexts. Our findings encourage the use of discourse-semantic annotations to enhance the generalization capacity of self-attention models for <a href="https://en.wikipedia.org/wiki/Reading_comprehension">reading comprehension</a>.</abstract>
      <url hash="01eb0be2">D19-1257</url>
      <attachment hash="d738d991">D19-1257.Attachment.pdf</attachment>
      <doi>10.18653/v1/D19-1257</doi>
      <bibkey>mihaylov-frank-2019-discourse</bibkey>
      <pwccode url="https://github.com/Heidelberg-NLP/discourse-aware-semantic-self-attention" additional="false">Heidelberg-NLP/discourse-aware-semantic-self-attention</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/narrativeqa">NarrativeQA</pwcdataset>
    </paper>
    <paper id="258">
      <title>Revealing the Importance of Semantic Retrieval for <a href="https://en.wikipedia.org/wiki/Machine_reading">Machine Reading</a> at Scale</title>
      <author><first>Yixin</first><last>Nie</last></author>
      <author><first>Songhe</first><last>Wang</last></author>
      <author><first>Mohit</first><last>Bansal</last></author>
      <pages>2553–2566</pages>
      <abstract>Machine Reading at Scale (MRS) is a challenging task in which a <a href="https://en.wikipedia.org/wiki/System">system</a> is given an input query and is asked to produce a precise output by reading information from a large knowledge base. The <a href="https://en.wikipedia.org/wiki/Task_(project_management)">task</a> has gained popularity with its natural combination of <a href="https://en.wikipedia.org/wiki/Information_retrieval">information retrieval (IR)</a> and <a href="https://en.wikipedia.org/wiki/Machine_learning">machine comprehension (MC)</a>. Advancements in <a href="https://en.wikipedia.org/wiki/Representation_learning">representation learning</a> have led to separated progress in both IR and MC ; however, very few studies have examined the relationship and combined design of <a href="https://en.wikipedia.org/wiki/Information_retrieval">retrieval</a> and <a href="https://en.wikipedia.org/wiki/Sentence_processing">comprehension</a> at different levels of <a href="https://en.wikipedia.org/wiki/Granularity">granularity</a>, for development of MRS systems. In this work, we give general guidelines on system design for MRS by proposing a simple yet effective <a href="https://en.wikipedia.org/wiki/Pipeline_(software)">pipeline system</a> with special consideration on hierarchical semantic retrieval at both paragraph and sentence level, and their potential effects on the downstream task. The system is evaluated on both fact verification and open-domain multihop QA, achieving state-of-the-art results on the leaderboard test sets of both FEVER and HOTPOTQA. To further demonstrate the importance of semantic retrieval, we present ablation and analysis studies to quantify the contribution of neural retrieval modules at both paragraph-level and sentence-level, and illustrate that intermediate semantic retrieval modules are vital for not only effectively filtering upstream information and thus saving downstream computation, but also for shaping upstream data distribution and providing better data for downstream modeling.</abstract>
      <url hash="9bcff291">D19-1258</url>
      <doi>10.18653/v1/D19-1258</doi>
      <bibkey>nie-etal-2019-revealing</bibkey>
      <pwccode url="https://github.com/easonnie/semanticRetrievalMRS" additional="true">easonnie/semanticRetrievalMRS</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/fever">FEVER</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/hotpotqa">HotpotQA</pwcdataset>
    </paper>
    <paper id="259">
      <title>PubMedQA : A Dataset for Biomedical Research Question Answering<fixed-case>P</fixed-case>ub<fixed-case>M</fixed-case>ed<fixed-case>QA</fixed-case>: A Dataset for Biomedical Research Question Answering</title>
      <author><first>Qiao</first><last>Jin</last></author>
      <author><first>Bhuwan</first><last>Dhingra</last></author>
      <author><first>Zhengping</first><last>Liu</last></author>
      <author><first>William</first><last>Cohen</last></author>
      <author><first>Xinghua</first><last>Lu</last></author>
      <pages>2567–2577</pages>
      <abstract>We introduce PubMedQA, a novel biomedical question answering (QA) dataset collected from PubMed abstracts. The task of PubMedQA is to answer research questions with yes / no / maybe (e.g. : Do preoperative <a href="https://en.wikipedia.org/wiki/Statin">statins</a> reduce <a href="https://en.wikipedia.org/wiki/Atrial_fibrillation">atrial fibrillation</a> after coronary artery bypass grafting?) using the corresponding <a href="https://en.wikipedia.org/wiki/Abstract_(summary)">abstracts</a>. PubMedQA has 1k expert-annotated, 61.2k unlabeled and 211.3k artificially generated QA instances. Each PubMedQA instance is composed of (1) a question which is either an existing research article title or derived from one, (2) a context which is the corresponding abstract without its conclusion, (3) a long answer, which is the conclusion of the abstract and, presumably, answers the research question, and (4) a yes / no / maybe answer which summarizes the conclusion. PubMedQA is the first QA dataset where reasoning over biomedical research texts, especially their quantitative contents, is required to answer the questions. Our best performing <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a>, multi-phase fine-tuning of BioBERT with long answer bag-of-word statistics as additional supervision, achieves 68.1 % <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a>, compared to single human performance of 78.0 % accuracy and majority-baseline of 55.2 % <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a>, leaving much room for improvement. PubMedQA is publicly available at https://pubmedqa.github.io.</abstract>
      <url hash="48b27e55">D19-1259</url>
      <doi>10.18653/v1/D19-1259</doi>
      <bibkey>jin-etal-2019-pubmedqa</bibkey>
      <pwccode url="" additional="true" />
      <pwcdataset url="https://paperswithcode.com/dataset/pubmedqa">PubMedQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/bioasq">BioASQ</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/hotpotqa">HotpotQA</pwcdataset>
    </paper>
    <paper id="264">
      <title>Incorporating Graph Attention Mechanism into Knowledge Graph Reasoning Based on <a href="https://en.wikipedia.org/wiki/Deep_learning">Deep Reinforcement Learning</a></title>
      <author><first>Heng</first><last>Wang</last></author>
      <author><first>Shuangyin</first><last>Li</last></author>
      <author><first>Rong</first><last>Pan</last></author>
      <author><first>Mingzhi</first><last>Mao</last></author>
      <pages>2623–2631</pages>
      <abstract>Knowledge Graph (KG) reasoning aims at finding reasoning paths for relations, in order to solve the problem of incompleteness in KG. Many previous path-based methods like PRA and DeepPath suffer from lacking memory components, or stuck in training. Therefore, their performances always rely on well-pretraining. In this paper, we present a deep reinforcement learning based model named by AttnPath, which incorporates LSTM and Graph Attention Mechanism as the memory components. We define two metrics, Mean Selection Rate (MSR) and Mean Replacement Rate (MRR), to quantitatively measure how difficult it is to learn the query relations, and take advantages of them to fine-tune the model under the framework of <a href="https://en.wikipedia.org/wiki/Reinforcement_learning">reinforcement learning</a>. Meanwhile, a novel mechanism of <a href="https://en.wikipedia.org/wiki/Reinforcement_learning">reinforcement learning</a> is proposed by forcing an agent to walk forward every step to avoid the agent stalling at the same entity node constantly. Based on this operation, the proposed <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> not only can get rid of the pretraining process, but also achieves state-of-the-art performance comparing with the other <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a>. We test our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> on FB15K-237 and NELL-995 datasets with different <a href="https://en.wikipedia.org/wiki/Task_(computing)">tasks</a>. Extensive experiments show that our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> is effective and competitive with many current state-of-the-art methods, and also performs well in practice.</abstract>
      <url hash="3944fb92">D19-1264</url>
      <attachment hash="53da6f03">D19-1264.Attachment.zip</attachment>
      <doi>10.18653/v1/D19-1264</doi>
      <bibkey>wang-etal-2019-incorporating</bibkey>
    </paper>
    <paper id="266">
      <title>DIVINE : A Generative Adversarial Imitation Learning Framework for Knowledge Graph Reasoning<fixed-case>DIVINE</fixed-case>: A Generative Adversarial Imitation Learning Framework for Knowledge Graph Reasoning</title>
      <author><first>Ruiping</first><last>Li</last></author>
      <author><first>Xiang</first><last>Cheng</last></author>
      <pages>2642–2651</pages>
      <abstract>Knowledge graphs (KGs) often suffer from <a href="https://en.wikipedia.org/wiki/Glossary_of_graph_theory_terms">sparseness</a> and <a href="https://en.wikipedia.org/wiki/Glossary_of_graph_theory_terms">incompleteness</a>. Knowledge graph reasoning provides a feasible way to address such problems. Recent studies on knowledge graph reasoning have shown that reinforcement learning (RL) based methods can provide state-of-the-art performance. However, existing RL-based methods require numerous trials for <a href="https://en.wikipedia.org/wiki/Pathfinding">path-finding</a> and rely heavily on meticulous reward engineering to fit specific dataset, which is inefficient and laborious to apply to fast-evolving KGs. To this end, in this paper, we present DIVINE, a novel plug-and-play framework based on generative adversarial imitation learning for enhancing existing RL-based methods. DIVINE guides the path-finding process, and learns reasoning policies and reward functions self-adaptively through imitating the demonstrations automatically sampled from KGs. Experimental results on two benchmark datasets show that our <a href="https://en.wikipedia.org/wiki/Software_framework">framework</a> improves the performance of existing RL-based methods while eliminating extra reward engineering.</abstract>
      <url hash="c4756c08">D19-1266</url>
      <doi>10.18653/v1/D19-1266</doi>
      <bibkey>li-cheng-2019-divine</bibkey>
    </paper>
    <paper id="269">
      <title>Collaborative Policy Learning for Open Knowledge Graph Reasoning</title>
      <author><first>Cong</first><last>Fu</last></author>
      <author><first>Tong</first><last>Chen</last></author>
      <author><first>Meng</first><last>Qu</last></author>
      <author><first>Woojeong</first><last>Jin</last></author>
      <author><first>Xiang</first><last>Ren</last></author>
      <pages>2672–2681</pages>
      <abstract>In recent years, there has been a surge of interests in interpretable graph reasoning methods. However, these <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> often suffer from limited performance when working on sparse and incomplete graphs, due to the lack of evidential paths that can reach target entities. Here we study open knowledge graph reasoninga task that aims to reason for missing facts over a <a href="https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)">graph</a> augmented by a <a href="https://en.wikipedia.org/wiki/Text_corpus">background text corpus</a>. A key challenge of the task is to filter out irrelevant facts extracted from corpus, in order to maintain an effective <a href="https://en.wikipedia.org/wiki/Feasible_region">search space</a> during path inference. We propose a novel reinforcement learning framework to train two <a href="https://en.wikipedia.org/wiki/Collaborative_learning">collaborative agents</a> jointly, i.e., a multi-hop graph reasoner and a fact extractor. The fact extraction agent generates fact triples from corpora to enrich the graph on the fly ; while the reasoning agent provides feedback to the fact extractor and guides it towards promoting facts that are helpful for the interpretable reasoning. Experiments on two public datasets demonstrate the effectiveness of the proposed approach.</abstract>
      <url hash="f0815f2d">D19-1269</url>
      <attachment hash="c397b5e4">D19-1269.Attachment.pdf</attachment>
      <doi>10.18653/v1/D19-1269</doi>
      <bibkey>fu-etal-2019-collaborative</bibkey>
      <pwccode url="https://github.com/shanzhenren/CPL" additional="true">shanzhenren/CPL</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/umls">UMLS</pwcdataset>
    </paper>
    <paper id="272">
      <title>Keep Calm and Switch On ! Preserving Sentiment and Fluency in Semantic Text Exchange</title>
      <author><first>Steven Y.</first><last>Feng</last></author>
      <author><first>Aaron W.</first><last>Li</last></author>
      <author><first>Jesse</first><last>Hoey</last></author>
      <pages>2701–2711</pages>
      <abstract>In this paper, we present a novel method for measurably adjusting the semantics of text while preserving its sentiment and fluency, a task we call semantic text exchange. This is useful for text data augmentation and the semantic correction of text generated by <a href="https://en.wikipedia.org/wiki/Chatbot">chatbots</a> and <a href="https://en.wikipedia.org/wiki/Virtual_assistant">virtual assistants</a>. We introduce a <a href="https://en.wikipedia.org/wiki/Pipeline_(software)">pipeline</a> called SMERTI that combines <a href="https://en.wikipedia.org/wiki/Entity–relationship_model">entity replacement</a>, similarity masking, and text infilling. We measure our <a href="https://en.wikipedia.org/wiki/Pipeline_(software)">pipeline</a>’s success by its Semantic Text Exchange Score (STES): the ability to preserve the original text’s sentiment and fluency while adjusting semantic content. We propose to use masking (replacement) rate threshold as an adjustable parameter to control the amount of semantic change in the text. Our experiments demonstrate that SMERTI can outperform baseline models on <a href="https://en.wikipedia.org/wiki/Yelp">Yelp reviews</a>, <a href="https://en.wikipedia.org/wiki/Amazon_(company)">Amazon reviews</a>, and <a href="https://en.wikipedia.org/wiki/Headline">news headlines</a>.</abstract>
      <url hash="2b8437d8">D19-1272</url>
      <attachment hash="131922bd">D19-1272.Attachment.zip</attachment>
      <doi>10.18653/v1/D19-1272</doi>
      <bibkey>feng-etal-2019-keep</bibkey>
      <pwccode url="https://github.com/styfeng/SMERTI" additional="false">styfeng/SMERTI</pwccode>
    </paper>
    <paper id="280">
      <title>Interactive Language Learning by Question Answering</title>
      <author><first>Xingdi</first><last>Yuan</last></author>
      <author><first>Marc-Alexandre</first><last>Côté</last></author>
      <author><first>Jie</first><last>Fu</last></author>
      <author><first>Zhouhan</first><last>Lin</last></author>
      <author><first>Chris</first><last>Pal</last></author>
      <author><first>Yoshua</first><last>Bengio</last></author>
      <author><first>Adam</first><last>Trischler</last></author>
      <pages>2796–2813</pages>
      <abstract>Humans observe and interact with the world to acquire knowledge. However, most existing machine reading comprehension (MRC) tasks miss the interactive, information-seeking component of comprehension. Such <a href="https://en.wikipedia.org/wiki/Task_(project_management)">tasks</a> present <a href="https://en.wikipedia.org/wiki/Conceptual_model">models</a> with static documents that contain all necessary information, usually concentrated in a single short substring. Thus, <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> can achieve strong performance through simple word- and phrase-based pattern matching. We address this problem by formulating a novel text-based question answering task : Question Answering with Interactive Text (QAit). In QAit, an agent must interact with a partially observable text-based environment to gather information required to answer questions. QAit poses questions about the existence, location, and attributes of objects found in the environment. The <a href="https://en.wikipedia.org/wiki/Data">data</a> is built using a text-based game generator that defines the underlying dynamics of interaction with the environment. We propose and evaluate a set of <a href="https://en.wikipedia.org/wiki/Baseline_(configuration_management)">baseline models</a> for the QAit task that includes deep reinforcement learning agents. Experiments show that the <a href="https://en.wikipedia.org/wiki/Task_(project_management)">task</a> presents a major challenge for machine reading systems, while humans solve it with relative ease.</abstract>
      <url hash="3377d37c">D19-1280</url>
      <attachment hash="9603e859">D19-1280.Attachment.zip</attachment>
      <doi>10.18653/v1/D19-1280</doi>
      <bibkey>yuan-etal-2019-interactive</bibkey>
      <pwccode url="https://github.com/xingdi-eric-yuan/qait_public" additional="false">xingdi-eric-yuan/qait_public</pwccode>
    </paper>
    <paper id="283">
      <title>Learning with Limited Data for Multilingual Reading Comprehension</title>
      <author><first>Kyungjae</first><last>Lee</last></author>
      <author><first>Sunghyun</first><last>Park</last></author>
      <author><first>Hojae</first><last>Han</last></author>
      <author><first>Jinyoung</first><last>Yeo</last></author>
      <author><first>Seung-won</first><last>Hwang</last></author>
      <author><first>Juho</first><last>Lee</last></author>
      <pages>2840–2850</pages>
      <abstract>This paper studies the problem of supporting question answering in a new language with limited training resources. As an extreme scenario, when no such resource exists, one can (1) transfer labels from another language, and (2) generate labels from unlabeled data, using translator and automatic labeling function respectively. However, these approaches inevitably introduce noises to the training data, due to translation or generation errors, which require a judicious use of data with varying confidence. To address this challenge, we propose a weakly-supervised framework that quantifies such noises from automatically generated labels, to deemphasize or fix noisy data in training. On reading comprehension task, we demonstrate the effectiveness of our model on low-resource languages with varying similarity to <a href="https://en.wikipedia.org/wiki/English_language">English</a>, namely, <a href="https://en.wikipedia.org/wiki/Korean_language">Korean</a> and <a href="https://en.wikipedia.org/wiki/French_language">French</a>.</abstract>
      <url hash="4171697f">D19-1283</url>
      <doi>10.18653/v1/D19-1283</doi>
      <bibkey>lee-etal-2019-learning</bibkey>
    </paper>
    <paper id="287">
      <title>Representation of Constituents in Neural Language Models : Coordination Phrase as a Case Study</title>
      <author><first>Aixiu</first><last>An</last></author>
      <author><first>Peng</first><last>Qian</last></author>
      <author><first>Ethan</first><last>Wilcox</last></author>
      <author><first>Roger</first><last>Levy</last></author>
      <pages>2888–2899</pages>
      <abstract>Neural language models have achieved state-of-the-art performances on many NLP tasks, and recently have been shown to learn a number of hierarchically-sensitive syntactic dependencies between individual words. However, equally important for <a href="https://en.wikipedia.org/wiki/Language_processing_in_the_brain">language processing</a> is the ability to combine words into phrasal constituents, and use constituent-level features to drive downstream expectations. Here we investigate neural models’ ability to represent <a href="https://en.wikipedia.org/wiki/Constituent_(linguistics)">constituent-level features</a>, using coordinated noun phrases as a case study. We assess whether different neural language models trained on <a href="https://en.wikipedia.org/wiki/English_language">English</a> and <a href="https://en.wikipedia.org/wiki/French_language">French</a> represent phrase-level number and gender features, and use those <a href="https://en.wikipedia.org/wiki/Feature_(machine_learning)">features</a> to drive downstream expectations. Our results suggest that models use a linear combination of NP constituent number to drive CoordNP / verb number agreement. This <a href="https://en.wikipedia.org/wiki/Behavior">behavior</a> is highly regular and even sensitive to local syntactic context, however it differs crucially from observed human behavior. Models have less success with <a href="https://en.wikipedia.org/wiki/Gender_of_connectors_and_fasteners">gender agreement</a>. Models trained on large corpora perform best, and there is no obvious advantage for <a href="https://en.wikipedia.org/wiki/Statistical_model">models</a> trained using explicit syntactic supervision.</abstract>
      <url hash="e0f707bc">D19-1287</url>
      <doi>10.18653/v1/D19-1287</doi>
      <bibkey>an-etal-2019-representation</bibkey>
      <pwccode url="https://github.com/cpllab/rnn_psycholing_coordination" additional="false">cpllab/rnn_psycholing_coordination</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/penn-treebank">Penn Treebank</pwcdataset>
    </paper>
    <paper id="288">
      <title>Towards Zero-shot Language Modeling</title>
      <author><first>Edoardo Maria</first><last>Ponti</last></author>
      <author><first>Ivan</first><last>Vulić</last></author>
      <author><first>Ryan</first><last>Cotterell</last></author>
      <author><first>Roi</first><last>Reichart</last></author>
      <author><first>Anna</first><last>Korhonen</last></author>
      <pages>2900–2910</pages>
      <abstract>Can we construct a neural language model which is inductively biased towards learning <a href="https://en.wikipedia.org/wiki/Human_language">human language</a>? Motivated by this question, we aim at constructing an informative prior for held-out languages on the task of character-level, open-vocabulary language modelling. We obtain this <a href="https://en.wikipedia.org/wiki/Prior_probability">prior</a> as the posterior over network weights conditioned on the data from a sample of training languages, which is approximated through <a href="https://en.wikipedia.org/wiki/Laplace’s_method">Laplace’s method</a>. Based on a large and diverse sample of languages, the use of our <a href="https://en.wikipedia.org/wiki/Prior_probability">prior</a> outperforms baseline models with an uninformative prior in both zero-shot and few-shot settings, showing that the <a href="https://en.wikipedia.org/wiki/Prior_probability">prior</a> is imbued with universal linguistic knowledge. Moreover, we harness broad language-specific information available for most languages of the world, i.e., <a href="https://en.wikipedia.org/wiki/Feature_(linguistics)">features</a> from <a href="https://en.wikipedia.org/wiki/Linguistic_description">typological databases</a>, as distant supervision for held-out languages. We explore several language modelling conditioning techniques, including concatenation and meta-networks for parameter generation. They appear beneficial in the <a href="https://en.wikipedia.org/wiki/Limited_series_(comics)">few-shot setting</a>, but ineffective in the <a href="https://en.wikipedia.org/wiki/Limited_series_(comics)">zero-shot setting</a>. Since the paucity of even plain digital text affects the majority of the world’s languages, we hope that these insights will broaden the scope of applications for <a href="https://en.wikipedia.org/wiki/Language_technology">language technology</a>.</abstract>
      <url hash="306f45e2">D19-1288</url>
      <attachment hash="5e9bebe1">D19-1288.Attachment.pdf</attachment>
      <doi>10.18653/v1/D19-1288</doi>
      <bibkey>ponti-etal-2019-towards</bibkey>
      <revision id="1" href="D19-1288v1" hash="2eb35a35" />
      <revision id="2" href="D19-1288v2" hash="306f45e2" date="2021-08-12">Fixed equation relevant content</revision>
    </paper>
    <paper id="290">
      <title>Modeling Frames in Argumentation</title>
      <author><first>Yamen</first><last>Ajjour</last></author>
      <author><first>Milad</first><last>Alshomary</last></author>
      <author><first>Henning</first><last>Wachsmuth</last></author>
      <author><first>Benno</first><last>Stein</last></author>
      <pages>2922–2932</pages>
      <abstract>In <a href="https://en.wikipedia.org/wiki/Argumentation_theory">argumentation</a>, <a href="https://en.wikipedia.org/wiki/Framing_(social_sciences)">framing</a> is used to emphasize a specific aspect of a controversial topic while concealing others. When talking about <a href="https://en.wikipedia.org/wiki/Drug_liberalization">legalizing drugs</a>, for instance, its economical aspect may be emphasized. In general, we call a set of arguments that focus on the same aspect a frame. An argumentative text has to serve the right frame(s) to convince the audience to adopt the author’s stance (e.g., being pro or con legalizing drugs). More specifically, an author has to choose <a href="https://en.wikipedia.org/wiki/Framing_(social_sciences)">frames</a> that fit the audience’s cultural background and interests. This paper introduces frame identification, which is the task of splitting a set of arguments into non-overlapping frames. We present a fully unsupervised approach to this task, which first removes topical information and then identifies <a href="https://en.wikipedia.org/wiki/Framing_(social_sciences)">frames</a> using <a href="https://en.wikipedia.org/wiki/Cluster_analysis">clustering</a>. For evaluation purposes, we provide a <a href="https://en.wikipedia.org/wiki/Text_corpus">corpus</a> with 12, 326 debate-portal arguments, organized along the frames of the debates’ topics. On this <a href="https://en.wikipedia.org/wiki/Text_corpus">corpus</a>, our approach outperforms different strong baselines, achieving an F1-score of 0.28.</abstract>
      <url hash="6d39304f">D19-1290</url>
      <doi>10.18653/v1/D19-1290</doi>
      <bibkey>ajjour-etal-2019-modeling</bibkey>
    </paper>
    <paper id="293">
      <title>Nonsense ! : Quality Control via Two-Step Reason Selection for Annotating Local Acceptability and Related Attributes in News Editorials</title>
      <author><first>Wonsuk</first><last>Yang</last></author>
      <author><first>Seungwon</first><last>Yoon</last></author>
      <author><first>Ada</first><last>Carpenter</last></author>
      <author><first>Jong</first><last>Park</last></author>
      <pages>2954–2963</pages>
      <abstract>Annotation quality control is a critical aspect for building reliable corpora through <a href="https://en.wikipedia.org/wiki/Annotation">linguistic annotation</a>. In this study, we present a simple but powerful quality control method using two-step reason selection. We gathered sentential annotations of local acceptance and three related attributes through a <a href="https://en.wikipedia.org/wiki/Crowdsourcing">crowdsourcing platform</a>. For each attribute, the reason for the choice of the attribute value is selected in a two-step manner. The options given for reason selection were designed to facilitate the detection of a nonsensical reason selection. We assume that a sentential annotation that contains a nonsensical reason is less reliable than the one without such reason. Our method, based solely on this assumption, is found to retain the <a href="https://en.wikipedia.org/wiki/Annotation">annotations</a> with satisfactory quality out of the entire annotations mixed with those of low quality.</abstract>
      <url hash="4e049941">D19-1293</url>
      <attachment hash="e0d3709c">D19-1293.Attachment.zip</attachment>
      <doi>10.18653/v1/D19-1293</doi>
      <bibkey>yang-etal-2019-nonsense</bibkey>
    </paper>
    <paper id="294">
      <title>Evaluating Pronominal Anaphora in <a href="https://en.wikipedia.org/wiki/Machine_translation">Machine Translation</a> : An Evaluation Measure and a Test Suite</title>
      <author><first>Prathyusha</first><last>Jwalapuram</last></author>
      <author><first>Shafiq</first><last>Joty</last></author>
      <author><first>Irina</first><last>Temnikova</last></author>
      <author><first>Preslav</first><last>Nakov</last></author>
      <pages>2964–2975</pages>
      <abstract>The ongoing neural revolution in <a href="https://en.wikipedia.org/wiki/Machine_translation">machine translation</a> has made it easier to model larger contexts beyond the sentence-level, which can potentially help resolve some discourse-level ambiguities such as pronominal anaphora, thus enabling better translations. Unfortunately, even when the resulting improvements are seen as substantial by humans, they remain virtually unnoticed by traditional automatic evaluation measures like <a href="https://en.wikipedia.org/wiki/BLEU">BLEU</a>, as only a few words end up being affected. Thus, specialized <a href="https://en.wikipedia.org/wiki/Evaluation">evaluation measures</a> are needed. With this aim in mind, we contribute an extensive, targeted dataset that can be used as a test suite for pronoun translation, covering multiple source languages and different pronoun errors drawn from real system translations, for <a href="https://en.wikipedia.org/wiki/English_language">English</a>. We further propose an evaluation measure to differentiate good and bad pronoun translations. We also conduct a <a href="https://en.wikipedia.org/wiki/User_study">user study</a> to report correlations with <a href="https://en.wikipedia.org/wiki/Judgement">human judgments</a>.</abstract>
      <url hash="bdb5eb6c">D19-1294</url>
      <attachment hash="23a426b4">D19-1294.Attachment.zip</attachment>
      <doi>10.18653/v1/D19-1294</doi>
      <bibkey>jwalapuram-etal-2019-evaluating</bibkey>
      <pwccode url="" additional="true" />
      <pwcdataset url="https://paperswithcode.com/dataset/wmt-2014">WMT 2014</pwcdataset>
    <title_es>Evaluación de la anáfora pronominal en la traducción automática: una medida de evaluación y un conjunto de pruebas</title_es>
      <title_ar>تقييم الجناس المنطقي في الترجمة الآلية: مقياس تقييم ومجموعة اختبار</title_ar>
      <title_fr>Évaluation de l'anaphore pronominale en traduction automatique : une mesure d'évaluation et une suite de tests</title_fr>
      <title_pt>Avaliando a anáfora pronominal na tradução automática: uma medida de avaliação e um conjunto de testes</title_pt>
      <title_zh>机器翻译中评估代名词性析:估度量及试套件</title_zh>
      <title_ja>機械翻訳における名詞アナフォラの評価：評価尺度とテストスイート</title_ja>
      <title_ru>Оценка Pronominal Anaphora в машинном переводе: мера оценки и тестовый набор</title_ru>
      <title_hi>मशीन अनुवाद में प्रोनोमिनल एनाफोरा का मूल्यांकन: एक मूल्यांकन उपाय और एक टेस्ट सूट</title_hi>
      <title_ga>Anaphora Pronominal a Mheas san Aistriúchán Meaisín: Beart Measúnaithe agus Sraith Tástála</title_ga>
      <title_ka>პრონომინალური ანაფორა მაქსინური გადაწყვეტილების განსაზღვრება: განსაზღვრება და ტესტის სუტი</title_ka>
      <title_hu>A Pronominális Anaphora értékelése a gépi fordításban: Egy értékelési intézkedés és egy tesztcsomag</title_hu>
      <title_el>Αξιολόγηση της Προφανής Ανάφορας στη Μηχανική Μετάφραση: Ένα Μέτρο Αξιολόγησης και μια Σουίτα Δοκιμών</title_el>
      <title_it>Valutazione dell'anafora pronominale nella traduzione automatica: una misura di valutazione e una suite di test</title_it>
      <title_kk>Компьютердің аудармасында прономиналдық анафорын бағалау: Бағалау өлшемі мен сынақтар тізімі</title_kk>
      <title_ms>Evaluating Pronominal Anaphora in Machine Translation: An Evaluation Measure and a Test Suite</title_ms>
      <title_lt>Pavyzdinės anaforos vertimo mašin a vertinimas: vertinimo priemonė ir bandymų rinkinys</title_lt>
      <title_mk>Оценувањето на прономиналната анафора во машинска превод: мерка за евалуација и тестиран апартман</title_mk>
      <title_ml>മെഷീന്‍ പരിഭാഷയിലെ പ്രോമോനിയല്‍ അനാഫ്രാ പരിശോധനത്തെ പരിശോധിക്കുന്നു</title_ml>
      <title_mn>Машин хөрөнгө оруулалтын нэр нэр дэвшүүлэгт Анафорыг дүгнэх: Дэлхийн шалгалтын хэмжээг, шалгалтын хэмжээг</title_mn>
      <title_pl>Ocena anafory pronominalnej w tłumaczeniu maszynowym: środek oceny i zestaw testowy</title_pl>
      <title_ro>Evaluarea anaforei pronunțiale în traducerea automată: o măsură de evaluare și o suită de testare</title_ro>
      <title_mt>Evalwazzjoni tal-Anafora Pronominali fit-Traduzzjoni tal-Makkinarju: Miżura ta’ Evalwazzjoni u Suite tat-Test</title_mt>
      <title_so>Qiimeynta tarjumaadka mashiinka:</title_so>
      <title_sv>Utvärdering av uttalad anafora i maskinöversättning: En utvärderingsåtgärd och en testsvit</title_sv>
      <title_si>පරීක්ෂණ අවශ්‍ය විශ්වාස කරනවා පරීක්ෂණ සුයිට්</title_si>
      <title_ta>இயந்திரத்தில் பொருள் மொழிபெயர்ப்பில் ஒழுங்குபடுகிறது</title_ta>
      <title_ur>ماشین ترجمہ میں پرونومیل انفاورا کا ارزش کیا جاتا ہے: ایک ارزش میزان اور ایک تست سئٹ</title_ur>
      <title_sr>Pronominalna anafora u prevodu mašine: mjera procjene i testa</title_sr>
      <title_no>Evaluerer pronominal anafora i maskinsomsetjing: Eit evalueringsmål og eit testpakke</title_no>
      <title_uz>Name</title_uz>
      <title_vi>Đánh giá Anaphora dự tính trong máy dịch: một phần đánh giá và một phòng thử.</title_vi>
      <title_bg>Оценка на произнозната анафора в машинния превод: мярка за оценка и тест комплект</title_bg>
      <title_hr>Pronominalna anafora u prevodu strojeva: mjera procjene i testiranja</title_hr>
      <title_da>Evaluering af Pronominal Anaphora i maskinoversættelse: En evalueringsforanstaltning og en testpakke</title_da>
      <title_nl>Het evalueren van pronominale anafora in machinevertaling: een evaluatiemaatregel en een testsuite</title_nl>
      <title_de>Bewertung der Pronominalen Anaphora in der maschinellen Übersetzung: Eine Evaluationsmaßnahme und eine Testsuite</title_de>
      <title_sw>Kuthibitisha Uchambuzi wa Tafsiri ya Mashiniki: Hatua ya Uchunguzi na Mkutano wa Jariba</title_sw>
      <title_id>Evaluasi Anafora Pronominal dalam Translation Mesin: Sebuah Ukuran Evaluasi dan Suite Uji</title_id>
      <title_ko>평가 척도와 테스트 집합</title_ko>
      <title_fa>ارزیابی Anaphora Pronominal in Machine Translation: An Evaluation Measure and a Test Suite</title_fa>
      <title_tr>Ullançy terjimede Anafora Taýýarlama: Bir Taýýarlyk Tapylmagy we Bir Test Suytynda</title_tr>
      <title_af>Name</title_af>
      <title_am>ዶሴ `%s'ን ማስፈጠር አልተቻለም፦ %s</title_am>
      <title_hy>Մեքենայի թարգմանման պրոնամինալ անաֆորան գնահատելը՝ գնահատման չափություն և փորձարկումների համակարգ</title_hy>
      <title_az>Makinelərin çevirilməsində Pronomil Anafora değerləşdirilməsi: Qıymet Ölçüsi və Test Suit</title_az>
      <title_bs>Pronominalna anafora u prevodu strojeva: mjera procjene i testnog paketa</title_bs>
      <title_ca>Evaluation Pronominal Anaphora in Machine Translation: An Evaluation Measure and a Test Suite</title_ca>
      <title_cs>Hodnocení pronominální anafory v strojovém překladu: hodnotící opatření a testovací sada</title_cs>
      <title_bn>মেশিন অনুবাদে প্রোনোমিনাল আনাফোরা মুক্তি প্রদান করা হচ্ছে: একটি পরীক্ষার মাপ এবং একটি পরীক্ষা সাইট</title_bn>
      <title_sq>Evaluating Pronominal Anaphora in Machine Translation: An Evaluation Measure and a Test Suite</title_sq>
      <title_et>Pronominaalse anafoori hindamine masintõlkes: hindamismeetmed ja testikomplekt</title_et>
      <title_fi>Pronominaalisen anaforan arviointi konekäännöksessä: arviointitoimenpide ja testisarja</title_fi>
      <title_jv>Language</title_jv>
      <title_sk>Ocenjevanje izgovorne anafore v strojnem prevodu: ocenjevalni ukrep in testni komplet</title_sk>
      <title_he>הערכה של אנפורה פרונומילית בתרגום מכונות: מדידת הערכה וסוויט מבחן</title_he>
      <title_ha>KCharselect unicode block name</title_ha>
      <title_bo>འདིའི་ནང་གི་རྩིས་འཁོར་གཞུང་གི་ལྟ་བུ་བཏོན་བཟོ་བྱེད་པའི་Anaphora</title_bo>
      <abstract_fr>La révolution neuronale en cours dans la traduction automatique a facilité la modélisation de contextes plus vastes au-delà du niveau de la phrase, ce qui peut potentiellement aider à résoudre certaines ambiguïtés au niveau du discours, telles que l'anaphore pronominale, permettant ainsi de meilleures traductions. Malheureusement, même lorsque les améliorations qui en résultent sont considérées comme substantielles par les humains, elles restent pratiquement inaperçues par les mesures d'évaluation automatique traditionnelles comme BLEU, car seuls quelques mots finissent par être affectés. Des mesures d'évaluation spécialisées sont donc nécessaires. Dans cet objectif, nous fournissons un ensemble de données complet et ciblé qui peut être utilisé comme suite de tests pour la traduction de pronoms, couvrant plusieurs langues sources et différentes erreurs de pronom tirées de traductions système réelles, pour l'anglais. Nous proposons également une mesure d'évaluation pour différencier les bonnes et les mauvaises traductions de pronoms. Nous menons également une étude auprès des utilisateurs afin de signaler les corrélations avec les jugements humains.</abstract_fr>
      <abstract_pt>A revolução neural em curso na tradução automática tornou mais fácil modelar contextos maiores além do nível da frase, o que pode ajudar a resolver algumas ambiguidades no nível do discurso, como a anáfora pronominal, permitindo melhores traduções. Infelizmente, mesmo quando as melhorias resultantes são vistas como substanciais pelos humanos, elas permanecem praticamente despercebidas por medidas tradicionais de avaliação automática como o BLEU, pois apenas algumas palavras acabam sendo afetadas. Assim, são necessárias medidas de avaliação especializadas. Com esse objetivo em mente, contribuímos com um conjunto de dados extenso e direcionado que pode ser usado como um conjunto de testes para tradução de pronomes, abrangendo vários idiomas de origem e diferentes erros de pronome extraídos de traduções de sistemas reais, para inglês. Propomos ainda uma medida de avaliação para diferenciar traduções de pronomes boas e ruins. Também realizamos um estudo de usuário para relatar correlações com julgamentos humanos.</abstract_pt>
      <abstract_ar>جعلت الثورة العصبية المستمرة في الترجمة الآلية من السهل نمذجة سياقات أكبر خارج مستوى الجملة ، والتي يمكن أن تساعد في حل بعض الغموض على مستوى الخطاب مثل الجناس الضميري ، وبالتالي تمكين ترجمات أفضل. لسوء الحظ ، حتى عندما ينظر البشر إلى التحسينات الناتجة على أنها كبيرة ، فإنها تظل تقريبًا غير ملحوظة من خلال مقاييس التقييم التلقائي التقليدية مثل BLEU ، حيث تتأثر بضع كلمات فقط. وبالتالي ، هناك حاجة إلى تدابير تقييم متخصصة. مع وضع هذا الهدف في الاعتبار ، نساهم بمجموعة بيانات شاملة وموجهة يمكن استخدامها كمجموعة اختبار لترجمة الضمائر ، والتي تغطي لغات مصدر متعددة وأخطاء ضمائر مختلفة مستمدة من ترجمات نظام حقيقية للغة الإنجليزية. نقترح كذلك إجراء تقييم للتمييز بين ترجمات الضمائر الجيدة والسيئة. نجري أيضًا دراسة مستخدم للإبلاغ عن الارتباطات مع الأحكام البشرية.</abstract_ar>
      <abstract_es>La revolución neuronal en curso en la traducción automática ha hecho que sea más fácil modelar contextos más amplios más allá del nivel de oración, lo que puede ayudar a resolver algunas ambigüedades a nivel del discurso, como la anáfora pronominal, lo que permite mejores traducciones. Desafortunadamente, incluso cuando los humanos consideran que las mejoras resultantes son sustanciales, pasan prácticamente desapercibidas para las medidas de evaluación automática tradicionales como BLEU, ya que solo unas pocas palabras terminan siendo afectadas. Por lo tanto, se necesitan medidas de evaluación especializadas. Con este objetivo en mente, contribuimos con un conjunto de datos amplio y específico que se puede utilizar como un conjunto de pruebas para la traducción de pronombres, que cubre múltiples idiomas de origen y diferentes errores de pronombres extraídos de traducciones del sistema real, para el inglés. Además, proponemos una medida de evaluación para diferenciar las traducciones de pronombres buenos y malos. También realizamos un estudio de usuarios para informar las correlaciones con los juicios humanos.</abstract_es>
      <abstract_ja>機械翻訳における進行中の神経革命により、文章レベルを超えたより大きなコンテキストをモデル化することが容易になり、これは潜在的に、プロノミナルアナフォラのようないくつかの話題レベルの曖昧さを解決するのに役立つ可能性があり、したがってより良い翻訳を可能にします。残念なことに、結果として生じる改善が人間によって実質的であると見なされても、BLEUのような伝統的な自動評価手段によって事実上注目されないままであり、結果的に影響を受けるのはわずかな単語に過ぎない。したがって、専門的な評価措置が必要である。この目的を念頭に置いて、複数のソース言語と実際のシステム翻訳から抽出されたさまざまな代名詞の誤りをカバーする、代名詞翻訳のテストスイートとして使用できる、幅広い、ターゲットを絞ったデータセットを英語に提供します。さらに、良い代名詞と悪い代名詞の翻訳を区別するための評価尺度を提案します。また、人間の判断との相関を報告するためのユーザー調査も実施しています。</abstract_ja>
      <abstract_zh>机器翻译之方行者神经使句级之外益大上下文建模转易,或有助于语歧义,如代名词anaphora,以成其译。 不幸者,虽由此生改进为人实质性,犹未为故事所评(如BLEU)所注意,盖少单词终染也。 故须专论。 念此一趣,献一广者,有针对性数集,可以为代词译之试套件,涵盖多言异代词,施于英语。 更立评事,以别善恶代词译。 又加用户研究,以告人伦之相关性。</abstract_zh>
      <abstract_hi>मशीन अनुवाद में चल रही तंत्रिका क्रांति ने वाक्य-स्तर से परे बड़े संदर्भों को मॉडल करना आसान बना दिया है, जो संभावित रूप से कुछ प्रवचन-स्तर की अस्पष्टताओं को हल करने में मदद कर सकता है जैसे कि प्रोनोमिनल एनाफोरा, इस प्रकार बेहतर अनुवाद को सक्षम करता है। दुर्भाग्य से, यहां तक कि जब परिणामस्वरूप सुधारों को मनुष्यों द्वारा पर्याप्त के रूप में देखा जाता है, तो वे BLEU जैसे पारंपरिक स्वचालित मूल्यांकन उपायों द्वारा लगभग अनदेखा रहते हैं, क्योंकि केवल कुछ शब्द प्रभावित होते हैं। इस प्रकार, विशेष मूल्यांकन उपायों की आवश्यकता होती है। इस उद्देश्य को ध्यान में रखते हुए, हम एक व्यापक, लक्षित डेटासेट का योगदान करते हैं जिसका उपयोग सर्वनाम अनुवाद के लिए एक परीक्षण सूट के रूप में किया जा सकता है, जिसमें अंग्रेजी के लिए कई स्रोत भाषाओं और वास्तविक सिस्टम अनुवादों से खींची गई विभिन्न सर्वनाम त्रुटियों को शामिल किया जा सकता है। हम आगे अच्छे और बुरे सर्वनाम अनुवादों को अलग करने के लिए एक मूल्यांकन उपाय का प्रस्ताव करते हैं। हम मानव निर्णयों के साथ सहसंबंधों की रिपोर्ट करने के लिए एक उपयोगकर्ता अध्ययन भी करते हैं।</abstract_hi>
      <abstract_ru>Продолжающаяся нейронная революция в машинном переводе облегчила моделирование больших контекстов за пределами уровня предложения, что потенциально может помочь разрешить некоторые двусмысленности на уровне дискурса, такие как прономинальная анафора, тем самым обеспечивая лучшие переводы. К сожалению, даже когда результирующие улучшения рассматриваются как существенные людьми, они остаются практически незамеченными традиционными автоматическими оценочными мерами, такими как BLEU, поскольку только несколько слов в конечном итоге оказываются затронутыми. Таким образом, необходимы специальные меры по оценке. С этой целью мы вносим обширный целевой набор данных, который может быть использован в качестве тестового набора для перевода местоимений, охватывающего несколько исходных языков и различные ошибки местоимений, взятые из реальных системных переводов, для английского языка. Мы также предлагаем меру оценки для различения хороших и плохих переводов местоимений. Мы также проводим пользовательское исследование, чтобы сообщить о корреляции с человеческими суждениями.</abstract_ru>
      <abstract_ga>Tá sé níos fusa ag an réabhlóid néareolaíoch leanúnach san aistriúchán meaisín comhthéacsanna níos mó a shamhaltú thar leibhéal na habairte, rud a d’fhéadfadh cabhrú le roinnt débhríochtaí ar leibhéal an dioscúrsa a réiteach amhail anafóra forainmneach, rud a chumasódh aistriúcháin níos fearr. Ar an drochuair, fiú nuair a mheasann daoine gur feabhsuithe suntasacha iad na feabhsuithe a leanann as, is beag nach dtugtar faoi deara iad ag bearta meastóireachta uathoibríocha traidisiúnta amhail BLEU, toisc nach mbeidh tionchar ach ag cúpla focal orthu. Mar sin, tá gá le sainbhearta meastóireachta. Agus an aidhm seo san áireamh againn, cuirimid tacar sonraí fairsing, spriocdhírithe ar fáil ar féidir a úsáid mar shraith trialacha d’aistriúchán forainmneacha, a chlúdaíonn ilfhoinsí teangacha agus earráidí forainmneacha éagsúla a bhaintear as fíor-aistriúcháin chórais, don Bhéarla. Molaimid freisin beart meastóireachta chun aistriúcháin forainmneacha maithe agus olca a idirdhealú. Déanaimid staidéar úsáideora freisin chun comhghaolta le breithiúnais daonna a thuairisciú.</abstract_ga>
      <abstract_ka>მიმდინარე ნეიროლური რეგჲლუციაში მაქსინური განგორმაციაში უფრო მარტივი კონტექსტების მოდელედ უფრო მარტივი შესაძლებელია, რომელიც შესაძლებელია გადახმარება რამდენიმე კონტექსტების განსაზღვრება მართლად, როდესაც ადამიანების შესაძლებლობების შესაძლებლობების შესაძლებლობები აღმოჩნდება, ისინი სამყაროდ არ იცნობენ ტრადიციონალური ავტომატური განსაზღვრებებით როგორც BLEU, რაც მხოლოდ რამ ამიტომ, სპეციალური განსაზღვრების მოზომილებები უნდა იყოს. ამ მიზეზის შესახებ, ჩვენ მივიღეთ განსაზღვრებული, მიზემობული მონაცემების სექტი, რომელიც შეიძლება გამოიყენება როგორც ტესტის სექტი განსაზღვრებისთვის, რამდენიმე მსგავსი წლის და განსხვავ ჩვენ უფრო მეტად გავაკეთებთ განსხვავება კარგი და ცოტა განსხვავება. ჩვენ ასევე მომხმარებელი სწავლის გავაკეთებთ, რომელიც ადამიანის სწავლის კოლექციების შესახებ.</abstract_ka>
      <abstract_hu>A gépi fordítás folyamatban lévő idegi forradalma megkönnyítette a mondatszinten túli nagyobb kontextusok modellezését, ami potenciálisan segíthet megoldani néhány diskurzus szintű kétértelműséget, mint például a pronominális anafóra, így jobb fordításokat tesz lehetővé. Sajnos még akkor is, ha az ebből eredő fejlesztéseket az emberek jelentősnek tekintik, gyakorlatilag észrevétlenül maradnak a hagyományos automatikus értékelési intézkedések, mint például a BLEU, mivel végül csak néhány szó érinti őket. Ezért speciális értékelési intézkedésekre van szükség. Ennek érdekében kiterjedt, célzott adatkészletet adunk hozzá, amely tesztcsomagként használható névmások fordításához, amely több forrásnyelvet és különböző névmási hibákat fed fel a valódi rendszerfordításokból, angol nyelven. Javasoljuk továbbá egy értékelési intézkedést a jó és rossz névmások fordításának megkülönböztetésére. Felhasználói tanulmányt is végzünk az emberi ítéletekkel való összefüggésekről.</abstract_hu>
      <abstract_el>Η συνεχιζόμενη νευρική επανάσταση στη μηχανική μετάφραση έχει καταστήσει ευκολότερη την μοντελοποίηση μεγαλύτερων πλαισίων πέρα από το επίπεδο της πρότασης, γεγονός που μπορεί ενδεχομένως να βοηθήσει στην επίλυση ορισμένων ασάφειες σε επίπεδο λόγου, όπως η προφορική αναφορά, επιτρέποντας έτσι καλύτερες μεταφράσεις. Δυστυχώς, ακόμη και όταν οι βελτιώσεις που προκύπτουν θεωρούνται σημαντικές από τους ανθρώπους, παραμένουν σχεδόν απαρατήρητες από τα παραδοσιακά μέτρα αυτόματης αξιολόγησης όπως η BLEU, καθώς μόνο λίγες λέξεις καταλήγουν να επηρεάζονται. Συνεπώς, απαιτούνται εξειδικευμένα μέτρα αξιολόγησης. Με αυτόν τον στόχο κατά νου, συνεισφέρουμε ένα εκτεταμένο, στοχευμένο σύνολο δεδομένων που μπορεί να χρησιμοποιηθεί ως δοκιμαστική σουίτα για μετάφραση αντωνυμίας, καλύπτοντας πολλαπλές γλώσσες προέλευσης και διαφορετικά λάθη αντωνυμίας που προέρχονται από πραγματικές μεταφράσεις συστήματος, για τα αγγλικά. Επιπλέον προτείνουμε ένα μέτρο αξιολόγησης για τη διαφοροποίηση των καλών και των κακών μεταφράσεων αντωνυμίας. Επίσης διεξάγουμε μια μελέτη χρηστών για να αναφέρουμε συσχετισμούς με ανθρώπινες κρίσεις.</abstract_el>
      <abstract_it>La rivoluzione neurale in corso nella traduzione automatica ha reso più facile modellare contesti più ampi oltre il livello di frase, il che può potenzialmente aiutare a risolvere alcune ambiguità a livello di discorso come l'anafora pronominale, consentendo così traduzioni migliori. Purtroppo, anche quando i miglioramenti risultanti sono visti come sostanziali dagli esseri umani, rimangono praticamente inosservati dalle misure di valutazione automatiche tradizionali come BLEU, poiché solo poche parole finiscono per essere influenzate. Sono quindi necessarie misure di valutazione specializzate. Con questo obiettivo in mente, contribuiamo con un ampio set di dati mirato che può essere utilizzato come suite di test per la traduzione di pronomi, coprendo più lingue di origine e diversi errori di pronomi tratti da traduzioni reali di sistema, per l'inglese. Proponiamo inoltre una misura di valutazione per differenziare le traduzioni dei pronomi buoni e cattivi. Conduciamo anche uno studio utente per segnalare le correlazioni con i giudizi umani.</abstract_it>
      <abstract_mk>Сегашната нервна револуција во машинскиот превод го олесни моделирањето на поголемите контексти надвор од нивото на речениците, што потенцијално може да помогне во решавањето на некои двигурности на нивото на дискурс, како што е прономиналната анафора, овозможувајќи подобри преводи. За жал, дури и кога резултатите на подобрувањата се сметаат за значителни од страна на луѓето, тие остануваат виртуелно непознати од традиционалните автоматски мерки за проценка како БЛЕУ, бидејќи само неколку зборови завршуваат со влијание. Затоа се потребни специјализирани мерки за проценка. Со оваа цел во предвид, придонесуваме на екстремен, целосен набор на податоци кој може да се користи како тестирачки апартман за превод на прогнози, кој покрива повеќе јазици од извор и различни грешки од прогнози изведени од реални системски преводи, на англиски. We further propose an evaluation measure to differentiate good and bad pronoun translations.  Исто така, спроведуваме студија на корисниците за известување на корелациите со човечките пресуди.</abstract_mk>
      <abstract_lt>Šiuo metu vykstanti nervų revoliucija vertimo mašinomis srityje palengvino didesnių kontekstų modeliavimą už sakinių lygį, kuris gali padėti išspręsti tam tikrus diskurso lygio dviprasmiškumus, pavyzdžiui, pronominę anaforą, ir taip sudaryti sąlygas geresniems vertimams. Deja, net jei žmonės mano, kad tokie patobulinimai yra dideli, jie beveik nepastebimi tradicinėmis automatinėmis vertinimo priemonėmis, pvz., BLEU, nes galiausiai daromas poveikis tik keliems žodžiams. Todėl reikia specialių vertinimo priemonių. Atsižvelgdami į šį tikslą, prisidedame prie plataus masto tikslinio duomenų rinkinio, kuris gali būti naudojamas kaip bandymų rinkinys aiškiam vertimui, apimantis daugelį šaltinių kalbų ir įvairias aiškias klaidas, padarytas iš tikrų sistemų vertimų, anglų kalba. We further propose an evaluation measure to differentiate good and bad pronoun translations.  Taip pat atliekame naudotojų tyrimą, kad praneštume apie koreliacijas su žmonių sprendimais.</abstract_lt>
      <abstract_kk>Компьютердің аудармасындағы невралдық революциясы сөйлем деңгейіндегі үлкен контексттерді үлгілеу үшін оңай көмектеседі. Бұл кейбір дискурс деңгейіндегі амбигвиттерді шешуге көмектеседі, мысалы, нафора де Кешіріңіз, адамдардың жақсартулары көбірек деп көрінгенде де, олар БЛЕС секілді әдімгі автоматты оқу мерзімдерінің көбірек бірнеше сөздерге әсер етеді. Бұл үшін арнаулы оқу мерзімі қажет. Бұл мақсаттың арқылы, біз нақты жүйелік аудармалардан, ағылшынша тілдерінің бірнеше көзі тілдерін және бірнеше нақты аудармалардан көрсетілген жүйелік аудармалардан қолданылатын кеңейтілген, мақсатты деректер жиын Біз жақсы және жарамсыз аудармаларды түрлендіру үшін бағалау мерзімін ұсынамыз. Сонымен қатар, адамдардың тәжірибелерімен қатынау үшін пайдаланушыларды зерттеуді жасаймыз.</abstract_kk>
      <abstract_ms>Revolusi saraf yang sedang berlangsung dalam terjemahan mesin telah membuat ia lebih mudah untuk memmodelkan konteks yang lebih besar di luar aras kalimat, yang mungkin boleh membantu menyelesaikan beberapa ambiguiti aras diskors seperti anafora pronominal, sehingga memungkinkan terjemahan yang lebih baik. Malangnya, walaupun peningkatan yang berkaitan dilihat sebagai penting oleh manusia, mereka tetap hampir tidak dikesan oleh tindakan penilaian automatik tradisional seperti BLEU, kerana hanya beberapa perkataan akhirnya terkena kesan. Jadi, tindakan penilaian khusus diperlukan. Dengan tujuan ini dalam fikiran, kami menyumbangkan set data yang luas, sasaran yang boleh digunakan sebagai suite ujian untuk terjemahan pronon, meliputi bahasa sumber berbilang dan ralat pronon berbeza yang dilukis dari terjemahan sistem sebenar, untuk bahasa Inggeris. Kami juga mengusulkan tindakan penilaian untuk membezakan terjemahan yang baik dan buruk. Kami juga melakukan kajian pengguna untuk melaporkan korelasi dengan penilaian manusia.</abstract_ms>
      <abstract_mt>Ir-rivoluzzjoni newrali li għaddejja fit-traduzzjoni bil-magna għamlitha aktar faċli li jiġu mudellati kuntesti akbar lil hinn mil-livell tas-sentenza, li potenzjalment jistgħu jgħinu biex jiġu solvuti xi ambigwitajiet fil-livell ta’ diskors bħall-anafora pronominali, u b’hekk ikunu jistgħu jiġu tradotti aħjar. Sfortunatament, anke meta t-titjib li jirriżulta jitqies bħala sostanzjali mill-bnedmin, dawn jibqgħu virtwalment mhux innotati minn miżuri ta’ evalwazzjoni awtomatiċi tradizzjonali bħall-BLEU, peress li ftit kliem biss jispiċċaw jiġu affettwati. Thus, specialized evaluation measures are needed.  Fid-dawl ta’ dan l-għan, a ħna nikkontribwixxu sett ta’ dejta estensiv u mmirat li jista’ jintuża bħala sett ta’ testijiet għat-traduzzjoni pronouna, li jkopri diversi lingwi tas-sors u żbalji pronoun differenti meħuda minn traduzzjonijiet reali tas-sistema, għall-Ingliż. Aħna nipproponu wkoll miżura ta’ evalwazzjoni biex niddifferenzjaw traduzzjonijiet pronunzjati tajbin u ħżiena. Għandna nagħmlu wkoll studju tal-utenti biex nirrappurtaw korrelazzjonijiet mas-sentenzi umani.</abstract_mt>
      <abstract_ml>മെഷിന്‍ പരിഭാഷയിലെ ന്യൂറല്‍ വിപ്ലവത്തിന്റെ നടന്നുകൊണ്ടിരിക്കുന്ന വിപ്ലവത്തെ വാക്ക് നില്‍ക്കുന്നതിനു പുറത്തുള്ള വലിയ പരിഹാസങ്ങള്‍ മാതൃകയാക്കുവാന്‍  നിര്‍ഭാഗ്യവശാല്‍, ഭാവിയുടെ മുന്നേറ്റങ്ങള്‍ മനുഷ്യരാല്‍ പ്രധാനപ്പെട്ടതായി കാണുമ്പോള്‍ പോലും അവയൊക്കെ പാരമ്പര്യമായ സ്വയം വില്ലാസപ്രകാരം ബില അതുകൊണ്ട്, വിശേഷിച്ച വിലാസങ്ങള്‍ ആവശ്യമുണ്ട്. With this aim in mind, we contribute an extensive, targeted dataset that can be used as a test suite for pronoun translation, covering multiple source languages and different pronoun errors drawn from real system translations, for English.  നല്ലതും ചീത്ത പരിഭാഷകളും വ്യത്യസ്തമാക്കുവാന്‍ വേണ്ടി നമ്മള്‍ ഒരു വിലാസം പ്രായശ്ചിത്തമാക്കുന്നു. മനുഷ്യരുടെ വിധികളുമായി ബന്ധങ്ങള്‍ റിപ്പോര്‍ട്ട് ചെയ്യാന്‍ ഞങ്ങള്‍ ഒരു ഉപയോക്താവിന്‍റെ പഠനം നട</abstract_ml>
      <abstract_mn>Машин хөгжлийн мэдрэлийн хувьсгал нь илүү амархан болгож өгүүлбэл хэлбэрээс илүү том тохиолдлыг загварчлах нь илүү амархан болгодог. Энэ нь илүү сайн хөгжлийн хэлбэрүүдийг шийдэхэд тусалдаг. Харамсалтай нь хүн төрөлхтний үр дүнг сайжруулахад хүртэл уламжлалтаар автоматик оюун шалгалтын хэмжээсүүд БЛЕУ шиг харагдахгүй байдаг. Зөвхөн хэдэн үг л нөлөөлдөг. Иймд мэргэжлийн оюутнууд хэрэгтэй. Энэ зорилгоор бид үнэндээ хэлбэрийн хэл болон бодит системийн орчуулалтаас, Англи хэл дээр зурагдсан өөр өөр хэлбэрийн алдаа болох шинжлэх ухааны шинжлэх ухаан, зорилготой өгөгдлийн сангуудыг нэмэгдүүлнэ. Бид бас сайн, муу хэлбэрийг өөрчлөх үнэлгээний хэмжээг санал болгож байна. Бид мөн хэрэглэгчийн судалгааг хүний шүүмжлэлтэй холбоотой байдлыг мэдээллийн тулд хийдэг.</abstract_mn>
      <abstract_ro>Revoluția neurală în curs de desfășurare în traducerea automată a făcut mai ușoară modelarea contextelor mai mari dincolo de nivelul propoziției, ceea ce poate ajuta la rezolvarea unor ambiguități la nivel de discurs, cum ar fi anafora pronunțială, permițând astfel traduceri mai bune. Din păcate, chiar și atunci când îmbunătățirile rezultate sunt considerate substanțiale de oameni, ele rămân practic neobservate de măsurile tradiționale de evaluare automată precum BLEU, deoarece doar câteva cuvinte sfârșesc prin a fi afectate. Astfel, sunt necesare măsuri specializate de evaluare. Cu acest scop în minte, contribuim cu un set de date amplu, orientat, care poate fi utilizat ca o suită de test pentru traducerea pronumelor, acoperind mai multe limbi sursă și diferite erori de pronume extrase din traduceri reale de sistem, pentru limba engleză. De asemenea, propunem o măsură de evaluare pentru a diferenția traducerile pronumelor bune și rele. De asemenea, efectuăm un studiu de utilizator pentru a raporta corelațiile cu judecățile umane.</abstract_ro>
      <abstract_no>Den gjeldande neuralrevolusjonen i maskineomsetjinga har gjort det lettere å modellere større kontekstar enn setningsnivået, som eventuelt kan hjelpa til å løyse nokre avtalenivå som pronominal anafora, slik at det kan gjera bedre omsetjingar. Dessverre, sjølv når resultatet forbedringane vert sett som stor av menneske, blir dei ganske uventa av tradisjonelle automatiske evalueringsmåtar som BLEU, men berre noen ord blir påvirka. Dette er derfor nødvendig spesialiserte evalueringsmåtar. Med denne målet er vi bidra til eit utvida, målte dataset som kan brukast som test suite for pronoun translation, dekker fleire kjeldespråk og ulike pronoun- feil teikna frå verkeleg systemomsetjingar, for engelsk. Vi foreslår meir eit evalueringsmål for å distisera godt og dårlig uttrykk. Vi gjer også ein brukarstudie for å rapportera korrelasjonar med menneske sprøyter.</abstract_no>
      <abstract_sr>Nastavljajuća neuralna revolucija u prevodu mašine je olakšala modelu većih konteksta izvan nivoa rečenica, što može potencijalno pomoći da riješi neke ambiguitete na nivou diskursa poput pronominalne anafore, tako da omogućava bolji prevod. Nažalost, čak i kad se ljudi smatraju rezultatima poboljšanjima značajnim, ostaju praktično nepoznati tradicionalnim mjerama automatske procjene poput BLEU, jer se na kraju utječe samo na nekoliko reči. Zato su potrebne specijalizovane mjere procjene. Sa ovim ciljem na umu, doprinosimo široku, ciljenu setu podataka koja se može koristiti kao test apartman za prevod pronoun, pokrivajući višestruke izvorne jezike i različite greške pronoun izvedene iz prevoda realnog sistema, za engleski. Dalje predlažemo mjeru procjene za razliku dobrih i loših prevoda. Takoðe vodimo studiju korisnika kako bi prijavili korelacije sa ljudskim sudovima.</abstract_sr>
      <abstract_pl>Trwająca rewolucja neuronowa w tłumaczeniu maszynowym ułatwiła modelowanie większych kontekstów poza poziomem zdań, co może potencjalnie pomóc rozwiązać niektóre niejasności na poziomie dyskursu, takie jak anafora zaimkowa, umożliwiając tym samym lepsze tłumaczenia. Niestety, nawet jeśli wynikające z tego usprawnienia są postrzegane przez ludzi jako istotne, pozostają one praktycznie niezauważone przez tradycyjne środki automatycznej oceny, takie jak BLEU, ponieważ dotyczy to tylko kilku słów. Potrzebne są zatem specjalistyczne środki oceny. Mając na uwadze ten cel, wnosimy obszerny, ukierunkowany zestaw danych, który może być wykorzystywany jako zestaw testowy do tłumaczenia zaimków, obejmujący wiele języków źródłowych i różne błędy zaimków wyciągnięte z rzeczywistych tłumaczeń systemowych dla angielskiego. Ponadto proponujemy środek oceny, aby odróżnić dobre i złe tłumaczenia zaimków. Przeprowadzamy również badania użytkowników w celu zgłaszania korelacji z osądami ludzkimi.</abstract_pl>
      <abstract_ta>இயந்திர மொழிபெயர்ப்பில் நடக்கும் புதிய புரட்சியை சுலபமாக்கி விசாரண- மட்டத்திற்கு முன்னால் பெரிய புரட்சிகளை மாற்றுவதற்கு எளிதாக்கி விட்டது, அது சில துரதிர்ஷ்டவசமாக, முடிவுகள் மனிதர்களால் பெரிய முன்னேற்றங்களாக பார்க்கப்படும் போதும், பிலியு போன்ற சில வார்த்தைகள் பாதிக்கப்படும் போதும ஆகையால், குறிப்பிட்ட மதிப்பீடு அளவுகள் தேவைப்படுகிறது. இந்த இதயத்தில், நாம் ஒரு விரிவான, இலக்கப்பட்ட தகவல் அமைப்பு சோதனை மொழிபெயர்ப்பிற்காக பயன்படுத்த முடியும், பல மூல மொழிகளை மற்றும் உண்மையான மொழிபெயர நன்மையையும் கெட்ட மொழிபெயர்ப்புகளையும் வேறுபாடு செய்ய நாம் மேலும் ஒரு மதிப்பீடு அளவையும் பரிந நாம் மனித த தீர்ப்புகளுடன் தொடர்புகளை அறிவிக்க ஒரு பயனர் படிப்பு செய்கிறோம்.</abstract_ta>
      <abstract_si>The ongoing neural revital in machine translation has done it more Easy to Model Large Contexts than the dictionary-level, that can be assistive in resolution of a number of speakers-level non-guiguities, e.g. express anaphora, so allowing better translation. අවාසනාවන්තයෙන්, මිනිස්සු වලින් ප්‍රමාණයක් දැකලා තියෙනවා නමුත්, ඔවුන් ප්‍රමාණයෙන්ම ස්වයංක්‍රීය විශ්වාසිත විශ්වාසිත වි ඉතින්, විශේෂ විශේෂ විශේෂ විශේෂණ අවශ්‍යයි. මේ අදහසක් මතකයේ තියෙන්නේ, අපි ප්‍රමාණය, ලක්ෂිත දත්ත සෙට් එකක් සම්බන්ධ කරන්න පුළුවන් විදිහට පරීක්ෂණාවක් වෙනුවෙන් පරීක්ෂණාවක්  අපි තවදුරටත් විශ්වාස කරන්න පුළුවන් හොඳයි නරක ප්‍රවේශනයක් වෙනස් කරන්න. අපි පරීක්ෂකයෙක් අධ්‍යානය කරනවා මිනිස්සු විශ්වාසයෙන් සම්බන්ධතාවක් වාර්තා කරන්න.</abstract_si>
      <abstract_sv>Den pågående neurala revolutionen inom maskinöversättning har gjort det lättare att modellera större kontexter bortom meningsnivån, vilket potentiellt kan hjälpa till att lösa vissa diskursnivåtvetydigheter såsom pronominal anafora, vilket möjliggör bättre översättningar. Tyvärr förblir de nästan obemärkta av traditionella automatiska utvärderingsåtgärder som BLEU, även när de resulterande förbättringarna uppfattas som betydande av människor, eftersom bara några få ord påverkas. Därför behövs särskilda utvärderingsåtgärder. Med detta mål i åtanke bidrar vi med en omfattande, målinriktad datauppsättning som kan användas som en testsvit för pronomens översättning, som täcker flera källspråk och olika pronomens fel hämtade från verkliga systemöversättningar, för engelska. Vi föreslår vidare en utvärderingsåtgärd för att skilja mellan bra och dåliga pronomens översättningar. Vi genomför även en användarstudie för att rapportera korrelationer med mänskliga bedömningar.</abstract_sv>
      <abstract_so>Ruushka neurada ee ku socda turjumidda mashiinka ayaa si fudud u sahlisay in la sameyno tartanka waaweyn ee ka dhaafa heerka ereyga, taasoo suurtagal ah u caawin karta inay xambaaraan khilaafka hadalka, tusaale ahaan anaphora-ka-dirka ah, sidaa darteed waxay u sahlan kartaa turjumaadyo wanaagsan. Nasiib xumaa, xataa marka horumarinta loo arko sida dadka oo kale, waxay ku jiraan mid aan si dhab ah uga maahmin qiyaastii asalka ah sida BLEU oo kale, sababtoo ah in dhawr erayo oo keliya ay saameyn ku dhamaadaan. Sidaas darteed waxaa loo baahan yahay qiyaastii gaarka ah. Markaas waxan darteed waxaynu ka qeybqaadanaynaa sawir aad u ballaadhan oo la hagay, kaas oo looga isticmaali karo koob turjumid ah, oo ku qoran luqado badan oo sourceed ah iyo qaladyo kala duduwan oo laga soo qoray turjumaadda runta ah, Ingiriis. Sidoo kale waxaynu soo jeedinnaa qiyaastii si aan u kala soocaano turjumaadda wanaagsan oo xun. Sidoo kale waxaynu sameynaa waxbarasho isticmaalayaal si aan u ogeysiinno xiriirka xukummada dadka.</abstract_so>
      <abstract_ur>ماشین ترجمہ میں ادھر رہے نئورل انقلاب نے مجلس سطح کے بعد بہت بڑے کنٹکس ڈالنے کے لئے آسان بنایا ہے، جو بہترین ترجمہ کو حل کرنے کی مدد کرسکتا ہے، جیسے نامزد انفورا، اس کے ذریعہ بہتر ترجمہ کرنے کی امکان دیتا ہے. بدبختی کے ساتھ، اگرچہ انسان کے ذریعے نتیجے کی سوداگری بہت اچھی طرح دیکھی جاتی ہیں، وہ بالکل بے سمجھ رہے ہیں، جیسے بلیوس کی طرح سیدھی سیدھی ارزیابی اندازے، جیسے کچھ کلمات صرف تھوڑی سی باتوں پر اثر ہو جاتی ہیں. اسی طرح، مخصوص ارزیابی اندازے ضرورت ہیں۔ یہ ذهن کے مطابق، ہم ایک گھیرے، موجب ڈاٹ سٹ کے ساتھ اضافہ کر رہے ہیں جس کا استعمال کرنا چاہتا ہے ایک ٹیسٹ سوئٹ کے طور پر، بہت سی سراسر زبانیں اور مختلف ٹیسٹ ٹیسٹ سے، انگلیسی کے لئے اضافہ کر سکتے ہیں. ہم اس کے بعد ایک ارزش اندازہ پیش کریں گے کہ بھلائی اور بد ترجمہ اختلاف کریں۔ ہم نے بھی انسان کے فیصلے کے ساتھ تعلقات کی راپور کرنے کے لئے ایک کارساز کی تحقیق کرتا ہے۔</abstract_ur>
      <abstract_uz>The ongoing neural revolution in machine translation has made it easier to model larger contexts beyond the sentence-level, which can potentially help resolve some discourse-level ambiguities such as pronominal anaphora, thus enabling better translations.  Afsuski, natijalar odamlar asosiy o'zgarishlarni ko'rib turganda, ular balki oddiy avtomatik qiymatlashni o'ylab qoladi, balki faqat bir necha so'zlar bunga o'zgarib keladi. Шундай қилиб, махсус қийматга назар солиш зарур. Bu maqsadda, biz bir xil tizim tarjimalari uchun foydalanish mumkin, bir nechta manba tillarini koʻchirish va haqiqiqiy tizim tarjimalaridan boshqa notoʻgʻri qoʻllaniladik. Biz yaxshi va muvaffaqiyatli tarjimalarni o'zgartirib chiqarishni davom qilamiz. Biz inson xususiyatlari bilan bog'liqni tahrirlash uchun foydalanuvchi o'qituvchini bajaramiz.</abstract_uz>
      <abstract_vi>Sự cách mạng thần kinh liên tục trong phiên dịch cỗ máy đã làm cho dễ mô hình các liên quan lớn hơn vượt qua mức án, mà có thể giúp giải quyết một số mơ hồ cấp độ thuyết giáo như sơ, như thông lỗ đít đại khái, để có thể dịch tốt hơn. Thật không may, ngay cả khi cải tiến hậu quả được cho là rất lớn bởi con người, chúng hầu như không bị chú ý bởi các biện pháp đánh giá tự động truyền thống như May Mắn, vì chỉ có vài từ sẽ bị ảnh hưởng. Do đó, cần phải có biện pháp đánh giá đặc biệt. Với mục tiêu này, chúng tôi đóng góp một bộ dữ liệu đầy đủ, mục tiêu có thể sử dụng làm phòng thí nghiệm dịch đại từ, bao gồm ngôn ngữ đa nguồn và các từ sai lầm phát từ hệ thống thật, cho tiếng Anh. Chúng tôi còn đề xuất một biện pháp đánh giá để phân biệt dịch đại từ tốt và xấu. Chúng tôi cũng tiến hành một nghiên cứu người dùng để báo cáo liên quan đến phán xét.</abstract_vi>
      <abstract_bg>Продължаващата невронна революция в машинния превод улесни моделирането на по-големи контексти извън нивото на изречение, което потенциално може да помогне за разрешаването на някои неясноти на ниво дискурс, като например произнобната анафора, като по този начин позволява по-добри преводи. За съжаление, дори когато получените подобрения се считат за значителни от хората, те остават практически незабелязани от традиционните автоматични мерки за оценка като Блеу, тъй като само няколко думи в крайна сметка са засегнати. По този начин са необходими специализирани мерки за оценка. С тази цел допринасяме за обширен, целенасочен набор от данни, който може да се използва като тест комплект за превод на местоимения, обхващащ множество изходни езици и различни грешки на местоименията, извлечени от реални системни преводи, за английски език. Освен това предлагаме мярка за оценка за разграничаване на добрите и лошите преводи на местоименията. Също така провеждаме потребителско проучване, за да докладваме корелации с човешките преценки.</abstract_bg>
      <abstract_nl>De aanhoudende neurale revolutie in machinevertaling heeft het gemakkelijker gemaakt om grotere contexten te modelleren buiten het zinnenniveau, wat mogelijk kan helpen sommige discours-niveau ambiguïteiten zoals pronominale anafora op te lossen, waardoor betere vertalingen mogelijk zijn. Helaas blijven de daaruit voortvloeiende verbeteringen vrijwel onopgemerkt door traditionele automatische evaluatiemaatregelen zoals BLEU, zelfs als de mens de daaruit voortvloeiende verbeteringen als substantieel beschouwt, aangezien slechts een paar woorden worden beïnvloed. Daarom zijn gespecialiseerde evaluatiemaatregelen nodig. Met dit doel in het achterhoofd dragen we een uitgebreide, doelgerichte dataset bij die kan worden gebruikt als een testsuite voor de vertaling van voornaamwoorden, die meerdere brontalen en verschillende voornaamwoordfouten omvat die zijn getrokken uit echte systeemvertalingen, voor het Engels. Verder stellen we een evaluatiemaatregel voor om goede en slechte voornaamwoordvertalingen te onderscheiden. We voeren ook een gebruikersstudie uit om correlaties met menselijke oordelen te rapporteren.</abstract_nl>
      <abstract_da>Den igangværende neurale revolution i maskinoversættelse har gjort det lettere at modellere større sammenhænge ud over sætningsniveauet, hvilket potentielt kan hjælpe med at løse nogle diskursniveau tvetydigheder såsom pronominal anafora, hvilket muliggør bedre oversættelser. Desværre forbliver de næsten ubemærket af traditionelle automatiske evalueringsforanstaltninger som BLEU, selv når de resulterende forbedringer betragtes som væsentlige af mennesker, da kun få ord ender med at blive påvirket. Der er således behov for specialiserede evalueringsforanstaltninger. Med dette mål i tankerne bidrager vi med et omfattende, målrettet datasæt, der kan bruges som en testpakke til stedord oversættelse, der dækker flere kildesprog og forskellige stedord fejl trukket fra rigtige systemoversættelser, til engelsk. Vi foreslår endvidere en evalueringsforanstaltning for at skelne mellem gode og dårlige stedord oversættelser. Vi gennemfører også en brugerundersøgelse for at rapportere korrelationer med menneskelige vurderinger.</abstract_da>
      <abstract_hr>Nastavljajuća neurološka revolucija u prevodu strojeva olakšala je modelovanje većih konteksta izvan razine rečenica, što može potencijalno pomoći riješiti neke nesposobnosti na razini diskursa poput pronominalne anafore, tako omogućavajući bolje prevode. Nažalost, čak i kad se ljudi smatraju rezultatima poboljšanjima značajnim, ostaju praktično nepoznati tradicionalnim mjerama automatske procjene poput BLEU-a, jer se na kraju utjecaju samo na nekoliko riječi. Stoga su potrebne specijalizirane mjere procjene. S tim ciljem na umu, doprinosimo širokoj, ciljnoj seti podataka koji se može koristiti kao test apartman za prevod pronoun, pokrivajući višestruke izvorne jezike i različite greške pronoun izvedene iz prevoda pravih sustava za engleski jezik. Dalje predlažemo mjeru procjene za razliku dobrih i loših prevoda. Također provodimo studiju korisnika kako bi prijavili korelacije s ljudskim osuđivanjem.</abstract_hr>
      <abstract_de>Die fortschreitende neuronale Revolution in der maschinellen Übersetzung hat es einfacher gemacht, größere Kontexte jenseits der Satzebene zu modellieren, was möglicherweise dazu beitragen kann, einige Mehrdeutigkeiten auf Diskursebene wie pronominale Anaphora zu lösen und so bessere Übersetzungen zu ermöglichen. Leider bleiben die daraus resultierenden Verbesserungen, selbst wenn sie vom Menschen als wesentlich angesehen werden, bei herkömmlichen automatischen Auswertungsmaßnahmen wie BLEU praktisch unbemerkt, da nur wenige Worte davon betroffen sind. Daher sind spezielle Evaluierungsmaßnahmen erforderlich. Zu diesem Zweck tragen wir einen umfangreichen, zielgerichteten Datensatz bei, der als Testsuite für die Pronomen-Übersetzung verwendet werden kann, der mehrere Ausgangssprachen und verschiedene Pronomen-Fehler abdeckt, die aus realen Systemübersetzungen stammen. Weiterhin schlagen wir eine Bewertungsmaßnahme vor, um gute und schlechte Pronomen-Übersetzungen zu unterscheiden. Wir führen auch eine Nutzerstudie durch, um Zusammenhänge mit menschlichen Urteilen zu melden.</abstract_de>
      <abstract_ko>기계 번역에서 현재 진행되고 있는 신경 혁명은 문장 차원 외에 더욱 큰 언어 환경을 구축하기 쉽고 일부 문장 차원의 잘못된 의미, 예를 들어 대명사 반지를 해결하여 더욱 좋은 번역을 실현하는 데 도움이 될 수 있다.불행하게도, 인류는 이로 인한 개선이 실질적이라고 생각하지만, BLEU와 같은 전통적인 자동 평가 방법은 이러한 개선에 거의 주의하지 않는다. 왜냐하면 몇 개의 단어만 최종적으로 영향을 받기 때문이다.따라서 전문적인 평가 조치가 필요하다.이 목표를 고려하여 우리는 광범위하고 목적성이 있는 데이터 집합을 제공하여 대명사 번역의 테스트 세트로 여러 가지 원시 언어와 실제 시스템 번역에서 추출한 서로 다른 대명사 오류를 포함하여 영어에 적용할 수 있다.우리는 좋은 것과 나쁜 것을 구분하는 대명사 번역을 위한 평가 기준을 한층 더 제시했다.우리는 인간의 판단과 관련성을 보고하기 위해 사용자 연구도 진행했다.</abstract_ko>
      <abstract_fa>انقلاب عصبی در ترجمه ماشین در حال حاضر به مدل موضوع بزرگ فراتر از سطح جمله آسان ساخته است که ممکن است کمک کند تا برخی غیرقابل گفتگوی سطح سخنرانی مثل آنفورا معمولی حل کند، بنابراین به ترجمه‌های بهتر تواند کرد. متأسفانه، حتی هنگامی که توسط انسان به عنوان توسعه‌های نتیجه‌ی توسعه‌ی توسعه‌ی توسعه‌ی توسعه‌ی توسعه‌ی توسعه‌ی توسعه‌ی توسعه‌ی توسعه‌ی توسعه‌ی توسعه‌های تحلیل‌ بنابراین، مقدار ارزیابی متخصص نیاز دارند. با این هدف در ذهن، ما یک مجموعه داده های گسترده و هدف داریم که می تواند به عنوان suite آزمایش برای ترجمه‌های فرانسوی استفاده شود، با زبانهای متعدد منبع و اشتباه‌های متفاوتی از ترجمه‌های سیستم واقعی، برای انگلیسی استفاده شود. ما پیشنهاد می‌کنیم یک اندازه ارزیابی برای فرق کردن ترجمه‌های خوب و بد. ما همچنین یک مطالعه کاربر را برای گزارش ارتباطات با قاعدات انسان انجام می دهیم.</abstract_fa>
      <abstract_id>Revolusi saraf yang sedang berlangsung dalam terjemahan mesin telah memudahkan model konteks yang lebih besar di luar tingkat kalimat, yang potensial dapat membantu memecahkan beberapa ambiguitas tingkat diskors seperti anafora pronominal, sehingga memungkinkan terjemahan yang lebih baik. Sayangnya, bahkan ketika peningkatan yang berasal dilihat sebagai substansial oleh manusia, mereka tetap hampir tidak diperhatikan oleh tindakan evaluasi otomatis tradisional seperti BLEU, karena hanya beberapa kata akhirnya terpengaruh. Jadi, tindakan evaluasi khusus diperlukan. With this aim in mind, we contribute an extensive, targeted dataset that can be used as a test suite for pronoun translation, covering multiple source languages and different pronoun errors drawn from real system translations, for English.  Kami lebih lanjut mengusulkan ukuran evaluasi untuk membedakan terjemahan yang baik dan buruk. Kami juga melakukan penelitian pengguna untuk melaporkan korelasi dengan penilaian manusia.</abstract_id>
      <abstract_sw>The ongoing neural revolution in machine translation has made it easier to model larger contexts beyond the sentence-level, which can potentially help resolve some discourse-level ambiguities such as pronominal anaphora, thus enabling better translations.  Kwa bahati mbaya, hata kama maboresho ya matokeo yanapoonekana kuwa ni muhimu na binadamu, bado hubaki kutokuelewana na hatua za kutathmini za kitamaduni kama vile BLEU, kwa kuwa maneno machache yanaishia kuathirika. Kwa hiyo, hatua maalum za uchunguzi zinahitaji. Kwa lengo hili la akili, tunachangia seti ya taarifa zinazolengwa na lengo ambazo zinaweza kutumika kama kituo cha jaribio kwa kutafsiri utafiti mkubwa, wakiandika lugha mbalimbali za vyanzo na makosa tofauti yaliyotolewa kutoka kwenye tafsiri halisi ya mfumo, kwa Kiingereza. Tunazipendekeza zaidi hatua ya uchunguzi ili kutofautisha tafsiri nzuri na mbaya zinazoelezwa. Pia tunafanya utafiti wa mtumiaji kutangaza uhusiano na sheria za binadamu.</abstract_sw>
      <abstract_sq>Revolucioni neural në vazhdim në përkthimin e makinave e ka bërë më të lehtë të modelohet kontekste më të mëdha përtej nivelit të fjalës, që mund të ndihmojë në zgjidhjen e disa ambiguateteve të nivelit të diskursit të tilla si anafora pronominale, duke mundësuar kështu përkthime më të mira. Unfortunately, even when the resulting improvements are seen as substantial by humans, they remain virtually unnoticed by traditional automatic evaluation measures like BLEU, as only a few words end up being affected.  Kështu, janë të nevojshme masa të specializuara vlerësimi. Me këtë qëllim në mendje, ne kontribuojmë një grup të dhënash të gjerë, të synuar që mund të përdoret si një grup testimi për përkthimin e pronumeve, duke mbuluar gjuhë të shumta burimesh dhe gabime të ndryshme pronumesh të nxjerra nga përkthimet e vërtetë të sistemit, për anglisht. Ne propozojmë më tej një masë vlerësimi për të dalluar përkthimet e mira dhe të këqija. Ne kryejmë gjithashtu një studim përdorues për të raportuar korrelacionet me gjykimet njerëzore.</abstract_sq>
      <abstract_tr>Maşynyň terjimesinde geçýän näural revolusiýasy sözlem derejesinden öňünde uly contextler örän aňsatlyk bilen döredi. Bu da beter terjime edenler ýaly käbir gürrüň derejesini çözmek üçin kömek edip biler. Gynansakda, netijeli gelişmeler adamlar tarafından örän möhüm bolup görünende bile, BLEU ýaly geleneksel awtomatik değerlendirme ölçülerinden bilinmedikleri üçin birkaç söz diňe täsir edildi. Şonuň üçin aýratyn deňlenme çözgütleri gerek. Bu maksady aklında, biz esasy sistemiň terjimelerinden ullanýan, be ýleki maksadatly data setegi üçin ullanýan teste takmynasyna kömekleýäris. Biz gowy we ýalňyş sözleri döwletmek üçin deňlenme ölçüsini teklip edip görýäris. Biz ýöne adamlaryň häsiýetleri bilen baglaşyklaryny bildirmek üçin ulanylaryň adyny çykarýarys.</abstract_tr>
      <abstract_af>Die voortgaande neurale revolusie in masjien vertaling het dit makliker gemaak om groter konteks te model buite die setvlak, wat potensieel kan help om sommige diskurse-vlak ambiguities soos pronominal anaphora te oplos, sodat dit beter vertalings aanwend word. Ongelukkig, selfs wanneer die resulteerde verbeteringe as substantieel deur mense gesien word, bly hulle virtueel onbekend deur tradisionele outomatiese evalueringsmaatskappe soos BLES, as slegs 'n paar woorde eindig word om te beëindig. So is spesialiseerde evalueringsmaat nodig. Met hierdie doel in gedagte, bydra ons 'n uitbreidige, doelde datastel wat kan gebruik word as 'n toets suite vir pronoun vertaling, omdekking veelvuldige bron tale en verskillende pronoun foute geteken van werklike stelsel vertalings, vir Engels. Ons beveel verder 'n evalueringsmaat om goeie en slegte uitspraak vertalings te verander. Ons het ook 'n gebruiker studie gedoen om verbindings met menslike verordeninge te rapporteer.</abstract_af>
      <abstract_am>የመኪና ትርጉም ውስጥ የሚሄደውን የነዌብ የውሃት ዓመፅ ከክፍሉ ደረጃ በላይ ትልቁ ሁኔታዎችን ማሳየት አቀላል፡፡ በርግጥ፣ ፍጻሜው የሰው አካባቢ ሆኖ ቢታዩ፣ እንደባሕላዊ ባሕላዊ አውቶማቲካዊ ማስታወቂያ በተለይ ይኖራል፣ ጥቂት ቃላት ብቻ ሲቀሩ ነው፡፡ ስለዚህ የግዛት ማስታወቂያ መጠቀሚያ ያስፈልጋል፡፡ በዚህ ምክንያት አካሄዱ፣ የስፋት፣ የደረጃ መረጃዎች ማዕከላዊ ትርጓሜ እንዲሆን የሚጠቀሙትን የድምፅ ጉዳይ እና የመስመር ትርጓሜዎች የተለየ ብዙዎች የsource ቋንቋዎች እና የተለያዩ የስህተት ስህተት እና እንግሊዝኛ ነው፡፡ ደግሞም መልካሙንና ክፉ ትርጓሜዎችን ለመለየት የመረጃ መስፈሪያ እናሳልጋለን፡፡ እና የሰው ፍርድ ግንኙነት ለመውሰድ የተጠቃሚውን ትምህርት እናደርጋለን፡፡</abstract_am>
      <abstract_hy>Մեքենայի թարգմանման ներկայիս նյարդային հեղափոխությունը ավելի հեշտացրեց մոդելավորել ավելի մեծ կոնտեքստներ նախադասության մակարդակից դուրս, ինչը պոտենցիալ կարող է օգնել լուծել որոշ խոսակցային մակարդակի անհավասարություններ, ինչպիսիք են պրոնոմինալ անաֆոր Unfortunately, even when the resulting improvements are seen as substantial by humans, they remain virtually unnoticed by traditional automatic evaluation measures like BLEU, as only a few words end up being affected.  Այսպիսով, անհրաժեշտ են հատուկ գնահատման միջոցներ: Այս նպատակով մենք ներդրում ենք մի էքսպանցիոնալ, նպատակային տվյալների համակարգ, որը կարող է օգտագործվել որպես փորձարկման համակարգ արտահայտված թարգմանման համար, որը ներառում է բազմաթիվ աղբյուրների լեզուներ և տարբեր արտահայտված սխալներ, որոնք արտահայ Մենք նաև առաջարկում ենք գնահատման չափում, որպեսզի տարբերակենք լավ և վատ արտահայտված թարգմանությունները: Մենք նաև կատարում ենք օգտագործողների ուսումնասիրություն, որպեսզի տեղեկացնենք կապը մարդկային դատողությունների հետ:</abstract_hy>
      <abstract_az>Makinelərin çevirindəki nöral revolusyon, cümlənin səviyyəsindən daha böyük məlumatları modelləşdirmək üçün daha asanlaşdırdı. Bu, mümkün olaraq, anafora kimi bəzi söhbət səviyyəsini çəkməyə kömək edə bilər, böylece daha yaxşı tercümələri fərqləndirər. Maalesef ki, insanların sonuçları böyük təmizlənəcəyi zaman, onlar BLEU kimi nəticəli avtomatik təmizləmə ölçüləri ilə çox dərk edilməmişdir, çünki ancaq bir neçə söz təsirlənir. Beləliklə, xüsusiyyətli değerlendirmə ölçüləri lazımdır. Bu məqsəd fikrində, biz həqiqət sistem tercümələrindən çəkilən, İngilizce üçün istifadə edilə biləcək çoxlu mənbə dillərini və müxtəlif sözlərin xətalarını daxil edirik. Biz yaxşı və pis sözlərin tercümələrini dəyişdirmək üçün değerlənmə ölçüsünü təklif edirik. Biz də insanların hökmləri ilə bağlantıları bildirmək üçün istifadəçi təhsil edirik.</abstract_az>
      <abstract_bn>মেশিন অনুবাদের চলমান নিউরেল বিপ্লবের বাক্য-স্তরের বাইরে বড় প্রতিযোগিতাকে মডেল করার জন্য সহজ করে দিয়েছে, যা সম্ভাব্য কিছু কথোপকথন-পর্যায়ের উদ্দেশ্য সমাধান দুর্ভাগ্যবশত, যদিও ফলাফলের উন্নয়ন মানুষের দ্বারা গুরুত্বপূর্ণ হিসেবে দেখা যায়, তারপরেও তারা ঐতিহ্যবাহী স্বয়ংক্রিয়ভাবে মূল্যায়নের মাধ Thus, specialized evaluation measures are needed.  এই উদ্দেশ্যের মাধ্যমে আমরা একটি বিস্তারিত, লক্ষ্য করা ডাটাসেট যা প্রধান অনুবাদের জন্য একটি পরীক্ষা স্যুট হিসেবে ব্যবহার করা যাবে, যা কার্যক্রমে বেশ কয়ে আমরা আরো একটি মূল্য প্রস্তাব করি ভালো এবং খারাপ অনুবাদ বিচ্ছিন্ন করার জন্য। আমরা মানুষের বিচারের সাথে সম্পর্ক রিপোর্ট করার জন্য একজন ব্যবহারকারী গবেষণা করি।</abstract_bn>
      <abstract_bs>Nastavljajuća neuralna revolucija u prevodu strojeva je olakšala modelu većih konteksta izvan nivoa rečenica, što može potencijalno pomoći da riješi neke ambiguitete na nivou diskursa poput pronominalne anafore, tako omogućavajući bolje prevode. Nažalost, čak i kad se ljudi smatraju rezultatima poboljšanjima značajnim, ostaju praktično nepoznati tradicionalnim mjerama automatske procjene poput BLEU-a, jer se na kraju utjecaju samo na nekoliko riječi. Stoga su potrebne specijalizirane mjere procjene. S tim ciljem na umu, doprinosimo široku, ciljnu kompletu podataka koja se može koristiti kao test apartman za prevod pronoun, pokrivanje višestrukih izvorskih jezika i različitih grešaka pronoun izvedenih iz pravog prevoda sistema za engleski jezik. Dalje predlažemo mjeru procjene za razliku dobrih i loših prevoda. Također vodimo studiju korisnika kako bi prijavili korelacije sa ljudskim osuđivanjima.</abstract_bs>
      <abstract_ca>La revolució neural en curs en la traducció màquina ha fet més fàcil modelar contextes més grans més enllà del nivell de frases, que poden ajudar a resoldre algunes ambigüitats del nivell de discurs, com l'anafora pronominal, permetant millors traduccions. Malauradament, fins i tot quan els humans consideren les millors resultants substancials, encara són virtualment desconeguts per mesures d'evaluació automàtica tradicionals com la BLEU, perquè només unes quantes paraules acaben afectades. Així que es necessiten mesures especialitzades d'evaluació. Tenint en compte aquest objectiu, contribuim a un conjunt de dades extens i mirats que es pot utilitzar com una suite de provas per traducció pronòmica, que cobreix múltiples llengües de fonts i diferents errors pronòmics derivats de traduccions reals del sistema, en anglès. Proposem també una mesura d'evaluació per diferenciar les traduccions bones i dolentes. També fem un estudi d'usuari per informar de les correlacions amb els judicis humans.</abstract_ca>
      <abstract_cs>Probíhající neuronová revoluce v strojovém překladu usnadnila modelování větších kontextů mimo úroveň věty, což může potenciálně pomoci vyřešit některé nejasnosti na úrovni diskurzu, jako je pronominální anafora, a tím umožnit lepší překlady. Bohužel, i když jsou výsledná zlepšení lidmi považována za významná, zůstávají prakticky nepozorovaná tradičními automatickými hodnotícími opatřeními, jako je BLEU, neboť je nakonec ovlivněna jen několik slov. Proto jsou zapotřebí specializovaná hodnotící opatření. S tímto cílem přispíváme rozsáhlý, cílený datový soubor, který může být použit jako testovací sada pro překlad zájmen, pokrývající více zdrojových jazyků a různé chyby zájmen čerpané z reálných systémových překladů, pro angličtinu. Dále navrhujeme hodnotící opatření pro rozlišení dobrých a špatných překladů zájmen. Dále provádíme uživatelskou studii, abychom nahlásili korelace s lidskými úsudky.</abstract_cs>
      <abstract_et>Käimasolev neurorevolutsioon masintõlkes on lihtsustanud suuremate kontekstide modelleerimist väljaspool lausetaset, mis võib potentsiaalselt aidata lahendada mõningaid diskursustaseme ebamäärasusi, nagu pronominaalne anafoor, võimaldades seega paremaid tõlkeid. Kahjuks jäävad need traditsioonilised automaatsed hindamismeetmed, nagu BLEU, praktiliselt märkamatuks isegi siis, kui inimesed peavad sellest tulenevaid parandusi märkamatuks, kuna see mõjutab vaid mõnda sõna. Seega on vaja spetsiaalseid hindamismeetmeid. Seda eesmärki silmas pidades anname kaasa ulatusliku, sihipärase andmekogumi, mida saab kasutada pronounitõlke testikomplektina, mis hõlmab mitmeid lähtekeeli ja erinevaid reaalsetest süsteemitõlketest saadud pronounitõrkeid inglise keele jaoks. Lisaks pakume välja hindamismeetmed, et eristada häid ja halbu asenimetõlkeid. Samuti teeme kasutajate uuringu, et teatada seostest inimeste otsustega.</abstract_et>
      <abstract_fi>Konekäännöksen hermovallankumous on helpottanut lausetason laajempien kontekstien mallintamista, mikä voi mahdollisesti auttaa ratkaisemaan joitakin diskurssitason epäselvyyksiä, kuten pronominaalista anaforaa, mahdollistaen näin paremman käännöksen. Valitettavasti jopa silloin, kun ihmiset katsovat, että parannukset ovat merkittäviä, perinteiset automaattiset arviointitoimenpiteet, kuten BLEU, eivät huomaa niitä käytännössä, sillä niihin vaikuttaa vain muutama sana. Siksi tarvitaan erityisiä arviointitoimenpiteitä. Tätä tarkoitusta silmällä pitäen annamme kattavan, kohdennetun aineiston, jota voidaan käyttää pronominkäännöksen testikokonaisuutena, joka kattaa useita lähdekieliä ja erilaisia pronominivirheitä, jotka on saatu todellisista järjestelmäkäännöksistä, englanniksi. Lisäksi ehdotamme arviointitoimenpidettä hyvien ja huonojen pronominkäännösten erottamiseksi toisistaan. Teemme myös käyttäjätutkimuksen raportoidaksemme korrelaatioita ihmisten arvosteluihin.</abstract_fi>
      <abstract_sk>Tekoča nevronska revolucija v strojnem prevajanju je olajšala modeliranje večjih kontekstov zunaj stavkovne ravni, kar lahko potencialno pomaga odpraviti nekatere dvoumnosti na ravni diskurza, kot je pronominalna anafora, in tako omogočiti boljše prevode. Na žalost, tudi če ljudje vidijo, da so izboljšave bistvene, jih tradicionalni avtomatski ukrepi ocenjevanja, kot je BLEU, skoraj neopaženi, saj na koncu prizadene le nekaj besed. Zato so potrebni specializirani ukrepi ocenjevanja. S tem ciljem prispevamo obsežen, ciljno usmerjen nabor podatkov, ki se lahko uporablja kot testni komplet za prevajanje zaimkov, ki zajema več izvornih jezikov in različne zaimkovne napake, pridobljene iz resničnih sistemskih prevodov, za angleščino. Predlagamo tudi ocenjevalni ukrep za razlikovanje dobrih in slabih zaimkovnih prevodov. Izvajamo tudi uporabniško študijo za poročanje korelacij s človeškimi presojami.</abstract_sk>
      <abstract_he>המהפכה העצבית הממשיכת בתרגום המכונה קלה יותר לדוגמא קשרים גדולים מעבר לרמת המשפטים, אשר יכול לעזור לפתור כמה סביבות רמת דיבורים כמו אנפורה פרומונמינלית, כך מאפשר תרגומות טובות יותר. למרבה הצער, אפילו כאשר השיפורים הנוצאים נראים משמעותיים על ידי בני אדם, הם נשארים כמעט לא שמים לב על ידי אמצעי הערכה אוטומטיים מסורתיים כמו BLEU, כי רק כמה מילים בסופו של דבר נפגעו. כך, אמצעי הערכה מיוחדים נדרשים. בהתחשב במטרה הזו, אנחנו תורמים קבוצת נתונים ממוקדת ומתוקפת שאפשר להשתמש בתור סוויטה מבחן לתרגום מוכר, מכסה שפות מקורות רבות ושגיאות מוכרות שונות ממתרגומות מערכת אמיתיות, לאנגלית. אנו מציעים עוד מדידת הערכה כדי להפריד התרגשות טובות ורעות. אנחנו גם מבצעים מחקר משתמשים כדי לדווח על קשרים עם שיפוטים אנושיים.</abstract_he>
      <abstract_ha>Tsarin neurar da ke tafiya cikin fassarar masu ƙaranci ya sauƙaƙara ka motsar matsalari masu ƙaranci bakin-daraja, wanda zai iya amfani da yin cire wasu ambigui na magana kamar anafora na kanana, don haka yana ƙara fassarori mafi alhẽri. Babu'am, kõ da dai an ga mafarinta kamar mutane, sai su bar su a kan yin amfani da ƙaddara na ɗabi'a kamar BLEU, kuma amma sai kaɗan na ƙare su kan yin zargi. Sabõda haka, ana ƙayyade ƙaddara masu ƙaddara. Ga wannan hankalin, Munã ƙara da tsarin da aka faɗa ɗa, wanda za'a iya amfani da shi kamar wata fitina wa fassarar da za'a faɗaɗa, sunã rufe harshen masu yawa da shiryoyin inganci masu motsa daga fassarar-na'urar gaskiya, wa Ingiriya. Kayya, Munã goyya wata evaluci dõmin ka rarraba fassarar da mai kyau da misãlan fassarar. Tuna sami wani karatun mai amfani da shi dõmin mu yi bayani ga masu husũma da hukuncin mutane.</abstract_ha>
      <abstract_jv>Tulung ing Lalah, sampeyan ngono cah-cah sing paling-sistem nganggep dino sing paling-sistem anyar, nik awak dhéwé kuwi nggawe barang-sistem kuwi cah-sistem sing perusahaan sistem sing koyo "blo". Punika, awak dhéwé éntuk sing disimperasekan kanggo dianggap. Awak dhéwé nggalakno iki, kita kontribusi akeh bantên, bukêng-tanggal dataset sing bisa nggawe ngubah ujian kanggo tarjamahan kanggo perangkat itoleh, kawula-ujian karo sistem sing itoleh akeh basa sampeyan lan akeh perusahaan langkung sampeyan uga Awak dhéwé nggunakake kuwi nggawe tarjamahan kanggo nggawe luwih apik lan luwih apik. Awak dhéwé éntukno kelas penggunaké nggawe gerangkat ngéwangi mên karo jugal uwong.</abstract_jv>
      <abstract_bo>The ongoing neural revolution in machine translation has made it easier to model larger contexts beyond the sentence-level, which can potentially help resolve some discourse-level ambiguities such as pronominal anaphora, thus enabling better translations. ཡིན་ནའང་མི་ཆུང་པར། རྒྱལ་ཁབ་གི་ཡར་རྒྱས་འགྲོ་བཞིན་པའི་ཚད་ལྟར་རྐྱེན་པས། དེར་བརྟེན། དམིགས་བསལ་གྱི་ཞིབ་དཔྱད་ཚད་ལ་དགོས་པ་ཡིན། With this aim in mind, we contribute an extensive, targeted dataset that can be used as a test suite for pronoun translation, covering multiple source languages and different pronoun errors drawn from real system translations, for English. ང་ཚོས་རྗེས་མི་ཚིག་ཉེན་དང་བརྗོད་མེད་པར་ཉེན་རྐྱེན་གྱི་ཚད་གཞི་གཅིག་སྟོན་གྱི་ཡོད། ང་ཚོས་མི་དང་མི་དང་མཐུན་རྐྱེན་ལ་སྦྲེལ་བ་བྱེད་པའི་ལག་ལེན་པའི་གནད་དོན་ལ་ལྟ་སློང་བྱེད་དགོས།</abstract_bo>
      </paper>
    <paper id="295">
      <title>A Regularization Approach for Incorporating Event Knowledge and Coreference Relations into Neural Discourse Parsing</title>
      <author><first>Zeyu</first><last>Dai</last></author>
      <author><first>Ruihong</first><last>Huang</last></author>
      <pages>2976–2987</pages>
      <abstract>We argue that external commonsense knowledge and linguistic constraints need to be incorporated into neural network models for mitigating data sparsity issues and further improving the performance of discourse parsing. Realizing that external knowledge and linguistic constraints may not always apply in understanding a particular context, we propose a regularization approach that tightly integrates these constraints with contexts for deriving word representations. Meanwhile, it balances attentions over contexts and constraints through adding a regularization term into the <a href="https://en.wikipedia.org/wiki/Loss_function">objective function</a>. Experiments show that our knowledge regularization approach outperforms all previous systems on the benchmark dataset <a href="https://en.wikipedia.org/wiki/PDTB">PDTB</a> for discourse parsing.</abstract>
      <url hash="3a5ba893">D19-1295</url>
      <doi>10.18653/v1/D19-1295</doi>
      <bibkey>dai-huang-2019-regularization</bibkey>
    </paper>
    <paper id="299">
      <title>Enhancing Neural Data-To-Text Generation Models with External Background Knowledge</title>
      <author><first>Shuang</first><last>Chen</last></author>
      <author><first>Jinpeng</first><last>Wang</last></author>
      <author><first>Xiaocheng</first><last>Feng</last></author>
      <author><first>Feng</first><last>Jiang</last></author>
      <author><first>Bing</first><last>Qin</last></author>
      <author><first>Chin-Yew</first><last>Lin</last></author>
      <pages>3022–3032</pages>
      <abstract>Recent neural models for data-to-text generation rely on massive parallel pairs of data and text to learn the writing knowledge. They often assume that writing knowledge can be acquired from the training data alone. However, when people are writing, they not only rely on the data but also consider <a href="https://en.wikipedia.org/wiki/Common_knowledge_(logic)">related knowledge</a>. In this paper, we enhance neural data-to-text models with external knowledge in a simple but effective way to improve the fidelity of generated text. Besides relying on parallel data and text as in previous work, our model attends to relevant external knowledge, encoded as a temporary memory, and combines this <a href="https://en.wikipedia.org/wiki/Knowledge">knowledge</a> with the context representation of data before generating words. This allows the <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> to infer relevant facts which are not explicitly stated in the data table from an external knowledge source. Experimental results on twenty-one Wikipedia infobox-to-text datasets show our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a>, KBAtt, consistently improves a state-of-the-art <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> on most of the datasets. In addition, to quantify when and why <a href="https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning">external knowledge</a> is effective, we design a <a href="https://en.wikipedia.org/wiki/Metric_(mathematics)">metric</a>, KBGain, which shows a strong correlation with the observed performance boost. This result demonstrates the relevance of external knowledge and sparseness of original data are the main factors affecting system performance.</abstract>
      <url hash="39823ccc">D19-1299</url>
      <doi>10.18653/v1/D19-1299</doi>
      <bibkey>chen-etal-2019-enhancing</bibkey>
    </paper>
    <paper id="300">
      <title>Reading Like HER : Human Reading Inspired Extractive Summarization<fixed-case>HER</fixed-case>: Human Reading Inspired Extractive Summarization</title>
      <author><first>Ling</first><last>Luo</last></author>
      <author><first>Xiang</first><last>Ao</last></author>
      <author><first>Yan</first><last>Song</last></author>
      <author><first>Feiyang</first><last>Pan</last></author>
      <author><first>Min</first><last>Yang</last></author>
      <author><first>Qing</first><last>He</last></author>
      <pages>3033–3043</pages>
      <abstract>In this work, we re-examine the problem of extractive text summarization for long documents. We observe that the process of extracting summarization of human can be divided into two stages : 1) a rough reading stage to look for sketched information, and 2) a subsequent careful reading stage to select key sentences to form the summary. By simulating such a two-stage process, we propose a novel approach for extractive summarization. We formulate the <a href="https://en.wikipedia.org/wiki/Problem_solving">problem</a> as a contextual-bandit problem and solve it with policy gradient. We adopt a <a href="https://en.wikipedia.org/wiki/Convolutional_neural_network">convolutional neural network</a> to encode gist of paragraphs for rough reading, and a decision making policy with an adapted termination mechanism for careful reading. Experiments on the CNN and DailyMail datasets show that our proposed method can provide high-quality summaries with varied length, and significantly outperform the state-of-the-art extractive methods in terms of ROUGE metrics.</abstract>
      <url hash="29e7eb68">D19-1300</url>
      <attachment hash="18f8b250">D19-1300.Attachment.zip</attachment>
      <doi>10.18653/v1/D19-1300</doi>
      <bibkey>luo-etal-2019-reading</bibkey>
    </paper>
    <paper id="302">
      <title>NCLS : Neural Cross-Lingual Summarization<fixed-case>NCLS</fixed-case>: Neural Cross-Lingual Summarization</title>
      <author><first>Junnan</first><last>Zhu</last></author>
      <author><first>Qian</first><last>Wang</last></author>
      <author><first>Yining</first><last>Wang</last></author>
      <author><first>Yu</first><last>Zhou</last></author>
      <author><first>Jiajun</first><last>Zhang</last></author>
      <author><first>Shaonan</first><last>Wang</last></author>
      <author><first>Chengqing</first><last>Zong</last></author>
      <pages>3054–3064</pages>
      <abstract>Cross-lingual summarization (CLS) is the task to produce a summary in one particular language for a source document in a different language. Existing methods simply divide this task into two steps : <a href="https://en.wikipedia.org/wiki/Automatic_summarization">summarization</a> and <a href="https://en.wikipedia.org/wiki/Automatic_translation">translation</a>, leading to the problem of <a href="https://en.wikipedia.org/wiki/Error_propagation">error propagation</a>. To handle that, we present an end-to-end CLS framework, which we refer to as Neural Cross-Lingual Summarization (NCLS), for the first time. Moreover, we propose to further improve NCLS by incorporating two related tasks, monolingual summarization and <a href="https://en.wikipedia.org/wiki/Machine_translation">machine translation</a>, into the training process of CLS under <a href="https://en.wikipedia.org/wiki/Multi-task_learning">multi-task learning</a>. Due to the lack of supervised CLS data, we propose a round-trip translation strategy to acquire two high-quality large-scale CLS datasets based on existing monolingual summarization datasets. Experimental results have shown that our NCLS achieves remarkable improvement over traditional pipeline methods on both English-to-Chinese and Chinese-to-English CLS human-corrected test sets. In addition, NCLS with <a href="https://en.wikipedia.org/wiki/Multi-task_learning">multi-task learning</a> can further significantly improve the quality of generated summaries. We make our dataset and code publicly available here : http://www.nlpr.ia.ac.cn/cip/dataset.htm.</abstract>
      <url hash="9560f001">D19-1302</url>
      <doi>10.18653/v1/D19-1302</doi>
      <bibkey>zhu-etal-2019-ncls</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/ncls">NCLS</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/lcsts">LCSTS</pwcdataset>
    </paper>
    <paper id="303">
      <title>Clickbait? Sensational Headline Generation with Auto-tuned Reinforcement Learning</title>
      <author><first>Peng</first><last>Xu</last></author>
      <author><first>Chien-Sheng</first><last>Wu</last></author>
      <author><first>Andrea</first><last>Madotto</last></author>
      <author><first>Pascale</first><last>Fung</last></author>
      <pages>3065–3075</pages>
      <abstract>Sensational headlines are <a href="https://en.wikipedia.org/wiki/Headline">headlines</a> that capture people’s attention and generate <a href="https://en.wikipedia.org/wiki/Interest_(emotion)">reader interest</a>. Conventional abstractive headline generation methods, unlike human writers, do not optimize for maximal reader attention. In this paper, we propose a <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> that generates <a href="https://en.wikipedia.org/wiki/Sensationalism">sensational headlines</a> without labeled data. We first train a sensationalism scorer by classifying online headlines with many comments (clickbait) against a baseline of headlines generated from a summarization model. The score from the sensationalism scorer is used as the <a href="https://en.wikipedia.org/wiki/Reward_system">reward</a> for a reinforcement learner. However, maximizing the noisy sensationalism reward will generate unnatural phrases instead of sensational headlines. To effectively leverage this noisy reward, we propose a novel <a href="https://en.wikipedia.org/wiki/Loss_function">loss function</a>, Auto-tuned Reinforcement Learning (ARL), to dynamically balance <a href="https://en.wikipedia.org/wiki/Reinforcement_learning">reinforcement learning (RL)</a> with <a href="https://en.wikipedia.org/wiki/Maximum_likelihood_estimation">maximum likelihood estimation (MLE)</a>. Human evaluation shows that 60.8 % of samples generated by our <a href="https://en.wikipedia.org/wiki/Statistical_model">model</a> are sensational, which is significantly better than the Pointer-Gen baseline and other RL models.</abstract>
      <url hash="0d3cc3e6">D19-1303</url>
      <doi>10.18653/v1/D19-1303</doi>
      <bibkey>xu-etal-2019-clickbait</bibkey>
      <pwccode url="https://github.com/HLTCHKUST/sensational_headline" additional="false">HLTCHKUST/sensational_headline</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/lcsts">LCSTS</pwcdataset>
    </paper>
    <paper id="305">
      <title>Surface Realisation Using Full Delexicalisation</title>
      <author><first>Anastasia</first><last>Shimorina</last></author>
      <author><first>Claire</first><last>Gardent</last></author>
      <pages>3086–3096</pages>
      <abstract>Surface realisation (SR) maps a meaning representation to a sentence and can be viewed as consisting of three subtasks : <a href="https://en.wikipedia.org/wiki/Word_order">word ordering</a>, <a href="https://en.wikipedia.org/wiki/Inflection">morphological inflection</a> and <a href="https://en.wikipedia.org/wiki/Contraction_(grammar)">contraction generation</a> (e.g., clitic attachment in Portuguese or elision in French). We propose a modular approach to surface realisation which models each of these components separately, and evaluate our approach on the 10 languages covered by the SR’18 Surface Realisation Shared Task shallow track. We provide a detailed evaluation of how <a href="https://en.wikipedia.org/wiki/Word_order">word order</a>, <a href="https://en.wikipedia.org/wiki/Morphology_(linguistics)">morphological realisation</a> and <a href="https://en.wikipedia.org/wiki/Contraction_(grammar)">contractions</a> are handled by the model and an analysis of the differences in word ordering performance across languages.</abstract>
      <url hash="dd41aedc">D19-1305</url>
      <attachment hash="3b0ff29c">D19-1305.Attachment.zip</attachment>
      <doi>10.18653/v1/D19-1305</doi>
      <bibkey>shimorina-gardent-2019-surface</bibkey>
    </paper>
    <paper id="308">
      <title>Mixture Content Selection for Diverse Sequence Generation</title>
      <author><first>Jaemin</first><last>Cho</last></author>
      <author><first>Minjoon</first><last>Seo</last></author>
      <author><first>Hannaneh</first><last>Hajishirzi</last></author>
      <pages>3121–3131</pages>
      <abstract>Generating diverse sequences is important in many NLP applications such as question generation or <a href="https://en.wikipedia.org/wiki/Automatic_summarization">summarization</a> that exhibit semantically one-to-many relationships between source and the target sequences. We present a method to explicitly separate diversification from generation using a general plug-and-play module (called SELECTOR) that wraps around and guides an existing encoder-decoder model. The diversification stage uses a mixture of experts to sample different <a href="https://en.wikipedia.org/wiki/Mask_(computing)">binary masks</a> on the source sequence for diverse content selection. The generation stage uses a standard encoder-decoder model given each selected content from the source sequence. Due to the non-differentiable nature of discrete sampling and the lack of ground truth labels for binary mask, we leverage a proxy for ground truth mask and adopt stochastic hard-EM for training. In question generation (SQuAD) and abstractive summarization (CNN-DM), our method demonstrates significant improvements in <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a>, <a href="https://en.wikipedia.org/wiki/Diversity_index">diversity</a> and <a href="https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets">training efficiency</a>, including state-of-the-art top-1 accuracy in both datasets, 6 % gain in top-5 accuracy, and 3.7 times faster training over a state-of-the-art model. Our code is publicly available at https://github.com/clovaai/FocusSeq2Seq.</abstract>
      <url hash="4a88ffa8">D19-1308</url>
      <doi>10.18653/v1/D19-1308</doi>
      <bibkey>cho-etal-2019-mixture</bibkey>
      <pwccode url="https://github.com/clovaai/FocusSeq2Seq" additional="false">clovaai/FocusSeq2Seq</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/cnn-daily-mail-1">CNN/Daily Mail</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
    </paper>
    <paper id="309">
      <title>An End-to-End Generative Architecture for <a href="https://en.wikipedia.org/wiki/Paraphrase_generation">Paraphrase Generation</a></title>
      <author><first>Qian</first><last>Yang</last></author>
      <author><first>Zhouyuan</first><last>Huo</last></author>
      <author><first>Dinghan</first><last>Shen</last></author>
      <author><first>Yong</first><last>Cheng</last></author>
      <author><first>Wenlin</first><last>Wang</last></author>
      <author><first>Guoyin</first><last>Wang</last></author>
      <author><first>Lawrence</first><last>Carin</last></author>
      <pages>3132–3142</pages>
      <abstract>Generating high-quality paraphrases is a fundamental yet challenging natural language processing task. Despite the effectiveness of previous work based on <a href="https://en.wikipedia.org/wiki/Generative_model">generative models</a>, there remain problems with <a href="https://en.wikipedia.org/wiki/Exposure_bias">exposure bias</a> in <a href="https://en.wikipedia.org/wiki/Recurrent_neural_network">recurrent neural networks</a>, and often a failure to generate realistic sentences. To overcome these challenges, we propose the first end-to-end conditional generative architecture for generating <a href="https://en.wikipedia.org/wiki/Paraphrase">paraphrases</a> via adversarial training, which does not depend on extra linguistic information. Extensive experiments on four public datasets demonstrate the proposed method achieves state-of-the-art results, outperforming previous generative architectures on both automatic metrics (BLEU, <a href="https://en.wikipedia.org/wiki/METEOR">METEOR</a>, and TER) and human evaluations.</abstract>
      <url hash="ac977bba">D19-1309</url>
      <doi>10.18653/v1/D19-1309</doi>
      <bibkey>yang-etal-2019-end</bibkey>
    </paper>
    <paper id="314">
      <title>Enhancing AMR-to-Text Generation with Dual Graph Representations<fixed-case>AMR</fixed-case>-to-Text Generation with Dual Graph Representations</title>
      <author><first>Leonardo F. R.</first><last>Ribeiro</last></author>
      <author><first>Claire</first><last>Gardent</last></author>
      <author><first>Iryna</first><last>Gurevych</last></author>
      <pages>3183–3194</pages>
      <abstract>Generating text from <a href="https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)">graph-based data</a>, such as Abstract Meaning Representation (AMR), is a challenging task due to the inherent difficulty in how to properly encode the structure of a <a href="https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)">graph</a> with labeled edges. To address this difficulty, we propose a novel graph-to-sequence model that encodes different but complementary perspectives of the structural information contained in the AMR graph. The <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> learns parallel top-down and bottom-up representations of nodes capturing contrasting views of the graph. We also investigate the use of different node message passing strategies, employing different state-of-the-art graph encoders to compute node representations based on incoming and outgoing perspectives. In our experiments, we demonstrate that the dual graph representation leads to improvements in AMR-to-text generation, achieving state-of-the-art results on two AMR datasets</abstract>
      <url hash="00ad4bf3">D19-1314</url>
      <attachment hash="cfbdd0df">D19-1314.Attachment.zip</attachment>
      <doi>10.18653/v1/D19-1314</doi>
      <bibkey>ribeiro-etal-2019-enhancing</bibkey>
      <pwccode url="https://github.com/UKPLab/emnlp2019-dualgraph" additional="false">UKPLab/emnlp2019-dualgraph</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/ldc2017t10">LDC2017T10</pwcdataset>
    </paper>
    <paper id="316">
      <title>Toward a Task of Feedback Comment Generation for Writing Learning</title>
      <author><first>Ryo</first><last>Nagata</last></author>
      <pages>3206–3215</pages>
      <abstract>In this paper, we introduce a novel task called feedback comment generation   a task of automatically generating feedback comments such as a hint or an explanatory note for writing learning for non-native learners of English. There has been almost no work on this <a href="https://en.wikipedia.org/wiki/Task_(project_management)">task</a> nor corpus annotated with feedback comments. We have taken the first step by creating learner corpora consisting of approximately 1,900 essays where all preposition errors are manually annotated with feedback comments. We have tested three baseline methods on the <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a>, showing that a simple neural retrieval-based method sets a baseline performance with an <a href="https://en.wikipedia.org/wiki/F-measure">F-measure</a> of 0.34 to 0.41. Finally, we have looked into the results to explore what modifications we need to make to achieve better performance. We also have explored problems unaddressed in this work</abstract>
      <url hash="bb1dee57">D19-1316</url>
      <doi>10.18653/v1/D19-1316</doi>
      <bibkey>nagata-2019-toward</bibkey>
    </paper>
    <paper id="318">
      <title>Deep Copycat Networks for Text-to-Text Generation</title>
      <author><first>Julia</first><last>Ive</last></author>
      <author><first>Pranava</first><last>Madhyastha</last></author>
      <author><first>Lucia</first><last>Specia</last></author>
      <pages>3227–3236</pages>
      <abstract>Most text-to-text generation tasks, for example <a href="https://en.wikipedia.org/wiki/Automatic_summarization">text summarisation</a> and text simplification, require copying words from the input to the output. We introduce <a href="https://en.wikipedia.org/wiki/Copycat">Copycat</a>, a transformer-based pointer network for such tasks which obtains competitive results in abstractive text summarisation and generates more abstractive summaries. We propose a further extension of this architecture for automatic post-editing, where generation is conditioned over two inputs (source language and machine translation), and the model is capable of deciding where to copy information from. This approach achieves competitive performance when compared to state-of-the-art automated post-editing systems. More importantly, we show that it addresses a well-known limitation of automatic post-editing-overcorrecting translations-and that our novel mechanism for copying source language words improves the results.</abstract>
      <url hash="e0ebe41f">D19-1318</url>
      <doi>10.18653/v1/D19-1318</doi>
      <bibkey>ive-etal-2019-deep</bibkey>
    </paper>
    <paper id="319">
      <title>Towards Controllable and Personalized Review Generation</title>
      <author><first>Pan</first><last>Li</last></author>
      <author><first>Alexander</first><last>Tuzhilin</last></author>
      <pages>3237–3245</pages>
      <abstract>In this paper, we propose a novel model RevGAN that automatically generates controllable and personalized user reviews based on the arbitrarily given sentimental and stylistic information. RevGAN utilizes the combination of three novel components, including self-attentive recursive autoencoders, conditional discriminators, and personalized decoders. We test its performance on the several real-world datasets, where our model significantly outperforms state-of-the-art generation models in terms of <a href="https://en.wikipedia.org/wiki/Sentence_(linguistics)">sentence quality</a>, <a href="https://en.wikipedia.org/wiki/Coherence_(linguistics)">coherence</a>, <a href="https://en.wikipedia.org/wiki/Personalization">personalization</a>, and human evaluations. We also empirically show that the generated reviews could not be easily distinguished from the organically produced reviews and that they follow the same statistical linguistics laws.</abstract>
      <url hash="a0ca602c">D19-1319</url>
      <doi>10.18653/v1/D19-1319</doi>
      <bibkey>li-tuzhilin-2019-towards</bibkey>
    </paper>
    <paper id="321">
      <title>Long and Diverse Text Generation with Planning-based Hierarchical Variational Model</title>
      <author><first>Zhihong</first><last>Shao</last></author>
      <author><first>Minlie</first><last>Huang</last></author>
      <author><first>Jiangtao</first><last>Wen</last></author>
      <author><first>Wenfei</first><last>Xu</last></author>
      <author><first>Xiaoyan</first><last>Zhu</last></author>
      <pages>3257–3268</pages>
      <abstract>Existing neural methods for data-to-text generation are still struggling to produce long and diverse texts : they are insufficient to model input data dynamically during generation, to capture inter-sentence coherence, or to generate diversified expressions. To address these issues, we propose a Planning-based Hierarchical Variational Model (PHVM). Our model first plans a sequence of groups (each group is a subset of input items to be covered by a sentence) and then realizes each sentence conditioned on the planning result and the previously generated context, thereby decomposing long text generation into dependent sentence generation sub-tasks. To capture expression diversity, we devise a hierarchical latent structure where a global planning latent variable models the diversity of reasonable planning and a sequence of local latent variables controls sentence realization. Experiments show that our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> outperforms state-of-the-art baselines in long and diverse text generation.</abstract>
      <url hash="5764f45c">D19-1321</url>
      <attachment hash="f9f56b7c">D19-1321.Attachment.zip</attachment>
      <doi>10.18653/v1/D19-1321</doi>
      <bibkey>shao-etal-2019-long</bibkey>
      <pwccode url="https://github.com/ZhihongShao/Planning-based-Hierarchical-Variational-Model" additional="false">ZhihongShao/Planning-based-Hierarchical-Variational-Model</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/100doh">100DOH</pwcdataset>
    </paper>
    <paper id="325">
      <title>Domain Adaptive Text Style Transfer</title>
      <author><first>Dianqi</first><last>Li</last></author>
      <author><first>Yizhe</first><last>Zhang</last></author>
      <author><first>Zhe</first><last>Gan</last></author>
      <author><first>Yu</first><last>Cheng</last></author>
      <author><first>Chris</first><last>Brockett</last></author>
      <author><first>Bill</first><last>Dolan</last></author>
      <author><first>Ming-Ting</first><last>Sun</last></author>
      <pages>3304–3313</pages>
      <abstract>Text style transfer without <a href="https://en.wikipedia.org/wiki/Parallel_computing">parallel data</a> has achieved some practical success. However, in the scenario where less data is available, these <a href="https://en.wikipedia.org/wiki/Method_(computer_programming)">methods</a> may yield poor performance. In this paper, we examine <a href="https://en.wikipedia.org/wiki/Domain_adaptation">domain adaptation</a> for text style transfer to leverage massively available data from other domains. These <a href="https://en.wikipedia.org/wiki/Data">data</a> may demonstrate domain shift, which impedes the benefits of utilizing such <a href="https://en.wikipedia.org/wiki/Data">data</a> for training. To address this challenge, we propose simple yet effective domain adaptive text style transfer models, enabling domain-adaptive information exchange. The proposed models presumably learn from the source domain to : (i) distinguish stylized information and generic content information ; (ii) maximally preserve content information ; and (iii) adaptively transfer the styles in a domain-aware manner. We evaluate the proposed models on two style transfer tasks (sentiment and formality) over multiple target domains where only limited non-parallel data is available. Extensive experiments demonstrate the effectiveness of the proposed <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> compared to the <a href="https://en.wikipedia.org/wiki/Baseline_(configuration_management)">baselines</a>.</abstract>
      <url hash="0589a6e6">D19-1325</url>
      <attachment hash="9d1872c2">D19-1325.Attachment.pdf</attachment>
      <doi>10.18653/v1/D19-1325</doi>
      <bibkey>li-etal-2019-domain</bibkey>
      <pwccode url="https://github.com/cookielee77/DAST" additional="false">cookielee77/DAST</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/gyafc">GYAFC</pwcdataset>
    </paper>
    <paper id="326">
      <title>Let’s Ask Again : Refine Network for Automatic Question Generation</title>
      <author><first>Preksha</first><last>Nema</last></author>
      <author><first>Akash Kumar</first><last>Mohankumar</last></author>
      <author><first>Mitesh M.</first><last>Khapra</last></author>
      <author><first>Balaji Vasan</first><last>Srinivasan</last></author>
      <author><first>Balaraman</first><last>Ravindran</last></author>
      <pages>3314–3323</pages>
      <abstract>In this work, we focus on the task of Automatic Question Generation (AQG) where given a passage and an answer the task is to generate the corresponding question. It is desired that the generated question should be (i) grammatically correct (ii) answerable from the passage and (iii) specific to the given answer. An analysis of existing AQG models shows that they produce questions which do not adhere to one or more of the above-mentioned qualities. In particular, the generated questions look like an incomplete draft of the desired question with a clear scope for refinement. To alleviate this shortcoming, we propose a method which tries to mimic the human process of generating questions by first creating an initial draft and then refining it. More specifically, we propose Refine Network (RefNet) which contains two <a href="https://en.wikipedia.org/wiki/Code">decoders</a>. The second <a href="https://en.wikipedia.org/wiki/Code">decoder</a> uses a dual attention network which pays attention to both (i) the original passage and (ii) the question (initial draft) generated by the first <a href="https://en.wikipedia.org/wiki/Code">decoder</a>. In effect, <a href="https://en.wikipedia.org/wiki/Information_technology">it</a> refines the question generated by the first <a href="https://en.wikipedia.org/wiki/Code">decoder</a>, thereby making it more correct and complete. We evaluate RefNet on three datasets, viz., SQuAD, HOTPOT-QA, and DROP, and show that it outperforms existing state-of-the-art methods by 7-16 % on all of these datasets. Lastly, we show that we can improve the quality of the second decoder on specific <a href="https://en.wikipedia.org/wiki/Metric_(mathematics)">metrics</a>, such as, <a href="https://en.wikipedia.org/wiki/Fluency">fluency</a> and answerability by explicitly rewarding revisions that improve on the corresponding <a href="https://en.wikipedia.org/wiki/Metric_(mathematics)">metric</a> during training.<fixed-case>the above-mentioned qualities</fixed-case>. In particular, the generated questions look like an incomplete draft of the desired question with a clear scope for refinement. <fixed-case>To alleviate this shortcoming</fixed-case>, we propose a method which tries to mimic the human process of generating questions by first creating an initial draft and then refining it. More specifically, we propose Refine Network (RefNet) which contains two decoders. The second decoder uses a dual attention network which pays attention to both (i) the original passage and (ii) the question (initial draft) generated by the first decoder. In effect, it refines the question generated by the first decoder, thereby making it more correct and complete. We evaluate RefNet on three datasets, <i>viz.</i>, SQuAD, HOTPOT-QA, and DROP, and show that it outperforms existing state-of-the-art methods by 7-16% on all of these datasets. Lastly, we show that we can improve the quality of the second decoder on specific metrics, such as, fluency and answerability by explicitly rewarding revisions that improve on the corresponding metric during training. The code has been made publicly available .</abstract>
      <url hash="98c0da59">D19-1326</url>
      <attachment hash="c769599b">D19-1326.Attachment.pdf</attachment>
      <doi>10.18653/v1/D19-1326</doi>
      <bibkey>nema-etal-2019-lets</bibkey>
      <pwccode url="https://github.com/PrekshaNema25/RefNet-QG" additional="false">PrekshaNema25/RefNet-QG</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/drop">DROP</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/hotpotqa">HotpotQA</pwcdataset>
    </paper>
    <paper id="329">
      <title>Towards Realistic Practices In Low-Resource Natural Language Processing : The Development Set</title>
      <author><first>Katharina</first><last>Kann</last></author>
      <author><first>Kyunghyun</first><last>Cho</last></author>
      <author><first>Samuel R.</first><last>Bowman</last></author>
      <pages>3342–3349</pages>
      <abstract>Development sets are impractical to obtain for real low-resource languages, since using all available data for training is often more effective. However, development sets are widely used in research papers that purport to deal with low-resource natural language processing (NLP). Here, we aim to answer the following questions : Does using a development set for early stopping in the low-resource setting influence results as compared to a more realistic alternative, where the number of training epochs is tuned on development languages? And does <a href="https://en.wikipedia.org/wiki/Information_technology">it</a> lead to overestimation or underestimation of performance? We repeat multiple experiments from recent work on neural models for low-resource NLP and compare results for <a href="https://en.wikipedia.org/wiki/Conceptual_model">models</a> obtained by training with and without development sets. On average over languages, <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">absolute accuracy</a> differs by up to 1.4 %. However, for some languages and tasks, differences are as big as 18.0 % <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a>. Our results highlight the importance of realistic experimental setups in the publication of low-resource NLP research results.</abstract>
      <url hash="51ca80c6">D19-1329</url>
      <attachment hash="1c583cfe">D19-1329.Attachment.zip</attachment>
      <doi>10.18653/v1/D19-1329</doi>
      <bibkey>kann-etal-2019-towards</bibkey>
    </paper>
    <paper id="330">
      <title>Synchronously Generating Two Languages with Interactive Decoding</title>
      <author><first>Yining</first><last>Wang</last></author>
      <author><first>Jiajun</first><last>Zhang</last></author>
      <author><first>Long</first><last>Zhou</last></author>
      <author><first>Yuchen</first><last>Liu</last></author>
      <author><first>Chengqing</first><last>Zong</last></author>
      <pages>3350–3355</pages>
      <abstract>In this paper, we introduce a novel interactive approach to translate a source language into two different languages simultaneously and interactively. Specifically, the generation of one language relies on not only previously generated outputs by itself, but also the outputs predicted in the other language. Experimental results on IWSLT and WMT datasets demonstrate that our method can obtain significant improvements over both conventional Neural Machine Translation (NMT) model and multilingual NMT model.</abstract>
      <url hash="3bd64156">D19-1330</url>
      <doi>10.18653/v1/D19-1330</doi>
      <bibkey>wang-etal-2019-synchronously</bibkey>
    </paper>
    <paper id="331">
      <title>On NMT Search Errors and Model Errors : Cat Got Your Tongue?<fixed-case>NMT</fixed-case> Search Errors and Model Errors: Cat Got Your Tongue?</title>
      <author><first>Felix</first><last>Stahlberg</last></author>
      <author><first>Bill</first><last>Byrne</last></author>
      <pages>3356–3362</pages>
      <abstract>We report on search errors and model errors in neural machine translation (NMT). We present an exact inference procedure for neural sequence models based on a combination of <a href="https://en.wikipedia.org/wiki/Beam_search">beam search</a> and <a href="https://en.wikipedia.org/wiki/Depth-first_search">depth-first search</a>. We use our exact search to find the global best model scores under a Transformer base model for the entire WMT15 English-German test set. Surprisingly, <a href="https://en.wikipedia.org/wiki/Beam_search">beam search</a> fails to find these global best model scores in most cases, even with a very large beam size of 100. For more than 50 % of the sentences, the <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> in fact assigns its global best score to the empty translation, revealing a massive failure of neural models in properly accounting for adequacy. We show by constraining search with a minimum translation length that at the root of the problem of empty translations lies an inherent bias towards shorter translations. We conclude that vanilla NMT in its current form requires just the right amount of beam search errors, which, from a modelling perspective, is a highly unsatisfactory conclusion indeed, as the model often prefers an empty translation.</abstract>
      <url hash="e16783ca">D19-1331</url>
      <doi>10.18653/v1/D19-1331</doi>
      <bibkey>stahlberg-byrne-2019-nmt</bibkey>
    </paper>
    <paper id="332">
      <title>Going on a vacation takes longer than Going for a walk : A Study of Temporal Commonsense Understanding</title>
      <author><first>Ben</first><last>Zhou</last></author>
      <author><first>Daniel</first><last>Khashabi</last></author>
      <author><first>Qiang</first><last>Ning</last></author>
      <author><first>Dan</first><last>Roth</last></author>
      <pages>3363–3369</pages>
      <abstract>Understanding time is crucial for understanding events expressed in <a href="https://en.wikipedia.org/wiki/Natural_language">natural language</a>. Because people rarely say the obvious, it is often necessary to have <a href="https://en.wikipedia.org/wiki/Commonsense_knowledge">commonsense knowledge</a> about various temporal aspects of events, such as <a href="https://en.wikipedia.org/wiki/Time">duration</a>, <a href="https://en.wikipedia.org/wiki/Frequency">frequency</a>, and temporal order. However, this important problem has so far received limited attention. This paper systematically studies this temporal commonsense problem. Specifically, we define five classes of temporal commonsense, and use <a href="https://en.wikipedia.org/wiki/Crowdsourcing">crowdsourcing</a> to develop a new dataset, MCTACO, that serves as a test set for this task. We find that the best current methods used on MCTACO are still far behind human performance, by about 20 %, and discuss several directions for improvement. We hope that the new dataset and our study here can foster more future research on this topic.</abstract>
      <url hash="4a2356af">D19-1332</url>
      <attachment hash="5dc051b3">D19-1332.Attachment.zip</attachment>
      <doi>10.18653/v1/D19-1332</doi>
      <bibkey>zhou-etal-2019-going</bibkey>
    </paper>
    <paper id="333">
      <title>QAInfomax : Learning Robust Question Answering System by Mutual Information Maximization<fixed-case>QAI</fixed-case>nfomax: Learning Robust Question Answering System by Mutual Information Maximization</title>
      <author><first>Yi-Ting</first><last>Yeh</last></author>
      <author><first>Yun-Nung</first><last>Chen</last></author>
      <pages>3370–3375</pages>
      <abstract>Standard accuracy metrics indicate that modern reading comprehension systems have achieved strong performance in many question answering datasets. However, the extent these <a href="https://en.wikipedia.org/wiki/System">systems</a> truly understand <a href="https://en.wikipedia.org/wiki/Language">language</a> remains unknown, and existing <a href="https://en.wikipedia.org/wiki/System">systems</a> are not good at distinguishing distractor sentences which look related but do not answer the question. To address this problem, we propose QAInfomax as a <a href="https://en.wikipedia.org/wiki/Regularization_(mathematics)">regularizer</a> in <a href="https://en.wikipedia.org/wiki/Reading_comprehension">reading comprehension systems</a> by maximizing <a href="https://en.wikipedia.org/wiki/Mutual_information">mutual information</a> among passages, a question, and its answer. QAInfomax helps regularize the <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> to not simply learn the superficial correlation for answering the questions. The experiments show that our proposed QAInfomax achieves the state-of-the-art performance on the benchmark Adversarial-SQuAD dataset.</abstract>
      <url hash="8fa14f67">D19-1333</url>
      <attachment hash="a161d613">D19-1333.Attachment.zip</attachment>
      <doi>10.18653/v1/D19-1333</doi>
      <bibkey>yeh-chen-2019-qainfomax</bibkey>
      <pwccode url="https://github.com/MiuLab/QAInfomax" additional="false">MiuLab/QAInfomax</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
    </paper>
    <paper id="335">
      <title>How Reasonable are Common-Sense Reasoning Tasks : A Case-Study on the Winograd Schema Challenge and SWAG<fixed-case>W</fixed-case>inograd Schema Challenge and <fixed-case>SWAG</fixed-case></title>
      <author><first>Paul</first><last>Trichelair</last></author>
      <author><first>Ali</first><last>Emami</last></author>
      <author><first>Adam</first><last>Trischler</last></author>
      <author><first>Kaheer</first><last>Suleman</last></author>
      <author><first>Jackie Chi Kit</first><last>Cheung</last></author>
      <pages>3382–3387</pages>
      <abstract>Recent studies have significantly improved the state-of-the-art on common-sense reasoning (CSR) benchmarks like the <a href="https://en.wikipedia.org/wiki/Winograd_Schema_Challenge">Winograd Schema Challenge (WSC)</a> and SWAG. The question we ask in this paper is whether improved performance on these <a href="https://en.wikipedia.org/wiki/Benchmark_(computing)">benchmarks</a> represents genuine progress towards common-sense-enabled systems. We make case studies of both <a href="https://en.wikipedia.org/wiki/Benchmarking">benchmarks</a> and design protocols that clarify and qualify the results of previous work by analyzing threats to the validity of previous experimental designs. Our protocols account for several properties prevalent in common-sense benchmarks including size limitations, structural regularities, and variable instance difficulty.</abstract>
      <url hash="671b987a">D19-1335</url>
      <attachment hash="42769ba0">D19-1335.Attachment.pdf</attachment>
      <doi>10.18653/v1/D19-1335</doi>
      <bibkey>trichelair-etal-2019-reasonable</bibkey>
      <pwccode url="https://github.com/ptrichel/How-Reasonable-are-Common-Sense-Reasoning-Tasks" additional="false">ptrichel/How-Reasonable-are-Common-Sense-Reasoning-Tasks</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/swag">SWAG</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wsc">WSC</pwcdataset>
    </paper>
    <paper id="336">
      <title>Pun-GAN : Generative Adversarial Network for Pun Generation<fixed-case>GAN</fixed-case>: Generative Adversarial Network for Pun Generation</title>
      <author><first>Fuli</first><last>Luo</last></author>
      <author><first>Shunyao</first><last>Li</last></author>
      <author><first>Pengcheng</first><last>Yang</last></author>
      <author><first>Lei</first><last>Li</last></author>
      <author><first>Baobao</first><last>Chang</last></author>
      <author><first>Zhifang</first><last>Sui</last></author>
      <author><first>Xu</first><last>Sun</last></author>
      <pages>3388–3393</pages>
      <abstract>In this paper, we focus on the task of generating a <a href="https://en.wikipedia.org/wiki/Pun">pun sentence</a> given a pair of <a href="https://en.wikipedia.org/wiki/Word_sense">word senses</a>. A major challenge for pun generation is the lack of large-scale pun corpus to guide <a href="https://en.wikipedia.org/wiki/Supervised_learning">supervised learning</a>. To remedy this, we propose an adversarial generative network for pun generation (Pun-GAN). It consists of a <a href="https://en.wikipedia.org/wiki/Generator_(mathematics)">generator</a> to produce <a href="https://en.wikipedia.org/wiki/Pun">pun sentences</a>, and a <a href="https://en.wikipedia.org/wiki/Discriminator">discriminator</a> to distinguish between the generated <a href="https://en.wikipedia.org/wiki/Pun">pun sentences</a> and the real sentences with specific word senses. The output of the <a href="https://en.wikipedia.org/wiki/Discriminator">discriminator</a> is then used as a reward to train the generator via <a href="https://en.wikipedia.org/wiki/Reinforcement_learning">reinforcement learning</a>, encouraging it to produce pun sentences which can support two word senses simultaneously. Experiments show that the proposed Pun-GAN can generate sentences that are more ambiguous and diverse in both automatic and human evaluation.</abstract>
      <url hash="659e139b">D19-1336</url>
      <doi>10.18653/v1/D19-1336</doi>
      <bibkey>luo-etal-2019-pun</bibkey>
      <pwccode url="https://github.com/lishunyao97/Pun-GAN" additional="false">lishunyao97/Pun-GAN</pwccode>
    </paper>
    <paper id="337">
      <title>Multi-Task Learning with <a href="https://en.wikipedia.org/wiki/Language_model">Language Modeling</a> for Question Generation</title>
      <author><first>Wenjie</first><last>Zhou</last></author>
      <author><first>Minghua</first><last>Zhang</last></author>
      <author><first>Yunfang</first><last>Wu</last></author>
      <pages>3394–3399</pages>
      <abstract>This paper explores the task of answer-aware questions generation. Based on the attention-based pointer generator model, we propose to incorporate an auxiliary task of language modeling to help question generation in a hierarchical multi-task learning structure. Our joint-learning model enables the <a href="https://en.wikipedia.org/wiki/Encoder">encoder</a> to learn a better representation of the input sequence, which will guide the <a href="https://en.wikipedia.org/wiki/Codec">decoder</a> to generate more coherent and fluent questions. On both SQuAD and MARCO datasets, our multi-task learning model boosts the performance, achieving state-of-the-art results. Moreover, human evaluation further proves the high quality of our generated questions.</abstract>
      <url hash="d7b65387">D19-1337</url>
      <doi>10.18653/v1/D19-1337</doi>
      <bibkey>zhou-etal-2019-multi</bibkey>
    </paper>
    <paper id="338">
      <title>Autoregressive Text Generation Beyond Feedback Loops</title>
      <author><first>Florian</first><last>Schmidt</last></author>
      <author><first>Stephan</first><last>Mandt</last></author>
      <author><first>Thomas</first><last>Hofmann</last></author>
      <pages>3400–3406</pages>
      <abstract>Autoregressive state transitions, where predictions are conditioned on past predictions, are the predominant choice for both deterministic and stochastic sequential models. However, autoregressive feedback exposes the evolution of the hidden state trajectory to potential biases from well-known train-test discrepancies. In this paper, we combine a latent state space model with a CRF observation model. We argue that such autoregressive observation models form an interesting middle ground that expresses local correlations on the word level but keeps the state evolution non-autoregressive. On unconditional sentence generation we show performance improvements compared to RNN and GAN baselines while avoiding some prototypical failure modes of <a href="https://en.wikipedia.org/wiki/Autoregressive_model">autoregressive models</a>.</abstract>
      <url hash="53d6384c">D19-1338</url>
      <attachment hash="f66377a5">D19-1338.Attachment.pdf</attachment>
      <doi>10.18653/v1/D19-1338</doi>
      <bibkey>schmidt-etal-2019-autoregressive</bibkey>
      <pwccode url="https://github.com/schmiflo/crf-generation" additional="false">schmiflo/crf-generation</pwccode>
    </paper>
    <paper id="339">
      <title>The Woman Worked as a Babysitter : On Biases in Language Generation</title>
      <author><first>Emily</first><last>Sheng</last></author>
      <author><first>Kai-Wei</first><last>Chang</last></author>
      <author><first>Premkumar</first><last>Natarajan</last></author>
      <author><first>Nanyun</first><last>Peng</last></author>
      <pages>3407–3412</pages>
      <abstract>We present a systematic study of biases in natural language generation (NLG) by analyzing text generated from prompts that contain mentions of different demographic groups. In this work, we introduce the notion of the regard towards a demographic, use the varying levels of regard towards different demographics as a defining metric for bias in NLG, and analyze the extent to which sentiment scores are a relevant proxy metric for <a href="https://en.wikipedia.org/wiki/Opinion_poll">regard</a>. To this end, we collect strategically-generated text from <a href="https://en.wikipedia.org/wiki/Language_model">language models</a> and manually annotate the text with both sentiment and regard scores. Additionally, we build an automatic regard classifier through <a href="https://en.wikipedia.org/wiki/Transfer_learning">transfer learning</a>, so that we can analyze biases in unseen text. Together, these <a href="https://en.wikipedia.org/wiki/Methodology">methods</a> reveal the extent of the biased nature of language model generations. Our analysis provides a study of biases in NLG, bias metrics and correlated human judgments, and empirical evidence on the usefulness of our annotated dataset.</abstract>
      <url hash="6438614a">D19-1339</url>
      <attachment hash="8194cfcd">D19-1339.Attachment.zip</attachment>
      <doi>10.18653/v1/D19-1339</doi>
      <bibkey>sheng-etal-2019-woman</bibkey>
      <pwccode url="https://github.com/ewsheng/nlg-bias" additional="false">ewsheng/nlg-bias</pwccode>
    </paper>
    <paper id="342">
      <title>Recognizing Conflict Opinions in Aspect-level Sentiment Classification with Dual Attention Networks</title>
      <author><first>Xingwei</first><last>Tan</last></author>
      <author><first>Yi</first><last>Cai</last></author>
      <author><first>Changxi</first><last>Zhu</last></author>
      <pages>3426–3431</pages>
      <abstract>Aspect-level sentiment classification, which is a fine-grained sentiment analysis task, has received lots of attention these years. There is a phenomenon that people express both positive and negative sentiments towards an aspect at the same time. Such opinions with conflicting sentiments, however, are ignored by existing studies, which design <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> based on the absence of them. We argue that the exclusion of conflict opinions is problematic, for the reason that it represents an important style of human thinking   dialectic thinking. If a real-world sentiment classification system ignores the existence of conflict opinions when it is designed, it will incorrectly mixed conflict opinions into other sentiment polarity categories in action. Existing models have problems when recognizing <a href="https://en.wikipedia.org/wiki/Consensus_decision-making">conflicting opinions</a>, such as data sparsity. In this paper, we propose a multi-label classification model with dual attention mechanism to address these problems.</abstract>
      <url hash="647e9e6e">D19-1342</url>
      <attachment hash="01abb99b">D19-1342.Attachment.zip</attachment>
      <doi>10.18653/v1/D19-1342</doi>
      <bibkey>tan-etal-2019-recognizing</bibkey>
    </paper>
    <paper id="343">
      <title>Investigating Dynamic Routing in Tree-Structured LSTM for <a href="https://en.wikipedia.org/wiki/Sentiment_analysis">Sentiment Analysis</a><fixed-case>LSTM</fixed-case> for Sentiment Analysis</title>
      <author><first>Jin</first><last>Wang</last></author>
      <author><first>Liang-Chih</first><last>Yu</last></author>
      <author><first>K. Robert</first><last>Lai</last></author>
      <author><first>Xuejie</first><last>Zhang</last></author>
      <pages>3432–3437</pages>
      <abstract>Deep neural network models such as <a href="https://en.wikipedia.org/wiki/Long_short-term_memory">long short-term memory (LSTM)</a> and tree-LSTM have been proven to be effective for <a href="https://en.wikipedia.org/wiki/Sentiment_analysis">sentiment analysis</a>. However, sequential LSTM is a bias model wherein the words in the tail of a sentence are more heavily emphasized than those in the header for building sentence representations. Even tree-LSTM, with useful structural information, could not avoid the bias problem because the root node will be dominant and the nodes in the bottom of the <a href="https://en.wikipedia.org/wiki/Parse_tree">parse tree</a> will be less emphasized even though they may contain salient information. To overcome the bias problem, this study proposes a capsule tree-LSTM model, introducing a dynamic routing algorithm as an aggregation layer to build sentence representation by assigning different weights to nodes according to their contributions to prediction. Experiments on Stanford Sentiment Treebank (SST) for sentiment classification and EmoBank for <a href="https://en.wikipedia.org/wiki/Regression_analysis">regression</a> show that the proposed method improved the performance of tree-LSTM and other neural network models. In addition, the deeper the <a href="https://en.wikipedia.org/wiki/Tree_structure">tree structure</a>, the bigger the improvement.</abstract>
      <url hash="e4bbf036">D19-1343</url>
      <doi>10.18653/v1/D19-1343</doi>
      <bibkey>wang-etal-2019-investigating</bibkey>
    </paper>
    <paper id="344">
      <title>A Label Informative Wide &amp; Deep Classifier for Patents and Papers</title>
      <author><first>Muyao</first><last>Niu</last></author>
      <author><first>Jie</first><last>Cai</last></author>
      <pages>3438–3443</pages>
      <abstract>In this paper, we provide a simple and effective <a href="https://en.wikipedia.org/wiki/Baseline_(configuration_management)">baseline</a> for classifying both patents and papers to the well-established Cooperative Patent Classification (CPC). We propose a label-informative classifier based on the Wide &amp; Deep structure, where the Wide part encodes string-level similarities between texts and labels, and the Deep part captures semantic-level similarities via non-linear transformations. Our <a href="https://en.wikipedia.org/wiki/Computer_simulation">model</a> trains on millions of patents, and transfers to papers by developing distant-supervised training set and domain-specific features. Extensive experiments show that our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> achieves comparable performance to the state-of-the-art <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> used in industry on both patents and papers. The output of this work should facilitate the searching, granting and filing of innovative ideas for patent examiners, attorneys and researchers.</abstract>
      <url hash="9557703b">D19-1344</url>
      <attachment hash="ea158db6">D19-1344.Attachment.zip</attachment>
      <doi>10.18653/v1/D19-1344</doi>
      <bibkey>niu-cai-2019-label</bibkey>
    </paper>
    <paper id="347">
      <title>Delta-training : Simple Semi-Supervised Text Classification using Pretrained Word Embeddings</title>
      <author><first>Hwiyeol</first><last>Jo</last></author>
      <author><first>Ceyda</first><last>Cinarel</last></author>
      <pages>3458–3463</pages>
      <abstract>We propose a novel and simple <a href="https://en.wikipedia.org/wiki/Methodology">method</a> for semi-supervised text classification. The method stems from the hypothesis that a <a href="https://en.wikipedia.org/wiki/Statistical_classification">classifier</a> with pretrained word embeddings always outperforms the same <a href="https://en.wikipedia.org/wiki/Statistical_classification">classifier</a> with randomly initialized word embeddings, as empirically observed in NLP tasks. Our method first builds two sets of <a href="https://en.wikipedia.org/wiki/Statistical_classification">classifiers</a> as a form of model ensemble, and then initializes their <a href="https://en.wikipedia.org/wiki/Word_embedding">word embeddings</a> differently : one using random, the other using pretrained word embeddings. We focus on different predictions between the two <a href="https://en.wikipedia.org/wiki/Statistical_classification">classifiers</a> on unlabeled data while following the self-training framework. We also use early-stopping in meta-epoch to improve the performance of our <a href="https://en.wikipedia.org/wiki/Methodology">method</a>. Our method, Delta-training, outperforms the self-training and the co-training framework in 4 different text classification datasets, showing robustness against error accumulation.</abstract>
      <url hash="65b706bb">D19-1347</url>
      <doi>10.18653/v1/D19-1347</doi>
      <bibkey>jo-cinarel-2019-delta</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/ag-news">AG News</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
    </paper>
    <paper id="348">
      <title>Visual Detection with Context for Document Layout Analysis</title>
      <author><first>Carlos</first><last>Soto</last></author>
      <author><first>Shinjae</first><last>Yoo</last></author>
      <pages>3464–3470</pages>
      <abstract>We present 1) a work in progress method to visually segment key regions of scientific articles using an object detection technique augmented with contextual features, and 2) a novel dataset of region-labeled articles. A continuing challenge in scientific literature mining is the difficulty of consistently extracting high-quality text from formatted PDFs. To address this, we adapt the object-detection technique Faster R-CNN for document layout detection, incorporating contextual information that leverages the inherently localized nature of article contents to improve the region detection performance. Due to the limited availability of high-quality region-labels for scientific articles, we also contribute a novel dataset of region annotations, the first version of which covers 9 region classes and 822 article pages. Initial experimental results demonstrate a 23.9 % absolute improvement in mean average precision over the baseline model by incorporating contextual features, and a processing speed 14x faster than a text-based technique. Ongoing work on further improvements is also discussed.</abstract>
      <url hash="ea0236d5">D19-1348</url>
      <attachment hash="c24afeb3">D19-1348.Attachment.pdf</attachment>
      <doi>10.18653/v1/D19-1348</doi>
      <bibkey>soto-yoo-2019-visual</bibkey>
    </paper>
    <paper id="350">
      <title>Neural Topic Model with <a href="https://en.wikipedia.org/wiki/Reinforcement_learning">Reinforcement Learning</a></title>
      <author><first>Lin</first><last>Gui</last></author>
      <author><first>Jia</first><last>Leng</last></author>
      <author><first>Gabriele</first><last>Pergola</last></author>
      <author><first>Yu</first><last>Zhou</last></author>
      <author><first>Ruifeng</first><last>Xu</last></author>
      <author><first>Yulan</first><last>He</last></author>
      <pages>3478–3483</pages>
      <abstract>In recent years, advances in neural variational inference have achieved many successes in <a href="https://en.wikipedia.org/wiki/Text_processing">text processing</a>. Examples include neural topic models which are typically built upon variational autoencoder (VAE) with an objective of minimising the error of reconstructing original documents based on the learned latent topic vectors. However, minimising reconstruction errors does not necessarily lead to high quality topics. In this paper, we borrow the idea of <a href="https://en.wikipedia.org/wiki/Reinforcement_learning">reinforcement learning</a> and incorporate topic coherence measures as reward signals to guide the learning of a VAE-based topic model. Furthermore, our proposed model is able to automatically separating background words dynamically from topic words, thus eliminating the pre-processing step of filtering infrequent and/or top frequent words, typically required for learning traditional topic models. Experimental results on the 20 Newsgroups and the NIPS datasets show superior performance both on perplexity and topic coherence measure compared to state-of-the-art neural topic models.</abstract>
      <url hash="2bc9f26d">D19-1350</url>
      <doi>10.18653/v1/D19-1350</doi>
      <bibkey>gui-etal-2019-neural</bibkey>
    </paper>
    <paper id="351">
      <title>Modelling Stopping Criteria for Search Results using <a href="https://en.wikipedia.org/wiki/Poisson_point_process">Poisson Processes</a><fixed-case>P</fixed-case>oisson Processes</title>
      <author><first>Alison</first><last>Sneyd</last></author>
      <author><first>Mark</first><last>Stevenson</last></author>
      <pages>3484–3489</pages>
      <abstract>Text retrieval systems often return large sets of documents, particularly when applied to large collections. Stopping criteria can reduce the number of these documents that need to be manually evaluated for <a href="https://en.wikipedia.org/wiki/Relevance">relevance</a> by predicting when a suitable level of <a href="https://en.wikipedia.org/wiki/Recall_(memory)">recall</a> has been achieved. In this work, a novel method for determining a stopping criterion is proposed that models the rate at which relevant documents occur using a <a href="https://en.wikipedia.org/wiki/Poisson_point_process">Poisson process</a>. This method allows a user to specify both a minimum desired level of <a href="https://en.wikipedia.org/wiki/Recall_(memory)">recall</a> to achieve and a desired probability of having achieved it. We evaluate our method on a <a href="https://en.wikipedia.org/wiki/Data_set">public dataset</a> and compare it with previous techniques for determining <a href="https://en.wikipedia.org/wiki/Stopping_time">stopping criteria</a>.</abstract>
      <url hash="3b4b9cdd">D19-1351</url>
      <doi>10.18653/v1/D19-1351</doi>
      <bibkey>sneyd-stevenson-2019-modelling</bibkey>
      <pwccode url="https://github.com/alisonsneyd/poisson_stopping_method" additional="false">alisonsneyd/poisson_stopping_method</pwccode>
    </paper>
    <paper id="352">
      <title>Cross-Domain Modeling of Sentence-Level Evidence for Document Retrieval</title>
      <author><first>Zeynep</first><last>Akkalyoncu Yilmaz</last></author>
      <author><first>Wei</first><last>Yang</last></author>
      <author><first>Haotian</first><last>Zhang</last></author>
      <author><first>Jimmy</first><last>Lin</last></author>
      <pages>3490–3496</pages>
      <abstract>This paper applies BERT to ad hoc document retrieval on news articles, which requires addressing two challenges : relevance judgments in existing test collections are typically provided only at the document level, and documents often exceed the length that BERT was designed to handle. Our solution is to aggregate <a href="https://en.wikipedia.org/wiki/Sentence_(law)">sentence-level evidence</a> to rank documents. Furthermore, we are able to leverage passage-level relevance judgments fortuitously available in other domains to fine-tune BERT models that are able to capture cross-domain notions of relevance, and can be directly used for ranking news articles. Our simple neural ranking models achieve state-of-the-art effectiveness on three standard test collections.</abstract>
      <url hash="689d957a">D19-1352</url>
      <doi>10.18653/v1/D19-1352</doi>
      <bibkey>akkalyoncu-yilmaz-etal-2019-cross</bibkey>
    </paper>
    <paper id="353">
      <title>The Challenges of Optimizing <a href="https://en.wikipedia.org/wiki/Machine_translation">Machine Translation</a> for Low Resource Cross-Language Information Retrieval</title>
      <author><first>Constantine</first><last>Lignos</last></author>
      <author><first>Daniel</first><last>Cohen</last></author>
      <author><first>Yen-Chieh</first><last>Lien</last></author>
      <author><first>Pratik</first><last>Mehta</last></author>
      <author><first>W. Bruce</first><last>Croft</last></author>
      <author><first>Scott</first><last>Miller</last></author>
      <pages>3497–3502</pages>
      <abstract>When performing <a href="https://en.wikipedia.org/wiki/Cross-language_information_retrieval">cross-language information retrieval (CLIR)</a> for lower-resourced languages, a common approach is to retrieve over the output of <a href="https://en.wikipedia.org/wiki/Machine_translation">machine translation (MT)</a>. However, there is no established guidance on how to optimize the resulting MT-IR system. In this paper, we examine the relationship between the performance of MT systems and both neural and term frequency-based IR models to identify how CLIR performance can be best predicted from MT quality. We explore performance at varying amounts of MT training data, byte pair encoding (BPE) merge operations, and across two IR collections and retrieval models. We find that the choice of IR collection can substantially affect the predictive power of MT tuning decisions and evaluation, potentially introducing dissociations between MT-only and overall CLIR performance.</abstract>
      <url hash="3245226d">D19-1353</url>
      <attachment hash="5bc7e0a7">D19-1353.Attachment.zip</attachment>
      <doi>10.18653/v1/D19-1353</doi>
      <bibkey>lignos-etal-2019-challenges</bibkey>
    </paper>
    <paper id="355">
      <title>GlossBERT : BERT for <a href="https://en.wikipedia.org/wiki/Word-sense_disambiguation">Word Sense Disambiguation</a> with Gloss Knowledge<fixed-case>G</fixed-case>loss<fixed-case>BERT</fixed-case>: <fixed-case>BERT</fixed-case> for Word Sense Disambiguation with Gloss Knowledge</title>
      <author><first>Luyao</first><last>Huang</last></author>
      <author><first>Chi</first><last>Sun</last></author>
      <author><first>Xipeng</first><last>Qiu</last></author>
      <author><first>Xuanjing</first><last>Huang</last></author>
      <pages>3509–3514</pages>
      <abstract>Word Sense Disambiguation (WSD) aims to find the exact sense of an ambiguous word in a particular context. Traditional supervised methods rarely take into consideration the lexical resources like <a href="https://en.wikipedia.org/wiki/WordNet">WordNet</a>, which are widely utilized in knowledge-based methods. Recent studies have shown the effectiveness of incorporating gloss (sense definition) into <a href="https://en.wikipedia.org/wiki/Neural_network">neural networks</a> for WSD. However, compared with traditional word expert supervised methods, <a href="https://en.wikipedia.org/wiki/They">they</a> have not achieved much improvement. In this paper, we focus on how to better leverage <a href="https://en.wikipedia.org/wiki/Gloss_(annotation)">gloss knowledge</a> in a supervised neural WSD system. We construct context-gloss pairs and propose three BERT based models for WSD. We fine-tune the pre-trained BERT model and achieve new state-of-the-art results on WSD task.</abstract>
      <url hash="dd2b49e0">D19-1355</url>
      <doi>10.18653/v1/D19-1355</doi>
      <bibkey>huang-etal-2019-glossbert</bibkey>
      <pwccode url="https://github.com/HSLCY/GlossBERT" additional="true">HSLCY/GlossBERT</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/wic-tsv">WiC-TSV</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/word-sense-disambiguation-a-unified">Word Sense Disambiguation: a Unified Evaluation Framework and Empirical Comparison</pwcdataset>
    </paper>
    <paper id="357">
      <title>Bridging the Defined and the Defining : Exploiting Implicit Lexical Semantic Relations in Definition Modeling</title>
      <author><first>Koki</first><last>Washio</last></author>
      <author><first>Satoshi</first><last>Sekine</last></author>
      <author><first>Tsuneaki</first><last>Kato</last></author>
      <pages>3521–3527</pages>
      <abstract>Definition modeling includes acquiring <a href="https://en.wikipedia.org/wiki/Word_embedding">word embeddings</a> from <a href="https://en.wikipedia.org/wiki/Dictionary_definition">dictionary definitions</a> and generating definitions of words. While the meanings of defining words are important in dictionary definitions, it is crucial to capture the lexical semantic relations between defined words and defining words. However, thus far, the utilization of such <a href="https://en.wikipedia.org/wiki/Binary_relation">relations</a> has not been explored for definition modeling. In this paper, we propose definition modeling methods that use <a href="https://en.wikipedia.org/wiki/Lexical_semantics">lexical semantic relations</a>. To utilize implicit semantic relations in definitions, we use unsupervisedly obtained pattern-based word-pair embeddings that represent semantic relations of word pairs. Experimental results indicate that our methods improve the performance in learning embeddings from definitions, as well as definition generation.</abstract>
      <url hash="1d264906">D19-1357</url>
      <attachment hash="74735682">D19-1357.Attachment.zip</attachment>
      <doi>10.18653/v1/D19-1357</doi>
      <bibkey>washio-etal-2019-bridging</bibkey>
    </paper>
    <paper id="358">
      <title>Do n’t Just Scratch the Surface : Enhancing Word Representations for <a href="https://en.wikipedia.org/wiki/Korean_language">Korean</a> with Hanja<fixed-case>K</fixed-case>orean with Hanja</title>
      <author><first>Kang Min</first><last>Yoo</last></author>
      <author><first>Taeuk</first><last>Kim</last></author>
      <author><first>Sang-goo</first><last>Lee</last></author>
      <pages>3528–3533</pages>
      <abstract>We propose a simple yet effective approach for improving Korean word representations using additional linguistic annotation (i.e. Hanja). We employ cross-lingual transfer learning in training word representations by leveraging the fact that <a href="https://en.wikipedia.org/wiki/Hanja">Hanja</a> is closely related to <a href="https://en.wikipedia.org/wiki/Chinese_language">Chinese</a>. We evaluate the intrinsic quality of <a href="https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning">representations</a> learned through our approach using the <a href="https://en.wikipedia.org/wiki/Analogy">word analogy</a> and similarity tests. In addition, we demonstrate their effectiveness on several <a href="https://en.wikipedia.org/wiki/Downstream_(networking)">downstream tasks</a>, including a novel Korean news headline generation task.</abstract>
      <url hash="38af3cc7">D19-1358</url>
      <attachment hash="a264cccc">D19-1358.Attachment.zip</attachment>
      <doi>10.18653/v1/D19-1358</doi>
      <bibkey>yoo-etal-2019-dont</bibkey>
      <pwccode url="https://github.com/shin285/KOMORAN" additional="true">shin285/KOMORAN</pwccode>
    </paper>
    <paper id="360">
      <title>Hierarchical Meta-Embeddings for Code-Switching Named Entity Recognition</title>
      <author><first>Genta Indra</first><last>Winata</last></author>
      <author><first>Zhaojiang</first><last>Lin</last></author>
      <author><first>Jamin</first><last>Shin</last></author>
      <author><first>Zihan</first><last>Liu</last></author>
      <author><first>Pascale</first><last>Fung</last></author>
      <pages>3541–3547</pages>
      <abstract>In countries that speak multiple main languages, mixing up different languages within a conversation is commonly called <a href="https://en.wikipedia.org/wiki/Code-switching">code-switching</a>. Previous works addressing this challenge mainly focused on word-level aspects such as <a href="https://en.wikipedia.org/wiki/Word_embedding">word embeddings</a>. However, in many cases, languages share common subwords, especially for closely related languages, but also for languages that are seemingly irrelevant. Therefore, we propose Hierarchical Meta-Embeddings (HME) that learn to combine multiple monolingual word-level and subword-level embeddings to create language-agnostic lexical representations. On the task of <a href="https://en.wikipedia.org/wiki/Named-entity_recognition">Named Entity Recognition</a> for English-Spanish code-switching data, our model achieves the state-of-the-art performance in the multilingual settings. We also show that, in cross-lingual settings, our model not only leverages closely related languages, but also learns from languages with different roots. Finally, we show that combining different subunits are crucial for capturing code-switching entities.</abstract>
      <url hash="fd9c5e09">D19-1360</url>
      <doi>10.18653/v1/D19-1360</doi>
      <bibkey>winata-etal-2019-hierarchical</bibkey>
      <pwccode url="https://github.com/gentaiscool/meta-emb" additional="false">gentaiscool/meta-emb</pwccode>
    </paper>
    <paper id="361">
      <title>Fine-tune BERT with Sparse Self-Attention Mechanism<fixed-case>BERT</fixed-case> with Sparse Self-Attention Mechanism</title>
      <author><first>Baiyun</first><last>Cui</last></author>
      <author><first>Yingming</first><last>Li</last></author>
      <author><first>Ming</first><last>Chen</last></author>
      <author><first>Zhongfei</first><last>Zhang</last></author>
      <pages>3548–3553</pages>
      <abstract>In this paper, we develop a novel Sparse Self-Attention Fine-tuning model (referred as SSAF) which integrates sparsity into self-attention mechanism to enhance the fine-tuning performance of BERT. In particular, sparsity is introduced into the self-attention by replacing <a href="https://en.wikipedia.org/wiki/Softmax_function">softmax function</a> with a controllable sparse transformation when fine-tuning with BERT. It enables us to learn a structurally sparse attention distribution, which leads to a more interpretable representation for the whole input. The proposed <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> is evaluated on <a href="https://en.wikipedia.org/wiki/Sentiment_analysis">sentiment analysis</a>, <a href="https://en.wikipedia.org/wiki/Question_answering">question answering</a>, and <a href="https://en.wikipedia.org/wiki/Natural-language_understanding">natural language inference tasks</a>. The extensive experimental results across multiple datasets demonstrate its effectiveness and superiority to the <a href="https://en.wikipedia.org/wiki/Baseline_(medicine)">baseline methods</a>.</abstract>
      <url hash="80b4ffae">D19-1361</url>
      <doi>10.18653/v1/D19-1361</doi>
      <bibkey>cui-etal-2019-fine</bibkey>
    </paper>
    <paper id="362">
      <title>Feature-Dependent Confusion Matrices for Low-Resource NER Labeling with Noisy Labels<fixed-case>NER</fixed-case> Labeling with Noisy Labels</title>
      <author><first>Lukas</first><last>Lange</last></author>
      <author><first>Michael A.</first><last>Hedderich</last></author>
      <author><first>Dietrich</first><last>Klakow</last></author>
      <pages>3554–3559</pages>
      <abstract>In low-resource settings, the performance of supervised labeling models can be improved with automatically annotated or distantly supervised data, which is cheap to create but often noisy. Previous works have shown that significant improvements can be reached by injecting information about the confusion between clean and noisy labels in this additional training data into the classifier training. However, for noise estimation, these approaches either do not take the input <a href="https://en.wikipedia.org/wiki/Feature_(machine_learning)">features</a> (in our case word embeddings) into account, or they need to learn the noise modeling from scratch which can be difficult in a low-resource setting. We propose to cluster the training data using the input <a href="https://en.wikipedia.org/wiki/Feature_(machine_learning)">features</a> and then compute different <a href="https://en.wikipedia.org/wiki/Confusion_matrix">confusion matrices</a> for each <a href="https://en.wikipedia.org/wiki/Cluster_analysis">cluster</a>. To the best of our knowledge, our approach is the first to leverage feature-dependent noise modeling with pre-initialized confusion matrices. We evaluate on low-resource named entity recognition settings in several languages, showing that our methods improve upon other confusion-matrix based methods by up to 9 %.</abstract>
      <url hash="3dc28596">D19-1362</url>
      <doi>10.18653/v1/D19-1362</doi>
      <bibkey>lange-etal-2019-feature</bibkey>
      <pwccode url="https://github.com/uds-lsv/noise-matrix-ner" additional="false">uds-lsv/noise-matrix-ner</pwccode>
    </paper>
    <paper id="363">
      <title>A Multi-Pairwise Extension of Procrustes Analysis for Multilingual Word Translation<fixed-case>P</fixed-case>rocrustes Analysis for Multilingual Word Translation</title>
      <author><first>Hagai</first><last>Taitelbaum</last></author>
      <author><first>Gal</first><last>Chechik</last></author>
      <author><first>Jacob</first><last>Goldberger</last></author>
      <pages>3560–3565</pages>
      <abstract>In this paper we present a novel approach to simultaneously representing multiple languages in a common space. Procrustes Analysis (PA) is commonly used to find the optimal orthogonal word mapping in the <a href="https://en.wikipedia.org/wiki/Multilingualism">bilingual case</a>. The proposed Multi Pairwise Procrustes Analysis (MPPA) is a natural extension of the PA algorithm to multilingual word mapping. Unlike previous PA extensions that require a k-way dictionary, this approach requires only pairwise bilingual dictionaries that are much easier to construct.</abstract>
      <url hash="e1e0ebdf">D19-1363</url>
      <doi>10.18653/v1/D19-1363</doi>
      <bibkey>taitelbaum-etal-2019-multi</bibkey>
    </paper>
    <paper id="366">
      <title>Multiple Text Style Transfer by using Word-level Conditional Generative Adversarial Network with Two-Phase Training</title>
      <author><first>Chih-Te</first><last>Lai</last></author>
      <author><first>Yi-Te</first><last>Hong</last></author>
      <author><first>Hong-You</first><last>Chen</last></author>
      <author><first>Chi-Jen</first><last>Lu</last></author>
      <author><first>Shou-De</first><last>Lin</last></author>
      <pages>3579–3584</pages>
      <abstract>The objective of non-parallel text style transfer, or controllable text generation, is to alter specific attributes (e.g. sentiment, <a href="https://en.wikipedia.org/wiki/Mood_(psychology)">mood</a>, <a href="https://en.wikipedia.org/wiki/Grammatical_tense">tense</a>, <a href="https://en.wikipedia.org/wiki/Politeness">politeness</a>, etc) of a given text while preserving its remaining attributes and content. Generative adversarial network (GAN) is a popular model to ensure the transferred sentences are realistic and have the desired target styles. However, training GAN often suffers from mode collapse problem, which causes that the transferred text is little related to the original text. In this paper, we propose a new GAN model with a word-level conditional architecture and a two-phase training procedure. By using a style-related condition architecture before generating a word, our <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a> is able to maintain style-unrelated words while changing the others. By separating the training procedure into reconstruction and transfer phases, our <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a> is able to learn a proper text generation process, which further improves the content preservation. We test our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> on polarity sentiment transfer and multiple-attribute transfer tasks. The empirical results show that our model achieves comparable evaluation scores in both transfer accuracy and <a href="https://en.wikipedia.org/wiki/Fluency">fluency</a> but significantly outperforms other state-of-the-art models in content compatibility on three real-world datasets.</abstract>
      <url hash="31912278">D19-1366</url>
      <attachment hash="98552145">D19-1366.Attachment.zip</attachment>
      <doi>10.18653/v1/D19-1366</doi>
      <bibkey>lai-etal-2019-multiple</bibkey>
    </paper>
    <paper id="373">
      <title>Combining Global Sparse Gradients with Local Gradients in Distributed Neural Network Training</title>
      <author><first>Alham Fikri</first><last>Aji</last></author>
      <author><first>Kenneth</first><last>Heafield</last></author>
      <author><first>Nikolay</first><last>Bogoychev</last></author>
      <pages>3626–3631</pages>
      <abstract>One way to reduce network traffic in multi-node data-parallel stochastic gradient descent is to only exchange the largest gradients. However, doing so damages the <a href="https://en.wikipedia.org/wiki/Gradient">gradient</a> and degrades the <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a>’s performance. Transformer models degrade dramatically while the impact on <a href="https://en.wikipedia.org/wiki/Radio-frequency_identification">RNNs</a> is smaller. We restore gradient quality by combining the compressed global gradient with the node’s locally computed uncompressed gradient. Neural machine translation experiments show that Transformer convergence is restored while <a href="https://en.wikipedia.org/wiki/Random-access_memory">RNNs</a> converge faster. With our method, training on 4 nodes converges up to 1.5x as fast as with uncompressed gradients and scales 3.5x relative to single-node training.</abstract>
      <url hash="e1f6a600">D19-1373</url>
      <doi>10.18653/v1/D19-1373</doi>
      <bibkey>aji-etal-2019-combining</bibkey>
    </paper>
    <paper id="376">
      <title>PaLM : A Hybrid Parser and Language Model<fixed-case>P</fixed-case>a<fixed-case>LM</fixed-case>: A Hybrid Parser and Language Model</title>
      <author><first>Hao</first><last>Peng</last></author>
      <author><first>Roy</first><last>Schwartz</last></author>
      <author><first>Noah A.</first><last>Smith</last></author>
      <pages>3644–3651</pages>
      <abstract>We present PaLM, a hybrid parser and neural language model. Building on an RNN language model, PaLM adds an attention layer over text spans in the left context. An unsupervised constituency parser can be derived from its attention weights, using a greedy decoding algorithm. We evaluate PaLM on <a href="https://en.wikipedia.org/wiki/Language_model">language modeling</a>, and empirically show that it outperforms strong baselines. If syntactic annotations are available, the attention component can be trained in a supervised manner, providing syntactically-informed representations of the context, and further improving <a href="https://en.wikipedia.org/wiki/Language_model">language modeling</a> performance.</abstract>
      <url hash="dd7535c6">D19-1376</url>
      <attachment hash="11afd6f1">D19-1376.Attachment.pdf</attachment>
      <doi>10.18653/v1/D19-1376</doi>
      <bibkey>peng-etal-2019-palm</bibkey>
      <pwccode url="https://github.com/Noahs-ARK/PaLM" additional="false">Noahs-ARK/PaLM</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/penn-treebank">Penn Treebank</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikitext-2">WikiText-2</pwcdataset>
    </paper>
    <paper id="378">
      <title>Global Reasoning over Database Structures for Text-to-SQL Parsing<fixed-case>SQL</fixed-case> Parsing</title>
      <author><first>Ben</first><last>Bogin</last></author>
      <author><first>Matt</first><last>Gardner</last></author>
      <author><first>Jonathan</first><last>Berant</last></author>
      <pages>3659–3664</pages>
      <abstract>State-of-the-art semantic parsers rely on auto-regressive decoding, emitting one symbol at a time. When tested against complex databases that are unobserved at training time (zero-shot), the <a href="https://en.wikipedia.org/wiki/Parsing">parser</a> often struggles to select the correct set of database constants in the new database, due to the local nature of decoding. % since their decisions are based on weak, local information only. In this work, we propose a <a href="https://en.wikipedia.org/wiki/Semantic_parser">semantic parser</a> that globally reasons about the structure of the output query to make a more contextually-informed selection of database constants. We use <a href="https://en.wikipedia.org/wiki/Message_passing">message-passing</a> through a graph neural network to softly select a subset of <a href="https://en.wikipedia.org/wiki/Constant_(computer_programming)">database constants</a> for the output query, conditioned on the question. Moreover, we train a <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> to rank queries based on the global alignment of database constants to question words. We apply our techniques to the current state-of-the-art model for Spider, a zero-shot semantic parsing dataset with complex databases, increasing <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> from 39.4 % to 47.4 %.</abstract>
      <url hash="4e473b1d">D19-1378</url>
      <attachment hash="d96c44c2">D19-1378.Attachment.pdf</attachment>
      <doi>10.18653/v1/D19-1378</doi>
      <bibkey>bogin-etal-2019-global</bibkey>
      <pwccode url="https://github.com/benbogin/spider-schema-gnn-global" additional="false">benbogin/spider-schema-gnn-global</pwccode>
    </paper>
    <paper id="379">
      <title>Transductive Learning of Neural Language Models for Syntactic and Semantic Analysis</title>
      <author><first>Hiroki</first><last>Ouchi</last></author>
      <author><first>Jun</first><last>Suzuki</last></author>
      <author><first>Kentaro</first><last>Inui</last></author>
      <pages>3665–3671</pages>
      <abstract>In <a href="https://en.wikipedia.org/wiki/Transductive_learning">transductive learning</a>, an unlabeled test set is used for <a href="https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets">model training</a>. Although this <a href="https://en.wikipedia.org/wiki/Setting_(narrative)">setting</a> deviates from the common assumption of a completely unseen test set, it is applicable in many real-world scenarios, wherein the texts to be processed are known in advance. However, despite its practical advantages, <a href="https://en.wikipedia.org/wiki/Transductive_learning">transductive learning</a> is underexplored in <a href="https://en.wikipedia.org/wiki/Natural_language_processing">natural language processing</a>. Here we conduct an empirical study of <a href="https://en.wikipedia.org/wiki/Transductive_learning">transductive learning</a> for neural models and demonstrate its utility in syntactic and semantic tasks. Specifically, we fine-tune language models (LMs) on an unlabeled test set to obtain test-set-specific word representations. Through extensive experiments, we demonstrate that despite its simplicity, transductive LM fine-tuning consistently improves state-of-the-art neural models in in-domain and out-of-domain settings.</abstract>
      <url hash="bf1a8b29">D19-1379</url>
      <attachment hash="f7bff9a7">D19-1379.Attachment.pdf</attachment>
      <doi>10.18653/v1/D19-1379</doi>
      <bibkey>ouchi-etal-2019-transductive</bibkey>
    </paper>
    <paper id="380">
      <title>Efficient Sentence Embedding using <a href="https://en.wikipedia.org/wiki/Discrete_cosine_transform">Discrete Cosine Transform</a></title>
      <author><first>Nada</first><last>Almarwani</last></author>
      <author><first>Hanan</first><last>Aldarmaki</last></author>
      <author><first>Mona</first><last>Diab</last></author>
      <pages>3672–3678</pages>
      <abstract>Vector averaging remains one of the most popular sentence embedding methods in spite of its obvious disregard for <a href="https://en.wikipedia.org/wiki/Syntax">syntactic structure</a>. While more complex sequential or convolutional networks potentially yield superior <a href="https://en.wikipedia.org/wiki/Statistical_classification">classification</a> performance, the improvements in <a href="https://en.wikipedia.org/wiki/Statistical_classification">classification accuracy</a> are typically mediocre compared to the simple vector averaging. As an efficient alternative, we propose the use of discrete cosine transform (DCT) to compress word sequences in an order-preserving manner. The lower order DCT coefficients represent the overall feature patterns in sentences, which results in suitable embeddings for tasks that could benefit from syntactic features. Our results in semantic probing tasks demonstrate that DCT embeddings indeed preserve more syntactic information compared with vector averaging. With practically equivalent complexity, the model yields better overall performance in downstream classification tasks that correlate with syntactic features, which illustrates the capacity of <a href="https://en.wikipedia.org/wiki/Discrete_cosine_transform">DCT</a> to preserve word order information.</abstract>
      <url hash="b38f106a">D19-1380</url>
      <doi>10.18653/v1/D19-1380</doi>
      <bibkey>almarwani-etal-2019-efficient</bibkey>
      <pwccode url="https://github.com/N-Almarwani/DCT_Sentence_Embedding" additional="false">N-Almarwani/DCT_Sentence_Embedding</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/mpqa-opinion-corpus">MPQA Opinion Corpus</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/senteval">SentEval</pwcdataset>
    </paper>
    <paper id="381">
      <title>A Search-based Neural Model for Biomedical Nested and Overlapping Event Detection</title>
      <author><first>Kurt Junshean</first><last>Espinosa</last></author>
      <author><first>Makoto</first><last>Miwa</last></author>
      <author><first>Sophia</first><last>Ananiadou</last></author>
      <pages>3679–3686</pages>
      <abstract>We tackle the nested and overlapping event detection task and propose a novel search-based neural network (SBNN) structured prediction model that treats the task as a search problem on a relation graph of trigger-argument structures. Unlike existing structured prediction tasks such as dependency parsing, the task targets to detect DAG structures, which constitute <a href="https://en.wikipedia.org/wiki/Event_(computing)">events</a>, from the relation graph. We define actions to construct events and use all the beams in a <a href="https://en.wikipedia.org/wiki/Beam_search">beam search</a> to detect all event structures that may be overlapping and nested. The search process constructs <a href="https://en.wikipedia.org/wiki/Event_(probability_theory)">events</a> in a bottom-up manner while modelling the global properties for nested and overlapping structures simultaneously using <a href="https://en.wikipedia.org/wiki/Neural_network">neural networks</a>. We show that the model achieves performance comparable to the state-of-the-art model Turku Event Extraction System (TEES) on the BioNLP Cancer Genetics (CG) Shared Task 2013 without the use of any syntactic and hand-engineered features. Further analyses on the development set show that our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> is more computationally efficient while yielding higher <a href="https://en.wikipedia.org/wiki/F-number">F1-score</a> performance.</abstract>
      <url hash="f8e6654b">D19-1381</url>
      <doi>10.18653/v1/D19-1381</doi>
      <bibkey>espinosa-etal-2019-search</bibkey>
    </paper>
    <paper id="383">
      <title>Pretrained Language Models for Sequential Sentence Classification</title>
      <author><first>Arman</first><last>Cohan</last></author>
      <author><first>Iz</first><last>Beltagy</last></author>
      <author><first>Daniel</first><last>King</last></author>
      <author><first>Bhavana</first><last>Dalvi</last></author>
      <author><first>Dan</first><last>Weld</last></author>
      <pages>3693–3699</pages>
      <abstract>As a step toward better document-level understanding, we explore <a href="https://en.wikipedia.org/wiki/Categorization">classification</a> of a sequence of sentences into their corresponding categories, a task that requires understanding sentences in context of the document. Recent successful <a href="https://en.wikipedia.org/wiki/Conceptual_model">models</a> for this task have used hierarchical models to contextualize sentence representations, and Conditional Random Fields (CRFs) to incorporate dependencies between subsequent labels. In this work, we show that pretrained language models, BERT (Devlin et al., 2018) in particular, can be used for this task to capture contextual dependencies without the need for hierarchical encoding nor a CRF. Specifically, we construct a joint sentence representation that allows BERT Transformer layers to directly utilize contextual information from all words in all sentences. Our approach achieves state-of-the-art results on four <a href="https://en.wikipedia.org/wiki/Data_set">datasets</a>, including a new dataset of structured scientific abstracts.</abstract>
      <url hash="b3240e06">D19-1383</url>
      <doi>10.18653/v1/D19-1383</doi>
      <bibkey>cohan-etal-2019-pretrained</bibkey>
      <pwccode url="https://github.com/allenai/sequential_sentence_classification" additional="false">allenai/sequential_sentence_classification</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/csabstrcut-dataset">CSAbstruct Dataset</pwcdataset>
    </paper>
    <paper id="386">
      <title>Summary Cloze : A New Task for Content Selection in Topic-Focused Summarization</title>
      <author><first>Daniel</first><last>Deutsch</last></author>
      <author><first>Dan</first><last>Roth</last></author>
      <pages>3720–3729</pages>
      <abstract>A key challenge in topic-focused summarization is determining what information should be included in the summary, a problem known as content selection. In this work, we propose a new method for studying content selection in topic-focused summarization called the summary cloze task. The goal of the summary cloze task is to generate the next sentence of a summary conditioned on the beginning of the summary, a topic, and a reference document(s). The main challenge is deciding what information in the references is relevant to the topic and partial summary and should be included in the summary. Although the cloze task does not address all aspects of the traditional summarization problem, the more narrow scope of the <a href="https://en.wikipedia.org/wiki/Task_(computing)">task</a> allows us to collect a large-scale datset of nearly 500k summary cloze instances from Wikipedia. We report experimental results on this new dataset using various extractive models and a two-step abstractive model that first extractively selects a small number of sentences and then abstractively summarizes them. Our results show that the topic and partial summary help the <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> identify relevant content, but the task remains a significant challenge.</abstract>
      <url hash="4847f74b">D19-1386</url>
      <attachment hash="69d08f8c">D19-1386.Attachment.zip</attachment>
      <doi>10.18653/v1/D19-1386</doi>
      <bibkey>deutsch-roth-2019-summary</bibkey>
    </paper>
    <paper id="387">
      <title>Text Summarization with Pretrained Encoders</title>
      <author id="yang-liu"><first>Yang</first><last>Liu</last></author>
      <author><first>Mirella</first><last>Lapata</last></author>
      <pages>3730–3740</pages>
      <abstract>Bidirectional Encoder Representations from Transformers (BERT) represents the latest incarnation of pretrained language models which have recently advanced a wide range of natural language processing tasks. In this paper, we showcase how BERT can be usefully applied in <a href="https://en.wikipedia.org/wiki/Automatic_summarization">text summarization</a> and propose a general framework for both extractive and abstractive models. We introduce a novel document-level encoder based on BERT which is able to express the <a href="https://en.wikipedia.org/wiki/Semantics">semantics</a> of a document and obtain representations for its sentences. Our extractive model is built on top of this <a href="https://en.wikipedia.org/wiki/Encoder">encoder</a> by stacking several inter-sentence Transformer layers. For abstractive summarization, we propose a new fine-tuning schedule which adopts different optimizers for the <a href="https://en.wikipedia.org/wiki/Encoder">encoder</a> and the <a href="https://en.wikipedia.org/wiki/Code">decoder</a> as a means of alleviating the mismatch between the two (the former is pretrained while the latter is not). We also demonstrate that a two-staged fine-tuning approach can further boost the quality of the generated summaries. Experiments on three datasets show that our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> achieves state-of-the-art results across the board in both extractive and abstractive settings.</abstract>
      <url hash="473d159e">D19-1387</url>
      <attachment hash="f3fc37de">D19-1387.Attachment.pdf</attachment>
      <doi>10.18653/v1/D19-1387</doi>
      <bibkey>liu-lapata-2019-text</bibkey>
      <pwccode url="https://github.com/nlpyang/PreSumm" additional="true">nlpyang/PreSumm</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/cnn-daily-mail-1">CNN/Daily Mail</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/xsum">XSum</pwcdataset>
    </paper>
    <paper id="390">
      <title>Improving Latent Alignment in Text Summarization by Generalizing the Pointer Generator</title>
      <author><first>Xiaoyu</first><last>Shen</last></author>
      <author><first>Yang</first><last>Zhao</last></author>
      <author><first>Hui</first><last>Su</last></author>
      <author><first>Dietrich</first><last>Klakow</last></author>
      <pages>3762–3773</pages>
      <abstract>Pointer Generators have been the de facto standard for modern summarization systems. However, this architecture faces two major drawbacks : Firstly, the <a href="https://en.wikipedia.org/wiki/Pointer_(computer_programming)">pointer</a> is limited to copying the exact words while ignoring possible <a href="https://en.wikipedia.org/wiki/Inflection_point">inflections</a> or <a href="https://en.wikipedia.org/wiki/Abstraction_(computer_science)">abstractions</a>, which restricts its power of capturing richer latent alignment. Secondly, the copy mechanism results in a strong bias towards extractive generations, where most sentences are produced by simply copying from the source text. In this paper, we address these problems by allowing the <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a> to edit pointed tokens instead of always hard copying them. The editing is performed by transforming the pointed word vector into a target space with a learned <a href="https://en.wikipedia.org/wiki/Glossary_of_graph_theory_terms">relation embedding</a>. On three large-scale summarization dataset, we show the model is able to (1) capture more latent alignment relations than exact word matches, (2) improve word alignment accuracy, allowing for better model interpretation and controlling, (3) generate higher-quality summaries validated by both qualitative and quantitative evaluations and (4) bring more abstraction to the generated summaries.</abstract>
      <url hash="ae83d072">D19-1390</url>
      <attachment hash="fb1f601e">D19-1390.Attachment.pdf</attachment>
      <doi>10.18653/v1/D19-1390</doi>
      <bibkey>shen-etal-2019-improving</bibkey>
    </paper>
    <paper id="391">
      <title>Learning Semantic Parsers from Denotations with Latent Structured Alignments and Abstract Programs</title>
      <author><first>Bailin</first><last>Wang</last></author>
      <author><first>Ivan</first><last>Titov</last></author>
      <author><first>Mirella</first><last>Lapata</last></author>
      <pages>3774–3785</pages>
      <abstract>Semantic parsing aims to map natural language utterances onto machine interpretable meaning representations, aka programs whose execution against a real-world environment produces a <a href="https://en.wikipedia.org/wiki/Denotation">denotation</a>. Weakly-supervised semantic parsers are trained on utterance-denotation pairs treating <a href="https://en.wikipedia.org/wiki/Computer_program">programs</a> as latent. The task is challenging due to the large search space and spuriousness of programs which may execute to the correct answer but do not generalize to unseen examples. Our goal is to instill an <a href="https://en.wikipedia.org/wiki/Inductive_bias">inductive bias</a> in the <a href="https://en.wikipedia.org/wiki/Parsing">parser</a> to help it distinguish between spurious and correct programs. We capitalize on the intuition that correct programs would likely respect certain <a href="https://en.wikipedia.org/wiki/Structural_analysis">structural constraints</a> were they to be aligned to the question (e.g., program fragments are unlikely to align to overlapping text spans) and propose to model alignments as structured latent variables. In order to make the latent-alignment framework tractable, we decompose the parsing task into (1) predicting a partial abstract program and (2) refining it while modeling structured alignments with <a href="https://en.wikipedia.org/wiki/Differential_dynamic_programming">differential dynamic programming</a>. We obtain state-of-the-art performance on the WikiTableQuestions and WikiSQL datasets. When compared to a standard attention baseline, we observe that the proposed structured-alignment mechanism is highly beneficial.</abstract>
      <url hash="e06ebce3">D19-1391</url>
      <doi>10.18653/v1/D19-1391</doi>
      <bibkey>wang-etal-2019-learning</bibkey>
      <pwccode url="https://github.com/berlino/weaksp_em19" additional="false">berlino/weaksp_em19</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/wikitablequestions">WikiTableQuestions</pwcdataset>
    </paper>
    <paper id="392">
      <title>Broad-Coverage Semantic Parsing as Transduction</title>
      <author><first>Sheng</first><last>Zhang</last></author>
      <author><first>Xutai</first><last>Ma</last></author>
      <author><first>Kevin</first><last>Duh</last></author>
      <author><first>Benjamin</first><last>Van Durme</last></author>
      <pages>3786–3798</pages>
      <abstract>We unify different broad-coverage semantic parsing tasks into a transduction parsing paradigm, and propose an attention-based neural transducer that incrementally builds meaning representation via a sequence of semantic relations. By leveraging multiple attention mechanisms, the neural transducer can be effectively trained without relying on a pre-trained aligner. Experiments separately conducted on three broad-coverage semantic parsing tasks   AMR, SDP and UCCA   demonstrate that our attention-based neural transducer improves the state of the art on both AMR and UCCA, and is competitive with the state of the art on SDP.</abstract>
      <url hash="ef9e09c2">D19-1392</url>
      <doi>10.18653/v1/D19-1392</doi>
      <bibkey>zhang-etal-2019-broad</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/ldc2017t10">LDC2017T10</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/universal-dependencies">Universal Dependencies</pwcdataset>
    </paper>
    <paper id="397">
      <title>Looking Beyond Label Noise : Shifted Label Distribution Matters in Distantly Supervised Relation Extraction</title>
      <author><first>Qinyuan</first><last>Ye</last></author>
      <author><first>Liyuan</first><last>Liu</last></author>
      <author><first>Maosen</first><last>Zhang</last></author>
      <author><first>Xiang</first><last>Ren</last></author>
      <pages>3841–3850</pages>
      <abstract>In recent years there is a surge of interest in applying distant supervision (DS) to automatically generate training data for relation extraction (RE). In this paper, we study the problem what limits the performance of DS-trained neural models, conduct thorough analyses, and identify a factor that can influence the performance greatly, shifted label distribution. Specifically, we found this problem commonly exists in real-world DS datasets, and without special handing, typical DS-RE models can not automatically adapt to this shift, thus achieving deteriorated performance. To further validate our intuition, we develop a simple yet effective adaptation method for DS-trained models, bias adjustment, which updates <a href="https://en.wikipedia.org/wiki/Statistical_model">models</a> learned over the source domain (i.e., DS training set) with a label distribution estimated on the target domain (i.e., test set). Experiments demonstrate that bias adjustment achieves consistent performance gains on DS-trained models, especially on neural models, with an up to 23 % relative F1 improvement, which verifies our assumptions. Our code and data can be found at https://github.com/INK-USC/shifted-label-distribution.</abstract>
      <url hash="17fa7f8d">D19-1397</url>
      <attachment hash="7873afec">D19-1397.Attachment.pdf</attachment>
      <doi>10.18653/v1/D19-1397</doi>
      <bibkey>ye-etal-2019-looking</bibkey>
      <pwccode url="https://github.com/INK-USC/shifted-label-distribution" additional="false">INK-USC/shifted-label-distribution</pwccode>
    </paper>
    <paper id="398">
      <title>Easy First Relation Extraction with <a href="https://en.wikipedia.org/wiki/Redundancy_(information_theory)">Information Redundancy</a></title>
      <author><first>Shuai</first><last>Ma</last></author>
      <author><first>Gang</first><last>Wang</last></author>
      <author><first>Yansong</first><last>Feng</last></author>
      <author><first>Jinpeng</first><last>Huai</last></author>
      <pages>3851–3861</pages>
      <abstract>Many existing relation extraction (RE) models make decisions globally using integer linear programming (ILP). However, it is nontrivial to make use of <a href="https://en.wikipedia.org/wiki/Integer_linear_programming">integer linear programming</a> as a blackbox solver for <a href="https://en.wikipedia.org/wiki/RE_(complexity)">RE</a>. Its cost of time and memory may become unacceptable with the increase of data scale, and <a href="https://en.wikipedia.org/wiki/Redundancy_(information_theory)">redundant information</a> needs to be encoded cautiously for ILP. In this paper, we propose an easy first approach for relation extraction with information redundancies, embedded in the results produced by local sentence level extractors, during which conflict decisions are resolved with domain and uniqueness constraints. Information redundancies are leveraged to support both easy first collective inference for easy decisions in the first stage and ILP for hard decisions in a subsequent stage. Experimental study shows that our approach improves the <a href="https://en.wikipedia.org/wiki/Efficiency">efficiency</a> and <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> of RE, and outperforms both ILP and neural network-based methods.</abstract>
      <url hash="050b5a5f">D19-1398</url>
      <doi>10.18653/v1/D19-1398</doi>
      <bibkey>ma-etal-2019-easy</bibkey>
    </paper>
    <paper id="403">
      <title>Induction Networks for Few-Shot Text Classification</title>
      <author><first>Ruiying</first><last>Geng</last></author>
      <author><first>Binhua</first><last>Li</last></author>
      <author><first>Yongbin</first><last>Li</last></author>
      <author><first>Xiaodan</first><last>Zhu</last></author>
      <author><first>Ping</first><last>Jian</last></author>
      <author><first>Jian</first><last>Sun</last></author>
      <pages>3904–3913</pages>
      <abstract>Text classification tends to struggle when data is deficient or when <a href="https://en.wikipedia.org/wiki/Information_technology">it</a> needs to adapt to unseen classes. In such challenging scenarios, recent studies have used <a href="https://en.wikipedia.org/wiki/Meta-learning">meta-learning</a> to simulate the few-shot task, in which new queries are compared to a small support set at the sample-wise level. However, this sample-wise comparison may be severely disturbed by the various expressions in the same class. Therefore, we should be able to learn a general representation of each class in the support set and then compare it to new queries. In this paper, we propose a novel Induction Network to learn such a generalized class-wise representation, by innovatively leveraging the dynamic routing algorithm in <a href="https://en.wikipedia.org/wiki/Meta-learning">meta-learning</a>. In this way, we find the <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> is able to induce and generalize better. We evaluate the proposed model on a well-studied sentiment classification dataset (English) and a real-world dialogue intent classification dataset (Chinese). Experiment results show that on both datasets, the proposed model significantly outperforms the existing state-of-the-art approaches, proving the effectiveness of class-wise generalization in few-shot text classification.</abstract>
      <url hash="b711c6ee">D19-1403</url>
      <doi>10.18653/v1/D19-1403</doi>
      <bibkey>geng-etal-2019-induction</bibkey>
      <pwccode url="" additional="true" />
    </paper>
    <paper id="404">
      <title>Benchmarking Zero-shot Text Classification : Datasets, Evaluation and Entailment Approach</title>
      <author><first>Wenpeng</first><last>Yin</last></author>
      <author><first>Jamaal</first><last>Hay</last></author>
      <author><first>Dan</first><last>Roth</last></author>
      <pages>3914–3923</pages>
      <abstract>Zero-shot text classification (0Shot-TC) is a challenging NLU problem to which little attention has been paid by the research community. 0Shot-TC aims to associate an appropriate label with a piece of text, irrespective of the text domain and the aspect (e.g., topic, <a href="https://en.wikipedia.org/wiki/Emotion">emotion</a>, <a href="https://en.wikipedia.org/wiki/Event_(philosophy)">event</a>, etc.) described by the label. And there are only a few articles studying 0Shot-TC, all focusing only on topical categorization which, we argue, is just the tip of the iceberg in 0Shot-TC. In addition, the chaotic experiments in literature make no uniform comparison, which blurs the progress. This work benchmarks the 0Shot-TC problem by providing unified datasets, standardized evaluations, and state-of-the-art baselines. Our contributions include : i) The datasets we provide facilitate studying 0Shot-TC relative to conceptually different and diverse aspects : the topic aspect includes sports and politics as labels ; the emotion aspect includes joy and anger ; the situation aspect includes medical assistance and water shortage. ii) We extend the existing evaluation setup (label-partially-unseen)   given a dataset, train on some labels, test on all labels   to include a more challenging yet realistic evaluation label-fully-unseen 0Shot-TC (Chang et al., 2008), aiming at classifying text snippets without seeing task specific training data at all. iii) We unify the 0Shot-TC of diverse aspects within a textual entailment formulation and study it this way.</abstract>
      <url hash="9b6c515c">D19-1404</url>
      <doi>10.18653/v1/D19-1404</doi>
      <bibkey>yin-etal-2019-benchmarking</bibkey>
      <pwccode url="https://github.com/yinwenpeng/BenchmarkingZeroShot" additional="true">yinwenpeng/BenchmarkingZeroShot</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/fever">FEVER</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
    </paper>
    <paper id="409">
      <title>Judge the Judges : A Large-Scale Evaluation Study of Neural Language Models for Online Review Generation</title>
      <author><first>Cristina</first><last>Garbacea</last></author>
      <author><first>Samuel</first><last>Carton</last></author>
      <author><first>Shiyan</first><last>Yan</last></author>
      <author><first>Qiaozhu</first><last>Mei</last></author>
      <pages>3968–3981</pages>
      <abstract>We conduct a large-scale, systematic study to evaluate the existing evaluation methods for <a href="https://en.wikipedia.org/wiki/Natural-language_generation">natural language generation</a> in the context of generating online product reviews. We compare human-based evaluators with a variety of automated evaluation procedures, including discriminative evaluators that measure how well machine-generated text can be distinguished from human-written text, as well as word overlap metrics that assess how similar the generated text compares to human-written references. We determine to what extent these different evaluators agree on the <a href="https://en.wikipedia.org/wiki/Ranking">ranking</a> of a dozen of state-of-the-art generators for online product reviews. We find that human evaluators do not correlate well with discriminative evaluators, leaving a bigger question of whether adversarial accuracy is the correct objective for <a href="https://en.wikipedia.org/wiki/Natural-language_generation">natural language generation</a>. In general, distinguishing machine-generated text is challenging even for human evaluators, and human decisions correlate better with lexical overlaps. We find <a href="https://en.wikipedia.org/wiki/Lexical_diversity">lexical diversity</a> an intriguing metric that is indicative of the assessments of different evaluators. A post-experiment survey of participants provides insights into how to evaluate and improve the quality of <a href="https://en.wikipedia.org/wiki/Natural-language_generation">natural language generation systems</a>.</abstract>
      <url hash="249ef933">D19-1409</url>
      <attachment hash="b9ace279">D19-1409.Attachment.pdf</attachment>
      <doi>10.18653/v1/D19-1409</doi>
      <bibkey>garbacea-etal-2019-judge</bibkey>
      <pwccode url="https://github.com/Crista23/JudgeTheJudges" additional="false">Crista23/JudgeTheJudges</pwccode>
    </paper>
    <paper id="410">
      <title>Sentence-BERT : Sentence Embeddings using Siamese BERT-Networks<fixed-case>BERT</fixed-case>: Sentence Embeddings using <fixed-case>S</fixed-case>iamese <fixed-case>BERT</fixed-case>-Networks</title>
      <author><first>Nils</first><last>Reimers</last></author>
      <author><first>Iryna</first><last>Gurevych</last></author>
      <pages>3982–3992</pages>
      <abstract>BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). However, it requires that both sentences are fed into the network, which causes a massive computational overhead : Finding the most similar pair in a collection of 10,000 sentences requires about 50 million inference computations (~65 hours) with BERT. The construction of BERT makes it unsuitable for semantic similarity search as well as for <a href="https://en.wikipedia.org/wiki/Unsupervised_learning">unsupervised tasks</a> like <a href="https://en.wikipedia.org/wiki/Cluster_analysis">clustering</a>. In this publication, we present Sentence-BERT (SBERT), a modification of the pretrained BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. This reduces the effort for finding the most similar pair from 65 hours with BERT / RoBERTa to about 5 seconds with SBERT, while maintaining the accuracy from BERT. We evaluate SBERT and SRoBERTa on common STS tasks and transfer learning tasks, where it outperforms other state-of-the-art sentence embeddings methods.</abstract>
      <url hash="63f6c5c5">D19-1410</url>
      <doi>10.18653/v1/D19-1410</doi>
      <bibkey>reimers-gurevych-2019-sentence</bibkey>
      <pwccode url="https://github.com/UKPLab/sentence-transformers" additional="true">UKPLab/sentence-transformers</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mpqa-opinion-corpus">MPQA Opinion Corpus</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sick">SICK</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sts-benchmark">STS Benchmark</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/semantic-textual-similarity-2012-2016">Semantic Textual Similarity (2012 - 2016)</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/senteval">SentEval</pwcdataset>
    </paper>
    <paper id="411">
      <title>Learning Only from Relevant Keywords and Unlabeled Documents</title>
      <author><first>Nontawat</first><last>Charoenphakdee</last></author>
      <author><first>Jongyeong</first><last>Lee</last></author>
      <author><first>Yiping</first><last>Jin</last></author>
      <author><first>Dittaya</first><last>Wanvarie</last></author>
      <author><first>Masashi</first><last>Sugiyama</last></author>
      <pages>3993–4002</pages>
      <abstract>We consider a document classification problem where document labels are absent but only relevant keywords of a target class and unlabeled documents are given. Although <a href="https://en.wikipedia.org/wiki/Heuristic_(computer_science)">heuristic methods</a> based on pseudo-labeling have been considered, theoretical understanding of this problem has still been limited. Moreover, previous methods can not easily incorporate well-developed techniques in supervised text classification. In this paper, we propose a theoretically guaranteed learning framework that is simple to implement and has flexible choices of <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a>, e.g., linear models or <a href="https://en.wikipedia.org/wiki/Neural_network">neural networks</a>. We demonstrate how to optimize the area under the receiver operating characteristic curve (AUC) effectively and also discuss how to adjust it to optimize other well-known evaluation metrics such as the <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> and F1-measure. Finally, we show the effectiveness of our <a href="https://en.wikipedia.org/wiki/Software_framework">framework</a> using <a href="https://en.wikipedia.org/wiki/Benchmark_(computing)">benchmark datasets</a>.</abstract>
      <url hash="ea391956">D19-1411</url>
      <attachment hash="e9a42639">D19-1411.Attachment.rar</attachment>
      <doi>10.18653/v1/D19-1411</doi>
      <bibkey>charoenphakdee-etal-2019-learning</bibkey>
    </paper>
    <paper id="416">
      <title>Enhancing Variational Autoencoders with Mutual Information Neural Estimation for Text Generation</title>
      <author><first>Dong</first><last>Qian</last></author>
      <author><first>William K.</first><last>Cheung</last></author>
      <pages>4047–4057</pages>
      <abstract>While broadly applicable to many natural language processing (NLP) tasks, variational autoencoders (VAEs) are hard to train due to the posterior collapse issue where the <a href="https://en.wikipedia.org/wiki/Latent_variable">latent variable</a> fails to encode the input data effectively. Various approaches have been proposed to alleviate this problem to improve the capability of the <a href="https://en.wikipedia.org/wiki/Airborne_early_warning_and_control">VAE</a>. In this paper, we propose to introduce a mutual information (MI) term between the input and its <a href="https://en.wikipedia.org/wiki/Latent_variable">latent variable</a> to regularize the objective of the VAE. Since estimating the MI in the high-dimensional space is intractable, we employ <a href="https://en.wikipedia.org/wiki/Neural_network">neural networks</a> for the estimation of the MI and provide a <a href="https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets">training algorithm</a> based on the convex duality approach. Our experimental results on three benchmark datasets demonstrate that the proposed model, compared to the state-of-the-art baselines, exhibits less posterior collapse and has comparable or better performance in <a href="https://en.wikipedia.org/wiki/Language_model">language modeling</a> and text generation. We also qualitatively evaluate the inferred latent space and show that the proposed model can generate more reasonable and diverse sentences via linear interpolation in the latent space.</abstract>
      <url hash="2f52477f">D19-1416</url>
      <attachment hash="ed1c9857">D19-1416.Attachment.pdf</attachment>
      <doi>10.18653/v1/D19-1416</doi>
      <bibkey>qian-cheung-2019-enhancing</bibkey>
    </paper>
    <paper id="417">
      <title>Sampling Bias in Deep Active Classification : An Empirical Study</title>
      <author><first>Ameya</first><last>Prabhu</last></author>
      <author><first>Charles</first><last>Dognin</last></author>
      <author><first>Maneesh</first><last>Singh</last></author>
      <pages>4058–4068</pages>
      <abstract>The exploding cost and time needed for data labeling and <a href="https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets">model training</a> are bottlenecks for training DNN models on large datasets. Identifying smaller representative data samples with strategies like <a href="https://en.wikipedia.org/wiki/Active_learning">active learning</a> can help mitigate such bottlenecks. Previous works on <a href="https://en.wikipedia.org/wiki/Active_learning">active learning</a> in <a href="https://en.wikipedia.org/wiki/Natural_language_processing">NLP</a> identify the problem of <a href="https://en.wikipedia.org/wiki/Sampling_bias">sampling bias</a> in the samples acquired by uncertainty-based querying and develop costly approaches to address it. Using a large empirical study, we demonstrate that active set selection using the posterior entropy of deep models like FastText.zip (FTZ) is robust to sampling biases and to various algorithmic choices (query size and strategies) unlike that suggested by traditional literature. We also show that FTZ based query strategy produces sample sets similar to those from more sophisticated approaches (e.g ensemble networks). Finally, we show the effectiveness of the selected samples by creating tiny high-quality datasets, and utilizing them for fast and cheap training of large models. Based on the above, we propose a simple <a href="https://en.wikipedia.org/wiki/Baseline_(configuration_management)">baseline</a> for deep active text classification that outperforms the <a href="https://en.wikipedia.org/wiki/State_of_the_art">state of the art</a>. We expect the presented work to be useful and informative for <a href="https://en.wikipedia.org/wiki/Data_compression">dataset compression</a> and for problems involving active, semi-supervised or online learning scenarios. Code and models are available at : https://github.com/drimpossible/Sampling-Bias-Active-Learning.</abstract>
      <url hash="0f03fa2c">D19-1417</url>
      <attachment hash="57ddc445">D19-1417.Attachment.pdf</attachment>
      <doi>10.18653/v1/D19-1417</doi>
      <bibkey>prabhu-etal-2019-sampling</bibkey>
      <pwccode url="https://github.com/Xtra-Computing/thundersvm" additional="true">Xtra-Computing/thundersvm</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/ag-news">AG News</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/dbpedia">DBpedia</pwcdataset>
    </paper>
    <paper id="418">
      <title>Do n’t Take the Easy Way Out : Ensemble Based Methods for Avoiding Known Dataset Biases</title>
      <author><first>Christopher</first><last>Clark</last></author>
      <author><first>Mark</first><last>Yatskar</last></author>
      <author><first>Luke</first><last>Zettlemoyer</last></author>
      <pages>4069–4082</pages>
      <abstract>State-of-the-art models often make use of superficial patterns in the data that do not generalize well to out-of-domain or adversarial settings. For example, textual entailment models often learn that particular key words imply <a href="https://en.wikipedia.org/wiki/Logical_consequence">entailment</a>, irrespective of context, and visual question answering models learn to predict prototypical answers, without considering evidence in the image. In this paper, we show that if we have prior knowledge of such biases, we can train a <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> to be more robust to domain shift. Our method has two stages : we (1) train a naive model that makes predictions exclusively based on dataset biases, and (2) train a <a href="https://en.wikipedia.org/wiki/Robust_statistics">robust model</a> as part of an <a href="https://en.wikipedia.org/wiki/Statistical_ensemble_(mathematical_physics)">ensemble</a> with the naive one in order to encourage it to focus on other patterns in the data that are more likely to generalize. Experiments on five datasets with out-of-domain test sets show significantly improved robustness in all settings, including a 12 point gain on a changing priors visual question answering dataset and a 9 point gain on an adversarial question answering test set.</abstract>
      <url hash="46211f3c">D19-1418</url>
      <doi>10.18653/v1/D19-1418</doi>
      <bibkey>clark-etal-2019-dont</bibkey>
      <pwccode url="https://github.com/chrisc36/debias" additional="true">chrisc36/debias</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/triviaqa">TriviaQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/vqa-cp">VQA-CP</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/visual-question-answering">Visual Question Answering</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/visual-question-answering-v2-0">Visual Question Answering v2.0</pwcdataset>
    </paper>
    <paper id="422">
      <title>Hierarchically-Refined Label Attention Network for Sequence Labeling</title>
      <author><first>Leyang</first><last>Cui</last></author>
      <author><first>Yue</first><last>Zhang</last></author>
      <pages>4115–4128</pages>
      <abstract>CRF has been used as a powerful model for statistical sequence labeling. For neural sequence labeling, however, BiLSTM-CRF does not always lead to better results compared with BiLSTM-softmax local classification. This can be because the simple Markov label transition model of CRF does not give much information gain over strong <a href="https://en.wikipedia.org/wiki/Neural_coding">neural encoding</a>. For better representing label sequences, we investigate a hierarchically-refined label attention network, which explicitly leverages label embeddings and captures potential long-term label dependency by giving each word incrementally refined label distributions with hierarchical attention. Results on POS tagging, NER and CCG supertagging show that the proposed model not only improves the overall tagging accuracy with similar number of parameters, but also significantly speeds up the training and testing compared to BiLSTM-CRF.</abstract>
      <url hash="476d82e8">D19-1422</url>
      <attachment hash="d082bafa">D19-1422.Attachment.rar</attachment>
      <doi>10.18653/v1/D19-1422</doi>
      <bibkey>cui-zhang-2019-hierarchically</bibkey>
      <pwccode url="https://github.com/Nealcly/LAN" additional="true">Nealcly/LAN</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/ccgbank">CCGbank</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/ontonotes-5-0">OntoNotes 5.0</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/penn-treebank">Penn Treebank</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/universal-dependencies">Universal Dependencies</pwcdataset>
    </paper>
    <paper id="423">
      <title>Certified Robustness to Adversarial Word Substitutions</title>
      <author><first>Robin</first><last>Jia</last></author>
      <author><first>Aditi</first><last>Raghunathan</last></author>
      <author><first>Kerem</first><last>Göksel</last></author>
      <author><first>Percy</first><last>Liang</last></author>
      <pages>4129–4142</pages>
      <abstract>State-of-the-art <a href="https://en.wikipedia.org/wiki/Neuro-linguistic_programming">NLP models</a> can often be fooled by adversaries that apply seemingly innocuous label-preserving transformations (e.g., <a href="https://en.wikipedia.org/wiki/Paraphrase">paraphrasing</a>) to input text. The number of possible <a href="https://en.wikipedia.org/wiki/Transformation_(function)">transformations</a> scales exponentially with text length, so <a href="https://en.wikipedia.org/wiki/Data_augmentation">data augmentation</a> can not cover all <a href="https://en.wikipedia.org/wiki/Transformation_(function)">transformations</a> of an input. This paper considers one exponentially large family of label-preserving transformations, in which every word in the input can be replaced with a similar word. We train the first <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> that are provably robust to all word substitutions in this family. Our training procedure uses Interval Bound Propagation (IBP) to minimize an upper bound on the worst-case loss that any combination of word substitutions can induce. To evaluate models’ robustness to these transformations, we measure <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> on adversarially chosen word substitutions applied to test examples. Our IBP-trained models attain 75 % adversarial accuracy on both <a href="https://en.wikipedia.org/wiki/Sentiment_analysis">sentiment analysis</a> on <a href="https://en.wikipedia.org/wiki/IMDB">IMDB</a> and natural language inference on SNLI ; in comparison, on <a href="https://en.wikipedia.org/wiki/IMDB">IMDB</a>, models trained normally and ones trained with data augmentation achieve adversarial accuracy of only 12 % and 41 %, respectively.</abstract>
      <url hash="bb7f3d22">D19-1423</url>
      <attachment hash="46a6ecaa">D19-1423.Attachment.zip</attachment>
      <doi>10.18653/v1/D19-1423</doi>
      <bibkey>jia-etal-2019-certified</bibkey>
      <pwccode url="https://worksheets.codalab.org/worksheets/0x79feda5f1998497db75422eca8fcd689" additional="true">worksheets/0x79feda5f</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
    </paper>
    <paper id="424">
      <title>Visualizing and Understanding the Effectiveness of BERT<fixed-case>BERT</fixed-case></title>
      <author><first>Yaru</first><last>Hao</last></author>
      <author><first>Li</first><last>Dong</last></author>
      <author><first>Furu</first><last>Wei</last></author>
      <author><first>Ke</first><last>Xu</last></author>
      <pages>4143–4152</pages>
      <abstract>Language model pre-training, such as <a href="https://en.wikipedia.org/wiki/BERT">BERT</a>, has achieved remarkable results in many NLP tasks. However, it is unclear why the pre-training-then-fine-tuning paradigm can improve performance and generalization capability across different tasks. In this paper, we propose to visualize loss landscapes and <a href="https://en.wikipedia.org/wiki/Trajectory_optimization">optimization trajectories</a> of fine-tuning BERT on specific datasets. First, we find that pre-training reaches a good initial point across downstream tasks, which leads to wider optima and easier <a href="https://en.wikipedia.org/wiki/Mathematical_optimization">optimization</a> compared with training from scratch. We also demonstrate that the fine-tuning procedure is robust to <a href="https://en.wikipedia.org/wiki/Overfitting">overfitting</a>, even though BERT is highly over-parameterized for downstream tasks. Second, the visualization results indicate that fine-tuning BERT tends to generalize better because of the flat and wide optima, and the consistency between the training loss surface and the generalization error surface. Third, the lower layers of BERT are more invariant during <a href="https://en.wikipedia.org/wiki/Fine-tuning">fine-tuning</a>, which suggests that the <a href="https://en.wikipedia.org/wiki/Layers_(digital_image_editing)">layers</a> that are close to input learn more transferable representations of language.</abstract>
      <url hash="6207bd28">D19-1424</url>
      <doi>10.18653/v1/D19-1424</doi>
      <bibkey>hao-etal-2019-visualizing</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mrpc">MRPC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="425">
      <title>Topics to Avoid : Demoting Latent Confounds in <a href="https://en.wikipedia.org/wiki/Text_classification">Text Classification</a></title>
      <author><first>Sachin</first><last>Kumar</last></author>
      <author><first>Shuly</first><last>Wintner</last></author>
      <author><first>Noah A.</first><last>Smith</last></author>
      <author><first>Yulia</first><last>Tsvetkov</last></author>
      <pages>4153–4163</pages>
      <abstract>Despite impressive performance on many text classification tasks, <a href="https://en.wikipedia.org/wiki/Deep_learning">deep neural networks</a> tend to learn frequent superficial patterns that are specific to the training data and do not always generalize well. In this work, we observe this limitation with respect to the task of native language identification. We find that standard text classifiers which perform well on the test set end up learning topical features which are confounds of the <a href="https://en.wikipedia.org/wiki/Prediction">prediction task</a> (e.g., if the input text mentions Sweden, the <a href="https://en.wikipedia.org/wiki/Statistical_classification">classifier</a> predicts that the author’s native language is Swedish). We propose a method that represents the latent topical confounds and a model which unlearns confounding features by predicting both the label of the input text and the confound ; but we train the two predictors adversarially in an alternating fashion to learn a text representation that predicts the correct label but is less prone to using information about the confound. We show that this <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a> generalizes better and learns features that are indicative of the writing style rather than the content.<i>native language identification</i>. We find that standard text classifiers which perform well on the test set end up learning topical features which are confounds of the prediction task (e.g., if the input text mentions Sweden, the classifier predicts that the author’s native language is Swedish). We propose a method that represents the latent topical confounds and a model which “unlearns” confounding features by predicting both the label of the input text and the confound; but we train the two predictors adversarially in an alternating fashion to learn a text representation that predicts the correct label but is less prone to using information about the confound. We show that this model generalizes better and learns features that are indicative of the writing style rather than the content.</abstract>
      <url hash="0f642588">D19-1425</url>
      <doi>10.18653/v1/D19-1425</doi>
      <bibkey>kumar-etal-2019-topics</bibkey>
      <pwccode url="https://github.com/Sachin19/adversarial-classify" additional="false">Sachin19/adversarial-classify</pwccode>
    </paper>
    <paper id="426">
      <title>Learning to Ask for Conversational Machine Learning</title>
      <author><first>Shashank</first><last>Srivastava</last></author>
      <author><first>Igor</first><last>Labutov</last></author>
      <author><first>Tom</first><last>Mitchell</last></author>
      <pages>4164–4174</pages>
      <abstract>Natural language has recently been explored as a new medium of supervision for training <a href="https://en.wikipedia.org/wiki/Machine_learning">machine learning models</a>. Here, we explore learning classification tasks using <a href="https://en.wikipedia.org/wiki/Language">language</a> in a conversational setting   where the automated learner does not simply receive language input from a teacher, but can proactively engage the teacher by asking questions. We present a reinforcement learning framework, where the learner’s actions correspond to question types and the reward for asking a question is based on how the teacher’s response changes performance of the resulting machine learning model on the learning task. In this framework, learning good question-asking strategies corresponds to asking sequences of questions that maximize the cumulative (discounted) reward, and hence quickly lead to effective <a href="https://en.wikipedia.org/wiki/Statistical_classification">classifiers</a>. Empirical analysis across three domains shows that learned question-asking strategies expedite classifier training by asking appropriate questions at different points in the learning process. The approach allows learning classifiers from a blend of strategies, including learning from observations, explanations and clarifications.</abstract>
      <url hash="6e53e90c">D19-1426</url>
      <doi>10.18653/v1/D19-1426</doi>
      <bibkey>srivastava-etal-2019-learning</bibkey>
    </paper>
    <paper id="427">
      <title>Language Modeling for <a href="https://en.wikipedia.org/wiki/Code_switching">Code-Switching</a> : Evaluation, Integration of Monolingual Data, and Discriminative Training</title>
      <author><first>Hila</first><last>Gonen</last></author>
      <author><first>Yoav</first><last>Goldberg</last></author>
      <pages>4175–4185</pages>
      <abstract>We focus on the problem of <a href="https://en.wikipedia.org/wiki/Language_model">language modeling</a> for code-switched language, in the context of automatic speech recognition (ASR). Language modeling for code-switched language is challenging for (at least) three reasons : (1) lack of available large-scale code-switched data for training ; (2) lack of a replicable evaluation setup that is ASR directed yet isolates <a href="https://en.wikipedia.org/wiki/Language_model">language modeling</a> performance from the other intricacies of the ASR system ; and (3) the reliance on <a href="https://en.wikipedia.org/wiki/Generative_model">generative modeling</a>. We tackle these three issues : we propose an ASR-motivated evaluation setup which is decoupled from an ASR system and the choice of vocabulary, and provide an evaluation dataset for English-Spanish code-switching. This setup lends itself to a discriminative training approach, which we demonstrate to work better than generative language modeling. Finally, we explore a variety of training protocols and verify the effectiveness of training with large amounts of monolingual data followed by fine-tuning with small amounts of code-switched data, for both the generative and discriminative cases.</abstract>
      <url hash="05ae5fe3">D19-1427</url>
      <attachment hash="501af36a">D19-1427.Attachment.zip</attachment>
      <doi>10.18653/v1/D19-1427</doi>
      <bibkey>gonen-goldberg-2019-language</bibkey>
    </paper>
    <paper id="432">
      <title>Distributionally Robust Language Modeling</title>
      <author><first>Yonatan</first><last>Oren</last></author>
      <author><first>Shiori</first><last>Sagawa</last></author>
      <author><first>Tatsunori B.</first><last>Hashimoto</last></author>
      <author><first>Percy</first><last>Liang</last></author>
      <pages>4227–4237</pages>
      <abstract>Language models are generally trained on <a href="https://en.wikipedia.org/wiki/Data">data</a> spanning a wide range of topics (e.g., <a href="https://en.wikipedia.org/wiki/News">news</a>, <a href="https://en.wikipedia.org/wiki/Review">reviews</a>, <a href="https://en.wikipedia.org/wiki/Fiction">fiction</a>), but they might be applied to an a priori unknown target distribution (e.g., restaurant reviews). In this paper, we first show that training on text outside the test distribution can degrade test performance when using standard maximum likelihood (MLE) training. To remedy this without the knowledge of the test distribution, we propose an approach which trains a <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> that performs well over a wide range of potential test distributions. In particular, we derive a new distributionally robust optimization (DRO) procedure which minimizes the loss of the model over the worst-case mixture of topics with sufficient overlap with the training distribution. Our approach, called topic conditional value at risk (topic CVaR), obtains a 5.5 point perplexity reduction over <a href="https://en.wikipedia.org/wiki/Model-driven_engineering">MLE</a> when the language models are trained on a mixture of Yelp reviews and news and tested only on <a href="https://en.wikipedia.org/wiki/Review">reviews</a>.</abstract>
      <url hash="97d455c7">D19-1432</url>
      <doi>10.18653/v1/D19-1432</doi>
      <bibkey>oren-etal-2019-distributionally</bibkey>
      <pwccode url="https://worksheets.codalab.org/worksheets/0xf8122ebd24e94209a2a1764007509098" additional="false">worksheets/0xf8122ebd</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/billion-word-benchmark">Billion Word Benchmark</pwcdataset>
    </paper>
    <paper id="436">
      <title>ARAML : A Stable Adversarial Training Framework for Text Generation<fixed-case>ARAML</fixed-case>: A Stable Adversarial Training Framework for Text Generation</title>
      <author><first>Pei</first><last>Ke</last></author>
      <author><first>Fei</first><last>Huang</last></author>
      <author><first>Minlie</first><last>Huang</last></author>
      <author><first>Xiaoyan</first><last>Zhu</last></author>
      <pages>4271–4281</pages>
      <abstract>Most of the existing generative adversarial networks (GAN) for text generation suffer from the instability of reinforcement learning training algorithms such as policy gradient, leading to unstable performance. To tackle this problem, we propose a novel <a href="https://en.wikipedia.org/wiki/Software_framework">framework</a> called Adversarial Reward Augmented Maximum Likelihood (ARAML). During adversarial training, the <a href="https://en.wikipedia.org/wiki/Discriminator">discriminator</a> assigns rewards to samples which are acquired from a <a href="https://en.wikipedia.org/wiki/Stationary_distribution">stationary distribution</a> near the data rather than the generator’s distribution. The generator is optimized with <a href="https://en.wikipedia.org/wiki/Maximum_likelihood_estimation">maximum likelihood estimation</a> augmented by the discriminator’s rewards instead of policy gradient. Experiments show that our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> can outperform state-of-the-art text GANs with a more stable training process.</abstract>
      <url hash="3ce4d864">D19-1436</url>
      <doi>10.18653/v1/D19-1436</doi>
      <bibkey>ke-etal-2019-araml</bibkey>
      <pwccode url="https://github.com/kepei1106/ARAML" additional="false">kepei1106/ARAML</pwccode>
    </paper>
    <paper id="437">
      <title>FlowSeq : Non-Autoregressive Conditional Sequence Generation with Generative Flow<fixed-case>F</fixed-case>low<fixed-case>S</fixed-case>eq: Non-Autoregressive Conditional Sequence Generation with Generative Flow</title>
      <author><first>Xuezhe</first><last>Ma</last></author>
      <author><first>Chunting</first><last>Zhou</last></author>
      <author><first>Xian</first><last>Li</last></author>
      <author><first>Graham</first><last>Neubig</last></author>
      <author><first>Eduard</first><last>Hovy</last></author>
      <pages>4282–4292</pages>
      <abstract>Most sequence-to-sequence (seq2seq) models are <a href="https://en.wikipedia.org/wiki/Autoregressive_model">autoregressive</a> ; they generate each token by conditioning on previously generated tokens. In contrast, non-autoregressive seq2seq models generate all tokens in one pass, which leads to increased efficiency through <a href="https://en.wikipedia.org/wiki/Parallel_computing">parallel processing</a> on hardware such as <a href="https://en.wikipedia.org/wiki/Graphics_processing_unit">GPUs</a>. However, directly modeling the <a href="https://en.wikipedia.org/wiki/Joint_probability_distribution">joint distribution</a> of all tokens simultaneously is challenging, and even with increasingly complex model structures accuracy lags significantly behind <a href="https://en.wikipedia.org/wiki/Autoregressive_model">autoregressive models</a>. In this paper, we propose a simple, efficient, and effective <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> for non-autoregressive sequence generation using <a href="https://en.wikipedia.org/wiki/Latent_variable_model">latent variable models</a>. Specifically, we turn to generative flow, an elegant technique to model complex distributions using <a href="https://en.wikipedia.org/wiki/Neural_network">neural networks</a>, and design several layers of flow tailored for modeling the conditional density of sequential latent variables. We evaluate this <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> on three neural machine translation (NMT) benchmark datasets, achieving comparable performance with state-of-the-art non-autoregressive NMT models and almost constant decoding time w.r.t the sequence length.</abstract>
      <url hash="199e8b4d">D19-1437</url>
      <attachment hash="0f504b10">D19-1437.Attachment.zip</attachment>
      <doi>10.18653/v1/D19-1437</doi>
      <bibkey>ma-etal-2019-flowseq</bibkey>
      <pwccode url="https://github.com/XuezheMax/flowseq" additional="true">XuezheMax/flowseq</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/wmt-2014">WMT 2014</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wmt-2016">WMT 2016</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wmt-2016-news">WMT 2016 News</pwcdataset>
    </paper>
    <paper id="438">
      <title>Compositional Generalization for Primitive Substitutions</title>
      <author><first>Yuanpeng</first><last>Li</last></author>
      <author><first>Liang</first><last>Zhao</last></author>
      <author><first>Jianyu</first><last>Wang</last></author>
      <author><first>Joel</first><last>Hestness</last></author>
      <pages>4293–4302</pages>
      <abstract>Compositional generalization is a basic mechanism in human language learning, but current <a href="https://en.wikipedia.org/wiki/Neural_network">neural networks</a> lack such ability. In this paper, we conduct fundamental research for encoding compositionality in <a href="https://en.wikipedia.org/wiki/Neural_network">neural networks</a>. Conventional methods use a single representation for the input sentence, making it hard to apply prior knowledge of compositionality. In contrast, our approach leverages such knowledge with two representations, one generating attention maps, and the other mapping attended input words to output symbols. We reduce the <a href="https://en.wikipedia.org/wiki/Entropy">entropy</a> in each <a href="https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning">representation</a> to improve <a href="https://en.wikipedia.org/wiki/Generalization">generalization</a>. Our experiments demonstrate significant improvements over the conventional methods in five NLP tasks including instruction learning and <a href="https://en.wikipedia.org/wiki/Machine_translation">machine translation</a>. In the SCAN domain, <a href="https://en.wikipedia.org/wiki/Information_technology">it</a> boosts <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracies</a> from 14.0 % to 98.8 % in Jump task, and from 92.0 % to 99.7 % in TurnLeft task. It also beats human performance on a few-shot learning task. We hope the proposed approach can help ease future research towards human-level compositional language learning.</abstract>
      <url hash="08b8b083">D19-1438</url>
      <attachment hash="f44f9832">D19-1438.Attachment.zip</attachment>
      <doi>10.18653/v1/D19-1438</doi>
      <bibkey>li-etal-2019-compositional</bibkey>
      <pwccode url="https://github.com/yli1/CGPS" additional="false">yli1/CGPS</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/scan">SCAN</pwcdataset>
    </paper>
    <paper id="439">
      <title>WikiCREM : A Large Unsupervised Corpus for <a href="https://en.wikipedia.org/wiki/Coreference_resolution">Coreference Resolution</a><fixed-case>W</fixed-case>iki<fixed-case>CREM</fixed-case>: A Large Unsupervised Corpus for Coreference Resolution</title>
      <author><first>Vid</first><last>Kocijan</last></author>
      <author><first>Oana-Maria</first><last>Camburu</last></author>
      <author><first>Ana-Maria</first><last>Cretu</last></author>
      <author><first>Yordan</first><last>Yordanov</last></author>
      <author><first>Phil</first><last>Blunsom</last></author>
      <author><first>Thomas</first><last>Lukasiewicz</last></author>
      <pages>4303–4312</pages>
      <abstract>Pronoun resolution is a major area of <a href="https://en.wikipedia.org/wiki/Natural-language_understanding">natural language understanding</a>. However, large-scale training sets are still scarce, since manually labelling data is costly. In this work, we introduce WikiCREM (Wikipedia CoREferences Masked) a large-scale, yet accurate dataset of pronoun disambiguation instances. We use a language-model-based approach for pronoun resolution in combination with our WikiCREM dataset. We compare a series of models on a collection of diverse and challenging coreference resolution problems, where we match or outperform previous state-of-the-art approaches on 6 out of 7 datasets, such as GAP, DPR, WNLI, PDP, WinoBias, and WinoGender. We release our <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a> to be used off-the-shelf for solving pronoun disambiguation.</abstract>
      <url hash="95051606">D19-1439</url>
      <attachment hash="2c955f40">D19-1439.Attachment.zip</attachment>
      <doi>10.18653/v1/D19-1439</doi>
      <bibkey>kocijan-etal-2019-wikicrem</bibkey>
      <pwccode url="https://github.com/vid-koci/bert-commonsense" additional="false">vid-koci/bert-commonsense</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/wikicrem">WikiCREM</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wsc">WSC</pwcdataset>
    </paper>
    <paper id="440">
      <title>Identifying and Explaining Discriminative Attributes</title>
      <author><first>Armins</first><last>Stepanjans</last></author>
      <author><first>André</first><last>Freitas</last></author>
      <pages>4313–4322</pages>
      <abstract>Identifying what is at the center of the meaning of a word and what discriminates it from other words is a fundamental natural language inference task. This paper describes an explicit word vector representation model (WVM) to support the identification of discriminative attributes. A core contribution of the paper is a quantitative and qualitative comparative analysis of different types of data sources and Knowledge Bases in the construction of explainable and explicit WVMs : (i) <a href="https://en.wikipedia.org/wiki/Knowledge_graph">knowledge graphs</a> built from dictionary definitions, (ii) entity-attribute-relationships graphs derived from images and (iii) commonsense knowledge graphs. Using a detailed quantitative and qualitative analysis, we demonstrate that these data sources have complementary semantic aspects, supporting the creation of explicit semantic vector spaces. The explicit vector spaces are evaluated using the task of discriminative attribute identification, showing comparable performance to the state-of-the-art systems in the <a href="https://en.wikipedia.org/wiki/Task_(project_management)">task</a> (F1-score = 0.69), while delivering full model transparency and explainability.</abstract>
      <url hash="de678589">D19-1440</url>
      <doi>10.18653/v1/D19-1440</doi>
      <bibkey>stepanjans-freitas-2019-identifying</bibkey>
      <pwccode url="https://github.com/ab-10/Hawk" additional="false">ab-10/Hawk</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/visual-genome">Visual Genome</pwcdataset>
    </paper>
    <paper id="443">
      <title>Transformer Dissection : An Unified Understanding for Transformer’s Attention via the Lens of Kernel</title>
      <author><first>Yao-Hung Hubert</first><last>Tsai</last></author>
      <author><first>Shaojie</first><last>Bai</last></author>
      <author><first>Makoto</first><last>Yamada</last></author>
      <author><first>Louis-Philippe</first><last>Morency</last></author>
      <author><first>Ruslan</first><last>Salakhutdinov</last></author>
      <pages>4344–4353</pages>
      <abstract>Transformer is a powerful architecture that achieves superior performance on various sequence learning tasks, including <a href="https://en.wikipedia.org/wiki/Neural_machine_translation">neural machine translation</a>, <a href="https://en.wikipedia.org/wiki/Natural-language_understanding">language understanding</a>, and sequence prediction. At the core of the <a href="https://en.wikipedia.org/wiki/Transformer">Transformer</a> is the <a href="https://en.wikipedia.org/wiki/Attentional_control">attention mechanism</a>, which concurrently processes all inputs in the streams. In this paper, we present a new formulation of <a href="https://en.wikipedia.org/wiki/Attention">attention</a> via the lens of the <a href="https://en.wikipedia.org/wiki/Kernel_(linear_algebra)">kernel</a>. To be more precise, we realize that the <a href="https://en.wikipedia.org/wiki/Attention">attention</a> can be seen as applying kernel smoother over the inputs with the kernel scores being the similarities between inputs. This new formulation gives us a better way to understand individual components of the Transformer’s attention, such as the better way to integrate the positional embedding. Another important advantage of our kernel-based formulation is that it paves the way to a larger space of composing Transformer’s attention. As an example, we propose a new variant of Transformer’s attention which models the input as a product of symmetric kernels. This approach achieves competitive performance to the current state of the art <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a> with less <a href="https://en.wikipedia.org/wiki/Computation">computation</a>. In our experiments, we empirically study different kernel construction strategies on two widely used tasks : <a href="https://en.wikipedia.org/wiki/Neural_machine_translation">neural machine translation</a> and sequence prediction.</abstract>
      <url hash="ba859bb0">D19-1443</url>
      <doi>10.18653/v1/D19-1443</doi>
      <bibkey>tsai-etal-2019-transformer</bibkey>
    </paper>
    <paper id="444">
      <title>Learning to Learn and Predict : A Meta-Learning Approach for Multi-Label Classification</title>
      <author><first>Jiawei</first><last>Wu</last></author>
      <author><first>Wenhan</first><last>Xiong</last></author>
      <author><first>William Yang</first><last>Wang</last></author>
      <pages>4354–4364</pages>
      <abstract>Many tasks in <a href="https://en.wikipedia.org/wiki/Natural_language_processing">natural language processing</a> can be viewed as multi-label classification problems. However, most of the existing <a href="https://en.wikipedia.org/wiki/Statistical_model">models</a> are trained with the standard cross-entropy loss function and use a <a href="https://en.wikipedia.org/wiki/Linear_prediction">fixed prediction policy</a> (e.g., a threshold of 0.5) for all the labels, which completely ignores the <a href="https://en.wikipedia.org/wiki/Computational_complexity_theory">complexity</a> and dependencies among different labels. In this paper, we propose a meta-learning method to capture these complex label dependencies. More specifically, our method utilizes a meta-learner to jointly learn the <a href="https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets">training policies</a> and <a href="https://en.wikipedia.org/wiki/Prediction">prediction policies</a> for different labels. The training policies are then used to train the <a href="https://en.wikipedia.org/wiki/Statistical_classification">classifier</a> with the cross-entropy loss function, and the <a href="https://en.wikipedia.org/wiki/Prediction">prediction policies</a> are further implemented for <a href="https://en.wikipedia.org/wiki/Prediction">prediction</a>. Experimental results on fine-grained entity typing and text classification demonstrate that our proposed method can obtain more accurate multi-label classification results.</abstract>
      <url hash="3760c1bb">D19-1444</url>
      <doi>10.18653/v1/D19-1444</doi>
      <bibkey>wu-etal-2019-learning</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/figer">FIGER</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/rcv1">RCV1</pwcdataset>
    </paper>
    <paper id="445">
      <title>Revealing the Dark Secrets of BERT<fixed-case>BERT</fixed-case></title>
      <author><first>Olga</first><last>Kovaleva</last></author>
      <author><first>Alexey</first><last>Romanov</last></author>
      <author><first>Anna</first><last>Rogers</last></author>
      <author><first>Anna</first><last>Rumshisky</last></author>
      <pages>4365–4374</pages>
      <abstract>BERT-based architectures currently give state-of-the-art performance on many NLP tasks, but little is known about the exact mechanisms that contribute to its success. In the current work, we focus on the interpretation of self-attention, which is one of the fundamental underlying components of BERT. Using a subset of GLUE tasks and a set of handcrafted features-of-interest, we propose the methodology and carry out a qualitative and quantitative analysis of the information encoded by the individual BERT’s heads. Our findings suggest that there is a limited set of attention patterns that are repeated across different heads, indicating the overall model overparametrization. While different heads consistently use the same attention patterns, they have varying impact on performance across different tasks. We show that manually disabling attention in certain heads leads to a performance improvement over the regular fine-tuned BERT models.</abstract>
      <url hash="e7d5fc31">D19-1445</url>
      <attachment hash="a2703784">D19-1445.Attachment.zip</attachment>
      <doi>10.18653/v1/D19-1445</doi>
      <bibkey>kovaleva-etal-2019-revealing</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/framenet">FrameNet</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mrpc">MRPC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/qnli">QNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/superglue">SuperGLUE</pwcdataset>
    </paper>
    <paper id="446">
      <title>Machine Translation With Weakly Paired Documents</title>
      <author><first>Lijun</first><last>Wu</last></author>
      <author><first>Jinhua</first><last>Zhu</last></author>
      <author><first>Di</first><last>He</last></author>
      <author><first>Fei</first><last>Gao</last></author>
      <author><first>Tao</first><last>Qin</last></author>
      <author><first>Jianhuang</first><last>Lai</last></author>
      <author><first>Tie-Yan</first><last>Liu</last></author>
      <pages>4375–4384</pages>
      <abstract>Neural machine translation, which achieves near human-level performance in some languages, strongly relies on the large amounts of parallel sentences, which hinders its applicability to low-resource language pairs. Recent works explore the possibility of unsupervised machine translation with <a href="https://en.wikipedia.org/wiki/Monolingualism">monolingual data</a> only, leading to much lower <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> compared with the <a href="https://en.wikipedia.org/wiki/Supervised_learning">supervised one</a>. Observing that weakly paired bilingual documents are much easier to collect than <a href="https://en.wikipedia.org/wiki/Multilingualism">bilingual sentences</a>, e.g., from <a href="https://en.wikipedia.org/wiki/Wikipedia">Wikipedia</a>, news websites or books, in this paper, we investigate training translation models with weakly paired bilingual documents. Our <a href="https://en.wikipedia.org/wiki/Tactic_(method)">approach</a> contains two components. 1) We provide a simple approach to mine implicitly bilingual sentence pairs from document pairs which can then be used as supervised training signals. 2) We leverage the topic consistency of two weakly paired documents and learn the sentence translation model by constraining the word distribution-level alignments. We evaluate our method on weakly paired documents from Wikipedia on six tasks, the widely used WMT16 GermanEnglish, WMT13 SpanishEnglish and WMT16 RomanianEnglish translation tasks. We obtain 24.1/30.3, 28.1/27.6 and 30.1/27.6 BLEU points separately, outperforming previous results by more than 5 BLEU points in each direction and reducing the gap between unsupervised translation and supervised translation up to 50 %.<tex-math>\leftrightarrow</tex-math>English, WMT13 Spanish<tex-math>\leftrightarrow</tex-math>English and WMT16 Romanian<tex-math>\leftrightarrow</tex-math>English translation tasks. We obtain 24.1/30.3, 28.1/27.6 and 30.1/27.6 BLEU points separately, outperforming previous results by more than 5 BLEU points in each direction and reducing the gap between unsupervised translation and supervised translation up to 50%.</abstract>
      <url hash="cc0542ce">D19-1446</url>
      <doi>10.18653/v1/D19-1446</doi>
      <bibkey>wu-etal-2019-machine</bibkey>
    </paper>
    <paper id="448">
      <title>The Bottom-up Evolution of Representations in the Transformer : A Study with <a href="https://en.wikipedia.org/wiki/Machine_translation">Machine Translation</a> and <a href="https://en.wikipedia.org/wiki/Language_model">Language Modeling</a> Objectives</title>
      <author><first>Elena</first><last>Voita</last></author>
      <author><first>Rico</first><last>Sennrich</last></author>
      <author><first>Ivan</first><last>Titov</last></author>
      <pages>4396–4406</pages>
      <abstract>We seek to understand how the representations of individual tokens and the structure of the learned <a href="https://en.wikipedia.org/wiki/Feature_space">feature space</a> evolve between layers in <a href="https://en.wikipedia.org/wiki/Deep_learning">deep neural networks</a> under different learning objectives. We chose the Transformers for our analysis as they have been shown effective with various tasks, including machine translation (MT), standard left-to-right language models (LM) and masked language modeling (MLM). Previous work used black-box probing tasks to show that the representations learned by the Transformer differ significantly depending on the objective. In this work, we use canonical correlation analysis and mutual information estimators to study how information flows across Transformer layers and observe that the choice of the objective determines this process. For example, as you go from bottom to top layers, information about the past in left-to-right language models gets vanished and predictions about the future get formed. In contrast, for MLM, representations initially acquire information about the context around the token, partially forgetting the token identity and producing a more generalized token representation. The token identity then gets recreated at the top <a href="https://en.wikipedia.org/wiki/Multimedia_Messaging_Service">MLM layers</a>.</abstract>
      <url hash="9fb1462c">D19-1448</url>
      <attachment hash="ebcbb759">D19-1448.Attachment.zip</attachment>
      <doi>10.18653/v1/D19-1448</doi>
      <bibkey>voita-etal-2019-bottom</bibkey>
    </paper>
    <paper id="449">
      <title>Do We Really Need Fully Unsupervised Cross-Lingual Embeddings?</title>
      <author><first>Ivan</first><last>Vulić</last></author>
      <author><first>Goran</first><last>Glavaš</last></author>
      <author><first>Roi</first><last>Reichart</last></author>
      <author><first>Anna</first><last>Korhonen</last></author>
      <pages>4407–4418</pages>
      <abstract>Recent efforts in cross-lingual word embedding (CLWE) learning have predominantly focused on fully unsupervised approaches that project monolingual embeddings into a shared cross-lingual space without any cross-lingual signal. The lack of any <a href="https://en.wikipedia.org/wiki/Supervisor">supervision</a> makes such approaches conceptually attractive. Yet, their only core difference from (weakly) supervised projection-based CLWE methods is in the way they obtain a seed dictionary used to initialize an iterative self-learning procedure. The fully unsupervised methods have arguably become more robust, and their primary use case is CLWE induction for pairs of resource-poor and distant languages. In this paper, we question the ability of even the most robust unsupervised CLWE approaches to induce meaningful CLWEs in these more challenging settings. A series of bilingual lexicon induction (BLI) experiments with 15 diverse languages (210 language pairs) show that fully unsupervised CLWE methods still fail for a large number of language pairs (e.g., they yield zero BLI performance for 87/210 pairs). Even when they succeed, they never surpass the performance of weakly supervised methods (seeded with 500-1,000 translation pairs) using the same self-learning procedure in any BLI setup, and the gaps are often substantial. These findings call for revisiting the main motivations behind fully unsupervised CLWE methods.</abstract>
      <url hash="d684f2e4">D19-1449</url>
      <attachment hash="50f8ee59">D19-1449.Attachment.zip</attachment>
      <doi>10.18653/v1/D19-1449</doi>
      <bibkey>vulic-etal-2019-really</bibkey>
      <pwccode url="https://github.com/ivulic/panlex-bli" additional="false">ivulic/panlex-bli</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/panlex">Panlex</pwcdataset>
    </paper>
    <paper id="450">
      <title>Weakly-Supervised Concept-based Adversarial Learning for Cross-lingual Word Embeddings</title>
      <author><first>Haozhou</first><last>Wang</last></author>
      <author><first>James</first><last>Henderson</last></author>
      <author><first>Paola</first><last>Merlo</last></author>
      <pages>4419–4430</pages>
      <abstract>Distributed representations of words which map each word to a <a href="https://en.wikipedia.org/wiki/Continuous_or_discrete_variable">continuous vector</a> have proven useful in capturing important linguistic information not only in a single language but also across different languages. Current <a href="https://en.wikipedia.org/wiki/Unsupervised_learning">unsupervised adversarial approaches</a> show that it is possible to build a mapping matrix that aligns two sets of monolingual word embeddings without high quality parallel data, such as a <a href="https://en.wikipedia.org/wiki/Dictionary">dictionary</a> or a sentence-aligned corpus. However, without an additional step of <a href="https://en.wikipedia.org/wiki/Refinement_(computing)">refinement</a>, the preliminary <a href="https://en.wikipedia.org/wiki/Map_(mathematics)">mapping</a> learnt by these methods is unsatisfactory, leading to poor performance for typologically distant languages. In this paper, we propose a weakly-supervised adversarial training method to overcome this limitation, based on the intuition that mapping across languages is better done at the concept level than at the word level. We propose a concept-based adversarial training method which improves the performance of previous <a href="https://en.wikipedia.org/wiki/Unsupervised_learning">unsupervised adversarial methods</a> for most languages, and especially for typologically distant language pairs.</abstract>
      <url hash="0bcd037c">D19-1450</url>
      <doi>10.18653/v1/D19-1450</doi>
      <bibkey>wang-etal-2019-weakly</bibkey>
    </paper>
    <paper id="451">
      <title>Aligning Cross-Lingual Entities with Multi-Aspect Information</title>
      <author><first>Hsiu-Wei</first><last>Yang</last></author>
      <author><first>Yanyan</first><last>Zou</last></author>
      <author><first>Peng</first><last>Shi</last></author>
      <author><first>Wei</first><last>Lu</last></author>
      <author><first>Jimmy</first><last>Lin</last></author>
      <author><first>Xu</first><last>Sun</last></author>
      <pages>4431–4441</pages>
      <abstract>Multilingual knowledge graphs (KGs), such as YAGO and DBpedia, represent entities in different languages. The task of cross-lingual entity alignment is to match entities in a source language with their counterparts in target languages. In this work, we investigate embedding-based approaches to encode <a href="https://en.wikipedia.org/wiki/Entity–relationship_model">entities</a> from multilingual KGs into the same <a href="https://en.wikipedia.org/wiki/Vector_space">vector space</a>, where equivalent entities are close to each other. Specifically, we apply graph convolutional networks (GCNs) to combine multi-aspect information of entities, including topological connections, relations, and attributes of entities, to learn entity embeddings. To exploit the literal descriptions of entities expressed in different languages, we propose two uses of a pretrained multilingual BERT model to bridge cross-lingual gaps. We further propose two strategies to integrate GCN-based and BERT-based modules to boost performance. Extensive experiments on two benchmark datasets demonstrate that our <a href="https://en.wikipedia.org/wiki/Methodology">method</a> significantly outperforms existing <a href="https://en.wikipedia.org/wiki/System">systems</a>.</abstract>
      <url hash="3a73924c">D19-1451</url>
      <doi>10.18653/v1/D19-1451</doi>
      <bibkey>yang-etal-2019-aligning</bibkey>
      <pwccode url="https://github.com/h324yang/HMAN" additional="false">h324yang/HMAN</pwccode>
    </paper>
    <paper id="452">
      <title>Contrastive Language Adaptation for Cross-Lingual Stance Detection</title>
      <author><first>Mitra</first><last>Mohtarami</last></author>
      <author><first>James</first><last>Glass</last></author>
      <author><first>Preslav</first><last>Nakov</last></author>
      <pages>4442–4452</pages>
      <abstract>We study cross-lingual stance detection, which aims to leverage labeled data in one language to identify the relative perspective (or stance) of a given document with respect to a claim in a different target language. In particular, we introduce a novel contrastive language adaptation approach applied to memory networks, which ensures accurate alignment of stances in the source and target languages, and can effectively deal with the challenge of limited labeled data in the target language. The evaluation results on public benchmark datasets and comparison against current state-of-the-art approaches demonstrate the effectiveness of our approach.</abstract>
      <url hash="008bff5c">D19-1452</url>
      <doi>10.18653/v1/D19-1452</doi>
      <bibkey>mohtarami-etal-2019-contrastive</bibkey>
    </paper>
    <paper id="453">
      <title>Jointly Learning to Align and Translate with Transformer Models</title>
      <author><first>Sarthak</first><last>Garg</last></author>
      <author><first>Stephan</first><last>Peitz</last></author>
      <author><first>Udhyakumar</first><last>Nallasamy</last></author>
      <author><first>Matthias</first><last>Paulik</last></author>
      <pages>4453–4462</pages>
      <abstract>The state of the art in <a href="https://en.wikipedia.org/wiki/Machine_translation">machine translation (MT)</a> is governed by <a href="https://en.wikipedia.org/wiki/Neural_machine_translation">neural approaches</a>, which typically provide superior translation accuracy over <a href="https://en.wikipedia.org/wiki/Statistical_machine_translation">statistical approaches</a>. However, on the closely related task of <a href="https://en.wikipedia.org/wiki/Word_alignment">word alignment</a>, traditional statistical word alignment models often remain the go-to solution. In this paper, we present an approach to train a Transformer model to produce both accurate translations and alignments. We extract discrete alignments from the attention probabilities learnt during regular neural machine translation model training and leverage them in a multi-task framework to optimize towards translation and alignment objectives. We demonstrate that our approach produces competitive results compared to GIZA++ trained IBM alignment models without sacrificing translation accuracy and outperforms previous attempts on Transformer model based word alignment. Finally, by incorporating IBM model alignments into our multi-task training, we report significantly better alignment accuracies compared to GIZA++ on three publicly available data sets.</abstract>
      <url hash="2e033e15">D19-1453</url>
      <attachment hash="91699233">D19-1453.Attachment.zip</attachment>
      <doi>10.18653/v1/D19-1453</doi>
      <bibkey>garg-etal-2019-jointly</bibkey>
      <pwccode url="https://github.com/pytorch/fairseq" additional="false">pytorch/fairseq</pwccode>
    </paper>
    <paper id="455">
      <title>Self-Assembling Modular Networks for Interpretable Multi-Hop Reasoning</title>
      <author><first>Yichen</first><last>Jiang</last></author>
      <author><first>Mohit</first><last>Bansal</last></author>
      <pages>4474–4484</pages>
      <abstract>Multi-hop QA requires a model to connect multiple pieces of evidence scattered in a long context to answer the question. The recently proposed HotpotQA (Yang et al., 2018) dataset is comprised of questions embodying four different multi-hop reasoning paradigms (two bridge entity setups, checking multiple properties, and comparing two entities), making it challenging for a single neural network to handle all four. In this work, we present an interpretable, controller-based Self-Assembling Neural Modular Network (Hu et al., 2017, 2018) for multi-hop reasoning, where we design four novel modules (Find, Relocate, Compare, NoOp) to perform unique types of language reasoning. Based on a question, our layout controller RNN dynamically infers a series of reasoning modules to construct the entire <a href="https://en.wikipedia.org/wiki/Computer_network">network</a>. Empirically, we show that our dynamic, multi-hop modular network achieves significant improvements over the static, single-hop baseline (on both regular and adversarial evaluation). We further demonstrate the interpretability of our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> via three analyses. First, the controller can softly decompose the multi-hop question into multiple single-hop sub-questions to promote compositional reasoning behavior of the main network. Second, the <a href="https://en.wikipedia.org/wiki/Controller_(computing)">controller</a> can predict layouts that conform to the layouts designed by human experts. Finally, the intermediate module can infer the entity that connects two distantly-located supporting facts by addressing the sub-question from the controller.</abstract>
      <url hash="76f697fd">D19-1455</url>
      <doi>10.18653/v1/D19-1455</doi>
      <bibkey>jiang-bansal-2019-self</bibkey>
      <pwccode url="https://github.com/jiangycTarheel/NMN-MultiHopQA" additional="false">jiangycTarheel/NMN-MultiHopQA</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/hotpotqa">HotpotQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
    </paper>
    <paper id="459">
      <title>Taskmaster-1 : Toward a Realistic and Diverse Dialog Dataset</title>
      <author><first>Bill</first><last>Byrne</last></author>
      <author><first>Karthik</first><last>Krishnamoorthi</last></author>
      <author><first>Chinnadhurai</first><last>Sankar</last></author>
      <author><first>Arvind</first><last>Neelakantan</last></author>
      <author><first>Ben</first><last>Goodrich</last></author>
      <author><first>Daniel</first><last>Duckworth</last></author>
      <author><first>Semih</first><last>Yavuz</last></author>
      <author><first>Amit</first><last>Dubey</last></author>
      <author><first>Kyu-Young</first><last>Kim</last></author>
      <author><first>Andy</first><last>Cedilnik</last></author>
      <pages>4516–4525</pages>
      <abstract>A significant barrier to progress in data-driven approaches to building dialog systems is the lack of high quality, goal-oriented conversational data. To help satisfy this elementary requirement, we introduce the initial release of the Taskmaster-1 dataset which includes 13,215 task-based dialogs comprising six domains. Two <a href="https://en.wikipedia.org/wiki/Procedure_(term)">procedures</a> were used to create this <a href="https://en.wikipedia.org/wiki/Collection_(abstract_data_type)">collection</a>, each with unique advantages. The first involves a two-person, spoken Wizard of Oz (WOz) approach in which trained agents and crowdsourced workers interact to complete the task while the second is self-dialog in which crowdsourced workers write the entire dialog themselves. We do not restrict the workers to detailed scripts or to a small knowledge base and hence we observe that our <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> contains more realistic and diverse conversations in comparison to existing datasets. We offer several baseline models including state of the art neural seq2seq architectures with benchmark performance as well as qualitative human evaluations. Dialogs are labeled with <a href="https://en.wikipedia.org/wiki/Application_programming_interface">API calls</a> and arguments, a simple and cost effective approach which avoids the requirement of complex annotation schema. The layer of abstraction between the dialog model and the service provider API allows for a given <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a> to interact with multiple services that provide similar functionally. Finally, the dataset will evoke interest in <a href="https://en.wikipedia.org/wiki/Writing">written vs. spoken language</a>, <a href="https://en.wikipedia.org/wiki/Discourse_analysis">discourse patterns</a>, <a href="https://en.wikipedia.org/wiki/Error_detection_and_correction">error handling</a> and other <a href="https://en.wikipedia.org/wiki/Linguistic_description">linguistic phenomena</a> related to dialog system research, development and design.</abstract>
      <url hash="d882cf57">D19-1459</url>
      <doi>10.18653/v1/D19-1459</doi>
      <bibkey>byrne-etal-2019-taskmaster</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/taskmaster-1">Taskmaster-1</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/taskmaster-2">Taskmaster-2</pwcdataset>
    </paper>
    <paper id="460">
      <title>Multi-Domain Goal-Oriented Dialogues (MultiDoGO): Strategies toward Curating and Annotating Large Scale Dialogue Data<fixed-case>M</fixed-case>ulti<fixed-case>D</fixed-case>o<fixed-case>GO</fixed-case>): Strategies toward Curating and Annotating Large Scale Dialogue Data</title>
      <author><first>Denis</first><last>Peskov</last></author>
      <author><first>Nancy</first><last>Clarke</last></author>
      <author><first>Jason</first><last>Krone</last></author>
      <author><first>Brigi</first><last>Fodor</last></author>
      <author><first>Yi</first><last>Zhang</last></author>
      <author><first>Adel</first><last>Youssef</last></author>
      <author><first>Mona</first><last>Diab</last></author>
      <pages>4526–4536</pages>
      <abstract>The need for high-quality, large-scale, goal-oriented dialogue datasets continues to grow as <a href="https://en.wikipedia.org/wiki/Virtual_assistant">virtual assistants</a> become increasingly wide-spread. However, publicly available datasets useful for this area are limited either in their size, <a href="https://en.wikipedia.org/wiki/Linguistic_diversity">linguistic diversity</a>, domain coverage, or <a href="https://en.wikipedia.org/wiki/Granularity">annotation granularity</a>. In this paper, we present <a href="https://en.wikipedia.org/wiki/Strategy">strategies</a> toward curating and annotating large scale goal oriented dialogue data. We introduce the MultiDoGO dataset to overcome these limitations. With a total of over 81 K dialogues harvested across six domains, MultiDoGO is over 8 times the size of MultiWOZ, the other largest comparable dialogue dataset currently available to the public. Over 54 K of these harvested conversations are annotated for intent classes and slot labels. We adopt a Wizard-of-Oz approach wherein a crowd-sourced worker (the customer) is paired with a trained annotator (the agent). The data curation process was controlled via biases to ensure a diversity in dialogue flows following variable dialogue policies. We provide distinct class label tags for agents vs. customer utterances, along with applicable slot labels. We also compare and contrast our <a href="https://en.wikipedia.org/wiki/Strategy">strategies</a> on annotation granularity, i.e. turn vs. sentence level. Furthermore, we compare and contrast annotations curated by leveraging professional annotators vs the crowd. We believe our strategies for eliciting and annotating such a dialogue dataset scales across modalities and domains and potentially languages in the future. To demonstrate the efficacy of our devised strategies we establish neural baselines for <a href="https://en.wikipedia.org/wiki/Statistical_classification">classification</a> on the agent and customer utterances as well as slot labeling for each domain.</abstract>
      <url hash="4c03d59e">D19-1460</url>
      <attachment hash="c87dd1ad">D19-1460.Attachment.pdf</attachment>
      <doi>10.18653/v1/D19-1460</doi>
      <bibkey>peskov-etal-2019-multi</bibkey>
    </paper>
    <paper id="461">
      <title>Build it Break it Fix it for Dialogue Safety : Robustness from Adversarial Human Attack</title>
      <author><first>Emily</first><last>Dinan</last></author>
      <author><first>Samuel</first><last>Humeau</last></author>
      <author><first>Bharath</first><last>Chintagunta</last></author>
      <author><first>Jason</first><last>Weston</last></author>
      <pages>4537–4546</pages>
      <abstract>The detection of offensive language in the context of a <a href="https://en.wikipedia.org/wiki/Dialogue">dialogue</a> has become an increasingly important application of <a href="https://en.wikipedia.org/wiki/Natural_language_processing">natural language processing</a>. The detection of trolls in public forums (Galn-Garca et al., 2016), and the deployment of <a href="https://en.wikipedia.org/wiki/Chatbot">chatbots</a> in the <a href="https://en.wikipedia.org/wiki/Public_domain">public domain</a> (Wolf et al., 2017) are two examples that show the necessity of guarding against adversarially offensive behavior on the part of humans. In this work, we develop a training scheme for a <a href="https://en.wikipedia.org/wiki/Statistical_model">model</a> to become robust to such human attacks by an iterative build it, break it, fix it scheme with humans and models in the loop. In detailed experiments we show this <a href="https://en.wikipedia.org/wiki/Scientific_method">approach</a> is considerably more robust than previous <a href="https://en.wikipedia.org/wiki/System">systems</a>. Further, we show that offensive language used within a conversation critically depends on the dialogue context, and can not be viewed as a single sentence offensive detection task as in most previous work. Our newly collected <a href="https://en.wikipedia.org/wiki/Task_(project_management)">tasks</a> and methods are all made open source and publicly available.</abstract>
      <url hash="c08f12e3">D19-1461</url>
      <attachment hash="85902fb1">D19-1461.Attachment.zip</attachment>
      <doi>10.18653/v1/D19-1461</doi>
      <bibkey>dinan-etal-2019-build</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/convai2">ConvAI2</pwcdataset>
    </paper>
    <paper id="462">
      <title>GECOR : An End-to-End Generative Ellipsis and Co-reference Resolution Model for Task-Oriented Dialogue<fixed-case>GECOR</fixed-case>: An End-to-End Generative Ellipsis and Co-reference Resolution Model for Task-Oriented Dialogue</title>
      <author><first>Jun</first><last>Quan</last></author>
      <author><first>Deyi</first><last>Xiong</last></author>
      <author><first>Bonnie</first><last>Webber</last></author>
      <author><first>Changjian</first><last>Hu</last></author>
      <pages>4547–4557</pages>
      <abstract>Ellipsis and co-reference are common and ubiquitous especially in <a href="https://en.wikipedia.org/wiki/Dialogue">multi-turn dialogues</a>. In this paper, we treat the resolution of ellipsis and co-reference in <a href="https://en.wikipedia.org/wiki/Dialogue">dialogue</a> as a problem of generating omitted or referred expressions from the <a href="https://en.wikipedia.org/wiki/Dialogue">dialogue context</a>. We therefore propose a unified end-to-end Generative Ellipsis and CO-reference Resolution model (GECOR) in the context of dialogue. The <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a> can generate a new pragmatically complete user utterance by alternating the generation and copy mode for each user utterance. A multi-task learning framework is further proposed to integrate the GECOR into an end-to-end task-oriented dialogue. In order to train both the GECOR and the multi-task learning framework, we manually construct a new dataset on the basis of the public dataset CamRest676 with both ellipsis and co-reference annotation. On this dataset, intrinsic evaluations on the resolution of ellipsis and co-reference show that the GECOR model significantly outperforms the sequence-to-sequence (seq2seq) baseline model in terms of EM, BLEU and F1 while extrinsic evaluations on the downstream dialogue task demonstrate that our multi-task learning framework with GECOR achieves a higher success rate of task completion than TSCP, a state-of-the-art end-to-end task-oriented dialogue model.</abstract>
      <url hash="03b90349">D19-1462</url>
      <doi>10.18653/v1/D19-1462</doi>
      <bibkey>quan-etal-2019-gecor</bibkey>
    </paper>
    <paper id="464">
      <title>Aspect-based Sentiment Classification with Aspect-specific Graph Convolutional Networks</title>
      <author><first>Chen</first><last>Zhang</last></author>
      <author><first>Qiuchi</first><last>Li</last></author>
      <author><first>Dawei</first><last>Song</last></author>
      <pages>4568–4578</pages>
      <abstract>Due to their inherent capability in semantic alignment of aspects and their context words, attention mechanism and Convolutional Neural Networks (CNNs) are widely applied for aspect-based sentiment classification. However, these models lack a mechanism to account for relevant syntactical constraints and long-range word dependencies, and hence may mistakenly recognize syntactically irrelevant contextual words as clues for judging aspect sentiment. To tackle this problem, we propose to build a Graph Convolutional Network (GCN) over the dependency tree of a sentence to exploit syntactical information and word dependencies. Based on <a href="https://en.wikipedia.org/wiki/Information_technology">it</a>, a novel aspect-specific sentiment classification framework is raised. Experiments on three benchmarking collections illustrate that our proposed model has comparable effectiveness to a range of state-of-the-art models, and further demonstrate that both syntactical information and long-range word dependencies are properly captured by the graph convolution structure.</abstract>
      <url hash="b4212332">D19-1464</url>
      <doi>10.18653/v1/D19-1464</doi>
      <bibkey>zhang-etal-2019-aspect</bibkey>
      <pwccode url="https://github.com/GeneZC/ASGCN" additional="false">GeneZC/ASGCN</pwccode>
    </paper>
    <paper id="466">
      <title>Transferable End-to-End Aspect-based Sentiment Analysis with Selective Adversarial Learning</title>
      <author><first>Zheng</first><last>Li</last></author>
      <author><first>Xin</first><last>Li</last></author>
      <author><first>Ying</first><last>Wei</last></author>
      <author><first>Lidong</first><last>Bing</last></author>
      <author><first>Yu</first><last>Zhang</last></author>
      <author><first>Qiang</first><last>Yang</last></author>
      <pages>4590–4600</pages>
      <abstract>Joint extraction of aspects and sentiments can be effectively formulated as a sequence labeling problem. However, such formulation hinders the effectiveness of <a href="https://en.wikipedia.org/wiki/Supervised_learning">supervised methods</a> due to the lack of annotated sequence data in many domains. To address this issue, we firstly explore an unsupervised domain adaptation setting for this <a href="https://en.wikipedia.org/wiki/Task_(computing)">task</a>. Prior work can only use common syntactic relations between <a href="https://en.wikipedia.org/wiki/Grammatical_aspect">aspect</a> and opinion words to bridge the domain gaps, which highly relies on external linguistic resources. To resolve it, we propose a novel Selective Adversarial Learning (SAL) method to align the inferred correlation vectors that automatically capture their latent relations. The SAL method can dynamically learn an alignment weight for each word such that more important words can possess higher alignment weights to achieve fine-grained (word-level) adaptation. Empirically, extensive experiments demonstrate the effectiveness of the proposed SAL method.</abstract>
      <url hash="6fbc2b8b">D19-1466</url>
      <attachment hash="d5d26f98">D19-1466.Attachment.pdf</attachment>
      <doi>10.18653/v1/D19-1466</doi>
      <bibkey>li-etal-2019-transferable</bibkey>
      <pwccode url="https://github.com/hsqmlzno1/Transferable-E2E-ABSA" additional="false">hsqmlzno1/Transferable-E2E-ABSA</pwccode>
    </paper>
    <paper id="467">
      <title>CAN : Constrained Attention Networks for Multi-Aspect Sentiment Analysis<fixed-case>CAN</fixed-case>: Constrained Attention Networks for Multi-Aspect Sentiment Analysis</title>
      <author><first>Mengting</first><last>Hu</last></author>
      <author><first>Shiwan</first><last>Zhao</last></author>
      <author><first>Li</first><last>Zhang</last></author>
      <author><first>Keke</first><last>Cai</last></author>
      <author><first>Zhong</first><last>Su</last></author>
      <author><first>Renhong</first><last>Cheng</last></author>
      <author><first>Xiaowei</first><last>Shen</last></author>
      <pages>4601–4610</pages>
      <abstract>Aspect level sentiment classification is a fine-grained sentiment analysis task. To detect the sentiment towards a particular aspect in a sentence, previous studies have developed various attention-based methods for generating aspect-specific sentence representations. However, the <a href="https://en.wikipedia.org/wiki/Attention">attention</a> may inherently introduce <a href="https://en.wikipedia.org/wiki/Noise_(electronics)">noise</a> and downgrade the performance. In this paper, we propose constrained attention networks (CAN), a simple yet effective solution, to regularize the <a href="https://en.wikipedia.org/wiki/Attention">attention</a> for multi-aspect sentiment analysis, which alleviates the drawback of the <a href="https://en.wikipedia.org/wiki/Attention">attention mechanism</a>. Specifically, we introduce orthogonal regularization on multiple aspects and sparse regularization on each single aspect. Experimental results on two public datasets demonstrate the effectiveness of our approach. We further extend our approach to multi-task settings and outperform the state-of-the-art methods.</abstract>
      <url hash="9eb46fd3">D19-1467</url>
      <attachment hash="ab26dfec">D19-1467.Attachment.zip</attachment>
      <doi>10.18653/v1/D19-1467</doi>
      <bibkey>hu-etal-2019-constrained</bibkey>
    </paper>
    <paper id="468">
      <title>Leveraging Just a Few Keywords for Fine-Grained Aspect Detection Through Weakly Supervised Co-Training</title>
      <author><first>Giannis</first><last>Karamanolakis</last></author>
      <author><first>Daniel</first><last>Hsu</last></author>
      <author><first>Luis</first><last>Gravano</last></author>
      <pages>4611–4621</pages>
      <abstract>User-generated reviews can be decomposed into fine-grained segments (e.g., sentences, clauses), each evaluating a different aspect of the principal entity (e.g., <a href="https://en.wikipedia.org/wiki/Price">price</a>, <a href="https://en.wikipedia.org/wiki/Quality_(business)">quality</a>, appearance). Automatically detecting these aspects can be useful for both users and downstream opinion mining applications. Current supervised approaches for learning aspect classifiers require many fine-grained aspect labels, which are labor-intensive to obtain. And, unfortunately, <a href="https://en.wikipedia.org/wiki/Unsupervised_learning">unsupervised topic models</a> often fail to capture the aspects of interest. In this work, we consider weakly supervised approaches for training aspect classifiers that only require the user to provide a small set of seed words (i.e., weakly positive indicators) for the aspects of interest. First, we show that current weakly supervised approaches fail to leverage the predictive power of seed words for aspect detection. Next, we propose a student-teacher approach that effectively leverages seed words in a bag-of-words classifier (teacher) ; in turn, we use the teacher to train a second model (student) that is potentially more powerful (e.g., a neural network that uses pre-trained word embeddings). Finally, we show that iterative co-training can be used to cope with noisy seed words, leading to both improved teacher and student models. Our proposed approach consistently outperforms previous <a href="https://en.wikipedia.org/wiki/Supervised_learning">weakly supervised approaches</a> (by 14.1 absolute F1 points on average) in six different domains of product reviews and six multilingual datasets of restaurant reviews.</abstract>
      <url hash="8e961040">D19-1468</url>
      <attachment hash="988ef1d0">D19-1468.Attachment.zip</attachment>
      <doi>10.18653/v1/D19-1468</doi>
      <bibkey>karamanolakis-etal-2019-leveraging</bibkey>
      <pwccode url="https://github.com/gkaramanolakis/ISWD" additional="false">gkaramanolakis/ISWD</pwccode>
    </paper>
    <paper id="472">
      <title>Text-based inference of moral sentiment change</title>
      <author><first>Jing Yi</first><last>Xie</last></author>
      <author><first>Renato</first><last>Ferreira Pinto Junior</last></author>
      <author><first>Graeme</first><last>Hirst</last></author>
      <author><first>Yang</first><last>Xu</last></author>
      <pages>4654–4663</pages>
      <abstract>We present a text-based framework for investigating moral sentiment change of the public via <a href="https://en.wikipedia.org/wiki/Longitudinal_study">longitudinal corpora</a>. Our framework is based on the premise that language use can inform people’s moral perception toward right or wrong, and we build our <a href="https://en.wikipedia.org/wiki/Methodology">methodology</a> by exploring moral biases learned from diachronic word embeddings. We demonstrate how a parameter-free model supports inference of historical shifts in moral sentiment toward concepts such as <a href="https://en.wikipedia.org/wiki/Slavery">slavery</a> and <a href="https://en.wikipedia.org/wiki/Democracy">democracy</a> over centuries at three incremental levels : moral relevance, moral polarity, and fine-grained moral dimensions. We apply this methodology to visualizing moral time courses of individual concepts and analyzing the relations between psycholinguistic variables and rates of moral sentiment change at scale. Our work offers opportunities for applying <a href="https://en.wikipedia.org/wiki/Natural_language_processing">natural language processing</a> toward characterizing moral sentiment change in society.</abstract>
      <url hash="be2c059a">D19-1472</url>
      <attachment hash="5f4a824c">D19-1472.Attachment.zip</attachment>
      <doi>10.18653/v1/D19-1472</doi>
      <bibkey>xie-etal-2019-text</bibkey>
    </paper>
    <paper id="474">
      <title>Multilingual and Multi-Aspect Hate Speech Analysis</title>
      <author><first>Nedjma</first><last>Ousidhoum</last></author>
      <author><first>Zizheng</first><last>Lin</last></author>
      <author><first>Hongming</first><last>Zhang</last></author>
      <author><first>Yangqiu</first><last>Song</last></author>
      <author><first>Dit-Yan</first><last>Yeung</last></author>
      <pages>4675–4684</pages>
      <abstract>Current research on hate speech analysis is typically oriented towards monolingual and single classification tasks. In this paper, we present a new multilingual multi-aspect hate speech analysis dataset and use it to test the current state-of-the-art multilingual multitask learning approaches. We evaluate our dataset in various classification settings, then we discuss how to leverage our annotations in order to improve hate speech detection and classification in general.</abstract>
      <url hash="a5dc94c2">D19-1474</url>
      <doi>10.18653/v1/D19-1474</doi>
      <bibkey>ousidhoum-etal-2019-multilingual</bibkey>
      <pwccode url="https://github.com/HKUST-KnowComp/MLMA_hate_speech" additional="false">HKUST-KnowComp/MLMA_hate_speech</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/mlma-hate-speech">MLMA Hate Speech</pwcdataset>
    </paper>
    <paper id="476">
      <title>A Deep Neural Information Fusion Architecture for Textual Network Embeddings</title>
      <author><first>Zenan</first><last>Xu</last></author>
      <author><first>Qinliang</first><last>Su</last></author>
      <author><first>Xiaojun</first><last>Quan</last></author>
      <author><first>Weijia</first><last>Zhang</last></author>
      <pages>4698–4706</pages>
      <abstract>Textual network embeddings aim to learn a low-dimensional representation for every node in the <a href="https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)">network</a> so that both the structural and textual information from the networks can be well preserved in the representations. Traditionally, the structural and textual embeddings were learned by <a href="https://en.wikipedia.org/wiki/Conceptual_model">models</a> that rarely take the mutual influences between them into account. In this paper, a <a href="https://en.wikipedia.org/wiki/Deep_learning">deep neural architecture</a> is proposed to effectively fuse the two kinds of informations into one <a href="https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning">representation</a>. The novelties of the proposed architecture are manifested in the aspects of a newly defined <a href="https://en.wikipedia.org/wiki/Loss_function">objective function</a>, the complementary information fusion method for structural and textual features, and the mutual gate mechanism for textual feature extraction. Experimental results show that the proposed <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> outperforms the comparing methods on all three datasets.</abstract>
      <url hash="c7c8dda8">D19-1476</url>
      <doi>10.18653/v1/D19-1476</doi>
      <bibkey>xu-etal-2019-deep</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/cora">Cora</pwcdataset>
    </paper>
    <paper id="477">
      <title>You Shall Know a User by the Company It Keeps : Dynamic Representations for Social Media Users in NLP<fixed-case>NLP</fixed-case></title>
      <author><first>Marco</first><last>Del Tredici</last></author>
      <author><first>Diego</first><last>Marcheggiani</last></author>
      <author><first>Sabine</first><last>Schulte im Walde</last></author>
      <author><first>Raquel</first><last>Fernández</last></author>
      <pages>4707–4717</pages>
      <abstract>Information about individuals can help to better understand what they say, particularly in <a href="https://en.wikipedia.org/wiki/Social_media">social media</a> where texts are short. Current approaches to modelling social media users pay attention to their social connections, but exploit this information in a static way, treating all connections uniformly. This ignores the fact, well known in <a href="https://en.wikipedia.org/wiki/Sociolinguistics">sociolinguistics</a>, that an individual may be part of several communities which are not equally relevant in all communicative situations. We present a <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> based on Graph Attention Networks that captures this observation. It dynamically explores the <a href="https://en.wikipedia.org/wiki/Social_graph">social graph</a> of a user, computes a user representation given the most relevant connections for a target task, and combines it with linguistic information to make a prediction. We apply our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> to three different tasks, evaluate it against alternative <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a>, and analyse the results extensively, showing that it significantly outperforms other current methods.</abstract>
      <url hash="b09fd31f">D19-1477</url>
      <attachment hash="41c5ec4e">D19-1477.Attachment.pdf</attachment>
      <doi>10.18653/v1/D19-1477</doi>
      <bibkey>del-tredici-etal-2019-shall</bibkey>
    </paper>
    <paper id="482">
      <title>A Benchmark Dataset for Learning to Intervene in Online Hate Speech</title>
      <author><first>Jing</first><last>Qian</last></author>
      <author><first>Anna</first><last>Bethke</last></author>
      <author><first>Yinyin</first><last>Liu</last></author>
      <author><first>Elizabeth</first><last>Belding</last></author>
      <author><first>William Yang</first><last>Wang</last></author>
      <pages>4755–4764</pages>
      <abstract>Countering <a href="https://en.wikipedia.org/wiki/Online_hate_speech">online hate speech</a> is a critical yet challenging task, but one which can be aided by the use of Natural Language Processing (NLP) techniques. Previous research has primarily focused on the development of NLP methods to automatically and effectively detect online hate speech while disregarding further action needed to calm and discourage individuals from using <a href="https://en.wikipedia.org/wiki/Hate_speech">hate speech</a> in the future. In addition, most existing hate speech datasets treat each post as an isolated instance, ignoring the <a href="https://en.wikipedia.org/wiki/Context_(language_use)">conversational context</a>. In this paper, we propose a novel task of generative hate speech intervention, where the goal is to automatically generate responses to intervene during online conversations that contain <a href="https://en.wikipedia.org/wiki/Hate_speech">hate speech</a>. As a part of this work, we introduce two fully-labeled large-scale hate speech intervention datasets collected from Gab and <a href="https://en.wikipedia.org/wiki/Reddit">Reddit</a>. These datasets provide conversation segments, hate speech labels, as well as intervention responses written by Mechanical Turk Workers. In this paper, we also analyze the <a href="https://en.wikipedia.org/wiki/Data_set">datasets</a> to understand the common intervention strategies and explore the performance of common automatic response generation methods on these new <a href="https://en.wikipedia.org/wiki/Data_set">datasets</a> to provide a benchmark for future research.</abstract>
      <url hash="79dc6510">D19-1482</url>
      <doi>10.18653/v1/D19-1482</doi>
      <bibkey>qian-etal-2019-benchmark</bibkey>
      <pwccode url="https://github.com/jing-qian/A-Benchmark-Dataset-for-Learning-to-Intervene-in-Online-Hate-Speech" additional="false">jing-qian/A-Benchmark-Dataset-for-Learning-to-Intervene-in-Online-Hate-Speech</pwccode>
    </paper>
    <paper id="484">
      <title>CodeSwitch-Reddit : Exploration of Written Multilingual Discourse in Online Discussion Forums<fixed-case>C</fixed-case>ode<fixed-case>S</fixed-case>witch-<fixed-case>R</fixed-case>eddit: Exploration of Written Multilingual Discourse in Online Discussion Forums</title>
      <author><first>Ella</first><last>Rabinovich</last></author>
      <author><first>Masih</first><last>Sultani</last></author>
      <author><first>Suzanne</first><last>Stevenson</last></author>
      <pages>4776–4786</pages>
      <abstract>In contrast to many decades of research on oral code-switching, the study of written multilingual productions has only recently enjoyed a surge of interest. Many open questions remain regarding the sociolinguistic underpinnings of written code-switching, and progress has been limited by a lack of suitable resources. We introduce a novel, large, and diverse <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> of written code-switched productions, curated from topical threads of multiple bilingual communities on the Reddit discussion platform, and explore questions that were mainly addressed in the context of <a href="https://en.wikipedia.org/wiki/Spoken_language">spoken language</a> thus far. We investigate whether findings in oral code-switching concerning content and style, as well as speaker proficiency, are carried over into <a href="https://en.wikipedia.org/wiki/Code-switching">written code-switching</a> in <a href="https://en.wikipedia.org/wiki/Internet_forum">discussion forums</a>. The released <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> can further facilitate a range of research and practical activities.</abstract>
      <url hash="e89ebb6a">D19-1484</url>
      <attachment hash="ab858988">D19-1484.Attachment.zip</attachment>
      <doi>10.18653/v1/D19-1484</doi>
      <bibkey>rabinovich-etal-2019-codeswitch</bibkey>
      <pwccode url="https://github.com/ellarabi/CodeSwitch-Reddit" additional="false">ellarabi/CodeSwitch-Reddit</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/codeswitch-reddit">CodeSwitch-Reddit</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/gyafc">GYAFC</pwcdataset>
    </paper>
    <paper id="485">
      <title>Modeling Conversation Structure and Temporal Dynamics for Jointly Predicting Rumor Stance and Veracity</title>
      <author><first>Penghui</first><last>Wei</last></author>
      <author><first>Nan</first><last>Xu</last></author>
      <author><first>Wenji</first><last>Mao</last></author>
      <pages>4787–4798</pages>
      <abstract>Automatically verifying rumorous information has become an important and challenging task in <a href="https://en.wikipedia.org/wiki/Natural_language_processing">natural language processing</a> and <a href="https://en.wikipedia.org/wiki/Social_media_analytics">social media analytics</a>. Previous studies reveal that people’s stances towards rumorous messages can provide indicative clues for identifying the veracity of rumors, and thus determining the stances of public reactions is a crucial preceding step for rumor veracity prediction. In this paper, we propose a hierarchical multi-task learning framework for jointly predicting rumor stance and veracity on <a href="https://en.wikipedia.org/wiki/Twitter">Twitter</a>, which consists of two components. The bottom component of our framework classifies the stances of tweets in a conversation discussing a rumor via modeling the structural property based on a novel graph convolutional network. The top component predicts the rumor veracity by exploiting the temporal dynamics of stance evolution. Experimental results on two benchmark datasets show that our method outperforms previous methods in both rumor stance classification and veracity prediction.</abstract>
      <url hash="448b4231">D19-1485</url>
      <doi>10.18653/v1/D19-1485</doi>
      <bibkey>wei-etal-2019-modeling</bibkey>
    </paper>
    <paper id="487">
      <title>Domain Adaptation for Person-Job Fit with Transferable Deep Global Match Network</title>
      <author><first>Shuqing</first><last>Bian</last></author>
      <author><first>Wayne Xin</first><last>Zhao</last></author>
      <author><first>Yang</first><last>Song</last></author>
      <author><first>Tao</first><last>Zhang</last></author>
      <author><first>Ji-Rong</first><last>Wen</last></author>
      <pages>4810–4820</pages>
      <abstract>Person-job fit has been an important task which aims to automatically match job positions with suitable candidates. Previous methods mainly focus on solving the match task in single-domain setting, which may not work well when labeled data is limited. We study the domain adaptation problem for person-job fit. We first propose a deep global match network for capturing the global semantic interactions between two sentences from a job posting and a candidate resume respectively. Furthermore, we extend the match network and implement domain adaptation in three levels, sentence-level representation, sentence-level match, and global match. Extensive experiment results on a large real-world dataset consisting of six domains have demonstrated the effectiveness of the proposed model, especially when there is not sufficient labeled data.</abstract>
      <url hash="f515e254">D19-1487</url>
      <doi>10.18653/v1/D19-1487</doi>
      <bibkey>bian-etal-2019-domain</bibkey>
    </paper>
    <paper id="488">
      <title>Heterogeneous Graph Attention Networks for Semi-supervised Short Text Classification</title>
      <author><first>Hu</first><last>Linmei</last></author>
      <author><first>Tianchi</first><last>Yang</last></author>
      <author><first>Chuan</first><last>Shi</last></author>
      <author><first>Houye</first><last>Ji</last></author>
      <author><first>Xiaoli</first><last>Li</last></author>
      <pages>4821–4830</pages>
      <abstract>Short text classification has found rich and critical applications in news and tweet tagging to help users find relevant information. Due to lack of labeled training data in many practical use cases, there is a pressing need for studying semi-supervised short text classification. Most existing studies focus on <a href="https://en.wikipedia.org/wiki/Longitudinal_study">long texts</a> and achieve unsatisfactory performance on short texts due to the sparsity and limited labeled data. In this paper, we propose a novel heterogeneous graph neural network based method for semi-supervised short text classification, leveraging full advantage of few labeled data and large unlabeled data through information propagation along the <a href="https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)">graph</a>. In particular, we first present a flexible HIN (heterogeneous information network) framework for modeling the short texts, which can integrate any type of additional information as well as capture their relations to address the semantic sparsity. Then, we propose Heterogeneous Graph ATtention networks (HGAT) to embed the HIN for short text classification based on a dual-level attention mechanism, including node-level and type-level attentions. The attention mechanism can learn the importance of different neighboring nodes as well as the importance of different node (information) types to a current node. Extensive experimental results have demonstrated that our proposed model outperforms state-of-the-art methods across six benchmark datasets significantly.</abstract>
      <url hash="20094a63">D19-1488</url>
      <doi>10.18653/v1/D19-1488</doi>
      <bibkey>linmei-etal-2019-heterogeneous</bibkey>
    </paper>
    <paper id="489">
      <title>Comparing and Developing Tools to Measure the Readability of Domain-Specific Texts</title>
      <author><first>Elissa</first><last>Redmiles</last></author>
      <author><first>Lisa</first><last>Maszkiewicz</last></author>
      <author><first>Emily</first><last>Hwang</last></author>
      <author><first>Dhruv</first><last>Kuchhal</last></author>
      <author><first>Everest</first><last>Liu</last></author>
      <author><first>Miraida</first><last>Morales</last></author>
      <author><first>Denis</first><last>Peskov</last></author>
      <author><first>Sudha</first><last>Rao</last></author>
      <author><first>Rock</first><last>Stevens</last></author>
      <author><first>Kristina</first><last>Gligorić</last></author>
      <author><first>Sean</first><last>Kross</last></author>
      <author><first>Michelle</first><last>Mazurek</last></author>
      <author><first>Hal</first><last>Daumé III</last></author>
      <pages>4831–4842</pages>
      <abstract>The <a href="https://en.wikipedia.org/wiki/Readability">readability</a> of a digital text can influence people’s ability to learn new things about a range topics from <a href="https://en.wikipedia.org/wiki/Web_resource">digital resources</a> (e.g., <a href="https://en.wikipedia.org/wiki/Wikipedia">Wikipedia</a>, <a href="https://en.wikipedia.org/wiki/WebMD">WebMD</a>). Readability also impacts <a href="https://en.wikipedia.org/wiki/Search_engine_results_page">search rankings</a>, and is used to evaluate the performance of <a href="https://en.wikipedia.org/wiki/Natural_language_processing">NLP systems</a>. Despite this, we lack a thorough understanding of how to validly measure <a href="https://en.wikipedia.org/wiki/Readability">readability</a> at scale, especially for domain-specific texts. In this work, we present a comparison of the <a href="https://en.wikipedia.org/wiki/Validity_(statistics)">validity</a> of well-known readability measures and introduce a novel approach, Smart Cloze, which is designed to address shortcomings of existing measures. We compare these approaches across four different corpora : crowdworker-generated stories, Wikipedia articles, security and privacy advice, and health information. On these corpora, we evaluate the convergent and content validity of each measure, and detail tradeoffs in score precision, domain-specificity, and participant burden. These results provide a foundation for more accurate readability measurements and better evaluation of new <a href="https://en.wikipedia.org/wiki/Natural-language_processing">natural-language-processing systems</a> and tools.</abstract>
      <url hash="49d175fd">D19-1489</url>
      <doi>10.18653/v1/D19-1489</doi>
      <bibkey>redmiles-etal-2019-comparing</bibkey>
    </paper>
    <paper id="490">
      <title>News2vec : News Network Embedding with Subnode Information<fixed-case>N</fixed-case>ews2vec: News Network Embedding with Subnode Information</title>
      <author><first>Ye</first><last>Ma</last></author>
      <author><first>Lu</first><last>Zong</last></author>
      <author><first>Yikang</first><last>Yang</last></author>
      <author><first>Jionglong</first><last>Su</last></author>
      <pages>4843–4852</pages>
      <abstract>With the development of <a href="https://en.wikipedia.org/wiki/Natural_language_processing">NLP technologies</a>, <a href="https://en.wikipedia.org/wiki/News">news</a> can be automatically categorized and labeled according to a variety of characteristics, at the same time be represented as low dimensional embeddings. However, it lacks a systematic approach that effectively integrates the inherited features and inter-textual knowledge of news to represent the collective information with a dense vector. With the aim of filling this gap, the News2vec model is proposed to allow the distributed representation of news taking into account its associated features. To describe the cross-document linkages between news, a <a href="https://en.wikipedia.org/wiki/Social_network">network</a> consisting of <a href="https://en.wikipedia.org/wiki/News">news</a> and its attributes is constructed. Moreover, the News2vec model treats the news node as a bag of features by developing the Subnode model. Based on the <a href="https://en.wikipedia.org/wiki/Bias_of_an_estimator">biased random walk</a> and the skip-gram model, each <a href="https://en.wikipedia.org/wiki/News">news feature</a> is mapped to a vector, and the <a href="https://en.wikipedia.org/wiki/News">news</a> is thus represented as the sum of its features. This approach offers an easy solution to create embeddings for unseen news nodes based on its attributes. To evaluate our model, dimension reduction plots and correlation heat-maps are created to visualize the news vectors, together with the application of two downstream tasks, the stock movement prediction and news recommendation. By comparing with other established text / sentence embedding models, we show that News2vec achieves state-of-the-art performance on these news-related tasks.</abstract>
      <url hash="78eb0c75">D19-1490</url>
      <doi>10.18653/v1/D19-1490</doi>
      <bibkey>ma-etal-2019-news2vec</bibkey>
    </paper>
    <paper id="491">
      <title>Recursive Context-Aware Lexical Simplification</title>
      <author><first>Sian</first><last>Gooding</last></author>
      <author><first>Ekaterina</first><last>Kochmar</last></author>
      <pages>4853–4863</pages>
      <abstract>This paper presents a novel architecture for recursive context-aware lexical simplification, REC-LS, that is capable of (1) making use of the wider context when detecting the words in need of simplification and suggesting alternatives, and (2) taking previous simplification steps into account. We show that our system outputs lexical simplifications that are grammatically correct and semantically appropriate, and outperforms the current state-of-the-art systems in <a href="https://en.wikipedia.org/wiki/Lexical_simplification">lexical simplification</a>.</abstract>
      <url hash="8787c47a">D19-1491</url>
      <attachment hash="504e5e6d">D19-1491.Attachment.pdf</attachment>
      <doi>10.18653/v1/D19-1491</doi>
      <bibkey>gooding-kochmar-2019-recursive</bibkey>
    </paper>
    <paper id="493">
      <title>Neural News Recommendation with Heterogeneous User Behavior</title>
      <author><first>Chuhan</first><last>Wu</last></author>
      <author><first>Fangzhao</first><last>Wu</last></author>
      <author><first>Mingxiao</first><last>An</last></author>
      <author><first>Tao</first><last>Qi</last></author>
      <author><first>Jianqiang</first><last>Huang</last></author>
      <author><first>Yongfeng</first><last>Huang</last></author>
      <author><first>Xing</first><last>Xie</last></author>
      <pages>4874–4883</pages>
      <abstract>News recommendation is important for <a href="https://en.wikipedia.org/wiki/Online_newspaper">online news platforms</a> to help users find interested news and alleviate <a href="https://en.wikipedia.org/wiki/Information_overload">information overload</a>. Existing news recommendation methods usually rely on the news click history to model <a href="https://en.wikipedia.org/wiki/Interest_(emotion)">user interest</a>. However, these methods may suffer from the data sparsity problem, since the news click behaviors of many users in online news platforms are usually very limited. Fortunately, some other kinds of user behaviors such as <a href="https://en.wikipedia.org/wiki/Web_navigation">webpage browsing</a> and <a href="https://en.wikipedia.org/wiki/Web_search_query">search queries</a> can also provide useful clues of users’ news reading interest. In this paper, we propose a neural news recommendation approach which can exploit heterogeneous user behaviors. Our approach contains two major <a href="https://en.wikipedia.org/wiki/Modular_programming">modules</a>, i.e., <a href="https://en.wikipedia.org/wiki/News_media">news representation</a> and <a href="https://en.wikipedia.org/wiki/User_interface">user representation</a>. In the news representation module, we learn representations of news from their titles via CNN networks, and apply attention networks to select important words. In the user representation module, we propose an attentive multi-view learning framework to learn unified representations of users from their heterogeneous behaviors such as <a href="https://en.wikipedia.org/wiki/Web_search_query">search queries</a>, clicked news and browsed webpages. In addition, we use word- and record-level attentions to select informative words and behavior records. Experiments on a real-world dataset validate the effectiveness of our approach.</abstract>
      <url hash="2a02c187">D19-1493</url>
      <doi>10.18653/v1/D19-1493</doi>
      <bibkey>wu-etal-2019-neural</bibkey>
    </paper>
    <paper id="495">
      <title>Event Representation Learning Enhanced with External Commonsense Knowledge</title>
      <author><first>Xiao</first><last>Ding</last></author>
      <author><first>Kuo</first><last>Liao</last></author>
      <author><first>Ting</first><last>Liu</last></author>
      <author><first>Zhongyang</first><last>Li</last></author>
      <author><first>Junwen</first><last>Duan</last></author>
      <pages>4894–4903</pages>
      <abstract>Prior work has proposed effective methods to learn <a href="https://en.wikipedia.org/wiki/Event_(computing)">event representations</a> that can capture syntactic and semantic information over <a href="https://en.wikipedia.org/wiki/Text_corpus">text corpus</a>, demonstrating their effectiveness for downstream tasks such as script event prediction. On the other hand, events extracted from raw texts lacks of <a href="https://en.wikipedia.org/wiki/Commonsense_knowledge">commonsense knowledge</a>, such as the intents and emotions of the event participants, which are useful for distinguishing event pairs when there are only subtle differences in their surface realizations. To address this issue, this paper proposes to leverage external commonsense knowledge about the intent and sentiment of the event. Experiments on three event-related tasks, i.e., event similarity, script event prediction and <a href="https://en.wikipedia.org/wiki/Stock_market_prediction">stock market prediction</a>, show that our model obtains much better event embeddings for the tasks, achieving 78 % improvements on hard similarity task, yielding more precise inferences on subsequent events under given contexts, and better accuracies in predicting the <a href="https://en.wikipedia.org/wiki/Volatility_(finance)">volatilities</a> of the stock market.</abstract>
      <url hash="c2ce4d99">D19-1495</url>
      <doi>10.18653/v1/D19-1495</doi>
      <bibkey>ding-etal-2019-event-representation</bibkey>
      <pwccode url="https://github.com/MagiaSN/CommonsenseERL_EMNLP_2019" additional="false">MagiaSN/CommonsenseERL_EMNLP_2019</pwccode>
    </paper>
    <paper id="497">
      <title>A Neural Citation Count Prediction Model based on Peer Review Text</title>
      <author><first>Siqing</first><last>Li</last></author>
      <author><first>Wayne Xin</first><last>Zhao</last></author>
      <author><first>Eddy Jing</first><last>Yin</last></author>
      <author><first>Ji-Rong</first><last>Wen</last></author>
      <pages>4914–4924</pages>
      <abstract>Citation count prediction (CCP) has been an important research task for automatically estimating the future impact of a scholarly paper. Previous studies mainly focus on extracting or mining useful features from the paper itself or the associated authors. An important kind of <a href="https://en.wikipedia.org/wiki/Signal_(IPC)">data signals</a>, <a href="https://en.wikipedia.org/wiki/Peer_review">peer review text</a>, has not been utilized for the CCP task. In this paper, we take the initiative to utilize <a href="https://en.wikipedia.org/wiki/Peer_review">peer review data</a> for the CCP task with a neural prediction model. Our focus is to learn a comprehensive semantic representation for peer review text for improving the <a href="https://en.wikipedia.org/wiki/Prediction">prediction</a> performance. To achieve this goal, we incorporate the abstract-review match mechanism and the cross-review match mechanism to learn deep features from peer review text. We also consider integrating hand-crafted features via a <a href="https://en.wikipedia.org/wiki/Component-based_software_engineering">wide component</a>. The deep and wide components jointly make the <a href="https://en.wikipedia.org/wiki/Prediction">prediction</a>. Extensive experiments have demonstrated the usefulness of the peer review data and the effectiveness of the proposed <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a>. Our <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> has been released online.</abstract>
      <url hash="e5e6e9ba">D19-1497</url>
      <doi>10.18653/v1/D19-1497</doi>
      <bibkey>li-etal-2019-neural</bibkey>
    </paper>
    <paper id="499">
      <title>Semi-supervised Text Style Transfer : Cross Projection in Latent Space</title>
      <author><first>Mingyue</first><last>Shang</last></author>
      <author><first>Piji</first><last>Li</last></author>
      <author><first>Zhenxin</first><last>Fu</last></author>
      <author><first>Lidong</first><last>Bing</last></author>
      <author><first>Dongyan</first><last>Zhao</last></author>
      <author><first>Shuming</first><last>Shi</last></author>
      <author><first>Rui</first><last>Yan</last></author>
      <pages>4937–4946</pages>
      <abstract>Text style transfer task requires the model to transfer a sentence of one style to another style while retaining its original content meaning, which is a challenging problem that has long suffered from the shortage of parallel data. In this paper, we first propose a semi-supervised text style transfer model that combines the small-scale parallel data with the large-scale nonparallel data. With these two types of training data, we introduce a <a href="https://en.wikipedia.org/wiki/Projection_(linear_algebra)">projection function</a> between the latent space of different styles and design two <a href="https://en.wikipedia.org/wiki/Constraint_(mathematics)">constraints</a> to train it. We also introduce two other simple but effective <a href="https://en.wikipedia.org/wiki/Semi-supervised_learning">semi-supervised methods</a> to compare with. To evaluate the performance of the proposed methods, we build and release a novel style transfer dataset that alters sentences between the style of ancient Chinese poem and the modern Chinese.</abstract>
      <url hash="89258bf9">D19-1499</url>
      <doi>10.18653/v1/D19-1499</doi>
      <bibkey>shang-etal-2019-semi</bibkey>
    </paper>
    <paper id="500">
      <title>Question Answering for Privacy Policies : Combining Computational and Legal Perspectives</title>
      <author><first>Abhilasha</first><last>Ravichander</last></author>
      <author><first>Alan W</first><last>Black</last></author>
      <author><first>Shomir</first><last>Wilson</last></author>
      <author><first>Thomas</first><last>Norton</last></author>
      <author><first>Norman</first><last>Sadeh</last></author>
      <pages>4947–4958</pages>
      <abstract>Privacy policies are long and complex documents that are difficult for users to read and understand. Yet, they have legal effects on how <a href="https://en.wikipedia.org/wiki/User_data">user data</a> can be collected, managed and used. Ideally, we would like to empower users to inform themselves about the issues that matter to them, and enable them to selectively explore these issues. We present PrivacyQA, a <a href="https://en.wikipedia.org/wiki/Text_corpus">corpus</a> consisting of 1750 questions about the <a href="https://en.wikipedia.org/wiki/Privacy_policy">privacy policies</a> of <a href="https://en.wikipedia.org/wiki/Mobile_app">mobile applications</a>, and over 3500 expert annotations of relevant answers. We observe that a strong neural baseline underperforms <a href="https://en.wikipedia.org/wiki/Human_performance">human performance</a> by almost 0.3 <a href="https://en.wikipedia.org/wiki/F-number">F1</a> on PrivacyQA, suggesting considerable room for improvement for future systems. Further, we use this <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> to categorically identify challenges to <a href="https://en.wikipedia.org/wiki/Question_answering">question answerability</a>, with domain-general implications for any <a href="https://en.wikipedia.org/wiki/Question_answering">question answering system</a>. The PrivacyQA corpus offers a challenging corpus for <a href="https://en.wikipedia.org/wiki/Question_answering">question answering</a>, with genuine real world utility.</abstract>
      <url hash="98b0787d">D19-1500</url>
      <doi>10.18653/v1/D19-1500</doi>
      <bibkey>ravichander-etal-2019-question</bibkey>
      <pwccode url="https://github.com/AbhilashaRavichander/PrivacyQA_EMNLP" additional="false">AbhilashaRavichander/PrivacyQA_EMNLP</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
    </paper>
    <paper id="502">
      <title>Fine-Grained Entity Typing via Hierarchical Multi Graph Convolutional Networks</title>
      <author><first>Hailong</first><last>Jin</last></author>
      <author><first>Lei</first><last>Hou</last></author>
      <author><first>Juanzi</first><last>Li</last></author>
      <author><first>Tiansi</first><last>Dong</last></author>
      <pages>4969–4978</pages>
      <abstract>This paper addresses the problem of inferring the fine-grained type of an entity from a <a href="https://en.wikipedia.org/wiki/Knowledge_base">knowledge base</a>. We convert this problem into the task of graph-based semi-supervised classification, and propose Hierarchical Multi Graph Convolutional Network (HMGCN), a novel Deep Learning architecture to tackle this problem. We construct three kinds of <a href="https://en.wikipedia.org/wiki/Connectivity_matrix">connectivity matrices</a> to capture different kinds of semantic correlations between entities. A recursive regularization is proposed to model the subClassOf relations between types in given <a href="https://en.wikipedia.org/wiki/Type_hierarchy">type hierarchy</a>. Extensive experiments with two large-scale public datasets show that our proposed method significantly outperforms four state-of-the-art methods.</abstract>
      <url hash="d43f5f47">D19-1502</url>
      <doi>10.18653/v1/D19-1502</doi>
      <bibkey>jin-etal-2019-fine</bibkey>
    </paper>
    <paper id="504">
      <title>Practical Correlated Topic Modeling and Analysis via the Rectified Anchor Word Algorithm</title>
      <author><first>Moontae</first><last>Lee</last></author>
      <author><first>Sungjun</first><last>Cho</last></author>
      <author><first>David</first><last>Bindel</last></author>
      <author><first>David</first><last>Mimno</last></author>
      <pages>4991–5001</pages>
      <abstract>Despite great scalability on large data and their ability to understand correlations between topics, spectral topic models have not been widely used due to the absence of reliability in real data and lack of practical implementations. This paper aims to solidify the foundations of spectral topic inference and provide a practical implementation for anchor-based topic modeling. Beginning with vocabulary curation, we scrutinize every single inference step with other viable options. We also evaluate our matrix-based approach against popular alternatives including a tensor-based spectral method as well as probabilistic algorithms. Our quantitative and qualitative experiments demonstrate the power of Rectified Anchor Word algorithm in various real datasets, providing a complete guide to practical correlated topic modeling.</abstract>
      <url hash="ada5b9a3">D19-1504</url>
      <attachment hash="6094c10f">D19-1504.Attachment.zip</attachment>
      <doi>10.18653/v1/D19-1504</doi>
      <bibkey>lee-etal-2019-practical</bibkey>
    </paper>
    <paper id="507">
      <title>Subword Language Model for Query Auto-Completion</title>
      <author><first>Gyuwan</first><last>Kim</last></author>
      <pages>5022–5032</pages>
      <abstract>Current neural query auto-completion (QAC) systems rely on character-level language models, but they slow down when queries are long. We present how to utilize subword language models for the fast and accurate generation of query completion candidates. Representing queries with <a href="https://en.wikipedia.org/wiki/Subword">subwords</a> shorten a decoding length significantly. To deal with issues coming from introducing subword language model, we develop a retrace algorithm and a reranking method by approximate marginalization. As a result, our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> achieves up to 2.5 times faster while maintaining a similar quality of generated results compared to the character-level baseline. Also, we propose a new evaluation metric, mean recoverable length (MRL), measuring how many upcoming characters the <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> could complete correctly. It provides more explicit meaning and eliminates the need for prefix length sampling for existing rank-based metrics. Moreover, we performed a comprehensive analysis with ablation study to figure out the importance of each component.</abstract>
      <url hash="996f828b">D19-1507</url>
      <doi>10.18653/v1/D19-1507</doi>
      <bibkey>kim-2019-subword</bibkey>
      <pwccode url="https://github.com/clovaai/subword-qac" additional="false">clovaai/subword-qac</pwccode>
    </paper>
    <paper id="508">
      <title>Enhancing Dialogue Symptom Diagnosis with Global Attention and Symptom Graph</title>
      <author><first>Xinzhu</first><last>Lin</last></author>
      <author><first>Xiahui</first><last>He</last></author>
      <author><first>Qin</first><last>Chen</last></author>
      <author><first>Huaixiao</first><last>Tou</last></author>
      <author><first>Zhongyu</first><last>Wei</last></author>
      <author><first>Ting</first><last>Chen</last></author>
      <pages>5033–5042</pages>
      <abstract>Symptom diagnosis is a challenging yet profound problem in <a href="https://en.wikipedia.org/wiki/Natural_language_processing">natural language processing</a>. Most previous research focus on investigating the standard <a href="https://en.wikipedia.org/wiki/Electronic_health_record">electronic medical records</a> for symptom diagnosis, while the dialogues between doctors and patients that contain more rich information are not well studied. In this paper, we first construct a dialogue symptom diagnosis dataset based on an online medical forum with a large amount of dialogues between patients and doctors. Then, we provide some <a href="https://en.wikipedia.org/wiki/Benchmarking">benchmark models</a> on this <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> to boost the research of dialogue symptom diagnosis. In order to further enhance the performance of symptom diagnosis over dialogues, we propose a global attention mechanism to capture more symptom related information, and build a symptom graph to model the associations between symptoms rather than treating each symptom independently. Experimental results show that both the global attention and symptom graph are effective to boost dialogue symptom diagnosis. In particular, our proposed <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> achieves the state-of-the-art performance on the constructed <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a>.</abstract>
      <url hash="3db8a9eb">D19-1508</url>
      <doi>10.18653/v1/D19-1508</doi>
      <bibkey>lin-etal-2019-enhancing</bibkey>
    </paper>
    <paper id="510">
      <title>Encode, Tag, Realize : High-Precision Text Editing</title>
      <author><first>Eric</first><last>Malmi</last></author>
      <author><first>Sebastian</first><last>Krause</last></author>
      <author><first>Sascha</first><last>Rothe</last></author>
      <author><first>Daniil</first><last>Mirylenka</last></author>
      <author><first>Aliaksei</first><last>Severyn</last></author>
      <pages>5054–5065</pages>
      <abstract>We propose LaserTagger-a sequence tagging approach that casts text generation as a text editing task. Target texts are reconstructed from the inputs using three main edit operations : keeping a token, deleting it, and adding a phrase before the token. To predict the edit operations, we propose a novel model, which combines a BERT encoder with an autoregressive Transformer decoder. This approach is evaluated on English text on four tasks : sentence fusion, sentence splitting, abstractive summarization, and <a href="https://en.wikipedia.org/wiki/Grammar">grammar correction</a>. LaserTagger achieves new state-of-the-art results on three of these tasks, performs comparably to a set of strong seq2seq baselines with a large number of training examples, and outperforms them when the number of examples is limited. Furthermore, we show that at inference time tagging can be more than two orders of magnitude faster than comparable seq2seq models, making it more attractive for running in a live environment.</abstract>
      <url hash="923babda">D19-1510</url>
      <attachment hash="82e99688">D19-1510.Attachment.zip</attachment>
      <doi>10.18653/v1/D19-1510</doi>
      <bibkey>malmi-etal-2019-encode</bibkey>
      <pwccode url="https://github.com/google-research/lasertagger" additional="true">google-research/lasertagger</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/discofuse">DiscoFuse</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikisplit">WikiSplit</pwcdataset>
    </paper>
    <paper id="511">
      <title>Answer-guided and Semantic Coherent Question Generation in Open-domain Conversation</title>
      <author><first>Weichao</first><last>Wang</last></author>
      <author><first>Shi</first><last>Feng</last></author>
      <author><first>Daling</first><last>Wang</last></author>
      <author><first>Yifei</first><last>Zhang</last></author>
      <pages>5066–5076</pages>
      <abstract>Generating intriguing question is a key step towards building human-like open-domain chatbots. Although some recent works have focused on this task, compared with questions raised by humans, significant gaps remain in maintaining semantic coherence with post, which may result in generating dull or deviated questions. We observe that the answer has strong semantic coherence to its question and post, which can be used to guide question generation. Thus, we devise two methods to further enhance semantic coherence between post and question under the guidance of answer. First, the coherence score between generated question and answer is used as the <a href="https://en.wikipedia.org/wiki/Reinforcement_learning">reward function</a> in a reinforcement learning framework, to encourage the cases that are consistent with the answer in semantic. Second, we incorporate <a href="https://en.wikipedia.org/wiki/Adversarial_system">adversarial training</a> to explicitly control question generation in the direction of question-answer coherence. Extensive experiments show that our two methods outperform state-of-the-art baseline algorithms with large margins in raising semantic coherent questions.</abstract>
      <url hash="d17931e9">D19-1511</url>
      <doi>10.18653/v1/D19-1511</doi>
      <bibkey>wang-etal-2019-answer</bibkey>
    </paper>
    <paper id="513">
      <title>A Topic Augmented Text Generation Model : Joint Learning of Semantics and Structural Features</title>
      <author><first>Hongyin</first><last>Tang</last></author>
      <author><first>Miao</first><last>Li</last></author>
      <author><first>Beihong</first><last>Jin</last></author>
      <pages>5090–5099</pages>
      <abstract>Text generation is among the most fundamental tasks in <a href="https://en.wikipedia.org/wiki/Natural_language_processing">natural language processing</a>. In this paper, we propose a text generation model that learns <a href="https://en.wikipedia.org/wiki/Semantics">semantics</a> and structural features simultaneously. This model captures structural features by a sequential variational autoencoder component and leverages a topic modeling component based on <a href="https://en.wikipedia.org/wiki/Normal_distribution">Gaussian distribution</a> to enhance the recognition of text semantics. To make the reconstructed text more coherent to the topics, the <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a> further adapts the <a href="https://en.wikipedia.org/wiki/Encoder">encoder</a> of the topic modeling component for a <a href="https://en.wikipedia.org/wiki/Discriminator">discriminator</a>. The results of experiments over several datasets demonstrate that our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> outperforms several states of the art models in terms of <a href="https://en.wikipedia.org/wiki/Perplexity">text perplexity</a> and <a href="https://en.wikipedia.org/wiki/Coherence_(linguistics)">topic coherence</a>. Moreover, the latent representations learned by our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> is superior to others in a text classification task. Finally, given the input texts, our <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a> can generate meaningful texts which hold similar structures but under different topics.</abstract>
      <url hash="a3a11dea">D19-1513</url>
      <attachment hash="e24af89a">D19-1513.Attachment.zip</attachment>
      <doi>10.18653/v1/D19-1513</doi>
      <bibkey>tang-etal-2019-topic</bibkey>
    </paper>
    <paper id="515">
      <title>Phrase Grounding by Soft-Label Chain Conditional Random Field</title>
      <author><first>Jiacheng</first><last>Liu</last></author>
      <author><first>Julia</first><last>Hockenmaier</last></author>
      <pages>5112–5122</pages>
      <abstract>The phrase grounding task aims to ground each entity mention in a given caption of an <a href="https://en.wikipedia.org/wiki/Image">image</a> to a corresponding region in that image. Although there are clear dependencies between how different mentions of the same caption should be grounded, previous structured prediction methods that aim to capture such <a href="https://en.wikipedia.org/wiki/Dependent_and_independent_variables">dependencies</a> need to resort to <a href="https://en.wikipedia.org/wiki/Approximate_inference">approximate inference</a> or non-differentiable losses. In this paper, we formulate phrase grounding as a sequence labeling task where we treat candidate regions as potential labels, and use neural chain Conditional Random Fields (CRFs) to model dependencies among regions for adjacent mentions. In contrast to standard sequence labeling tasks, the phrase grounding task is defined such that there may be multiple correct candidate regions. To address this multiplicity of gold labels, we define so-called Soft-Label Chain CRFs, and present an <a href="https://en.wikipedia.org/wiki/Algorithm">algorithm</a> that enables convenient end-to-end training. Our method establishes a new state-of-the-art on phrase grounding on the Flickr30k Entities dataset. Analysis shows that our model benefits both from the <a href="https://en.wikipedia.org/wiki/Entity–relationship_model">entity dependencies</a> captured by the CRF and from the soft-label training regime. Our code is available at<url>github.com/liujch1998/SoftLabelCCRF</url>
      </abstract>
      <url hash="709301cd">D19-1515</url>
      <attachment hash="c8e9ff87">D19-1515.Attachment.zip</attachment>
      <doi>10.18653/v1/D19-1515</doi>
      <bibkey>liu-hockenmaier-2019-phrase</bibkey>
      <pwccode url="https://github.com/liujch1998/SoftLabelCCRF" additional="false">liujch1998/SoftLabelCCRF</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/refcoco">RefCOCO</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/visual-genome">Visual Genome</pwcdataset>
    </paper>
    <paper id="518">
      <title>DEBUG : A Dense Bottom-Up Grounding Approach for Natural Language Video Localization<fixed-case>DEBUG</fixed-case>: A Dense Bottom-Up Grounding Approach for Natural Language Video Localization</title>
      <author><first>Chujie</first><last>Lu</last></author>
      <author><first>Long</first><last>Chen</last></author>
      <author><first>Chilie</first><last>Tan</last></author>
      <author><first>Xiaolin</first><last>Li</last></author>
      <author><first>Jun</first><last>Xiao</last></author>
      <pages>5144–5153</pages>
      <abstract>In this paper, we focus on natural language video localization : localizing (ie, grounding) a natural language description in a long and untrimmed video sequence. All currently published models for addressing this problem can be categorized into two types : (i) <a href="https://en.wikipedia.org/wiki/Top-down_and_bottom-up_design">top-down approach</a> : it does classification and <a href="https://en.wikipedia.org/wiki/Regression_analysis">regression</a> for a set of pre-cut video segment candidates ; (ii) <a href="https://en.wikipedia.org/wiki/Top-down_and_bottom-up_design">bottom-up approach</a> : it directly predicts probabilities for each video frame as the temporal boundaries (ie, start and end time point). However, both two approaches suffer several limitations : the former is computation-intensive for densely placed candidates, while the latter has trailed the performance of the top-down counterpart thus far. To this end, we propose a novel dense bottom-up framework : DEnse Bottom-Up Grounding (DEBUG). DEBUG regards all frames falling in the ground truth segment as foreground, and each foreground frame regresses the unique distances from its location to bi-directional ground truth boundaries. Extensive experiments on three challenging benchmarks (TACoS, Charades-STA, and ActivityNet Captions) show that DEBUG is able to match the speed of bottom-up models while surpassing the performance of the state-of-the-art top-down models.</abstract>
      <url hash="3bfa9a14">D19-1518</url>
      <doi>10.18653/v1/D19-1518</doi>
      <bibkey>lu-etal-2019-debug</bibkey>
    </paper>
    <paper id="521">
      <title>Open Domain Web Keyphrase Extraction Beyond Language Modeling</title>
      <author><first>Lee</first><last>Xiong</last></author>
      <author><first>Chuan</first><last>Hu</last></author>
      <author><first>Chenyan</first><last>Xiong</last></author>
      <author><first>Daniel</first><last>Campos</last></author>
      <author><first>Arnold</first><last>Overwijk</last></author>
      <pages>5175–5184</pages>
      <abstract>This paper studies <a href="https://en.wikipedia.org/wiki/Keyphrase_extraction">keyphrase extraction</a> in real-world scenarios where documents are from diverse domains and have variant content quality. We curate and release OpenKP, a large scale open domain keyphrase extraction dataset with near one hundred thousand web documents and expert keyphrase annotations. To handle the variations of domain and content quality, we develop BLING-KPE, a neural keyphrase extraction model that goes beyond <a href="https://en.wikipedia.org/wiki/Natural-language_understanding">language understanding</a> using visual presentations of documents and weak supervision from <a href="https://en.wikipedia.org/wiki/Web_search_query">search queries</a>. Experimental results on OpenKP confirm the effectiveness of BLING-KPE and the contributions of its neural architecture, visual features, and search log weak supervision. Zero-shot evaluations on DUC-2001 demonstrate the improved generalization ability of <a href="https://en.wikipedia.org/wiki/Machine_learning">learning</a> from the open domain data compared to a specific domain.</abstract>
      <url hash="ce1604c7">D19-1521</url>
      <attachment hash="996e532b">D19-1521.Attachment.zip</attachment>
      <doi>10.18653/v1/D19-1521</doi>
      <bibkey>xiong-etal-2019-open</bibkey>
      <pwccode url="" additional="true" />
      <pwcdataset url="https://paperswithcode.com/dataset/kp20k">KP20k</pwcdataset>
    </paper>
    <paper id="525">
      <title>Adversarial Reprogramming of Text Classification Neural Networks</title>
      <author><first>Paarth</first><last>Neekhara</last></author>
      <author><first>Shehzeen</first><last>Hussain</last></author>
      <author><first>Shlomo</first><last>Dubnov</last></author>
      <author><first>Farinaz</first><last>Koushanfar</last></author>
      <pages>5216–5225</pages>
      <abstract>In this work, we develop methods to repurpose text classification neural networks for alternate tasks without modifying the <a href="https://en.wikipedia.org/wiki/Network_architecture">network architecture</a> or parameters. We propose a context based vocabulary remapping method that performs a computationally inexpensive input transformation to reprogram a victim classification model for a new set of sequences. We propose algorithms for training such an input transformation in both white box and black box settings where the adversary may or may not have access to the victim model’s architecture and parameters. We demonstrate the application of our model and the vulnerability of neural networks by adversarially repurposing various text-classification models including LSTM, bi-directional LSTM and CNN for alternate classification tasks.</abstract>
      <url hash="77170004">D19-1525</url>
      <attachment hash="952fdd14">D19-1525.Attachment.pdf</attachment>
      <doi>10.18653/v1/D19-1525</doi>
      <bibkey>neekhara-etal-2019-adversarial</bibkey>
      <pwccode url="https://github.com/paarthneekhara/rnn_adversarial_reprogramming" additional="false">paarthneekhara/rnn_adversarial_reprogramming</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/imagenet">ImageNet</pwcdataset>
    </paper>
    <paper id="527">
      <title>On Efficient Retrieval of Top Similarity Vectors</title>
      <author><first>Shulong</first><last>Tan</last></author>
      <author><first>Zhixin</first><last>Zhou</last></author>
      <author><first>Zhaozhuo</first><last>Xu</last></author>
      <author><first>Ping</first><last>Li</last></author>
      <pages>5236–5246</pages>
      <abstract>Retrieval of relevant vectors produced by <a href="https://en.wikipedia.org/wiki/Representation_learning">representation learning</a> critically influences the efficiency in natural language processing (NLP) tasks. In this paper, we demonstrate an efficient method for searching vectors via a typical non-metric matching function : <a href="https://en.wikipedia.org/wiki/Inner_product_space">inner product</a>. Our method, which constructs an approximate Inner Product Delaunay Graph (IPDG) for top-1 Maximum Inner Product Search (MIPS), transforms retrieving the most suitable latent vectors into a graph search problem with great benefits of efficiency. Experiments on data representations learned for different machine learning tasks verify the outperforming effectiveness and efficiency of the proposed IPDG.</abstract>
      <url hash="faa08f97">D19-1527</url>
      <doi>10.18653/v1/D19-1527</doi>
      <bibkey>tan-etal-2019-efficient</bibkey>
    </paper>
    <paper id="531">
      <title>Examining Gender Bias in <a href="https://en.wikipedia.org/wiki/Language">Languages</a> with Grammatical Gender</title>
      <author><first>Pei</first><last>Zhou</last></author>
      <author><first>Weijia</first><last>Shi</last></author>
      <author><first>Jieyu</first><last>Zhao</last></author>
      <author><first>Kuan-Hao</first><last>Huang</last></author>
      <author><first>Muhao</first><last>Chen</last></author>
      <author><first>Ryan</first><last>Cotterell</last></author>
      <author><first>Kai-Wei</first><last>Chang</last></author>
      <pages>5276–5284</pages>
      <abstract>Recent studies have shown that <a href="https://en.wikipedia.org/wiki/Word_embedding">word embeddings</a> exhibit <a href="https://en.wikipedia.org/wiki/Gender_bias">gender bias</a> inherited from the training corpora. However, most studies to date have focused on quantifying and mitigating such <a href="https://en.wikipedia.org/wiki/Bias">bias</a> only in <a href="https://en.wikipedia.org/wiki/English_language">English</a>. These analyses can not be directly extended to <a href="https://en.wikipedia.org/wiki/Language">languages</a> that exhibit <a href="https://en.wikipedia.org/wiki/Agreement_(linguistics)">morphological agreement</a> on gender, such as <a href="https://en.wikipedia.org/wiki/Spanish_language">Spanish</a> and <a href="https://en.wikipedia.org/wiki/French_language">French</a>. In this paper, we propose new metrics for evaluating <a href="https://en.wikipedia.org/wiki/Gender_bias">gender bias</a> in <a href="https://en.wikipedia.org/wiki/Word_embedding">word embeddings</a> of these languages and further demonstrate evidence of <a href="https://en.wikipedia.org/wiki/Gender_bias">gender bias</a> in bilingual embeddings which align these languages with <a href="https://en.wikipedia.org/wiki/English_language">English</a>. Finally, we extend an existing approach to mitigate gender bias in <a href="https://en.wikipedia.org/wiki/Word_embedding">word embedding</a> of these <a href="https://en.wikipedia.org/wiki/Language">languages</a> under both monolingual and bilingual settings. Experiments on modified Word Embedding Association Test, word similarity, word translation, and word pair translation tasks show that the proposed approaches can effectively reduce the gender bias while preserving the utility of the original embeddings.</abstract>
      <url hash="1a3ff84c">D19-1531</url>
      <doi>10.18653/v1/D19-1531</doi>
      <bibkey>zhou-etal-2019-examining</bibkey>
      <pwccode url="https://github.com/shaoxia57/Bias_in_Gendered_Languages" additional="false">shaoxia57/Bias_in_Gendered_Languages</pwccode>
    </paper>
    <paper id="532">
      <title>Weakly Supervised Cross-lingual Semantic Relation Classification via Knowledge Distillation</title>
      <author><first>Yogarshi</first><last>Vyas</last></author>
      <author><first>Marine</first><last>Carpuat</last></author>
      <pages>5285–5296</pages>
      <abstract>Words in different languages rarely cover the exact same <a href="https://en.wikipedia.org/wiki/Semantic_space">semantic space</a>. This work characterizes differences in meaning between words across languages using semantic relations that have been used to relate the meaning of English words. However, because of translation ambiguity, semantic relations are not always preserved by translation. We introduce a cross-lingual relation classifier trained only with English examples and a <a href="https://en.wikipedia.org/wiki/Bilingual_dictionary">bilingual dictionary</a>. Our <a href="https://en.wikipedia.org/wiki/Classifier_(linguistics)">classifier</a> relies on a novel attention-based distillation approach to account for translation ambiguity when transferring knowledge from <a href="https://en.wikipedia.org/wiki/English_language">English</a> to cross-lingual settings. On new English-Chinese and English-Hindi test sets, the resulting models largely outperform baselines that more naively rely on bilingual embeddings or dictionaries for cross-lingual transfer, and approach the performance of fully supervised systems on English tasks.</abstract>
      <url hash="8a1a4f89">D19-1532</url>
      <attachment hash="4a39f306">D19-1532.Attachment.pdf</attachment>
      <doi>10.18653/v1/D19-1532</doi>
      <bibkey>vyas-carpuat-2019-weakly</bibkey>
    </paper>
    <paper id="534">
      <title>Do NLP Models Know Numbers? Probing Numeracy in Embeddings<fixed-case>NLP</fixed-case> Models Know Numbers? Probing Numeracy in Embeddings</title>
      <author><first>Eric</first><last>Wallace</last></author>
      <author><first>Yizhong</first><last>Wang</last></author>
      <author><first>Sujian</first><last>Li</last></author>
      <author><first>Sameer</first><last>Singh</last></author>
      <author><first>Matt</first><last>Gardner</last></author>
      <pages>5307–5315</pages>
      <abstract>The ability to understand and work with numbers (numeracy) is critical for many complex reasoning tasks. Currently, most NLP models treat numbers in text in the same way as other tokensthey embed them as distributed vectors. Is this enough to capture <a href="https://en.wikipedia.org/wiki/Numeracy">numeracy</a>? We begin by investigating the numerical reasoning capabilities of a state-of-the-art <a href="https://en.wikipedia.org/wiki/Question_answering">question answering model</a> on the DROP dataset. We find this <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> excels on questions that require <a href="https://en.wikipedia.org/wiki/Numerical_analysis">numerical reasoning</a>, i.e., <a href="https://en.wikipedia.org/wiki/Information_technology">it</a> already captures <a href="https://en.wikipedia.org/wiki/Numeracy">numeracy</a>. To understand how this capability emerges, we probe token embedding methods (e.g., BERT, GloVe) on synthetic list maximum, number decoding, and addition tasks. A surprising degree of <a href="https://en.wikipedia.org/wiki/Numeracy">numeracy</a> is naturally present in standard <a href="https://en.wikipedia.org/wiki/Embedding">embeddings</a>. For example, GloVe and <a href="https://en.wikipedia.org/wiki/Word2vec">word2vec</a> accurately encode <a href="https://en.wikipedia.org/wiki/Magnitude_(mathematics)">magnitude</a> for numbers up to 1,000. Furthermore, character-level embeddings are even more preciseELMo captures <a href="https://en.wikipedia.org/wiki/Numeracy">numeracy</a> the best for all pre-trained methodsbut BERT, which uses sub-word units, is less exact.</abstract>
      <url hash="5d141a03">D19-1534</url>
      <attachment hash="53f44f46">D19-1534.Attachment.zip</attachment>
      <doi>10.18653/v1/D19-1534</doi>
      <bibkey>wallace-etal-2019-nlp</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/drop">DROP</pwcdataset>
    </paper>
    <paper id="535">
      <title>A Split-and-Recombine Approach for Follow-up Query Analysis</title>
      <author><first>Qian</first><last>Liu</last></author>
      <author><first>Bei</first><last>Chen</last></author>
      <author><first>Haoyan</first><last>Liu</last></author>
      <author><first>Jian-Guang</first><last>Lou</last></author>
      <author><first>Lei</first><last>Fang</last></author>
      <author><first>Bin</first><last>Zhou</last></author>
      <author><first>Dongmei</first><last>Zhang</last></author>
      <pages>5316–5326</pages>
      <abstract>Context-dependent semantic parsing has proven to be an important yet challenging <a href="https://en.wikipedia.org/wiki/Task_(project_management)">task</a>. To leverage the advances in context-independent semantic parsing, we propose to perform follow-up query analysis, aiming to restate context-dependent natural language queries with <a href="https://en.wikipedia.org/wiki/Context_(language_use)">contextual information</a>. To accomplish the task, we propose STAR, a novel approach with a well-designed two-phase process. It is parser-independent and able to handle multifarious follow-up scenarios in different domains. Experiments on the FollowUp dataset show that STAR outperforms the state-of-the-art baseline by a large margin of nearly 8 %. The superiority on <a href="https://en.wikipedia.org/wiki/Parsing">parsing</a> results verifies the feasibility of follow-up query analysis. We also explore the extensibility of STAR on the SQA dataset, which is very promising.</abstract>
      <url hash="6fb3a4c9">D19-1535</url>
      <doi>10.18653/v1/D19-1535</doi>
      <bibkey>liu-etal-2019-split</bibkey>
      <pwccode url="https://github.com/microsoft/EMNLP2019-Split-And-Recombine" additional="false">microsoft/EMNLP2019-Split-And-Recombine</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/followup">FollowUp</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sqa">SQA</pwcdataset>
    </paper>
    <paper id="537">
      <title>Editing-Based SQL Query Generation for Cross-Domain Context-Dependent Questions<fixed-case>SQL</fixed-case> Query Generation for Cross-Domain Context-Dependent Questions</title>
      <author><first>Rui</first><last>Zhang</last></author>
      <author><first>Tao</first><last>Yu</last></author>
      <author><first>Heyang</first><last>Er</last></author>
      <author><first>Sungrok</first><last>Shim</last></author>
      <author><first>Eric</first><last>Xue</last></author>
      <author><first>Xi Victoria</first><last>Lin</last></author>
      <author><first>Tianze</first><last>Shi</last></author>
      <author><first>Caiming</first><last>Xiong</last></author>
      <author><first>Richard</first><last>Socher</last></author>
      <author><first>Dragomir</first><last>Radev</last></author>
      <pages>5338–5349</pages>
      <abstract>We focus on the cross-domain context-dependent text-to-SQL generation task. Based on the observation that adjacent natural language questions are often linguistically dependent and their corresponding SQL queries tend to overlap, we utilize the interaction history by editing the previous predicted query to improve the generation quality. Our editing mechanism views SQL as sequences and reuses generation results at the token level in a simple manner. It is flexible to change individual tokens and robust to <a href="https://en.wikipedia.org/wiki/Propagation_of_uncertainty">error propagation</a>. Furthermore, to deal with complex table structures in different domains, we employ an utterance-table encoder and a table-aware decoder to incorporate the context of the user utterance and the table schema. We evaluate our approach on the SParC dataset and demonstrate the benefit of editing compared with the state-of-the-art baselines which generate SQL from scratch. Our code is available at.<url>https://github.com/ryanzhumich/sparc_atis_pytorch</url>.</abstract>
      <url hash="ac1e8afc">D19-1537</url>
      <doi>10.18653/v1/D19-1537</doi>
      <bibkey>zhang-etal-2019-editing</bibkey>
      <pwccode url="https://github.com/ryanzhumich/editsql" additional="true">ryanzhumich/editsql</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/atis">ATIS</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sparc">SParC</pwcdataset>
    </paper>
    <paper id="539">
      <title>Cloze-driven Pretraining of Self-attention Networks</title>
      <author><first>Alexei</first><last>Baevski</last></author>
      <author><first>Sergey</first><last>Edunov</last></author>
      <author><first>Yinhan</first><last>Liu</last></author>
      <author><first>Luke</first><last>Zettlemoyer</last></author>
      <author><first>Michael</first><last>Auli</last></author>
      <pages>5360–5369</pages>
      <abstract>We present a new approach for pretraining a bi-directional transformer model that provides significant performance gains across a variety of language understanding problems. Our model solves a cloze-style word reconstruction task, where each word is ablated and must be predicted given the rest of the text. Experiments demonstrate large performance gains on GLUE and new state of the art results on NER as well as constituency parsing benchmarks, consistent with BERT. We also present a detailed analysis of a number of factors that contribute to effective pretraining, including data domain and size, model capacity, and variations on the cloze objective.</abstract>
      <url hash="c855992a">D19-1539</url>
      <doi>10.18653/v1/D19-1539</doi>
      <bibkey>baevski-etal-2019-cloze</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/conll-2003">CoNLL-2003</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mrpc">MRPC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/penn-treebank">Penn Treebank</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/qnli">QNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="540">
      <title>Bridging the Gap between Relevance Matching and <a href="https://en.wikipedia.org/wiki/Semantic_matching">Semantic Matching</a> for Short Text Similarity Modeling</title>
      <author><first>Jinfeng</first><last>Rao</last></author>
      <author><first>Linqing</first><last>Liu</last></author>
      <author><first>Yi</first><last>Tay</last></author>
      <author><first>Wei</first><last>Yang</last></author>
      <author><first>Peng</first><last>Shi</last></author>
      <author><first>Jimmy</first><last>Lin</last></author>
      <pages>5370–5381</pages>
      <abstract>A core problem of information retrieval (IR) is relevance matching, which is to rank documents by relevance to a user’s query. On the other hand, many NLP problems, such as <a href="https://en.wikipedia.org/wiki/Question_answering">question answering</a> and paraphrase identification, can be considered variants of <a href="https://en.wikipedia.org/wiki/Semantic_matching">semantic matching</a>, which is to measure the <a href="https://en.wikipedia.org/wiki/Semantic_distance">semantic distance</a> between two pieces of short texts. While at a high level both <a href="https://en.wikipedia.org/wiki/Relevance_(information_retrieval)">relevance</a> and <a href="https://en.wikipedia.org/wiki/Semantic_matching">semantic matching</a> require modeling textual similarity, many existing techniques for one can not be easily adapted to the other. To bridge this gap, we propose a novel model, HCAN (Hybrid Co-Attention Network), that comprises (1) a hybrid encoder module that includes ConvNet-based and LSTM-based encoders, (2) a relevance matching module that measures soft term matches with importance weighting at multiple granularities, and (3) a semantic matching module with co-attention mechanisms that capture context-aware semantic relatedness. Evaluations on multiple IR and NLP benchmarks demonstrate state-of-the-art effectiveness compared to approaches that do not exploit pretraining on external data. Extensive ablation studies suggest that relevance and semantic matching signals are complementary across many problem settings, regardless of the choice of underlying <a href="https://en.wikipedia.org/wiki/Encoder">encoders</a>.</abstract>
      <url hash="cd7a5e23">D19-1540</url>
      <doi>10.18653/v1/D19-1540</doi>
      <bibkey>rao-etal-2019-bridging</bibkey>
    </paper>
    <paper id="542">
      <title>Transfer Fine-Tuning : A BERT Case Study<fixed-case>BERT</fixed-case> Case Study</title>
      <author><first>Yuki</first><last>Arase</last></author>
      <author><first>Jun’ichi</first><last>Tsujii</last></author>
      <pages>5393–5404</pages>
      <abstract>A semantic equivalence assessment is defined as a task that assesses <a href="https://en.wikipedia.org/wiki/Semantic_similarity">semantic equivalence</a> in a sentence pair by binary judgment (i.e., paraphrase identification) or grading (i.e., semantic textual similarity measurement). It constitutes a set of <a href="https://en.wikipedia.org/wiki/Task_(project_management)">tasks</a> crucial for research on <a href="https://en.wikipedia.org/wiki/Natural-language_understanding">natural language understanding</a>. Recently, BERT realized a breakthrough in sentence representation learning (Devlin et al., 2019), which is broadly transferable to various NLP tasks. While <a href="https://en.wikipedia.org/wiki/BERT">BERT</a>’s performance improves by increasing its model size, the required <a href="https://en.wikipedia.org/wiki/Computational_power">computational power</a> is an obstacle preventing practical applications from adopting the <a href="https://en.wikipedia.org/wiki/Technology">technology</a>. Herein, we propose to inject phrasal paraphrase relations into BERT in order to generate suitable representations for semantic equivalence assessment instead of increasing the model size. Experiments on standard natural language understanding tasks confirm that our method effectively improves a smaller BERT model while maintaining the model size. The generated <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> exhibits superior performance compared to a larger BERT model on semantic equivalence assessment tasks. Furthermore, it achieves larger performance gains on tasks with limited training datasets for <a href="https://en.wikipedia.org/wiki/Fine-tuning">fine-tuning</a>, which is a property desirable for <a href="https://en.wikipedia.org/wiki/Transfer_learning">transfer learning</a>.</abstract>
      <url hash="778c76fc">D19-1542</url>
      <doi>10.18653/v1/D19-1542</doi>
      <bibkey>arase-tsujii-2019-transfer</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/cola">CoLA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/qnli">QNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="548">
      <title>Modeling Graph Structure in Transformer for Better AMR-to-Text Generation<fixed-case>AMR</fixed-case>-to-Text Generation</title>
      <author><first>Jie</first><last>Zhu</last></author>
      <author><first>Junhui</first><last>Li</last></author>
      <author><first>Muhua</first><last>Zhu</last></author>
      <author><first>Longhua</first><last>Qian</last></author>
      <author><first>Min</first><last>Zhang</last></author>
      <author><first>Guodong</first><last>Zhou</last></author>
      <pages>5459–5468</pages>
      <abstract>Recent studies on AMR-to-text generation often formalize the task as a sequence-to-sequence (seq2seq) learning problem by converting an Abstract Meaning Representation (AMR) graph into a word sequences. Graph structures are further modeled into the seq2seq framework in order to utilize the structural information in the AMR graphs. However, previous approaches only consider the relations between directly connected concepts while ignoring the rich structure in AMR graphs. In this paper we eliminate such a strong limitation and propose a novel structure-aware self-attention approach to better model the relations between indirectly connected concepts in the state-of-the-art seq2seq model, i.e. the Transformer. In particular, a few different <a href="https://en.wikipedia.org/wiki/Methodology">methods</a> are explored to learn <a href="https://en.wikipedia.org/wiki/Representation_(arts)">structural representations</a> between two concepts. Experimental results on English AMR benchmark datasets show that our approach significantly outperforms the <a href="https://en.wikipedia.org/wiki/State_of_the_art">state-of-the-art</a> with 29.66 and 31.82 BLEU scores on LDC2015E86 and LDC2017T10, respectively. To the best of our knowledge, these are the best results achieved so far by <a href="https://en.wikipedia.org/wiki/Supervised_learning">supervised models</a> on the <a href="https://en.wikipedia.org/wiki/Benchmark_(computing)">benchmarks</a>.</abstract>
      <url hash="5c6aeba3">D19-1548</url>
      <doi>10.18653/v1/D19-1548</doi>
      <bibkey>zhu-etal-2019-modeling</bibkey>
      <pwccode url="https://github.com/Amazing-J/structural-transformer" additional="false">Amazing-J/structural-transformer</pwccode>
    </paper>
    <paper id="553">
      <title>Specificity-Driven Cascading Approach for Unsupervised Sentiment Modification</title>
      <author><first>Pengcheng</first><last>Yang</last></author>
      <author><first>Junyang</first><last>Lin</last></author>
      <author><first>Jingjing</first><last>Xu</last></author>
      <author><first>Jun</first><last>Xie</last></author>
      <author><first>Qi</first><last>Su</last></author>
      <author><first>Xu</first><last>Sun</last></author>
      <pages>5508–5517</pages>
      <abstract>The task of unsupervised sentiment modification aims to reverse the sentiment polarity of the input text while preserving its semantic content without any parallel data. Most previous work follows a two-step process. They first separate the content from the original sentiment, and then directly generate text with the target sentiment only based on the content produced by the first step. However, the second step bears both the target sentiment addition and content reconstruction, thus resulting in a lack of specific information like proper nouns in the generated text. To remedy this, we propose a specificity-driven cascading approach in this work, which can effectively increase the specificity of the generated text and further improve content preservation. In addition, we propose a more reasonable <a href="https://en.wikipedia.org/wiki/Metric_(mathematics)">metric</a> to evaluate <a href="https://en.wikipedia.org/wiki/Sentiment_analysis">sentiment modification</a>. The experiments show that our approach outperforms competitive baselines by a large margin, which achieves 11 % and 38 % relative improvements of the overall metric on the Yelp and Amazon datasets, respectively.</abstract>
      <url hash="e1013c8b">D19-1553</url>
      <doi>10.18653/v1/D19-1553</doi>
      <bibkey>yang-etal-2019-specificity</bibkey>
    </paper>
    <paper id="556">
      <title>From the Token to the Review : A Hierarchical Multimodal approach to <a href="https://en.wikipedia.org/wiki/Opinion_mining">Opinion Mining</a></title>
      <author><first>Alexandre</first><last>Garcia</last></author>
      <author><first>Pierre</first><last>Colombo</last></author>
      <author><first>Florence</first><last>d’Alché-Buc</last></author>
      <author><first>Slim</first><last>Essid</last></author>
      <author><first>Chloé</first><last>Clavel</last></author>
      <pages>5539–5548</pages>
      <abstract>The task of predicting fine grained user opinion based on spontaneous spoken language is a key problem arising in the development of Computational Agents as well as in the development of social network based opinion miners. Unfortunately, gathering reliable data on which a <a href="https://en.wikipedia.org/wiki/Statistical_model">model</a> can be trained is notoriously difficult and existing works rely only on coarsely labeled opinions. In this work we aim at bridging the gap separating fine grained opinion models already developed for <a href="https://en.wikipedia.org/wiki/Written_language">written language</a> and coarse grained models developed for spontaneous multimodal opinion mining. We take advantage of the implicit hierarchical structure of opinions to build a joint fine and coarse grained opinion model that exploits different views of the opinion expression. The resulting model shares some properties with attention-based models and is shown to provide competitive results on a recently released multimodal fine grained annotated corpus.</abstract>
      <url hash="1ed74faa">D19-1556</url>
      <attachment hash="298bf746">D19-1556.Attachment.pdf</attachment>
      <doi>10.18653/v1/D19-1556</doi>
      <bibkey>garcia-etal-2019-token</bibkey>
    </paper>
    <paper id="558">
      <title>Domain-Invariant Feature Distillation for Cross-Domain Sentiment Classification</title>
      <author><first>Mengting</first><last>Hu</last></author>
      <author><first>Yike</first><last>Wu</last></author>
      <author><first>Shiwan</first><last>Zhao</last></author>
      <author><first>Honglei</first><last>Guo</last></author>
      <author><first>Renhong</first><last>Cheng</last></author>
      <author><first>Zhong</first><last>Su</last></author>
      <pages>5559–5568</pages>
      <abstract>Cross-domain sentiment classification has drawn much attention in recent years. Most existing approaches focus on learning domain-invariant representations in both the source and target domains, while few of them pay attention to the domain-specific information. Despite the non-transferability of the domain-specific information, simultaneously learning domain-dependent representations can facilitate the learning of domain-invariant representations. In this paper, we focus on aspect-level cross-domain sentiment classification, and propose to distill the domain-invariant sentiment features with the help of an orthogonal domain-dependent task, i.e. aspect detection, which is built on the aspects varying widely in different domains. We conduct extensive experiments on three public datasets and the experimental results demonstrate the effectiveness of our method.</abstract>
      <url hash="a41fa305">D19-1558</url>
      <doi>10.18653/v1/D19-1558</doi>
      <bibkey>hu-etal-2019-domain</bibkey>
    </paper>
    <paper id="562">
      <title>Rethinking Attribute Representation and Injection for Sentiment Classification</title>
      <author><first>Reinald Kim</first><last>Amplayo</last></author>
      <pages>5602–5613</pages>
      <abstract>Text attributes, such as user and product information in <a href="https://en.wikipedia.org/wiki/Review">product reviews</a>, have been used to improve the performance of sentiment classification models. The de facto standard method is to incorporate them as additional biases in the <a href="https://en.wikipedia.org/wiki/Attentional_control">attention mechanism</a>, and more performance gains are achieved by extending the model architecture. In this paper, we show that the above <a href="https://en.wikipedia.org/wiki/Method_(computer_programming)">method</a> is the least effective way to represent and inject attributes. To demonstrate this hypothesis, unlike previous models with complicated architectures, we limit our base model to a simple BiLSTM with attention classifier, and instead focus on how and where the attributes should be incorporated in the <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a>. We propose to represent <a href="https://en.wikipedia.org/wiki/Attribute_(computing)">attributes</a> as chunk-wise importance weight matrices and consider four locations in the <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a> (i.e., embedding, encoding, attention, classifier) to inject attributes. Experiments show that our proposed method achieves significant improvements over the standard approach and that attention mechanism is the worst location to inject <a href="https://en.wikipedia.org/wiki/Attribute_(computing)">attributes</a>, contradicting prior work. We also outperform the <a href="https://en.wikipedia.org/wiki/State_of_the_art">state-of-the-art</a> despite our use of a simple base model. Finally, we show that these <a href="https://en.wikipedia.org/wiki/Representation_(mathematics)">representations</a> transfer well to other <a href="https://en.wikipedia.org/wiki/Task_(project_management)">tasks</a>. Model implementation and datasets are released here : https://github.com/rktamplayo/CHIM.</abstract>
      <url hash="b2258426">D19-1562</url>
      <doi>10.18653/v1/D19-1562</doi>
      <bibkey>amplayo-2019-rethinking</bibkey>
      <pwccode url="https://github.com/rktamplayo/CHIM" additional="false">rktamplayo/CHIM</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
    </paper>
    <paper id="563">
      <title>A Knowledge Regularized Hierarchical Approach for Emotion Cause Analysis</title>
      <author><first>Chuang</first><last>Fan</last></author>
      <author><first>Hongyu</first><last>Yan</last></author>
      <author><first>Jiachen</first><last>Du</last></author>
      <author><first>Lin</first><last>Gui</last></author>
      <author><first>Lidong</first><last>Bing</last></author>
      <author><first>Min</first><last>Yang</last></author>
      <author><first>Ruifeng</first><last>Xu</last></author>
      <author><first>Ruibin</first><last>Mao</last></author>
      <pages>5614–5624</pages>
      <abstract>Emotion cause analysis, which aims to identify the reasons behind emotions, is a key topic in <a href="https://en.wikipedia.org/wiki/Sentiment_analysis">sentiment analysis</a>. A variety of neural network models have been proposed recently, however, these previous models mostly focus on the learning architecture with local textual information, ignoring the discourse and prior knowledge, which play crucial roles in human text comprehension. In this paper, we propose a new method to extract emotion cause with a hierarchical neural model and knowledge-based regularizations, which aims to incorporate discourse context information and restrain the parameters by sentiment lexicon and <a href="https://en.wikipedia.org/wiki/Common_knowledge">common knowledge</a>. The experimental results demonstrate that our proposed method achieves the state-of-the-art performance on two public datasets in different languages (Chinese and English), outperforming a number of competitive baselines by at least 2.08 % in <a href="https://en.wikipedia.org/wiki/F-measure">F-measure</a>.</abstract>
      <url hash="72270898">D19-1563</url>
      <doi>10.18653/v1/D19-1563</doi>
      <bibkey>fan-etal-2019-knowledge</bibkey>
    </paper>
    <paper id="564">
      <title>Automatic Argument Quality Assessment-New Datasets and Methods</title>
      <author><first>Assaf</first><last>Toledo</last></author>
      <author><first>Shai</first><last>Gretz</last></author>
      <author><first>Edo</first><last>Cohen-Karlik</last></author>
      <author><first>Roni</first><last>Friedman</last></author>
      <author><first>Elad</first><last>Venezian</last></author>
      <author><first>Dan</first><last>Lahav</last></author>
      <author><first>Michal</first><last>Jacovi</last></author>
      <author><first>Ranit</first><last>Aharonov</last></author>
      <author><first>Noam</first><last>Slonim</last></author>
      <pages>5625–5635</pages>
      <abstract>We explore the task of automatic assessment of argument quality. To that end, we actively collected 6.3k arguments, more than a factor of five compared to previously examined data. Each argument was explicitly and carefully annotated for its quality. In addition, 14k pairs of arguments were annotated independently, identifying the higher quality argument in each pair. In spite of the inherent subjective nature of the task, both <a href="https://en.wikipedia.org/wiki/Annotation">annotation schemes</a> led to surprisingly consistent results. We release the labeled datasets to the community. Furthermore, we suggest <a href="https://en.wikipedia.org/wiki/Artificial_neural_network">neural methods</a> based on a recently released <a href="https://en.wikipedia.org/wiki/Language_model">language model</a>, for argument ranking as well as for argument-pair classification. In the former <a href="https://en.wikipedia.org/wiki/Task_(project_management)">task</a>, our results are comparable to <a href="https://en.wikipedia.org/wiki/State_of_the_art">state-of-the-art</a> ; in the latter <a href="https://en.wikipedia.org/wiki/Task_(project_management)">task</a> our results significantly outperform earlier <a href="https://en.wikipedia.org/wiki/Methodology">methods</a>.</abstract>
      <url hash="b704f652">D19-1564</url>
      <attachment hash="d784820f">D19-1564.Attachment.zip</attachment>
      <doi>10.18653/v1/D19-1564</doi>
      <bibkey>toledo-etal-2019-automatic</bibkey>
    </paper>
    <paper id="567">
      <title>Sequential Learning of Convolutional Features for Effective Text Classification</title>
      <author><first>Avinash</first><last>Madasu</last></author>
      <author><first>Vijjini</first><last>Anvesh Rao</last></author>
      <pages>5658–5667</pages>
      <abstract>Text classification has been one of the major problems in <a href="https://en.wikipedia.org/wiki/Natural_language_processing">natural language processing</a>. With the advent of <a href="https://en.wikipedia.org/wiki/Deep_learning">deep learning</a>, convolutional neural network (CNN) has been a popular solution to this task. However, CNNs which were first proposed for <a href="https://en.wikipedia.org/wiki/Digital_image">images</a>, face many crucial challenges in the context of <a href="https://en.wikipedia.org/wiki/Text_processing">text processing</a>, namely in their elementary blocks : convolution filters and <a href="https://en.wikipedia.org/wiki/Max_pooling">max pooling</a>. These challenges have largely been overlooked by the most existing CNN models proposed for <a href="https://en.wikipedia.org/wiki/Text_classification">text classification</a>. In this paper, we present an experimental study on the fundamental blocks of CNNs in text categorization. Based on this critique, we propose Sequential Convolutional Attentive Recurrent Network (SCARN). The proposed SCARN model utilizes both the advantages of recurrent and convolutional structures efficiently in comparison to previously proposed recurrent convolutional models. We test our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> on different text classification datasets across tasks like <a href="https://en.wikipedia.org/wiki/Sentiment_analysis">sentiment analysis</a> and question classification. Extensive experiments establish that SCARN outperforms other recurrent convolutional architectures with significantly less parameters. Furthermore, SCARN achieves better performance compared to equally large various <a href="https://en.wikipedia.org/wiki/Deep_learning">deep CNN</a> and LSTM architectures.</abstract>
      <url hash="6c8199ef">D19-1567</url>
      <doi>10.18653/v1/D19-1567</doi>
      <bibkey>madasu-anvesh-rao-2019-sequential</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="571">
      <title>Simple and Effective Noisy Channel Modeling for Neural Machine Translation</title>
      <author><first>Kyra</first><last>Yee</last></author>
      <author><first>Yann</first><last>Dauphin</last></author>
      <author><first>Michael</first><last>Auli</last></author>
      <pages>5696–5701</pages>
      <abstract>Previous work on neural noisy channel modeling relied on <a href="https://en.wikipedia.org/wiki/Latent_variable_model">latent variable models</a> that incrementally process the source and target sentence. This makes decoding decisions based on partial source prefixes even though the full source is available. We pursue an alternative approach based on standard sequence to sequence models which utilize the entire source. These <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> perform remarkably well as channel models, even though they have neither been trained on, nor designed to factor over incomplete target sentences. Experiments with neural language models trained on billions of words show that noisy channel models can outperform a direct model by up to 3.2 BLEU on WMT’17 German-English translation. We evaluate on four language-pairs and our channel models consistently outperform strong alternatives such right-to-left reranking models and ensembles of direct models.</abstract>
      <url hash="ba5d2fd3">D19-1571</url>
      <attachment hash="7cf9fd08">D19-1571.Attachment.pdf</attachment>
      <doi>10.18653/v1/D19-1571</doi>
      <bibkey>yee-etal-2019-simple</bibkey>
      <pwccode url="https://github.com/pytorch/fairseq" additional="false">pytorch/fairseq</pwccode>
    </paper>
    <paper id="572">
      <title>MultiFiT : Efficient Multi-lingual Language Model Fine-tuning<fixed-case>M</fixed-case>ulti<fixed-case>F</fixed-case>i<fixed-case>T</fixed-case>: Efficient Multi-lingual Language Model Fine-tuning</title>
      <author><first>Julian</first><last>Eisenschlos</last></author>
      <author><first>Sebastian</first><last>Ruder</last></author>
      <author><first>Piotr</first><last>Czapla</last></author>
      <author><first>Marcin</first><last>Kadras</last></author>
      <author><first>Sylvain</first><last>Gugger</last></author>
      <author><first>Jeremy</first><last>Howard</last></author>
      <pages>5702–5707</pages>
      <abstract>Pretrained language models are promising particularly for low-resource languages as they only require unlabelled data. However, training existing <a href="https://en.wikipedia.org/wiki/Statistical_model">models</a> requires huge amounts of compute, while pretrained cross-lingual models often underperform on low-resource languages. We propose Multi-lingual language model Fine-Tuning (MultiFiT) to enable practitioners to train and fine-tune language models efficiently in their own language. In addition, we propose a zero-shot method using an existing pretrained cross-lingual model. We evaluate our methods on two widely used cross-lingual classification datasets where they outperform models pretrained on orders of magnitude more data and compute. We release all <a href="https://en.wikipedia.org/wiki/Conceptual_model">models</a> and code.</abstract>
      <url hash="47422f32">D19-1572</url>
      <attachment hash="53573e7a">D19-1572.Attachment.zip</attachment>
      <doi>10.18653/v1/D19-1572</doi>
      <bibkey>eisenschlos-etal-2019-multifit</bibkey>
      <pwccode url="" additional="true" />
      <pwcdataset url="https://paperswithcode.com/dataset/mldoc">MLDoc</pwcdataset>
    </paper>
    <paper id="573">
      <title>Hint-Based Training for Non-Autoregressive Machine Translation</title>
      <author><first>Zhuohan</first><last>Li</last></author>
      <author><first>Zi</first><last>Lin</last></author>
      <author><first>Di</first><last>He</last></author>
      <author><first>Fei</first><last>Tian</last></author>
      <author><first>Tao</first><last>Qin</last></author>
      <author><first>Liwei</first><last>Wang</last></author>
      <author><first>Tie-Yan</first><last>Liu</last></author>
      <pages>5708–5713</pages>
      <abstract>Due to the unparallelizable nature of the autoregressive factorization, AutoRegressive Translation (ART) models have to generate tokens sequentially during decoding and thus suffer from high inference latency. Non-AutoRegressive Translation (NART) models were proposed to reduce the inference time, but could only achieve inferior translation accuracy. In this paper, we proposed a novel approach to leveraging the hints from hidden states and word alignments to help the training of NART models. The results achieve significant improvement over previous NART models for the WMT14 En-De and De-En datasets and are even comparable to a strong LSTM-based ART baseline but one order of magnitude faster in inference.</abstract>
      <url hash="412e5a71">D19-1573</url>
      <attachment hash="f5a0fb55">D19-1573.Attachment.zip</attachment>
      <doi>10.18653/v1/D19-1573</doi>
      <bibkey>li-etal-2019-hint</bibkey>
      <pwccode url="https://github.com/zhuohan123/hint-nart" additional="false">zhuohan123/hint-nart</pwccode>
    </paper>
    <paper id="577">
      <title>Quantifying the Semantic Core of Gender Systems</title>
      <author><first>Adina</first><last>Williams</last></author>
      <author><first>Damian</first><last>Blasi</last></author>
      <author><first>Lawrence</first><last>Wolf-Sonkin</last></author>
      <author><first>Hanna</first><last>Wallach</last></author>
      <author><first>Ryan</first><last>Cotterell</last></author>
      <pages>5734–5739</pages>
      <abstract>Many of the world’s languages employ <a href="https://en.wikipedia.org/wiki/Grammatical_gender">grammatical gender</a> on the <a href="https://en.wikipedia.org/wiki/Lexeme">lexeme</a>. For instance, in <a href="https://en.wikipedia.org/wiki/Spanish_language">Spanish</a>, house casa is feminine, whereas the word for paper papel is masculine. To a speaker of a <a href="https://en.wikipedia.org/wiki/Genderless_language">genderless language</a>, this categorization seems to exist with neither <a href="https://en.wikipedia.org/wiki/Rhyme">rhyme</a> nor reason. But, is the association of <a href="https://en.wikipedia.org/wiki/Noun">nouns</a> to <a href="https://en.wikipedia.org/wiki/Gender">gender classes</a> truly arbitrary? In this work, we present the first large-scale investigation of the arbitrariness of gender assignment that uses canonical correlation analysis as a method for correlating the gender of inanimate nouns with their lexical semantic meaning. We find that the gender systems of 18 languages exhibit a significant correlation with an externally grounded definition of lexical semantics.</abstract>
      <url hash="bddda6e3">D19-1577</url>
      <doi>10.18653/v1/D19-1577</doi>
      <bibkey>williams-etal-2019-quantifying</bibkey>
    </paper>
    <paper id="578">
      <title>Perturbation Sensitivity Analysis to Detect Unintended Model Biases</title>
      <author><first>Vinodkumar</first><last>Prabhakaran</last></author>
      <author><first>Ben</first><last>Hutchinson</last></author>
      <author><first>Margaret</first><last>Mitchell</last></author>
      <pages>5740–5745</pages>
      <abstract>Data-driven statistical Natural Language Processing (NLP) techniques leverage large amounts of language data to build models that can understand language. However, most language data reflect the public discourse at the time the data was produced, and hence NLP models are susceptible to learning incidental associations around named referents at a particular point in time, in addition to general linguistic meaning. An NLP system designed to model notions such as <a href="https://en.wikipedia.org/wiki/Sentimentality">sentiment</a> and <a href="https://en.wikipedia.org/wiki/Toxicity">toxicity</a> should ideally produce scores that are independent of the identity of such entities mentioned in text and their <a href="https://en.wikipedia.org/wiki/Association_(psychology)">social associations</a>. For example, in a general purpose sentiment analysis system, a phrase such as I hate Katy Perry should be interpreted as having the same sentiment as I hate Taylor Swift. Based on this idea, we propose a generic evaluation framework, Perturbation Sensitivity Analysis, which detects unintended model biases related to named entities, and requires no new annotations or corpora. We demonstrate the utility of this analysis by employing it on two different NLP models   a sentiment model and a toxicity model   applied on online comments in English language from four different genres.</abstract>
      <url hash="a7ed6ede">D19-1578</url>
      <doi>10.18653/v1/D19-1578</doi>
      <bibkey>prabhakaran-etal-2019-perturbation</bibkey>
    </paper>
    <paper id="582">
      <title>Event Detection with Multi-Order Graph Convolution and Aggregated Attention</title>
      <author><first>Haoran</first><last>Yan</last></author>
      <author><first>Xiaolong</first><last>Jin</last></author>
      <author><first>Xiangbin</first><last>Meng</last></author>
      <author><first>Jiafeng</first><last>Guo</last></author>
      <author><first>Xueqi</first><last>Cheng</last></author>
      <pages>5766–5770</pages>
      <abstract>Syntactic relations are broadly used in many NLP tasks. For event detection, syntactic relation representations based on dependency tree can better capture the interrelations between candidate trigger words and related entities than sentence representations. But, existing studies only use <a href="https://en.wikipedia.org/wiki/First-order_logic">first-order syntactic relations</a> (i.e., the arcs) in dependency trees to identify trigger words. For this reason, this paper proposes a new method for event detection, which uses a dependency tree based graph convolution network with aggregative attention to explicitly model and aggregate multi-order syntactic representations in sentences. Experimental comparison with state-of-the-art baselines shows the superiority of the proposed method.</abstract>
      <url hash="2c76f42d">D19-1582</url>
      <doi>10.18653/v1/D19-1582</doi>
      <bibkey>yan-etal-2019-event</bibkey>
    </paper>
    <paper id="584">
      <title>HMEAE : Hierarchical Modular Event Argument Extraction<fixed-case>HMEAE</fixed-case>: Hierarchical Modular Event Argument Extraction</title>
      <author><first>Xiaozhi</first><last>Wang</last></author>
      <author><first>Ziqi</first><last>Wang</last></author>
      <author><first>Xu</first><last>Han</last></author>
      <author><first>Zhiyuan</first><last>Liu</last></author>
      <author><first>Juanzi</first><last>Li</last></author>
      <author><first>Peng</first><last>Li</last></author>
      <author><first>Maosong</first><last>Sun</last></author>
      <author><first>Jie</first><last>Zhou</last></author>
      <author><first>Xiang</first><last>Ren</last></author>
      <pages>5777–5783</pages>
      <abstract>Existing <a href="https://en.wikipedia.org/wiki/Event_(probability_theory)">event extraction methods</a> classify each argument role independently, ignoring the conceptual correlations between different argument roles. In this paper, we propose a Hierarchical Modular Event Argument Extraction (HMEAE) model, to provide effective <a href="https://en.wikipedia.org/wiki/Inductive_bias">inductive bias</a> from the concept hierarchy of event argument roles. Specifically, we design a neural module network for each basic unit of the concept hierarchy, and then hierarchically compose relevant unit modules with logical operations into a role-oriented modular network to classify a specific argument role. As many argument roles share the same high-level unit module, their correlation can be utilized to extract specific event arguments better. Experiments on real-world datasets show that HMEAE can effectively leverage useful knowledge from the concept hierarchy and significantly outperform the state-of-the-art baselines. The source code can be obtained from https://github.com/thunlp/HMEAE.</abstract>
      <url hash="518e05ff">D19-1584</url>
      <attachment hash="fe805f95">D19-1584.Attachment.zip</attachment>
      <doi>10.18653/v1/D19-1584</doi>
      <bibkey>wang-etal-2019-hmeae</bibkey>
      <pwccode url="https://github.com/thunlp/HMEAE" additional="false">thunlp/HMEAE</pwccode>
    </paper>
    <paper id="587">
      <title>Split or Merge : Which is Better for Unsupervised RST Parsing?<fixed-case>RST</fixed-case> Parsing?</title>
      <author><first>Naoki</first><last>Kobayashi</last></author>
      <author><first>Tsutomu</first><last>Hirao</last></author>
      <author><first>Kengo</first><last>Nakamura</last></author>
      <author><first>Hidetaka</first><last>Kamigaito</last></author>
      <author><first>Manabu</first><last>Okumura</last></author>
      <author><first>Masaaki</first><last>Nagata</last></author>
      <pages>5797–5802</pages>
      <abstract>Rhetorical Structure Theory (RST) parsing is crucial for many downstream NLP tasks that require a discourse structure for a text. Most of the previous RST parsers have been based on supervised learning approaches. That is, they require an annotated corpus of sufficient size and quality, and heavily rely on the language and domain dependent corpus. In this paper, we present two language-independent unsupervised RST parsing methods based on <a href="https://en.wikipedia.org/wiki/Dynamic_programming">dynamic programming</a>. The first one builds the optimal tree in terms of a dissimilarity score function that is defined for splitting a text span into smaller ones. The second builds the optimal tree in terms of a similarity score function that is defined for merging two adjacent spans into a large one. Experimental results on English and German RST treebanks showed that our <a href="https://en.wikipedia.org/wiki/Parsing">parser</a> based on span merging achieved the best score, around 0.8 F_1 score, which is close to the scores of the previous supervised parsers.<tex-math>_1</tex-math> score, which is close to the scores of the previous supervised parsers.</abstract>
      <url hash="3dd798d7">D19-1587</url>
      <doi>10.18653/v1/D19-1587</doi>
      <bibkey>kobayashi-etal-2019-split</bibkey>
    </paper>
    <paper id="588">
      <title>BERT for <a href="https://en.wikipedia.org/wiki/Coreference_resolution">Coreference Resolution</a> : Baselines and Analysis<fixed-case>BERT</fixed-case> for Coreference Resolution: Baselines and Analysis</title>
      <author><first>Mandar</first><last>Joshi</last></author>
      <author><first>Omer</first><last>Levy</last></author>
      <author><first>Luke</first><last>Zettlemoyer</last></author>
      <author><first>Daniel</first><last>Weld</last></author>
      <pages>5803–5808</pages>
      <abstract>We apply BERT to <a href="https://en.wikipedia.org/wiki/Coreference_resolution">coreference resolution</a>, achieving a new state of the art on the GAP (+11.5 F1) and OntoNotes (+3.9 F1) benchmarks. A qualitative analysis of model predictions indicates that, compared to ELMo and BERT-base, BERT-large is particularly better at distinguishing between related but distinct entities (e.g., President and CEO), but that there is still room for improvement in modeling document-level context, conversations, and mention paraphrasing. We will release all code and trained models upon publication.</abstract>
      <url hash="3f77a000">D19-1588</url>
      <doi>10.18653/v1/D19-1588</doi>
      <bibkey>joshi-etal-2019-bert</bibkey>
      <pwccode url="https://github.com/mandarjoshi90/coref" additional="true">mandarjoshi90/coref</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/conll-2012-1">CoNLL-2012</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/gap-coreference-dataset">GAP Coreference Dataset</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/ontonotes-5-0">OntoNotes 5.0</pwcdataset>
    </paper>
    <paper id="591">
      <title>What Part of the <a href="https://en.wikipedia.org/wiki/Neural_network">Neural Network</a> Does This? Understanding LSTMs by Measuring and Dissecting Neurons<fixed-case>LSTM</fixed-case>s by Measuring and Dissecting Neurons</title>
      <author><first>Ji</first><last>Xin</last></author>
      <author><first>Jimmy</first><last>Lin</last></author>
      <author><first>Yaoliang</first><last>Yu</last></author>
      <pages>5823–5830</pages>
      <abstract>Memory neurons of long short-term memory (LSTM) networks encode and process information in powerful yet mysterious ways. While there has been work to analyze their behavior in carrying low-level information such as linguistic properties, how they directly contribute to label prediction remains unclear. We find inspiration from biologists and study the affinity between individual neurons and labels, propose a novel <a href="https://en.wikipedia.org/wiki/Metric_(mathematics)">metric</a> to quantify the sensitivity of neurons to each label, and conduct experiments to show the validity of our proposed <a href="https://en.wikipedia.org/wiki/Metric_(mathematics)">metric</a>. We discover that some neurons are trained to specialize on a subset of labels, and while dropping an arbitrary neuron has little effect on the overall <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> of the model, dropping label-specialized neurons predictably and significantly degrades prediction accuracy on the associated label. We further examine the consistency of neuron-label affinity across different models. These observations provide insight into the inner mechanisms of LSTMs.</abstract>
      <url hash="1f6e7856">D19-1591</url>
      <doi>10.18653/v1/D19-1591</doi>
      <bibkey>xin-etal-2019-part</bibkey>
    </paper>
    <paper id="594">
      <title>Text Genre and Training Data Size in Human-like Parsing</title>
      <author><first>John</first><last>Hale</last></author>
      <author><first>Adhiguna</first><last>Kuncoro</last></author>
      <author><first>Keith</first><last>Hall</last></author>
      <author><first>Chris</first><last>Dyer</last></author>
      <author><first>Jonathan</first><last>Brennan</last></author>
      <pages>5846–5852</pages>
      <abstract>Domain-specific training typically makes <a href="https://en.wikipedia.org/wiki/Natural_language_processing">NLP systems</a> work better. We show that this extends to cognitive modeling as well by relating the states of a neural phrase-structure parser to electrophysiological measures from human participants. These measures were recorded as participants listened to a spoken recitation of the same <a href="https://en.wikipedia.org/wiki/Literature">literary text</a> that was supplied as input to the neural parser. Given more training data, the system derives a better <a href="https://en.wikipedia.org/wiki/Cognitive_model">cognitive model</a>   but only when the training examples come from the same textual genre. This finding is consistent with the idea that humans adapt syntactic expectations to particular genres during <a href="https://en.wikipedia.org/wiki/Sentence_processing">language comprehension</a> (Kaan and Chun, 2018 ; Branigan and Pickering, 2017).</abstract>
      <url hash="827efa00">D19-1594</url>
      <doi>10.18653/v1/D19-1594</doi>
      <bibkey>hale-etal-2019-text</bibkey>
    </paper>
    <paper id="596">
      <title>Sunny and Dark Outside? ! Improving Answer Consistency in VQA through Entailed Question Generation<fixed-case>VQA</fixed-case> through Entailed Question Generation</title>
      <author><first>Arijit</first><last>Ray</last></author>
      <author><first>Karan</first><last>Sikka</last></author>
      <author><first>Ajay</first><last>Divakaran</last></author>
      <author><first>Stefan</first><last>Lee</last></author>
      <author><first>Giedrius</first><last>Burachas</last></author>
      <pages>5860–5865</pages>
      <abstract>While <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> for Visual Question Answering (VQA) have steadily improved over the years, interacting with one quickly reveals that these <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> lack <a href="https://en.wikipedia.org/wiki/Consistency">consistency</a>. For instance, if a <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> answers red to What color is the balloon?, it might answer no if asked, Is the balloon red?. These responses violate simple notions of <a href="https://en.wikipedia.org/wiki/Logical_consequence">entailment</a> and raise questions about how effectively VQA models ground language. In this work, we introduce a <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a>, ConVQA, and metrics that enable quantitative evaluation of <a href="https://en.wikipedia.org/wiki/Consistency">consistency</a> in <a href="https://en.wikipedia.org/wiki/Quality_assurance">VQA</a>. For a given observable fact in an <a href="https://en.wikipedia.org/wiki/Image_(mathematics)">image</a> (e.g. the balloon’s color), we generate a set of logically consistent question-answer (QA) pairs (e.g. Is the balloon red?) and also collect a human-annotated set of common-sense based consistent QA pairs (e.g. Is the balloon the same color as tomato sauce?). Further, we propose a consistency-improving data augmentation module, a Consistency Teacher Module (CTM). CTM automatically generates entailed (or similar-intent) questions for a source QA pair and fine-tunes the VQA model if the VQA’s answer to the entailed question is consistent with the source QA pair. We demonstrate that our CTM-based training improves the consistency of VQA models on the Con-VQA datasets and is a strong baseline for further research.</abstract>
      <url hash="7f203131">D19-1596</url>
      <attachment hash="41d07abc">D19-1596.Attachment.pdf</attachment>
      <doi>10.18653/v1/D19-1596</doi>
      <bibkey>ray-etal-2019-sunny</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/visual-genome">Visual Genome</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/visual-question-answering">Visual Question Answering</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/visual-question-answering-v2-0">Visual Question Answering v2.0</pwcdataset>
    </paper>
    <paper id="600">
      <title>A Span-Extraction Dataset for Chinese Machine Reading Comprehension<fixed-case>C</fixed-case>hinese Machine Reading Comprehension</title>
      <author><first>Yiming</first><last>Cui</last></author>
      <author><first>Ting</first><last>Liu</last></author>
      <author><first>Wanxiang</first><last>Che</last></author>
      <author><first>Li</first><last>Xiao</last></author>
      <author><first>Zhipeng</first><last>Chen</last></author>
      <author><first>Wentao</first><last>Ma</last></author>
      <author><first>Shijin</first><last>Wang</last></author>
      <author><first>Guoping</first><last>Hu</last></author>
      <pages>5883–5889</pages>
      <abstract>Machine Reading Comprehension (MRC) has become enormously popular recently and has attracted a lot of attention. However, the existing reading comprehension datasets are mostly in <a href="https://en.wikipedia.org/wiki/English_language">English</a>. In this paper, we introduce a Span-Extraction dataset for Chinese machine reading comprehension to add <a href="https://en.wikipedia.org/wiki/Language_diversity">language diversities</a> in this area. The <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> is composed by near 20,000 real questions annotated on Wikipedia paragraphs by human experts. We also annotated a challenge set which contains the questions that need comprehensive understanding and multi-sentence inference throughout the context. We present several <a href="https://en.wikipedia.org/wiki/Baseline_(configuration_management)">baseline systems</a> as well as anonymous submissions for demonstrating the difficulties in this <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a>. With the release of the <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a>, we hosted the Second Evaluation Workshop on Chinese Machine Reading Comprehension (CMRC 2018). We hope the release of the <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> could further accelerate the Chinese machine reading comprehension research. Resources are available : https://github.com/ymcui/cmrc2018</abstract>
      <url hash="231e0ea4">D19-1600</url>
      <doi>10.18653/v1/D19-1600</doi>
      <bibkey>cui-etal-2019-span</bibkey>
      <pwccode url="https://github.com/ymcui/cmrc2018" additional="false">ymcui/cmrc2018</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/cmrc">CMRC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/cmrc-2018">CMRC 2018</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
    </paper>
    <paper id="601">
      <title>MICRON : Multigranular Interaction for Contextualizing RepresentatiON in Non-factoid Question Answering<fixed-case>MICRON</fixed-case>: Multigranular Interaction for Contextualizing <fixed-case>R</fixed-case>epresentati<fixed-case>ON</fixed-case> in Non-factoid Question Answering</title>
      <author><first>Hojae</first><last>Han</last></author>
      <author><first>Seungtaek</first><last>Choi</last></author>
      <author><first>Haeju</first><last>Park</last></author>
      <author><first>Seung-won</first><last>Hwang</last></author>
      <pages>5890–5895</pages>
      <abstract>This paper studies the problem of non-factoid question answering, where the answer may span over multiple sentences. Existing solutions can be categorized into representation- and interaction-focused approaches. We combine their complementary strength, by a hybrid approach allowing multi-granular interactions, but represented at word level, enabling an easy integration with strong word-level signals. Specifically, we propose MICRON : Multigranular Interaction for Contextualizing RepresentatiON, a novel approach which derives contextualized uni-gram representation from <a href="https://en.wikipedia.org/wiki/N-gram">n-grams</a>. Our contributions are as follows : First, we enable multi-granular matches between question and answer n-grams. Second, by contextualizing word representation with surrounding n-grams, MICRON can naturally utilize word-based signals for query term weighting, known to be effective in <a href="https://en.wikipedia.org/wiki/Information_retrieval">information retrieval</a>. We validate MICRON in two public non-factoid question answering datasets : WikiPassageQA and InsuranceQA, showing our model achieves the state of the art among baselines with reported performances on both datasets.<tex-math>n</tex-math>-grams. Second, by contextualizing word representation with surrounding n-grams, MICRON can naturally utilize word-based signals for query term weighting, known to be effective in information retrieval. We validate MICRON in two public non-factoid question answering datasets: WikiPassageQA and InsuranceQA, showing our model achieves the state of the art among baselines with reported performances on both datasets.</abstract>
      <url hash="36a22459">D19-1601</url>
      <doi>10.18653/v1/D19-1601</doi>
      <bibkey>han-etal-2019-micron</bibkey>
    </paper>
    <paper id="602">
      <title>Machine Reading Comprehension Using Structural Knowledge Graph-aware Network</title>
      <author><first>Delai</first><last>Qiu</last></author>
      <author><first>Yuanzhe</first><last>Zhang</last></author>
      <author><first>Xinwei</first><last>Feng</last></author>
      <author><first>Xiangwen</first><last>Liao</last></author>
      <author><first>Wenbin</first><last>Jiang</last></author>
      <author><first>Yajuan</first><last>Lyu</last></author>
      <author><first>Kang</first><last>Liu</last></author>
      <author><first>Jun</first><last>Zhao</last></author>
      <pages>5896–5901</pages>
      <abstract>Leveraging external knowledge is an emerging trend in machine comprehension task. Previous work usually utilizes <a href="https://en.wikipedia.org/wiki/Knowledge_graph">knowledge graphs</a> such as <a href="https://en.wikipedia.org/wiki/ConceptNet">ConceptNet</a> as external knowledge, and extracts triples from them to enhance the initial representation of the machine comprehension context. However, such <a href="https://en.wikipedia.org/wiki/Methodology">method</a> can not capture the structural information in the <a href="https://en.wikipedia.org/wiki/Knowledge_graph">knowledge graph</a>. To this end, we propose a Structural Knowledge Graph-aware Network(SKG) model, constructing sub-graphs for entities in the machine comprehension context. Our <a href="https://en.wikipedia.org/wiki/Method_(computer_programming)">method</a> dynamically updates the representation of the knowledge according to the structural information of the constructed sub-graph. Experiments show that SKG achieves state-of-the-art performance on the ReCoRD dataset.</abstract>
      <url hash="d8d98fe4">D19-1602</url>
      <doi>10.18653/v1/D19-1602</doi>
      <bibkey>qiu-etal-2019-machine</bibkey>
    </paper>
    <paper id="604">
      <title>Improving Answer Selection and Answer Triggering using Hard Negatives</title>
      <author><first>Sawan</first><last>Kumar</last></author>
      <author><first>Shweta</first><last>Garg</last></author>
      <author><first>Kartik</first><last>Mehta</last></author>
      <author><first>Nikhil</first><last>Rasiwasia</last></author>
      <pages>5911–5917</pages>
      <abstract>In this paper, we establish the effectiveness of using hard negatives, coupled with a siamese network and a suitable <a href="https://en.wikipedia.org/wiki/Loss_function">loss function</a>, for the tasks of answer selection and answer triggering. We show that the choice of <a href="https://en.wikipedia.org/wiki/Sampling_(statistics)">sampling strategy</a> is key for achieving improved performance on these tasks. Evaluating on recent answer selection datasets-InsuranceQA, SelQA, and an internal QA dataset, we show that using hard negatives with relatively simple model architectures (bag of words and LSTM-CNN) drives significant performance gains. On InsuranceQA, this <a href="https://en.wikipedia.org/wiki/Strategy">strategy</a> alone improves over previously reported results by a minimum of 1.6 points in P@1. Using <a href="https://en.wikipedia.org/wiki/Negative_(photography)">hard negatives</a> with a Transformer encoder provides a further improvement of 2.3 points. Further, we propose to use quadruplet loss for answer triggering, with the aim of producing globally meaningful similarity scores. We show that quadruplet loss function coupled with the selection of hard negatives enables <a href="https://en.wikipedia.org/wiki/Bag-of-words_model">bag-of-words models</a> to improve <a href="https://en.wikipedia.org/wiki/Score_(statistics)">F1 score</a> by 2.3 points over previous baselines, on SelQA answer triggering dataset. Our results provide key insights into answer selection and answer triggering tasks.</abstract>
      <url hash="a3dc2655">D19-1604</url>
      <attachment hash="8a48f51d">D19-1604.Attachment.pdf</attachment>
      <doi>10.18653/v1/D19-1604</doi>
      <bibkey>kumar-etal-2019-improving</bibkey>
    </paper>
    <paper id="607">
      <title>Zero-shot Reading Comprehension by Cross-lingual Transfer Learning with Multi-lingual Language Representation Model</title>
      <author><first>Tsung-Yuan</first><last>Hsu</last></author>
      <author><first>Chi-Liang</first><last>Liu</last></author>
      <author><first>Hung-yi</first><last>Lee</last></author>
      <pages>5933–5940</pages>
      <abstract>Because it is not feasible to collect training data for every language, there is a growing interest in cross-lingual transfer learning. In this paper, we systematically explore zero-shot cross-lingual transfer learning on reading comprehension tasks with language representation model pre-trained on <a href="https://en.wikipedia.org/wiki/Multilingualism">multi-lingual corpus</a>. The experimental results show that with pre-trained language representation zero-shot learning is feasible, and translating the source data into the target language is not necessary and even degrades the performance. We further explore what does the <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> learn in zero-shot setting.</abstract>
      <url hash="7edc1c2e">D19-1607</url>
      <attachment hash="0bfb6a6c">D19-1607.Attachment.pdf</attachment>
      <doi>10.18653/v1/D19-1607</doi>
      <bibkey>hsu-etal-2019-zero</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/drcd">DRCD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
    </paper>
    <paper id="611">
      <title>Polly Want a Cracker : Analyzing Performance of Parroting on Paraphrase Generation Datasets</title>
      <author><first>Hong-Ren</first><last>Mao</last></author>
      <author><first>Hung-Yi</first><last>Lee</last></author>
      <pages>5960–5968</pages>
      <abstract>Paraphrase generation is an interesting and challenging NLP task which has numerous practical applications. In this paper, we analyze <a href="https://en.wikipedia.org/wiki/Data_set">datasets</a> commonly used for <a href="https://en.wikipedia.org/wiki/Paraphrase_generation">paraphrase generation</a> research, and show that simply parroting input sentences surpasses state-of-the-art models in the literature when evaluated on standard <a href="https://en.wikipedia.org/wiki/Metric_(mathematics)">metrics</a>. Our findings illustrate that a <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> could be seemingly adept at generating <a href="https://en.wikipedia.org/wiki/Paraphrase">paraphrases</a>, despite only making trivial changes to the input sentence or even none at all.</abstract>
      <url hash="baca6c5f">D19-1611</url>
      <attachment hash="09fb9586">D19-1611.Attachment.zip</attachment>
      <doi>10.18653/v1/D19-1611</doi>
      <bibkey>mao-lee-2019-polly</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/coco">COCO</pwcdataset>
    </paper>
    <paper id="612">
      <title>Query-focused Sentence Compression in <a href="https://en.wikipedia.org/wiki/Time_complexity">Linear Time</a></title>
      <author><first>Abram</first><last>Handler</last></author>
      <author><first>Brendan</first><last>O’Connor</last></author>
      <pages>5969–5975</pages>
      <abstract>Search applications often display shortened sentences which must contain certain query terms and must fit within the space constraints of a user interface. This work introduces a new transition-based sentence compression technique developed for such settings. Our query-focused method constructs length and lexically constrained compressions in <a href="https://en.wikipedia.org/wiki/Time_complexity">linear time</a>, by growing a <a href="https://en.wikipedia.org/wiki/Graph_(abstract_data_type)">subgraph</a> in the dependency parse of a sentence. This theoretically efficient approach achieves an 11x empirical speedup over baseline ILP methods, while better reconstructing gold constrained shortenings. Such speedups help query-focused applications, because users are measurably hindered by interface lags. Additionally, our technique does not require an <a href="https://en.wikipedia.org/wiki/Linear_programming">ILP solver</a> or a <a href="https://en.wikipedia.org/wiki/Graphics_processing_unit">GPU</a>.</abstract>
      <url hash="55aef0c2">D19-1612</url>
      <attachment hash="1bf059c8">D19-1612.Attachment.zip</attachment>
      <doi>10.18653/v1/D19-1612</doi>
      <bibkey>handler-oconnor-2019-query</bibkey>
    </paper>
    <paper id="614">
      <title>Generating Highly Relevant Questions</title>
      <author><first>Jiazuo</first><last>Qiu</last></author>
      <author><first>Deyi</first><last>Xiong</last></author>
      <pages>5983–5987</pages>
      <abstract>The neural seq2seq based question generation (QG) is prone to generating generic and undiversified questions that are poorly relevant to the given passage and target answer. In this paper, we propose two <a href="https://en.wikipedia.org/wiki/Methodology">methods</a> to address the issue. (1) By a partial copy mechanism, we prioritize words that are morphologically close to words in the input passage when generating questions ; (2) By a QA-based reranker, from the n-best list of question candidates, we select questions that are preferred by both the QA and QG model. Experiments and analyses demonstrate that the proposed two methods substantially improve the relevance of generated questions to passages and answers.</abstract>
      <url hash="f345c434">D19-1614</url>
      <doi>10.18653/v1/D19-1614</doi>
      <bibkey>qiu-xiong-2019-generating</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
    </paper>
    <paper id="619">
      <title>An Empirical Comparison on Imitation Learning and <a href="https://en.wikipedia.org/wiki/Reinforcement_learning">Reinforcement Learning</a> for Paraphrase Generation</title>
      <author><first>Wanyu</first><last>Du</last></author>
      <author><first>Yangfeng</first><last>Ji</last></author>
      <pages>6012–6018</pages>
      <abstract>Generating paraphrases from given sentences involves decoding words step by step from a large vocabulary. To learn a <a href="https://en.wikipedia.org/wiki/Code">decoder</a>, <a href="https://en.wikipedia.org/wiki/Supervised_learning">supervised learning</a> which maximizes the likelihood of tokens always suffers from the <a href="https://en.wikipedia.org/wiki/Exposure_bias">exposure bias</a>. Although both <a href="https://en.wikipedia.org/wiki/Reinforcement_learning">reinforcement learning (RL)</a> and imitation learning (IL) have been widely used to alleviate the bias, the lack of direct comparison leads to only a partial image on their benefits. In this work, we present an empirical study on how <a href="https://en.wikipedia.org/wiki/RL_(complexity)">RL</a> and IL can help boost the performance of generating paraphrases, with the pointer-generator as a base model. Experiments on the benchmark datasets show that (1) imitation learning is constantly better than <a href="https://en.wikipedia.org/wiki/Reinforcement_learning">reinforcement learning</a> ; and (2) the pointer-generator models with imitation learning outperform the state-of-the-art methods with a large margin.</abstract>
      <url hash="a137e089">D19-1619</url>
      <attachment hash="e301f893">D19-1619.Attachment.zip</attachment>
      <doi>10.18653/v1/D19-1619</doi>
      <bibkey>du-ji-2019-empirical</bibkey>
      <pwccode url="https://github.com/ddddwy/Reinforce-Paraphrase-Generation" additional="false">ddddwy/Reinforce-Paraphrase-Generation</pwccode>
    </paper>
    <paper id="623">
      <title>Deep Reinforcement Learning with Distributional Semantic Rewards for Abstractive Summarization</title>
      <author><first>Siyao</first><last>Li</last></author>
      <author><first>Deren</first><last>Lei</last></author>
      <author><first>Pengda</first><last>Qin</last></author>
      <author><first>William Yang</first><last>Wang</last></author>
      <pages>6038–6044</pages>
      <abstract>Deep reinforcement learning (RL) has been a commonly-used strategy for the abstractive summarization task to address both the exposure bias and non-differentiable task issues. However, the conventional reward Rouge-L simply looks for exact n-grams matches between candidates and annotated references, which inevitably makes the generated sentences repetitive and incoherent. In this paper, instead of Rouge-L, we explore the practicability of utilizing the <a href="https://en.wikipedia.org/wiki/Distributional_semantics">distributional semantics</a> to measure the matching degrees. With <a href="https://en.wikipedia.org/wiki/Distributional_semantics">distributional semantics</a>, sentence-level evaluation can be obtained, and semantically-correct phrases can also be generated without being limited to the surface form of the reference sentences. Human judgments on Gigaword and CNN / Daily Mail datasets show that our proposed distributional semantics reward (DSR) has distinct superiority in capturing the lexical and compositional diversity of natural language.</abstract>
      <url hash="8dffce7d">D19-1623</url>
      <doi>10.18653/v1/D19-1623</doi>
      <bibkey>li-etal-2019-deep</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/cnn-daily-mail-1">CNN/Daily Mail</pwcdataset>
    </paper>
    <paper id="627">
      <title>What Does This Word Mean? Explaining Contextualized Embeddings with Natural Language Definition</title>
      <author><first>Ting-Yun</first><last>Chang</last></author>
      <author><first>Yun-Nung</first><last>Chen</last></author>
      <pages>6064–6070</pages>
      <abstract>Contextualized word embeddings have boosted many NLP tasks compared with traditional static word embeddings. However, the word with a specific sense may have different contextualized embeddings due to its various contexts. To further investigate what contextualized word embeddings capture, this paper analyzes whether they can indicate the corresponding sense definitions and proposes a general framework that is capable of explaining word meanings given contextualized word embeddings for better interpretation. The experiments show that both ELMo and BERT embeddings can be well interpreted via a readable textual form, and the findings may benefit the research community for a better understanding of what the <a href="https://en.wikipedia.org/wiki/Embedding">embeddings</a> capture.</abstract>
      <url hash="2aeab679">D19-1627</url>
      <attachment hash="42460d18">D19-1627.Attachment.zip</attachment>
      <doi>10.18653/v1/D19-1627</doi>
      <bibkey>chang-chen-2019-word</bibkey>
    </paper>
    <paper id="629">
      <title>WIQA : A dataset for What if... reasoning over procedural text<fixed-case>WIQA</fixed-case>: A dataset for “What if...” reasoning over procedural text</title>
      <author><first>Niket</first><last>Tandon</last></author>
      <author><first>Bhavana</first><last>Dalvi</last></author>
      <author><first>Keisuke</first><last>Sakaguchi</last></author>
      <author><first>Peter</first><last>Clark</last></author>
      <author><first>Antoine</first><last>Bosselut</last></author>
      <pages>6076–6085</pages>
      <abstract>We introduce WIQA, the first large-scale dataset of What if... questions over procedural text. WIQA contains a collection of paragraphs, each annotated with multiple influence graphs describing how one change affects another, and a large (40k) collection of What if...? multiple-choice questions derived from these. For example, given a paragraph about beach erosion, would stormy weather hasten or decelerate erosion? WIQA contains three kinds of questions : perturbations to steps mentioned in the paragraph ; external (out-of-paragraph) perturbations requiring commonsense knowledge ; and irrelevant (no effect) perturbations. We find that state-of-the-art <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> achieve 73.8 % <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a>, well below the <a href="https://en.wikipedia.org/wiki/Human_factors_and_ergonomics">human performance</a> of 96.3 %. We analyze the challenges, in particular tracking chains of influences, and present the <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> as an open challenge to the community.</abstract>
      <url hash="87cc642e">D19-1629</url>
      <attachment hash="0169a258">D19-1629.Attachment.zip</attachment>
      <doi>10.18653/v1/D19-1629</doi>
      <bibkey>tandon-etal-2019-wiqa</bibkey>
    </paper>
    <paper id="633">
      <title>Mask-Predict : Parallel Decoding of Conditional Masked Language Models</title>
      <author><first>Marjan</first><last>Ghazvininejad</last></author>
      <author><first>Omer</first><last>Levy</last></author>
      <author><first>Yinhan</first><last>Liu</last></author>
      <author><first>Luke</first><last>Zettlemoyer</last></author>
      <pages>6112–6121</pages>
      <abstract>Most <a href="https://en.wikipedia.org/wiki/Machine_translation">machine translation systems</a> generate text autoregressively from left to right. We, instead, use a masked language modeling objective to train a model to predict any subset of the target words, conditioned on both the input text and a partially masked target translation. This approach allows for efficient iterative decoding, where we first predict all of the target words non-autoregressively, and then repeatedly mask out and regenerate the subset of words that the model is least confident about. By applying this strategy for a constant number of iterations, our model improves state-of-the-art performance levels for non-autoregressive and parallel decoding translation models by over 4 BLEU on average. It is also able to reach within about 1 BLEU point of a typical left-to-right transformer model, while decoding significantly faster.</abstract>
      <url hash="1c166c44">D19-1633</url>
      <doi>10.18653/v1/D19-1633</doi>
      <bibkey>ghazvininejad-etal-2019-mask</bibkey>
      <pwccode url="https://github.com/facebookresearch/Mask-Predict" additional="true">facebookresearch/Mask-Predict</pwccode>
    </paper>
    <paper id="636">
      <title>A Modular Architecture for Unsupervised Sarcasm Generation</title>
      <author><first>Abhijit</first><last>Mishra</last></author>
      <author><first>Tarun</first><last>Tater</last></author>
      <author><first>Karthik</first><last>Sankaranarayanan</last></author>
      <pages>6144–6154</pages>
      <abstract>In this paper, we propose a novel framework for sarcasm generation ; the system takes a literal negative opinion as input and translates it into a sarcastic version. Our <a href="https://en.wikipedia.org/wiki/Software_framework">framework</a> does not require any <a href="https://en.wikipedia.org/wiki/Paired_data">paired data</a> for training. Sarcasm emanates from context-incongruity which becomes apparent as the sentence unfolds. Our framework introduces incongruity into the literal input version through modules that : (a) filter factual content from the input opinion, (b) retrieve incongruous phrases related to the filtered facts and (c) synthesize sarcastic text from the incongruous filtered and incongruous phrases. The framework employs reinforced neural sequence to sequence learning and information retrieval and is trained only using unlabeled non-sarcastic and sarcastic opinions. Since no labeled dataset exists for such a task, for evaluation, we manually prepare a benchmark dataset containing <a href="https://en.wikipedia.org/wiki/Literal_and_figurative_language">literal opinions</a> and their <a href="https://en.wikipedia.org/wiki/Sarcasm">sarcastic paraphrases</a>. Qualitative and quantitative performance analyses on the data reveal our system’s superiority over baselines built using known unsupervised statistical and neural machine translation and style transfer techniques.</abstract>
      <url hash="5b47f12f">D19-1636</url>
      <doi>10.18653/v1/D19-1636</doi>
      <bibkey>mishra-etal-2019-modular</bibkey>
    </paper>
    <paper id="640">
      <title>Detect Camouflaged Spam Content via StoneSkipping : Graph and Text Joint Embedding for Chinese Character Variation Representation<fixed-case>S</fixed-case>tone<fixed-case>S</fixed-case>kipping: Graph and Text Joint Embedding for <fixed-case>C</fixed-case>hinese Character Variation Representation</title>
      <author><first>Zhuoren</first><last>Jiang</last></author>
      <author><first>Zhe</first><last>Gao</last></author>
      <author><first>Guoxiu</first><last>He</last></author>
      <author><first>Yangyang</first><last>Kang</last></author>
      <author><first>Changlong</first><last>Sun</last></author>
      <author><first>Qiong</first><last>Zhang</last></author>
      <author><first>Luo</first><last>Si</last></author>
      <author><first>Xiaozhong</first><last>Liu</last></author>
      <pages>6187–6196</pages>
      <abstract>The task of Chinese text spam detection is very challenging due to both glyph and phonetic variations of Chinese characters. This paper proposes a novel framework to jointly model Chinese variational, semantic, and contextualized representations for Chinese text spam detection task. In particular, a Variation Family-enhanced Graph Embedding (VFGE) algorithm is designed based on a Chinese character variation graph. The VFGE can learn both the <a href="https://en.wikipedia.org/wiki/Graph_embedding">graph embeddings</a> of the Chinese characters (local) and the latent variation families (global). Furthermore, an enhanced bidirectional language model, with a combination gate function and an aggregation learning function, is proposed to integrate the graph and text information while capturing the sequential information. Extensive experiments have been conducted on both SMS and review datasets, to show the proposed method outperforms a series of state-of-the-art models for Chinese spam detection.</abstract>
      <url hash="bf1bd036">D19-1640</url>
      <doi>10.18653/v1/D19-1640</doi>
      <bibkey>jiang-etal-2019-detect</bibkey>
      <pwccode url="https://github.com/Giruvegan/stoneskipping" additional="false">Giruvegan/stoneskipping</pwccode>
    </paper>
    <paper id="642">
      <title>An Improved Neural Baseline for Temporal Relation Extraction</title>
      <author><first>Qiang</first><last>Ning</last></author>
      <author><first>Sanjay</first><last>Subramanian</last></author>
      <author><first>Dan</first><last>Roth</last></author>
      <pages>6203–6209</pages>
      <abstract>Determining temporal relations (e.g., before or after) between events has been a challenging natural language understanding task, partly due to the difficulty to generate large amounts of high-quality training data. Consequently, neural approaches have not been widely used on <a href="https://en.wikipedia.org/wiki/Information_technology">it</a>, or showed only moderate improvements. This paper proposes a new neural system that achieves about 10 % absolute improvement in <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> over the previous best <a href="https://en.wikipedia.org/wiki/System">system</a> (25 % error reduction) on two benchmark datasets. The proposed system is trained on the state-of-the-art MATRES dataset and applies contextualized word embeddings, a Siamese encoder of a temporal common sense knowledge base, and global inference via integer linear programming (ILP). We suggest that the new approach could serve as a strong baseline for future research in this area.</abstract>
      <url hash="a15f0db1">D19-1642</url>
      <attachment hash="e0273662">D19-1642.Attachment.zip</attachment>
      <doi>10.18653/v1/D19-1642</doi>
      <bibkey>ning-etal-2019-improved</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/tcr">TCR</pwcdataset>
    </paper>
    <paper id="645">
      <title>Cross-Sentence N-ary Relation Extraction using Lower-Arity Universal Schemas</title>
      <author><first>Kosuke</first><last>Akimoto</last></author>
      <author><first>Takuya</first><last>Hiraoka</last></author>
      <author><first>Kunihiko</first><last>Sadamasa</last></author>
      <author><first>Mathias</first><last>Niepert</last></author>
      <pages>6225–6231</pages>
      <abstract>Most existing relation extraction approaches exclusively target <a href="https://en.wikipedia.org/wiki/Binary_relation">binary relations</a>, and n-ary relation extraction is relatively unexplored. Current state-of-the-art n-ary relation extraction method is based on a supervised learning approach and, therefore, may suffer from the lack of sufficient relation labels. In this paper, we propose a novel approach to cross-sentence n-ary relation extraction based on universal schemas. To alleviate the sparsity problem and to leverage inherent decomposability of n-ary relations, we propose to learn relation representations of lower-arity facts that result from decomposing higher-arity facts. The proposed method computes a score of a new n-ary fact by aggregating scores of its decomposed lower-arity facts. We conduct experiments with <a href="https://en.wikipedia.org/wiki/Data_set">datasets</a> for ternary relation extraction and empirically show that our method improves the n-ary relation extraction performance compared to previous methods.</abstract>
      <url hash="95552c9f">D19-1645</url>
      <attachment hash="02449cb1">D19-1645.Attachment.zip</attachment>
      <doi>10.18653/v1/D19-1645</doi>
      <bibkey>akimoto-etal-2019-cross</bibkey>
    </paper>
    <paper id="646">
      <title>Gazetteer-Enhanced Attentive Neural Networks for Named Entity Recognition</title>
      <author><first>Hongyu</first><last>Lin</last></author>
      <author><first>Yaojie</first><last>Lu</last></author>
      <author><first>Xianpei</first><last>Han</last></author>
      <author><first>Le</first><last>Sun</last></author>
      <author><first>Bin</first><last>Dong</last></author>
      <author><first>Shanshan</first><last>Jiang</last></author>
      <pages>6232–6237</pages>
      <abstract>Current region-based NER models only rely on fully-annotated training data to learn effective region encoder, which often face the training data bottleneck. To alleviate this problem, this paper proposes Gazetteer-Enhanced Attentive Neural Networks, which can enhance region-based NER by learning name knowledge of entity mentions from easily-obtainable gazetteers, rather than only from fully-annotated data. Specially, we first propose an attentive neural network (ANN), which explicitly models the mention-context association and therefore is convenient for integrating externally-learned knowledge. Then we design an auxiliary gazetteer network, which can effectively encode name regularity of mentions only using <a href="https://en.wikipedia.org/wiki/Gazetteer">gazetteers</a>. Finally, the learned gazetteer network is incorporated into ANN for better <a href="https://en.wikipedia.org/wiki/Network_topology">NER</a>. Experiments show that our ANN can achieve the state-of-the-art performance on ACE2005 named entity recognition benchmark. Besides, incorporating gazetteer network can further improve the performance and significantly reduce the requirement of training data.</abstract>
      <url hash="8177c4a8">D19-1646</url>
      <doi>10.18653/v1/D19-1646</doi>
      <bibkey>lin-etal-2019-gazetteer</bibkey>
    </paper>
    <paper id="650">
      <title>ner and pos when nothing is capitalized<fixed-case>ner and pos when nothing is capitalized</fixed-case></title>
      <author><first>Stephen</first><last>Mayhew</last></author>
      <author><first>Tatiana</first><last>Tsygankova</last></author>
      <author><first>Dan</first><last>Roth</last></author>
      <pages>6256–6261</pages>
      <abstract>For those languages which use it, <a href="https://en.wikipedia.org/wiki/Capitalization">capitalization</a> is an important signal for the fundamental NLP tasks of <a href="https://en.wikipedia.org/wiki/Named-entity_recognition">Named Entity Recognition (NER)</a> and <a href="https://en.wikipedia.org/wiki/Part-of-speech_tagging">Part of Speech (POS) tagging</a>. In fact, it is such a strong signal that <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> performance on these tasks drops sharply in common lowercased scenarios, such as noisy web text or machine translation outputs. In this work, we perform a systematic analysis of solutions to this problem, modifying only the casing of the train or test data using lowercasing and truecasing methods. While prior work and first impressions might suggest training a caseless model, or using a truecaser at test time, we show that the most effective strategy is a concatenation of cased and lowercased training data, producing a single <a href="https://en.wikipedia.org/wiki/Statistical_model">model</a> with high performance on both cased and uncased text. As shown in our experiments, this result holds across tasks and input representations. Finally, we show that our proposed solution gives an 8 % F1 improvement in mention detection on noisy out-of-domain Twitter data.</abstract>
      <url hash="9ae055a6">D19-1650</url>
      <doi>10.18653/v1/D19-1650</doi>
      <bibkey>mayhew-etal-2019-ner</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/broad-twitter-corpus">Broad Twitter Corpus</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/conll-2003">CoNLL-2003</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/penn-treebank">Penn Treebank</pwcdataset>
    </paper>
    <paper id="653">
      <title>Revealing and Predicting Online Persuasion Strategy with Elementary Units</title>
      <author><first>Gaku</first><last>Morio</last></author>
      <author><first>Ryo</first><last>Egawa</last></author>
      <author><first>Katsuhide</first><last>Fujita</last></author>
      <pages>6274–6279</pages>
      <abstract>In online arguments, identifying how users construct their arguments to persuade others is important in order to understand a persuasive strategy directly. However, existing research lacks empirical investigations on highly semantic aspects of elementary units (EUs), such as propositions for a persuasive online argument. Therefore, this paper focuses on a pilot study, revealing a <a href="https://en.wikipedia.org/wiki/Persuasion">persuasion strategy</a> using <a href="https://en.wikipedia.org/wiki/European_Union">EUs</a>. Our contributions are as follows : (1) annotating five types of EUs in a persuasive forum, the so-called ChangeMyView, (2) revealing both intuitive and non-intuitive strategic insights for the persuasion by analyzing 4612 annotated EUs, and (3) proposing baseline neural models that identify the EU boundary and type. Our observations imply that <a href="https://en.wikipedia.org/wiki/European_Union">EUs</a> definitively characterize online persuasion strategies.</abstract>
      <url hash="5b7058af">D19-1653</url>
      <doi>10.18653/v1/D19-1653</doi>
      <bibkey>morio-etal-2019-revealing</bibkey>
    </paper>
    <paper id="654">
      <title>A Challenge Dataset and Effective Models for Aspect-Based Sentiment Analysis</title>
      <author><first>Qingnan</first><last>Jiang</last></author>
      <author><first>Lei</first><last>Chen</last></author>
      <author><first>Ruifeng</first><last>Xu</last></author>
      <author><first>Xiang</first><last>Ao</last></author>
      <author><first>Min</first><last>Yang</last></author>
      <pages>6280–6285</pages>
      <abstract>Aspect-based sentiment analysis (ABSA) has attracted increasing attention recently due to its broad applications. In existing ABSA datasets, most sentences contain only one aspect or multiple aspects with the same sentiment polarity, which makes ABSA task degenerate to sentence-level sentiment analysis. In this paper, we present a new large-scale Multi-Aspect Multi-Sentiment (MAMS) dataset, in which each sentence contains at least two different aspects with different sentiment polarities. The release of this <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> would push forward the research in this field. In addition, we propose simple yet effective CapsNet and CapsNet-BERT models which combine the strengths of recent NLP advances. Experiments on our new <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> show that the proposed <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> significantly outperforms the state-of-the-art baseline methods</abstract>
      <url hash="897ff3de">D19-1654</url>
      <doi>10.18653/v1/D19-1654</doi>
      <bibkey>jiang-etal-2019-challenge</bibkey>
      <pwccode url="https://github.com/siat-nlp/MAMS-for-ABSA" additional="false">siat-nlp/MAMS-for-ABSA</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/mams">MAMS</pwcdataset>
    </paper>
    <paper id="655">
      <title>Learning with Noisy Labels for Sentence-level Sentiment Classification</title>
      <author><first>Hao</first><last>Wang</last></author>
      <author><first>Bing</first><last>Liu</last></author>
      <author><first>Chaozhuo</first><last>Li</last></author>
      <author><first>Yan</first><last>Yang</last></author>
      <author><first>Tianrui</first><last>Li</last></author>
      <pages>6286–6292</pages>
      <abstract>Deep neural networks (DNNs) can fit (or even over-fit) the training data very well. If a DNN model is trained using data with noisy labels and tested on data with clean labels, the <a href="https://en.wikipedia.org/wiki/Statistical_model">model</a> may perform poorly. This paper studies the problem of learning with noisy labels for sentence-level sentiment classification. We propose a novel DNN model called NetAb (as shorthand for <a href="https://en.wikipedia.org/wiki/Convolutional_neural_network">convolutional neural Networks</a> with Ab-networks) to handle noisy labels during training. NetAb consists of two <a href="https://en.wikipedia.org/wiki/Convolutional_neural_network">convolutional neural networks</a>, one with a noise transition layer for dealing with the input noisy labels and the other for predicting ‘clean’ labels. We train the two <a href="https://en.wikipedia.org/wiki/Neural_network">networks</a> using their respective <a href="https://en.wikipedia.org/wiki/Loss_function">loss functions</a> in a mutual reinforcement manner. Experimental results demonstrate the effectiveness of the proposed <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a>.</abstract>
      <url hash="f2790245">D19-1655</url>
      <doi>10.18653/v1/D19-1655</doi>
      <bibkey>wang-etal-2019-learning-noisy</bibkey>
    </paper>
    <paper id="656">
      <title>DENS : A Dataset for Multi-class Emotion Analysis<fixed-case>DENS</fixed-case>: A Dataset for Multi-class Emotion Analysis</title>
      <author><first>Chen</first><last>Liu</last></author>
      <author><first>Muhammad</first><last>Osama</last></author>
      <author><first>Anderson</first><last>De Andrade</last></author>
      <pages>6293–6298</pages>
      <abstract>We introduce a new <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> for multi-class emotion analysis from long-form narratives in English. The Dataset for Emotions of Narrative Sequences (DENS) was collected from both classic literature available on Project Gutenberg and modern online narratives avail- able on <a href="https://en.wikipedia.org/wiki/Wattpad">Wattpad</a>, annotated using <a href="https://en.wikipedia.org/wiki/Amazon_Mechanical_Turk">Amazon Mechanical Turk</a>. A number of statistics and baseline benchmarks are provided for the <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a>. Of the tested techniques, we find that the <a href="https://en.wikipedia.org/wiki/Fine-tuning">fine-tuning</a> of a pre-trained BERT model achieves the best results, with an average micro-F1 score of 60.4 %. Our results show that the <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> provides a novel opportunity in emotion analysis that requires moving beyond existing sentence-level techniques.</abstract>
      <url hash="923196cc">D19-1656</url>
      <attachment hash="8a0653db">D19-1656.Attachment.zip</attachment>
      <doi>10.18653/v1/D19-1656</doi>
      <bibkey>liu-etal-2019-dens</bibkey>
    </paper>
    <paper id="657">
      <title>Multi-Task Stance Detection with Sentiment and Stance Lexicons</title>
      <author><first>Yingjie</first><last>Li</last></author>
      <author><first>Cornelia</first><last>Caragea</last></author>
      <pages>6299–6305</pages>
      <abstract>Stance detection aims to detect whether the opinion holder is in support of or against a given target. Recent works show improvements in stance detection by using either the <a href="https://en.wikipedia.org/wiki/Attentional_control">attention mechanism</a> or <a href="https://en.wikipedia.org/wiki/Sentiment_analysis">sentiment information</a>. In this paper, we propose a multi-task framework that incorporates target-specific attention mechanism and at the same time takes sentiment classification as an auxiliary task. Moreover, we used a sentiment lexicon and constructed a stance lexicon to provide guidance for the attention layer. Experimental results show that the proposed model significantly outperforms state-of-the-art deep learning methods on the SemEval-2016 dataset.</abstract>
      <url hash="e5aaecd9">D19-1657</url>
      <doi>10.18653/v1/D19-1657</doi>
      <bibkey>li-caragea-2019-multi</bibkey>
    </paper>
    <paper id="658">
      <title>A Robust Self-Learning Framework for Cross-Lingual Text Classification</title>
      <author><first>Xin</first><last>Dong</last></author>
      <author><first>Gerard</first><last>de Melo</last></author>
      <pages>6306–6310</pages>
      <abstract>Based on massive amounts of data, recent pretrained contextual representation models have made significant strides in advancing a number of different English NLP tasks. However, for other languages, relevant training data may be lacking, while state-of-the-art deep learning methods are known to be data-hungry. In this paper, we present an elegantly simple robust self-learning framework to include unlabeled non-English samples in the fine-tuning process of pretrained multilingual representation models. We leverage a multilingual model’s own predictions on unlabeled non-English data in order to obtain additional information that can be used during further <a href="https://en.wikipedia.org/wiki/Fine-tuning">fine-tuning</a>. Compared with original multilingual models and other cross-lingual classification models, we observe significant gains in effectiveness on document and sentiment classification for a range of diverse languages.</abstract>
      <url hash="da8f6a00">D19-1658</url>
      <doi>10.18653/v1/D19-1658</doi>
      <bibkey>dong-de-melo-2019-robust</bibkey>
    </paper>
    <paper id="660">
      <title>Label Embedding using Hierarchical Structure of Labels for Twitter Classification<fixed-case>T</fixed-case>witter Classification</title>
      <author><first>Taro</first><last>Miyazaki</last></author>
      <author><first>Kiminobu</first><last>Makino</last></author>
      <author><first>Yuka</first><last>Takei</last></author>
      <author><first>Hiroki</first><last>Okamoto</last></author>
      <author><first>Jun</first><last>Goto</last></author>
      <pages>6317–6322</pages>
      <abstract>Twitter is used for various <a href="https://en.wikipedia.org/wiki/Application_software">applications</a> such as <a href="https://en.wikipedia.org/wiki/Emergency_management">disaster monitoring</a> and <a href="https://en.wikipedia.org/wiki/Electronic_news-gathering">news material gathering</a>. In these <a href="https://en.wikipedia.org/wiki/Application_software">applications</a>, each Tweet is classified into pre-defined classes. These <a href="https://en.wikipedia.org/wiki/Class_(set_theory)">classes</a> have a semantic relationship with each other and can be classified into a <a href="https://en.wikipedia.org/wiki/Hierarchical_organization">hierarchical structure</a>, which is regarded as important information. Label texts of pre-defined classes themselves also include important clues for <a href="https://en.wikipedia.org/wiki/Taxonomy_(biology)">classification</a>. Therefore, we propose a <a href="https://en.wikipedia.org/wiki/Methodology">method</a> that can consider the <a href="https://en.wikipedia.org/wiki/Hierarchical_organization">hierarchical structure</a> of labels and label texts themselves. We conducted evaluation over the Text REtrieval Conference (TREC) 2018 Incident Streams (IS) track dataset, and we found that our method outperformed the methods of the conference participants.</abstract>
      <url hash="62558d31">D19-1660</url>
      <doi>10.18653/v1/D19-1660</doi>
      <bibkey>miyazaki-etal-2019-label</bibkey>
    </paper>
    <paper id="664">
      <title>In Plain Sight : <a href="https://en.wikipedia.org/wiki/Media_bias">Media Bias</a> Through the Lens of Factual Reporting</title>
      <author><first>Lisa</first><last>Fan</last></author>
      <author><first>Marshall</first><last>White</last></author>
      <author><first>Eva</first><last>Sharma</last></author>
      <author><first>Ruisi</first><last>Su</last></author>
      <author><first>Prafulla Kumar</first><last>Choubey</last></author>
      <author><first>Ruihong</first><last>Huang</last></author>
      <author><first>Lu</first><last>Wang</last></author>
      <pages>6343–6349</pages>
      <abstract>The increasing prevalence of political bias in <a href="https://en.wikipedia.org/wiki/News_media">news media</a> calls for greater public awareness of it, as well as robust methods for its detection. While prior work in <a href="https://en.wikipedia.org/wiki/Natural_language_processing">NLP</a> has primarily focused on the lexical bias captured by linguistic attributes such as <a href="https://en.wikipedia.org/wiki/Word_choice">word choice</a> and <a href="https://en.wikipedia.org/wiki/Syntax">syntax</a>, other types of bias stem from the actual content selected for inclusion in the text. In this work, we investigate the effects of <a href="https://en.wikipedia.org/wiki/Information_bias">informational bias</a> : factual content that can nevertheless be deployed to sway reader opinion. We first produce a new dataset, BASIL, of 300 <a href="https://en.wikipedia.org/wiki/Article_(publishing)">news articles</a> annotated with 1,727 bias spans and find evidence that <a href="https://en.wikipedia.org/wiki/Information_bias">informational bias</a> appears in <a href="https://en.wikipedia.org/wiki/Article_(publishing)">news articles</a> more frequently than lexical bias. We further study our <a href="https://en.wikipedia.org/wiki/Annotation">annotations</a> to observe how <a href="https://en.wikipedia.org/wiki/Information_bias">informational bias</a> surfaces in <a href="https://en.wikipedia.org/wiki/Article_(publishing)">news articles</a> by different media outlets. Lastly, a baseline model for informational bias prediction is presented by fine-tuning BERT on our labeled data, indicating the challenges of the task and future directions.</abstract>
      <url hash="1e25b7b7">D19-1664</url>
      <attachment hash="245db9bf">D19-1664.Attachment.zip</attachment>
      <doi>10.18653/v1/D19-1664</doi>
      <bibkey>fan-etal-2019-plain</bibkey>
      <pwccode url="https://github.com/marshallwhiteorg/emnlp19-media-bias" additional="false">marshallwhiteorg/emnlp19-media-bias</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/basil">BASIL</pwcdataset>
    </paper>
    <paper id="666">
      <title>Investigating Sports Commentator Bias within a Large Corpus of American Football Broadcasts<fixed-case>A</fixed-case>merican Football Broadcasts</title>
      <author><first>Jack</first><last>Merullo</last></author>
      <author><first>Luke</first><last>Yeh</last></author>
      <author><first>Abram</first><last>Handler</last></author>
      <author><first>Alvin</first><last>Grissom II</last></author>
      <author><first>Brendan</first><last>O’Connor</last></author>
      <author><first>Mohit</first><last>Iyyer</last></author>
      <pages>6355–6361</pages>
      <abstract>Sports broadcasters inject drama into <a href="https://en.wikipedia.org/wiki/Sports_commentator">play-by-play commentary</a> by building team and player narratives through <a href="https://en.wikipedia.org/wiki/Subjectivity">subjective analyses</a> and <a href="https://en.wikipedia.org/wiki/Anecdote">anecdotes</a>. Prior studies based on <a href="https://en.wikipedia.org/wiki/Small_data">small datasets</a> and manual coding show that such theatrics evince commentator bias in <a href="https://en.wikipedia.org/wiki/Broadcasting_of_sports_events">sports broadcasts</a>. To examine this phenomenon, we assemble <a href="https://en.wikipedia.org/wiki/American_football">FOOTBALL</a>, which contains 1,455 broadcast transcripts from American football games across six decades that are automatically annotated with 250 K player mentions and linked with <a href="https://en.wikipedia.org/wiki/Race_(human_categorization)">racial metadata</a>. We identify major confounding factors for researchers examining racial bias in FOOTBALL, and perform a computational analysis that supports conclusions from prior social science studies.</abstract>
      <url hash="16471977">D19-1666</url>
      <attachment hash="fcbc645f">D19-1666.Attachment.pdf</attachment>
      <doi>10.18653/v1/D19-1666</doi>
      <bibkey>merullo-etal-2019-investigating</bibkey>
      <pwccode url="https://github.com/jmerullo/football" additional="false">jmerullo/football</pwccode>
    </paper>
    <paper id="667">
      <title>Charge-Based Prison Term Prediction with Deep Gating Network</title>
      <author><first>Huajie</first><last>Chen</last></author>
      <author><first>Deng</first><last>Cai</last></author>
      <author><first>Wei</first><last>Dai</last></author>
      <author><first>Zehui</first><last>Dai</last></author>
      <author><first>Yadong</first><last>Ding</last></author>
      <pages>6362–6367</pages>
      <abstract>Judgment prediction for <a href="https://en.wikipedia.org/wiki/Legal_case">legal cases</a> has attracted much research efforts for its practice use, of which the ultimate goal is prison term prediction. While existing work merely predicts the total prison term, in reality a defendant is often charged with multiple crimes. In this paper, we argue that charge-based prison term prediction (CPTP) not only better fits realistic needs, but also makes the total prison term prediction more accurate and interpretable. We collect the first large-scale structured data for <a href="https://en.wikipedia.org/wiki/CPTP">CPTP</a> and evaluate several competitive <a href="https://en.wikipedia.org/wiki/Baseline_(configuration_management)">baselines</a>. Based on the observation that fine-grained feature selection is the key to achieving good performance, we propose the Deep Gating Network (DGN) for charge-specific feature selection and aggregation. Experiments show that DGN achieves the state-of-the-art performance.</abstract>
      <url hash="32116e43">D19-1667</url>
      <doi>10.18653/v1/D19-1667</doi>
      <bibkey>chen-etal-2019-charge</bibkey>
    </paper>
    <paper id="672">
      <title>What Matters for Neural Cross-Lingual Named Entity Recognition : An Empirical Analysis</title>
      <author><first>Xiaolei</first><last>Huang</last></author>
      <author><first>Jonathan</first><last>May</last></author>
      <author><first>Nanyun</first><last>Peng</last></author>
      <pages>6395–6401</pages>
      <abstract>Building named entity recognition (NER) models for languages that do not have much training data is a challenging task. While recent work has shown promising results on cross-lingual transfer from high-resource languages, it is unclear what knowledge is transferred. In this paper, we first propose a simple and efficient neural architecture for cross-lingual NER. Experiments show that our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> achieves competitive performance with the <a href="https://en.wikipedia.org/wiki/State_of_the_art">state-of-the-art</a>. We further explore how <a href="https://en.wikipedia.org/wiki/Transfer_learning">transfer learning</a> works for cross-lingual NER on two transferable factors : sequential order and multilingual embedding. Our results shed light on future research for improving cross-lingual NER.</abstract>
      <url hash="65627d34">D19-1672</url>
      <doi>10.18653/v1/D19-1672</doi>
      <bibkey>huang-etal-2019-matters</bibkey>
    </paper>
    <paper id="674">
      <title>Generating Natural Anagrams : Towards <a href="https://en.wikipedia.org/wiki/Language_generation">Language Generation</a> Under Hard Combinatorial Constraints</title>
      <author><first>Masaaki</first><last>Nishino</last></author>
      <author><first>Sho</first><last>Takase</last></author>
      <author><first>Tsutomu</first><last>Hirao</last></author>
      <author><first>Masaaki</first><last>Nagata</last></author>
      <pages>6408–6412</pages>
      <abstract>An <a href="https://en.wikipedia.org/wiki/Anagram">anagram</a> is a sentence or a phrase that is made by permutating the characters of an input sentence or a phrase. For example, Trims cash is an anagram of Christmas. Existing automatic anagram generation methods can find possible combinations of words form an <a href="https://en.wikipedia.org/wiki/Anagram">anagram</a>. However, they do not pay much attention to the naturalness of the generated <a href="https://en.wikipedia.org/wiki/Anagram">anagrams</a>. In this paper, we show that simple <a href="https://en.wikipedia.org/wiki/Depth-first_search">depth-first search</a> can yield natural anagrams when it is combined with modern neural language models. Human evaluation results show that the proposed <a href="https://en.wikipedia.org/wiki/Methodology">method</a> can generate significantly more natural anagrams than baseline methods.</abstract>
      <url hash="b560c9f7">D19-1674</url>
      <doi>10.18653/v1/D19-1674</doi>
      <bibkey>nishino-etal-2019-generating</bibkey>
    </paper>
    <paper id="675">
      <title>STANCY : Stance Classification Based on Consistency Cues<fixed-case>STANCY</fixed-case>: Stance Classification Based on Consistency Cues</title>
      <author><first>Kashyap</first><last>Popat</last></author>
      <author><first>Subhabrata</first><last>Mukherjee</last></author>
      <author><first>Andrew</first><last>Yates</last></author>
      <author><first>Gerhard</first><last>Weikum</last></author>
      <pages>6413–6418</pages>
      <abstract>Controversial claims are abundant in <a href="https://en.wikipedia.org/wiki/Mass_media">online media</a> and <a href="https://en.wikipedia.org/wiki/Internet_forum">discussion forums</a>. A better understanding of such claims requires analyzing them from different perspectives. Stance classification is a necessary step for inferring these <a href="https://en.wikipedia.org/wiki/Point_of_view_(philosophy)">perspectives</a> in terms of supporting or opposing the claim. In this work, we present a <a href="https://en.wikipedia.org/wiki/Neural_network">neural network model</a> for stance classification leveraging BERT representations and augmenting them with a novel consistency constraint. Experiments on the Perspectrum dataset, consisting of claims and users’ perspectives from various debate websites, demonstrate the effectiveness of our approach over state-of-the-art baselines.</abstract>
      <url hash="90d2ec99">D19-1675</url>
      <doi>10.18653/v1/D19-1675</doi>
      <bibkey>popat-etal-2019-stancy</bibkey>
    </paper>
    <paper id="676">
      <title>Cross-lingual intent classification in a low resource industrial setting</title>
      <author><first>Talaat</first><last>Khalil</last></author>
      <author><first>Kornel</first><last>Kiełczewski</last></author>
      <author><first>Georgios Christos</first><last>Chouliaras</last></author>
      <author><first>Amina</first><last>Keldibek</last></author>
      <author><first>Maarten</first><last>Versteegh</last></author>
      <pages>6419–6424</pages>
      <abstract>This paper explores different approaches to multilingual intent classification in a low resource setting. Recent advances in multilingual text representations promise cross-lingual transfer for <a href="https://en.wikipedia.org/wiki/Classifier_(linguistics)">classifiers</a>. We investigate the potential for this transfer in an applied industrial setting and compare to multilingual classification using <a href="https://en.wikipedia.org/wiki/Machine_translation">machine translated text</a>. Our results show that while the recently developed methods show promise, practical application calls for a combination of techniques for useful results.</abstract>
      <url hash="2033360d">D19-1676</url>
      <doi>10.18653/v1/D19-1676</doi>
      <bibkey>khalil-etal-2019-cross</bibkey>
    </paper>
    </volume>
  <volume id="2" ingest-date="2019-11-25">
    <meta>
      <booktitle>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP): Tutorial Abstracts</booktitle>
      <editor><first>Timothy</first><last>Baldwin</last></editor>
      <editor><first>Marine</first><last>Carpuat</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Hong Kong, China</address>
      <month>November</month>
      <year>2019</year>
    </meta>
    <paper id="1">
      <title>Dive into Deep Learning for Natural Language Processing</title>
      <author><first>Haibin</first><last>Lin</last></author>
      <author><first>Xingjian</first><last>Shi</last></author>
      <author><first>Leonard</first><last>Lausen</last></author>
      <author><first>Aston</first><last>Zhang</last></author>
      <author><first>He</first><last>He</last></author>
      <author><first>Sheng</first><last>Zha</last></author>
      <author><first>Alexander</first><last>Smola</last></author>
      <abstract>Deep learning has become the dominant approach to NLP problems, especially when applied on large scale corpora. Recent progress on unsupervised pre-training techniques such as BERT, ELMo, GPT-2, and language modeling in general, when applied on large corpora, is shown to be effective in improving a wide variety of downstream tasks. These techniques push the limits of available hardware, requiring specialized frameworks optimized for GPU, ASIC, and distributed cloud-based training.

A few complexities pose challenges to scale these models and algorithms effectively. Compared to other areas where deep learning is applied, these NLP models contain a variety of moving parts: text normalization and tokenization, word representation at subword-level and word-level, variable-length models such as RNN and attention, and sequential decoder based on beam search, among others.

In this hands-on tutorial, we take a closer look at the challenges from these complexities and see how with proper tooling with Apache MXNet and GluonNLP, we can overcome these challenges and achieve state-of-the-art results for real-world problems. GluonNLP is a powerful new toolkit that combines MXNet’s speed, the flexibility of Gluon, and an extensive new library automating the most laborious aspects of deep learning for NLP.</abstract>
      <bibkey>lin-etal-2019-dive</bibkey>
    </paper>
    <paper id="2">
      <title>Processing and Understanding Mixed Language Data</title>
      <author><first>Monojit</first><last>Choudhury</last></author>
      <author><first>Anirudh</first><last>Srinivasan</last></author>
      <author><first>Sandipan</first><last>Dandapat</last></author>
      <abstract>Multilingual communities exhibit code-mixing, that is, mixing of two or more socially stable languages in a single conversation, sometimes even in a single utterance. This phenomenon has been widely studied by linguists and interaction scientists in the spoken language of such communities. However, with the prevalence of social media and other informal interactive platforms, code-switching is now also ubiquitously observed in user-generated text. As multilingual communities are more the norm from a global perspective, it becomes essential that code-switched text and speech are adequately handled by language technologies and NUIs.

Code-mixing is extremely prevalent in all multilingual societies. Current studies have shown that as much as 20% of user generated content from some geographies, like South Asia, parts of Europe, and Singapore, are code-mixed. Thus, it is very important to handle code-mixed content as a part of NLP systems and applications for these geographies.

In the past 5 years, there has been an active interest in computational models for code-mixing with a substantive research outcome in terms of publications, datasets and systems. However, it is not easy to find a single point of access for a complete and coherent overview of the research. This tutorial is expecting to fill this gap and provide new researchers in the area with a foundation in both linguistic and computational aspects of code-mixing. We hope that this then becomes a starting point for those who wish to pursue research, design, development and deployment of code-mixed systems in multilingual societies.</abstract>
      <bibkey>choudhury-etal-2019-processing</bibkey>
    </paper>
    <paper id="3">
      <title>Data Collection and End-to-End Learning for Conversational <fixed-case>AI</fixed-case></title>
      <author><first>Tsung-Hsien</first><last>Wen</last></author>
      <author><first>Pei-Hao</first><last>Su</last></author>
      <author><first>Paweł</first><last>Budzianowski</last></author>
      <author><first>Iñigo</first><last>Casanueva</last></author>
      <author><first>Ivan</first><last>Vulić</last></author>
      <abstract>A fundamental long-term goal of conversational AI is to merge two main dialogue system paradigms into a standalone multi-purpose system. Such a system should be capable of conversing about arbitrary topics (Paradigm 1: open-domain dialogue systems), and simultaneously assist humans with completing a wide range of tasks with well-defined semantics such as restaurant search and booking, customer service applications, or ticket bookings (Paradigm 2: task-based dialogue systems).

The recent developmental leaps in conversational AI technology are undoubtedly linked to more and more sophisticated deep learning algorithms that capture patterns in increasing amounts of data generated by various data collection mechanisms. The goal of this tutorial is therefore twofold. First, it aims at familiarising the research community with the recent advances in algorithmic design of statistical dialogue systems for both open-domain and task-based dialogue paradigms. The focus of the tutorial is on recently introduced end-to-end learning for dialogue systems and their relation to more common modular systems. In theory, learning end-to-end from data offers seamless and unprecedented portability of dialogue systems to a wide spectrum of tasks and languages. From a practical point of view, there are still plenty of research challenges and opportunities remaining: in this tutorial we analyse this gap between theory and practice, and introduce the research community with the main advantages as well as with key practical limitations of current end-to-end dialogue learning.

The critical requirement of each statistical dialogue system is the data at hand. The system cannot provide assistance for the task without having appropriate task-related data to learn from. Therefore, the second major goal of this tutorial is to provide a comprehensive overview of the current approaches to data collection for dialogue, and analyse the current gaps and challenges with diverse data collection protocols, as well as their relation to and current limitations of data-driven end-to-end dialogue modeling. We will again analyse this relation and limitations both from research and industry perspective, and provide key insights on the application of state-of-the-art methodology into industry-scale conversational AI systems.</abstract>
      <bibkey>wen-etal-2019-data</bibkey>
    </paper>
    <paper id="4">
      <title>Bias and Fairness in Natural Language Processing</title>
      <author><first>Kai-Wei</first><last>Chang</last></author>
      <author><first>Vinodkumar</first><last>Prabhakaran</last></author>
      <author><first>Vicente</first><last>Ordonez</last></author>
      <abstract>Recent advances in data-driven machine learning techniques (e.g., deep neural networks) have revolutionized many natural language processing applications. These approaches automatically learn how to make decisions based on the statistics and diagnostic information from large amounts of training data. Despite the remarkable accuracy of machine learning in various applications, learning algorithms run the risk of relying on societal biases encoded in the training data to make predictions. This often occurs even when gender and ethnicity information is not explicitly provided to the system because learning algorithms are able to discover implicit associations between individuals and their demographic information based on other variables such as names, titles, home addresses, etc. Therefore, machine learning algorithms risk potentially encouraging unfair and discriminatory decision making and raise serious privacy concerns. Without properly quantifying and reducing the reliance on such correlations, broad adoption of these models might have the undesirable effect of magnifying harmful stereotypes or implicit biases that rely on sensitive demographic attributes.

In this tutorial, we will review the history of bias and fairness studies in machine learning and language processing and present recent community effort in quantifying and mitigating bias in natural language processing models for a wide spectrum of tasks, including word embeddings, co-reference resolution, machine translation, and vision-and-language tasks. In particular, we will focus on the following topics:

+ Definitions of fairness and bias.

+ Data, algorithms, and models that propagate and even amplify social bias to NLP applications and metrics to quantify these biases.

+ Algorithmic solutions; learning objective; design principles to prevent social bias in NLP systems and their potential drawbacks.

The tutorial will bring researchers and practitioners to be aware of this issue, and encourage the research community to propose innovative solutions to promote fairness in NLP.</abstract>
      <bibkey>chang-etal-2019-bias</bibkey>
    </paper>
    <paper id="5">
      <title>Discreteness in Neural Natural Language Processing</title>
      <author><first>Lili</first><last>Mou</last></author>
      <author><first>Hao</first><last>Zhou</last></author>
      <author><first>Lei</first><last>Li</last></author>
      <abstract>This tutorial provides a comprehensive guide to the process of discreteness in neural NLP.

As a gentle start, we will briefly introduce the background of deep learning based NLP, where we point out the ubiquitous discreteness of natural language and its challenges in neural information processing. Particularly, we will focus on how such discreteness plays a role in the input space, the latent space, and the output space of a neural network. In each part, we will provide examples, discuss machine learning techniques, as well as demonstrate NLP applications.</abstract>
      <bibkey>mou-etal-2019-discreteness</bibkey>
    </paper>
    <paper id="6">
      <title>Graph-based Deep Learning in Natural Language Processing</title>
      <author><first>Shikhar</first><last>Vashishth</last></author>
      <author><first>Naganand</first><last>Yadati</last></author>
      <author><first>Partha</first><last>Talukdar</last></author>
      <abstract>This tutorial aims to introduce recent advances in graph-based deep learning techniques such as Graph Convolutional Networks (GCNs) for Natural Language Processing (NLP). It provides a brief introduction to deep learning methods on non-Euclidean domains such as graphs and justifies their relevance in NLP. It then covers recent advances in applying graph-based deep learning methods for various NLP tasks, such as semantic role labeling, machine translation, relationship extraction, and many more.</abstract>
      <bibkey>vashishth-etal-2019-graph</bibkey>
    </paper>
    <paper id="7">
      <title>Semantic Specialization of Distributional Word Vectors</title>
      <author><first>Goran</first><last>Glavaś</last></author>
      <author><first>Edoardo</first><last>Maria Ponti</last></author>
      <author><first>Ivan</first><last>Vulić</last></author>
      <abstract>Distributional word vectors have become an indispensable component of most state-of-art NLP models. As a major artefact of the underlying distributional hypothesis, distributional word vector spaces conflate various paradigmatic and syntagmatic lexico-semantic relations. For example, relations such as synonymy/similarity (e.g., car-automobile) or lexical entailment (e.g., car-vehicle) often cannot be distinguished from antonymy (e.g., black-white), meronymy (e.g., car-wheel) or broader thematic relatedness (e.g., car-driver) based on the distances in the distributional vector space. This inherent property of distributional spaces often harms performance in downstream applications, since different lexico-semantic relations support different classes of NLP applications. For instance, Semantic Similarity provides guidance for Paraphrasing, Dialogue State Tracking, and Text Simplification, Lexical Entailment supports Natural Language Inference and Taxonomy Induction, whereas broader thematic relatedness yields gains for Named Entity Recognition, Parsing, and Text Classification and Retrieval.

A plethora of methods have been proposed to emphasize specific lexico-semantic relations in a reshaped (i.e., specialized) vector space. A common solution is to move beyond purely unsupervised word representation learning and include external lexico-semantic knowledge, in a process commonly referred to as semantic specialization. In this tutorial, we provide a thorough overview of specialization methods, covering: 1) joint specialization methods, which augment distributional learning objectives with external linguistic constraints, 2) post-processing retrofitting models, which fine-tune pre-trained distributional vectors to better reflect external linguistic constraints, and 3) the most recently proposed post-specialization methods that generalize the perturbations of the post-processing methods to the whole distributional space. In addition to providing a comprehensive overview of specialization methods, we will introduce the most recent developments, such as (among others): handling asymmetric relations (e.g., hypernymy-hyponymy) in Euclidean and hyperbolic spaces by accounting for vector magnitude as well as for vector distance; cross-lingual transfer of semantic specialization for languages without external lexico-semantic resources; downstream effects of specializing distributional vector spaces; injecting external knowledge into unsupervised pretraining architectures such as ELMo or BERT.</abstract>
      <bibkey>glavas-etal-2019-semantic</bibkey>
    </paper>
  </volume>
  <volume id="3" ingest-date="2019-11-03">
    <meta>
      <booktitle>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP): System Demonstrations</booktitle>
      <url hash="e8f8d220">D19-3</url>
      <editor><first>Sebastian</first><last>Padó</last></editor>
      <editor><first>Ruihong</first><last>Huang</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Hong Kong, China</address>
      <month>November</month>
      <year>2019</year>
    </meta>
    <frontmatter>
      <url hash="ffebe534">D19-3000</url>
      <bibkey>emnlp-2019-2019-empirical</bibkey>
    </frontmatter>
    <paper id="3">
      <title>ALTER : Auxiliary Text Rewriting Tool for Natural Language Generation<fixed-case>ALTER</fixed-case>: Auxiliary Text Rewriting Tool for Natural Language Generation</title>
      <author><first>Qiongkai</first><last>Xu</last></author>
      <author><first>Chenchen</first><last>Xu</last></author>
      <author><first>Lizhen</first><last>Qu</last></author>
      <pages>13–18</pages>
      <abstract>In this paper, we describe ALTER, an auxiliary text rewriting tool that facilitates the rewriting process for natural language generation tasks, such as <a href="https://en.wikipedia.org/wiki/Paraphrasing">paraphrasing</a>, text simplification, fairness-aware text rewriting, and text style transfer. Our tool is characterized by two features, i) recording of word-level revision histories and ii) flexible auxiliary edit support and feedback to annotators. The text rewriting assist and traceable rewriting history are potentially beneficial to the future research of <a href="https://en.wikipedia.org/wiki/Natural-language_generation">natural language generation</a>.</abstract>
      <url hash="467db22c">D19-3003</url>
      <doi>10.18653/v1/D19-3003</doi>
      <bibkey>xu-etal-2019-alter</bibkey>
      <pwccode url="https://github.com/xuqiongkai/ALTER" additional="false">xuqiongkai/ALTER</pwccode>
    </paper>
    <paper id="5">
      <title>Automatic Taxonomy Induction and Expansion</title>
      <author><first>Nicolas Rodolfo</first><last>Fauceglia</last></author>
      <author><first>Alfio</first><last>Gliozzo</last></author>
      <author><first>Sarthak</first><last>Dash</last></author>
      <author><first>Md. Faisal Mahbub</first><last>Chowdhury</last></author>
      <author><first>Nandana</first><last>Mihindukulasooriya</last></author>
      <pages>25–30</pages>
      <abstract>The Knowledge Graph Induction Service (KGIS) is an end-to-end knowledge induction system. One of its main capabilities is to automatically induce <a href="https://en.wikipedia.org/wiki/Taxonomy_(general)">taxonomies</a> from input documents using a hybrid approach that takes advantage of linguistic patterns, <a href="https://en.wikipedia.org/wiki/Semantic_Web">semantic web</a> and <a href="https://en.wikipedia.org/wiki/Neural_network">neural networks</a>. KGIS allows the user to semi-automatically curate and expand the induced taxonomy through a component called Smart SpreadSheet by exploiting <a href="https://en.wikipedia.org/wiki/Distributional_semantics">distributional semantics</a>. In this paper, we describe these taxonomy induction and expansion features of <a href="https://en.wikipedia.org/wiki/Geographic_information_system">KGIS</a>. A screencast video demonstrating the system is available in https://ibm.box.com/v/emnlp-2019-demo.</abstract>
      <url hash="de2a20af">D19-3005</url>
      <doi>10.18653/v1/D19-3005</doi>
      <bibkey>fauceglia-etal-2019-automatic</bibkey>
    </paper>
    <paper id="10">
      <title>EGG : a toolkit for research on Emergence of lanGuage in Games<fixed-case>EGG</fixed-case>: a toolkit for research on Emergence of lan<fixed-case>G</fixed-case>uage in Games</title>
      <author><first>Eugene</first><last>Kharitonov</last></author>
      <author><first>Rahma</first><last>Chaabouni</last></author>
      <author><first>Diane</first><last>Bouchacourt</last></author>
      <author><first>Marco</first><last>Baroni</last></author>
      <pages>55–60</pages>
      <abstract>There is renewed interest in simulating <a href="https://en.wikipedia.org/wiki/Language_emergence">language emergence</a> among <a href="https://en.wikipedia.org/wiki/Deep_learning">deep neural agents</a> that communicate to jointly solve a task, spurred by the practical aim to develop language-enabled interactive AIs, as well as by theoretical questions about the <a href="https://en.wikipedia.org/wiki/Evolution_of_language">evolution of human language</a>. However, optimizing deep architectures connected by a <a href="https://en.wikipedia.org/wiki/Communication_channel">discrete communication channel</a> (such as that in which <a href="https://en.wikipedia.org/wiki/Language">language</a> emerges) is technically challenging. We introduce EGG, a <a href="https://en.wikipedia.org/wiki/List_of_toolkits">toolkit</a> that greatly simplifies the implementation of emergent-language communication games. EGG’s modular design provides a set of building blocks that the user can combine to create new games, easily navigating the optimization and architecture space. We hope that the tool will lower the technical barrier, and encourage researchers from various backgrounds to do original work in this exciting area.</abstract>
      <url hash="971efee0">D19-3010</url>
      <doi>10.18653/v1/D19-3010</doi>
      <bibkey>kharitonov-etal-2019-egg</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/mnist">MNIST</pwcdataset>
    </paper>
    <paper id="11">
      <title>Entity resolution for noisy ASR transcripts<fixed-case>ASR</fixed-case> transcripts</title>
      <author><first>Arushi</first><last>Raghuvanshi</last></author>
      <author><first>Vijay</first><last>Ramakrishnan</last></author>
      <author><first>Varsha</first><last>Embar</last></author>
      <author><first>Lucien</first><last>Carroll</last></author>
      <author><first>Karthik</first><last>Raghunathan</last></author>
      <pages>61–66</pages>
      <abstract>Large vocabulary domain-agnostic Automatic Speech Recognition (ASR) systems often mistranscribe domain-specific words and phrases. Since these generic ASR systems are the first component of most voice assistants in production, building Natural Language Understanding (NLU) systems that are robust to these errors can be a challenging task. In this paper, we focus on handling ASR errors in <a href="https://en.wikipedia.org/wiki/Named_entity">named entities</a>, specifically <a href="https://en.wikipedia.org/wiki/Personal_name">person names</a>, for a voice-based collaboration assistant. We demonstrate an effective method for resolving person names that are mistranscribed by black-box ASR systems, using character and phoneme-based information retrieval techniques and <a href="https://en.wikipedia.org/wiki/Context_(language_use)">contextual information</a>, which improves <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> by 40.8 % on our <a href="https://en.wikipedia.org/wiki/Production_system_(computer_science)">production system</a>. We provide a live interactive demo to further illustrate the nuances of this problem and the effectiveness of our solution.</abstract>
      <url hash="10da8c20">D19-3011</url>
      <doi>10.18653/v1/D19-3011</doi>
      <bibkey>raghuvanshi-etal-2019-entity</bibkey>
    </paper>
    <paper id="12">
      <title>EUSP : An Easy-to-Use Semantic Parsing PlatForm<fixed-case>EUSP</fixed-case>: An Easy-to-Use Semantic Parsing <fixed-case>P</fixed-case>lat<fixed-case>F</fixed-case>orm</title>
      <author><first>Bo</first><last>An</last></author>
      <author><first>Chen</first><last>Bo</last></author>
      <author><first>Xianpei</first><last>Han</last></author>
      <author><first>Le</first><last>Sun</last></author>
      <pages>67–72</pages>
      <abstract>Semantic parsing aims to map <a href="https://en.wikipedia.org/wiki/Utterance">natural language utterances</a> into structured meaning representations. We present a modular platform, EUSP (Easy-to-Use Semantic Parsing PlatForm), that facilitates developers to build semantic parser from scratch. Instead of requiring a large amount of training data or complex grammar knowledge, in our platform developers can build grammar-based semantic parser or neural-based semantic parser through configure files which specify the modules and components that compose semantic parsing system. A high quality grammar-based semantic parsing system only requires domain lexicons rather than costly training data for a <a href="https://en.wikipedia.org/wiki/Semantic_parser">semantic parser</a>. Furthermore, we provide a browser-based method to generate the semantic parsing system to minimize the difficulty of development. Experimental results show that the neural-based semantic parser system achieves competitive performance on semantic parsing task, and grammar-based semantic parsers significantly improve the performance of a business search engine.</abstract>
      <url hash="3385eb65">D19-3012</url>
      <doi>10.18653/v1/D19-3012</doi>
      <bibkey>an-etal-2019-eusp</bibkey>
    </paper>
    <paper id="15">
      <title>HARE : a Flexible Highlighting Annotator for Ranking and Exploration<fixed-case>HARE</fixed-case>: a Flexible Highlighting Annotator for Ranking and Exploration</title>
      <author><first>Denis</first><last>Newman-Griffis</last></author>
      <author><first>Eric</first><last>Fosler-Lussier</last></author>
      <pages>85–90</pages>
      <abstract>Exploration and analysis of potential data sources is a significant challenge in the application of NLP techniques to novel information domains. We describe HARE, a system for highlighting relevant information in document collections to support <a href="https://en.wikipedia.org/wiki/Ranking">ranking</a> and <a href="https://en.wikipedia.org/wiki/Triage">triage</a>, which provides tools for post-processing and qualitative analysis for model development and tuning. We apply HARE to the use case of narrative descriptions of mobility information in clinical data, and demonstrate its utility in comparing candidate embedding features. We provide a web-based interface for annotation visualization and document ranking, with a modular backend to support interoperability with existing annotation tools. Our system is available online at https://github.com/OSU-slatelab/HARE.</abstract>
      <url hash="cb70474c">D19-3015</url>
      <doi>10.18653/v1/D19-3015</doi>
      <bibkey>newman-griffis-fosler-lussier-2019-hare</bibkey>
      <pwccode url="https://github.com/OSU-slatelab/HARE" additional="false">OSU-slatelab/HARE</pwccode>
    </paper>
    <paper id="18">
      <title>INMT : Interactive Neural Machine Translation Prediction<fixed-case>INMT</fixed-case>: Interactive Neural Machine Translation Prediction</title>
      <author><first>Sebastin</first><last>Santy</last></author>
      <author><first>Sandipan</first><last>Dandapat</last></author>
      <author><first>Monojit</first><last>Choudhury</last></author>
      <author><first>Kalika</first><last>Bali</last></author>
      <pages>103–108</pages>
      <abstract>In this paper, we demonstrate an Interactive Machine Translation interface, that assists human translators with on-the-fly hints and suggestions. This makes the end-to-end translation process faster, more efficient and creates high-quality translations. We augment the OpenNMT backend with a mechanism to accept the user input and generate conditioned translations.</abstract>
      <url hash="1b0ca490">D19-3018</url>
      <doi>10.18653/v1/D19-3018</doi>
      <bibkey>santy-etal-2019-inmt</bibkey>
      <pwccode url="https://github.com/microsoft/inmt" additional="false">microsoft/inmt</pwccode>
    </paper>
    <paper id="20">
      <title>Journalist-in-the-Loop : Continuous Learning as a Service for Rumour Analysis</title>
      <author><first>Twin</first><last>Karmakharm</last></author>
      <author><first>Nikolaos</first><last>Aletras</last></author>
      <author><first>Kalina</first><last>Bontcheva</last></author>
      <pages>115–120</pages>
      <abstract>Automatically identifying <a href="https://en.wikipedia.org/wiki/Rumor">rumours</a> in <a href="https://en.wikipedia.org/wiki/Social_media">social media</a> and assessing their veracity is an important task with downstream applications in <a href="https://en.wikipedia.org/wiki/Journalism">journalism</a>. A significant challenge is how to keep rumour analysis tools up-to-date as new information becomes available for particular <a href="https://en.wikipedia.org/wiki/Rumor">rumours</a> that spread in a <a href="https://en.wikipedia.org/wiki/Social_network">social network</a>. This paper presents a novel open-source web-based rumour analysis tool that can continuous learn from journalists. The system features a rumour annotation service that allows journalists to easily provide feedback for a given social media post through a <a href="https://en.wikipedia.org/wiki/Web_application">web-based interface</a>. The feedback allows the <a href="https://en.wikipedia.org/wiki/System">system</a> to improve an underlying state-of-the-art neural network-based rumour classification model. The <a href="https://en.wikipedia.org/wiki/System">system</a> can be easily integrated as a service into existing tools and platforms used by journalists using a <a href="https://en.wikipedia.org/wiki/Representational_state_transfer">REST API</a>.</abstract>
      <url hash="8dd18a3a">D19-3020</url>
      <doi>10.18653/v1/D19-3020</doi>
      <bibkey>karmakharm-etal-2019-journalist</bibkey>
    </paper>
    <paper id="21">
      <title>LIDA : Lightweight Interactive Dialogue Annotator<fixed-case>LIDA</fixed-case>: Lightweight Interactive Dialogue Annotator</title>
      <author><first>Edward</first><last>Collins</last></author>
      <author><first>Nikolai</first><last>Rozanov</last></author>
      <author><first>Bingbing</first><last>Zhang</last></author>
      <pages>121–126</pages>
      <abstract>Dialogue systems have the potential to change how people interact with machines but are highly dependent on the quality of the data used to train them. It is therefore important to develop good dialogue annotation tools which can improve the speed and quality of dialogue data annotation. With this in mind, we introduce LIDA, an <a href="https://en.wikipedia.org/wiki/Annotation">annotation tool</a> designed specifically for conversation data. As far as we know, LIDA is the first dialogue annotation system that handles the entire dialogue annotation pipeline from raw text, as may be the output of transcription services, to structured conversation data. Furthermore it supports the integration of arbitrary machine learning mod-els as annotation recommenders and also has a dedicated interface to resolve inter-annotator disagreements such as after crowdsourcing an-notations for a dataset. LIDA is fully open source, documented and publicly available. [ https://github.com/Wluper/lida ]   Screen Cast : https://vimeo.com/329824847</abstract>
      <url hash="9571b7b3">D19-3021</url>
      <doi>10.18653/v1/D19-3021</doi>
      <bibkey>collins-etal-2019-lida</bibkey>
      <pwccode url="https://github.com/Wluper/lida" additional="false">Wluper/lida</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/multiwoz">MultiWOZ</pwcdataset>
    </paper>
    <paper id="22">
      <title>LINSPECTOR WEB : A Multilingual Probing Suite for Word Representations<fixed-case>LINSPECTOR</fixed-case> <fixed-case>WEB</fixed-case>: A Multilingual Probing Suite for Word Representations</title>
      <author><first>Max</first><last>Eichler</last></author>
      <author><first>Gözde Gül</first><last>Şahin</last></author>
      <author><first>Iryna</first><last>Gurevych</last></author>
      <pages>127–132</pages>
      <abstract>We present LINSPECTOR WEB, an open source multilingual inspector to analyze word representations. Our system provides researchers working in low-resource settings with an easily accessible web based probing tool to gain quick insights into their <a href="https://en.wikipedia.org/wiki/Word_embedding">word embeddings</a> especially outside of the <a href="https://en.wikipedia.org/wiki/English_language">English language</a>. To do this we employ 16 simple linguistic probing tasks such as <a href="https://en.wikipedia.org/wiki/Grammatical_gender">gender</a>, <a href="https://en.wikipedia.org/wiki/Grammatical_case">case marking</a>, and <a href="https://en.wikipedia.org/wiki/Grammatical_tense">tense</a> for a diverse set of 28 languages. We support probing of static word embeddings along with pretrained AllenNLP models that are commonly used for NLP downstream tasks such as <a href="https://en.wikipedia.org/wiki/Named-entity_recognition">named entity recognition</a>, natural language inference and dependency parsing. The results are visualized in a <a href="https://en.wikipedia.org/wiki/Polar_chart">polar chart</a> and also provided as a table. LINSPECTOR WEB is available as an offline tool or at https://linspector.ukp.informatik.tu-darmstadt.de.</abstract>
      <url hash="8b702720">D19-3022</url>
      <doi>10.18653/v1/D19-3022</doi>
      <bibkey>eichler-etal-2019-linspector</bibkey>
      <pwccode url="https://github.com/UKPLab/linspector-web" additional="false">UKPLab/linspector-web</pwccode>
    </paper>
    <paper id="25">
      <title>Memory Grounded Conversational Reasoning</title>
      <author><first>Seungwhan</first><last>Moon</last></author>
      <author><first>Pararth</first><last>Shah</last></author>
      <author><first>Rajen</first><last>Subba</last></author>
      <author><first>Anuj</first><last>Kumar</last></author>
      <pages>145–150</pages>
      <abstract>We demonstrate a conversational system which engages the user through a multi-modal, multi-turn dialog over the user’s memories. The <a href="https://en.wikipedia.org/wiki/System">system</a> can perform QA over memories by responding to <a href="https://en.wikipedia.org/wiki/User_(computing)">user queries</a> to recall specific attributes and associated media (e.g. photos) of past episodic memories. The <a href="https://en.wikipedia.org/wiki/System">system</a> can also make proactive suggestions to surface related events or facts from past memories to make conversations more engaging and natural. To implement such a system, we collect a new corpus of memory grounded conversations, which comprises human-to-human role-playing dialogs given synthetic memory graphs with simulated attributes. Our proof-of-concept system operates on these synthetic memory graphs, however it can be trained and applied to real-world user memory data (e.g. photo albums, etc.) We present the architecture of the proposed conversational system, and example queries that the <a href="https://en.wikipedia.org/wiki/System">system</a> supports.</abstract>
      <url hash="da0a7a6e">D19-3025</url>
      <doi>10.18653/v1/D19-3025</doi>
      <bibkey>moon-etal-2019-memory-grounded</bibkey>
    </paper>
    <paper id="27">
      <title>MY-AKKHARA : A Romanization-based Burmese (Myanmar) Input Method<fixed-case>MY</fixed-case>-<fixed-case>AKKHARA</fixed-case>: A <fixed-case>R</fixed-case>omanization-based <fixed-case>B</fixed-case>urmese (<fixed-case>M</fixed-case>yanmar) Input Method</title>
      <author><first>Chenchen</first><last>Ding</last></author>
      <author><first>Masao</first><last>Utiyama</last></author>
      <author><first>Eiichiro</first><last>Sumita</last></author>
      <pages>157–162</pages>
      <abstract>MY-AKKHARA is a method used to input <a href="https://en.wikipedia.org/wiki/Burmese_language">Burmese texts</a> encoded in the <a href="https://en.wikipedia.org/wiki/Unicode">Unicode standard</a>, based on commonly accepted <a href="https://en.wikipedia.org/wiki/Latin_script">Latin transcription</a>. By using this <a href="https://en.wikipedia.org/wiki/Method_(computer_programming)">method</a>, arbitrary Burmese strings can be accurately inputted with 26 lowercase Latin letters. Meanwhile, the 26 uppercase Latin letters are designed as shortcuts of lowercase letter sequences. The frequency of Burmese characters is considered in MY-AKKHARA to realize an efficient keystroke distribution on a <a href="https://en.wikipedia.org/wiki/QWERTY">QWERTY keyboard</a>. Given that the <a href="https://en.wikipedia.org/wiki/Unicode">Unicode standard</a> has not been extensively used in digitization of Burmese, we hope that MY-AKKHARA can contribute to the widespread use of <a href="https://en.wikipedia.org/wiki/Unicode">Unicode</a> in Myanmar and can provide a platform for smart input methods for <a href="https://en.wikipedia.org/wiki/Burmese_language">Burmese</a> in the future. An implementation of MY-AKKHARA running in <a href="https://en.wikipedia.org/wiki/Microsoft_Windows">Windows</a> is released at http://www2.nict.go.jp/astrec-att/member/ding/my-akkhara.html</abstract>
      <url hash="02dd073d">D19-3027</url>
      <doi>10.18653/v1/D19-3027</doi>
      <bibkey>ding-etal-2019-akkhara</bibkey>
    </paper>
    <paper id="31">
      <title>PolyResponse : A Rank-based Approach to Task-Oriented Dialogue with Application in Restaurant Search and Booking<fixed-case>P</fixed-case>oly<fixed-case>R</fixed-case>esponse: A Rank-based Approach to Task-Oriented Dialogue with Application in Restaurant Search and Booking</title>
      <author><first>Matthew</first><last>Henderson</last></author>
      <author><first>Ivan</first><last>Vulić</last></author>
      <author><first>Iñigo</first><last>Casanueva</last></author>
      <author><first>Paweł</first><last>Budzianowski</last></author>
      <author><first>Daniela</first><last>Gerz</last></author>
      <author><first>Sam</first><last>Coope</last></author>
      <author><first>Georgios</first><last>Spithourakis</last></author>
      <author><first>Tsung-Hsien</first><last>Wen</last></author>
      <author><first>Nikola</first><last>Mrkšić</last></author>
      <author><first>Pei-Hao</first><last>Su</last></author>
      <pages>181–186</pages>
      <abstract>We present PolyResponse, a conversational search engine that supports task-oriented dialogue. It is a retrieval-based approach that bypasses the complex multi-component design of traditional task-oriented dialogue systems and the use of explicit semantics in the form of task-specific ontologies. The PolyResponse engine is trained on hundreds of millions of examples extracted from real conversations : it learns what responses are appropriate in different conversational contexts. It then ranks a large index of text and visual responses according to their similarity to the given context, and narrows down the list of relevant entities during the multi-turn conversation. We introduce a restaurant search and booking system powered by the PolyResponse engine, currently available in 8 different languages.</abstract>
      <url hash="533e5ea0">D19-3031</url>
      <doi>10.18653/v1/D19-3031</doi>
      <bibkey>henderson-etal-2019-polyresponse</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/imagenet">ImageNet</pwcdataset>
    </paper>
    <paper id="32">
      <title>PyOpenDial : A Python-based Domain-Independent Toolkit for Developing Spoken Dialogue Systems with Probabilistic Rules<fixed-case>P</fixed-case>y<fixed-case>O</fixed-case>pen<fixed-case>D</fixed-case>ial: A Python-based Domain-Independent Toolkit for Developing Spoken Dialogue Systems with Probabilistic Rules</title>
      <author><first>Youngsoo</first><last>Jang</last></author>
      <author><first>Jongmin</first><last>Lee</last></author>
      <author><first>Jaeyoung</first><last>Park</last></author>
      <author><first>Kyeng-Hun</first><last>Lee</last></author>
      <author><first>Pierre</first><last>Lison</last></author>
      <author><first>Kee-Eung</first><last>Kim</last></author>
      <pages>187–192</pages>
      <abstract>We present PyOpenDial, a Python-based domain-independent, open-source toolkit for spoken dialogue systems. Recent advances in core components of dialogue systems, such as <a href="https://en.wikipedia.org/wiki/Speech_recognition">speech recognition</a>, <a href="https://en.wikipedia.org/wiki/Natural-language_understanding">language understanding</a>, <a href="https://en.wikipedia.org/wiki/Dialogue_management">dialogue management</a>, and <a href="https://en.wikipedia.org/wiki/Natural-language_generation">language generation</a>, harness <a href="https://en.wikipedia.org/wiki/Deep_learning">deep learning</a> to achieve state-of-the-art performance. The original OpenDial, implemented in <a href="https://en.wikipedia.org/wiki/Java_(programming_language)">Java</a>, provides a plugin architecture to integrate external modules, but lacks Python bindings, making it difficult to interface with popular deep learning frameworks such as <a href="https://en.wikipedia.org/wiki/Tensorflow">Tensorflow</a> or <a href="https://en.wikipedia.org/wiki/PyTorch">PyTorch</a>. To this end, we re-implemented OpenDial in <a href="https://en.wikipedia.org/wiki/Python_(programming_language)">Python</a> and extended the toolkit with a number of novel functionalities for neural dialogue state tracking and action planning. We describe the overall <a href="https://en.wikipedia.org/wiki/Systems_architecture">architecture</a> and its extensions, and illustrate their use on an example where the system response model is implemented with a <a href="https://en.wikipedia.org/wiki/Recurrent_neural_network">recurrent neural network</a>.</abstract>
      <url hash="39384c01">D19-3032</url>
      <doi>10.18653/v1/D19-3032</doi>
      <bibkey>jang-etal-2019-pyopendial</bibkey>
    </paper>
    <paper id="34">
      <title>SEAGLE : A Platform for Comparative Evaluation of Semantic Encoders for Information Retrieval<fixed-case>SEAGLE</fixed-case>: A Platform for Comparative Evaluation of Semantic Encoders for Information Retrieval</title>
      <author><first>Fabian David</first><last>Schmidt</last></author>
      <author><first>Markus</first><last>Dietsche</last></author>
      <author><first>Simone Paolo</first><last>Ponzetto</last></author>
      <author><first>Goran</first><last>Glavaš</last></author>
      <pages>199–204</pages>
      <abstract>We introduce Seagle, a platform for comparative evaluation of semantic text encoding models on information retrieval (IR) tasks. Seagle implements (1) word embedding aggregators, which represent texts as algebraic aggregations of pretrained word embeddings and (2) pretrained semantic encoders, and allows for their comparative evaluation on arbitrary (monolingual and cross-lingual) IR collections. We benchmark Seagle’s models on monolingual document retrieval and cross-lingual sentence retrieval. Seagle functionality can be exploited via an easy-to-use web interface and its modular backend (micro-service architecture) can easily be extended with additional semantic search models.</abstract>
      <url hash="c81916b1">D19-3034</url>
      <doi>10.18653/v1/D19-3034</doi>
      <bibkey>schmidt-etal-2019-seagle</bibkey>
    </paper>
    <paper id="35">
      <title>A Stylometry Toolkit for Latin Literature<fixed-case>L</fixed-case>atin Literature</title>
      <author><first>Thomas J.</first><last>Bolt</last></author>
      <author><first>Jeffrey H.</first><last>Flynt</last></author>
      <author><first>Pramit</first><last>Chaudhuri</last></author>
      <author><first>Joseph P.</first><last>Dexter</last></author>
      <pages>205–210</pages>
      <abstract>Computational stylometry has become an increasingly important aspect of <a href="https://en.wikipedia.org/wiki/Literary_criticism">literary criticism</a>, but many humanists lack the technical expertise or language-specific NLP resources required to exploit computational methods. We demonstrate a stylometry toolkit for analysis of Latin literary texts, which is freely available at www.qcrit.org/stylometry. Our <a href="https://en.wikipedia.org/wiki/List_of_toolkits">toolkit</a> generates data for a diverse range of literary features and has an intuitive point-and-click interface. The features included have proven effective for multiple <a href="https://en.wikipedia.org/wiki/Literary_criticism">literary studies</a> and are calculated using custom <a href="https://en.wikipedia.org/wiki/Heuristic">heuristics</a> without the need for <a href="https://en.wikipedia.org/wiki/Parsing">syntactic parsing</a>. As such, the <a href="https://en.wikipedia.org/wiki/List_of_toolkits">toolkit</a> models one approach to the user-friendly generation of stylometric data, which could be extended to other premodern and non-English languages underserved by standard NLP resources.</abstract>
      <url hash="7f4838e4">D19-3035</url>
      <doi>10.18653/v1/D19-3035</doi>
      <bibkey>bolt-etal-2019-stylometry</bibkey>
    </paper>
    <paper id="36">
      <title>A <a href="https://en.wikipedia.org/wiki/Summarization">Summarization System</a> for Scientific Documents</title>
      <author><first>Shai</first><last>Erera</last></author>
      <author><first>Michal</first><last>Shmueli-Scheuer</last></author>
      <author><first>Guy</first><last>Feigenblat</last></author>
      <author><first>Ora</first><last>Peled Nakash</last></author>
      <author><first>Odellia</first><last>Boni</last></author>
      <author><first>Haggai</first><last>Roitman</last></author>
      <author><first>Doron</first><last>Cohen</last></author>
      <author><first>Bar</first><last>Weiner</last></author>
      <author><first>Yosi</first><last>Mass</last></author>
      <author><first>Or</first><last>Rivlin</last></author>
      <author><first>Guy</first><last>Lev</last></author>
      <author><first>Achiya</first><last>Jerbi</last></author>
      <author><first>Jonathan</first><last>Herzig</last></author>
      <author><first>Yufang</first><last>Hou</last></author>
      <author><first>Charles</first><last>Jochim</last></author>
      <author><first>Martin</first><last>Gleize</last></author>
      <author><first>Francesca</first><last>Bonin</last></author>
      <author><first>Francesca</first><last>Bonin</last></author>
      <author><first>David</first><last>Konopnicki</last></author>
      <pages>211–216</pages>
      <abstract>We present a novel <a href="https://en.wikipedia.org/wiki/System">system</a> providing summaries for Computer Science publications. Through a qualitative user study, we identified the most valuable scenarios for discovery, exploration and understanding of scientific documents. Based on these findings, we built a system that retrieves and summarizes scientific documents for a given information need, either in form of a free-text query or by choosing categorized values such as scientific tasks, <a href="https://en.wikipedia.org/wiki/Data_set">datasets</a> and more. Our <a href="https://en.wikipedia.org/wiki/System">system</a> ingested 270,000 papers, and its summarization module aims to generate concise yet detailed summaries. We validated our <a href="https://en.wikipedia.org/wiki/Software_development_process">approach</a> with <a href="https://en.wikipedia.org/wiki/Expert_witness">human experts</a>.</abstract>
      <url hash="f5448c7c">D19-3036</url>
      <doi>10.18653/v1/D19-3036</doi>
      <bibkey>erera-etal-2019-summarization</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/scisummnet">ScisummNet</pwcdataset>
    </paper>
    <paper id="39">
      <title>TEASPN : Framework and Protocol for Integrated Writing Assistance Environments<fixed-case>TEASPN</fixed-case>: Framework and Protocol for Integrated Writing Assistance Environments</title>
      <author><first>Masato</first><last>Hagiwara</last></author>
      <author><first>Takumi</first><last>Ito</last></author>
      <author><first>Tatsuki</first><last>Kuribayashi</last></author>
      <author><first>Jun</first><last>Suzuki</last></author>
      <author><first>Kentaro</first><last>Inui</last></author>
      <pages>229–234</pages>
      <abstract>Language technologies play a key role in assisting people with their writing. Although there has been steady progress in e.g., grammatical error correction (GEC), human writers are yet to benefit from this progress due to the high development cost of integrating with <a href="https://en.wikipedia.org/wiki/Writing_system">writing software</a>. We propose TEASPN, a <a href="https://en.wikipedia.org/wiki/Communication_protocol">protocol</a> and an open-source framework for achieving integrated writing assistance environments. The <a href="https://en.wikipedia.org/wiki/Communication_protocol">protocol</a> standardizes the way <a href="https://en.wikipedia.org/wiki/Computer-aided_software_engineering">writing software</a> communicates with servers that implement such technologies, allowing developers and researchers to integrate the latest developments in <a href="https://en.wikipedia.org/wiki/Natural_language_processing">natural language processing (NLP)</a> with low cost. As a result, users can enjoy the integrated experience in their favorite <a href="https://en.wikipedia.org/wiki/Writing_system">writing software</a>. The results from experiments with human participants show that users use a wide range of technologies and rate their writing experience favorably, allowing them to write more fluent text.</abstract>
      <url hash="a4c34a33">D19-3039</url>
      <doi>10.18653/v1/D19-3039</doi>
      <bibkey>hagiwara-etal-2019-teaspn</bibkey>
      <pwccode url="https://github.com/teaspn/teaspn-sdk" additional="false">teaspn/teaspn-sdk</pwccode>
    </paper>
    <paper id="41">
      <title>UER : An Open-Source Toolkit for Pre-training Models<fixed-case>UER</fixed-case>: An Open-Source Toolkit for Pre-training Models</title>
      <author><first>Zhe</first><last>Zhao</last></author>
      <author><first>Hui</first><last>Chen</last></author>
      <author><first>Jinbin</first><last>Zhang</last></author>
      <author><first>Xin</first><last>Zhao</last></author>
      <author><first>Tao</first><last>Liu</last></author>
      <author><first>Wei</first><last>Lu</last></author>
      <author><first>Xi</first><last>Chen</last></author>
      <author><first>Haotang</first><last>Deng</last></author>
      <author><first>Qi</first><last>Ju</last></author>
      <author><first>Xiaoyong</first><last>Du</last></author>
      <pages>241–246</pages>
      <abstract>Existing works, including <a href="https://en.wikipedia.org/wiki/ELMO">ELMO</a> and <a href="https://en.wikipedia.org/wiki/BERT">BERT</a>, have revealed the importance of pre-training for NLP tasks. While there does not exist a single pre-training model that works best in all cases, it is of necessity to develop a <a href="https://en.wikipedia.org/wiki/Software_framework">framework</a> that is able to deploy various pre-training models efficiently. For this purpose, we propose an assemble-on-demand pre-training toolkit, namely Universal Encoder Representations (UER). UER is loosely coupled, and encapsulated with <a href="https://en.wikipedia.org/wiki/Modular_programming">rich modules</a>. By assembling modules on demand, users can either reproduce a state-of-the-art pre-training model or develop a pre-training model that remains unexplored. With UER, we have built a model zoo, which contains pre-trained models based on different corpora, encoders, and targets (objectives). With proper pre-trained models, we could achieve new state-of-the-art results on a range of downstream datasets.</abstract>
      <url hash="6f62e1d1">D19-3041</url>
      <doi>10.18653/v1/D19-3041</doi>
      <bibkey>zhao-etal-2019-uer</bibkey>
      <pwccode url="https://github.com/dbiir/UER-py" additional="false">dbiir/UER-py</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
    </paper>
    </volume>
  <volume id="50" ingest-date="2019-11-03">
    <meta>
      <booktitle>Proceedings of the Second Workshop on Natural Language Processing for Internet Freedom: Censorship, Disinformation, and Propaganda</booktitle>
      <url hash="f700292a">D19-50</url>
      <editor><first>Anna</first><last>Feldman</last></editor>
      <editor><first>Giovanni</first><last>Da San Martino</last></editor>
      <editor><first>Alberto</first><last>Barrón-Cedeño</last></editor>
      <editor><first>Chris</first><last>Brew</last></editor>
      <editor><first>Chris</first><last>Leberknight</last></editor>
      <editor><first>Preslav</first><last>Nakov</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Hong Kong, China</address>
      <month>November</month>
      <year>2019</year>
    </meta>
    <frontmatter>
      <url hash="1b1d9516">D19-5000</url>
      <bibkey>emnlp-2019-natural</bibkey>
    </frontmatter>
    <paper id="2">
      <title>Detecting context abusiveness using hierarchical deep learning</title>
      <author><first>Ju-Hyoung</first><last>Lee</last></author>
      <author><first>Jun-U</first><last>Park</last></author>
      <author><first>Jeong-Won</first><last>Cha</last></author>
      <author><first>Yo-Sub</first><last>Han</last></author>
      <pages>10–19</pages>
      <abstract>Abusive text is a serious problem in <a href="https://en.wikipedia.org/wiki/Social_media">social media</a> and causes many issues among users as the number of users and the content volume increase. There are several attempts for detecting or preventing abusive text effectively. One simple yet effective approach is to use an abusive lexicon and determine the existence of an abusive word in text. This approach works well even when an abusive word is obfuscated. On the other hand, it is still a challenging problem to determine <a href="https://en.wikipedia.org/wiki/Abusive_power_and_control">abusiveness</a> in a text having no explicit abusive words. Especially, it is hard to identify sarcasm or offensiveness in context without any <a href="https://en.wikipedia.org/wiki/Abuse">abusive words</a>. We tackle this problem using an ensemble deep learning model. Our model consists of two parts of extracting local features and global features, which are crucial for identifying implicit abusiveness in <a href="https://en.wikipedia.org/wiki/Context_(language_use)">context level</a>. We evaluate our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> using three <a href="https://en.wikipedia.org/wiki/Benchmark_(computing)">benchmark data</a>. Our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> outperforms all the previous models for detecting <a href="https://en.wikipedia.org/wiki/Abusive_power_and_control">abusiveness</a> in a <a href="https://en.wikipedia.org/wiki/Text_(literary_theory)">text data</a> without abusive words. Furthermore, we combine our <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a> and an abusive lexicon method. The experimental results show that our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> has at least 4 % better performance compared with the previous approaches for identifying text abusiveness in case of with / without abusive words.</abstract>
      <url hash="b43be10a">D19-5002</url>
      <doi>10.18653/v1/D19-5002</doi>
      <bibkey>lee-etal-2019-detecting</bibkey>
    </paper>
    <paper id="4">
      <title>Identifying Nuances in <a href="https://en.wikipedia.org/wiki/Fake_news">Fake News</a> vs. <a href="https://en.wikipedia.org/wiki/Satire">Satire</a> : Using Semantic and Linguistic Cues</title>
      <author><first>Or</first><last>Levi</last></author>
      <author><first>Pedram</first><last>Hosseini</last></author>
      <author><first>Mona</first><last>Diab</last></author>
      <author><first>David</first><last>Broniatowski</last></author>
      <pages>31–35</pages>
      <abstract>The blurry line between nefarious fake news and protected-speech satire has been a notorious struggle for <a href="https://en.wikipedia.org/wiki/Social_media">social media platforms</a>. Further to the efforts of reducing exposure to <a href="https://en.wikipedia.org/wiki/Misinformation">misinformation</a> on <a href="https://en.wikipedia.org/wiki/Social_media">social media</a>, purveyors of <a href="https://en.wikipedia.org/wiki/Fake_news">fake news</a> have begun to masquerade as satire sites to avoid being demoted. In this work, we address the challenge of automatically classifying fake news versus <a href="https://en.wikipedia.org/wiki/Satire">satire</a>. Previous work have studied whether <a href="https://en.wikipedia.org/wiki/Fake_news">fake news</a> and <a href="https://en.wikipedia.org/wiki/Satire">satire</a> can be distinguished based on <a href="https://en.wikipedia.org/wiki/Language">language differences</a>. Contrary to <a href="https://en.wikipedia.org/wiki/Fake_news">fake news</a>, <a href="https://en.wikipedia.org/wiki/Satire">satire stories</a> are usually humorous and carry some political or social message. We hypothesize that these nuances could be identified using semantic and linguistic cues. Consequently, we train a machine learning method using semantic representation, with a state-of-the-art contextual language model, and with <a href="https://en.wikipedia.org/wiki/Feature_(linguistics)">linguistic features</a> based on textual coherence metrics. Empirical evaluation attests to the merits of our approach compared to the language-based baseline and sheds light on the nuances between <a href="https://en.wikipedia.org/wiki/Fake_news">fake news</a> and <a href="https://en.wikipedia.org/wiki/Satire">satire</a>. As avenues for future work, we consider studying additional linguistic features related to the humor aspect, and enriching the <a href="https://en.wikipedia.org/wiki/Data">data</a> with current news events, to help identify a political or social message.</abstract>
      <url hash="e09a9492">D19-5004</url>
      <doi>10.18653/v1/D19-5004</doi>
      <revision id="1" href="D19-5004v1" hash="3b8c1d53" />
      <revision id="2" href="D19-5004v2" hash="e09a9492">We corrected a few citations' bibtex entry and made sure they're based on the standard format of the official template.
- We added a short footnote (#2) just to clarify how we calculated the baseline in our paper (this was already available in our GitHub but we thought it may be better to be also in our paper)
- We paraphrased a few sentences in the related work section in one of the paragraphs to make sure they are distinguishable from the original paper.
- We added one sentence to the caption of Table 3 to make sure our numbers can be clearly interpreted by the readers.</revision>
      <bibkey>levi-etal-2019-identifying</bibkey>
      <pwccode url="https://github.com/adverifai/Satire_vs_Fake" additional="false">adverifai/Satire_vs_Fake</pwccode>
    </paper>
    <paper id="7">
      <title>Generating Sentential Arguments from Diverse Perspectives on Controversial Topic</title>
      <author><first>ChaeHun</first><last>Park</last></author>
      <author><first>Wonsuk</first><last>Yang</last></author>
      <author><first>Jong</first><last>Park</last></author>
      <pages>56–65</pages>
      <abstract>Considering diverse aspects of an argumentative issue is an essential step for mitigating a biased opinion and making reasonable decisions. A related generation model can produce flexible results that cover a wide range of topics, compared to the retrieval-based method that may show unstable performance for unseen data. In this paper, we study the problem of generating sentential arguments from multiple perspectives, and propose a neural method to address this problem. Our model, ArgDiver (Argument generation model from diverse perspectives), in a way a conversational system, successfully generates high-quality sentential arguments. At the same time, the automatically generated arguments by our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> show a higher <a href="https://en.wikipedia.org/wiki/Diversity_index">diversity</a> than those generated by any other baseline models. We believe that our work provides evidence for the potential of a good generation model in providing diverse perspectives on a controversial topic.</abstract>
      <url hash="cf19916d">D19-5007</url>
      <doi>10.18653/v1/D19-5007</doi>
      <bibkey>park-etal-2019-generating</bibkey>
    </paper>
    <paper id="9">
      <title>Unraveling the Search Space of Abusive Language in <a href="https://en.wikipedia.org/wiki/Wikipedia">Wikipedia</a> with Dynamic Lexicon Acquisition<fixed-case>W</fixed-case>ikipedia with Dynamic Lexicon Acquisition</title>
      <author><first>Wei-Fan</first><last>Chen</last></author>
      <author><first>Khalid</first><last>Al Khatib</last></author>
      <author><first>Matthias</first><last>Hagen</last></author>
      <author><first>Henning</first><last>Wachsmuth</last></author>
      <author><first>Benno</first><last>Stein</last></author>
      <pages>76–82</pages>
      <abstract>Many discussions on online platforms suffer from users offending others by using abusive terminology, threatening each other, or being sarcastic. Since an automatic detection of abusive language can support human moderators of <a href="https://en.wikipedia.org/wiki/Internet_forum">online discussion platforms</a>, detecting abusiveness has recently received increased attention. However, the existing approaches simply train one <a href="https://en.wikipedia.org/wiki/Statistical_classification">classifier</a> for the whole variety of <a href="https://en.wikipedia.org/wiki/Abusive_power_and_control">abusiveness</a>. In contrast, our approach is to distinguish explicitly abusive cases from the more shadowed ones. By dynamically extending a lexicon of abusive terms (e.g., including new obfuscations of abusive terms), our approach can support a moderator with explicit unraveled explanations for why something was flagged as abusive : due to known explicitly abusive terms, due to newly detected (obfuscated) terms, or due to shadowed cases.</abstract>
      <url hash="da0da50d">D19-5009</url>
      <doi>10.18653/v1/D19-5009</doi>
      <bibkey>chen-etal-2019-unraveling</bibkey>
    </paper>
    <paper id="13">
      <title>Fine-Tuned Neural Models for Propaganda Detection at the Sentence and Fragment levels</title>
      <author><first>Tariq</first><last>Alhindi</last></author>
      <author><first>Jonas</first><last>Pfeiffer</last></author>
      <author><first>Smaranda</first><last>Muresan</last></author>
      <pages>98–102</pages>
      <abstract>This paper presents the CUNLP submission for the NLP4IF 2019 shared-task on Fine-Grained Propaganda Detection. Our system finished 5th out of 26 teams on the sentence-level classification task and 5th out of 11 teams on the fragment-level classification task based on our scores on the blind test set. We present our models, a discussion of our ablation studies and experiments, and an analysis of our performance on all eighteen propaganda techniques present in the corpus of the shared task.</abstract>
      <url hash="6ca9d880">D19-5013</url>
      <doi>10.18653/v1/D19-5013</doi>
      <bibkey>alhindi-etal-2019-fine</bibkey>
    </paper>
    <paper id="16">
      <title>JUSTDeep at NLP4IF 2019 Task 1 : Propaganda Detection using Ensemble Deep Learning Models<fixed-case>JUSTD</fixed-case>eep at <fixed-case>NLP</fixed-case>4<fixed-case>IF</fixed-case> 2019 Task 1: Propaganda Detection using Ensemble Deep Learning Models</title>
      <author><first>Hani</first><last>Al-Omari</last></author>
      <author><first>Malak</first><last>Abdullah</last></author>
      <author><first>Ola</first><last>AlTiti</last></author>
      <author><first>Samira</first><last>Shaikh</last></author>
      <pages>113–118</pages>
      <abstract>The <a href="https://en.wikipedia.org/wiki/Internet">internet</a> and the high use of <a href="https://en.wikipedia.org/wiki/Social_media">social media</a> have enabled the modern-day journalism to publish, share and spread news that is difficult to distinguish if it is true or fake. Defining fake news is not well established yet, however, it can be categorized under several labels : false, biased, or framed to mislead the readers that are characterized as <a href="https://en.wikipedia.org/wiki/Propaganda">propaganda</a>. Digital content production technologies with <a href="https://en.wikipedia.org/wiki/Fallacy">logical fallacies</a> and emotional language can be used as <a href="https://en.wikipedia.org/wiki/Propaganda_techniques">propaganda techniques</a> to gain more readers or mislead the audience. Recently, several researchers have proposed deep learning (DL) models to address this issue. This research paper provides an ensemble deep learning model using BiLSTM, <a href="https://en.wikipedia.org/wiki/XGBoost">XGBoost</a>, and <a href="https://en.wikipedia.org/wiki/BERT">BERT</a> to detect <a href="https://en.wikipedia.org/wiki/Propaganda">propaganda</a>. The proposed <a href="https://en.wikipedia.org/wiki/Statistical_model">model</a> has been applied on the dataset provided by the challenge NLP4IF 2019, Task 1 Sentence Level Classification (SLC) and it shows a significant performance over the baseline model.</abstract>
      <url hash="67dabed3">D19-5016</url>
      <doi>10.18653/v1/D19-5016</doi>
      <bibkey>al-omari-etal-2019-justdeep</bibkey>
    </paper>
    <paper id="17">
      <title>Detection of Propaganda Using Logistic Regression</title>
      <author><first>Jinfen</first><last>Li</last></author>
      <author><first>Zhihao</first><last>Ye</last></author>
      <author><first>Lu</first><last>Xiao</last></author>
      <pages>119–124</pages>
      <abstract>Various <a href="https://en.wikipedia.org/wiki/Propaganda_techniques">propaganda techniques</a> are used to manipulate peoples perspectives in order to foster a predetermined agenda such as by the use of <a href="https://en.wikipedia.org/wiki/Fallacy">logical fallacies</a> or appealing to the emotions of the audience. In this paper, we develop a Logistic Regression-based tool that automatically classifies whether a sentence is propagandistic or not. We utilize features like TF-IDF, BERT vector, sentence length, readability grade level, emotion feature, LIWC feature and emphatic content feature to help us differentiate these two categories. The linguistic and semantic features combination results in 66.16 % of F1 score, which outperforms the <a href="https://en.wikipedia.org/wiki/Baseline_(configuration_management)">baseline</a> hugely.</abstract>
      <url hash="fd73ee8c">D19-5017</url>
      <doi>10.18653/v1/D19-5017</doi>
      <bibkey>li-etal-2019-detection</bibkey>
    </paper>
    <paper id="19">
      <title>Understanding BERT performance in propaganda analysis<fixed-case>BERT</fixed-case> performance in propaganda analysis</title>
      <author><first>Yiqing</first><last>Hua</last></author>
      <pages>135–138</pages>
      <abstract>In this paper, we describe our <a href="https://en.wikipedia.org/wiki/System">system</a> used in the shared task for fine-grained propaganda analysis at <a href="https://en.wikipedia.org/wiki/Sentence_(linguistics)">sentence level</a>. Despite the challenging nature of the task, our pretrained BERT model (team YMJA) fine tuned on the training dataset provided by the shared task scored 0.62 F1 on the test set and ranked third among 25 teams who participated in the contest. We present a set of illustrative experiments to better understand the performance of our BERT model on this shared task. Further, we explore beyond the given <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> for <a href="https://en.wikipedia.org/wiki/False_positives_and_false_negatives">false-positive cases</a> that likely to be produced by our <a href="https://en.wikipedia.org/wiki/System">system</a>. We show that despite the high performance on the given testset, our system may have the tendency of classifying opinion pieces as <a href="https://en.wikipedia.org/wiki/Propaganda">propaganda</a> and can not distinguish quotations of propaganda speech from actual usage of <a href="https://en.wikipedia.org/wiki/Propaganda">propaganda techniques</a>.</abstract>
      <url hash="7e6de020">D19-5019</url>
      <doi>10.18653/v1/D19-5019</doi>
      <bibkey>hua-2019-understanding</bibkey>
    </paper>
    <paper id="20">
      <title>Pretrained Ensemble Learning for Fine-Grained Propaganda Detection</title>
      <author><first>Ali</first><last>Fadel</last></author>
      <author><first>Ibraheem</first><last>Tuffaha</last></author>
      <author><first>Mahmoud</first><last>Al-Ayyoub</last></author>
      <pages>139–142</pages>
      <abstract>In this paper, we describe our team’s effort on the fine-grained propaganda detection on sentence level classification (SLC) task of NLP4IF 2019 workshop co-located with the EMNLP-IJCNLP 2019 conference. Our top performing <a href="https://en.wikipedia.org/wiki/System">system</a> results come from applying <a href="https://en.wikipedia.org/wiki/Ensemble_learning">ensemble average</a> on three pretrained models to make their predictions. The first two models use the uncased and cased versions of Bidirectional Encoder Representations from Transformers (BERT) (Devlin et al., 2018) while the third model uses Universal Sentence Encoder (USE) (Cer et al. Out of 26 participating teams, our system is ranked in the first place with 68.8312 <a href="https://en.wikipedia.org/wiki/F-number">F1-score</a> on the <a href="https://en.wikipedia.org/wiki/Software_development_process">development dataset</a> and in the sixth place with 61.3870 <a href="https://en.wikipedia.org/wiki/F-number">F1-score</a> on the <a href="https://en.wikipedia.org/wiki/Software_testing">testing dataset</a>.</abstract>
      <url hash="71d0b0df">D19-5020</url>
      <doi>10.18653/v1/D19-5020</doi>
      <bibkey>fadel-etal-2019-pretrained</bibkey>
    </paper>
    <paper id="22">
      <title>Sentence-Level Propaganda Detection in News Articles with Transfer Learning and BERT-BiLSTM-Capsule Model<fixed-case>BERT</fixed-case>-<fixed-case>B</fixed-case>i<fixed-case>LSTM</fixed-case>-Capsule Model</title>
      <author><first>George-Alexandru</first><last>Vlad</last></author>
      <author><first>Mircea-Adrian</first><last>Tanase</last></author>
      <author><first>Cristian</first><last>Onose</last></author>
      <author><first>Dumitru-Clementin</first><last>Cercel</last></author>
      <pages>148–154</pages>
      <abstract>In recent years, the need for <a href="https://en.wikipedia.org/wiki/Communication">communication</a> increased in <a href="https://en.wikipedia.org/wiki/Social_media">online social media</a>. Propaganda is a mechanism which was used throughout history to influence public opinion and <a href="https://en.wikipedia.org/wiki/Information_technology">it</a> is gaining a new dimension with the rising interest of online social media. This paper presents our submission to NLP4IF-2019 Shared Task SLC : Sentence-level Propaganda Detection in news articles. The challenge of this task is to build a robust binary classifier able to provide corresponding <a href="https://en.wikipedia.org/wiki/Propaganda">propaganda labels</a>, <a href="https://en.wikipedia.org/wiki/Propaganda">propaganda</a> or non-propaganda. Our model relies on a unified neural network, which consists of several deep leaning modules, namely BERT, BiLSTM and Capsule, to solve the sentencelevel propaganda classification problem. In addition, we take a pre-training approach on a somewhat similar task (i.e., emotion classification) improving results against the cold-start model. Among the 26 participant teams in the NLP4IF-2019 Task SLC, our solution ranked 12th with an F1-score 0.5868 on the official test data. Our proposed <a href="https://en.wikipedia.org/wiki/Solution">solution</a> indicates promising results since our <a href="https://en.wikipedia.org/wiki/System">system</a> significantly exceeds the baseline approach of the organizers by 0.1521 and is slightly lower than the winning <a href="https://en.wikipedia.org/wiki/System">system</a> by 0.0454.</abstract>
      <url hash="3de20da0">D19-5022</url>
      <doi>10.18653/v1/D19-5022</doi>
      <bibkey>vlad-etal-2019-sentence</bibkey>
    </paper>
    <paper id="23">
      <title>Synthetic Propaganda Embeddings To Train A Linear Projection</title>
      <author><first>Adam</first><last>Ek</last></author>
      <author><first>Mehdi</first><last>Ghanimifard</last></author>
      <pages>155–161</pages>
      <abstract>This paper presents a method of detecting fine-grained categories of propaganda in text. Given a sentence, our <a href="https://en.wikipedia.org/wiki/Methodology">method</a> aims to identify a span of words and predict the type of <a href="https://en.wikipedia.org/wiki/Propaganda">propaganda</a> used. To detect <a href="https://en.wikipedia.org/wiki/Propaganda">propaganda</a>, we explore a method for extracting features of propaganda from contextualized embeddings without fine-tuning the large parameters of the base model. We show that by generating synthetic embeddings we can train a <a href="https://en.wikipedia.org/wiki/Linear_function">linear function</a> with ReLU activation to extract useful labeled embeddings from an embedding space generated by a general-purpose language model. We also introduce an <a href="https://en.wikipedia.org/wiki/Statistical_inference">inference technique</a> to detect continuous spans in sequences of propaganda tokens in sentences. A result of the ensemble model is submitted to the first shared task in fine-grained propaganda detection at NLP4IF as Team Stalin. In this paper, we provide additional analysis regarding our method of detecting spans of propaganda with synthetically generated representations.</abstract>
      <url hash="58bdc011">D19-5023</url>
      <doi>10.18653/v1/D19-5023</doi>
      <bibkey>ek-ghanimifard-2019-synthetic</bibkey>
    </paper>
    </volume>
  <volume id="51" ingest-date="2019-11-03">
    <meta>
      <booktitle>Proceedings of the Second Workshop on Economics and Natural Language Processing</booktitle>
      <url hash="ce97fb4f">D19-51</url>
      <editor><first>Udo</first><last>Hahn</last></editor>
      <editor><first>Véronique</first><last>Hoste</last></editor>
      <editor><first>Zhu</first><last>Zhang</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Hong Kong</address>
      <month>November</month>
      <year>2019</year>
    </meta>
    <frontmatter>
      <url hash="31f9d3c2">D19-5100</url>
      <bibkey>emnlp-2019-economics</bibkey>
    </frontmatter>
    <paper id="2">
      <title>Financial Event Extraction Using Wikipedia-Based Weak Supervision<fixed-case>W</fixed-case>ikipedia-Based Weak Supervision</title>
      <author><first>Liat</first><last>Ein-Dor</last></author>
      <author><first>Ariel</first><last>Gera</last></author>
      <author><first>Orith</first><last>Toledo-Ronen</last></author>
      <author><first>Alon</first><last>Halfon</last></author>
      <author><first>Benjamin</first><last>Sznajder</last></author>
      <author><first>Lena</first><last>Dankin</last></author>
      <author><first>Yonatan</first><last>Bilu</last></author>
      <author><first>Yoav</first><last>Katz</last></author>
      <author><first>Noam</first><last>Slonim</last></author>
      <pages>10–15</pages>
      <abstract>Extraction of financial and economic events from <a href="https://en.wikipedia.org/wiki/Text_(literary_theory)">text</a> has previously been done mostly using rule-based methods, with more recent works employing <a href="https://en.wikipedia.org/wiki/Machine_learning">machine learning techniques</a>. This work is in line with this latter approach, leveraging relevant Wikipedia sections to extract weak labels for sentences describing economic events. Whereas previous weakly supervised approaches required a knowledge-base of such events, or corresponding financial figures, our approach requires no such additional data, and can be employed to extract economic events related to companies which are not even mentioned in the training data.</abstract>
      <url hash="bf638444">D19-5102</url>
      <doi>10.18653/v1/D19-5102</doi>
      <bibkey>ein-dor-etal-2019-financial</bibkey>
    </paper>
    <paper id="4">
      <title>Forecasting Firm Material Events from 8-K Reports</title>
      <author><first>Shuang (Sophie)</first><last>Zhai</last></author>
      <author><first>Zhu (Drew)</first><last>Zhang</last></author>
      <pages>22–30</pages>
      <abstract>In this paper, we show deep learning models can be used to forecast firm material event sequences based on the contents in the company’s 8-K Current Reports. Specifically, we exploit state-of-the-art neural architectures, including sequence-to-sequence (Seq2Seq) architecture and attention mechanisms, in the model. Our 8K-powered deep learning model demonstrates promising performance in forecasting firm future event sequences. The model is poised to benefit various stakeholders, including management and investors, by facilitating <a href="https://en.wikipedia.org/wiki/Risk_management">risk management</a> and <a href="https://en.wikipedia.org/wiki/Decision-making">decision making</a>.</abstract>
      <url hash="3b637408">D19-5104</url>
      <doi>10.18653/v1/D19-5104</doi>
      <bibkey>zhai-zhang-2019-forecasting</bibkey>
    </paper>
    <paper id="5">
      <title>Incorporating Fine-grained Events in Stock Movement Prediction</title>
      <author><first>Deli</first><last>Chen</last></author>
      <author><first>Yanyan</first><last>Zou</last></author>
      <author><first>Keiko</first><last>Harimoto</last></author>
      <author><first>Ruihan</first><last>Bao</last></author>
      <author><first>Xuancheng</first><last>Ren</last></author>
      <author><first>Xu</first><last>Sun</last></author>
      <pages>31–40</pages>
      <abstract>Considering event structure information has proven helpful in text-based stock movement prediction. However, existing works mainly adopt the coarse-grained events, which loses the specific semantic information of diverse event types. In this work, we propose to incorporate the fine-grained events in stock movement prediction. Firstly, we propose a professional finance event dictionary built by domain experts and use it to extract fine-grained events automatically from finance news. Then we design a neural model to combine finance news with fine-grained event structure and stock trade data to predict the stock movement. Besides, in order to improve the generalizability of the proposed method, we design an advanced model that uses the extracted fine-grained events as the distant supervised label to train a multi-task framework of <a href="https://en.wikipedia.org/wiki/Event_(probability_theory)">event extraction</a> and stock prediction. The experimental results show that our <a href="https://en.wikipedia.org/wiki/Methodology">method</a> outperforms all the <a href="https://en.wikipedia.org/wiki/Baseline_(configuration_management)">baselines</a> and has good generalizability.</abstract>
      <url hash="47dc775f">D19-5105</url>
      <attachment hash="c977e197">D19-5105.Attachment.zip</attachment>
      <doi>10.18653/v1/D19-5105</doi>
      <bibkey>chen-etal-2019-incorporating</bibkey>
    </paper>
    <paper id="6">
      <title>Group, Extract and Aggregate : Summarizing a Large Amount of Finance News for Forex Movement Prediction</title>
      <author><first>Deli</first><last>Chen</last></author>
      <author><first>Shuming</first><last>Ma</last></author>
      <author><first>Keiko</first><last>Harimoto</last></author>
      <author><first>Ruihan</first><last>Bao</last></author>
      <author><first>Qi</first><last>Su</last></author>
      <author><first>Xu</first><last>Sun</last></author>
      <pages>41–50</pages>
      <abstract>Incorporating related text information has proven successful in <a href="https://en.wikipedia.org/wiki/Stock_market_prediction">stock market prediction</a>. However, it is a huge challenge to utilize texts in the enormous forex (foreign currency exchange) market because the associated texts are too redundant. In this work, we propose a BERT-based Hierarchical Aggregation Model to summarize a large amount of finance news to predict <a href="https://en.wikipedia.org/wiki/Foreign_exchange_market">forex movement</a>. We firstly group news from different aspects : time, topic and category. Then we extract the most crucial news in each group by the SOTA extractive summarization method. Finally, we conduct interaction between the news and the trade data with attention to predict the <a href="https://en.wikipedia.org/wiki/Foreign_exchange_market">forex movement</a>. The experimental results show that the category based method performs best among three grouping methods and outperforms all the baselines. Besides, we study the influence of essential news attributes (category and region) by <a href="https://en.wikipedia.org/wiki/Statistical_inference">statistical analysis</a> and summarize the influence patterns for different currency pairs.</abstract>
      <url hash="d8861734">D19-5106</url>
      <attachment hash="2e4605d9">D19-5106.Attachment.zip</attachment>
      <doi>10.18653/v1/D19-5106</doi>
      <bibkey>chen-etal-2019-group</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/stocknet-1">StockNet</pwcdataset>
    </paper>
    <paper id="7">
      <title>Complaint Analysis and Classification for Economic and Food Safety</title>
      <author><first>João</first><last>Filgueiras</last></author>
      <author><first>Luís</first><last>Barbosa</last></author>
      <author><first>Gil</first><last>Rocha</last></author>
      <author><first>Henrique</first><last>Lopes Cardoso</last></author>
      <author><first>Luís Paulo</first><last>Reis</last></author>
      <author><first>João Pedro</first><last>Machado</last></author>
      <author><first>Ana Maria</first><last>Oliveira</last></author>
      <pages>51–60</pages>
      <abstract>Governmental institutions are employing artificial intelligence techniques to deal with their specific problems and exploit their huge amounts of both structured and unstructured information. In particular, <a href="https://en.wikipedia.org/wiki/Natural_language_processing">natural language processing</a> and <a href="https://en.wikipedia.org/wiki/Machine_learning">machine learning techniques</a> are being used to process citizen feedback. In this paper, we report on the use of such techniques for analyzing and classifying complaints, in the context of the Portuguese Economic and Food Safety Authority. Grounded in its operational process, we address three different classification problems : target economic activity, implied infraction severity level, and institutional competence. We show promising results obtained using feature-based approaches and traditional <a href="https://en.wikipedia.org/wiki/Statistical_classification">classifiers</a>, with accuracy scores above 70 %, and analyze the shortcomings of our current results and avenues for further improvement, taking into account the intended use of our <a href="https://en.wikipedia.org/wiki/Statistical_classification">classifiers</a> in helping human officers to cope with thousands of yearly complaints.</abstract>
      <url hash="91b17bb2">D19-5107</url>
      <doi>10.18653/v1/D19-5107</doi>
      <bibkey>filgueiras-etal-2019-complaint</bibkey>
    </paper>
    </volume>
  <volume id="52" ingest-date="2019-11-03">
    <meta>
      <booktitle>Proceedings of the 6th Workshop on Asian Translation</booktitle>
      <url hash="fd05fe9c">D19-52</url>
      <editor><first>Toshiaki</first><last>Nakazawa</last></editor>
      <editor><first>Chenchen</first><last>Ding</last></editor>
      <editor><first>Raj</first><last>Dabre</last></editor>
      <editor><first>Anoop</first><last>Kunchukuttan</last></editor>
      <editor><first>Nobushige</first><last>Doi</last></editor>
      <editor><first>Yusuke</first><last>Oda</last></editor>
      <editor><first>Ondřej</first><last>Bojar</last></editor>
      <editor><first>Shantipriya</first><last>Parida</last></editor>
      <editor><first>Isao</first><last>Goto</last></editor>
      <editor><first>Hidaya</first><last>Mino</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Hong Kong, China</address>
      <month>November</month>
      <year>2019</year>
    </meta>
    <frontmatter>
      <url hash="3396bd6a">D19-5200</url>
      <bibkey>emnlp-2019-asian</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Overview of the 6th Workshop on Asian Translation<fixed-case>A</fixed-case>sian Translation</title>
      <author><first>Toshiaki</first><last>Nakazawa</last></author>
      <author><first>Nobushige</first><last>Doi</last></author>
      <author><first>Shohei</first><last>Higashiyama</last></author>
      <author><first>Chenchen</first><last>Ding</last></author>
      <author><first>Raj</first><last>Dabre</last></author>
      <author><first>Hideya</first><last>Mino</last></author>
      <author><first>Isao</first><last>Goto</last></author>
      <author><first>Win Pa</first><last>Pa</last></author>
      <author><first>Anoop</first><last>Kunchukuttan</last></author>
      <author><first>Yusuke</first><last>Oda</last></author>
      <author><first>Shantipriya</first><last>Parida</last></author>
      <author><first>Ondřej</first><last>Bojar</last></author>
      <author><first>Sadao</first><last>Kurohashi</last></author>
      <pages>1–35</pages>
      <abstract>This paper presents the results of the shared tasks from the 6th workshop on Asian translation (WAT2019) including JaEn, JaZh scientific paper translation subtasks, JaEn, JaKo, JaEn patent translation subtasks, HiEn, MyEn, KmEn, TaEn mixed domain subtasks and RuJa news commentary translation task. For the WAT2019, 25 teams participated in the shared tasks. We also received 10 research paper submissions out of which 61 were accepted. About 400 translation results were submitted to the automatic evaluation server, and selected submis- sions were manually evaluated.</abstract>
      <url hash="ee1e33be">D19-5201</url>
      <doi>10.18653/v1/D19-5201</doi>
      <revision id="1" href="D19-5201v1" hash="4b03765d" />
      <revision id="2" href="D19-5201v2" hash="ee1e33be" date="2020-09-03">Adds an author.</revision>
      <bibkey>nakazawa-etal-2019-overview</bibkey>
    </paper>
    <paper id="2">
      <title>Compact and Robust Models for Japanese-English Character-level Machine Translation<fixed-case>J</fixed-case>apanese-<fixed-case>E</fixed-case>nglish Character-level Machine Translation</title>
      <author><first>Jinan</first><last>Dai</last></author>
      <author><first>Kazunori</first><last>Yamaguchi</last></author>
      <pages>36–44</pages>
      <abstract>Character-level translation has been proved to be able to achieve preferable translation quality without explicit segmentation, but training a character-level model needs a lot of hardware resources. In this paper, we introduced two character-level translation models which are mid-gated model and multi-attention model for Japanese-English translation. We showed that the mid-gated model achieved the better performance with respect to BLEU scores. We also showed that a relatively narrow beam of width 4 or 5 was sufficient for the mid-gated model. As for unknown words, we showed that the mid-gated model could somehow translate the one containing Katakana by coining out a close word. We also showed that the <a href="https://en.wikipedia.org/wiki/Statistical_model">model</a> managed to produce tolerable results for heavily noised sentences, even though the <a href="https://en.wikipedia.org/wiki/Statistical_model">model</a> was trained with the dataset without noise.</abstract>
      <url hash="69bbc3a2">D19-5202</url>
      <doi>10.18653/v1/D19-5202</doi>
      <bibkey>dai-yamaguchi-2019-compact</bibkey>
    </paper>
    <paper id="7">
      <title>NICT’s participation to WAT 2019 : <a href="https://en.wikipedia.org/wiki/Multilingualism">Multilingualism</a> and Multi-step Fine-Tuning for Low Resource NMT<fixed-case>NICT</fixed-case>’s participation to <fixed-case>WAT</fixed-case> 2019: Multilingualism and Multi-step Fine-Tuning for Low Resource <fixed-case>NMT</fixed-case></title>
      <author><first>Raj</first><last>Dabre</last></author>
      <author><first>Eiichiro</first><last>Sumita</last></author>
      <pages>76–80</pages>
      <abstract>In this paper we describe our submissions to WAT 2019 for the following tasks : EnglishTamil translation and RussianJapanese translation. Our team, NICT-5, focused on multilingual domain adaptation and back-translation for RussianJapanese translation and on simple fine-tuning for EnglishTamil translation. We noted that multi-stage fine tuning is essential in leveraging the power of <a href="https://en.wikipedia.org/wiki/Multilingualism">multilingualism</a> for an extremely low-resource language like RussianJapanese. Furthermore, we can improve the performance of such a low-resource language pair by exploiting a small but in-domain monolingual corpus via <a href="https://en.wikipedia.org/wiki/Back-translation">back-translation</a>. We managed to obtain second rank in both <a href="https://en.wikipedia.org/wiki/Task_(project_management)">tasks</a> for all translation directions.</abstract>
      <url hash="2a47ecac">D19-5207</url>
      <doi>10.18653/v1/D19-5207</doi>
      <bibkey>dabre-sumita-2019-nicts</bibkey>
    </paper>
    <paper id="16">
      <title>LTRC-MT Simple &amp; Effective Hindi-English Neural Machine Translation Systems at WAT 2019<fixed-case>LTRC</fixed-case>-<fixed-case>MT</fixed-case> Simple &amp; Effective <fixed-case>H</fixed-case>indi-<fixed-case>E</fixed-case>nglish Neural Machine Translation Systems at <fixed-case>WAT</fixed-case> 2019</title>
      <author><first>Vikrant</first><last>Goyal</last></author>
      <author><first>Dipti Misra</first><last>Sharma</last></author>
      <pages>137–140</pages>
      <abstract>This paper describes the Neural Machine Translation systems of IIIT-Hyderabad (LTRC-MT) for WAT 2019 Hindi-English shared task. We experimented with both Recurrent Neural Networks &amp; Transformer architectures. We also show the results of our experiments of training NMT models using additional data via backtranslation.</abstract>
      <url hash="78324b89">D19-5216</url>
      <doi>10.18653/v1/D19-5216</doi>
      <bibkey>goyal-sharma-2019-ltrc</bibkey>
    </paper>
    <paper id="18">
      <title>Supervised neural machine translation based on <a href="https://en.wikipedia.org/wiki/Data_augmentation">data augmentation</a> and improved training &amp; inference process</title>
      <author><first>Yixuan</first><last>Tong</last></author>
      <author><first>Liang</first><last>Liang</last></author>
      <author><first>Boyan</first><last>Liu</last></author>
      <author><first>Shanshan</first><last>Jiang</last></author>
      <author><first>Bin</first><last>Dong</last></author>
      <pages>147–151</pages>
      <abstract>This is the second time for SRCB to participate in WAT. This paper describes the <a href="https://en.wikipedia.org/wiki/Neural_machine_translation">neural machine translation systems</a> for the shared translation tasks of WAT 2019. We participated in ASPEC tasks and submitted results on English-Japanese, Japanese-English, Chinese-Japanese, and Japanese-Chinese four language pairs. We employed the Transformer model as the baseline and experimented relative position representation, data augmentation, deep layer model, ensemble. Experiments show that all these <a href="https://en.wikipedia.org/wiki/Methodology">methods</a> can yield substantial improvements.</abstract>
      <url hash="e378100f">D19-5218</url>
      <doi>10.18653/v1/D19-5218</doi>
      <bibkey>tong-etal-2019-supervised</bibkey>
    </paper>
    <paper id="22">
      <title>NLPRL at WAT2019 : Transformer-based Tamil   English Indic Task Neural Machine Translation System<fixed-case>NLPRL</fixed-case> at <fixed-case>WAT</fixed-case>2019: Transformer-based <fixed-case>T</fixed-case>amil – <fixed-case>E</fixed-case>nglish Indic Task Neural Machine Translation System</title>
      <author><first>Amit</first><last>Kumar</last></author>
      <author><first>Anil Kumar</first><last>Singh</last></author>
      <pages>171–174</pages>
      <abstract>This paper describes the Machine Translation system for Tamil-English Indic Task organized at WAT 2019. We use Transformer- based architecture for <a href="https://en.wikipedia.org/wiki/Neural_machine_translation">Neural Machine Translation</a>.</abstract>
      <url hash="8f17b056">D19-5222</url>
      <doi>10.18653/v1/D19-5222</doi>
      <bibkey>kumar-singh-2019-nlprl</bibkey>
    </paper>
    <paper id="23">
      <title>Idiap NMT System for WAT 2019 Multimodal Translation Task<fixed-case>NMT</fixed-case> System for <fixed-case>WAT</fixed-case> 2019 Multimodal Translation Task</title>
      <author><first>Shantipriya</first><last>Parida</last></author>
      <author><first>Ondřej</first><last>Bojar</last></author>
      <author><first>Petr</first><last>Motlicek</last></author>
      <pages>175–180</pages>
      <abstract>This paper describes the Idiap submission to WAT 2019 for the English-Hindi Multi-Modal Translation Task. We have used the state-of-the-art Transformer model and utilized the IITB English-Hindi parallel corpus as an additional data source. Among the different tracks of the multi-modal task, we have participated in the Text-Only track for the evaluation and challenge test sets. Our submission tops in its track among the competitors in terms of both automatic and manual evaluation. Based on automatic scores, our text-only submission also outperforms systems that consider <a href="https://en.wikipedia.org/wiki/Visual_system">visual information</a> in the multi-modal translation task.</abstract>
      <url hash="8e7ccc8d">D19-5223</url>
      <doi>10.18653/v1/D19-5223</doi>
      <bibkey>parida-etal-2019-idiap</bibkey>
    </paper>
    <paper id="26">
      <title>UCSYNLP-Lab Machine Translation Systems for WAT 2019<fixed-case>UCSYNLP</fixed-case>-Lab Machine Translation Systems for <fixed-case>WAT</fixed-case> 2019</title>
      <author><first>Yimon</first><last>ShweSin</last></author>
      <author><first>Win Pa</first><last>Pa</last></author>
      <author><first>KhinMar</first><last>Soe</last></author>
      <pages>195–199</pages>
      <abstract>This paper describes the UCSYNLP-Lab submission to WAT 2019 for Myanmar-English translation tasks in both direction. We have used the neural machine translation systems with attention model and utilized the UCSY-corpus and ALT corpus. In NMT with attention model, we use the word segmentation level as well as syllable segmentation level. Especially, we made the UCSY-corpus to be cleaned in WAT 2019. Therefore, the UCSY corpus for WAT 2019 is not identical to those used in WAT 2018. Experiments show that the <a href="https://en.wikipedia.org/wiki/Translation_(geometry)">translation systems</a> can produce the substantial improvements.</abstract>
      <url hash="040804a7">D19-5226</url>
      <doi>10.18653/v1/D19-5226</doi>
      <bibkey>shwesin-etal-2019-ucsynlp</bibkey>
    </paper>
    </volume>
  <volume id="53" ingest-date="2019-11-03">
    <meta>
      <booktitle>Proceedings of the Thirteenth Workshop on Graph-Based Methods for Natural Language Processing (TextGraphs-13)</booktitle>
      <url hash="ef851e40">D19-53</url>
      <editor><first>Dmitry</first><last>Ustalov</last></editor>
      <editor><first>Swapna</first><last>Somasundaran</last></editor>
      <editor><first>Peter</first><last>Jansen</last></editor>
      <editor><first>Goran</first><last>Glavaš</last></editor>
      <editor><first>Martin</first><last>Riedl</last></editor>
      <editor><first>Mihai</first><last>Surdeanu</last></editor>
      <editor><first>Michalis</first><last>Vazirgiannis</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Hong Kong</address>
      <month>November</month>
      <year>2019</year>
    </meta>
    <frontmatter>
      <url hash="0f73ad0c">D19-5300</url>
      <bibkey>emnlp-2019-graph</bibkey>
    </frontmatter>
    <paper id="2">
      <title>Relation Prediction for Unseen-Entities Using Entity-Word Graphs</title>
      <author><first>Yuki</first><last>Tagawa</last></author>
      <author><first>Motoki</first><last>Taniguchi</last></author>
      <author><first>Yasuhide</first><last>Miura</last></author>
      <author><first>Tomoki</first><last>Taniguchi</last></author>
      <author><first>Tomoko</first><last>Ohkuma</last></author>
      <author><first>Takayuki</first><last>Yamamoto</last></author>
      <author><first>Keiichi</first><last>Nemoto</last></author>
      <pages>11–16</pages>
      <abstract>Knowledge graphs (KGs) are generally used for various <a href="https://en.wikipedia.org/wiki/Neuro-linguistic_programming">NLP tasks</a>. However, as <a href="https://en.wikipedia.org/wiki/Knowledge_graph">KGs</a> still miss some information, it is necessary to develop Knowledge Graph Completion (KGC) methods. Most KGC researches do not focus on the Out-of-KGs entities (Unseen-entities), we need a method that can predict the relation for the entity pairs containing Unseen-entities to automatically add new entities to the KGs. In this study, we focus on relation prediction and propose a method to learn entity representations via a graph structure that uses Seen-entities, Unseen-entities and words as nodes created from the descriptions of all entities. In the experiments, our method shows a significant improvement in the relation prediction for the entity pairs containing Unseen-entities.</abstract>
      <url hash="62ec830a">D19-5302</url>
      <doi>10.18653/v1/D19-5302</doi>
      <bibkey>tagawa-etal-2019-relation</bibkey>
    </paper>
    <paper id="4">
      <title>Neural Speech Translation using <a href="https://en.wikipedia.org/wiki/Lattice_model_(physics)">Lattice Transformations</a> and Graph Networks</title>
      <author><first>Daniel</first><last>Beck</last></author>
      <author><first>Trevor</first><last>Cohn</last></author>
      <author><first>Gholamreza</first><last>Haffari</last></author>
      <pages>26–31</pages>
      <abstract>Speech translation systems usually follow a pipeline approach, using word lattices as an <a href="https://en.wikipedia.org/wiki/Intermediate_representation">intermediate representation</a>. However, previous work assume access to the original <a href="https://en.wikipedia.org/wiki/Transcription_(linguistics)">transcriptions</a> used to train the ASR system, which can limit applicability in real scenarios. In this work we propose an approach for <a href="https://en.wikipedia.org/wiki/Speech_translation">speech translation</a> through lattice transformations and neural models based on <a href="https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)">graph networks</a>. Experimental results show that our approach reaches competitive performance without relying on <a href="https://en.wikipedia.org/wiki/Transcription_(biology)">transcriptions</a>, while also being orders of magnitude faster than previous work.</abstract>
      <url hash="ca483c8e">D19-5304</url>
      <attachment hash="da33c246">D19-5304.Attachment.pdf</attachment>
      <doi>10.18653/v1/D19-5304</doi>
      <bibkey>beck-etal-2019-neural</bibkey>
    </paper>
    <paper id="5">
      <title>Using Graphs for Word Embedding with Enhanced Semantic Relations</title>
      <author><first>Matan</first><last>Zuckerman</last></author>
      <author><first>Mark</first><last>Last</last></author>
      <pages>32–41</pages>
      <abstract>Word embedding algorithms have become a common tool in the field of <a href="https://en.wikipedia.org/wiki/Natural_language_processing">natural language processing</a>. While some, like Word2Vec, are based on sequential text input, others are utilizing a <a href="https://en.wikipedia.org/wiki/Graph_of_a_function">graph representation of text</a>. In this paper, we introduce a new <a href="https://en.wikipedia.org/wiki/Algorithm">algorithm</a>, named WordGraph2Vec, or in short WG2V, which combines the two approaches to gain the benefits of both. The <a href="https://en.wikipedia.org/wiki/Algorithm">algorithm</a> uses a <a href="https://en.wikipedia.org/wiki/Directed_graph">directed word graph</a> to provide additional information for sequential text input algorithms. Our experiments on benchmark datasets show that text classification algorithms are nearly as accurate with WG2V as with other word embedding models while preserving more stable accuracy rankings.</abstract>
      <url hash="873a164b">D19-5305</url>
      <doi>10.18653/v1/D19-5305</doi>
      <bibkey>zuckerman-last-2019-using</bibkey>
    </paper>
    <paper id="6">
      <title>Identifying Supporting Facts for Multi-hop Question Answering with Document Graph Networks</title>
      <author><first>Mokanarangan</first><last>Thayaparan</last></author>
      <author><first>Marco</first><last>Valentino</last></author>
      <author><first>Viktor</first><last>Schlegel</last></author>
      <author><first>André</first><last>Freitas</last></author>
      <pages>42–51</pages>
      <abstract>Recent advances in <a href="https://en.wikipedia.org/wiki/Reading_comprehension">reading comprehension</a> have resulted in models that surpass human performance when the answer is contained in a single, continuous passage of text. However, complex Question Answering (QA) typically requires multi-hop reasoning-i.e. the integration of supporting facts from different sources, to infer the correct answer. This paper proposes Document Graph Network (DGN), a message passing architecture for the identification of supporting facts over a graph-structured representation of text. The evaluation on HotpotQA shows that DGN obtains competitive results when compared to a reading comprehension baseline operating on raw text, confirming the relevance of structured representations for supporting multi-hop reasoning.</abstract>
      <url hash="48e86752">D19-5306</url>
      <doi>10.18653/v1/D19-5306</doi>
      <bibkey>thayaparan-etal-2019-identifying</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/hotpotqa">HotpotQA</pwcdataset>
    </paper>
    <paper id="7">
      <title>Essentia : Mining Domain-specific Paraphrases with Word-Alignment Graphs<fixed-case>E</fixed-case>ssentia: Mining Domain-specific Paraphrases with Word-Alignment Graphs</title>
      <author><first>Danni</first><last>Ma</last></author>
      <author><first>Chen</first><last>Chen</last></author>
      <author><first>Behzad</first><last>Golshan</last></author>
      <author><first>Wang-Chiew</first><last>Tan</last></author>
      <pages>52–57</pages>
      <abstract>Paraphrases are important linguistic resources for a wide variety of <a href="https://en.wikipedia.org/wiki/Natural_language_processing">NLP applications</a>. Many techniques for automatic paraphrase mining from general corpora have been proposed. While these techniques are successful at discovering <a href="https://en.wikipedia.org/wiki/Paraphrase">generic paraphrases</a>, they often fail to identify <a href="https://en.wikipedia.org/wiki/Paraphrase">domain-specific paraphrases</a> (e.g., <a href="https://en.wikipedia.org/wiki/Employment">staff</a>, concierge in the hospitality domain). This is because current techniques are often based on <a href="https://en.wikipedia.org/wiki/Statistics">statistical methods</a>, while domain-specific corpora are too small to fit <a href="https://en.wikipedia.org/wiki/Statistics">statistical methods</a>. In this paper, we present an unsupervised graph-based technique to mine paraphrases from a small set of sentences that roughly share the same topic or intent. Our system, Essentia, relies on word-alignment techniques to create a word-alignment graph that merges and organizes tokens from input sentences. The resulting <a href="https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)">graph</a> is then used to generate candidate <a href="https://en.wikipedia.org/wiki/Paraphrase">paraphrases</a>. We demonstrate that our <a href="https://en.wikipedia.org/wiki/System">system</a> obtains high quality paraphrases, as evaluated by crowd workers. We further show that the majority of the identified <a href="https://en.wikipedia.org/wiki/Paraphrase">paraphrases</a> are domain-specific and thus complement existing paraphrase databases.<fixed-case>staff, concierge</fixed-case> in the hospitality domain). This is because current techniques are often based on statistical methods, while domain-specific corpora are too small to fit statistical methods. In this paper, we present an unsupervised graph-based technique to mine paraphrases from a small set of sentences that roughly share the same topic or intent. Our system, Essentia, relies on word-alignment techniques to create a word-alignment graph that merges and organizes tokens from input sentences. The resulting graph is then used to generate candidate paraphrases. We demonstrate that our system obtains high quality paraphrases, as evaluated by crowd workers. We further show that the majority of the identified paraphrases are domain-specific and thus complement existing paraphrase databases.</abstract>
      <url hash="b7baf52a">D19-5307</url>
      <doi>10.18653/v1/D19-5307</doi>
      <bibkey>ma-etal-2019-essentia</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/snips">SNIPS</pwcdataset>
    </paper>
    <paper id="8">
      <title>Layerwise Relevance Visualization in Convolutional Text Graph Classifiers</title>
      <author><first>Robert</first><last>Schwarzenberg</last></author>
      <author><first>Marc</first><last>Hübner</last></author>
      <author><first>David</first><last>Harbecke</last></author>
      <author><first>Christoph</first><last>Alt</last></author>
      <author><first>Leonhard</first><last>Hennig</last></author>
      <pages>58–62</pages>
      <abstract>Representations in the hidden layers of Deep Neural Networks (DNN) are often hard to interpret since it is difficult to project them into an interpretable domain. Graph Convolutional Networks (GCN) allow this <a href="https://en.wikipedia.org/wiki/Projection_(linear_algebra)">projection</a>, but existing explainability methods do not exploit this fact, i.e. do not focus their explanations on <a href="https://en.wikipedia.org/wiki/Intermediate_state">intermediate states</a>. In this work, we present a novel method that traces and visualizes <a href="https://en.wikipedia.org/wiki/Feature_(machine_learning)">features</a> that contribute to a <a href="https://en.wikipedia.org/wiki/Statistical_classification">classification decision</a> in the visible and hidden layers of a GCN. Our method exposes hidden cross-layer dynamics in the input <a href="https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)">graph structure</a>. We experimentally demonstrate that it yields meaningful layerwise explanations for a GCN sentence classifier.</abstract>
      <url hash="58d681cb">D19-5308</url>
      <attachment hash="ae702662">D19-5308.Attachment.pdf</attachment>
      <doi>10.18653/v1/D19-5308</doi>
      <bibkey>schwarzenberg-etal-2019-layerwise</bibkey>
      <pwccode url="https://github.com/DFKI-NLP/lrv" additional="false">DFKI-NLP/lrv</pwccode>
    </paper>
    <paper id="10">
      <title>ASU at TextGraphs 2019 Shared Task : Explanation ReGeneration using Language Models and Iterative Re-Ranking<fixed-case>ASU</fixed-case> at <fixed-case>T</fixed-case>ext<fixed-case>G</fixed-case>raphs 2019 Shared Task: Explanation <fixed-case>R</fixed-case>e<fixed-case>G</fixed-case>eneration using Language Models and Iterative Re-Ranking</title>
      <author><first>Pratyay</first><last>Banerjee</last></author>
      <pages>78–84</pages>
      <abstract>In this work we describe the system from Natural Language Processing group at Arizona State University for the TextGraphs 2019 Shared Task. The task focuses on Explanation Regeneration, an intermediate step towards general multi-hop inference on large graphs. Our approach consists of modeling the explanation regeneration task as a learning to rank problem, for which we use state-of-the-art language models and explore dataset preparation techniques. We utilize an iterative reranking based approach to further improve the <a href="https://en.wikipedia.org/wiki/Ranking">rankings</a>. Our <a href="https://en.wikipedia.org/wiki/System">system</a> secured 2nd rank in the <a href="https://en.wikipedia.org/wiki/Task_(project_management)">task</a> with a mean average precision (MAP) of 41.3 % on the test set.</abstract>
      <url hash="24d4f059">D19-5310</url>
      <attachment hash="69f73a92">D19-5310.Attachment.zip</attachment>
      <doi>10.18653/v1/D19-5310</doi>
      <bibkey>banerjee-2019-asu</bibkey>
    </paper>
    <paper id="13">
      <title>Chains-of-Reasoning at TextGraphs 2019 Shared Task : Reasoning over Chains of Facts for Explainable Multi-hop Inference<fixed-case>T</fixed-case>ext<fixed-case>G</fixed-case>raphs 2019 Shared Task: Reasoning over Chains of Facts for Explainable Multi-hop Inference</title>
      <author><first>Rajarshi</first><last>Das</last></author>
      <author><first>Ameya</first><last>Godbole</last></author>
      <author><first>Manzil</first><last>Zaheer</last></author>
      <author><first>Shehzaad</first><last>Dhuliawala</last></author>
      <author><first>Andrew</first><last>McCallum</last></author>
      <pages>101–117</pages>
      <abstract>This paper describes our submission to the shared task on Multi-hop Inference Explanation Regeneration in TextGraphs workshop at EMNLP 2019 (Jansen and Ustalov, 2019). Our <a href="https://en.wikipedia.org/wiki/System">system</a> identifies chains of facts relevant to explain an answer to an elementary science examination question. To counter the problem of ‘spurious chains’ leading to ‘semantic drifts’, we train a <a href="https://en.wikipedia.org/wiki/Ranker">ranker</a> that uses contextualized representation of facts to score its relevance for explaining an answer to a question. Our <a href="https://en.wikipedia.org/wiki/System">system</a> was ranked first w.r.t the mean average precision (MAP) metric outperforming the second best <a href="https://en.wikipedia.org/wiki/System">system</a> by 14.95 points.</abstract>
      <url hash="edfc157e">D19-5313</url>
      <doi>10.18653/v1/D19-5313</doi>
      <bibkey>das-etal-2019-chains</bibkey>
    </paper>
    <paper id="18">
      <title>Graph-Based Semi-Supervised Learning for Natural Language Understanding</title>
      <author><first>Zimeng</first><last>Qiu</last></author>
      <author><first>Eunah</first><last>Cho</last></author>
      <author><first>Xiaochun</first><last>Ma</last></author>
      <author><first>William</first><last>Campbell</last></author>
      <pages>151–158</pages>
      <abstract>Semi-supervised learning is an efficient method to augment training data automatically from unlabeled data. Development of many natural language understanding (NLU) applications has a challenge where unlabeled data is relatively abundant while labeled data is rather limited. In this work, we propose transductive graph-based semi-supervised learning models as well as their inductive variants for NLU. We evaluate the approach’s applicability using publicly available NLU data and models. In order to find similar utterances and construct a <a href="https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)">graph</a>, we use a paraphrase detection model. Results show that applying the inductive graph-based semi-supervised learning can improve the <a href="https://en.wikipedia.org/wiki/Errors_and_residuals">error rate</a> of the NLU model by 5 %.</abstract>
      <url hash="bf59ac80">D19-5318</url>
      <attachment hash="22faa5f9">D19-5318.Attachment.zip</attachment>
      <doi>10.18653/v1/D19-5318</doi>
      <bibkey>qiu-etal-2019-graph</bibkey>
    </paper>
    <paper id="19">
      <title>Graph Enhanced Cross-Domain Text-to-SQL Generation<fixed-case>SQL</fixed-case> Generation</title>
      <author><first>Siyu</first><last>Huo</last></author>
      <author><first>Tengfei</first><last>Ma</last></author>
      <author><first>Jie</first><last>Chen</last></author>
      <author><first>Maria</first><last>Chang</last></author>
      <author><first>Lingfei</first><last>Wu</last></author>
      <author><first>Michael</first><last>Witbrock</last></author>
      <pages>159–163</pages>
      <abstract>Semantic parsing is a fundamental problem in <a href="https://en.wikipedia.org/wiki/Natural-language_understanding">natural language understanding</a>, as it involves the mapping of natural language to structured forms such as <a href="https://en.wikipedia.org/wiki/Executable">executable queries</a> or <a href="https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning">logic-like knowledge representations</a>. Existing deep learning approaches for <a href="https://en.wikipedia.org/wiki/Semantic_parsing">semantic parsing</a> have shown promise on a variety of benchmark data sets, particularly on text-to-SQL parsing. However, most text-to-SQL parsers do not generalize to unseen data sets in different domains. In this paper, we propose a new cross-domain learning scheme to perform text-to-SQL translation and demonstrate its use on Spider, a large-scale cross-domain text-to-SQL data set. We improve upon a state-of-the-art Spider model, SyntaxSQLNet, by constructing a graph of column names for all databases and using graph neural networks to compute their embeddings. The resulting embeddings offer better cross-domain representations and <a href="https://en.wikipedia.org/wiki/SQL">SQL queries</a>, as evidenced by substantial improvement on the Spider data set compared to SyntaxSQLNet.</abstract>
      <url hash="0beb708a">D19-5319</url>
      <doi>10.18653/v1/D19-5319</doi>
      <bibkey>huo-etal-2019-graph</bibkey>
    </paper>
    <paper id="20">
      <title>Reasoning Over Paths via Knowledge Base Completion</title>
      <author><first>Saatviga</first><last>Sudhahar</last></author>
      <author><first>Andrea</first><last>Pierleoni</last></author>
      <author><first>Ian</first><last>Roberts</last></author>
      <pages>164–171</pages>
      <abstract>Reasoning over paths in large scale knowledge graphs is an important problem for many applications. In this paper we discuss a simple approach to automatically build and rank paths between a source and target entity pair with learned embeddings using a knowledge base completion model (KBC). We assembled a <a href="https://en.wikipedia.org/wiki/Knowledge_graph">knowledge graph</a> by mining the available <a href="https://en.wikipedia.org/wiki/Medical_literature">biomedical scientific literature</a> and extracted a set of high frequency paths to use for <a href="https://en.wikipedia.org/wiki/Data_validation">validation</a>. We demonstrate that our method is able to effectively rank a list of known paths between a pair of entities and also come up with plausible paths that are not present in the <a href="https://en.wikipedia.org/wiki/Knowledge_graph">knowledge graph</a>. For a given entity pair we are able to reconstruct the highest ranking path 60 % of the time within the top 10 ranked paths and achieve 49 % mean average precision. Our approach is compositional since any KBC model that can produce vector representations of entities can be used.</abstract>
      <url hash="1401c2e9">D19-5320</url>
      <doi>10.18653/v1/D19-5320</doi>
      <bibkey>sudhahar-etal-2019-reasoning</bibkey>
    </paper>
    <paper id="21">
      <title>Node Embeddings for Graph Merging : Case of Knowledge Graph Construction</title>
      <author><first>Ida</first><last>Szubert</last></author>
      <author><first>Mark</first><last>Steedman</last></author>
      <pages>172–176</pages>
      <abstract>Combining two <a href="https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)">graphs</a> requires merging the nodes which are counterparts of each other. In this process errors occur, resulting in incorrect merging or incorrect failure to merge. We find a high prevalence of such errors when using AskNET, an <a href="https://en.wikipedia.org/wiki/Algorithm">algorithm</a> for building Knowledge Graphs from <a href="https://en.wikipedia.org/wiki/Text_corpus">text corpora</a>. AskNET node matching method uses <a href="https://en.wikipedia.org/wiki/String_similarity">string similarity</a>, which we propose to replace with vector embedding similarity. We explore graph-based and word-based embedding models and show an overall <a href="https://en.wikipedia.org/wiki/Error_detection_and_correction">error reduction</a> of from 56 % to 23.6 %, with a reduction of over a half in both types of incorrect node matching.</abstract>
      <url hash="35af9257">D19-5321</url>
      <doi>10.18653/v1/D19-5321</doi>
      <bibkey>szubert-steedman-2019-node</bibkey>
    </paper>
    <paper id="22">
      <title>DBee : A Database for Creating and Managing Knowledge Graphs and Embeddings<fixed-case>DB</fixed-case>ee: A Database for Creating and Managing Knowledge Graphs and Embeddings</title>
      <author><first>Viktor</first><last>Schlegel</last></author>
      <author><first>André</first><last>Freitas</last></author>
      <pages>177–185</pages>
      <abstract>This paper describes DBee, a <a href="https://en.wikipedia.org/wiki/Database">database</a> to support the construction of data-intensive AI applications. DBee provides a unique <a href="https://en.wikipedia.org/wiki/Data_model">data model</a> which operates jointly over large-scale knowledge graphs (KGs) and embedding vector spaces (VSs). This model supports queries which exploit the semantic properties of both types of representations (KGs and VSs). Additionally, DBee aims to facilitate the construction of KGs and VSs, by providing a library of generators, which can be used to create, integrate and transform data into KGs and VSs.</abstract>
      <url hash="f9978e52">D19-5322</url>
      <doi>10.18653/v1/D19-5322</doi>
      <bibkey>schlegel-freitas-2019-dbee</bibkey>
    </paper>
    </volume>
  <volume id="54" ingest-date="2019-11-03">
    <meta>
      <booktitle>Proceedings of the 2nd Workshop on New Frontiers in Summarization</booktitle>
      <url hash="f86f0bd9">D19-54</url>
      <editor><first>Lu</first><last>Wang</last></editor>
      <editor><first>Jackie Chi Kit</first><last>Cheung</last></editor>
      <editor><first>Giuseppe</first><last>Carenini</last></editor>
      <editor id="fei-liu-utdallas"><first>Fei</first><last>Liu</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Hong Kong, China</address>
      <month>November</month>
      <year>2019</year>
    </meta>
    <frontmatter>
      <url hash="06c37bad">D19-5400</url>
      <bibkey>emnlp-2019-new</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Answering Naturally : Factoid to Full length Answer Generation</title>
      <author><first>Vaishali</first><last>Pal</last></author>
      <author><first>Manish</first><last>Shrivastava</last></author>
      <author><first>Irshad</first><last>Bhat</last></author>
      <pages>1–9</pages>
      <abstract>In recent years, the task of Question Answering over passages, also pitched as a <a href="https://en.wikipedia.org/wiki/Reading_comprehension">reading comprehension</a>, has evolved into a very active research area. A reading comprehension system extracts a span of text, comprising of <a href="https://en.wikipedia.org/wiki/Named_entity">named entities</a>, <a href="https://en.wikipedia.org/wiki/Calendar_date">dates</a>, <a href="https://en.wikipedia.org/wiki/Phrase">small phrases</a>, etc., which serve as the answer to a given question. However, these spans of text would result in an unnatural reading experience in a conversational system. Usually, <a href="https://en.wikipedia.org/wiki/Dialogue_system">dialogue systems</a> solve this issue by using template-based language generation. These <a href="https://en.wikipedia.org/wiki/System">systems</a>, though adequate for a domain specific task, are too restrictive and predefined for a domain independent system. In order to present the user with a more conversational experience, we propose a pointer generator based full-length answer generator which can be used with most QA systems. Our system generates a full length answer given a question and the extracted factoid / span answer without relying on the passage from where the answer was extracted. We also present a <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> of 315000 question, factoid answer and full length answer triples. We have evaluated our system using ROUGE-1,2,L and BLEU and achieved 74.05 BLEU score and 86.25 Rogue-L score.</abstract>
      <url hash="be124353">D19-5401</url>
      <doi>10.18653/v1/D19-5401</doi>
      <bibkey>pal-etal-2019-answering</bibkey>
    </paper>
    <paper id="3">
      <title>Abstractive Timeline Summarization</title>
      <author><first>Julius</first><last>Steen</last></author>
      <author><first>Katja</first><last>Markert</last></author>
      <pages>21–31</pages>
      <abstract>Timeline summarization (TLS) automatically identifies key dates of major events and provides short descriptions of what happened on these dates. Previous approaches to <a href="https://en.wikipedia.org/wiki/Transport_Layer_Security">TLS</a> have focused on extractive methods. In contrast, we suggest an abstractive timeline summarization system. Our system is entirely unsupervised, which makes it especially suited to TLS where there are very few gold summaries available for training of supervised systems. In addition, we present the first abstractive oracle experiments for <a href="https://en.wikipedia.org/wiki/Transport_Layer_Security">TLS</a>. Our system outperforms extractive competitors in terms of ROUGE when the number of input documents is high and the output requires strong <a href="https://en.wikipedia.org/wiki/Data_compression">compression</a>. In these cases, our oracle experiments confirm that our approach also has a higher <a href="https://en.wikipedia.org/wiki/Upper_and_lower_bounds">upper bound</a> for ROUGE scores than extractive methods. A study with human judges shows that our abstractive system also produces output that is easy to read and understand.</abstract>
      <url hash="d649a1aa">D19-5403</url>
      <doi>10.18653/v1/D19-5403</doi>
      <bibkey>steen-markert-2019-abstractive</bibkey>
    </paper>
    <paper id="4">
      <title>Learning to Create Sentence Semantic Relation Graphs for Multi-Document Summarization</title>
      <author><first>Diego</first><last>Antognini</last></author>
      <author><first>Boi</first><last>Faltings</last></author>
      <pages>32–41</pages>
      <abstract>Linking facts across documents is a challenging <a href="https://en.wikipedia.org/wiki/Task_(project_management)">task</a>, as the language used to express the same information in a sentence can vary significantly, which complicates the task of <a href="https://en.wikipedia.org/wiki/Multi-document_summarization">multi-document summarization</a>. Consequently, existing approaches heavily rely on hand-crafted features, which are domain-dependent and hard to craft, or additional annotated data, which is costly to gather. To overcome these limitations, we present a novel method, which makes use of two types of sentence embeddings : universal embeddings, which are trained on a large unrelated corpus, and domain-specific embeddings, which are learned during training. To this end, we develop SemSentSum, a fully data-driven model able to leverage both types of sentence embeddings by building a sentence semantic relation graph. SemSentSum achieves competitive results on two types of <a href="https://en.wikipedia.org/wiki/Abstract_(summary)">summary</a>, consisting of 665 bytes and 100 words. Unlike other state-of-the-art <a href="https://en.wikipedia.org/wiki/Conceptual_model">models</a>, neither hand-crafted features nor additional annotated data are necessary, and the <a href="https://en.wikipedia.org/wiki/Method_(computer_programming)">method</a> is easily adaptable for other tasks. To our knowledge, we are the first to use multiple sentence embeddings for the task of <a href="https://en.wikipedia.org/wiki/Multi-document_summarization">multi-document summarization</a>.</abstract>
      <url hash="cbf6538b">D19-5404</url>
      <doi>10.18653/v1/D19-5404</doi>
      <bibkey>antognini-faltings-2019-learning</bibkey>
    </paper>
    <paper id="8">
      <title>Towards Annotating and Creating Summary Highlights at Sub-sentence Level</title>
      <author><first>Kristjan</first><last>Arumae</last></author>
      <author><first>Parminder</first><last>Bhatia</last></author>
      <author id="fei-liu-utdallas"><first>Fei</first><last>Liu</last></author>
      <pages>64–69</pages>
      <abstract>Highlighting is a powerful tool to pick out important content and emphasize. Creating summary highlights at the sub-sentence level is particularly desirable, because sub-sentences are more concise than whole sentences. They are also better suited than individual words and phrases that can potentially lead to disfluent, fragmented summaries. In this paper we seek to generate summary highlights by annotating summary-worthy sub-sentences and teaching <a href="https://en.wikipedia.org/wiki/Classifier_(linguistics)">classifiers</a> to do the same. We frame the task as jointly selecting important sentences and identifying a single most informative textual unit from each sentence. This <a href="https://en.wikipedia.org/wiki/Formulation">formulation</a> dramatically reduces the <a href="https://en.wikipedia.org/wiki/Complexity">task complexity</a> involved in sentence compression. Our study provides new benchmarks and baselines for generating highlights at the <a href="https://en.wikipedia.org/wiki/Sentence_(linguistics)">sub-sentence level</a>.</abstract>
      <url hash="d5619bac">D19-5408</url>
      <doi>10.18653/v1/D19-5408</doi>
      <bibkey>arumae-etal-2019-towards</bibkey>
    </paper>
    <paper id="9">
      <title>SAMSum Corpus : A Human-annotated Dialogue Dataset for Abstractive Summarization<fixed-case>SAMS</fixed-case>um Corpus: A Human-annotated Dialogue Dataset for Abstractive Summarization</title>
      <author><first>Bogdan</first><last>Gliwa</last></author>
      <author><first>Iwona</first><last>Mochol</last></author>
      <author><first>Maciej</first><last>Biesek</last></author>
      <author><first>Aleksander</first><last>Wawer</last></author>
      <pages>70–79</pages>
      <abstract>This paper introduces the SAMSum Corpus, a new <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> with abstractive dialogue summaries. We investigate the challenges it poses for <a href="https://en.wikipedia.org/wiki/Automatic_summarization">automated summarization</a> by testing several <a href="https://en.wikipedia.org/wiki/Statistical_model">models</a> and comparing their results with those obtained on a <a href="https://en.wikipedia.org/wiki/Text_corpus">corpus of news articles</a>. We show that model-generated summaries of dialogues achieve higher ROUGE scores than the model-generated summaries of news   in contrast with human evaluators’ judgement. This suggests that a challenging task of abstractive dialogue summarization requires dedicated <a href="https://en.wikipedia.org/wiki/Conceptual_model">models</a> and non-standard <a href="https://en.wikipedia.org/wiki/Quality_(business)">quality measures</a>. To our knowledge, our study is the first attempt to introduce a high-quality chat-dialogues corpus, manually annotated with abstractive summarizations, which can be used by the research community for further studies.</abstract>
      <url hash="ffe6453f">D19-5409</url>
      <doi>10.18653/v1/D19-5409</doi>
      <bibkey>gliwa-etal-2019-samsum</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/samsum-corpus">SAMSum Corpus</pwcdataset>
    </paper>
    <paper id="10">
      <title>A Closer Look at Data Bias in Neural Extractive Summarization Models</title>
      <author><first>Ming</first><last>Zhong</last></author>
      <author><first>Danqing</first><last>Wang</last></author>
      <author><first>Pengfei</first><last>Liu</last></author>
      <author><first>Xipeng</first><last>Qiu</last></author>
      <author><first>Xuanjing</first><last>Huang</last></author>
      <pages>80–89</pages>
      <abstract>In this paper, we take stock of the current state of summarization datasets and explore how different factors of datasets influence the generalization behaviour of neural extractive summarization models. Specifically, we first propose several properties of datasets, which matter for the generalization of summarization models. Then we build the connection between <a href="https://en.wikipedia.org/wiki/Prior_probability">priors</a> residing in datasets and model designs, analyzing how different properties of datasets influence the choices of model structure design and training methods. Finally, by taking a typical dataset as an example, we rethink the process of the model design based on the experience of the above analysis. We demonstrate that when we have a deep understanding of the characteristics of datasets, a simple approach can bring significant improvements to the existing state-of-the-art model.</abstract>
      <url hash="17289290">D19-5410</url>
      <doi>10.18653/v1/D19-5410</doi>
      <bibkey>zhong-etal-2019-closer</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/cnn-daily-mail-1">CNN/Daily Mail</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/newsroom">NEWSROOM</pwcdataset>
    </paper>
    <paper id="13">
      <title>Analyzing Sentence Fusion in Abstractive Summarization</title>
      <author><first>Logan</first><last>Lebanoff</last></author>
      <author><first>John</first><last>Muchovej</last></author>
      <author><first>Franck</first><last>Dernoncourt</last></author>
      <author><first>Doo Soon</first><last>Kim</last></author>
      <author><first>Seokhwan</first><last>Kim</last></author>
      <author><first>Walter</first><last>Chang</last></author>
      <author id="fei-liu-utdallas"><first>Fei</first><last>Liu</last></author>
      <pages>104–110</pages>
      <abstract>While recent work in abstractive summarization has resulted in higher scores in automatic metrics, there is little understanding on how these systems combine information taken from multiple document sentences. In this paper, we analyze the outputs of five state-of-the-art abstractive summarizers, focusing on summary sentences that are formed by sentence fusion. We ask assessors to judge the <a href="https://en.wikipedia.org/wiki/Grammaticality">grammaticality</a>, <a href="https://en.wikipedia.org/wiki/Faithfulness">faithfulness</a>, and method of fusion for summary sentences. Our analysis reveals that system sentences are mostly grammatical, but often fail to remain faithful to the original article.</abstract>
      <url hash="311e2718">D19-5413</url>
      <doi>10.18653/v1/D19-5413</doi>
      <bibkey>lebanoff-etal-2019-analyzing</bibkey>
    </paper>
    <paper id="14">
      <title>Summarizing Relationships for Interactive Concept Map Browsers</title>
      <author><first>Abram</first><last>Handler</last></author>
      <author><first>Premkumar</first><last>Ganeshkumar</last></author>
      <author><first>Brendan</first><last>O’Connor</last></author>
      <author><first>Mohamed</first><last>AlTantawy</last></author>
      <pages>111–115</pages>
      <abstract>Concept maps are visual summaries, structured as <a href="https://en.wikipedia.org/wiki/Directed_graph">directed graphs</a> : important concepts from a dataset are displayed as vertexes, and edges between vertexes show natural language descriptions of the relationships between the concepts on the map. Thus far, preliminary attempts at automatically creating concept maps have focused on building static summaries. However, in interactive settings, users will need to dynamically investigate particular relationships between pairs of concepts. For instance, a historian using a concept map browser might decide to investigate the relationship between two politicians in a <a href="https://en.wikipedia.org/wiki/Archive">news archive</a>. We present a <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> which responds to such queries by returning one or more short, importance-ranked, natural language descriptions of the relationship between two requested concepts, for display in a <a href="https://en.wikipedia.org/wiki/User_interface">visual interface</a>. Our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> is trained on a new public dataset, collected for this <a href="https://en.wikipedia.org/wiki/Task_(project_management)">task</a>.</abstract>
      <url hash="1eea8602">D19-5414</url>
      <doi>10.18653/v1/D19-5414</doi>
      <bibkey>handler-etal-2019-summarizing</bibkey>
    </paper>
    </volume>
  <volume id="55" ingest-date="2019-11-03">
    <meta>
      <booktitle>Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT 2019)</booktitle>
      <url hash="ccc8e3df">D19-55</url>
      <editor><first>Wei</first><last>Xu</last></editor>
      <editor><first>Alan</first><last>Ritter</last></editor>
      <editor><first>Tim</first><last>Baldwin</last></editor>
      <editor><first>Afshin</first><last>Rahimi</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Hong Kong, China</address>
      <month>November</month>
      <year>2019</year>
    </meta>
    <frontmatter>
      <url hash="ed584fbe">D19-5500</url>
      <bibkey>emnlp-2019-noisy</bibkey>
    </frontmatter>
    <paper id="2">
      <title>Formality Style Transfer for Noisy, User-generated Conversations : Extracting Labeled, Parallel Data from Unlabeled Corpora</title>
      <author><first>Isak</first><last>Czeresnia Etinger</last></author>
      <author><first>Alan W</first><last>Black</last></author>
      <pages>11–16</pages>
      <abstract>Typical <a href="https://en.wikipedia.org/wiki/Data_set">datasets</a> used for style transfer in <a href="https://en.wikipedia.org/wiki/Natural_language_processing">NLP</a> contain aligned pairs of two opposite extremes of a style. As each existing <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> is sourced from a specific domain and context, most use cases will have a sizable mismatch from the vocabulary and sentence structures of any <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> available. This reduces the performance of the style transfer, and is particularly significant for noisy, user-generated text. To solve this problem, we show a technique to derive a dataset of aligned pairs (style-agnostic vs stylistic sentences) from an unlabeled corpus by using an auxiliary dataset, allowing for in-domain training. We test the technique with the Yahoo Formality Dataset and 6 novel datasets we produced, which consist of scripts from 5 popular TV-shows (Friends, Futurama, Seinfeld, Southpark, Stargate SG-1) and the Slate Star Codex online forum. We gather 1080 human evaluations, which show that our method produces a sizable change in formality while maintaining fluency and context ; and that it considerably outperforms OpenNMT’s Seq2Seq model directly trained on the Yahoo Formality Dataset. Additionally, we publish the full pipeline code and our novel <a href="https://en.wikipedia.org/wiki/Data_set">datasets</a>.</abstract>
      <url hash="a15b8d08">D19-5502</url>
      <attachment hash="b10aee2c">D19-5502.Attachment.zip</attachment>
      <doi>10.18653/v1/D19-5502</doi>
      <bibkey>czeresnia-etinger-black-2019-formality</bibkey>
    </paper>
    <paper id="3">
      <title>Multilingual Whispers : Generating Paraphrases with Translation</title>
      <author><first>Christian</first><last>Federmann</last></author>
      <author><first>Oussama</first><last>Elachqar</last></author>
      <author><first>Chris</first><last>Quirk</last></author>
      <pages>17–26</pages>
      <abstract>Naturally occurring paraphrase data, such as multiple <a href="https://en.wikipedia.org/wiki/Article_(publishing)">news stories</a> about the same event, is a useful but rare resource. This paper compares translation-based paraphrase gathering using human, automatic, or hybrid techniques to monolingual paraphrasing by experts and non-experts. We gather <a href="https://en.wikipedia.org/wiki/Translation">translations</a>, <a href="https://en.wikipedia.org/wiki/Paraphrase">paraphrases</a>, and empirical human quality assessments of these approaches. Neural machine translation techniques, especially when pivoting through related languages, provide a relatively robust source of <a href="https://en.wikipedia.org/wiki/Paraphrase">paraphrases</a> with diversity comparable to expert human paraphrases. Surprisingly, human translators do not reliably outperform <a href="https://en.wikipedia.org/wiki/Nervous_system">neural systems</a>. The resulting data release will not only be a useful test set, but will also allow additional explorations in translation and paraphrase quality assessments and relationships.</abstract>
      <url hash="541e928c">D19-5503</url>
      <doi>10.18653/v1/D19-5503</doi>
      <bibkey>federmann-etal-2019-multilingual</bibkey>
    </paper>
    <paper id="4">
      <title>Personalizing Grammatical Error Correction : Adaptation to Proficiency Level and L1<fixed-case>L</fixed-case>1</title>
      <author><first>Maria</first><last>Nadejde</last></author>
      <author><first>Joel</first><last>Tetreault</last></author>
      <pages>27–33</pages>
      <abstract>Grammar error correction (GEC) systems have become ubiquitous in a variety of <a href="https://en.wikipedia.org/wiki/Application_software">software applications</a>, and have started to approach human-level performance for some datasets. However, very little is known about how to efficiently personalize these <a href="https://en.wikipedia.org/wiki/System">systems</a> to the user’s characteristics, such as their proficiency level and first language, or to emerging domains of text. We present the first results on adapting a general purpose neural GEC system to both the proficiency level and the first language of a writer, using only a few thousand annotated sentences. Our study is the broadest of its kind, covering five proficiency levels and twelve different languages, and comparing three different adaptation scenarios : adapting to the proficiency level only, to the first language only, or to both aspects simultaneously. We show that tailoring to both scenarios achieves the largest performance improvement (3.6 F0.5) relative to a strong <a href="https://en.wikipedia.org/wiki/Baseline_(configuration_management)">baseline</a>.</abstract>
      <url hash="0897e58b">D19-5504</url>
      <attachment hash="17f60f91">D19-5504.Attachment.pdf</attachment>
      <doi>10.18653/v1/D19-5504</doi>
      <bibkey>nadejde-tetreault-2019-personalizing</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/fce">FCE</pwcdataset>
    </paper>
    <paper id="6">
      <title>Training on Synthetic Noise Improves Robustness to Natural Noise in Machine Translation</title>
      <author><first>Vladimir</first><last>Karpukhin</last></author>
      <author><first>Omer</first><last>Levy</last></author>
      <author><first>Jacob</first><last>Eisenstein</last></author>
      <author><first>Marjan</first><last>Ghazvininejad</last></author>
      <pages>42–47</pages>
      <abstract>Contemporary machine translation systems achieve greater coverage by applying subword models such as BPE and character-level CNNs, but these methods are highly sensitive to orthographical variations such as spelling mistakes. We show how training on a mild amount of random synthetic noise can dramatically improve <a href="https://en.wikipedia.org/wiki/Robust_statistics">robustness</a> to these variations, without diminishing performance on clean text. We focus on <a href="https://en.wikipedia.org/wiki/Translation_(biology)">translation</a> performance on natural typos, and show that robustness to such <a href="https://en.wikipedia.org/wiki/Noise">noise</a> can be achieved using a balanced diet of simple synthetic noises at training time, without access to the natural noise data or distribution.</abstract>
      <url hash="5550e86c">D19-5506</url>
      <doi>10.18653/v1/D19-5506</doi>
      <bibkey>karpukhin-etal-2019-training</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/mtnt">MTNT</pwcdataset>
    </paper>
    <paper id="8">
      <title>Tkol, Httt, and r / radiohead : High Affinity Terms in Reddit Communities<fixed-case>R</fixed-case>eddit Communities</title>
      <author><first>Abhinav</first><last>Bhandari</last></author>
      <author><first>Caitrin</first><last>Armstrong</last></author>
      <pages>57–67</pages>
      <abstract>Language is an important marker of a <a href="https://en.wikipedia.org/wiki/Cultural_group">cultural group</a>, large or small. One aspect of language variation between communities is the employment of highly specialized terms with unique significance to the group. We study these high affinity terms across a wide variety of communities by leveraging the rich diversity of <a href="https://en.wikipedia.org/wiki/Reddit">Reddit.com</a>. We provide a systematic exploration of high affinity terms, the often rapid semantic shifts they undergo, and their relationship to subreddit characteristics across 2600 diverse subreddits. Our results show that high affinity terms are effective signals of loyal communities, they undergo more semantic shift than low affinity terms, and that they are partial barrier to entry for new users. We conclude that <a href="https://en.wikipedia.org/wiki/Reddit">Reddit</a> is a robust and valuable data source for testing further theories about high affinity terms across communities.</abstract>
      <url hash="7eb2c99a">D19-5508</url>
      <doi>10.18653/v1/D19-5508</doi>
      <bibkey>bhandari-armstrong-2019-tkol</bibkey>
    </paper>
    <paper id="11">
      <title>Predicting Algorithm Classes for Programming Word Problems</title>
      <author><first>Vinayak</first><last>Athavale</last></author>
      <author><first>Aayush</first><last>Naik</last></author>
      <author><first>Rajas</first><last>Vanjape</last></author>
      <author><first>Manish</first><last>Shrivastava</last></author>
      <pages>84–93</pages>
      <abstract>We introduce the task of algorithm class prediction for programming word problems. A programming word problem is a problem written in <a href="https://en.wikipedia.org/wiki/Natural_language">natural language</a>, which can be solved using an <a href="https://en.wikipedia.org/wiki/Algorithm">algorithm</a> or a <a href="https://en.wikipedia.org/wiki/Computer_program">program</a>. We define <a href="https://en.wikipedia.org/wiki/Class_(computer_programming)">classes</a> of various programming word problems which correspond to the class of algorithms required to solve the <a href="https://en.wikipedia.org/wiki/Problem_solving">problem</a>. We present four new <a href="https://en.wikipedia.org/wiki/Data_set">datasets</a> for this task, two multiclass datasets with 550 and 1159 problems each and two multilabel datasets having 3737 and 3960 problems each. We pose the problem as a text classification problem and train <a href="https://en.wikipedia.org/wiki/Neural_network">neural network</a> and non-neural network based models on this task. Our best performing <a href="https://en.wikipedia.org/wiki/Statistical_classification">classifier</a> gets an <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> of 62.7 percent for the multiclass case on the five class classification dataset, Codeforces Multiclass-5 (CFMC5). We also do some human-level analysis and compare human performance with that of our text classification models. Our best <a href="https://en.wikipedia.org/wiki/Statistical_classification">classifier</a> has an <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> only 9 percent lower than that of a human on this task. To the best of our knowledge, these are the first reported results on such a <a href="https://en.wikipedia.org/wiki/Task_(project_management)">task</a>. We make our code and datasets publicly available.</abstract>
      <url hash="80bb0005">D19-5511</url>
      <doi>10.18653/v1/D19-5511</doi>
      <bibkey>athavale-etal-2019-predicting</bibkey>
    </paper>
    <paper id="12">
      <title>Automatic identification of writers’ intentions : Comparing different methods for predicting relationship goals in online dating profile texts</title>
      <author><first>Chris</first><last>van der Lee</last></author>
      <author><first>Tess</first><last>van der Zanden</last></author>
      <author><first>Emiel</first><last>Krahmer</last></author>
      <author><first>Maria</first><last>Mos</last></author>
      <author><first>Alexander</first><last>Schouten</last></author>
      <pages>94–100</pages>
      <abstract>Psychologically motivated, lexicon-based text analysis methods such as LIWC (Pennebaker et al., 2015) have been criticized by computational linguists for their lack of adaptability, but they have not often been systematically compared with either human evaluations or machine learning approaches. The goal of the current study was to assess the effectiveness and predictive ability of LIWC on a relationship goal classification task. In this paper, we compared the outcomes of (1) LIWC, (2) <a href="https://en.wikipedia.org/wiki/Machine_learning">machine learning</a>, and (3) a human baseline. A newly collected corpus of online dating profile texts (a genre not explored before in the ACL anthology) was used, accompanied by the profile writers’ self-selected relationship goal (long-term versus date). These three approaches were tested by comparing their performance on identifying both the intended relationship goal and content-related text labels. Results show that LIWC and <a href="https://en.wikipedia.org/wiki/Machine_learning">machine learning models</a> correlate with <a href="https://en.wikipedia.org/wiki/Evaluation">human evaluations</a> in terms of content-related labels. LIWC’s content-related labels corresponded more strongly to humans than those of the <a href="https://en.wikipedia.org/wiki/Classifier_(linguistics)">classifier</a>. Moreover, all <a href="https://en.wikipedia.org/wiki/Methodology">approaches</a> were similarly accurate in predicting the <a href="https://en.wikipedia.org/wiki/Goal">relationship goal</a>.</abstract>
      <url hash="b9687f74">D19-5512</url>
      <attachment hash="bc492a81">D19-5512.Attachment.pdf</attachment>
      <doi>10.18653/v1/D19-5512</doi>
      <bibkey>van-der-lee-etal-2019-automatic</bibkey>
    </paper>
    <paper id="13">
      <title>Contextualized Word Representations from Distant Supervision with and for NER<fixed-case>NER</fixed-case></title>
      <author><first>Abbas</first><last>Ghaddar</last></author>
      <author><first>Phillippe</first><last>Langlais</last></author>
      <pages>101–108</pages>
      <abstract>We describe a special type of deep contextualized word representation that is learned from distant supervision annotations and dedicated to <a href="https://en.wikipedia.org/wiki/Named-entity_recognition">named entity recognition</a>. Our extensive experiments on 7 <a href="https://en.wikipedia.org/wiki/Data_set">datasets</a> show systematic gains across all domains over strong baselines, and demonstrate that our representation is complementary to previously proposed <a href="https://en.wikipedia.org/wiki/Embedding">embeddings</a>. We report new state-of-the-art results on CONLL and ONTONOTES datasets.</abstract>
      <url hash="58d90f09">D19-5513</url>
      <doi>10.18653/v1/D19-5513</doi>
      <bibkey>ghaddar-langlais-2019-contextualized</bibkey>
    </paper>
    <paper id="15">
      <title>An In-depth Analysis of the Effect of Lexical Normalization on the Dependency Parsing of <a href="https://en.wikipedia.org/wiki/Social_media">Social Media</a></title>
      <author><first>Rob</first><last>van der Goot</last></author>
      <pages>115–120</pages>
      <abstract>Existing <a href="https://en.wikipedia.org/wiki/Natural_language_processing">natural language processing systems</a> have often been designed with standard texts in mind. However, when these <a href="https://en.wikipedia.org/wiki/Tool">tools</a> are used on the substantially different texts from <a href="https://en.wikipedia.org/wiki/Social_media">social media</a>, their performance drops dramatically. One solution is to translate social media data to standard language before processing, this is also called normalization. It is well-known that this improves performance for many natural language processing tasks on social media data. However, little is known about which types of <a href="https://en.wikipedia.org/wiki/Normalization_(statistics)">normalization replacements</a> have the most effect. Furthermore, it is unknown what the weaknesses of existing lexical normalization systems are in an extrinsic setting. In this paper, we analyze the effect of manual as well as automatic lexical normalization for dependency parsing. After our analysis, we conclude that for most categories, automatic normalization scores close to manually annotated normalization and that small annotation differences are important to take into consideration when exploiting <a href="https://en.wikipedia.org/wiki/Normalization_(statistics)">normalization</a> in a pipeline setup.</abstract>
      <url hash="99948515">D19-5515</url>
      <attachment hash="a8e786f3">D19-5515.Attachment.zip</attachment>
      <doi>10.18653/v1/D19-5515</doi>
      <bibkey>van-der-goot-2019-depth</bibkey>
    </paper>
    <paper id="18">
      <title>Normalising Non-standardised Orthography in Algerian Code-switched User-generated Data<fixed-case>A</fixed-case>lgerian Code-switched User-generated Data</title>
      <author><first>Wafia</first><last>Adouane</last></author>
      <author><first>Jean-Philippe</first><last>Bernardy</last></author>
      <author><first>Simon</first><last>Dobnik</last></author>
      <pages>131–140</pages>
      <abstract>We work with <a href="https://en.wikipedia.org/wiki/Algerian_language">Algerian</a>, an under-resourced non-standardised Arabic variety, for which we compile a new parallel corpus consisting of user-generated textual data matched with normalised and corrected human annotations following data-driven and our linguistically motivated standard. We use an end-to-end deep neural model designed to deal with context-dependent spelling correction and <a href="https://en.wikipedia.org/wiki/Normalization_(statistics)">normalisation</a>. Results indicate that a <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> with two CNN sub-network encoders and an LSTM decoder performs the best, and that <a href="https://en.wikipedia.org/wiki/Context_(language_use)">word context</a> matters. Additionally, pre-processing data token-by-token with an edit-distance based aligner significantly improves the performance. We get promising results for the spelling correction and normalisation, as a pre-processing step for downstream tasks, on detecting binary Semantic Textual Similarity.</abstract>
      <url hash="030b051b">D19-5518</url>
      <doi>10.18653/v1/D19-5518</doi>
      <bibkey>adouane-etal-2019-normalising</bibkey>
    </paper>
    <paper id="19">
      <title>Dialect Text Normalization to Normative Standard Finnish<fixed-case>F</fixed-case>innish</title>
      <author><first>Niko</first><last>Partanen</last></author>
      <author><first>Mika</first><last>Hämäläinen</last></author>
      <author><first>Khalid</first><last>Alnajjar</last></author>
      <pages>141–146</pages>
      <abstract>We compare different LSTMs and transformer models in terms of their effectiveness in normalizing <a href="https://en.wikipedia.org/wiki/Finnish_dialects">dialectal Finnish</a> into the normative standard Finnish. As <a href="https://en.wikipedia.org/wiki/Dialect">dialect</a> is the common way of communication for people online in Finnish, such a normalization is a necessary step to improve the accuracy of the existing Finnish NLP tools that are tailored for normative Finnish text. We work on a <a href="https://en.wikipedia.org/wiki/Text_corpus">corpus</a> consisting of <a href="https://en.wikipedia.org/wiki/Dialect_continuum">dialectal data</a> of 23 distinct <a href="https://en.wikipedia.org/wiki/Finnish_dialects">Finnish dialects</a>. The best functioning BRNN approach lowers the initial <a href="https://en.wikipedia.org/wiki/Word_error_rate">word error rate</a> of the <a href="https://en.wikipedia.org/wiki/Text_corpus">corpus</a> from 52.89 to 5.73.</abstract>
      <url hash="d5ccd4d6">D19-5519</url>
      <doi>10.18653/v1/D19-5519</doi>
      <bibkey>partanen-etal-2019-dialect</bibkey>
      <pwccode url="https://github.com/mikahama/murre" additional="false">mikahama/murre</pwccode>
    </paper>
    <paper id="21">
      <title>Exploring Multilingual Syntactic Sentence Representations</title>
      <author><first>Chen</first><last>Liu</last></author>
      <author><first>Anderson</first><last>De Andrade</last></author>
      <author><first>Muhammad</first><last>Osama</last></author>
      <pages>153–159</pages>
      <abstract>We study <a href="https://en.wikipedia.org/wiki/Methodology">methods</a> for learning <a href="https://en.wikipedia.org/wiki/Sentence_embedding">sentence embeddings</a> with <a href="https://en.wikipedia.org/wiki/Syntax">syntactic structure</a>. We focus on methods of learning syntactic sentence-embeddings by using a multilingual parallel-corpus augmented by Universal Parts-of-Speech tags. We evaluate the quality of the learned embeddings by examining sentence-level nearest neighbours and functional dissimilarity in the <a href="https://en.wikipedia.org/wiki/Embedding">embedding space</a>. We also evaluate the ability of the method to learn syntactic sentence-embeddings for low-resource languages and demonstrate strong evidence for <a href="https://en.wikipedia.org/wiki/Transfer_learning">transfer learning</a>. Our results show that syntactic sentence-embeddings can be learned while using less training data, fewer model parameters, and resulting in better evaluation metrics than state-of-the-art language models.</abstract>
      <url hash="68cddc3d">D19-5521</url>
      <doi>10.18653/v1/D19-5521</doi>
      <bibkey>liu-etal-2019-exploring</bibkey>
      <pwccode url="https://github.com/ccliu2/syn-emb" additional="false">ccliu2/syn-emb</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/opensubtitles">OpenSubtitles</pwcdataset>
    </paper>
    <paper id="23">
      <title>Latent semantic network induction in the context of linked example senses</title>
      <author><first>Hunter</first><last>Heidenreich</last></author>
      <author><first>Jake</first><last>Williams</last></author>
      <pages>170–180</pages>
      <abstract>The <a href="https://en.wikipedia.org/wiki/Princeton_WordNet">Princeton WordNet</a> is a powerful tool for studying <a href="https://en.wikipedia.org/wiki/Language">language</a> and developing natural language processing algorithms. With significant work developing it further, one line considers its extension through aligning its expert-annotated structure with other lexical resources. In contrast, this work explores a completely data-driven approach to network construction, forming a <a href="https://en.wikipedia.org/wiki/Wordnet">wordnet</a> using the entirety of the open-source, noisy, user-annotated dictionary, <a href="https://en.wikipedia.org/wiki/Wiktionary">Wiktionary</a>. Comparing baselines to <a href="https://en.wikipedia.org/wiki/WordNet">WordNet</a>, we find compelling evidence that our <a href="https://en.wikipedia.org/wiki/Social_network">network induction process</a> constructs a <a href="https://en.wikipedia.org/wiki/Social_network">network</a> with useful semantic structure. With thousands of semantically-linked examples that demonstrate sense usage from basic lemmas to multiword expressions (MWEs), we believe this work motivates future research.</abstract>
      <url hash="f3686011">D19-5523</url>
      <doi>10.18653/v1/D19-5523</doi>
      <bibkey>heidenreich-williams-2019-latent</bibkey>
    </paper>
    <paper id="25">
      <title>Modelling Uncertainty in Collaborative Document Quality Assessment</title>
      <author><first>Aili</first><last>Shen</last></author>
      <author><first>Daniel</first><last>Beck</last></author>
      <author><first>Bahar</first><last>Salehi</last></author>
      <author><first>Jianzhong</first><last>Qi</last></author>
      <author><first>Timothy</first><last>Baldwin</last></author>
      <pages>191–201</pages>
      <abstract>In the context of document quality assessment, previous work has mainly focused on predicting the quality of a document relative to a putative gold standard, without paying attention to the subjectivity of this task. To imitate people’s disagreement over inherently subjective tasks such as rating the quality of a Wikipedia article, a document quality assessment system should provide not only a prediction of the article quality but also the uncertainty over its predictions. This motivates us to measure the uncertainty in document quality predictions, in addition to making the label prediction. Experimental results show that both Gaussian processes (GPs) and random forests (RFs) can yield competitive results in predicting the quality of Wikipedia articles, while providing an estimate of uncertainty when there is inconsistency in the quality labels from the Wikipedia contributors. We additionally evaluate our methods in the context of a semi-automated document quality class assignment decision-making process, where there is asymmetric risk associated with overestimates and underestimates of document quality. Our experiments suggest that GPs provide more reliable estimates in this context.</abstract>
      <url hash="c882768c">D19-5525</url>
      <doi>10.18653/v1/D19-5525</doi>
      <bibkey>shen-etal-2019-modelling</bibkey>
    </paper>
    <paper id="26">
      <title>Conceptualisation and Annotation of Drug Nonadherence Information for Knowledge Extraction from Patient-Generated Texts</title>
      <author><first>Anja</first><last>Belz</last></author>
      <author><first>Richard</first><last>Hoile</last></author>
      <author><first>Elizabeth</first><last>Ford</last></author>
      <author><first>Azam</first><last>Mullick</last></author>
      <pages>202–211</pages>
      <abstract>Approaches to knowledge extraction (KE) in the health domain often start by annotating text to indicate the knowledge to be extracted, and then use the annotated text to train systems to perform the KE. This may work for annotat- ing named entities or other contiguous noun phrases (drugs, some drug effects), but be- comes increasingly difficult when items tend to be expressed across multiple, possibly non- contiguous, syntactic constituents (e.g. most descriptions of drug effects in user-generated text). Other issues include that it is not al- ways clear how <a href="https://en.wikipedia.org/wiki/Annotation">annotations</a> map to actionable insights, or how they scale up to, or can form part of, more complex KE tasks. This paper reports our efforts in developing an approach to extracting knowledge about drug nonadher- ence from health forums which led us to con- clude that development can not proceed in sep- arate steps but that all aspectsfrom concep- tualisation to annotation scheme development, annotation, KE system training and knowl- edge graph instantiationare interdependent and need to be co-developed. Our aim in this paper is two-fold : we describe a generally ap- plicable framework for developing a KE ap- proach, and present a specific KE approach, developed with the framework, for the task of gathering information about antidepressant drug nonadherence. We report the conceptual- isation, the annotation scheme, the annotated corpus, and an analysis of annotated texts.</abstract>
      <url hash="dea14260">D19-5526</url>
      <doi>10.18653/v1/D19-5526</doi>
      <bibkey>belz-etal-2019-conceptualisation</bibkey>
    </paper>
    <paper id="27">
      <title>What A Sunny Day : Toward Emoji-Sensitive Irony Detection</title>
      <author><first>Shirley Anugrah</first><last>Hayati</last></author>
      <author><first>Aditi</first><last>Chaudhary</last></author>
      <author><first>Naoki</first><last>Otani</last></author>
      <author><first>Alan W</first><last>Black</last></author>
      <pages>212–216</pages>
      <abstract>Irony detection is an important task with applications in identification of online abuse and <a href="https://en.wikipedia.org/wiki/Harassment">harassment</a>. With the ubiquitous use of <a href="https://en.wikipedia.org/wiki/Nonverbal_communication">non-verbal cues</a> such as <a href="https://en.wikipedia.org/wiki/Emoji">emojis</a> in <a href="https://en.wikipedia.org/wiki/Social_media">social media</a>, in this work we aim to study the role of these structures in irony detection. Since the existing irony detection datasets have 10 % ironic tweets with <a href="https://en.wikipedia.org/wiki/Emoji">emoji</a>, <a href="https://en.wikipedia.org/wiki/Statistical_classification">classifiers</a> trained on them are insensitive to <a href="https://en.wikipedia.org/wiki/Emoji">emojis</a>. We propose an <a href="https://en.wikipedia.org/wiki/Pipeline_(computing)">automated pipeline</a> for creating a more balanced dataset.</abstract>
      <url hash="7b83516c">D19-5527</url>
      <doi>10.18653/v1/D19-5527</doi>
      <bibkey>hayati-etal-2019-sunny</bibkey>
    </paper>
    <paper id="29">
      <title>Dense Node Representation for <a href="https://en.wikipedia.org/wiki/Geolocation">Geolocation</a></title>
      <author><first>Tommaso</first><last>Fornaciari</last></author>
      <author><first>Dirk</first><last>Hovy</last></author>
      <pages>224–230</pages>
      <abstract>Prior research has shown that <a href="https://en.wikipedia.org/wiki/Geolocation">geolocation</a> can be substantially improved by including user network information. While effective, it suffers from the <a href="https://en.wikipedia.org/wiki/Curse_of_dimensionality">curse of dimensionality</a>, since networks are usually represented as sparse adjacency matrices of connections, which grow exponentially with the number of users. In order to incorporate this <a href="https://en.wikipedia.org/wiki/Information">information</a>, we therefore need to limit the network size, in turn limiting performance and risking sample bias. In this paper, we address these limitations by instead using dense network representations. We explore two methods to learn continuous node representations from either 1) the network structure with node2vec (Grover and Leskovec, 2016), or 2) textual user mentions via doc2vec (Le and Mikolov, 2014). We combine both methods with input from <a href="https://en.wikipedia.org/wiki/Social_media">social media posts</a> in an attention-based convolutional neural network and evaluate the contribution of each component on <a href="https://en.wikipedia.org/wiki/Geolocation">geolocation</a> performance. Our method enables us to incorporate arbitrarily large networks in a fixed-length vector, without limiting the network size. Our <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> achieve competitive results with similar state-of-the-art methods, but with much fewer model parameters, while being applicable to <a href="https://en.wikipedia.org/wiki/Complex_network">networks</a> of virtually any size.</abstract>
      <url hash="04060539">D19-5529</url>
      <doi>10.18653/v1/D19-5529</doi>
      <bibkey>fornaciari-hovy-2019-dense</bibkey>
    </paper>
    <paper id="33">
      <title>Distant Supervised Relation Extraction with Separate Head-Tail CNN<fixed-case>CNN</fixed-case></title>
      <author><first>Rui</first><last>Xing</last></author>
      <author><first>Jie</first><last>Luo</last></author>
      <pages>249–258</pages>
      <abstract>Distant supervised relation extraction is an efficient and effective strategy to find relations between entities in texts. However, it inevitably suffers from mislabeling problem and the <a href="https://en.wikipedia.org/wiki/Noise_(signal_processing)">noisy data</a> will hinder the performance. In this paper, we propose the Separate Head-Tail Convolution Neural Network (SHTCNN), a novel neural relation extraction framework to alleviate this issue. In this method, we apply separate convolution and pooling to the head and tail entity respectively for extracting better semantic features of sentences, and coarse-to-fine strategy to filter out instances which do not have actual relations in order to alleviate noisy data issues. Experiments on a widely used dataset show that our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> achieves significant and consistent improvements in <a href="https://en.wikipedia.org/wiki/Relation_extraction">relation extraction</a> compared to statistical and vanilla CNN-based methods.</abstract>
      <url hash="caab7cbf">D19-5533</url>
      <doi>10.18653/v1/D19-5533</doi>
      <bibkey>xing-luo-2019-distant</bibkey>
    </paper>
    <paper id="36">
      <title>Benefits of Data Augmentation for NMT-based Text Normalization of <a href="https://en.wikipedia.org/wiki/User-generated_content">User-Generated Content</a><fixed-case>NMT</fixed-case>-based Text Normalization of User-Generated Content</title>
      <author><first>Claudia</first><last>Matos Veliz</last></author>
      <author><first>Orphee</first><last>De Clercq</last></author>
      <author><first>Veronique</first><last>Hoste</last></author>
      <pages>275–285</pages>
      <abstract>One of the most persistent characteristics of written user-generated content (UGC) is the use of non-standard words. This characteristic contributes to an increased difficulty to automatically process and analyze UGC. Text normalization is the task of transforming lexical variants to their canonical forms and is often used as a pre-processing step for conventional NLP tasks in order to overcome the performance drop that NLP systems experience when applied to UGC. In this work, we follow a Neural Machine Translation approach to <a href="https://en.wikipedia.org/wiki/Text_normalization">text normalization</a>. To train such an encoder-decoder model, large parallel training corpora of sentence pairs are required. However, obtaining large data sets with UGC and their normalized version is not trivial, especially for languages other than <a href="https://en.wikipedia.org/wiki/English_language">English</a>. In this paper, we explore how to overcome this data bottleneck for <a href="https://en.wikipedia.org/wiki/Dutch_language">Dutch</a>, a low-resource language. We start off with a small publicly available parallel Dutch data set comprising three UGC genres and compare two different approaches. The <a href="https://en.wikipedia.org/wiki/First_law_of_thermodynamics">first</a> is to manually normalize and add training data, a money and time-consuming task. The second approach is a set of data augmentation techniques which increase data size by converting existing resources into synthesized non-standard forms. Our results reveal that, while the different approaches yield similar results regarding the normalization issues in the test set, they also introduce a large amount of over-normalizations.</abstract>
      <url hash="27d47c70">D19-5536</url>
      <doi>10.18653/v1/D19-5536</doi>
      <bibkey>matos-veliz-etal-2019-benefits</bibkey>
    </paper>
    <paper id="40">
      <title>No, you’re not alone : A better way to find people with similar experiences on Reddit<fixed-case>R</fixed-case>eddit</title>
      <author><first>Zhilin</first><last>Wang</last></author>
      <author><first>Elena</first><last>Rastorgueva</last></author>
      <author><first>Weizhe</first><last>Lin</last></author>
      <author><first>Xiaodong</first><last>Wu</last></author>
      <pages>307–315</pages>
      <abstract>We present a probabilistic clustering algorithm that can help Reddit users to find posts that discuss experiences similar to their own. This <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a> is built upon the BERT Next Sentence Prediction model and reduces the <a href="https://en.wikipedia.org/wiki/Time_complexity">time complexity</a> for clustering all posts in a corpus from O(n2) to O(n) with respect to the number of posts. We demonstrate that such probabilistic clustering can yield a performance better than baseline clustering methods based on <a href="https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation">Latent Dirichlet Allocation</a> (Blei et al., 2003) and Word2Vec (Mikolov et al., 2013). Furthermore, there is a high degree of coherence between our probabilistic clustering and the exhaustive comparison O(n2) algorithm in which the similarity between every pair of posts is found. This makes the use of the BERT Next Sentence Prediction model more practical for unsupervised clustering tasks due to the high runtime overhead of each BERT computation.</abstract>
      <url hash="d77cdfd5">D19-5540</url>
      <doi>10.18653/v1/D19-5540</doi>
      <bibkey>wang-etal-2019-youre</bibkey>
    </paper>
    <paper id="41">
      <title>Improving Multi-label Emotion Classification by Integrating both General and Domain-specific Knowledge</title>
      <author><first>Wenhao</first><last>Ying</last></author>
      <author><first>Rong</first><last>Xiang</last></author>
      <author><first>Qin</first><last>Lu</last></author>
      <pages>316–321</pages>
      <abstract>Deep learning based general language models have achieved state-of-the-art results in many popular <a href="https://en.wikipedia.org/wiki/Task_(project_management)">tasks</a> such as <a href="https://en.wikipedia.org/wiki/Sentiment_analysis">sentiment analysis</a> and QA tasks. Text in domains like <a href="https://en.wikipedia.org/wiki/Social_media">social media</a> has its own salient characteristics. Domain knowledge should be helpful in domain relevant tasks. In this work, we devise a simple method to obtain <a href="https://en.wikipedia.org/wiki/Domain_knowledge">domain knowledge</a> and further propose a method to integrate <a href="https://en.wikipedia.org/wiki/Domain_knowledge">domain knowledge</a> with <a href="https://en.wikipedia.org/wiki/General_knowledge">general knowledge</a> based on deep language models to improve performance of <a href="https://en.wikipedia.org/wiki/Emotion_classification">emotion classification</a>. Experiments on Twitter data show that even though a deep language model fine-tuned by a target domain data has attained comparable results to that of previous state-of-the-art models, this fine-tuned model can still benefit from our extracted <a href="https://en.wikipedia.org/wiki/Domain_knowledge">domain knowledge</a> to obtain more improvement. This highlights the importance of making use of <a href="https://en.wikipedia.org/wiki/Domain_knowledge">domain knowledge</a> in <a href="https://en.wikipedia.org/wiki/Domain-specific_language">domain-specific applications</a>.</abstract>
      <url hash="a5ad553d">D19-5541</url>
      <doi>10.18653/v1/D19-5541</doi>
      <bibkey>ying-etal-2019-improving</bibkey>
    </paper>
    <paper id="42">
      <title>Adapting Deep Learning Methods for Mental Health Prediction on <a href="https://en.wikipedia.org/wiki/Social_media">Social Media</a></title>
      <author><first>Ivan</first><last>Sekulic</last></author>
      <author><first>Michael</first><last>Strube</last></author>
      <pages>322–327</pages>
      <abstract>Mental health poses a significant challenge for an individual’s well-being. Text analysis of rich resources, like <a href="https://en.wikipedia.org/wiki/Social_media">social media</a>, can contribute to deeper understanding of illnesses and provide means for their early detection. We tackle a challenge of detecting social media users’ mental status through deep learning-based models, moving away from traditional approaches to the task. In a binary classification task on predicting if a user suffers from one of nine different disorders, a hierarchical attention network outperforms previously set benchmarks for four of the disorders. Furthermore, we explore the limitations of our <a href="https://en.wikipedia.org/wiki/Statistical_model">model</a> and analyze phrases relevant for <a href="https://en.wikipedia.org/wiki/Statistical_classification">classification</a> by inspecting the <a href="https://en.wikipedia.org/wiki/Statistical_model">model</a>’s word-level attention weights.</abstract>
      <url hash="3dcfd7a3">D19-5542</url>
      <doi>10.18653/v1/D19-5542</doi>
      <bibkey>sekulic-strube-2019-adapting</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/smhd">SMHD</pwcdataset>
    </paper>
    <paper id="44">
      <title>An Ensemble of <a href="https://en.wikipedia.org/wiki/Humour">Humour</a>, <a href="https://en.wikipedia.org/wiki/Sarcasm">Sarcasm</a>, and Hate Speechfor Sentiment Classification in Online Reviews</title>
      <author><first>Rohan</first><last>Badlani</last></author>
      <author><first>Nishit</first><last>Asnani</last></author>
      <author><first>Manan</first><last>Rai</last></author>
      <pages>337–345</pages>
      <abstract>Due to the nature of online user reviews, <a href="https://en.wikipedia.org/wiki/Sentiment_analysis">sentiment analysis</a> on such <a href="https://en.wikipedia.org/wiki/Data">data</a> requires a deep semantic understanding of the text. Many <a href="https://en.wikipedia.org/wiki/Review">online reviews</a> are sarcastic, humorous, or hateful. Signals from such language nuances may reinforce or completely alter the <a href="https://en.wikipedia.org/wiki/Sentiment_analysis">sentiment</a> of a review as predicted by a <a href="https://en.wikipedia.org/wiki/Machine_learning">machine learning model</a> that attempts to detect <a href="https://en.wikipedia.org/wiki/Sentiment_analysis">sentiment</a> alone. Thus, having a <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> that is explicitly aware of these <a href="https://en.wikipedia.org/wiki/Feature_(machine_learning)">features</a> should help <a href="https://en.wikipedia.org/wiki/Information_technology">it</a> perform better on reviews that are characterized by them. We propose a composite two-step model that extracts features pertaining to <a href="https://en.wikipedia.org/wiki/Sarcasm">sarcasm</a>, <a href="https://en.wikipedia.org/wiki/Humour">humour</a>, <a href="https://en.wikipedia.org/wiki/Hate_speech">hate speech</a>, as well as <a href="https://en.wikipedia.org/wiki/Sentimentality">sentiment</a>, in the first step, feeding them in conjunction to inform <a href="https://en.wikipedia.org/wiki/Sentimentality">sentiment classification</a> in the second step. We show that this multi-step approach leads to a better empirical performance for sentiment classification than a <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> that predicts sentiment alone. A qualitative analysis reveals that the conjunctive approach can better capture the nuances of sentiment as expressed in online reviews.</abstract>
      <url hash="4be90a02">D19-5544</url>
      <attachment hash="5b55c246">D19-5544.Attachment.zip</attachment>
      <doi>10.18653/v1/D19-5544</doi>
      <bibkey>badlani-etal-2019-ensemble</bibkey>
    </paper>
    <paper id="47">
      <title>A Social Opinion Gold Standard for the Malta Government Budget 2018<fixed-case>M</fixed-case>alta Government Budget 2018</title>
      <author><first>Keith</first><last>Cortis</last></author>
      <author><first>Brian</first><last>Davis</last></author>
      <pages>364–369</pages>
      <abstract>We present a gold standard of annotated social opinion for the Malta Government Budget 2018. It consists of over 500 online posts in English and/or the Maltese less-resourced language, gathered from social media platforms, specifically, social networking services and newswires, which have been annotated with information about opinions expressed by the general public and other entities, in terms of sentiment polarity, emotion, sarcasm / irony, and negation. This <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> is a resource for <a href="https://en.wikipedia.org/wiki/Opinion_mining">opinion mining</a> based on social data, within the context of <a href="https://en.wikipedia.org/wiki/Politics">politics</a>. It is the first opinion annotated social dataset from Malta, which has very limited language resources available.</abstract>
      <url hash="c1710b17">D19-5547</url>
      <doi>10.18653/v1/D19-5547</doi>
      <bibkey>cortis-davis-2019-social</bibkey>
    </paper>
    <paper id="49">
      <title>Y’all should read this ! Identifying Plurality in Second-Person Personal Pronouns in English Texts<fixed-case>Y</fixed-case>’all should read this! Identifying Plurality in Second-Person Personal Pronouns in <fixed-case>E</fixed-case>nglish Texts</title>
      <author><first>Gabriel</first><last>Stanovsky</last></author>
      <author><first>Ronen</first><last>Tamari</last></author>
      <pages>375–380</pages>
      <abstract>Distinguishing between singular and plural you in English is a challenging task which has potential for downstream applications, such as <a href="https://en.wikipedia.org/wiki/Machine_translation">machine translation</a> or <a href="https://en.wikipedia.org/wiki/Coreference_resolution">coreference resolution</a>. While formal written English does not distinguish between these cases, other languages (such as Spanish), as well as other dialects of English (via phrases such as y’ all), do make this distinction. We make use of this to obtain distantly-supervised labels for the task on a large-scale in two domains. Following, we train a <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> to distinguish between the single / plural ‘you’, finding that although in-domain training achieves reasonable <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> (77 %), there is still a lot of room for improvement, especially in the domain-transfer scenario, which proves extremely challenging. Our code and data are publicly available.</abstract>
      <url hash="e927c731">D19-5549</url>
      <doi>10.18653/v1/D19-5549</doi>
      <bibkey>stanovsky-tamari-2019-yall</bibkey>
    </paper>
    <paper id="52">
      <title>Contextualized context2vec</title>
      <author><first>Kazuki</first><last>Ashihara</last></author>
      <author><first>Tomoyuki</first><last>Kajiwara</last></author>
      <author><first>Yuki</first><last>Arase</last></author>
      <author><first>Satoru</first><last>Uchida</last></author>
      <pages>397–406</pages>
      <abstract>Lexical substitution ranks substitution candidates from the viewpoint of paraphrasability for a target word in a given sentence. There are two major approaches for <a href="https://en.wikipedia.org/wiki/Lexical_substitution">lexical substitution</a> : (1) generating contextualized word embeddings by assigning multiple embeddings to one word and (2) generating context embeddings using the sentence. Herein we propose a method that combines these two approaches to contextualize word embeddings for <a href="https://en.wikipedia.org/wiki/Lexical_substitution">lexical substitution</a>. Experiments demonstrate that our <a href="https://en.wikipedia.org/wiki/Methodology">method</a> outperforms the current state-of-the-art method. We also create CEFR-LP, a new evaluation dataset for the lexical substitution task. It has a wider coverage of substitution candidates than previous <a href="https://en.wikipedia.org/wiki/Data_set">datasets</a> and assigns <a href="https://en.wikipedia.org/wiki/English_as_a_second_or_foreign_language">English proficiency levels</a> to all target words and substitution candidates.</abstract>
      <url hash="7ab30078">D19-5552</url>
      <doi>10.18653/v1/D19-5552</doi>
      <bibkey>ashihara-etal-2019-contextualized</bibkey>
    </paper>
    <paper id="55">
      <title>Unsupervised Neologism Normalization Using Embedding Space Mapping</title>
      <author><first>Nasser</first><last>Zalmout</last></author>
      <author><first>Kapil</first><last>Thadani</last></author>
      <author><first>Aasish</first><last>Pappu</last></author>
      <pages>425–430</pages>
      <abstract>This paper presents an approach for detecting and normalizing neologisms in <a href="https://en.wikipedia.org/wiki/Social_media">social media content</a>. Neologisms refer to recent expressions that are specific to certain entities or events and are being increasingly used by the public, but have not yet been accepted in mainstream language. Automated methods for handling <a href="https://en.wikipedia.org/wiki/Neologism">neologisms</a> are important for <a href="https://en.wikipedia.org/wiki/Natural-language_understanding">natural language understanding</a> and normalization, especially for informal genres with <a href="https://en.wikipedia.org/wiki/User-generated_content">user generated content</a>. We present an <a href="https://en.wikipedia.org/wiki/Unsupervised_learning">unsupervised approach</a> for detecting <a href="https://en.wikipedia.org/wiki/Neologism">neologisms</a> and then normalizing them to canonical words without relying on parallel training data. Our approach builds on the text normalization literature and introduces <a href="https://en.wikipedia.org/wiki/Adaptation">adaptations</a> to fit the specificities of this task, including <a href="https://en.wikipedia.org/wiki/Phonetic_transcription">phonetic and etymological considerations</a>. We evaluate the proposed techniques on a dataset of Reddit comments, with detected neologisms and corresponding <a href="https://en.wikipedia.org/wiki/Normalization_(statistics)">normalizations</a>.</abstract>
      <url hash="0ad6eb3f">D19-5555</url>
      <doi>10.18653/v1/D19-5555</doi>
      <bibkey>zalmout-etal-2019-unsupervised</bibkey>
    </paper>
    <paper id="57">
      <title>Towards Actual (Not Operational) Textual Style Transfer Auto-Evaluation</title>
      <author><first>Richard Yuanzhe</first><last>Pang</last></author>
      <pages>444–445</pages>
      <abstract>Regarding the problem of automatically generating paraphrases with modified styles or attributes, the difficulty lies in the lack of parallel corpora. Numerous advances have been proposed for the <a href="https://en.wikipedia.org/wiki/Electricity_generation">generation</a>. However, significant problems remain with the auto-evaluation of style transfer tasks. Based on the summary of Pang and Gimpel (2018) and Mir et al. (2019), style transfer evaluations rely on three metrics : post-transfer style classification accuracy, content or semantic similarity, and naturalness or fluency. We elucidate the dangerous current state of style transfer auto-evaluation research. Moreover, we propose ways to aggregate the three <a href="https://en.wikipedia.org/wiki/Metric_(mathematics)">metrics</a> into one <a href="https://en.wikipedia.org/wiki/Evaluation">evaluator</a>. This abstract aims to bring researchers to think about the future of style transfer and style transfer evaluation research.</abstract>
      <url hash="8d339d8a">D19-5557</url>
      <doi>10.18653/v1/D19-5557</doi>
      <bibkey>pang-2019-towards</bibkey>
    </paper>
    <paper id="58">
      <title>CodeSwitch-Reddit : Exploration of Written Multilingual Discourse in Online Discussion Forums<fixed-case>C</fixed-case>ode<fixed-case>S</fixed-case>witch-<fixed-case>R</fixed-case>eddit: Exploration of Written Multilingual Discourse in Online Discussion Forums</title>
      <author><first>Ella</first><last>Rabinovich</last></author>
      <author><first>Masih</first><last>Sultani</last></author>
      <author><first>Suzanne</first><last>Stevenson</last></author>
      <pages>446</pages>
      <abstract>In contrast to many decades of research on oral code-switching, the study of written multilingual productions has only recently enjoyed a surge of interest. Many open questions remain regarding the sociolinguistic underpinnings of written code-switching, and progress has been limited by a lack of suitable resources. We introduce a novel, large, and diverse <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> of written code-switched productions, curated from topical threads of multiple bilingual communities on the Reddit discussion platform, and explore questions that were mainly addressed in the context of <a href="https://en.wikipedia.org/wiki/Spoken_language">spoken language</a> thus far. We investigate whether findings in oral code-switching concerning content and style, as well as speaker proficiency, are carried over into <a href="https://en.wikipedia.org/wiki/Code-switching">written code-switching</a> in <a href="https://en.wikipedia.org/wiki/Internet_forum">discussion forums</a>. The released <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> can further facilitate a range of research and practical activities.</abstract>
      <url hash="72b1986e">D19-5558</url>
      <doi>10.18653/v1/D19-5558</doi>
      <bibkey>rabinovich-etal-2019-codeswitch-reddit</bibkey>
    </paper>
  </volume>
  <volume id="56" ingest-date="2019-11-03">
    <meta>
      <booktitle>Proceedings of the 3rd Workshop on Neural Generation and Translation</booktitle>
      <url hash="a8894c8d">D19-56</url>
      <editor><first>Alexandra</first><last>Birch</last></editor>
      <editor><first>Andrew</first><last>Finch</last></editor>
      <editor><first>Hiroaki</first><last>Hayashi</last></editor>
      <editor><first>Ioannis</first><last>Konstas</last></editor>
      <editor><first>Thang</first><last>Luong</last></editor>
      <editor><first>Graham</first><last>Neubig</last></editor>
      <editor><first>Yusuke</first><last>Oda</last></editor>
      <editor><first>Katsuhito</first><last>Sudoh</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Hong Kong</address>
      <month>November</month>
      <year>2019</year>
    </meta>
    <frontmatter>
      <url hash="5aa9c8a7">D19-5600</url>
      <bibkey>emnlp-2019-neural</bibkey>
    </frontmatter>
    <paper id="3">
      <title>Recycling a Pre-trained BERT Encoder for <a href="https://en.wikipedia.org/wiki/Neural_machine_translation">Neural Machine Translation</a><fixed-case>BERT</fixed-case> Encoder for Neural Machine Translation</title>
      <author><first>Kenji</first><last>Imamura</last></author>
      <author><first>Eiichiro</first><last>Sumita</last></author>
      <pages>23–31</pages>
      <abstract>In this paper, a pre-trained Bidirectional Encoder Representations from Transformers (BERT) model is applied to Transformer-based neural machine translation (NMT). In contrast to monolingual tasks, the number of unlearned model parameters in an NMT decoder is as huge as the number of learned parameters in the BERT model. To train all the models appropriately, we employ two-stage optimization, which first trains only the unlearned parameters by freezing the BERT model, and then fine-tunes all the sub-models. In our experiments, stable two-stage optimization was achieved, in contrast the BLEU scores of direct fine-tuning were extremely low. Consequently, the BLEU scores of the proposed method were better than those of the Transformer base model and the same <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a> without pre-training. Additionally, we confirmed that <a href="https://en.wikipedia.org/wiki/Neurotransmitter">NMT</a> with the BERT encoder is more effective in low-resource settings.</abstract>
      <url hash="ea922aaa">D19-5603</url>
      <doi>10.18653/v1/D19-5603</doi>
      <bibkey>imamura-sumita-2019-recycling</bibkey>
    </paper>
    <paper id="4">
      <title>Generating a Common Question from Multiple Documents using Multi-source Encoder-Decoder Models</title>
      <author><first>Woon Sang</first><last>Cho</last></author>
      <author><first>Yizhe</first><last>Zhang</last></author>
      <author><first>Sudha</first><last>Rao</last></author>
      <author><first>Chris</first><last>Brockett</last></author>
      <author><first>Sungjin</first><last>Lee</last></author>
      <pages>32–43</pages>
      <abstract>Ambiguous user queries in <a href="https://en.wikipedia.org/wiki/Web_search_engine">search engines</a> result in the retrieval of documents that often span multiple topics. One potential solution is for the <a href="https://en.wikipedia.org/wiki/Web_search_engine">search engine</a> to generate multiple refined queries, each of which relates to a subset of the documents spanning the same topic. A preliminary step towards this goal is to generate a question that captures common concepts of multiple documents. We propose a new task of generating common question from multiple documents and present simple variant of an existing multi-source encoder-decoder framework, called the Multi-Source Question Generator (MSQG). We first train an RNN-based single encoder-decoder generator from (single document, question) pairs. At test time, given multiple documents, the Distribute step of our MSQG model predicts target word distributions for each document using the trained <a href="https://en.wikipedia.org/wiki/Statistical_model">model</a>. The Aggregate step aggregates these <a href="https://en.wikipedia.org/wiki/Probability_distribution">distributions</a> to generate a common question. This simple yet effective strategy significantly outperforms several existing baseline models applied to the new task when evaluated using automated metrics and human judgments on the MS-MARCO-QA dataset.</abstract>
      <url hash="3eb39132">D19-5604</url>
      <doi>10.18653/v1/D19-5604</doi>
      <bibkey>cho-etal-2019-generating</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/ms-marco">MS MARCO</pwcdataset>
    </paper>
    <paper id="7">
      <title>Transformer-based Model for Single Documents Neural Summarization</title>
      <author><first>Elozino</first><last>Egonmwan</last></author>
      <author><first>Yllias</first><last>Chali</last></author>
      <pages>70–79</pages>
      <abstract>We propose a system that improves performance on single document summarization task using the CNN / DailyMail and Newsroom datasets. It follows the popular encoder-decoder paradigm, but with an extra focus on the <a href="https://en.wikipedia.org/wiki/Encoder">encoder</a>. The intuition is that the probability of correctly decoding an information significantly lies in the pattern and correctness of the <a href="https://en.wikipedia.org/wiki/Encoder">encoder</a>. Hence we introduce, encode encode   decode. A framework that encodes the source text first with a transformer, then a sequence-to-sequence (seq2seq) model. We find that the transformer and seq2seq model complement themselves adequately, making for a richer encoded vector representation. We also find that paying more attention to the vocabulary of target words during <a href="https://en.wikipedia.org/wiki/Abstraction">abstraction</a> improves performance. We experiment our hypothesis and framework on the task of extractive and abstractive single document summarization and evaluate using the standard CNN / DailyMail dataset and the recently released Newsroom dataset.</abstract>
      <url hash="918e6364">D19-5607</url>
      <doi>10.18653/v1/D19-5607</doi>
      <bibkey>egonmwan-chali-2019-transformer</bibkey>
    </paper>
    <paper id="8">
      <title>Making Asynchronous Stochastic Gradient Descent Work for Transformers</title>
      <author><first>Alham Fikri</first><last>Aji</last></author>
      <author><first>Kenneth</first><last>Heafield</last></author>
      <pages>80–89</pages>
      <abstract>Asynchronous stochastic gradient descent (SGD) converges poorly for Transformer models, so synchronous SGD has become the norm for Transformer training. This is unfortunate because asynchronous SGD is faster at raw training speed since it avoids waiting for <a href="https://en.wikipedia.org/wiki/Synchronization_(computer_science)">synchronization</a>. Moreover, the Transformer model is the basis for state-of-the-art models for several tasks, including <a href="https://en.wikipedia.org/wiki/Machine_translation">machine translation</a>, so training speed matters. To understand why asynchronous SGD under-performs, we blur the lines between asynchronous and synchronous methods. We find that summing several asynchronous updates, rather than applying them immediately, restores convergence behavior. With this <a href="https://en.wikipedia.org/wiki/Methodology">method</a>, the Transformer attains the same BLEU score 1.36 times as fast.</abstract>
      <url hash="585bb44e">D19-5608</url>
      <doi>10.18653/v1/D19-5608</doi>
      <bibkey>aji-heafield-2019-making</bibkey>
    </paper>
    <paper id="12">
      <title>On the Importance of the Kullback-Leibler Divergence Term in Variational Autoencoders for Text Generation<fixed-case>K</fixed-case>ullback-<fixed-case>L</fixed-case>eibler Divergence Term in Variational Autoencoders for Text Generation</title>
      <author><first>Victor</first><last>Prokhorov</last></author>
      <author><first>Ehsan</first><last>Shareghi</last></author>
      <author><first>Yingzhen</first><last>Li</last></author>
      <author><first>Mohammad Taher</first><last>Pilehvar</last></author>
      <author><first>Nigel</first><last>Collier</last></author>
      <pages>118–127</pages>
      <abstract>Variational Autoencoders (VAEs) are known to suffer from learning uninformative latent representation of the input due to issues such as approximated posterior collapse, or entanglement of the latent space. We impose an explicit <a href="https://en.wikipedia.org/wiki/Constraint_(mathematics)">constraint</a> on the Kullback-Leibler (KL) divergence term inside the VAE objective function. While the explicit <a href="https://en.wikipedia.org/wiki/Constraint_(mathematics)">constraint</a> naturally avoids posterior collapse, we use it to further understand the significance of the KL term in controlling the information transmitted through the VAE channel. Within this framework, we explore different properties of the estimated posterior distribution, and highlight the trade-off between the amount of information encoded in a latent code during training, and the generative capacity of the <a href="https://en.wikipedia.org/wiki/Statistical_model">model</a>.</abstract>
      <url hash="5f0bc7f4">D19-5612</url>
      <doi>10.18653/v1/D19-5612</doi>
      <bibkey>prokhorov-etal-2019-importance</bibkey>
      <pwccode url="https://github.com/VictorProkhorov/KL_Text_VAE" additional="false">VictorProkhorov/KL_Text_VAE</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/webtext">WebText</pwcdataset>
    </paper>
    <paper id="15">
      <title>Enhanced Transformer Model for Data-to-Text Generation</title>
      <author><first>Li</first><last>Gong</last></author>
      <author><first>Josep</first><last>Crego</last></author>
      <author><first>Jean</first><last>Senellart</last></author>
      <pages>148–156</pages>
      <abstract>Neural models have recently shown significant progress on data-to-text generation tasks in which descriptive texts are generated conditioned on <a href="https://en.wikipedia.org/wiki/Record_(computer_science)">database records</a>. In this work, we present a new Transformer-based data-to-text generation model which learns content selection and summary generation in an end-to-end fashion. We introduce two extensions to the baseline transformer model : First, we modify the latent representation of the input, which helps to significantly improve the content correctness of the output summary ; Second, we include an additional learning objective that accounts for content selection modelling. In addition, we propose two data augmentation methods that succeed to further improve performance of the resulting generation models. Evaluation experiments show that our final model outperforms current state-of-the-art systems as measured by different metrics : BLEU, content selection precision and content ordering. We made publicly available the transformer extension presented in this paper.</abstract>
      <url hash="48b2b8e5">D19-5615</url>
      <doi>10.18653/v1/D19-5615</doi>
      <bibkey>gong-etal-2019-enhanced</bibkey>
    </paper>
    <paper id="16">
      <title>Generalization in Generation : A closer look at Exposure Bias</title>
      <author><first>Florian</first><last>Schmidt</last></author>
      <pages>157–167</pages>
      <abstract>Exposure bias refers to the train-test discrepancy that seemingly arises when an <a href="https://en.wikipedia.org/wiki/Autoregressive_model">autoregressive generative model</a> uses only ground-truth contexts at training time but generated ones at test time. We separate the contribution of the learning framework and the <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a> to clarify the debate on consequences and review proposed counter-measures. In this light, we argue that <a href="https://en.wikipedia.org/wiki/Generalization">generalization</a> is the underlying property to address and propose unconditional generation as its fundamental benchmark. Finally, we combine <a href="https://en.wikipedia.org/wiki/Latent_variable_model">latent variable modeling</a> with a recent formulation of exploration in <a href="https://en.wikipedia.org/wiki/Reinforcement_learning">reinforcement learning</a> to obtain a rigorous handling of true and generated contexts. Results on <a href="https://en.wikipedia.org/wiki/Language_model">language modeling</a> and variational sentence auto-encoding confirm the <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a>’s generalization capability.</abstract>
      <url hash="a02abe4f">D19-5616</url>
      <attachment hash="9cb0e5ab">D19-5616.Attachment.pdf</attachment>
      <doi>10.18653/v1/D19-5616</doi>
      <bibkey>schmidt-2019-generalization</bibkey>
    </paper>
    <paper id="21">
      <title>A Margin-based Loss with Synthetic Negative Samples for Continuous-output Machine Translation</title>
      <author><first>Gayatri</first><last>Bhat</last></author>
      <author><first>Sachin</first><last>Kumar</last></author>
      <author><first>Yulia</first><last>Tsvetkov</last></author>
      <pages>199–205</pages>
      <abstract>Neural models that eliminate the softmax bottleneck by generating word embeddings (rather than <a href="https://en.wikipedia.org/wiki/Multinomial_distribution">multinomial distributions</a> over a vocabulary) attain faster training with fewer learnable parameters. These models are currently trained by maximizing densities of pretrained target embeddings under von Mises-Fisher distributions parameterized by corresponding model-predicted embeddings. This work explores the utility of margin-based loss functions in optimizing such <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a>. We present syn-margin loss, a novel margin-based loss that uses a synthetic negative sample constructed from only the predicted and target embeddings at every step. The <a href="https://en.wikipedia.org/wiki/Profit_(accounting)">loss</a> is efficient to compute, and we use a <a href="https://en.wikipedia.org/wiki/Geometric_analysis">geometric analysis</a> to argue that it is more consistent and interpretable than other margin-based losses. Empirically, we find that syn-margin provides small but significant improvements over both vMF and standard margin-based losses in continuous-output neural machine translation.</abstract>
      <url hash="5e9b2486">D19-5621</url>
      <doi>10.18653/v1/D19-5621</doi>
      <bibkey>bhat-etal-2019-margin</bibkey>
    </paper>
    <paper id="22">
      <title>Mixed Multi-Head Self-Attention for Neural Machine Translation</title>
      <author><first>Hongyi</first><last>Cui</last></author>
      <author><first>Shohei</first><last>Iida</last></author>
      <author><first>Po-Hsuan</first><last>Hung</last></author>
      <author><first>Takehito</first><last>Utsuro</last></author>
      <author><first>Masaaki</first><last>Nagata</last></author>
      <pages>206–214</pages>
      <abstract>Recently, the Transformer becomes a state-of-the-art architecture in the filed of neural machine translation (NMT). A key point of its high-performance is the multi-head self-attention which is supposed to allow the <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> to independently attend to information from different representation subspaces. However, there is no explicit mechanism to ensure that different attention heads indeed capture different features, and in practice, <a href="https://en.wikipedia.org/wiki/Redundancy_(engineering)">redundancy</a> has occurred in multiple heads. In this paper, we argue that using the same <a href="https://en.wikipedia.org/wiki/Attentional_control">global attention</a> in multiple heads limits multi-head self-attention’s capacity for learning distinct features. In order to improve the expressiveness of multi-head self-attention, we propose a novel Mixed Multi-Head Self-Attention (MMA) which models not only global and local attention but also forward and backward attention in different attention heads. This enables the <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> to learn distinct representations explicitly among multiple heads. In our experiments on both WAT17 English-Japanese as well as IWSLT14 German-English translation task, we show that, without increasing the number of parameters, our <a href="https://en.wikipedia.org/wiki/Conceptual_model">models</a> yield consistent and significant improvements (0.9 BLEU scores on average) over the strong Transformer baseline.</abstract>
      <url hash="0aef9957">D19-5622</url>
      <doi>10.18653/v1/D19-5622</doi>
      <bibkey>cui-etal-2019-mixed</bibkey>
    </paper>
    <paper id="24">
      <title>Interrogating the Explanatory Power of <a href="https://en.wikipedia.org/wiki/Attention">Attention</a> in Neural Machine Translation</title>
      <author><first>Pooya</first><last>Moradi</last></author>
      <author><first>Nishant</first><last>Kambhatla</last></author>
      <author><first>Anoop</first><last>Sarkar</last></author>
      <pages>221–230</pages>
      <abstract>Attention models have become a crucial component in <a href="https://en.wikipedia.org/wiki/Neural_machine_translation">neural machine translation (NMT)</a>. They are often implicitly or explicitly used to justify the <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a>’s decision in generating a specific token but it has not yet been rigorously established to what extent attention is a reliable source of information in <a href="https://en.wikipedia.org/wiki/Mathematical_model">NMT</a>. To evaluate the explanatory power of <a href="https://en.wikipedia.org/wiki/Attention">attention</a> for NMT, we examine the possibility of yielding the same prediction but with counterfactual attention models that modify crucial aspects of the trained <a href="https://en.wikipedia.org/wiki/Attention">attention model</a>. Using these counterfactual attention mechanisms we assess the extent to which they still preserve the generation of function and content words in the translation process. Compared to a state of the art attention model, our counterfactual attention models produce 68 % of <a href="https://en.wikipedia.org/wiki/Function_word">function words</a> and 21 % of <a href="https://en.wikipedia.org/wiki/Content_word">content words</a> in our German-English dataset. Our experiments demonstrate that attention models by themselves can not reliably explain the decisions made by a NMT model.</abstract>
      <url hash="6e2f31e7">D19-5624</url>
      <doi>10.18653/v1/D19-5624</doi>
      <bibkey>moradi-etal-2019-interrogating</bibkey>
      <pwccode url="https://github.com/sfu-natlang/attention_explanation" additional="false">sfu-natlang/attention_explanation</pwccode>
    </paper>
    <paper id="25">
      <title>Auto-Sizing the Transformer Network : Improving Speed, Efficiency, and Performance for Low-Resource Machine Translation</title>
      <author><first>Kenton</first><last>Murray</last></author>
      <author><first>Jeffery</first><last>Kinnison</last></author>
      <author><first>Toan Q.</first><last>Nguyen</last></author>
      <author><first>Walter</first><last>Scheirer</last></author>
      <author><first>David</first><last>Chiang</last></author>
      <pages>231–240</pages>
      <abstract>Neural sequence-to-sequence models, particularly the Transformer, are the state of the art in <a href="https://en.wikipedia.org/wiki/Machine_translation">machine translation</a>. Yet these <a href="https://en.wikipedia.org/wiki/Neural_network">neural networks</a> are very sensitive to <a href="https://en.wikipedia.org/wiki/Network_architecture">architecture</a> and hyperparameter settings. Optimizing these settings by grid or random search is computationally expensive because it requires many training runs. In this paper, we incorporate <a href="https://en.wikipedia.org/wiki/Architecture_search">architecture search</a> into a single training run through auto-sizing, which uses <a href="https://en.wikipedia.org/wiki/Regularization_(mathematics)">regularization</a> to delete <a href="https://en.wikipedia.org/wiki/Neuron">neurons</a> in a <a href="https://en.wikipedia.org/wiki/Computer_network">network</a> over the course of training. On very low-resource language pairs, we show that auto-sizing can improve BLEU scores by up to 3.9 points while removing one-third of the <a href="https://en.wikipedia.org/wiki/Parameter_(computer_programming)">parameters</a> from the <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a>.</abstract>
      <url hash="32e460c4">D19-5625</url>
      <doi>10.18653/v1/D19-5625</doi>
      <bibkey>murray-etal-2019-auto</bibkey>
      <pwccode url="https://github.com/KentonMurray/ProxGradPytorch" additional="false">KentonMurray/ProxGradPytorch</pwccode>
    </paper>
    <paper id="28">
      <title>Monash University’s Submissions to the WNGT 2019 Document Translation Task<fixed-case>WNGT</fixed-case> 2019 Document Translation Task</title>
      <author><first>Sameen</first><last>Maruf</last></author>
      <author><first>Gholamreza</first><last>Haffari</last></author>
      <pages>256–261</pages>
      <abstract>We describe the work of Monash University for the shared task of Rotowire document translation organised by the 3rd Workshop on Neural Generation and Translation (WNGT 2019). We submitted systems for both directions of the English-German language pair. Our main focus is on employing an established document-level neural machine translation model for this <a href="https://en.wikipedia.org/wiki/Task_(project_management)">task</a>. We achieve a <a href="https://en.wikipedia.org/wiki/BLEU">BLEU score</a> of 39.83 (41.46 BLEU per WNGT evaluation) for En-De and 45.06 (47.39 BLEU per WNGT evaluation) for De-En translation directions on the Rotowire test set. All experiments conducted in the process are also described.</abstract>
      <url hash="344d3463">D19-5628</url>
      <doi>10.18653/v1/D19-5628</doi>
      <bibkey>maruf-haffari-2019-monash</bibkey>
    </paper>
    <paper id="30">
      <title>University of Edinburgh’s submission to the Document-level Generation and Translation Shared Task<fixed-case>U</fixed-case>niversity of <fixed-case>E</fixed-case>dinburgh’s submission to the Document-level Generation and Translation Shared Task</title>
      <author><first>Ratish</first><last>Puduppully</last></author>
      <author><first>Jonathan</first><last>Mallinson</last></author>
      <author><first>Mirella</first><last>Lapata</last></author>
      <pages>268–272</pages>
      <abstract>The University of Edinburgh participated in all six tracks : NLG, MT, and MT+NLG with both <a href="https://en.wikipedia.org/wiki/English_language">English</a> and <a href="https://en.wikipedia.org/wiki/German_language">German</a> as targeted languages. For the NLG track, we submitted a multilingual system based on the Content Selection and Planning model of Puduppully et al (2019). For the MT track, we submitted Transformer-based Neural Machine Translation models, where out-of-domain parallel data was augmented with in-domain data extracted from monolingual corpora. Our MT+NLG systems disregard the structured input data and instead rely exclusively on the source summaries.</abstract>
      <url hash="f9cff2e3">D19-5630</url>
      <doi>10.18653/v1/D19-5630</doi>
      <bibkey>puduppully-etal-2019-university</bibkey>
      <pwccode url="https://github.com/ratishsp/data2text-table-plan-py" additional="false">ratishsp/data2text-table-plan-py</pwccode>
    </paper>
    <paper id="31">
      <title>Naver Labs Europe’s Systems for the Document-Level Generation and Translation Task at WNGT 2019<fixed-case>E</fixed-case>urope’s Systems for the Document-Level Generation and Translation Task at <fixed-case>WNGT</fixed-case> 2019</title>
      <author><first>Fahimeh</first><last>Saleh</last></author>
      <author><first>Alexandre</first><last>Berard</last></author>
      <author><first>Ioan</first><last>Calapodescu</last></author>
      <author><first>Laurent</first><last>Besacier</last></author>
      <pages>273–279</pages>
      <abstract>Recently, neural models led to significant improvements in both <a href="https://en.wikipedia.org/wiki/Machine_translation">machine translation (MT)</a> and <a href="https://en.wikipedia.org/wiki/Natural-language_generation">natural language generation tasks (NLG)</a>. However, generation of long descriptive summaries conditioned on <a href="https://en.wikipedia.org/wiki/Structured_data">structured data</a> remains an open challenge. Likewise, <a href="https://en.wikipedia.org/wiki/Metadata">MT</a> that goes beyond <a href="https://en.wikipedia.org/wiki/Metadata">sentence-level context</a> is still an open issue (e.g., document-level MT or <a href="https://en.wikipedia.org/wiki/Metadata">MT</a> with metadata). To address these challenges, we propose to leverage data from both tasks and do <a href="https://en.wikipedia.org/wiki/Transfer_learning">transfer learning</a> between MT, NLG, and MT with source-side metadata (MT+NLG). First, we train document-based MT systems with large amounts of <a href="https://en.wikipedia.org/wiki/Parallel_computing">parallel data</a>. Then, we adapt these <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> to pure NLG and MT+NLG tasks by <a href="https://en.wikipedia.org/wiki/Fine-tuning">fine-tuning</a> with smaller amounts of domain-specific data. This end-to-end NLG approach, without data selection and planning, outperforms the previous state of the art on the Rotowire NLG task. We participated to the Document Generation and Translation task at WNGT 2019, and ranked first in all tracks.</abstract>
      <url hash="82d4a484">D19-5631</url>
      <doi>10.18653/v1/D19-5631</doi>
      <bibkey>saleh-etal-2019-naver</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/rotowire">RotoWire</pwcdataset>
    </paper>
    </volume>
  <volume id="57" ingest-date="2019-11-03">
    <meta>
      <booktitle>Proceedings of The 5th Workshop on BioNLP Open Shared Tasks</booktitle>
      <url hash="6f78068d">D19-57</url>
      <editor><first>Kim</first><last>Jin-Dong</last></editor>
      <editor><first>Nédellec</first><last>Claire</last></editor>
      <editor><first>Bossy</first><last>Robert</last></editor>
      <editor><first>Deléger</first><last>Louise</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Hong Kong, China</address>
      <month>November</month>
      <year>2019</year>
    </meta>
    <frontmatter>
      <url hash="a9223019">D19-5700</url>
      <bibkey>emnlp-2019-bionlp</bibkey>
    </frontmatter>
    <paper id="1">
      <title>PharmaCoNER : Pharmacological Substances, Compounds and proteins Named Entity Recognition track<fixed-case>P</fixed-case>harma<fixed-case>C</fixed-case>o<fixed-case>NER</fixed-case>: Pharmacological Substances, Compounds and proteins Named Entity Recognition track</title>
      <author><first>Aitor</first><last>Gonzalez-Agirre</last></author>
      <author><first>Montserrat</first><last>Marimon</last></author>
      <author><first>Ander</first><last>Intxaurrondo</last></author>
      <author><first>Obdulia</first><last>Rabal</last></author>
      <author><first>Marta</first><last>Villegas</last></author>
      <author><first>Martin</first><last>Krallinger</last></author>
      <pages>1–10</pages>
      <abstract>One of the biomedical entity types of relevance for <a href="https://en.wikipedia.org/wiki/Medicine">medicine</a> or biosciences are <a href="https://en.wikipedia.org/wiki/Chemical_compound">chemical compounds</a> and <a href="https://en.wikipedia.org/wiki/Drug">drugs</a>. The correct detection these entities is critical for other text mining applications building on them, such as adverse drug-reaction detection, medication-related fake news or drug-target extraction. Although a significant effort was made to detect mentions of drugs / chemicals in English texts, so far only very limited attempts were made to recognize them in medical documents in other languages. Taking into account the growing amount of <a href="https://en.wikipedia.org/wiki/Medical_literature">medical publications</a> and <a href="https://en.wikipedia.org/wiki/Medical_record">clinical records</a> written in <a href="https://en.wikipedia.org/wiki/Spanish_language">Spanish</a>, we have organized the first shared task on detecting drug and chemical entities in <a href="https://en.wikipedia.org/wiki/Spanish_language">Spanish medical documents</a>. Additionally, we included a clinical concept-indexing sub-track asking teams to return SNOMED-CT identifiers related to drugs / chemicals for a collection of documents. For this task, named PharmaCoNER, we generated annotation guidelines together with a corpus of 1,000 manually annotated clinical case studies. A total of 22 teams participated in the sub-track 1, (77 system runs), and 7 teams in the sub-track 2 (19 system runs). Top scoring teams used sophisticated <a href="https://en.wikipedia.org/wiki/Deep_learning">deep learning approaches</a> yielding very competitive results with F-measures above 0.91. These results indicate that there is a real interest in promoting biomedical text mining efforts beyond <a href="https://en.wikipedia.org/wiki/English_language">English</a>. We foresee that the PharmaCoNER annotation guidelines, corpus and participant systems will foster the development of new resources for clinical and biomedical text mining systems of Spanish medical data.</abstract>
      <url hash="3ef99a3b">D19-5701</url>
      <doi>10.18653/v1/D19-5701</doi>
      <bibkey>gonzalez-agirre-etal-2019-pharmaconer</bibkey>
    </paper>
    <paper id="4">
      <title>IxaMed at PharmacoNER Challenge 2019<fixed-case>I</fixed-case>xa<fixed-case>M</fixed-case>ed at <fixed-case>P</fixed-case>harmaco<fixed-case>NER</fixed-case> Challenge 2019</title>
      <author><first>Xabier</first><last>Lahuerta</last></author>
      <author><first>Iakes</first><last>Goenaga</last></author>
      <author><first>Koldo</first><last>Gojenola</last></author>
      <author><first>Aitziber</first><last>Atutxa Salazar</last></author>
      <author><first>Maite</first><last>Oronoz</last></author>
      <pages>21–25</pages>
      <abstract>The aim of this paper is to present our approach (IxaMed) in the PharmacoNER 2019 task. The task consists of identifying chemical, drug, and gene / protein mentions from clinical case studies written in <a href="https://en.wikipedia.org/wiki/Spanish_language">Spanish</a>. The evaluation of the <a href="https://en.wikipedia.org/wiki/Task_(project_management)">task</a> is divided in two scenarios : one corresponding to the detection of named entities and one corresponding to the indexation of named entities that have been previously identified. In order to identify <a href="https://en.wikipedia.org/wiki/Named_entity">named entities</a> we have made use of a Bi-LSTM with a CRF on top in combination with different types of <a href="https://en.wikipedia.org/wiki/Word_embedding">word embeddings</a>. We have achieved our best result (86.81 F-Score) combining pretrained word embeddings of <a href="https://en.wikipedia.org/wiki/Wikipedia">Wikipedia</a> and <a href="https://en.wikipedia.org/wiki/Electronic_health_record">Electronic Health Records</a> (50 M words) with contextual string embeddings of <a href="https://en.wikipedia.org/wiki/Wikipedia">Wikipedia</a> and <a href="https://en.wikipedia.org/wiki/Electronic_health_record">Electronic Health Records</a>. On the other hand, for the indexation of the named entities we have used the <a href="https://en.wikipedia.org/wiki/Levenshtein_distance">Levenshtein distance</a> obtaining a 85.34 <a href="https://en.wikipedia.org/wiki/F-score">F-Score</a> as our best result.</abstract>
      <url hash="bda466ac">D19-5704</url>
      <doi>10.18653/v1/D19-5704</doi>
      <bibkey>lahuerta-etal-2019-ixamed</bibkey>
    </paper>
    <paper id="6">
      <title>A Deep Learning-Based System for PharmaCoNER<fixed-case>P</fixed-case>harma<fixed-case>C</fixed-case>o<fixed-case>NER</fixed-case></title>
      <author><first>Ying</first><last>Xiong</last></author>
      <author><first>Yedan</first><last>Shen</last></author>
      <author><first>Yuanhang</first><last>Huang</last></author>
      <author><first>Shuai</first><last>Chen</last></author>
      <author><first>Buzhou</first><last>Tang</last></author>
      <author><first>Xiaolong</first><last>Wang</last></author>
      <author><first>Qingcai</first><last>Chen</last></author>
      <author><first>Jun</first><last>Yan</last></author>
      <author><first>Yi</first><last>Zhou</last></author>
      <pages>33–37</pages>
      <abstract>The Biological Text Mining Unit at BSC and CNIO organized the first shared task on chemical &amp; drug mention recognition from Spanish medical texts called PharmaCoNER (Pharmacological Substances, Compounds and proteins and Named Entity Recognition track) in 2019, which includes two tracks : one for NER offset and entity classification (track 1) and the other one for concept indexing (track 2). We developed a pipeline system based on deep learning methods for this shared task, specifically, a subsystem based on BERT (Bidirectional Encoder Representations from Transformers) for NER offset and entity classification and a subsystem based on Bpool (Bi-LSTM with max / mean pooling) for concept indexing. Evaluation conducted on the shared task data showed that our system achieves a micro-average F1-score of 0.9105 on track 1 and a micro-average F1-score of 0.8391 on track 2.</abstract>
      <url hash="3b00cef2">D19-5706</url>
      <doi>10.18653/v1/D19-5706</doi>
      <bibkey>xiong-etal-2019-deep</bibkey>
    </paper>
    <paper id="8">
      <title>A Neural Pipeline Approach for the PharmaCoNER Shared Task using Contextual Exhaustive Models<fixed-case>P</fixed-case>harma<fixed-case>C</fixed-case>o<fixed-case>NER</fixed-case> Shared Task using Contextual Exhaustive Models</title>
      <author><first>Mohammad Golam</first><last>Sohrab</last></author>
      <author><first>Minh Thang</first><last>Pham</last></author>
      <author><first>Makoto</first><last>Miwa</last></author>
      <author><first>Hiroya</first><last>Takamura</last></author>
      <pages>47–55</pages>
      <abstract>We present a neural pipeline approach that performs named entity recognition (NER) and concept indexing (CI), which links them to concept unique identifiers (CUIs) in a knowledge base, for the PharmaCoNER shared task on pharmaceutical drugs and chemical entities. We proposed a neural NER model that captures the surrounding semantic information of a given sequence by capturing the forward- and backward-context of bidirectional LSTM (Bi-LSTM) output of a target span using contextual span representation-based exhaustive approach. The <a href="https://en.wikipedia.org/wiki/NER_model">NER model</a> enumerates all possible spans as potential entity mentions and classify them into entity types or no entity with <a href="https://en.wikipedia.org/wiki/Deep_learning">deep neural networks</a>. For representing span, we compare several different neural network architectures and their <a href="https://en.wikipedia.org/wiki/Network_topology">ensembling</a> for the <a href="https://en.wikipedia.org/wiki/NER_model">NER model</a>. We then perform dictionary matching for CI and, if there is no matching, we further compute similarity scores between a mention and CUIs using entity embeddings to assign the CUI with the highest score to the mention. We evaluate our <a href="https://en.wikipedia.org/wiki/Software_development_process">approach</a> on the two sub-tasks in the shared task. Among the five submitted runs, the best run for each <a href="https://en.wikipedia.org/wiki/Task_(project_management)">sub-task</a> achieved the F-score of 86.76 % on Sub-task 1 (NER) and the F-score of 79.97 % (strict) on Sub-task 2 (CI).</abstract>
      <url hash="a643037b">D19-5708</url>
      <doi>10.18653/v1/D19-5708</doi>
      <bibkey>sohrab-etal-2019-neural</bibkey>
    </paper>
    <paper id="9">
      <title>Biomedical Named Entity Recognition with Multilingual BERT<fixed-case>BERT</fixed-case></title>
      <author><first>Kai</first><last>Hakala</last></author>
      <author><first>Sampo</first><last>Pyysalo</last></author>
      <pages>56–61</pages>
      <abstract>We present the approach of the Turku NLP group to the PharmaCoNER task on Spanish biomedical named entity recognition. We apply a CRF-based baseline approach and multilingual BERT to the task, achieving an F-score of 88 % on the development data and 87 % on the test set with BERT. Our approach reflects a straightforward application of a state-of-the-art multilingual model that is not specifically tailored to either the language nor the application domain. The source code is available at : https://github.com/chaanim/pharmaconer</abstract>
      <url hash="a09bf826">D19-5709</url>
      <doi>10.18653/v1/D19-5709</doi>
      <bibkey>hakala-pyysalo-2019-biomedical</bibkey>
      <pwccode url="https://github.com/chaanim/pharmaconer" additional="false">chaanim/pharmaconer</pwccode>
    </paper>
    <paper id="10">
      <title>An Overview of the Active Gene Annotation Corpus and the BioNLP OST 2019 AGAC Track Tasks<fixed-case>B</fixed-case>io<fixed-case>NLP</fixed-case> <fixed-case>OST</fixed-case> 2019 <fixed-case>AGAC</fixed-case> Track Tasks</title>
      <author><first>Yuxing</first><last>Wang</last></author>
      <author><first>Kaiyin</first><last>Zhou</last></author>
      <author><first>Mina</first><last>Gachloo</last></author>
      <author><first>Jingbo</first><last>Xia</last></author>
      <pages>62–71</pages>
      <abstract>The active gene annotation corpus (AGAC) was developed to support <a href="https://en.wikipedia.org/wiki/Knowledge_discovery">knowledge discovery</a> for <a href="https://en.wikipedia.org/wiki/Drug_repurposing">drug repurposing</a>. Based on the corpus, the AGAC track of the BioNLP Open Shared Tasks 2019 was organized, to facilitate cross-disciplinary collaboration across BioNLP and Pharmacoinformatics communities, for <a href="https://en.wikipedia.org/wiki/Drug_repurposing">drug repurposing</a>. The AGAC track consists of three subtasks : 1) <a href="https://en.wikipedia.org/wiki/Named-entity_recognition">named entity recognition</a>, 2) thematic relation extraction, and 3) loss of function (LOF) / gain of function (GOF) topic classification. The AGAC track was participated by five teams, of which the performance are compared and analyzed. The the results revealed a substantial room for improvement in the design of the task, which we analyzed in terms of imbalanced data, selective annotation and latent topic annotation.</abstract>
      <url hash="33c1f265">D19-5710</url>
      <doi>10.18653/v1/D19-5710</doi>
      <bibkey>wang-etal-2019-overview</bibkey>
      <pwccode url="https://github.com/YaoXinZhi/BERT-CRF-for-BioNLP-OST2019-AGAC-Task1" additional="false">YaoXinZhi/BERT-CRF-for-BioNLP-OST2019-AGAC-Task1</pwccode>
    </paper>
    <paper id="16">
      <title>A Multi-Task Learning Framework for Extracting Bacteria Biotope Information</title>
      <author><first>Qi</first><last>Zhang</last></author>
      <author><first>Chao</first><last>Liu</last></author>
      <author><first>Ying</first><last>Chi</last></author>
      <author><first>Xuansong</first><last>Xie</last></author>
      <author><first>Xiansheng</first><last>Hua</last></author>
      <pages>105–109</pages>
      <abstract>This paper presents a novel transfer multi-task learning method for Bacteria Biotope rel+ner task at BioNLP-OST 2019. To alleviate the data deficiency problem in domain-specific information extraction, we use BERT(Bidirectional Encoder Representations from Transformers) and pre-train it using mask language models and next sentence prediction on both general corpus and medical corpus like <a href="https://en.wikipedia.org/wiki/PubMed">PubMed</a>. In fine-tuning stage, we fine-tune the relation extraction layer and mention recognition layer designed by us on the top of BERT to extract mentions and relations simultaneously. The evaluation results show that our method achieves the best performance on all metrics (including slot error rate, <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">precision</a> and recall) in the Bacteria Biotope rel+ner subtask.</abstract>
      <url hash="498528b1">D19-5716</url>
      <doi>10.18653/v1/D19-5716</doi>
      <bibkey>zhang-etal-2019-multi-task</bibkey>
    </paper>
    <paper id="18">
      <title>Using Snomed to recognize and index chemical and drug mentions.</title>
      <author><first>Pilar</first><last>López Úbeda</last></author>
      <author><first>Manuel Carlos</first><last>Díaz Galiano</last></author>
      <author><first>L. Alfonso</first><last>Urena Lopez</last></author>
      <author><first>Maite</first><last>Martin</last></author>
      <pages>115–120</pages>
      <abstract>In this paper we describe a new named entity extraction system. Our work proposes a system for the identification and annotation of drug names in Spanish biomedical texts based on machine learning and deep learning models. Subsequently, a standardized code using <a href="https://en.wikipedia.org/wiki/Snomed">Snomed</a> is assigned to these <a href="https://en.wikipedia.org/wiki/Medication">drugs</a>, for this purpose, Natural Language Processing tools and techniques have been used, and a dictionary of different sources of information has been built. The results are promising, we obtain 78 % in F1 score on the first sub-track and in the second task we map with <a href="https://en.wikipedia.org/wiki/Snomed">Snomed</a> correctly 72 % of the found entities.</abstract>
      <url hash="8b3387f5">D19-5718</url>
      <doi>10.18653/v1/D19-5718</doi>
      <bibkey>lopez-ubeda-etal-2019-using</bibkey>
    </paper>
    <paper id="20">
      <title>Linguistically Informed Relation Extraction and Neural Architectures for Nested Named Entity Recognition in BioNLP-OST 2019<fixed-case>B</fixed-case>io<fixed-case>NLP</fixed-case>-<fixed-case>OST</fixed-case> 2019</title>
      <author><first>Pankaj</first><last>Gupta</last></author>
      <author><first>Usama</first><last>Yaseen</last></author>
      <author><first>Hinrich</first><last>Schütze</last></author>
      <pages>132–142</pages>
      <abstract>Named Entity Recognition (NER) and Relation Extraction (RE) are essential tools in distilling knowledge from biomedical literature. This paper presents our findings from participating in BioNLP Shared Tasks 2019. We addressed <a href="https://en.wikipedia.org/wiki/Named-entity_recognition">Named Entity Recognition</a> including <a href="https://en.wikipedia.org/wiki/Named-entity_recognition">nested entities extraction</a>, Entity Normalization and <a href="https://en.wikipedia.org/wiki/Named-entity_recognition">Relation Extraction</a>. Our proposed approach of <a href="https://en.wikipedia.org/wiki/Named_entity">Named Entities</a> can be generalized to different languages and we have shown it’s effectiveness for English and Spanish text. We investigated linguistic features, hybrid loss including ranking and Conditional Random Fields (CRF), multi-task objective and token level ensembling strategy to improve NER. We employed dictionary based fuzzy and semantic search to perform Entity Normalization. Finally, our RE system employed Support Vector Machine (SVM) with <a href="https://en.wikipedia.org/wiki/Feature_(linguistics)">linguistic features</a>. Our NER submission (team : MIC-CIS) ranked first in BB-2019 norm+NER task with standard error rate (SER) of 0.7159 and showed competitive performance on PharmaCo NER task with F1-score of 0.8662. Our RE system ranked first in the SeeDev-binary Relation Extraction Task with F1-score of 0.3738.</abstract>
      <url hash="d1ee7d5f">D19-5720</url>
      <doi>10.18653/v1/D19-5720</doi>
      <bibkey>gupta-etal-2019-linguistically</bibkey>
      <pwccode url="https://github.com/uyaseen/bionlp-ost-2019" additional="false">uyaseen/bionlp-ost-2019</pwccode>
    </paper>
    <paper id="21">
      <title>An ensemble CNN method for biomedical entity normalization<fixed-case>CNN</fixed-case> method for biomedical entity normalization</title>
      <author><first>Pan</first><last>Deng</last></author>
      <author><first>Haipeng</first><last>Chen</last></author>
      <author><first>Mengyao</first><last>Huang</last></author>
      <author><first>Xiaowen</first><last>Ruan</last></author>
      <author><first>Liang</first><last>Xu</last></author>
      <pages>143–149</pages>
      <abstract>Different representations of the same concept could often be seen in <a href="https://en.wikipedia.org/wiki/Scientific_literature">scientific reports</a> and publications. Entity normalization (or entity linking) is the task to match the different <a href="https://en.wikipedia.org/wiki/Representation_(systemics)">representations</a> to their standard concepts. In this paper, we present a two-step ensemble CNN method that normalizes microbiology-related entities in free text to concepts in standard <a href="https://en.wikipedia.org/wiki/Dictionary">dictionaries</a>. The method is capable of linking entities when only a small microbiology-related biomedical corpus is available for training, and achieved reasonable performance in the online test of the BioNLP-OST19 shared task Bacteria Biotope.</abstract>
      <url hash="91c8e6ab">D19-5721</url>
      <doi>10.18653/v1/D19-5721</doi>
      <bibkey>deng-etal-2019-ensemble</bibkey>
    </paper>
    <paper id="22">
      <title>BOUN-ISIK Participation : An Unsupervised Approach for the Named Entity Normalization and Relation Extraction of Bacteria Biotopes<fixed-case>BOUN</fixed-case>-<fixed-case>ISIK</fixed-case> Participation: An Unsupervised Approach for the Named Entity Normalization and Relation Extraction of Bacteria Biotopes</title>
      <author><first>İlknur</first><last>Karadeniz</last></author>
      <author><first>Ömer Faruk</first><last>Tuna</last></author>
      <author><first>Arzucan</first><last>Özgür</last></author>
      <pages>150–157</pages>
      <abstract>This paper presents our participation to the Bacteria Biotope Task of the BioNLP Shared Task 2019. Our participation includes two systems for the two subtasks of the Bacteria Biotope Task : the normalization of entities (BB-norm) and the identification of the relations between the entities given a biomedical text (BB-rel). For the normalization of entities, we utilized <a href="https://en.wikipedia.org/wiki/Word_embedding">word embeddings</a> and syntactic re-ranking. For the relation extraction task, pre-defined rules are used. Although both approaches are unsupervised, in the sense that they do not need any labeled data, they achieved promising results. Especially, for the BB-norm task, the results have shown that the proposed method performs as good as <a href="https://en.wikipedia.org/wiki/Deep_learning">deep learning based methods</a>, which require labeled data.</abstract>
      <url hash="4dc34a55">D19-5722</url>
      <doi>10.18653/v1/D19-5722</doi>
      <bibkey>karadeniz-etal-2019-boun</bibkey>
    </paper>
    <paper id="24">
      <title>Integration of <a href="https://en.wikipedia.org/wiki/Deep_learning">Deep Learning</a> and Traditional <a href="https://en.wikipedia.org/wiki/Machine_learning">Machine Learning</a> for <a href="https://en.wikipedia.org/wiki/Knowledge_extraction">Knowledge Extraction</a> from Biomedical Literature</title>
      <author><first>Jihang</first><last>Mao</last></author>
      <author><first>Wanli</first><last>Liu</last></author>
      <pages>168–173</pages>
      <abstract>In this paper, we present our participation in the Bacteria Biotope (BB) task at BioNLP-OST 2019. Our system utilizes fine-tuned language representation models and machine learning approaches based on <a href="https://en.wikipedia.org/wiki/Word_embedding">word embedding</a> and lexical features for entities recognition, normalization and relation extraction. It achieves the state-of-the-art performance and is among the top two systems in five of all six subtasks.</abstract>
      <url hash="54076ad4">D19-5724</url>
      <doi>10.18653/v1/D19-5724</doi>
      <bibkey>mao-liu-2019-integration</bibkey>
    </paper>
    <paper id="25">
      <title>CRAFT Shared Tasks 2019 Overview   Integrated Structure, <a href="https://en.wikipedia.org/wiki/Semantics">Semantics</a>, and <a href="https://en.wikipedia.org/wiki/Coreference">Coreference</a><fixed-case>CRAFT</fixed-case> Shared Tasks 2019 Overview — Integrated Structure, Semantics, and Coreference</title>
      <author><first>William</first><last>Baumgartner</last></author>
      <author><first>Michael</first><last>Bada</last></author>
      <author><first>Sampo</first><last>Pyysalo</last></author>
      <author><first>Manuel R.</first><last>Ciosici</last></author>
      <author><first>Negacy</first><last>Hailu</last></author>
      <author><first>Harrison</first><last>Pielke-Lombardo</last></author>
      <author><first>Michael</first><last>Regan</last></author>
      <author><first>Lawrence</first><last>Hunter</last></author>
      <pages>174–184</pages>
      <abstract>As part of the BioNLP Open Shared Tasks 2019, the CRAFT Shared Tasks 2019 provides a platform to gauge the state of the art for three fundamental language processing tasks   dependency parse construction, coreference resolution, and ontology concept identification   over full-text biomedical articles. The structural annotation task requires the automatic generation of dependency parses for each sentence of an article given only the article text. The coreference resolution task focuses on linking coreferring base noun phrase mentions into chains using the symmetrical and transitive identity relation. The ontology concept annotation task involves the identification of concept mentions within text using the classes of ten distinct ontologies in the biomedical domain, both unmodified and augmented with extension classes. This paper provides an overview of each <a href="https://en.wikipedia.org/wiki/Task_(project_management)">task</a>, including descriptions of the data provided to participants and the evaluation metrics used, and discusses participant results relative to baseline performances for each of the three <a href="https://en.wikipedia.org/wiki/Task_(project_management)">tasks</a>.</abstract>
      <url hash="ab617d72">D19-5725</url>
      <doi>10.18653/v1/D19-5725</doi>
      <bibkey>baumgartner-etal-2019-craft</bibkey>
    </paper>
    <paper id="26">
      <title>UZH@CRAFT-ST : a Sequence-labeling Approach to Concept Recognition<fixed-case>UZH</fixed-case>@<fixed-case>CRAFT</fixed-case>-<fixed-case>ST</fixed-case>: a Sequence-labeling Approach to Concept Recognition</title>
      <author><first>Lenz</first><last>Furrer</last></author>
      <author><first>Joseph</first><last>Cornelius</last></author>
      <author><first>Fabio</first><last>Rinaldi</last></author>
      <pages>185–195</pages>
      <abstract>As our submission to the CRAFT shared task 2019, we present two neural approaches to concept recognition. We propose two different systems for joint named entity recognition (NER) and normalization (NEN), both of which model the task as a sequence labeling problem. Our first system is a BiLSTM network with two separate outputs for NER and NEN trained from scratch, whereas the second system is an instance of BioBERT fine-tuned on the concept-recognition task. We exploit two strategies for extending concept coverage, ontology pretraining and <a href="https://en.wikipedia.org/wiki/Backoff">backoff</a> with a dictionary lookup. Our results show that the backoff strategy effectively tackles the problem of unseen concepts, addressing a major limitation of the chosen <a href="https://en.wikipedia.org/wiki/Design">design</a>. In the cross-system comparison, BioBERT proves to be a strong basis for creating a concept-recognition system, although some entity types are predicted more accurately by the BiLSTM-based system.</abstract>
      <url hash="ab6a4ae5">D19-5726</url>
      <doi>10.18653/v1/D19-5726</doi>
      <bibkey>furrer-etal-2019-uzh</bibkey>
    </paper>
    <paper id="28">
      <title>Neural Dependency Parsing of Biomedical Text : TurkuNLP entry in the CRAFT Structural Annotation Task<fixed-case>T</fixed-case>urku<fixed-case>NLP</fixed-case> entry in the <fixed-case>CRAFT</fixed-case> Structural Annotation Task</title>
      <author><first>Thang Minh</first><last>Ngo</last></author>
      <author><first>Jenna</first><last>Kanerva</last></author>
      <author><first>Filip</first><last>Ginter</last></author>
      <author><first>Sampo</first><last>Pyysalo</last></author>
      <pages>206–215</pages>
      <abstract>We present the approach taken by the TurkuNLP group in the CRAFT Structural Annotation task, a shared task on dependency parsing. Our approach builds primarily on the Turku neural parser, a native dependency parser that ranked among the best in the recent CoNLL tasks on parsing Universal Dependencies. To adapt the <a href="https://en.wikipedia.org/wiki/Parsing">parser</a> to the biomedical domain, we considered and evaluated a number of approaches, including the generation of custom word embeddings, combination with other in-domain resources, and the incorporation of information from <a href="https://en.wikipedia.org/wiki/Named-entity_recognition">named entity recognition</a>. We achieved a labeled attachment score of 89.7 %, the best result among task participants.</abstract>
      <url hash="d38300ee">D19-5728</url>
      <doi>10.18653/v1/D19-5728</doi>
      <bibkey>ngo-etal-2019-neural</bibkey>
    </paper>
    </volume>
  <volume id="58" ingest-date="2019-11-03">
    <meta>
      <booktitle>Proceedings of the 2nd Workshop on Machine Reading for Question Answering</booktitle>
      <url hash="8bf69b4f">D19-58</url>
      <editor><first>Adam</first><last>Fisch</last></editor>
      <editor><first>Alon</first><last>Talmor</last></editor>
      <editor><first>Robin</first><last>Jia</last></editor>
      <editor><first>Minjoon</first><last>Seo</last></editor>
      <editor><first>Eunsol</first><last>Choi</last></editor>
      <editor><first>Danqi</first><last>Chen</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Hong Kong, China</address>
      <month>November</month>
      <year>2019</year>
    </meta>
    <frontmatter>
      <url hash="ec6509c8">D19-5800</url>
      <bibkey>emnlp-2019-machine</bibkey>
    </frontmatter>
    <paper id="2">
      <title>Inspecting Unification of Encoding and Matching with Transformer : A Case Study of Machine Reading Comprehension</title>
      <author><first>Hangbo</first><last>Bao</last></author>
      <author><first>Li</first><last>Dong</last></author>
      <author><first>Furu</first><last>Wei</last></author>
      <author><first>Wenhui</first><last>Wang</last></author>
      <author><first>Nan</first><last>Yang</last></author>
      <author><first>Lei</first><last>Cui</last></author>
      <author><first>Songhao</first><last>Piao</last></author>
      <author><first>Ming</first><last>Zhou</last></author>
      <pages>14–18</pages>
      <abstract>Most machine reading comprehension (MRC) models separately handle encoding and matching with different <a href="https://en.wikipedia.org/wiki/Network_architecture">network architectures</a>. In contrast, pretrained language models with Transformer layers, such as GPT (Radford et al., 2018) and BERT (Devlin et al., 2018), have achieved competitive performance on MRC. A research question that naturally arises is : apart from the benefits of pre-training, how many performance gain comes from the unified network architecture. In this work, we evaluate and analyze unifying encoding and matching components with Transformer for the MRC task. Experimental results on SQuAD show that the <a href="https://en.wikipedia.org/wiki/Unified_Model">unified model</a> outperforms previous networks that separately treat <a href="https://en.wikipedia.org/wiki/Code">encoding</a> and <a href="https://en.wikipedia.org/wiki/Matching_(graph_theory)">matching</a>. We also introduce a <a href="https://en.wikipedia.org/wiki/Metric_(mathematics)">metric</a> to inspect whether a Transformer layer tends to perform <a href="https://en.wikipedia.org/wiki/Code">encoding</a> or matching. The analysis results show that the <a href="https://en.wikipedia.org/wiki/Unified_model">unified model</a> learns different <a href="https://en.wikipedia.org/wiki/Mathematical_model">modeling strategies</a> compared with previous manually-designed models.</abstract>
      <url hash="e9e7b10c">D19-5802</url>
      <doi>10.18653/v1/D19-5802</doi>
      <bibkey>bao-etal-2019-inspecting</bibkey>
    </paper>
    <paper id="3">
      <title>CALOR-QUEST : generating a training corpus for Machine Reading Comprehension models from shallow semantic annotations<fixed-case>CALOR</fixed-case>-<fixed-case>QUEST</fixed-case> : generating a training corpus for Machine Reading Comprehension models from shallow semantic annotations</title>
      <author><first>Frederic</first><last>Bechet</last></author>
      <author><first>Cindy</first><last>Aloui</last></author>
      <author><first>Delphine</first><last>Charlet</last></author>
      <author><first>Geraldine</first><last>Damnati</last></author>
      <author><first>Johannes</first><last>Heinecke</last></author>
      <author><first>Alexis</first><last>Nasr</last></author>
      <author><first>Frederic</first><last>Herledan</last></author>
      <pages>19–26</pages>
      <abstract>Machine reading comprehension is a <a href="https://en.wikipedia.org/wiki/Task_(project_management)">task</a> related to <a href="https://en.wikipedia.org/wiki/Question_answering">Question-Answering</a> where questions are not generic in scope but are related to a particular document. Recently very large corpora (SQuAD, MS MARCO) containing triplets (document, question, answer) were made available to the scientific community to develop supervised methods based on deep neural networks with promising results. These methods need very large training corpus to be efficient, however such kind of <a href="https://en.wikipedia.org/wiki/Data">data</a> only exists for <a href="https://en.wikipedia.org/wiki/English_language">English</a> and <a href="https://en.wikipedia.org/wiki/Chinese_language">Chinese</a> at the moment. The aim of this study is the development of such resources for other languages by proposing to generate in a semi-automatic way questions from the semantic Frame analysis of large corpora. The collect of natural questions is reduced to a <a href="https://en.wikipedia.org/wiki/Validity_(statistics)">validation / test set</a>. We applied this method on the CALOR-Frame French corpus to develop the CALOR-QUEST resource presented in this paper.</abstract>
      <url hash="81fde26b">D19-5803</url>
      <doi>10.18653/v1/D19-5803</doi>
      <bibkey>bechet-etal-2019-calor-quest</bibkey>
    </paper>
    <paper id="5">
      <title>Answer-Supervised Question Reformulation for Enhancing Conversational Machine Comprehension</title>
      <author><first>Qian</first><last>Li</last></author>
      <author><first>Hui</first><last>Su</last></author>
      <author><first>Cheng</first><last>Niu</last></author>
      <author><first>Daling</first><last>Wang</last></author>
      <author><first>Zekang</first><last>Li</last></author>
      <author><first>Shi</first><last>Feng</last></author>
      <author><first>Yifei</first><last>Zhang</last></author>
      <pages>38–47</pages>
      <abstract>In conversational machine comprehension, it has become one of the research hotspots integrating conversational history information through question reformulation for obtaining better answers. However, the existing question reformulation models are trained only using supervised question labels annotated by annotators without considering any feedback information from answers. In this paper, we propose a novel Answer-Supervised Question Reformulation (ASQR) model for enhancing conversational machine comprehension with reinforcement learning technology. ASQR utilizes a pointer-copy-based question reformulation model as an agent, takes an action to predict the next word, and observes a reward for the whole sentence state after generating the end-of-sequence token. The experimental results on QuAC dataset prove that our ASQR model is more effective in conversational machine comprehension. Moreover, pretraining is essential in <a href="https://en.wikipedia.org/wiki/Reinforcement_learning">reinforcement learning models</a>, so we provide a high-quality annotated dataset for question reformulation by sampling a part of QuAC dataset.</abstract>
      <url hash="00be8755">D19-5805</url>
      <attachment hash="1da13632">D19-5805.Attachment.zip</attachment>
      <doi>10.18653/v1/D19-5805</doi>
      <bibkey>li-etal-2019-answer</bibkey>
    </paper>
    <paper id="7">
      <title>Improving the Robustness of Deep Reading Comprehension Models by Leveraging Syntax Prior</title>
      <author><first>Bowen</first><last>Wu</last></author>
      <author><first>Haoyang</first><last>Huang</last></author>
      <author><first>Zongsheng</first><last>Wang</last></author>
      <author><first>Qihang</first><last>Feng</last></author>
      <author><first>Jingsong</first><last>Yu</last></author>
      <author><first>Baoxun</first><last>Wang</last></author>
      <pages>53–57</pages>
      <abstract>Despite the remarkable progress on Machine Reading Comprehension (MRC) with the help of open-source datasets, recent studies indicate that most of the current MRC systems unfortunately suffer from weak <a href="https://en.wikipedia.org/wiki/Robustness_(computer_science)">robustness</a> against adversarial samples. To address this issue, we attempt to take <a href="https://en.wikipedia.org/wiki/Sentence_(linguistics)">sentence syntax</a> as the leverage in the answer predicting process which previously only takes account of phrase-level semantics. Furthermore, to better utilize the sentence syntax and improve the robustness, we propose a Syntactic Leveraging Network, which is designed to deal with adversarial samples by exploiting the syntactic elements of a question. The experiment results indicate that our method is promising for improving the generalization and robustness of MRC models against the influence of adversarial samples, with performance well-maintained.</abstract>
      <url hash="0fa71771">D19-5807</url>
      <doi>10.18653/v1/D19-5807</doi>
      <bibkey>wu-etal-2019-improving</bibkey>
    </paper>
    <paper id="8">
      <title>Reasoning Over Paragraph Effects in Situations</title>
      <author><first>Kevin</first><last>Lin</last></author>
      <author><first>Oyvind</first><last>Tafjord</last></author>
      <author><first>Peter</first><last>Clark</last></author>
      <author><first>Matt</first><last>Gardner</last></author>
      <pages>58–62</pages>
      <abstract>A key component of successfully reading a passage of text is the ability to apply knowledge gained from the passage to a new situation. In order to facilitate progress on this kind of reading, we present <a href="https://en.wikipedia.org/wiki/ROPES">ROPES</a>, a challenging <a href="https://en.wikipedia.org/wiki/Benchmarking">benchmark</a> for <a href="https://en.wikipedia.org/wiki/Reading_comprehension">reading comprehension</a> targeting Reasoning Over Paragraph Effects in Situations. We target expository language describing causes and effects (e.g., animal pollinators increase efficiency of fertilization in flowers), as they have clear implications for new situations. A <a href="https://en.wikipedia.org/wiki/System">system</a> is presented a background passage containing at least one of these relations, a novel situation that uses this <a href="https://en.wikipedia.org/wiki/Context_(language_use)">background</a>, and questions that require reasoning about effects of the relationships in the background passage in the context of the situation. We collect background passages from science textbooks and <a href="https://en.wikipedia.org/wiki/Wikipedia">Wikipedia</a> that contain such phenomena, and ask crowd workers to author situations, questions, and answers, resulting in a 14,322 question dataset. We analyze the challenges of this task and evaluate the performance of state-of-the-art reading comprehension models. The best <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> performs only slightly better than randomly guessing an answer of the correct type, at 61.6 % <a href="https://en.wikipedia.org/wiki/F-number">F1</a>, well below the human performance of 89.0 %.</abstract>
      <url hash="d2dfdeae">D19-5808</url>
      <attachment hash="e89779a0">D19-5808.Attachment.tgz</attachment>
      <doi>10.18653/v1/D19-5808</doi>
      <bibkey>lin-etal-2019-reasoning</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/ropes">ROPES</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/quarel">QuaRel</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sharc">ShARC</pwcdataset>
    </paper>
    <paper id="9">
      <title>Towards Answer-unaware Conversational Question Generation</title>
      <author><first>Mao</first><last>Nakanishi</last></author>
      <author><first>Tetsunori</first><last>Kobayashi</last></author>
      <author><first>Yoshihiko</first><last>Hayashi</last></author>
      <pages>63–71</pages>
      <abstract>Conversational question generation is a novel area of <a href="https://en.wikipedia.org/wiki/Neuro-linguistic_programming">NLP research</a> which has a range of potential applications. This paper is first to presents a <a href="https://en.wikipedia.org/wiki/Conceptual_framework">framework</a> for conversational question generation that is unaware of the corresponding answers. To properly generate a question coherent to the grounding text and the current conversation history, the proposed framework first locates the focus of a question in the text passage, and then identifies the question pattern that leads the sequential generation of the words in a question. The experiments using the CoQA dataset demonstrate that the quality of generated questions greatly improves if the question foci and the question patterns are correctly identified. In addition, it was shown that the question foci, even estimated with a reasonable <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a>, could contribute to the quality improvement. These results established that our research direction may be promising, but at the same time revealed that the identification of question patterns is a challenging issue, and it has to be largely refined to achieve a better quality in the end-to-end automatic question generation.</abstract>
      <url hash="7600060c">D19-5809</url>
      <doi>10.18653/v1/D19-5809</doi>
      <bibkey>nakanishi-etal-2019-towards</bibkey>
    </paper>
    <paper id="10">
      <title>Cross-Task Knowledge Transfer for Query-Based Text Summarization</title>
      <author><first>Elozino</first><last>Egonmwan</last></author>
      <author><first>Vittorio</first><last>Castelli</last></author>
      <author><first>Md Arafat</first><last>Sultan</last></author>
      <pages>72–77</pages>
      <abstract>We demonstrate the viability of <a href="https://en.wikipedia.org/wiki/Knowledge_transfer">knowledge transfer</a> between two related tasks : machine reading comprehension (MRC) and query-based text summarization. Using an <a href="https://en.wikipedia.org/wiki/Model-driven_architecture">MRC model</a> trained on the SQuAD1.1 dataset as a core system component, we first build an extractive query-based summarizer. For better precision, this <a href="https://en.wikipedia.org/wiki/Automatic_summarization">summarizer</a> also compresses the output of the MRC model using a novel sentence compression technique. We further leverage pre-trained machine translation systems to abstract our extracted summaries. Our models achieve state-of-the-art results on the publicly available CNN / Daily Mail and Debatepedia datasets, and can serve as simple yet powerful baselines for future systems. We also hope that these results will encourage research on <a href="https://en.wikipedia.org/wiki/Transfer_learning">transfer learning</a> from large MRC corpora to query-based summarization.</abstract>
      <url hash="d4f33d49">D19-5810</url>
      <doi>10.18653/v1/D19-5810</doi>
      <bibkey>egonmwan-etal-2019-cross</bibkey>
    </paper>
    <paper id="14">
      <title>Machine Comprehension Improves Domain-Specific Japanese Predicate-Argument Structure Analysis<fixed-case>J</fixed-case>apanese Predicate-Argument Structure Analysis</title>
      <author><first>Norio</first><last>Takahashi</last></author>
      <author><first>Tomohide</first><last>Shibata</last></author>
      <author><first>Daisuke</first><last>Kawahara</last></author>
      <author><first>Sadao</first><last>Kurohashi</last></author>
      <pages>98–104</pages>
      <abstract>To improve the <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> of predicate-argument structure (PAS) analysis, large-scale training data and knowledge for PAS analysis are indispensable. We focus on a specific domain, specifically Japanese blogs on driving, and construct two wide-coverage datasets as a form of QA using <a href="https://en.wikipedia.org/wiki/Crowdsourcing">crowdsourcing</a> : a PAS-QA dataset and a reading comprehension QA (RC-QA) dataset. We train a machine comprehension (MC) model based on these <a href="https://en.wikipedia.org/wiki/Data_set">datasets</a> to perform PAS analysis. Our experiments show that a stepwise training method is the most effective, which pre-trains an MC model based on the RC-QA dataset to acquire <a href="https://en.wikipedia.org/wiki/Domain_knowledge">domain knowledge</a> and then fine-tunes based on the PAS-QA dataset.</abstract>
      <url hash="784e0115">D19-5814</url>
      <doi>10.18653/v1/D19-5814</doi>
      <bibkey>takahashi-etal-2019-machine</bibkey>
    </paper>
    <paper id="18">
      <title>Bend but Do n’t Break? Multi-Challenge Stress Test for QA Models<fixed-case>QA</fixed-case> Models</title>
      <author><first>Hemant</first><last>Pugaliya</last></author>
      <author><first>James</first><last>Route</last></author>
      <author><first>Kaixin</first><last>Ma</last></author>
      <author><first>Yixuan</first><last>Geng</last></author>
      <author><first>Eric</first><last>Nyberg</last></author>
      <pages>125–136</pages>
      <abstract>The field of <a href="https://en.wikipedia.org/wiki/Question_answering">question answering (QA)</a> has seen rapid growth in new <a href="https://en.wikipedia.org/wiki/Task_(project_management)">tasks</a> and <a href="https://en.wikipedia.org/wiki/Modeling_and_simulation">modeling approaches</a> in recent years. Large scale datasets and focus on challenging linguistic phenomena have driven development in neural models, some of which have achieved parity with human performance in limited cases. However, an examination of state-of-the-art <a href="https://en.wikipedia.org/wiki/Mathematical_model">model output</a> reveals that a gap remains in reasoning ability compared to a human, and performance tends to degrade when <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> are exposed to less-constrained tasks. We are interested in more clearly defining the strengths and limitations of leading models across diverse QA challenges, intending to help future researchers with identifying pathways to generalizable performance. We conduct extensive qualitative and quantitative analyses on the results of four <a href="https://en.wikipedia.org/wiki/Statistical_model">models</a> across four datasets and relate common errors to model capabilities. We also illustrate limitations in the datasets we examine and discuss a way forward for achieving generalizable models and datasets that broadly test QA capabilities.</abstract>
      <url hash="688cbe26">D19-5818</url>
      <attachment hash="2caefec5">D19-5818.Attachment.pdf</attachment>
      <doi>10.18653/v1/D19-5818</doi>
      <bibkey>pugaliya-etal-2019-bend</bibkey>
    </paper>
    <paper id="23">
      <title>Extractive NarrativeQA with Heuristic Pre-Training<fixed-case>N</fixed-case>arrative<fixed-case>QA</fixed-case> with Heuristic Pre-Training</title>
      <author><first>Lea</first><last>Frermann</last></author>
      <pages>172–182</pages>
      <abstract>Although advances in neural architectures for NLP problems as well as <a href="https://en.wikipedia.org/wiki/Unsupervised_learning">unsupervised pre-training</a> have led to substantial improvements on <a href="https://en.wikipedia.org/wiki/Question_answering">question answering</a> and natural language inference, understanding of and reasoning over long texts still poses a substantial challenge. Here, we consider the task of <a href="https://en.wikipedia.org/wiki/Question_answering">question answering</a> from full narratives (e.g., books or movie scripts), or their summaries, tackling the NarrativeQA challenge (NQA ; Kocisky et al. We introduce a heuristic extractive version of the <a href="https://en.wikipedia.org/wiki/Data_set">data set</a>, which allows us to approach the more feasible problem of answer extraction (rather than generation). We train systems for passage retrieval as well as answer span prediction using this <a href="https://en.wikipedia.org/wiki/Data_set">data set</a>. We use pre-trained BERT embeddings for injecting prior knowledge into our <a href="https://en.wikipedia.org/wiki/System">system</a>. We show that our setup leads to state of the art performance on summary-level QA. On QA from full narratives, our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> outperforms previous <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> on the METEOR metric. We analyze the relative contributions of pre-trained embeddings and the extractive training paradigm, and provide a detailed error analysis.</abstract>
      <url hash="38ecd876">D19-5823</url>
      <doi>10.18653/v1/D19-5823</doi>
      <bibkey>frermann-2019-extractive</bibkey>
    </paper>
    <paper id="24">
      <title>CLER : Cross-task Learning with Expert Representation to Generalize Reading and Understanding<fixed-case>CLER</fixed-case>: Cross-task Learning with Expert Representation to Generalize Reading and Understanding</title>
      <author><first>Takumi</first><last>Takahashi</last></author>
      <author><first>Motoki</first><last>Taniguchi</last></author>
      <author><first>Tomoki</first><last>Taniguchi</last></author>
      <author><first>Tomoko</first><last>Ohkuma</last></author>
      <pages>183–190</pages>
      <abstract>This paper describes our <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a> for the <a href="https://en.wikipedia.org/wiki/Reading_comprehension">reading comprehension task</a> of the MRQA shared task. We propose CLER, which stands for Cross-task Learning with Expert Representation for the generalization of reading and understanding. To generalize its capabilities, the proposed model is composed of three key ideas : <a href="https://en.wikipedia.org/wiki/Multi-task_learning">multi-task learning</a>, mixture of experts, and <a href="https://en.wikipedia.org/wiki/Ensemble_learning">ensemble</a>. In-domain datasets are used to train and validate our <a href="https://en.wikipedia.org/wiki/Statistical_model">model</a>, and other out-of-domain datasets are used to validate the generalization of our <a href="https://en.wikipedia.org/wiki/Statistical_model">model</a>’s performances. In a submission run result, the proposed <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> achieved an average F1 score of 66.1 % in the out-of-domain setting, which is a 4.3 percentage point improvement over the official BERT baseline model.</abstract>
      <url hash="778010b7">D19-5824</url>
      <doi>10.18653/v1/D19-5824</doi>
      <bibkey>takahashi-etal-2019-cler</bibkey>
    </paper>
    <paper id="25">
      <title>Question Answering Using Hierarchical Attention on Top of BERT Features<fixed-case>BERT</fixed-case> Features</title>
      <author><first>Reham</first><last>Osama</last></author>
      <author><first>Nagwa</first><last>El-Makky</last></author>
      <author><first>Marwan</first><last>Torki</last></author>
      <pages>191–195</pages>
      <abstract>The <a href="https://en.wikipedia.org/wiki/Model_(person)">model</a> submitted works as follows. When supplied a question and a passage it makes use of the BERT embedding along with the hierarchical attention model which consists of 2 parts, the co-attention and the self-attention, to locate a continuous span of the passage that is the answer to the question.</abstract>
      <url hash="c6d5fd13">D19-5825</url>
      <doi>10.18653/v1/D19-5825</doi>
      <bibkey>osama-etal-2019-question</bibkey>
    </paper>
    <paper id="27">
      <title>Generalizing Question Answering System with Pre-trained Language Model Fine-tuning</title>
      <author><first>Dan</first><last>Su</last></author>
      <author><first>Yan</first><last>Xu</last></author>
      <author><first>Genta Indra</first><last>Winata</last></author>
      <author><first>Peng</first><last>Xu</last></author>
      <author><first>Hyeondey</first><last>Kim</last></author>
      <author><first>Zihan</first><last>Liu</last></author>
      <author><first>Pascale</first><last>Fung</last></author>
      <pages>203–211</pages>
      <abstract>With a large number of datasets being released and new techniques being proposed, Question answering (QA) systems have witnessed great breakthroughs in reading comprehension (RC)tasks. However, most existing methods focus on improving in-domain performance, leaving open the research question of how these mod-els and techniques can generalize to out-of-domain and unseen RC tasks. To enhance the generalization ability, we propose a multi-task learning framework that learns the shared representation across different tasks. Our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> is built on top of a large pre-trained language model, such as XLNet, and then fine-tuned on multiple RC datasets. Experimental results show the effectiveness of our <a href="https://en.wikipedia.org/wiki/Methodology">methods</a>, with an average Exact Match score of 56.59 and an <a href="https://en.wikipedia.org/wiki/F-number">average F1 score</a> of 68.98, which significantly improves the BERT-Large baseline by8.39 and 7.22, respectively</abstract>
      <url hash="718dd39f">D19-5827</url>
      <doi>10.18653/v1/D19-5827</doi>
      <bibkey>su-etal-2019-generalizing</bibkey>
    </paper>
    <paper id="28">
      <title>D-NET : A Pre-Training and Fine-Tuning Framework for Improving the Generalization of Machine Reading Comprehension<fixed-case>D</fixed-case>-<fixed-case>NET</fixed-case>: A Pre-Training and Fine-Tuning Framework for Improving the Generalization of Machine Reading Comprehension</title>
      <author><first>Hongyu</first><last>Li</last></author>
      <author><first>Xiyuan</first><last>Zhang</last></author>
      <author><first>Yibing</first><last>Liu</last></author>
      <author><first>Yiming</first><last>Zhang</last></author>
      <author><first>Quan</first><last>Wang</last></author>
      <author><first>Xiangyang</first><last>Zhou</last></author>
      <author><first>Jing</first><last>Liu</last></author>
      <author><first>Hua</first><last>Wu</last></author>
      <author><first>Haifeng</first><last>Wang</last></author>
      <pages>212–219</pages>
      <abstract>In this paper, we introduce a simple system Baidu submitted for MRQA (Machine Reading for Question Answering) 2019 Shared Task that focused on generalization of machine reading comprehension (MRC) models. Our <a href="https://en.wikipedia.org/wiki/System">system</a> is built on a framework of pretraining and fine-tuning, namely D-NET. The techniques of pre-trained language models and <a href="https://en.wikipedia.org/wiki/Multi-task_learning">multi-task learning</a> are explored to improve the generalization of MRC models and we conduct experiments to examine the effectiveness of these strategies. Our <a href="https://en.wikipedia.org/wiki/System">system</a> is ranked at top 1 of all the participants in terms of averaged F1 score. Our codes and models will be released at PaddleNLP.</abstract>
      <url hash="36aec995">D19-5828</url>
      <doi>10.18653/v1/D19-5828</doi>
      <bibkey>li-etal-2019-net</bibkey>
    </paper>
    <paper id="29">
      <title>An Exploration of Data Augmentation and Sampling Techniques for Domain-Agnostic Question Answering</title>
      <author><first>Shayne</first><last>Longpre</last></author>
      <author><first>Yi</first><last>Lu</last></author>
      <author><first>Zhucheng</first><last>Tu</last></author>
      <author><first>Chris</first><last>DuBois</last></author>
      <pages>220–227</pages>
      <abstract>To produce a domain-agnostic question answering model for the Machine Reading Question Answering (MRQA) 2019 Shared Task, we investigate the relative benefits of large pre-trained language models, various data sampling strategies, as well as query and context paraphrases generated by back-translation. We find a simple negative sampling technique to be particularly effective, even though it is typically used for <a href="https://en.wikipedia.org/wiki/Data_set">datasets</a> that include unanswerable questions, such as SQuAD 2.0. When applied in conjunction with per-domain sampling, our XLNet (Yang et al., 2019)-based submission achieved the second best Exact Match and F1 in the MRQA leaderboard competition.</abstract>
      <url hash="fbbd3f5d">D19-5829</url>
      <doi>10.18653/v1/D19-5829</doi>
      <bibkey>longpre-etal-2019-exploration</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/bioasq">BioASQ</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/drop">DROP</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/duorc">DuoRC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/hotpotqa">HotpotQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/natural-questions">Natural Questions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/newsqa">NewsQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/race">RACE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/searchqa">SearchQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/triviaqa">TriviaQA</pwcdataset>
    </paper>
  </volume>
  <volume id="59" ingest-date="2019-11-03">
    <meta>
      <booktitle>Proceedings of the First Workshop on Aggregating and Analysing Crowdsourced Annotations for NLP</booktitle>
      <url hash="9ec52430">D19-59</url>
      <editor><first>Silviu</first><last>Paun</last></editor>
      <editor><first>Dirk</first><last>Hovy</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Hong Kong</address>
      <month>November</month>
      <year>2019</year>
    </meta>
    <frontmatter>
      <url hash="ecb13bdd">D19-5900</url>
      <bibkey>emnlp-2019-aggregating</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Dependency Tree Annotation with Mechanical Turk<fixed-case>M</fixed-case>echanical <fixed-case>T</fixed-case>urk</title>
      <author><first>Stephen</first><last>Tratz</last></author>
      <pages>1–5</pages>
      <abstract>Crowdsourcing is frequently employed to quickly and inexpensively obtain valuable linguistic annotations but is rarely used for <a href="https://en.wikipedia.org/wiki/Parsing">parsing</a>, likely due to the perceived difficulty of the task and the limited training of the available workers. This paper presents what is, to the best of our knowledge, the first published use of Mechanical Turk (or similar platform) to crowdsource parse trees. We pay Turkers to construct unlabeled dependency trees for 500 English sentences using an interactive graphical dependency tree editor, collecting 10 annotations per sentence. Despite not requiring any training, several of the more prolific workers meet or exceed 90 % attachment agreement with the Penn Treebank (PTB) portion of our data, and, furthermore, for 72 % of these PTB sentences, at least one Turker produces a perfect parse. Thus, we find that, supported with a simple <a href="https://en.wikipedia.org/wiki/Graphical_user_interface">graphical interface</a>, people with presumably no prior experience can achieve surprisingly high degrees of <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> on this <a href="https://en.wikipedia.org/wiki/Task_(project_management)">task</a>. To facilitate research into aggregation techniques for complex crowdsourced annotations, we publicly release our annotated corpus.</abstract>
      <url hash="645ee11a">D19-5901</url>
      <doi>10.18653/v1/D19-5901</doi>
      <bibkey>tratz-2019-dependency</bibkey>
    </paper>
    <paper id="5">
      <title>Crowd-sourcing annotation of complex NLU tasks : A case study of argumentative content annotation<fixed-case>NLU</fixed-case> tasks: A case study of argumentative content annotation</title>
      <author><first>Tamar</first><last>Lavee</last></author>
      <author><first>Lili</first><last>Kotlerman</last></author>
      <author><first>Matan</first><last>Orbach</last></author>
      <author><first>Yonatan</first><last>Bilu</last></author>
      <author><first>Michal</first><last>Jacovi</last></author>
      <author><first>Ranit</first><last>Aharonov</last></author>
      <author><first>Noam</first><last>Slonim</last></author>
      <pages>29–38</pages>
      <abstract>Recent advancements in <a href="https://en.wikipedia.org/wiki/Machine_reading">machine reading</a> and listening comprehension involve the annotation of long texts. Such <a href="https://en.wikipedia.org/wiki/Task_(computing)">tasks</a> are typically time consuming, making crowd-annotations an attractive solution, yet their complexity often makes such a <a href="https://en.wikipedia.org/wiki/Solution">solution</a> unfeasible. In particular, a major concern is that crowd annotators may be tempted to skim through long texts, and answer questions without reading thoroughly. We present a case study of adapting this type of <a href="https://en.wikipedia.org/wiki/Task_(project_management)">task</a> to the crowd. The <a href="https://en.wikipedia.org/wiki/Task_(project_management)">task</a> is to identify claims in a several minute long debate speech. We show that sentence-by-sentence annotation does not scale and that labeling only a subset of sentences is insufficient. Instead, we propose a scheme for effectively performing the full, complex task with crowd annotators, allowing the collection of large scale annotated datasets. We believe that the encountered challenges and pitfalls, as well as lessons learned, are relevant in general when collecting data for large scale natural language understanding (NLU) tasks.</abstract>
      <url hash="119bcc23">D19-5905</url>
      <attachment hash="a2c431bd">D19-5905.Attachment.pdf</attachment>
      <doi>10.18653/v1/D19-5905</doi>
      <bibkey>lavee-etal-2019-crowd</bibkey>
    </paper>
    <paper id="6">
      <title>Computer Assisted Annotation of Tension Development in <a href="https://en.wikipedia.org/wiki/TED_(conference)">TED Talks</a> through <a href="https://en.wikipedia.org/wiki/Crowdsourcing">Crowdsourcing</a><fixed-case>TED</fixed-case> Talks through Crowdsourcing</title>
      <author><first>Seungwon</first><last>Yoon</last></author>
      <author><first>Wonsuk</first><last>Yang</last></author>
      <author><first>Jong</first><last>Park</last></author>
      <pages>39–47</pages>
      <abstract>We propose a method of machine-assisted annotation for the identification of tension development, annotating whether the <a href="https://en.wikipedia.org/wiki/Tension_(physics)">tension</a> is increasing, decreasing, or staying unchanged. We use a neural network based prediction model, whose predicted results are given to the annotators as initial values for the options that they are asked to choose. By presenting such initial values to the annotators, the annotation task becomes an evaluation task where the annotators inspect whether or not the predicted results are correct. To demonstrate the effectiveness of our <a href="https://en.wikipedia.org/wiki/Methodology">method</a>, we performed the <a href="https://en.wikipedia.org/wiki/Annotation">annotation task</a> in both in-house and crowdsourced environments. For the crowdsourced environment, we compared the <a href="https://en.wikipedia.org/wiki/Annotation">annotation</a> results with and without our method of machine-assisted annotation. We find that the results with our <a href="https://en.wikipedia.org/wiki/Methodology">method</a> showed a higher agreement to the gold standard than those without, though our <a href="https://en.wikipedia.org/wiki/Methodology">method</a> had little effect at reducing the time for annotation. Our codes for the experiment are made publicly available.</abstract>
      <url hash="2cbe5165">D19-5906</url>
      <doi>10.18653/v1/D19-5906</doi>
      <bibkey>yoon-etal-2019-computer</bibkey>
    </paper>
    <paper id="7">
      <title>CoSSAT : Code-Switched Speech Annotation Tool<fixed-case>C</fixed-case>o<fixed-case>SSAT</fixed-case>: Code-Switched Speech Annotation Tool</title>
      <author><first>Sanket</first><last>Shah</last></author>
      <author><first>Pratik</first><last>Joshi</last></author>
      <author><first>Sebastin</first><last>Santy</last></author>
      <author><first>Sunayana</first><last>Sitaram</last></author>
      <pages>48–52</pages>
      <abstract>Code-switching refers to the alternation of two or more languages in a conversation or utterance and is common in <a href="https://en.wikipedia.org/wiki/Multilingualism">multilingual communities</a> across the world. Building code-switched speech and natural language processing systems are challenging due to the lack of annotated speech and text data. We present a speech annotation interface CoSSAT, which helps annotators transcribe code-switched speech faster, more easily and more accurately than a traditional <a href="https://en.wikipedia.org/wiki/Interface_(computing)">interface</a>, by displaying candidate words from monolingual speech recognizers. We conduct a <a href="https://en.wikipedia.org/wiki/User_study">user study</a> on the transcription of Hindi-English code-switched speech with 10 annotators and describe quantitative and qualitative results.</abstract>
      <url hash="66bd9c9d">D19-5907</url>
      <attachment hash="d66f87f3">D19-5907.Attachment.pdf</attachment>
      <doi>10.18653/v1/D19-5907</doi>
      <bibkey>shah-etal-2019-cossat</bibkey>
    </paper>
  </volume>
  <volume id="60" ingest-date="2019-11-03">
    <meta>
      <booktitle>Proceedings of the First Workshop on Commonsense Inference in Natural Language Processing</booktitle>
      <url hash="84523773">D19-60</url>
      <editor><first>Simon</first><last>Ostermann</last></editor>
      <editor><first>Sheng</first><last>Zhang</last></editor>
      <editor><first>Michael</first><last>Roth</last></editor>
      <editor><first>Peter</first><last>Clark</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Hong Kong, China</address>
      <month>November</month>
      <year>2019</year>
    </meta>
    <frontmatter>
      <url hash="32aa3ecd">D19-6000</url>
      <bibkey>emnlp-2019-commonsense</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Cracking the Contextual Commonsense Code : Understanding Commonsense Reasoning Aptitude of Deep Contextual Representations</title>
      <author><first>Jeff</first><last>Da</last></author>
      <author><first>Jungo</first><last>Kasai</last></author>
      <pages>1–12</pages>
      <abstract>Pretrained deep contextual representations have advanced the state-of-the-art on various commonsense NLP tasks, but we lack a concrete understanding of the capability of these models. Thus, we investigate and challenge several aspects of BERT’s commonsense representation abilities. First, we probe BERT’s ability to classify various object attributes, demonstrating that BERT shows a strong ability in encoding various commonsense features in its embedding space, but is still deficient in many areas. Next, we show that, by augmenting BERT’s pretraining data with additional data related to the deficient attributes, we are able to improve performance on a downstream commonsense reasoning task while using a minimal amount of data. Finally, we develop a method of fine-tuning knowledge graphs embeddings alongside BERT and show the continued importance of explicit knowledge graphs.</abstract>
      <url hash="fd7e9696">D19-6001</url>
      <doi>10.18653/v1/D19-6001</doi>
      <bibkey>da-kasai-2019-cracking</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/atomic">ATOMIC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mcscript">MCScript</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/race">RACE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/webchild">WebChild</pwcdataset>
    </paper>
    <paper id="3">
      <title>Towards Generalizable Neuro-Symbolic Systems for Commonsense Question Answering</title>
      <author><first>Kaixin</first><last>Ma</last></author>
      <author><first>Jonathan</first><last>Francis</last></author>
      <author><first>Quanyang</first><last>Lu</last></author>
      <author><first>Eric</first><last>Nyberg</last></author>
      <author><first>Alessandro</first><last>Oltramari</last></author>
      <pages>22–32</pages>
      <abstract>Non-extractive commonsense QA remains a challenging AI task, as it requires systems to reason about, synthesize, and gather disparate pieces of information, in order to generate responses to queries. Recent approaches on such tasks show increased performance, only when <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> are either pre-trained with additional information or when domain-specific heuristics are used, without any special consideration regarding the knowledge resource type. In this paper, we perform a survey of recent commonsense QA methods and we provide a systematic analysis of popular knowledge resources and knowledge-integration methods, across benchmarks from multiple commonsense datasets. Our results and analysis show that attention-based injection seems to be a preferable choice for <a href="https://en.wikipedia.org/wiki/Knowledge_integration">knowledge integration</a> and that the degree of domain overlap, between <a href="https://en.wikipedia.org/wiki/Knowledge_base">knowledge bases</a> and <a href="https://en.wikipedia.org/wiki/Data_set">datasets</a>, plays a crucial role in determining model success.</abstract>
      <url hash="aba9c433">D19-6003</url>
      <doi>10.18653/v1/D19-6003</doi>
      <bibkey>ma-etal-2019-towards</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/commonsenseqa">CommonsenseQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/conceptnet">ConceptNet</pwcdataset>
    </paper>
    <paper id="5">
      <title>Commonsense about Human Senses : Labeled Data Collection Processes</title>
      <author><first>Ndapa</first><last>Nakashole</last></author>
      <pages>43–52</pages>
      <abstract>We consider the problem of extracting from text commonsense knowledge pertaining to <a href="https://en.wikipedia.org/wiki/Sense">human senses</a> such as sound and smell. First, we consider the problem of recognizing mentions of human senses in text. Our contribution is a <a href="https://en.wikipedia.org/wiki/Methodology">method</a> for acquiring <a href="https://en.wikipedia.org/wiki/Labeled_data">labeled data</a>. Experiments show the effectiveness of our proposed data labeling approach when used with standard <a href="https://en.wikipedia.org/wiki/Machine_learning">machine learning models</a> on the task of sense recognition in text. Second, we propose to extract novel, common sense relationships pertaining to sense perception concepts. Our contribution is a process for generating <a href="https://en.wikipedia.org/wiki/Labeled_data">labeled data</a> by leveraging large corpora and <a href="https://en.wikipedia.org/wiki/Crowdsourcing">crowdsourcing questionnaires</a>.</abstract>
      <url hash="aa36c389">D19-6005</url>
      <doi>10.18653/v1/D19-6005</doi>
      <bibkey>nakashole-2019-commonsense</bibkey>
    </paper>
    <paper id="9">
      <title>IIT-KGP at COIN 2019 : Using pre-trained Language Models for modeling Machine Comprehension<fixed-case>IIT</fixed-case>-<fixed-case>KGP</fixed-case> at <fixed-case>COIN</fixed-case> 2019: Using pre-trained Language Models for modeling Machine Comprehension</title>
      <author><first>Prakhar</first><last>Sharma</last></author>
      <author><first>Sumegh</first><last>Roychowdhury</last></author>
      <pages>80–84</pages>
      <abstract>In this paper, we describe our <a href="https://en.wikipedia.org/wiki/System">system</a> for COIN 2019 Shared Task 1 : Commonsense Inference in Everyday Narrations. We show the power of leveraging state-of-the-art pre-trained language models such as BERT(Bidirectional Encoder Representations from Transformers) and XLNet over other Commonsense Knowledge Base Resources such as ConceptNet and NELL for modeling machine comprehension. We used an ensemble of BERT-Large and XLNet-Large. Experimental results show that our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> give substantial improvements over the <a href="https://en.wikipedia.org/wiki/Baseline_(configuration_management)">baseline</a> and other <a href="https://en.wikipedia.org/wiki/System">systems</a> incorporating <a href="https://en.wikipedia.org/wiki/Knowledge_base">knowledge bases</a>. We bagged 2nd position on the final test set leaderboard with an accuracy of 90.5 %</abstract>
      <url hash="5219738d">D19-6009</url>
      <doi>10.18653/v1/D19-6009</doi>
      <bibkey>sharma-roychowdhury-2019-iit</bibkey>
    </paper>
    <paper id="11">
      <title>Pingan Smart Health and SJTU at COIN-Shared Task : utilizing Pre-trained Language Models and Common-sense Knowledge in Machine Reading Tasks<fixed-case>SJTU</fixed-case> at <fixed-case>COIN</fixed-case> - Shared Task: utilizing Pre-trained Language Models and Common-sense Knowledge in Machine Reading Tasks</title>
      <author><first>Xiepeng</first><last>Li</last></author>
      <author><first>Zhexi</first><last>Zhang</last></author>
      <author><first>Wei</first><last>Zhu</last></author>
      <author><first>Zheng</first><last>Li</last></author>
      <author><first>Yuan</first><last>Ni</last></author>
      <author><first>Peng</first><last>Gao</last></author>
      <author><first>Junchi</first><last>Yan</last></author>
      <author><first>Guotong</first><last>Xie</last></author>
      <pages>93–98</pages>
      <abstract>To solve the shared tasks of COIN : COmmonsense INference in Natural Language Processing) Workshop in, we need explore the impact of <a href="https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning">knowledge representation</a> in modeling commonsense knowledge to boost performance of machine reading comprehension beyond simple text matching. There are two approaches to represent knowledge in the low-dimensional space. The <a href="https://en.wikipedia.org/wiki/First_law_of_thermodynamics">first</a> is to leverage large-scale unsupervised text corpus to train fixed or contextual language representations. The second approach is to explicitly express knowledge into a knowledge graph (KG), and then fit a <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> to represent the facts in the KG. We have experimented both (a) improving the fine-tuning of pre-trained language models on a task with a small dataset size, by leveraging datasets of similar tasks ; and (b) incorporating the distributional representations of a KG onto the representations of pre-trained language models, via simply <a href="https://en.wikipedia.org/wiki/Concatenation">concatenation</a> or multi-head attention. We find out that : (a) for task 1, first fine-tuning on larger datasets like RACE (Lai et al., 2017) and SWAG (Zellersetal.,2018), and then fine-tuning on the target task improve the performance significantly ; (b) for task 2, we find out the incorporating a KG of commonsense knowledge, WordNet (Miller, 1995) into the Bert model (Devlin et al., 2018) is helpful, however, it will hurts the performace of XLNET (Yangetal.,2019), a more powerful pre-trained model.</abstract>
      <url hash="712c4967">D19-6011</url>
      <doi>10.18653/v1/D19-6011</doi>
      <bibkey>li-etal-2019-pingan</bibkey>
    </paper>
    <paper id="12">
      <title>BLCU-NLP at COIN-Shared Task1 : Stagewise Fine-tuning BERT for Commonsense Inference in Everyday Narrations<fixed-case>BLCU</fixed-case>-<fixed-case>NLP</fixed-case> at <fixed-case>COIN</fixed-case>-Shared Task1: Stagewise Fine-tuning <fixed-case>BERT</fixed-case> for Commonsense Inference in Everyday Narrations</title>
      <author><first>Chunhua</first><last>Liu</last></author>
      <author><first>Dong</first><last>Yu</last></author>
      <pages>99–103</pages>
      <abstract>This paper describes our <a href="https://en.wikipedia.org/wiki/System">system</a> for COIN Shared Task 1 : Commonsense Inference in Everyday Narrations. To inject more external knowledge to better reason over the narrative passage, question and answer, the <a href="https://en.wikipedia.org/wiki/System">system</a> adopts a stagewise fine-tuning method based on pre-trained BERT model. More specifically, the first stage is to fine-tune on addi- tional machine reading comprehension dataset to learn more <a href="https://en.wikipedia.org/wiki/Commonsense_knowledge">commonsense knowledge</a>. The second stage is to fine-tune on target-task (MCScript2.0) with MCScript (2018) dataset assisted. Experimental results show that our <a href="https://en.wikipedia.org/wiki/System">system</a> achieves significant improvements over the <a href="https://en.wikipedia.org/wiki/Baseline_(configuration_management)">baseline systems</a> with 84.2 % <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> on the official test dataset.</abstract>
      <url hash="118d0307">D19-6012</url>
      <doi>10.18653/v1/D19-6012</doi>
      <bibkey>liu-yu-2019-blcu</bibkey>
    </paper>
    <paper id="14">
      <title>Diversity-aware Event Prediction based on a Conditional Variational Autoencoder with Reconstruction</title>
      <author><first>Hirokazu</first><last>Kiyomaru</last></author>
      <author><first>Kazumasa</first><last>Omura</last></author>
      <author><first>Yugo</first><last>Murawaki</last></author>
      <author><first>Daisuke</first><last>Kawahara</last></author>
      <author><first>Sadao</first><last>Kurohashi</last></author>
      <pages>113–122</pages>
      <abstract>Typical event sequences are an important class of <a href="https://en.wikipedia.org/wiki/Commonsense_knowledge">commonsense knowledge</a>. Formalizing the task as the generation of a next event conditioned on a current event, previous work in event prediction employs sequence-to-sequence (seq2seq) models. However, what can happen after a given event is usually diverse, a fact that can hardly be captured by deterministic models. In this paper, we propose to incorporate a conditional variational autoencoder (CVAE) into seq2seq for its ability to represent diverse next events as a probabilistic distribution. We further extend the CVAE-based seq2seq with a reconstruction mechanism to prevent the <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> from concentrating on highly typical events. To facilitate fair and systematic evaluation of the diversity-aware models, we also extend existing evaluation datasets by tying each current event to multiple next events. Experiments show that the CVAE-based models drastically outperform deterministic models in terms of <a href="https://en.wikipedia.org/wiki/Precision_(statistics)">precision</a> and that the reconstruction mechanism improves the <a href="https://en.wikipedia.org/wiki/Precision_(statistics)">recall</a> of CVAE-based models without sacrificing <a href="https://en.wikipedia.org/wiki/Precision_(statistics)">precision</a>.</abstract>
      <url hash="17905c04">D19-6014</url>
      <doi>10.18653/v1/D19-6014</doi>
      <bibkey>kiyomaru-etal-2019-diversity</bibkey>
    </paper>
    <paper id="15">
      <title>Can a Gorilla Ride a Camel? Learning Semantic Plausibility from Text</title>
      <author><first>Ian</first><last>Porada</last></author>
      <author><first>Kaheer</first><last>Suleman</last></author>
      <author><first>Jackie Chi Kit</first><last>Cheung</last></author>
      <pages>123–129</pages>
      <abstract>Modeling semantic plausibility requires <a href="https://en.wikipedia.org/wiki/Commonsense_knowledge">commonsense knowledge</a> about the world and has been used as a testbed for exploring various <a href="https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning">knowledge representations</a>. Previous work has focused specifically on modeling physical plausibility and shown that <a href="https://en.wikipedia.org/wiki/Distribution_(mathematics)">distributional methods</a> fail when tested in a <a href="https://en.wikipedia.org/wiki/Supervised_learning">supervised setting</a>. At the same time, distributional models, namely large pretrained language models, have led to improved results for many natural language understanding tasks. In this work, we show that these pretrained language models are in fact effective at modeling physical plausibility in the <a href="https://en.wikipedia.org/wiki/Supervised_learning">supervised setting</a>. We therefore present the more difficult problem of learning to model physical plausibility directly from text. We create a training set by extracting <a href="https://en.wikipedia.org/wiki/Event_(probability_theory)">attested events</a> from a large corpus, and we provide a baseline for training on these <a href="https://en.wikipedia.org/wiki/Event_(probability_theory)">attested events</a> in a self-supervised manner and testing on a physical plausibility task. We believe results could be further improved by injecting explicit <a href="https://en.wikipedia.org/wiki/Commonsense_knowledge">commonsense knowledge</a> into a <a href="https://en.wikipedia.org/wiki/Distribution_(mathematics)">distributional model</a>.</abstract>
      <url hash="1adba42c">D19-6015</url>
      <doi>10.18653/v1/D19-6015</doi>
      <bibkey>porada-etal-2019-gorilla</bibkey>
    </paper>
    </volume>
  <volume id="61" ingest-date="2019-11-03">
    <meta>
      <booktitle>Proceedings of the 2nd Workshop on Deep Learning Approaches for Low-Resource NLP (DeepLo 2019)</booktitle>
      <url hash="4a7961c5">D19-61</url>
      <editor><first>Colin</first><last>Cherry</last></editor>
      <editor><first>Greg</first><last>Durrett</last></editor>
      <editor><first>George</first><last>Foster</last></editor>
      <editor><first>Reza</first><last>Haffari</last></editor>
      <editor><first>Shahram</first><last>Khadivi</last></editor>
      <editor><first>Nanyun</first><last>Peng</last></editor>
      <editor><first>Xiang</first><last>Ren</last></editor>
      <editor><first>Swabha</first><last>Swayamdipta</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Hong Kong, China</address>
      <month>November</month>
      <year>2019</year>
    </meta>
    <frontmatter>
      <url hash="7a5d13e0">D19-6100</url>
      <bibkey>emnlp-2019-deep</bibkey>
    </frontmatter>
    <paper id="2">
      <title>A Comparative Analysis of Unsupervised Language Adaptation Methods</title>
      <author><first>Gil</first><last>Rocha</last></author>
      <author><first>Henrique</first><last>Lopes Cardoso</last></author>
      <pages>11–21</pages>
      <abstract>To overcome the lack of annotated resources in less-resourced languages, recent approaches have been proposed to perform unsupervised language adaptation. In this paper, we explore three recent proposals : Adversarial Training, Sentence Encoder Alignment and Shared-Private Architecture. We highlight the differences of these approaches in terms of unlabeled data requirements and capability to overcome additional domain shift in the data. A comparative analysis in two different <a href="https://en.wikipedia.org/wiki/Task_(project_management)">tasks</a> is conducted, namely on Sentiment Classification and <a href="https://en.wikipedia.org/wiki/Natural-language_understanding">Natural Language Inference</a>. We show that adversarial training methods are more suitable when the source and target language datasets contain other variations in content besides the <a href="https://en.wikipedia.org/wiki/Language_shift">language shift</a>. Otherwise, sentence encoder alignment methods are very effective and can yield scores on the target language that are close to the source language scores.</abstract>
      <url hash="90565606">D19-6102</url>
      <doi>10.18653/v1/D19-6102</doi>
      <bibkey>rocha-lopes-cardoso-2019-comparative</bibkey>
    </paper>
    <paper id="3">
      <title>A logical-based corpus for cross-lingual evaluation</title>
      <author><first>Felipe</first><last>Salvatore</last></author>
      <author><first>Marcelo</first><last>Finger</last></author>
      <author><first>Roberto</first><last>Hirata Jr</last></author>
      <pages>22–30</pages>
      <abstract>At present, different deep learning models are presenting high accuracy on popular inference datasets such as SNLI, MNLI, and SciTail. However, there are different indicators that those <a href="https://en.wikipedia.org/wiki/Data_set">datasets</a> can be exploited by using some simple linguistic patterns. This fact poses difficulties to our understanding of the actual capacity of <a href="https://en.wikipedia.org/wiki/Machine_learning">machine learning models</a> to solve the complex task of textual inference. We propose a new set of syntactic tasks focused on contradiction detection that require specific capacities over linguistic logical forms such as : Boolean coordination, <a href="https://en.wikipedia.org/wiki/Quantifier_(logic)">quantifiers</a>, <a href="https://en.wikipedia.org/wiki/Definite_description">definite description</a>, and counting operators. We evaluate two kinds of <a href="https://en.wikipedia.org/wiki/Deep_learning">deep learning models</a> that implicitly exploit <a href="https://en.wikipedia.org/wiki/Language_structure">language structure</a> : <a href="https://en.wikipedia.org/wiki/Recurrent_neural_network">recurrent models</a> and the Transformer network BERT. We show that although BERT is clearly more efficient to generalize over most logical forms, there is space for improvement when dealing with counting operators. Since the syntactic tasks can be implemented in different languages, we show a successful case of cross-lingual transfer learning between <a href="https://en.wikipedia.org/wiki/English_language">English</a> and <a href="https://en.wikipedia.org/wiki/Portuguese_language">Portuguese</a>.</abstract>
      <url hash="2fcc2f42">D19-6103</url>
      <doi>10.18653/v1/D19-6103</doi>
      <bibkey>salvatore-etal-2019-logical</bibkey>
      <pwccode url="https://github.com/felipessalvatore/CLCD" additional="false">felipessalvatore/CLCD</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
    </paper>
    <paper id="5">
      <title>Bag-of-Words Transfer : Non-Contextual Techniques for Multi-Task Learning</title>
      <author><first>Seth</first><last>Ebner</last></author>
      <author><first>Felicity</first><last>Wang</last></author>
      <author><first>Benjamin</first><last>Van Durme</last></author>
      <pages>40–46</pages>
      <abstract>Many architectures for multi-task learning (MTL) have been proposed to take advantage of transfer among tasks, often involving complex models and training procedures. In this paper, we ask if the sentence-level representations learned in previous approaches provide significant benefit beyond that provided by simply improving word-based representations. To investigate this question, we consider three techniques that ignore sequence information : a syntactically-oblivious pooling encoder, pre-trained non-contextual word embeddings, and unigram generative regularization. Compared to a state-of-the-art MTL approach to textual inference, the simple techniques we use yield similar performance on a universe of task combinations while reducing training time and model size.</abstract>
      <url hash="24051dad">D19-6105</url>
      <doi>10.18653/v1/D19-6105</doi>
      <bibkey>ebner-etal-2019-bag</bibkey>
    </paper>
    <paper id="8">
      <title>Deep Bidirectional Transformers for Relation Extraction without Supervision</title>
      <author><first>Yannis</first><last>Papanikolaou</last></author>
      <author><first>Ian</first><last>Roberts</last></author>
      <author><first>Andrea</first><last>Pierleoni</last></author>
      <pages>67–75</pages>
      <abstract>We present a novel framework to deal with relation extraction tasks in cases where there is complete lack of supervision, either in the form of gold annotations, or relations from a <a href="https://en.wikipedia.org/wiki/Knowledge_base">knowledge base</a>. Our approach leverages syntactic parsing and pre-trained word embeddings to extract few but precise relations, which are then used to annotate a larger corpus, in a manner identical to distant supervision. The resulting <a href="https://en.wikipedia.org/wiki/Data_set">data set</a> is employed to fine tune a pre-trained BERT model in order to perform <a href="https://en.wikipedia.org/wiki/Relation_extraction">relation extraction</a>. Empirical evaluation on four <a href="https://en.wikipedia.org/wiki/Data_set">data sets</a> from the biomedical domain shows that our method significantly outperforms two simple baselines for unsupervised relation extraction and, even if not using any supervision at all, achieves slightly worse results than the state-of-the-art in three out of four data sets. Importantly, we show that it is possible to successfully fine tune a large pretrained language model with noisy data, as opposed to previous works that rely on gold data for fine tuning.</abstract>
      <url hash="9c323bf5">D19-6108</url>
      <doi>10.18653/v1/D19-6108</doi>
      <bibkey>papanikolaou-etal-2019-deep</bibkey>
    </paper>
    <paper id="11">
      <title>Fast Domain Adaptation of Semantic Parsers via Paraphrase Attention</title>
      <author><first>Avik</first><last>Ray</last></author>
      <author><first>Yilin</first><last>Shen</last></author>
      <author><first>Hongxia</first><last>Jin</last></author>
      <pages>94–103</pages>
      <abstract>Semantic parsers are used to convert user’s natural language commands to executable logical form in intelligent personal agents. Labeled datasets required to train such <a href="https://en.wikipedia.org/wiki/Parsing">parsers</a> are expensive to collect, and are never comprehensive. As a result, for effective post-deployment domain adaptation and personalization, semantic parsers are continuously retrained to learn new user vocabulary and paraphrase variety. However, state-of-the art attention based neural parsers are slow to retrain which inhibits real time domain adaptation. Secondly, these <a href="https://en.wikipedia.org/wiki/Parsing">parsers</a> do not leverage numerous <a href="https://en.wikipedia.org/wiki/Paraphrase">paraphrases</a> already present in the training dataset. Designing <a href="https://en.wikipedia.org/wiki/Parsing">parsers</a> which can simultaneously maintain high <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> and fast retraining time is challenging. In this paper, we present novel paraphrase attention based sequence-to-sequence / tree parsers which support fast near real time retraining. In addition, our <a href="https://en.wikipedia.org/wiki/Parsing">parsers</a> often boost <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> by jointly modeling the semantic dependencies of paraphrases. We evaluate our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> on benchmark datasets to demonstrate upto 9X speedup in retraining time compared to existing <a href="https://en.wikipedia.org/wiki/Parsing">parsers</a>, as well as achieving state-of-the-art <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a>.</abstract>
      <url hash="40688e21">D19-6111</url>
      <doi>10.18653/v1/D19-6111</doi>
      <bibkey>ray-etal-2019-fast</bibkey>
    </paper>
    <paper id="12">
      <title>Few-Shot and Zero-Shot Learning for Historical Text Normalization</title>
      <author><first>Marcel</first><last>Bollmann</last></author>
      <author><first>Natalia</first><last>Korchagina</last></author>
      <author><first>Anders</first><last>Søgaard</last></author>
      <pages>104–114</pages>
      <abstract>Historical text normalization often relies on <a href="https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets">small training datasets</a>. Recent work has shown that <a href="https://en.wikipedia.org/wiki/Multi-task_learning">multi-task learning</a> can lead to significant improvements by exploiting synergies with related datasets, but there has been no systematic study of different <a href="https://en.wikipedia.org/wiki/Multi-task_learning">multi-task learning architectures</a>. This paper evaluates 63 multi-task learning configurations for sequence-to-sequence-based historical text normalization across ten datasets from eight languages, using <a href="https://en.wikipedia.org/wiki/Autoencoding">autoencoding</a>, grapheme-to-phoneme mapping, and <a href="https://en.wikipedia.org/wiki/Lemmatization">lemmatization</a> as auxiliary tasks. We observe consistent, significant improvements across languages when training data for the target task is limited, but minimal or no improvements when training data is abundant. We also show that zero-shot learning outperforms the simple, but relatively strong, identity baseline.</abstract>
      <url hash="4845f825">D19-6112</url>
      <doi>10.18653/v1/D19-6112</doi>
      <bibkey>bollmann-etal-2019-shot</bibkey>
    </paper>
    <paper id="15">
      <title>Unlearn Dataset Bias in Natural Language Inference by Fitting the Residual</title>
      <author><first>He</first><last>He</last></author>
      <author><first>Sheng</first><last>Zha</last></author>
      <author><first>Haohan</first><last>Wang</last></author>
      <pages>132–142</pages>
      <abstract>Statistical natural language inference (NLI) models are susceptible to learning dataset bias : superficial cues that happen to associate with the label on a particular dataset, but are not useful in general, e.g., <a href="https://en.wikipedia.org/wiki/Affirmation_and_negation">negation words</a> indicate contradiction. As exposed by several recent challenge datasets, these <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> perform poorly when such association is absent, e.g., predicting that I love dogs. contradicts I do n’t love cats.. Our goal is to design <a href="https://en.wikipedia.org/wiki/Machine_learning">learning algorithms</a> that guard against known dataset bias. We formalize the concept of dataset bias under the framework of distribution shift and present a simple debiasing algorithm based on residual fitting, which we call DRiFt. We first learn a biased model that only uses <a href="https://en.wikipedia.org/wiki/Feature_(machine_learning)">features</a> that are known to relate to dataset bias. Then, we train a debiased model that fits to the <a href="https://en.wikipedia.org/wiki/Errors_and_residuals">residual</a> of the <a href="https://en.wikipedia.org/wiki/Bias_of_an_estimator">biased model</a>, focusing on examples that can not be predicted well by biased features only. We use DRiFt to train three high-performing NLI models on two benchmark datasets, SNLI and MNLI. Our debiased models achieve significant gains over baseline models on two challenge test sets, while maintaining reasonable performance on the original test sets.</abstract>
      <url hash="48f42dea">D19-6115</url>
      <attachment hash="d7fab5ad">D19-6115.Attachment.pdf</attachment>
      <doi>10.18653/v1/D19-6115</doi>
      <bibkey>he-etal-2019-unlearn</bibkey>
      <pwccode url="https://github.com/hhexiy/debiased" additional="false">hhexiy/debiased</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
    </paper>
    <paper id="16">
      <title>Metric Learning for Dynamic Text Classification</title>
      <author><first>Jeremy</first><last>Wohlwend</last></author>
      <author><first>Ethan R.</first><last>Elenberg</last></author>
      <author><first>Sam</first><last>Altschul</last></author>
      <author><first>Shawn</first><last>Henry</last></author>
      <author><first>Tao</first><last>Lei</last></author>
      <pages>143–152</pages>
      <abstract>Traditional text classifiers are limited to predicting over a fixed set of labels. However, in many real-world applications the label set is frequently changing. For example, in intent classification, new intents may be added over time while others are removed. We propose to address the problem of dynamic text classification by replacing the traditional, fixed-size output layer with a learned, semantically meaningful <a href="https://en.wikipedia.org/wiki/Metric_space">metric space</a>. Here the distances between textual inputs are optimized to perform nearest-neighbor classification across overlapping label sets. Changing the label set does not involve removing parameters, but rather simply adding or removing support points in the <a href="https://en.wikipedia.org/wiki/Metric_space">metric space</a>. Then the learned <a href="https://en.wikipedia.org/wiki/Metric_(mathematics)">metric</a> can be fine-tuned with only a few additional training examples. We demonstrate that this simple <a href="https://en.wikipedia.org/wiki/Strategy">strategy</a> is robust to changes in the label space. Furthermore, our results show that learning a non-Euclidean metric can improve performance in the low data regime, suggesting that further work on <a href="https://en.wikipedia.org/wiki/Metric_space">metric spaces</a> may benefit low-resource research.</abstract>
      <url hash="6fa58ca6">D19-6116</url>
      <doi>10.18653/v1/D19-6116</doi>
      <bibkey>wohlwend-etal-2019-metric</bibkey>
      <pwccode url="https://github.com/asappresearch/dynamic-classification" additional="false">asappresearch/dynamic-classification</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/web-of-science-dataset">WOS</pwcdataset>
    </paper>
    <paper id="18">
      <title>Cross-lingual Parsing with Polyglot Training and Multi-treebank Learning : A Faroese Case Study<fixed-case>F</fixed-case>aroese Case Study</title>
      <author><first>James</first><last>Barry</last></author>
      <author><first>Joachim</first><last>Wagner</last></author>
      <author><first>Jennifer</first><last>Foster</last></author>
      <pages>163–174</pages>
      <abstract>Cross-lingual dependency parsing involves transferring <a href="https://en.wikipedia.org/wiki/Syntax_(programming_languages)">syntactic knowledge</a> from one language to another. It is a crucial component for inducing dependency parsers in low-resource scenarios where no training data for a language exists. Using <a href="https://en.wikipedia.org/wiki/Faroese_language">Faroese</a> as the target language, we compare two approaches using annotation projection : first, projecting from multiple monolingual source models ; second, projecting from a single polyglot model which is trained on the combination of all source languages. Furthermore, we reproduce multi-source projection (Tyers et al., 2018), in which dependency trees of multiple sources are combined. Finally, we apply multi-treebank modelling to the projected treebanks, in addition to or alternatively to polyglot modelling on the source side. We find that polyglot training on the source languages produces an overall trend of better results on the target language but the single best result for the target language is obtained by projecting from monolingual source parsing models and then training multi-treebank POS tagging and parsing models on the target side.</abstract>
      <url hash="d85cdb75">D19-6118</url>
      <doi>10.18653/v1/D19-6118</doi>
      <bibkey>barry-etal-2019-cross</bibkey>
      <pwccode url="https://github.com/Jbar-ry/multilingual-parsing" additional="false">Jbar-ry/multilingual-parsing</pwccode>
    </paper>
    <paper id="19">
      <title>Inject Rubrics into Short Answer Grading System</title>
      <author><first>Tianqi</first><last>Wang</last></author>
      <author><first>Naoya</first><last>Inoue</last></author>
      <author><first>Hiroki</first><last>Ouchi</last></author>
      <author><first>Tomoya</first><last>Mizumoto</last></author>
      <author><first>Kentaro</first><last>Inui</last></author>
      <pages>175–182</pages>
      <abstract>Short Answer Grading (SAG) is a task of scoring students’ answers in examinations. Most existing SAG systems predict scores based only on the answers, including the model used as base line in this paper, which gives the-state-of-the-art performance. But they ignore important evaluation criteria such as <a href="https://en.wikipedia.org/wiki/Rubric_(academic)">rubrics</a>, which play a crucial role for evaluating answers in real-world situations. In this paper, we present a method to inject information from rubrics into SAG systems. We implement our approach on top of word-level attention mechanism to introduce the rubric information, in order to locate information in each answer that are highly related to the score. Our experimental results demonstrate that injecting rubric information effectively contributes to the performance improvement and that our proposed <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> outperforms the state-of-the-art SAG model on the widely used ASAP-SAS dataset under low-resource settings.</abstract>
      <url hash="9bf1aa97">D19-6119</url>
      <doi>10.18653/v1/D19-6119</doi>
      <bibkey>wang-etal-2019-inject</bibkey>
    </paper>
    <paper id="24">
      <title>Reevaluating Argument Component Extraction in Low Resource Settings</title>
      <author><first>Anirudh</first><last>Joshi</last></author>
      <author><first>Timothy</first><last>Baldwin</last></author>
      <author><first>Richard</first><last>Sinnott</last></author>
      <author><first>Cecile</first><last>Paris</last></author>
      <pages>219–224</pages>
      <abstract>Argument component extraction is a challenging and complex high-level semantic extraction task. As such, it is both expensive to annotate (meaning training data is limited and low-resource by nature), and hard for current-generation deep learning methods to model. In this paper, we reevaluate the performance of state-of-the-art approaches in both single- and multi-task learning settings using combinations of character-level, GloVe, ELMo, and BERT encodings using standard BiLSTM-CRF encoders. We use evaluation metrics that are more consistent with evaluation practice in <a href="https://en.wikipedia.org/wiki/Named-entity_recognition">named entity recognition</a> to understand how well current baselines address this challenge and compare their performance to lower-level semantic tasks such as CoNLL named entity recognition. We find that performance utilizing various pre-trained representations and training methodologies often leaves a lot to be desired as it currently stands, and suggest future pathways for improvement.</abstract>
      <url hash="d2824ce9">D19-6124</url>
      <doi>10.18653/v1/D19-6124</doi>
      <bibkey>joshi-etal-2019-reevaluating</bibkey>
    </paper>
    <paper id="28">
      <title>Transductive Auxiliary Task Self-Training for Neural Multi-Task Models</title>
      <author><first>Johannes</first><last>Bjerva</last></author>
      <author><first>Katharina</first><last>Kann</last></author>
      <author><first>Isabelle</first><last>Augenstein</last></author>
      <pages>253–258</pages>
      <abstract>Multi-task learning and self-training are two common ways to improve a <a href="https://en.wikipedia.org/wiki/Machine_learning">machine learning model</a>’s performance in settings with limited training data. Drawing heavily on ideas from those two approaches, we suggest transductive auxiliary task self-training : training a multi-task model on (i) a combination of main and auxiliary task training data, and (ii) test instances with auxiliary task labels which a single-task version of the <a href="https://en.wikipedia.org/wiki/Statistical_model">model</a> has previously generated. We perform extensive experiments on 86 combinations of languages and tasks. Our results are that, on average, transductive auxiliary task self-training improves absolute accuracy by up to 9.56 % over the pure multi-task model for <a href="https://en.wikipedia.org/wiki/Tag_(metadata)">dependency relation tagging</a> and by up to 13.03 % for <a href="https://en.wikipedia.org/wiki/Tag_(metadata)">semantic tagging</a>.</abstract>
      <url hash="6b6e9be0">D19-6128</url>
      <doi>10.18653/v1/D19-6128</doi>
      <bibkey>bjerva-etal-2019-transductive</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/universal-dependencies">Universal Dependencies</pwcdataset>
    </paper>
    <paper id="29">
      <title>Weakly Supervised Attentional Model for Low Resource Ad-hoc Cross-lingual Information Retrieval</title>
      <author><first>Lingjun</first><last>Zhao</last></author>
      <author><first>Rabih</first><last>Zbib</last></author>
      <author><first>Zhuolin</first><last>Jiang</last></author>
      <author><first>Damianos</first><last>Karakos</last></author>
      <author><first>Zhongqiang</first><last>Huang</last></author>
      <pages>259–264</pages>
      <abstract>We propose a weakly supervised neural model for Ad-hoc Cross-lingual Information Retrieval (CLIR) from low-resource languages. Low resource languages often lack relevance annotations for CLIR, and when available the training data usually has limited coverage for possible queries. In this paper, we design a <a href="https://en.wikipedia.org/wiki/Statistical_model">model</a> which does not require relevance annotations, instead it is trained on samples extracted from translation corpora as weak supervision. This model relies on an <a href="https://en.wikipedia.org/wiki/Attentional_control">attention mechanism</a> to learn spans in the foreign sentence that are relevant to the query. We report experiments on two low resource languages : <a href="https://en.wikipedia.org/wiki/Swahili_language">Swahili</a> and <a href="https://en.wikipedia.org/wiki/Tagalog_language">Tagalog</a>, trained on less that 100k parallel sentences each. The proposed model achieves 19 MAP points improvement compared to using CNNs for <a href="https://en.wikipedia.org/wiki/Feature_extraction">feature extraction</a>, 12 points improvement from machine translation-based CLIR, and up to 6 points improvement compared to probabilistic CLIR models.</abstract>
      <url hash="eb177a82">D19-6129</url>
      <doi>10.18653/v1/D19-6129</doi>
      <bibkey>zhao-etal-2019-weakly</bibkey>
    </paper>
    <paper id="30">
      <title>X-WikiRE : A Large, Multilingual Resource for Relation Extraction as Machine Comprehension<fixed-case>X</fixed-case>-<fixed-case>W</fixed-case>iki<fixed-case>RE</fixed-case>: A Large, Multilingual Resource for Relation Extraction as Machine Comprehension</title>
      <author><first>Mostafa</first><last>Abdou</last></author>
      <author><first>Cezar</first><last>Sas</last></author>
      <author><first>Rahul</first><last>Aralikatte</last></author>
      <author><first>Isabelle</first><last>Augenstein</last></author>
      <author><first>Anders</first><last>Søgaard</last></author>
      <pages>265–274</pages>
      <abstract>Although the vast majority of knowledge bases (KBs) are heavily biased towards <a href="https://en.wikipedia.org/wiki/English_language">English</a>, Wikipedias do cover very different topics in different languages. Exploiting this, we introduce a new multilingual dataset (X-WikiRE), framing relation extraction as a multilingual machine reading problem. We show that by leveraging this resource it is possible to robustly transfer models cross-lingually and that multilingual support significantly improves (zero-shot) relation extraction, enabling the population of low-resourced KBs from their well-populated counterparts.</abstract>
      <url hash="460ce040">D19-6130</url>
      <doi>10.18653/v1/D19-6130</doi>
      <bibkey>abdou-etal-2019-x</bibkey>
      <pwccode url="https://github.com/mhany90/Multi-WikiRE" additional="false">mhany90/Multi-WikiRE</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/x-wikire">X-WikiRE</pwcdataset>
    </paper>
    <paper id="32">
      <title>Zero-shot Dependency Parsing with Pre-trained Multilingual Sentence Representations</title>
      <author><first>Ke</first><last>Tran</last></author>
      <author><first>Arianna</first><last>Bisazza</last></author>
      <pages>281–288</pages>
      <abstract>We investigate whether off-the-shelf deep bidirectional sentence representations (Devlin et al., 2019) trained on a massively multilingual corpus (multilingual BERT) enable the development of an unsupervised universal dependency parser. This approach only leverages a mix of monolingual corpora in many languages and does not require any translation data making it applicable to low-resource languages. In our experiments we outperform the best CoNLL 2018 language-specific systems in all of the shared task’s six truly low-resource languages while using a single system. However, we also find that (i) parsing accuracy still varies dramatically when changing the training languages and (ii) in some target languages zero-shot transfer fails under all tested conditions, raising concerns on the ‘universality’ of the whole approach.</abstract>
      <url hash="b05c7ad9">D19-6132</url>
      <doi>10.18653/v1/D19-6132</doi>
      <bibkey>tran-bisazza-2019-zero</bibkey>
    </paper>
  </volume>
  <volume id="62" ingest-date="2019-11-03">
    <meta>
      <booktitle>Proceedings of the Tenth International Workshop on Health Text Mining and Information Analysis (LOUHI 2019)</booktitle>
      <url hash="c4a11d6c">D19-62</url>
      <editor><first>Eben</first><last>Holderness</last></editor>
      <editor><first>Antonio</first><last>Jimeno Yepes</last></editor>
      <editor><first>Alberto</first><last>Lavelli</last></editor>
      <editor><first>Anne-Lyse</first><last>Minard</last></editor>
      <editor><first>James</first><last>Pustejovsky</last></editor>
      <editor><first>Fabio</first><last>Rinaldi</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Hong Kong</address>
      <month>November</month>
      <year>2019</year>
    </meta>
    <frontmatter>
      <url hash="c7c6688f">D19-6200</url>
      <bibkey>emnlp-2019-international</bibkey>
    </frontmatter>
    <paper id="3">
      <title>On the Effectiveness of the Pooling Methods for Biomedical Relation Extraction with <a href="https://en.wikipedia.org/wiki/Deep_learning">Deep Learning</a></title>
      <author><first>Tuan Ngo</first><last>Nguyen</last></author>
      <author><first>Franck</first><last>Dernoncourt</last></author>
      <author><first>Thien Huu</first><last>Nguyen</last></author>
      <pages>18–27</pages>
      <abstract>Deep learning models have achieved state-of-the-art performances on many relation extraction datasets. A common element in these deep learning models involves the pooling mechanisms where a sequence of hidden vectors is aggregated to generate a single <a href="https://en.wikipedia.org/wiki/Representation_(mathematics)">representation vector</a>, serving as the <a href="https://en.wikipedia.org/wiki/Feature_(machine_learning)">features</a> to perform prediction for RE. Unfortunately, the <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> in the literature tend to employ different strategies to perform pooling for RE, leading to the challenge to determine the best pooling mechanism for this problem, especially in the <a href="https://en.wikipedia.org/wiki/Biomedicine">biomedical domain</a>. In order to answer this question, in this work, we conduct a comprehensive study to evaluate the effectiveness of different pooling mechanisms for the deep learning models in biomedical RE. The experimental results suggest that dependency-based pooling is the best pooling strategy for RE in the biomedical domain, yielding the state-of-the-art performance on two benchmark datasets for this problem.</abstract>
      <url hash="698c5db2">D19-6203</url>
      <doi>10.18653/v1/D19-6203</doi>
      <bibkey>nguyen-etal-2019-effectiveness</bibkey>
    </paper>
    <paper id="7">
      <title>Experiments with ad hoc ambiguous abbreviation expansion</title>
      <author><first>Agnieszka</first><last>Mykowiecka</last></author>
      <author><first>Malgorzata</first><last>Marciniak</last></author>
      <pages>44–53</pages>
      <abstract>The paper addresses experiments to expand ad hoc ambiguous abbreviations in medical notes on the basis of morphologically annotated texts, without using additional domain resources. We work on Polish data but the described approaches can be used for other languages too. We test two <a href="https://en.wikipedia.org/wiki/Numerical_methods_for_ordinary_differential_equations">methods</a> to select candidates for word abbreviation expansions. The first one automatically selects all words in text which might be an expansion of an abbreviation according to the <a href="https://en.wikipedia.org/wiki/Linguistic_prescription">language rules</a>. The second method uses clustering of abbreviation occurrences to select representative elements which are manually annotated to determine lists of potential expansions. We then train a <a href="https://en.wikipedia.org/wiki/Statistical_classification">classifier</a> to assign expansions to <a href="https://en.wikipedia.org/wiki/Abbreviation">abbreviations</a> based on three <a href="https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets">training sets</a> : automatically obtained, consisting of manual annotation, and concatenation of the two previous ones. The results obtained for the manually annotated training data significantly outperform automatically obtained training data. Adding the automatically obtained training data to the manually annotated data improves the results, in particular for less frequent abbreviations. In this context the proposed a priori data driven selection of possible extensions turned out to be crucial.</abstract>
      <url hash="93ce997a">D19-6207</url>
      <doi>10.18653/v1/D19-6207</doi>
      <bibkey>mykowiecka-marciniak-2019-experiments</bibkey>
    </paper>
    <paper id="9">
      <title>Extracting relevant information from physician-patient dialogues for automated clinical note taking</title>
      <author><first>Serena</first><last>Jeblee</last></author>
      <author><first>Faiza</first><last>Khan Khattak</last></author>
      <author><first>Noah</first><last>Crampton</last></author>
      <author><first>Muhammad</first><last>Mamdani</last></author>
      <author><first>Frank</first><last>Rudzicz</last></author>
      <pages>65–74</pages>
      <abstract>We present a <a href="https://en.wikipedia.org/wiki/System">system</a> for automatically extracting pertinent medical information from dialogues between clinicians and patients. The <a href="https://en.wikipedia.org/wiki/System">system</a> parses each dialogue and extracts <a href="https://en.wikipedia.org/wiki/Legal_person">entities</a> such as medications and symptoms, using <a href="https://en.wikipedia.org/wiki/Context_(language_use)">context</a> to predict which entities are relevant. We also classify the primary diagnosis for each conversation. In addition, we extract <a href="https://en.wikipedia.org/wiki/Topic_and_comment">topic information</a> and identify relevant utterances. This serves as a baseline for a system that extracts information from dialogues and automatically generates a patient note, which can be reviewed and edited by the clinician.</abstract>
      <url hash="316c0d0a">D19-6209</url>
      <doi>10.18653/v1/D19-6209</doi>
      <bibkey>jeblee-etal-2019-extracting</bibkey>
    </paper>
    <paper id="12">
      <title>What does the language of foods say about us?</title>
      <author><first>Hoang</first><last>Van</last></author>
      <author><first>Ahmad</first><last>Musa</last></author>
      <author><first>Hang</first><last>Chen</last></author>
      <author><first>Stephen</first><last>Kobourov</last></author>
      <author><first>Mihai</first><last>Surdeanu</last></author>
      <pages>87–96</pages>
      <abstract>In this work we investigate the signal contained in the language of food on <a href="https://en.wikipedia.org/wiki/Social_media">social media</a>. We experiment with a <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> of 24 million food-related tweets, and make several observations. First, thelanguageoffoodhaspredictive power. We are able to predict if states in the United States (US) are above the medianratesfortype2diabetesmellitus(T2DM), <a href="https://en.wikipedia.org/wiki/Income">income</a>, <a href="https://en.wikipedia.org/wiki/Poverty">poverty</a>, and <a href="https://en.wikipedia.org/wiki/Education">education</a>   outperforming previous work by 418 %. Second, we investigate the effect of <a href="https://en.wikipedia.org/wiki/Socioeconomics">socioeconomic factors</a> (income, <a href="https://en.wikipedia.org/wiki/Poverty">poverty</a>, and education) on predicting state-level T2DM rates. Socioeconomic factors do improve T2DM prediction, with the greatestimprovementcomingfrompovertyinformation(6%),but, importantly, thelanguage of food adds distinct information that is not captured by <a href="https://en.wikipedia.org/wiki/Socioeconomics">socioeconomics</a>. Third, we analyze how the language of food has changed over a five-year period (2013   2017), which is indicative of the shift in eating habits in the US during that period. We find several food trends, and that the language of food is used differently by different groups such as differentgenders. Last, weprovideanonlinevisualization tool for real-time queries and <a href="https://en.wikipedia.org/wiki/Semantic_analysis_(linguistics)">semantic analysis</a>.</abstract>
      <url hash="f298c11f">D19-6212</url>
      <doi>10.18653/v1/D19-6212</doi>
      <bibkey>van-etal-2019-language</bibkey>
    </paper>
    <paper id="13">
      <title>Dreaddit : A Reddit Dataset for <a href="https://en.wikipedia.org/wiki/Stress–strain_analysis">Stress Analysis</a> in <a href="https://en.wikipedia.org/wiki/Social_media">Social Media</a><fixed-case>D</fixed-case>readdit: A <fixed-case>R</fixed-case>eddit Dataset for Stress Analysis in Social Media</title>
      <author><first>Elsbeth</first><last>Turcan</last></author>
      <author><first>Kathy</first><last>McKeown</last></author>
      <pages>97–107</pages>
      <abstract>Stress is a nigh-universal human experience, particularly in the <a href="https://en.wikipedia.org/wiki/Online_and_offline">online world</a>. While <a href="https://en.wikipedia.org/wiki/Stress_(biology)">stress</a> can be a motivator, too much <a href="https://en.wikipedia.org/wiki/Stress_(biology)">stress</a> is associated with many negative health outcomes, making its identification useful across a range of domains. However, existing computational research typically only studies <a href="https://en.wikipedia.org/wiki/Stress_(biology)">stress</a> in domains such as <a href="https://en.wikipedia.org/wiki/Speech">speech</a>, or in short genres such as <a href="https://en.wikipedia.org/wiki/Twitter">Twitter</a>. We present Dreaddit, a new text corpus of lengthy multi-domain social media data for the identification of stress. Our <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> consists of 190 K posts from five different categories of <a href="https://en.wikipedia.org/wiki/Reddit">Reddit communities</a> ; we additionally label 3.5 K total segments taken from 3 K posts using <a href="https://en.wikipedia.org/wiki/Amazon_Mechanical_Turk">Amazon Mechanical Turk</a>. We present preliminary supervised learning methods for identifying <a href="https://en.wikipedia.org/wiki/Stress_(biology)">stress</a>, both neural and traditional, and analyze the complexity and diversity of the data and characteristics of each category.</abstract>
      <url hash="d29e4afc">D19-6213</url>
      <attachment hash="8136a22a">D19-6213.Attachment.zip</attachment>
      <doi>10.18653/v1/D19-6213</doi>
      <bibkey>turcan-mckeown-2019-dreaddit</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/dreaddit">Dreaddit</pwcdataset>
    </paper>
    <paper id="14">
      <title>Towards Understanding of Medical Randomized Controlled Trials by Conclusion Generation</title>
      <author><first>Alexander Te-Wei</first><last>Shieh</last></author>
      <author><first>Yung-Sung</first><last>Chuang</last></author>
      <author><first>Shang-Yu</first><last>Su</last></author>
      <author><first>Yun-Nung</first><last>Chen</last></author>
      <pages>108–117</pages>
      <abstract>Randomized controlled trials (RCTs) represent the paramount evidence of <a href="https://en.wikipedia.org/wiki/Medicine">clinical medicine</a>. Using <a href="https://en.wikipedia.org/wiki/Machine">machines</a> to interpret the massive amount of <a href="https://en.wikipedia.org/wiki/Randomized_controlled_trial">RCTs</a> has the potential of aiding clinical decision-making. We propose a RCT conclusion generation task from the PubMed 200k RCT sentence classification dataset to examine the effectiveness of sequence-to-sequence models on understanding RCTs. We first build a pointer-generator baseline model for <a href="https://en.wikipedia.org/wiki/Logical_consequence">conclusion generation</a>. Then we fine-tune the state-of-the-art GPT-2 language model, which is pre-trained with general domain data, for this new medical domain task. Both automatic and human evaluation show that our GPT-2 fine-tuned models achieve improved quality and <a href="https://en.wikipedia.org/wiki/Correctness_(computer_science)">correctness</a> in the generated conclusions compared to the baseline pointer-generator model. Further inspection points out the limitations of this current approach and future directions to explore.</abstract>
      <url hash="3ba5177c">D19-6214</url>
      <doi>10.18653/v1/D19-6214</doi>
      <bibkey>shieh-etal-2019-towards</bibkey>
      <pwccode url="https://github.com/MiuLab/RCT-Gen" additional="false">MiuLab/RCT-Gen</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/pubmed-rct">PubMed RCT</pwcdataset>
    </paper>
    <paper id="17">
      <title>Dilated LSTM with attention for Classification of Suicide Notes<fixed-case>LSTM</fixed-case> with attention for Classification of Suicide Notes</title>
      <author><first>Annika M</first><last>Schoene</last></author>
      <author><first>George</first><last>Lacey</last></author>
      <author><first>Alexander P</first><last>Turner</last></author>
      <author><first>Nina</first><last>Dethlefs</last></author>
      <pages>136–145</pages>
      <abstract>In this paper we present a dilated LSTM with attention mechanism for document-level classification of suicide notes, last statements and depressed notes. We achieve an <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> of 87.34 % compared to competitive baselines of 80.35 % (Logistic Model Tree) and 82.27 % (Bi-directional LSTM with Attention). Furthermore, we provide an analysis of both the grammatical and thematic content of <a href="https://en.wikipedia.org/wiki/Suicide_note">suicide notes</a>, <a href="https://en.wikipedia.org/wiki/Suicide_note">last statements</a> and <a href="https://en.wikipedia.org/wiki/Suicide_note">depressed notes</a>. We find that the use of <a href="https://en.wikipedia.org/wiki/Personal_pronoun">personal pronouns</a>, <a href="https://en.wikipedia.org/wiki/Cognition">cognitive processes</a> and references to loved ones are most important. Finally, we show through visualisations of <a href="https://en.wikipedia.org/wiki/Attention">attention weights</a> that the Dilated LSTM with <a href="https://en.wikipedia.org/wiki/Attention">attention</a> is able to identify the same distinguishing features across documents as the <a href="https://en.wikipedia.org/wiki/Linguistic_description">linguistic analysis</a>.</abstract>
      <url hash="914f72b4">D19-6217</url>
      <doi>10.18653/v1/D19-6217</doi>
      <bibkey>schoene-etal-2019-dilated</bibkey>
    </paper>
    <paper id="18">
      <title>Writing habits and telltale neighbors : analyzing clinical concept usage patterns with sublanguage embeddings</title>
      <author><first>Denis</first><last>Newman-Griffis</last></author>
      <author><first>Eric</first><last>Fosler-Lussier</last></author>
      <pages>146–156</pages>
      <abstract>Natural language processing techniques are being applied to increasingly diverse types of <a href="https://en.wikipedia.org/wiki/Electronic_health_record">electronic health records</a>, and can benefit from in-depth understanding of the distinguishing characteristics of medical document types. We present a method for characterizing the usage patterns of clinical concepts among different document types, in order to capture semantic differences beyond the lexical level. By training concept embeddings on clinical documents of different types and measuring the differences in their nearest neighborhood structures, we are able to measure divergences in concept usage while correcting for <a href="https://en.wikipedia.org/wiki/Noise_(signal_processing)">noise</a> in embedding learning. Experiments on the MIMIC-III corpus demonstrate that our approach captures clinically-relevant differences in concept usage and provides an intuitive way to explore semantic characteristics of clinical document collections.</abstract>
      <url hash="52b0c9e9">D19-6218</url>
      <doi>10.18653/v1/D19-6218</doi>
      <bibkey>newman-griffis-fosler-lussier-2019-writing</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/mimic-iii">MIMIC-III</pwcdataset>
    </paper>
    </volume>
  <volume id="63" ingest-date="2019-11-03">
    <meta>
      <booktitle>Proceedings of the 2nd Workshop on Multilingual Surface Realisation (MSR 2019)</booktitle>
      <url hash="38285244">D19-63</url>
      <editor><first>Simon</first><last>Mille</last></editor>
      <editor><first>Anja</first><last>Belz</last></editor>
      <editor><first>Bernd</first><last>Bohnet</last></editor>
      <editor><first>Yvette</first><last>Graham</last></editor>
      <editor><first>Leo</first><last>Wanner</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Hong Kong, China</address>
      <month>November</month>
      <year>2019</year>
    </meta>
    <frontmatter>
      <url hash="5bb5d823">D19-6300</url>
      <bibkey>emnlp-2019-multilingual</bibkey>
    </frontmatter>
    <paper id="7">
      <title>Surface Realization Shared Task 2019 (MSR19): The Team 6 Approach<fixed-case>MSR</fixed-case>19): The Team 6 Approach</title>
      <author><first>Thiago</first><last>Castro Ferreira</last></author>
      <author><first>Emiel</first><last>Krahmer</last></author>
      <pages>59–62</pages>
      <abstract>This study describes the approach developed by the Tilburg University team to the shallow track of the Multilingual Surface Realization Shared Task 2019 (SR’19) (Mille et al., 2019). Based on Ferreira et al. (2017) and on our 2018 submission Ferreira et al. (2018), the approach generates texts by first preprocessing an input dependency tree into an ordered linearized string, which is then realized using a rule-based and a statistical machine translation (SMT) model. This year our submission is able to realize texts in the 11 languages proposed for the task, different from our last year submission, which covered only 6 <a href="https://en.wikipedia.org/wiki/Indo-European_languages">Indo-European languages</a>. The <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a> is publicly available.</abstract>
      <url hash="2a9a52d4">D19-6307</url>
      <doi>10.18653/v1/D19-6307</doi>
      <bibkey>castro-ferreira-krahmer-2019-surface</bibkey>
    </paper>
    <paper id="11">
      <title>The DipInfoUniTo Realizer at SRST’19 : Learning to Rank and Deep Morphology Prediction for Multilingual Surface Realization<fixed-case>D</fixed-case>ip<fixed-case>I</fixed-case>nfo<fixed-case>U</fixed-case>ni<fixed-case>T</fixed-case>o Realizer at <fixed-case>SRST</fixed-case>’19: Learning to Rank and Deep Morphology Prediction for Multilingual Surface Realization</title>
      <author><first>Alessandro</first><last>Mazzei</last></author>
      <author><first>Valerio</first><last>Basile</last></author>
      <pages>81–87</pages>
      <abstract>We describe the <a href="https://en.wikipedia.org/wiki/System">system</a> presented at the SR’19 shared task by the DipInfoUnito team. Our <a href="https://en.wikipedia.org/wiki/Software_development_process">approach</a> is based on <a href="https://en.wikipedia.org/wiki/Supervised_learning">supervised machine learning</a>. In particular, we divide the SR task into two independent subtasks, namely word order prediction and morphology inflection prediction. Two <a href="https://en.wikipedia.org/wiki/Neural_network">neural networks</a> with different architectures run on the same input structure, each producing a partial output which is recombined in the final step in order to produce the predicted surface form. This work is a direct successor of the <a href="https://en.wikipedia.org/wiki/Architecture">architecture</a> presented at SR’19.</abstract>
      <url hash="46ce76c0">D19-6311</url>
      <doi>10.18653/v1/D19-6311</doi>
      <bibkey>mazzei-basile-2019-dipinfounito</bibkey>
    </paper>
    <paper id="12">
      <title>LORIA / Lorraine University at Multilingual Surface Realisation 2019<fixed-case>LORIA</fixed-case> / Lorraine University at Multilingual Surface Realisation 2019</title>
      <author><first>Anastasia</first><last>Shimorina</last></author>
      <author><first>Claire</first><last>Gardent</last></author>
      <pages>88–93</pages>
      <abstract>This paper presents the LORIA / Lorraine University submission at the Multilingual Surface Realisation shared task 2019 for the shallow track. We outline our <a href="https://en.wikipedia.org/wiki/Software_development_process">approach</a> and evaluate <a href="https://en.wikipedia.org/wiki/Information_technology">it</a> on 11 languages covered by the shared task. We provide a separate evaluation of each component of our <a href="https://en.wikipedia.org/wiki/Pipeline_(software)">pipeline</a>, concluding on some difficulties and suggesting directions for future work.</abstract>
      <url hash="356d1878">D19-6312</url>
      <doi>10.18653/v1/D19-6312</doi>
      <bibkey>shimorina-gardent-2019-loria</bibkey>
    </paper>
    <paper id="13">
      <title>Back-Translation as Strategy to Tackle the Lack of Corpus in Natural Language Generation from Semantic Representations</title>
      <author><first>Marco Antonio</first><last>Sobrevilla Cabezudo</last></author>
      <author><first>Simon</first><last>Mille</last></author>
      <author><first>Thiago</first><last>Pardo</last></author>
      <pages>94–103</pages>
      <abstract>This paper presents an exploratory study that aims to evaluate the usefulness of back-translation in Natural Language Generation (NLG) from semantic representations for non-English languages. Specifically, Abstract Meaning Representation and Brazilian Portuguese (BP) are chosen as semantic representation and language, respectively. Two methods (focused on Statistical and Neural Machine Translation) are evaluated on two datasets (one automatically generated and another one human-generated) to compare the performance in a real context. Also, several cuts according to quality measures are performed to evaluate the importance (or not) of the <a href="https://en.wikipedia.org/wiki/Data_quality">data quality</a> in NLG. Results show that there are still many improvements to be made but <a href="https://en.wikipedia.org/wiki/This_(song)">this</a> is a promising approach.</abstract>
      <url hash="14cfdf0b">D19-6313</url>
      <doi>10.18653/v1/D19-6313</doi>
      <bibkey>sobrevilla-cabezudo-etal-2019-back</bibkey>
    </paper>
  </volume>
  <volume id="64" ingest-date="2019-11-03">
    <meta>
      <booktitle>Proceedings of the Beyond Vision and LANguage: inTEgrating Real-world kNowledge (LANTERN)</booktitle>
      <url hash="7927f11d">D19-64</url>
      <editor><first>Aditya</first><last>Mogadala</last></editor>
      <editor><first>Dietrich</first><last>Klakow</last></editor>
      <editor><first>Sandro</first><last>Pezzelle</last></editor>
      <editor><first>Marie-Francine</first><last>Moens</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Hong Kong, China</address>
      <month>November</month>
      <year>2019</year>
    </meta>
    <frontmatter>
      <url hash="ef9b4846">D19-6400</url>
      <bibkey>emnlp-2019-beyond</bibkey>
    </frontmatter>
    <paper id="3">
      <title>Big Generalizations with Small Data : Exploring the Role of Training Samples in Learning Adjectives of Size</title>
      <author><first>Sandro</first><last>Pezzelle</last></author>
      <author><first>Raquel</first><last>Fernández</last></author>
      <pages>18–23</pages>
      <abstract>In this paper, we experiment with a recently proposed visual reasoning task dealing with quantities   modeling the multimodal, contextually-dependent meaning of size adjectives (‘big’, ‘small’)   and explore the impact of varying the training data on the learning behavior of a state-of-art system. In previous work, <a href="https://en.wikipedia.org/wiki/Conceptual_model">models</a> have been shown to fail in generalizing to unseen adjective-noun combinations. Here, we investigate whether, and to what extent, seeing some of these cases during training helps a <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> understand the rule subtending the task, i.e., that being big implies being not small, and vice versa. We show that relatively few examples are enough to understand this relationship, and that developing a specific, mutually exclusive representation of size adjectives is beneficial to the task.</abstract>
      <url hash="cf9f5923">D19-6403</url>
      <doi>10.18653/v1/D19-6403</doi>
      <bibkey>pezzelle-fernandez-2019-big</bibkey>
    </paper>
    <paper id="5">
      <title>On the Role of Scene Graphs in Image Captioning</title>
      <author><first>Dalin</first><last>Wang</last></author>
      <author><first>Daniel</first><last>Beck</last></author>
      <author><first>Trevor</first><last>Cohn</last></author>
      <pages>29–34</pages>
      <abstract>Scene graphs represent semantic information in images, which can help <a href="https://en.wikipedia.org/wiki/Image">image captioning system</a> to produce more descriptive outputs versus using only the <a href="https://en.wikipedia.org/wiki/Image">image</a> as context. Recent captioning approaches rely on ad-hoc approaches to obtain <a href="https://en.wikipedia.org/wiki/Graph_of_a_function">graphs</a> for <a href="https://en.wikipedia.org/wiki/Image">images</a>. However, those <a href="https://en.wikipedia.org/wiki/Graph_of_a_function">graphs</a> introduce <a href="https://en.wikipedia.org/wiki/Noise_(electronics)">noise</a> and it is unclear the effect of <a href="https://en.wikipedia.org/wiki/Parsing">parser errors</a> on captioning accuracy. In this work, we investigate to what extent <a href="https://en.wikipedia.org/wiki/Scene_graph">scene graphs</a> can help image captioning. Our results show that a state-of-the-art scene graph parser can boost performance almost as much as the ground truth graphs, showing that the bottleneck currently resides more on the captioning models than on the performance of the scene graph parser.</abstract>
      <url hash="c33730ea">D19-6405</url>
      <doi>10.18653/v1/D19-6405</doi>
      <bibkey>wang-etal-2019-role</bibkey>
    </paper>
    <paper id="6">
      <title>Understanding the Effect of Textual Adversaries in Multimodal Machine Translation</title>
      <author><first>Koel</first><last>Dutta Chowdhury</last></author>
      <author><first>Desmond</first><last>Elliott</last></author>
      <pages>35–40</pages>
      <abstract>It is assumed that multimodal machine translation systems are better than text-only systems at translating phrases that have a direct correspondence in the image. This assumption has been challenged in experiments demonstrating that state-of-the-art multimodal systems perform equally well in the presence of randomly selected images, but, more recently, it has been shown that masking entities from the source language sentence during training can help to overcome this problem. In this paper, we conduct experiments with both visual and textual adversaries in order to understand the role of incorrect textual inputs to such systems. Our results show that when the source language sentence contains mistakes, multimodal translation systems do not leverage the additional visual signal to produce the correct <a href="https://en.wikipedia.org/wiki/Translation">translation</a>. We also find that the degradation of <a href="https://en.wikipedia.org/wiki/Translation_(biology)">translation</a> performance caused by textual adversaries is significantly higher than by visual adversaries.</abstract>
      <url hash="98a693ce">D19-6406</url>
      <doi>10.18653/v1/D19-6406</doi>
      <bibkey>dutta-chowdhury-elliott-2019-understanding</bibkey>
    </paper>
    <paper id="7">
      <title>Learning to request guidance in emergent language</title>
      <author><first>Benjamin</first><last>Kolb</last></author>
      <author><first>Leon</first><last>Lang</last></author>
      <author><first>Henning</first><last>Bartsch</last></author>
      <author><first>Arwin</first><last>Gansekoele</last></author>
      <author><first>Raymond</first><last>Koopmanschap</last></author>
      <author><first>Leonardo</first><last>Romor</last></author>
      <author><first>David</first><last>Speck</last></author>
      <author><first>Mathijs</first><last>Mul</last></author>
      <author><first>Elia</first><last>Bruni</last></author>
      <pages>41–50</pages>
      <abstract>Previous research into agent communication has shown that a pre-trained guide can speed up the <a href="https://en.wikipedia.org/wiki/Learning">learning process</a> of an imitation learning agent. The guide achieves this by providing the agent with <a href="https://en.wikipedia.org/wiki/Message_passing">discrete messages</a> in an emerged language about how to solve the task. We extend this one-directional communication by a one-bit communication channel from the learner back to the guide : It is able to ask the guide for help, and we limit the guidance by penalizing the learner for these requests. During <a href="https://en.wikipedia.org/wiki/Training">training</a>, the agent learns to control this <a href="https://en.wikipedia.org/wiki/Gate">gate</a> based on its current observation. We find that the amount of requested guidance decreases over time and <a href="https://en.wikipedia.org/wiki/Advice_(opinion)">guidance</a> is requested in situations of high uncertainty. We investigate the <a href="https://en.wikipedia.org/wiki/Intelligent_agent">agent</a>’s performance in cases of open and closed gates and discuss potential motives for the observed gating behavior.</abstract>
      <url hash="e43409e7">D19-6407</url>
      <attachment hash="d80efcb3">D19-6407.Attachment.zip</attachment>
      <doi>10.18653/v1/D19-6407</doi>
      <bibkey>kolb-etal-2019-learning</bibkey>
    </paper>
    <paper id="9">
      <title>Seeded self-play for <a href="https://en.wikipedia.org/wiki/Language_acquisition">language learning</a></title>
      <author><first>Abhinav</first><last>Gupta</last></author>
      <author><first>Ryan</first><last>Lowe</last></author>
      <author><first>Jakob</first><last>Foerster</last></author>
      <author><first>Douwe</first><last>Kiela</last></author>
      <author><first>Joelle</first><last>Pineau</last></author>
      <pages>62–66</pages>
      <abstract>How can we teach <a href="https://en.wikipedia.org/wiki/Intelligent_agent">artificial agents</a> to use <a href="https://en.wikipedia.org/wiki/Human_language">human language</a> flexibly to solve problems in real-world environments? We have an example of this in nature : human babies eventually learn to use <a href="https://en.wikipedia.org/wiki/Human_language">human language</a> to solve problems, and they are taught with an adult human-in-the-loop. Unfortunately, current <a href="https://en.wikipedia.org/wiki/List_of_machine_learning_methods">machine learning methods</a> (e.g. from deep reinforcement learning) are too data inefficient to learn <a href="https://en.wikipedia.org/wiki/Language">language</a> in this way. An outstanding goal is finding an <a href="https://en.wikipedia.org/wiki/Algorithm">algorithm</a> with a suitable ‘language learning prior’ that allows it to learn human language, while minimizing the number of on-policy human interactions. In this paper, we propose to learn such a prior in simulation using an approach we call, Learning to Learn to Communicate (L2C). Specifically, in L2C we train a meta-learning agent in simulation to interact with populations of pre-trained agents, each with their own distinct communication protocol. Once the meta-learning agent is able to quickly adapt to each population of agents, it can be deployed in new <a href="https://en.wikipedia.org/wiki/Population">populations</a>, including populations speaking human language. Our key insight is that such populations can be obtained via self-play, after pre-training agents with imitation learning on a small amount of off-policy human language data. We call this latter technique Seeded Self-Play (S2P). Our preliminary experiments show that agents trained with L2C and S2P need fewer on-policy samples to learn a compositional language in a <a href="https://en.wikipedia.org/wiki/Lewis_signaling_game">Lewis signaling game</a>.</abstract>
      <url hash="a57f3d4d">D19-6409</url>
      <doi>10.18653/v1/D19-6409</doi>
      <revision id="1" href="D19-6409v1" hash="eac7dcf7" />
      <revision id="2" href="D19-6409v2" hash="a57f3d4d" date="2021-01-03">Updated title</revision>
      <bibkey>gupta-etal-2019-seeded</bibkey>
    </paper>
  </volume>
  <volume id="65" ingest-date="2019-11-03">
    <meta>
      <booktitle>Proceedings of the Fourth Workshop on Discourse in Machine Translation (DiscoMT 2019)</booktitle>
      <url hash="e612d975">D19-65</url>
      <editor><first>Andrei</first><last>Popescu-Belis</last></editor>
      <editor><first>Sharid</first><last>Loáiciga</last></editor>
      <editor><first>Christian</first><last>Hardmeier</last></editor>
      <editor><first>Deyi</first><last>Xiong</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Hong Kong, China</address>
      <month>November</month>
      <year>2019</year>
    </meta>
    <frontmatter>
      <url hash="b94ae676">D19-6500</url>
      <bibkey>emnlp-2019-discourse</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Analysing Coreference in Transformer Outputs</title>
      <author><first>Ekaterina</first><last>Lapshinova-Koltunski</last></author>
      <author><first>Cristina</first><last>España-Bonet</last></author>
      <author><first>Josef</first><last>van Genabith</last></author>
      <pages>1–12</pages>
      <abstract>We analyse coreference phenomena in three neural machine translation systems trained with different data settings with or without access to explicit intra- and cross-sentential anaphoric information. We compare <a href="https://en.wikipedia.org/wiki/System">system</a> performance on two different <a href="https://en.wikipedia.org/wiki/Genre">genres</a> : <a href="https://en.wikipedia.org/wiki/News">news</a> and <a href="https://en.wikipedia.org/wiki/TED_(conference)">TED talks</a>. To do this, we manually annotate (the possibly incorrect) coreference chains in the MT outputs and evaluate the coreference chain translations. We define an error typology that aims to go further than pronoun translation adequacy and includes types such as incorrect word selection or missing words. The features of coreference chains in automatic translations are also compared to those of the source texts and <a href="https://en.wikipedia.org/wiki/Translation">human translations</a>. The analysis shows stronger potential translationese effects in machine translated outputs than in human translations.</abstract>
      <url hash="bf4cf622">D19-6501</url>
      <doi>10.18653/v1/D19-6501</doi>
      <bibkey>lapshinova-koltunski-etal-2019-analysing</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/parcorfull">ParCorFull</pwcdataset>
    </paper>
    <paper id="3">
      <title>When and Why is Document-level Context Useful in <a href="https://en.wikipedia.org/wiki/Neural_machine_translation">Neural Machine Translation</a>?</title>
      <author><first>Yunsu</first><last>Kim</last></author>
      <author><first>Duc Thanh</first><last>Tran</last></author>
      <author><first>Hermann</first><last>Ney</last></author>
      <pages>24–34</pages>
      <abstract>Document-level context has received lots of attention for compensating neural machine translation (NMT) of isolated sentences. However, recent advances in document-level NMT focus on sophisticated integration of the context, explaining its improvement with only a few selected examples or targeted test sets. We extensively quantify the causes of improvements by a document-level model in general test sets, clarifying the limit of the usefulness of document-level context in NMT. We show that most of the improvements are not interpretable as utilizing the <a href="https://en.wikipedia.org/wiki/Context_(language_use)">context</a>. We also show that a minimal encoding is sufficient for the <a href="https://en.wikipedia.org/wiki/Context-free_grammar">context modeling</a> and very long context is not helpful for <a href="https://en.wikipedia.org/wiki/Context-free_grammar">NMT</a>.</abstract>
      <url hash="8095d1ed">D19-6503</url>
      <attachment hash="6fe8d7b9">D19-6503.Attachment.pdf</attachment>
      <doi>10.18653/v1/D19-6503</doi>
      <bibkey>kim-etal-2019-document</bibkey>
      <pwccode url="https://github.com/ducthanhtran/sockeye_document_context" additional="false">ducthanhtran/sockeye_document_context</pwccode>
    </paper>
    <paper id="4">
      <title>Data augmentation using <a href="https://en.wikipedia.org/wiki/Back_translation">back-translation</a> for context-aware neural machine translation</title>
      <author><first>Amane</first><last>Sugiyama</last></author>
      <author><first>Naoki</first><last>Yoshinaga</last></author>
      <pages>35–44</pages>
      <abstract>A single sentence does not always convey information that is enough to translate it into other languages. Some target languages need to add or specialize words that are omitted or ambiguous in the source languages (e.g, <a href="https://en.wikipedia.org/wiki/Zero_(linguistics)">zero pronouns</a> in translating Japanese to English or <a href="https://en.wikipedia.org/wiki/Epicene_pronoun">epicene pronouns</a> in translating English to French). To translate such ambiguous sentences, we need contexts beyond a single sentence, and have so far explored context-aware neural machine translation (NMT). However, a large amount of parallel corpora is not easily available to train accurate context-aware NMT models. In this study, we first obtain large-scale pseudo parallel corpora by back-translating monolingual data, and then investigate its impact on the translation accuracy of context-aware NMT models. We evaluated context-aware NMT models trained with small parallel corpora and the large-scale pseudo parallel corpora on English-Japanese and English-French datasets to demonstrate the large impact of the <a href="https://en.wikipedia.org/wiki/Data_augmentation">data augmentation</a> for context-aware NMT models.</abstract>
      <url hash="8bf53b2f">D19-6504</url>
      <doi>10.18653/v1/D19-6504</doi>
      <bibkey>sugiyama-yoshinaga-2019-data</bibkey>
    </paper>
    <paper id="6">
      <title>Analysing concatenation approaches to document-level NMT in two different domains<fixed-case>NMT</fixed-case> in two different domains</title>
      <author><first>Yves</first><last>Scherrer</last></author>
      <author><first>Jörg</first><last>Tiedemann</last></author>
      <author><first>Sharid</first><last>Loáiciga</last></author>
      <pages>51–61</pages>
      <abstract>In this paper, we investigate how different aspects of <a href="https://en.wikipedia.org/wiki/Context_(language_use)">discourse context</a> affect the performance of recent neural MT systems. We describe two popular datasets covering news and movie subtitles and we provide a thorough analysis of the distribution of various document-level features in their domains. Furthermore, we train a set of context-aware MT models on both datasets and propose a comparative evaluation scheme that contrasts coherent context with artificially scrambled documents and absent context, arguing that the impact of discourse-aware MT models will become visible in this way. Our results show that the <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> are indeed affected by the manipulation of the test data, providing a different view on document-level translation quality than absolute sentence-level scores.</abstract>
      <url hash="525ce679">D19-6506</url>
      <doi>10.18653/v1/D19-6506</doi>
      <bibkey>scherrer-etal-2019-analysing</bibkey>
    </paper>
  </volume>
  <volume id="66" ingest-date="2019-11-03">
    <meta>
      <booktitle>Proceedings of the Second Workshop on Fact Extraction and VERification (FEVER)</booktitle>
      <url hash="a61e632d">D19-66</url>
      <editor><first>James</first><last>Thorne</last></editor>
      <editor><first>Andreas</first><last>Vlachos</last></editor>
      <editor><first>Oana</first><last>Cocarascu</last></editor>
      <editor><first>Christos</first><last>Christodoulopoulos</last></editor>
      <editor><first>Arpit</first><last>Mittal</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Hong Kong, China</address>
      <month>November</month>
      <year>2019</year>
    </meta>
    <frontmatter>
      <url hash="0edcc137">D19-6600</url>
      <bibkey>emnlp-2019-fact</bibkey>
    </frontmatter>
    <paper id="3">
      <title>Neural Multi-Task Learning for Stance Prediction</title>
      <author><first>Wei</first><last>Fang</last></author>
      <author><first>Moin</first><last>Nadeem</last></author>
      <author><first>Mitra</first><last>Mohtarami</last></author>
      <author><first>James</first><last>Glass</last></author>
      <pages>13–19</pages>
      <abstract>We present a multi-task learning model that leverages large amount of textual information from existing datasets to improve stance prediction. In particular, we utilize multiple NLP tasks under both unsupervised and supervised settings for the target stance prediction task. Our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> obtains state-of-the-art performance on a public benchmark dataset, Fake News Challenge, outperforming current approaches by a wide margin.</abstract>
      <url hash="a123bec3">D19-6603</url>
      <doi>10.18653/v1/D19-6603</doi>
      <bibkey>fang-etal-2019-neural</bibkey>
    </paper>
    <paper id="4">
      <title>GEM : Generative Enhanced Model for adversarial attacks<fixed-case>GEM</fixed-case>: Generative Enhanced Model for adversarial attacks</title>
      <author><first>Piotr</first><last>Niewinski</last></author>
      <author><first>Maria</first><last>Pszona</last></author>
      <author><first>Maria</first><last>Janicka</last></author>
      <pages>20–26</pages>
      <abstract>We present our Generative Enhanced Model (GEM) that we used to create samples awarded the first prize on the FEVER 2.0 Breakers Task. GEM is the extended language model developed upon GPT-2 architecture. The addition of novel target vocabulary input to the already existing <a href="https://en.wikipedia.org/wiki/Context_(language_use)">context input</a> enabled controlled text generation. The training procedure resulted in creating a <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a> that inherited the knowledge of pretrained GPT-2, and therefore was ready to generate natural-like English sentences in the task domain with some additional control. As a result, <a href="https://en.wikipedia.org/wiki/Graphics_Environment_Manager">GEM</a> generated malicious claims that mixed facts from various articles, so it became difficult to classify their truthfulness.</abstract>
      <url hash="a95dc9a9">D19-6604</url>
      <attachment hash="e936a6c3">D19-6604.Attachment.pdf</attachment>
      <doi>10.18653/v1/D19-6604</doi>
      <bibkey>niewinski-etal-2019-gem</bibkey>
    </paper>
    <paper id="9">
      <title>Unsupervised Question Answering for Fact-Checking</title>
      <author><first>Mayank</first><last>Jobanputra</last></author>
      <pages>52–56</pages>
      <abstract>Recent <a href="https://en.wikipedia.org/wiki/Deep_learning">Deep Learning (DL) models</a> have succeeded in achieving human-level accuracy on various natural language tasks such as <a href="https://en.wikipedia.org/wiki/Question_answering">question-answering</a>, <a href="https://en.wikipedia.org/wiki/Natural-language_understanding">natural language inference (NLI)</a>, and <a href="https://en.wikipedia.org/wiki/Textual_entailment">textual entailment</a>. These <a href="https://en.wikipedia.org/wiki/Task_(project_management)">tasks</a> not only require the <a href="https://en.wikipedia.org/wiki/Context_(language_use)">contextual knowledge</a> but also the <a href="https://en.wikipedia.org/wiki/Reason">reasoning abilities</a> to be solved efficiently. In this paper, we propose an unsupervised question-answering based approach for a similar task, <a href="https://en.wikipedia.org/wiki/Fact-checking">fact-checking</a>. We transform the FEVER dataset into a Cloze-task by masking named entities provided in the claims. To predict the answer token, we utilize pre-trained Bidirectional Encoder Representations from Transformers (BERT). The classifier computes label based on the correctly answered questions and a threshold. Currently, the <a href="https://en.wikipedia.org/wiki/Statistical_classification">classifier</a> is able to classify the claims as SUPPORTS and MANUAL_REVIEW. This approach achieves a label accuracy of 80.2 % on the development set and 80.25 % on the test set of the transformed dataset.</abstract>
      <url hash="f867bd73">D19-6609</url>
      <doi>10.18653/v1/D19-6609</doi>
      <bibkey>jobanputra-2019-unsupervised</bibkey>
    </paper>
    <paper id="10">
      <title>Improving Evidence Detection by Leveraging Warrants</title>
      <author><first>Keshav</first><last>Singh</last></author>
      <author><first>Paul</first><last>Reisert</last></author>
      <author><first>Naoya</first><last>Inoue</last></author>
      <author><first>Pride</first><last>Kavumba</last></author>
      <author><first>Kentaro</first><last>Inui</last></author>
      <pages>57–62</pages>
      <abstract>Recognizing the implicit link between a claim and a piece of evidence (i.e. warrant) is the key to improving the performance of evidence detection. In this work, we explore the effectiveness of automatically extracted warrants for evidence detection. Given a claim and candidate evidence, our proposed method extracts multiple warrants via similarity search from an existing, structured corpus of arguments. We then attentively aggregate the extracted warrants, considering the consistency between the given argument and the acquired <a href="https://en.wikipedia.org/wiki/Warrant_(law)">warrants</a>. Although a qualitative analysis on the warrants shows that the extraction method needs to be improved, our results indicate that our <a href="https://en.wikipedia.org/wiki/Methodology">method</a> can still improve the performance of evidence detection.</abstract>
      <url hash="ec4f1cb2">D19-6610</url>
      <doi>10.18653/v1/D19-6610</doi>
      <bibkey>singh-etal-2019-improving</bibkey>
    </paper>
    <paper id="12">
      <title>Extract and Aggregate : A Novel Domain-Independent Approach to Factual Data Verification</title>
      <author><first>Anton</first><last>Chernyavskiy</last></author>
      <author><first>Dmitry</first><last>Ilvovsky</last></author>
      <pages>69–78</pages>
      <abstract>Triggered by Internet development, a large amount of information is published in online sources. However, it is a well-known fact that publications are inundated with inaccurate data. That is why <a href="https://en.wikipedia.org/wiki/Fact-checking">fact-checking</a> has become a significant topic in the last 5 years. It is widely accepted that factual data verification is a challenge even for the experts. This paper presents a domain-independent fact checking system. It can solve the fact verification problem entirely or at the individual stages. The proposed <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> combines various advanced methods of <a href="https://en.wikipedia.org/wiki/Text_mining">text data analysis</a>, such as BERT and Infersent. The theoretical and empirical study of the <a href="https://en.wikipedia.org/wiki/Feature_(machine_learning)">system features</a> is carried out. Based on FEVER and Fact Checking Challenge test-collections, experimental results demonstrate that our model can achieve the score on a par with state-of-the-art models designed by the specificity of particular datasets.</abstract>
      <url hash="d68e0165">D19-6612</url>
      <doi>10.18653/v1/D19-6612</doi>
      <bibkey>chernyavskiy-ilvovsky-2019-extract</bibkey>
    </paper>
    <paper id="15">
      <title>FEVER Breaker’s Run of Team NbAuzDrLqg<fixed-case>FEVER</fixed-case> Breaker’s Run of Team <fixed-case>N</fixed-case>b<fixed-case>A</fixed-case>uz<fixed-case>D</fixed-case>r<fixed-case>L</fixed-case>qg</title>
      <author><first>Youngwoo</first><last>Kim</last></author>
      <author><first>James</first><last>Allan</last></author>
      <pages>99–104</pages>
      <abstract>We describe our submission for the Breaker phase of the second Fact Extraction and VERification (FEVER) Shared Task. Our adversarial data can be explained by two perspectives. First, we aimed at testing <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a>’s ability to retrieve evidence, when appropriate query terms could not be easily generated from the claim. Second, we test model’s ability to precisely understand the implications of the texts, which we expect to be rare in FEVER 1.0 dataset. Overall, we suggested six types of <a href="https://en.wikipedia.org/wiki/Adversarial_system">adversarial attacks</a>. The evaluation on the submitted <a href="https://en.wikipedia.org/wiki/System">systems</a> showed that the <a href="https://en.wikipedia.org/wiki/System">systems</a> were only able get both the evidence and label correct in 20 % of the data. We also demonstrate our adversarial run analysis in the data development process.</abstract>
      <url hash="10ac9e95">D19-6615</url>
      <doi>10.18653/v1/D19-6615</doi>
      <bibkey>kim-allan-2019-fever</bibkey>
    </paper>
    <paper id="17">
      <title>Team GPLSI. Approach for automated fact checking<fixed-case>GPLSI</fixed-case>. Approach for automated fact checking</title>
      <author><first>Aimée</first><last>Alonso-Reina</last></author>
      <author><first>Robiert</first><last>Sepúlveda-Torres</last></author>
      <author><first>Estela</first><last>Saquete</last></author>
      <author><first>Manuel</first><last>Palomar</last></author>
      <pages>110–114</pages>
      <abstract>Fever Shared 2.0 Task is a challenge meant for developing automated fact checking systems. Our approach for the Fever 2.0 is based on a previous proposal developed by Team Athene UKP TU Darmstadt. Our proposal modifies the sentence retrieval phase, using statement extraction and representation in the form of triplets (subject, object, action). Triplets are extracted from the claim and compare to <a href="https://en.wikipedia.org/wiki/Multiple_birth">triplets</a> extracted from Wikipedia articles using <a href="https://en.wikipedia.org/wiki/Semantic_similarity">semantic similarity</a>. Our results are satisfactory but there is room for improvement.</abstract>
      <url hash="3173f18f">D19-6617</url>
      <doi>10.18653/v1/D19-6617</doi>
      <bibkey>alonso-reina-etal-2019-team</bibkey>
    </paper>
  </volume>
</collection>