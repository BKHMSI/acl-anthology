<collection id="2021.mmtlrl">
  <volume id="1" ingest-date="2021-11-09">
    <meta>
      <booktitle>Proceedings of the First Workshop on Multimodal Machine Translation for Low Resource Languages (MMTLRL 2021)</booktitle>
      <editor><first>Thoudam</first><last>Doren Singh</last></editor>
      <editor><first>Cristina</first><last>Espa&#241;a i Bonet</last></editor>
      <editor><first>Sivaji</first><last>Bandyopadhyay</last></editor>
      <editor><first>Josef</first><last>van Genabith</last></editor>
      <publisher>INCOMA Ltd.</publisher>
      <address>Online (Virtual Mode)</address>
      <month>September</month>
      <year>2021</year>
    </meta>
    <frontmatter>
      <url hash="cd3e0b50">2021.mmtlrl-1.0</url>
      <bibkey>mmtlrl-2021-multimodal</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Models and Tasks for Human-Centered Machine Translation</title>
      <author><first>Marine</first><last>Carpuat</last></author>
      <pages>1</pages>
      <abstract>In this talk, I will describe current research directions in my group that aim to make machine translation (MT) more human-centered. Instead of viewing MT solely as a task that aims to transduce a source sentence into a well-formed target language equivalent, we revisit all steps of the MT research and development lifecycle with the goal of designing MT systems that are able to help people communicate across language barriers. I will present methods to better characterize the parallel training data that powers MT systems, and how the degree of equivalence impacts translation quality. I will introduce models that enable flexible conditional language generation, and will discuss recent work on framing machine translation tasks and evaluation to center human factors.</abstract>
      <url hash="1210d2dd">2021.mmtlrl-1.1</url>
      <bibkey>carpuat-2021-models</bibkey>
    </paper>
    <paper id="5">
      <title>Multimodal Simultaneous Machine Translation</title>
      <author><first>Lucia</first><last>Specia</last></author>
      <pages>30</pages>
      <abstract>Simultaneous machine translation (SiMT) aims to translate a continuous input text stream into another language with the lowest latency and highest quality possible. Therefore, translation has to start with an incomplete source text, which is read progressively, creating the need for anticipation. In this talk I will present work where we seek to understand whether the addition of visual information can compensate for the missing source context. We analyse the impact of different multimodal approaches and visual features on state-of-the-art SiMT frameworks, including fixed and dynamic policy approaches using reinforcement learning. Our results show that visual context is helpful and that visually-grounded models based on explicit object region information perform the best. Our qualitative analysis illustrates cases where only the multimodal systems are able to translate correctly from English into gender-marked languages, as well as deal with differences in word order, such as adjective-noun placement between English and French.</abstract>
      <url hash="9eebbc4e">2021.mmtlrl-1.5</url>
      <bibkey>specia-2021-multimodal</bibkey>
    </paper>
    <paper id="6">
      <title>Multimodal Neural Machine Translation System for <fixed-case>E</fixed-case>nglish to <fixed-case>B</fixed-case>engali</title>
      <author><first>Shantipriya</first><last>Parida</last></author>
      <author><first>Subhadarshi</first><last>Panda</last></author>
      <author><first>Satya Prakash</first><last>Biswal</last></author>
      <author><first>Ketan</first><last>Kotwal</last></author>
      <author><first>Arghyadeep</first><last>Sen</last></author>
      <author><first>Satya Ranjan</first><last>Dash</last></author>
      <author><first>Petr</first><last>Motlicek</last></author>
      <pages>31&#8211;39</pages>
      <abstract>Multimodal Machine Translation (MMT) systems utilize additional information from other modalities beyond text to improve the quality of machine translation (MT). The additional modality is typically in the form of images. Despite proven advantages, it is indeed difficult to develop an MMT system for various languages primarily due to the lack of a suitable multimodal dataset. In this work, we develop an MMT for English-&gt; Bengali using a recently published Bengali Visual Genome (BVG) dataset that contains images with associated bilingual textual descriptions. Through a comparative study of the developed MMT system vis-a-vis a Text-to-text translation, we demonstrate that the use of multimodal data not only improves the translation performance improvement in BLEU score of +1.3 on the development set, +3.9 on the evaluation test, and +0.9 on the challenge test set but also helps to resolve ambiguities in the pure text description. As per best of our knowledge, our English-Bengali MMT system is the first attempt in this direction, and thus, can act as a baseline for the subsequent research in MMT for low resource languages.</abstract>
      <url hash="bdc63cae">2021.mmtlrl-1.6</url>
      <bibkey>parida-etal-2021-multimodal</bibkey>
    </paper>
    </volume>
</collection>