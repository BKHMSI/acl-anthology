<?xml version='1.0' encoding='utf-8'?>
<collection id="2020.tacl">
  <volume id="1">
    <meta>
      <booktitle>Transactions of the Association for Computational Linguistics, Volume 8</booktitle>
      <editor><last>Johnson</last><first>Mark</first></editor>
      <editor><last>Roark</last><first>Brian</first></editor>
      <editor><last>Nenkova</last><first>Ani</first></editor>
      <publisher>MIT Press</publisher>
      <address>Cambridge, MA</address>
      <year>2020</year>
    </meta>
    <frontmatter>
      <bibkey>tacl-2020-transactions</bibkey>
    </frontmatter>
    <paper id="2">
      <title>AMR-To-Text Generation with Graph Transformer<fixed-case>AMR</fixed-case>-To-Text Generation with Graph Transformer</title>
      <author><first>Tianming</first><last>Wang</last></author>
      <author><first>Xiaojun</first><last>Wan</last></author>
      <author><first>Hanqi</first><last>Jin</last></author>
      <doi>10.1162/tacl_a_00297</doi>
      <abstract>Abstract meaning representation (AMR)-to-text generation is the challenging task of generating natural language texts from AMR graphs, where <a href="https://en.wikipedia.org/wiki/Vertex_(graph_theory)">nodes</a> represent concepts and <a href="https://en.wikipedia.org/wiki/Glossary_of_graph_theory_terms">edges</a> denote relations. The current state-of-the-art methods use graph-to-sequence models ; however, they still can not significantly outperform the previous sequence-to-sequence models or statistical approaches. In this paper, we propose a novel graph-to-sequence model (Graph Transformer) to address this task. The <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> directly encodes the AMR graphs and learns the <a href="https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)">node representations</a>. A pairwise interaction function is used for computing the semantic relations between the concepts. Moreover, <a href="https://en.wikipedia.org/wiki/Attentional_control">attention mechanisms</a> are used for aggregating the information from the incoming and outgoing neighbors, which help the <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a> to capture the semantic information effectively. Our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> outperforms the state-of-the-art <a href="https://en.wikipedia.org/wiki/Artificial_neural_network">neural approach</a> by 1.5 BLEU points on LDC2015E86 and 4.8 BLEU points on LDC2017T10 and achieves new state-of-the-art performances.</abstract>
      <pages>19–33</pages>
      <url hash="9dce31b7">2020.tacl-1.2</url>
      <bibkey>wang-etal-2020-amr</bibkey>
    </paper>
    <paper id="4">
      <title>Membership Inference Attacks on Sequence-to-Sequence Models : Is My Data In Your Machine Translation System?<fixed-case>I</fixed-case>s My Data In Your Machine Translation System?</title>
      <author><first>Sorami</first><last>Hisamoto</last></author>
      <author><first>Matt</first><last>Post</last></author>
      <author><first>Kevin</first><last>Duh</last></author>
      <doi>10.1162/tacl_a_00299</doi>
      <abstract>Data privacy is an important issue for <a href="https://en.wikipedia.org/wiki/Machine_learning">machine learning</a> as a service providers. We focus on the problem of membership inference attacks : Given a data sample and black-box access to a <a href="https://en.wikipedia.org/wiki/Statistical_model">model’s API</a>, determine whether the sample existed in the model’s training data. Our contribution is an investigation of this problem in the context of sequence-to-sequence models, which are important in applications such as <a href="https://en.wikipedia.org/wiki/Machine_translation">machine translation</a> and <a href="https://en.wikipedia.org/wiki/Closed-circuit_television">video captioning</a>. We define the membership inference problem for sequence generation, provide an open dataset based on state-of-the-art machine translation models, and report initial results on whether these models leak private information against several kinds of membership inference attacks.</abstract>
      <pages>49–63</pages>
      <url hash="dd95e987">2020.tacl-1.4</url>
      <bibkey>hisamoto-etal-2020-membership</bibkey>
      <pwccode url="https://github.com/sorami/TACL-Membership" additional="false">sorami/TACL-Membership</pwccode>
    </paper>
    <paper id="5">
      <title>SpanBERT : Improving Pre-training by Representing and Predicting Spans<fixed-case>S</fixed-case>pan<fixed-case>BERT</fixed-case>: Improving Pre-training by Representing and Predicting Spans</title>
      <author><first>Mandar</first><last>Joshi</last></author>
      <author><first>Danqi</first><last>Chen</last></author>
      <author><first>Yinhan</first><last>Liu</last></author>
      <author><first>Daniel S.</first><last>Weld</last></author>
      <author><first>Luke</first><last>Zettlemoyer</last></author>
      <author><first>Omer</first><last>Levy</last></author>
      <doi>10.1162/tacl_a_00300</doi>
      <abstract>We present SpanBERT, a pre-training method that is designed to better represent and predict spans of text. Our approach extends BERT by (1) masking contiguous random spans, rather than random tokens, and (2) training the span boundary representations to predict the entire content of the masked span, without relying on the individual token representations within it. SpanBERT consistently outperforms <a href="https://en.wikipedia.org/wiki/BERT">BERT</a> and our better-tuned baselines, with substantial gains on span selection tasks such as <a href="https://en.wikipedia.org/wiki/Question_answering">question answering</a> and <a href="https://en.wikipedia.org/wiki/Coreference_resolution">coreference resolution</a>. In particular, with the same training data and model size as BERTlarge, our single <a href="https://en.wikipedia.org/wiki/Statistical_model">model</a> obtains 94.6 % and 88.7 % <a href="https://en.wikipedia.org/wiki/F-number">F1</a> on SQuAD 1.1 and 2.0 respectively. We also achieve a new state of the art on the OntoNotes coreference resolution task (79.6 % F1), strong performance on the TACRED relation extraction benchmark, and even gains on GLUE.1</abstract>
      <pages>64–77</pages>
      <url hash="27aaf55c">2020.tacl-1.5</url>
      <bibkey>joshi-etal-2020-spanbert</bibkey>
      <pwccode url="https://github.com/facebookresearch/SpanBERT" additional="true">facebookresearch/SpanBERT</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/cola">CoLA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/conll-2012-1">CoNLL-2012</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/hotpotqa">HotpotQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mrpc">MRPC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mrqa-2019">MRQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/natural-questions">Natural Questions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/newsqa">NewsQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/ontonotes-5-0">OntoNotes 5.0</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/qnli">QNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/quora-question-pairs">Quora Question Pairs</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/rte">RTE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/re-tacred">Re-TACRED</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sts-benchmark">STS Benchmark</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/searchqa">SearchQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/superglue">SuperGLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/tacred">TACRED</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/triviaqa">TriviaQA</pwcdataset>
    </paper>
    <paper id="7">
      <title>A Knowledge-Enhanced Pretraining Model for Commonsense Story Generation</title>
      <author><first>Jian</first><last>Guan</last></author>
      <author><first>Fei</first><last>Huang</last></author>
      <author><first>Zhihao</first><last>Zhao</last></author>
      <author><first>Xiaoyan</first><last>Zhu</last></author>
      <author><first>Minlie</first><last>Huang</last></author>
      <doi>10.1162/tacl_a_00302</doi>
      <abstract>Story generation, namely, generating a reasonable story from a leading context, is an important but challenging task. In spite of the success in modeling fluency and local coherence, existing neural language generation models (e.g., GPT-2) still suffer from repetition, logic conflicts, and lack of long-range coherence in generated stories. We conjecture that this is because of the difficulty of associating relevant <a href="https://en.wikipedia.org/wiki/Commonsense_knowledge">commonsense knowledge</a>, understanding the <a href="https://en.wikipedia.org/wiki/Causality">causal relationships</a>, and planning entities and events with proper temporal order. In this paper, we devise a knowledge-enhanced pretraining model for commonsense story generation. We propose to utilize <a href="https://en.wikipedia.org/wiki/Commonsense_knowledge">commonsense knowledge</a> from external knowledge bases to generate reasonable stories. To further capture the causal and temporal dependencies between the sentences in a reasonable story, we use <a href="https://en.wikipedia.org/wiki/Multi-task_learning">multi-task learning</a>, which combines a discriminative objective to distinguish true and fake stories during <a href="https://en.wikipedia.org/wiki/Fine-tuning">fine-tuning</a>. Automatic and manual evaluation shows that our model can generate more reasonable stories than state-of-the-art baselines, particularly in terms of logic and global coherence.</abstract>
      <pages>93–108</pages>
      <url hash="c075234a">2020.tacl-1.7</url>
      <bibkey>guan-etal-2020-knowledge</bibkey>
      <pwccode url="https://github.com/thu-coai/CommonsenseStoryGen" additional="false">thu-coai/CommonsenseStoryGen</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/atomic">ATOMIC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/rocstories">ROCStories</pwcdataset>
    </paper>
    <paper id="8">
      <title>Improving Candidate Generation for Low-resource Cross-lingual Entity Linking</title>
      <author><first>Shuyan</first><last>Zhou</last></author>
      <author><first>Shruti</first><last>Rijhwani</last></author>
      <author><first>John</first><last>Wieting</last></author>
      <author><first>Jaime</first><last>Carbonell</last></author>
      <author><first>Graham</first><last>Neubig</last></author>
      <doi>10.1162/tacl_a_00303</doi>
      <abstract>Cross-lingual entity linking (XEL) is the task of finding referents in a target-language knowledge base (KB) for mentions extracted from source-language texts. The first step of (X)EL is candidate generation, which retrieves a list of plausible candidate entities from the target-language KB for each mention. Approaches based on resources from <a href="https://en.wikipedia.org/wiki/Wikipedia">Wikipedia</a> have proven successful in the realm of relatively high-resource languages, but these do not extend well to low-resource languages with few, if any, <a href="https://en.wikipedia.org/wiki/Wikipedia">Wikipedia pages</a>. Recently, transfer learning methods have been shown to reduce the demand for resources in the low-resource languages by utilizing resources in closely related languages, but the performance still lags far behind their high-resource counterparts. In this paper, we first assess the problems faced by current entity candidate generation methods for low-resource XEL, then propose three improvements that (1) reduce the disconnect between entity mentions and KB entries, and (2) improve the robustness of the model to low-resource scenarios. The methods are simple, but effective : We experiment with our approach on seven XEL datasets and find that they yield an average gain of 16.9 % in Top-30 gold candidate recall, compared with state-of-the-art baselines. Our improved <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> also yields an average gain of 7.9 % in in-KB accuracy of end-to-end XEL.1</abstract>
      <pages>109–124</pages>
      <url hash="74d04e8d">2020.tacl-1.8</url>
      <bibkey>zhou-etal-2020-improving-candidate</bibkey>
      <pwccode url="https://github.com/shuyanzhou/pbel_plus" additional="false">shuyanzhou/pbel_plus</pwccode>
    </paper>
    <paper id="11">
      <title>Theoretical Limitations of Self-Attention in Neural Sequence Models</title>
      <author><first>Michael</first><last>Hahn</last></author>
      <doi>10.1162/tacl_a_00306</doi>
      <abstract>Transformers are emerging as the new workhorse of <a href="https://en.wikipedia.org/wiki/Natural_language_processing">NLP</a>, showing great success across tasks. Unlike <a href="https://en.wikipedia.org/wiki/Linear_time-invariant_system">LSTMs</a>, <a href="https://en.wikipedia.org/wiki/Transformer">transformers</a> process input sequences entirely through self-attention. Previous work has suggested that the computational capabilities of self-attention to process hierarchical structures are limited. In this work, we mathematically investigate the computational power of self-attention to model <a href="https://en.wikipedia.org/wiki/Formal_language">formal languages</a>. Across both soft and hard attention, we show strong theoretical limitations of the computational abilities of self-attention, finding that it can not model periodic finite-state languages, nor hierarchical structure, unless the number of layers or heads increases with input length. These limitations seem surprising given the practical success of self-attention and the prominent role assigned to hierarchical structure in <a href="https://en.wikipedia.org/wiki/Linguistics">linguistics</a>, suggesting that <a href="https://en.wikipedia.org/wiki/Natural_language">natural language</a> can be approximated well with models that are too weak for the formal languages typically assumed in theoretical linguistics.</abstract>
      <pages>156–171</pages>
      <url hash="7be349b5">2020.tacl-1.11</url>
      <bibkey>hahn-2020-theoretical</bibkey>
    <title_es>Limitaciones teóricas de la autoatención en modelos de secuencia neuronal</title_es>
      <title_ar>القيود النظرية للانتباه الذاتي في نماذج التسلسل العصبي</title_ar>
      <title_pt>Limitações Teóricas da Autoatenção em Modelos de Sequência Neural</title_pt>
      <title_ja>ニューラルシーケンスモデルにおける自己関心の理論的制限</title_ja>
      <title_zh>神经序模自意之论局限性</title_zh>
      <title_hi>तंत्रिका अनुक्रम मॉडल में आत्म-ध्यान की सैद्धांतिक सीमाएं</title_hi>
      <title_ga>Teorainneacha Teoiriciúla ar Fhéinaird i Múnlaí Seichimh Néaracha</title_ga>
      <title_hu>Az önfigyelem elméleti korlátai neurális szekvencia modellekben</title_hu>
      <title_ka>თავისუფალური მოდელში თავისუფალური დაახლოების ტეორეტიკური ლიმიტაციები</title_ka>
      <title_el>Θεωρητικοί περιορισμοί της αυτοπροσοχής σε μοντέλα νευρωνικής ακολουθίας</title_el>
      <title_lt>Teoriniai savitarpio dėmesio apribojimai nervų sekos modeliuose</title_lt>
      <title_mk>Теоретските ограничувања на самовниманието во моделите на неврална секвенција</title_mk>
      <title_it>Limitazioni teoriche di auto-attenzione nei modelli di sequenza neurale</title_it>
      <title_kk>Невралдық реттеу үлгілерінде өзіңіздің теоретикалық шектеулері</title_kk>
      <title_ms>Had teori perhatian-diri dalam Model Kelajuan Neural</title_ms>
      <title_mt>Limitazzjonijiet Teoretiċi tal-Attenzjoni Awto-Attenzjali fil-Mudelli tas-Sekwenza Newrali</title_mt>
      <title_ml>സ്വയം ശ്രദ്ധിക്കുന്നതിന്റെ തിയോറിക്കല്‍ അതിരുകള്‍ നെയുറല്‍ സെക്കന്റ് മോഡലുകളില്‍</title_ml>
      <title_mn>Өөрийгөө анхаарлын теоретикийн хязгаарлалт</title_mn>
      <title_no>Teoretiske avgrensingar av selvmerksomhet i neiralsekvensmodeller</title_no>
      <title_pl>Teoretyczne ograniczenia uwagi w modelach sekwencji neuronowej</title_pl>
      <title_ro>Limitările teoretice ale auto-atenţiei în modelele de secvenţă neurală</title_ro>
      <title_si>ස්වයංක්‍රියාත්මක සීමාන්‍තික සීමා</title_si>
      <title_sr>Teoretičke ograničenje samopouzdanja u modelima neurološke sekvence</title_sr>
      <title_so>Theoretical Limitations of Self-Attention in Neural Sequence Models</title_so>
      <title_sv>Teoretiska begränsningar av självuppmärksamhet i neurala sekvensmodeller</title_sv>
      <title_ta>நெருக்கல் வரிசை மாதிரிகளில் தானாகவே கவனம் வரம்புகள்</title_ta>
      <title_ur>نائرل سکوئنس موڈلز میں Self-Attention کی تئوریٹیکی محدودیت</title_ur>
      <title_vi>Giới hạn lý thuyết của tự chú ý trong chế độ Chuỗi thần kinh</title_vi>
      <title_uz>Name</title_uz>
      <title_bg>Теоретични ограничения на самовниманието в модели на неврални последователности</title_bg>
      <title_hr>Teoretičke ograničenje samopouzdanja u modelima neurološke sekvence</title_hr>
      <title_nl>Theoretische beperkingen van zelfaandacht in neuronale sequentiemodellen</title_nl>
      <title_da>Teoretiske begrænsninger af selvopmærksomhed i neurale sekvensmodeller</title_da>
      <title_de>Theoretische Einschränkungen der Selbstaufmerksamkeit in neuronalen Sequenzmodellen</title_de>
      <title_id>Limitasi teori perhatian diri dalam Model Sekuensi Neural</title_id>
      <title_fa>محدودیت تئوریتیک توجه خودخواهی در مدل‌های مختلف عصبی</title_fa>
      <title_ko>신경 서열 모델에서의 자기주의 이론적 한계성</title_ko>
      <title_sw>Mipaka ya msingi ya Kujihisia Huru katika Mifumo ya Kifaransa</title_sw>
      <title_tr>Nöral sıralan Modellerinde özi-Attünsiň teoretikleri</title_tr>
      <title_af>Teoretiese beperking van Selfwaarskuwing in Neural Sequence Models</title_af>
      <title_sq>Theoretical Limitations of Self-Attention in Neural Sequence Models</title_sq>
      <title_am>ምርጫዎች</title_am>
      <title_hy>Նյարդային հաջորդականության մոդելներում ինքնաուշադրության տեսական սահմանափակումները</title_hy>
      <title_az>Neural Sequence Modell톛rind톛 칐z-칐z칲n칲n 칐z칲n칲 G칬zl톛m톛si</title_az>
      <title_bn>নিউরেল সেকেন্স মোডেলে স্বয়ংক্রিয়ভাবে সীমাবদ্ধ</title_bn>
      <title_bs>Teoretičke ograničenje samopouzdanja u modelima neurološke sekvence</title_bs>
      <title_ca>Limitacions teòriques d'auto-atenció en models de seqüències neuronals</title_ca>
      <title_cs>Teoretická omezení sebepozornosti v modelech neuronových sekvencí</title_cs>
      <title_et>Enesetähelepanu teoreetilised piirangud neurojärjestuse mudelites</title_et>
      <title_fi>Itsehuomion teoreettiset rajoitukset hermosekvenssimalleissa</title_fi>
      <title_sk>Teoretične omejitve samopozornosti v modelih nevralnih sekvenc</title_sk>
      <title_ha>KCharselect unicode block name</title_ha>
      <title_he>הגבלות תיאורטיות של תשומת לב עצמית במודלים של רצף נוירולי</title_he>
      <title_jv>Terotetik limitations of Self-Attention in Neral Sence model</title_jv>
      <title_bo>སྒེར་གྱི་དབྱེ་སྟངས་ལ་རང་ཉིད་ལྟར་བསམ་བློ་གཏོང་ཀྱི་སྐོར་ཚད་གཞི་སྒྲིག་དང་།</title_bo>
      <abstract_ar>تظهر المحولات باعتبارها العمود الفقري الجديد في البرمجة اللغوية العصبية ، مما يُظهر نجاحًا كبيرًا عبر المهام. على عكس LSTMs ، تعالج المحولات تسلسل المدخلات بالكامل من خلال الانتباه الذاتي. اقترح العمل السابق أن القدرات الحسابية للانتباه الذاتي لعملية الهياكل الهرمية محدودة. في هذا العمل ، نحقق رياضيًا في القوة الحسابية للانتباه الذاتي لنمذجة اللغات الرسمية. عبر كل من الاهتمام الناعم والصلب ، نظهر قيودًا نظرية قوية للقدرات الحسابية للانتباه الذاتي ، ووجدنا أنه لا يمكن نمذجة لغات الحالة المحدودة الدورية ، ولا الهيكل الهرمي ، ما لم يزداد عدد الطبقات أو الرؤوس مع طول الإدخال. تبدو هذه القيود مفاجئة بالنظر إلى النجاح العملي للانتباه الذاتي والدور البارز المخصص للبنية الهرمية في علم اللغة ، مما يشير إلى أنه يمكن تقريب اللغة الطبيعية جيدًا مع النماذج التي تكون ضعيفة جدًا بالنسبة للغات الرسمية المفترضة عادةً في علم اللغة النظري.</abstract_ar>
      <abstract_es>Los transformadores están emergiendo como el nuevo caballo de batalla de la PNL, mostrando un gran éxito en todas las tareas. A diferencia de los LSTM, los transformadores procesan las secuencias de entrada completamente mediante la autoatención Trabajos anteriores han sugerido que las capacidades computacionales de autoatención a las estructuras jerárquicas de procesos son limitadas. En este trabajo, investigamos matemáticamente el poder computacional de la autoatención para modelar lenguajes formales. A través de la atención suave y dura, mostramos fuertes limitaciones teóricas de las habilidades computacionales de la autoatención, encontrando que no puede modelar lenguajes periódicos de estados finitos, ni estructura jerárquica, a menos que el número de capas o cabezas aumente con la longitud de entrada. Estas limitaciones parecen sorprendentes dado el éxito práctico de la autoatención y el papel prominente asignado a la estructura jerárquica en la lingüística, lo que sugiere que el lenguaje natural se puede aproximar bien con modelos que son demasiado débiles para los lenguajes formales que normalmente se asumen en la lingüística teórica.</abstract_es>
      <abstract_pt>Os transformadores estão surgindo como o novo cavalo de batalha da PNL, mostrando grande sucesso em todas as tarefas. Ao contrário dos LSTMs, os transformadores processam as sequências de entrada inteiramente por meio de autoatenção. Trabalhos anteriores sugeriram que as capacidades computacionais de autoatenção para processar estruturas hierárquicas são limitadas. Neste trabalho, investigamos matematicamente o poder computacional da autoatenção para modelar linguagens formais. Tanto na atenção suave quanto na atenção dura, mostramos fortes limitações teóricas das habilidades computacionais da autoatenção, descobrindo que ela não pode modelar linguagens periódicas de estado finito, nem estrutura hierárquica, a menos que o número de camadas ou cabeças aumente com o comprimento da entrada. Essas limitações parecem surpreendentes, dado o sucesso prático da autoatenção e o papel proeminente atribuído à estrutura hierárquica na linguística, sugerindo que a linguagem natural pode ser bem aproximada com modelos que são muito fracos para as linguagens formais tipicamente assumidas na linguística teórica.</abstract_pt>
      <abstract_ja>トランスフォーマーは、NLPの新しいワークホースとして登場し、タスク全体で大きな成功を示しています。 LSTMとは異なり、変圧器は入力シーケンスを完全に自己注目によって処理する。 これまでの研究では、プロセス階層構造への自己注目の計算能力は限られていることが示唆されている。 本作では、形式言語をモデル化するための自己注目の計算力を数学的に調査している。 ソフトアテンションとハードアテンションの両方にわたって、私たちは自己注目の計算能力の強力な理論的制限を示しています。レイヤーまたはヘッドの数が入力の長さとともに増加しない限り、周期的な有限状態言語をモデル化することも、階層構造をモデル化することもできないことを発見しました。 これらの制限は、自己注目の実用的な成功と言語学における階層構造に割り当てられた顕著な役割を考えると驚くべきように思われ、理論言語学で典型的に想定される形式言語に対して弱すぎるモデルで自然言語をうまく近似できることを示唆している。</abstract_ja>
      <abstract_zh>变形金刚方为NLP新主,百务咸有大功。 与 LSTM 不同,变压器悉以意处之。 前言自注层次结构计算能力有限也。 以数学自对模型形式语言计算能力。 夫软注意力,示计算能力强论局限性,见其不能拟周期性有限言语,亦不能拟层结构,非层头随输增长也。 鉴自注之实成,及语言学之赋予等构,此局限性似可讶,此自然语言可善近于理语言学中常假形式语言弱也。</abstract_zh>
      <abstract_hi>ट्रांसफॉर्मर एनएलपी के नए वर्कहॉर्स के रूप में उभर रहे हैं, जो कार्यों में बड़ी सफलता दिखा रहे हैं। एलएसटीएम के विपरीत, ट्रांसफॉर्मर पूरी तरह से आत्म-ध्यान के माध्यम से इनपुट अनुक्रमों को संसाधित करते हैं। पिछले काम ने सुझाव दिया है कि पदानुक्रमित संरचनाओं को संसाधित करने के लिए आत्म-ध्यान की कम्प्यूटेशनल क्षमताएं सीमित हैं। इस काम में, हम गणितीय रूप से मॉडल औपचारिक भाषाओं के लिए आत्म-ध्यान की कम्प्यूटेशनल शक्ति की जांच करते हैं। नरम और कठोर दोनों ध्यान में, हम आत्म-ध्यान की कम्प्यूटेशनल क्षमताओं की मजबूत सैद्धांतिक सीमाओं को दिखाते हैं, यह पाते हुए कि यह आवधिक परिमित-राज्य भाषाओं को मॉडल नहीं कर सकता है, न ही पदानुक्रमित संरचना, जब तक कि परतों या सिर की संख्या इनपुट लंबाई के साथ नहीं बढ़ जाती है। ये सीमाएं आत्म-ध्यान की व्यावहारिक सफलता और भाषाविज्ञान में पदानुक्रमित संरचना को सौंपी गई प्रमुख भूमिका को देखते हुए आश्चर्यजनक लगती हैं, यह सुझाव देते हुए कि प्राकृतिक भाषा को उन मॉडलों के साथ अच्छी तरह से अनुमानित किया जा सकता है जो आमतौर पर सैद्धांतिक भाषाविज्ञान में ग्रहण की जाने वाली औपचारिक भाषाओं के लिए बहुत कमजोर हैं।</abstract_hi>
      <abstract_ga>Tá claochladáin ag teacht chun cinn mar chapall oibre nua NLP, rud a léiríonn rath iontach ar fud na dtascanna. Murab ionann agus LSTManna, déanann claochladáin seichimh ionchuir a phróiseáil go hiomlán trí fhéinaird. Tá sé tugtha le tuiscint in obair roimhe seo go bhfuil teorainn leis na hacmhainní ríomhaireachta a bhaineann le féinaird ar struchtúir ordlathacha a phróiseáil. Sa obair seo, déanaimid iniúchadh matamaitice ar chumhacht ríomhaireachtúil an fhéinaird ar mhúnlaí teangacha foirmiúla. Idir aird bhog agus chrua araon, léirímid teorainneacha teoiriciúla láidre ar chumais ríomhaireachtúla an fhéinaird, ag fáil amach nach féidir leis teangacha tréimhsiúla de chuid an stáit chríochnaigh a shamhaltú, ná struchtúr ordlathach, ach amháin má thagann méadú ar líon na sraitheanna nó na gcloigne le fad ionchuir. Is ábhar iontais na srianta seo i bhfianaise rath praiticiúil an fhéinaird agus an ról feiceálach a thugtar do struchtúr ordlathach sa teangeolaíocht, rud a thugann le tuiscint gur féidir teanga nádúrtha a chomhfhogasú go maith le samhlacha atá ró-lag do na teangacha foirmiúla a ghlactar go hiondúil sa teangeolaíocht theoiriciúil.</abstract_ga>
      <abstract_el>Οι μετασχηματιστές αναδύονται ως το νέο άλογο εργασίας του δείχνοντας μεγάλη επιτυχία σε όλες τις εργασίες. Σε αντίθεση με τα LSTMs, οι μετασχηματιστές επεξεργάζονται ακολουθίες εισόδου εξ ολοκλήρου μέσω της αυτοπροσοχής. Προηγούμενες εργασίες έχουν προτείνει ότι οι υπολογιστικές δυνατότητες της αυτοπροσοχής στις ιεραρχικές δομές διεργασιών είναι περιορισμένες. Σε αυτή την εργασία, ερευνούμε μαθηματικά την υπολογιστική δύναμη της αυτοπροσοχής σε μοντέλα τυπικών γλωσσών. Σε όλη τη μαλακή και σκληρή προσοχή, δείχνουμε ισχυρούς θεωρητικούς περιορισμούς των υπολογιστικών ικανοτήτων της αυτοπροσοχής, διαπιστώνοντας ότι δεν μπορεί να μοντελοποιήσει περιοδικές γλώσσες πεπερασμένων καταστάσεων, ούτε ιεραρχική δομή, εκτός αν ο αριθμός των στρωμάτων ή των κεφαλών αυξάνεται με το μήκος εισαγωγής. Αυτοί οι περιορισμοί φαίνονται έκπληκτοι δεδομένου της πρακτικής επιτυχίας της αυτοπροσοχής και του εξέχουσας ρόλου που αποδίδεται στην ιεραρχική δομή στη γλωσσολογία, υποδηλώνοντας ότι η φυσική γλώσσα μπορεί να προσεγγιστεί καλά με μοντέλα που είναι πολύ αδύναμα για τις τυπικές γλώσσες που συνήθως υιοθετούνται στη θεωρητική γλωσσολογία.</abstract_el>
      <abstract_hu>A transzformátorok az NLP új munkalójaként jelennek meg, amelyek nagy sikert mutatnak a feladatok között. Az LSTM-ekkel ellentétben a transzformátorok teljes mértékben önfigyelemmel dolgozzák fel a bemeneti szekvenciákat. Korábbi munkák azt sugallják, hogy a folyamathierarchikus struktúrákra való önfigyelem számítástechnikai képességei korlátozottak. Ebben a munkában matematikailag vizsgáljuk az önfigyelem számítógépes erejét a formai nyelvek modellezésére. Az önfigyelem számítástechnikai képességeinek erős elméleti korlátait mutatjuk mind a lágy, mind a kemény figyelem mentén, megállapítva, hogy nem modellezhet periodikus véges állapotú nyelveket, sem hierarchikus struktúrát, hacsak a rétegek vagy fejek száma nem nő a bemeneti hosszúsággal. Ezek a korlátozások meglepőnek tűnnek, tekintettel az önfigyelem gyakorlati sikerére és a hierarchikus struktúra kiemelkedő szerepére a nyelvészetben, ami arra utal, hogy a természetes nyelvet jól lehet közelíteni olyan modellekkel, amelyek túl gyengék az elméleti nyelvészetben jellemzően feltételezett formai nyelvek számára.</abstract_hu>
      <abstract_ka>ტრანფორმაციები გახდება როგორც NLP-ის ახალი სამუშაო კონდი, რომელიც სამუშაო დამუშაობა. LSTMs-ის განსხვავებაში, ტრანფორმენტერები პროცესის შეცდომის შეცდომის სერექტის გარეშე სერექტიურად. წინა სამუშაო მუშაოდ იყო, რომ სამუშაო თავიდან დაახლოების კომპუტაციალური შესაძლებლობა იერაქტიკალური სტრუქტურების პროცესი ამ სამუშაოში, ჩვენ მათემატიკურად განსხვავებთ თავისუფლიო ენების მოდელზე კომპუტაციალური ძალიან. ჩვენ თავისუფლიოს კომპუტაციალური შესაძლებლობის ძალიან თეორეტიკური ზომილებები ჩვენ ჩვენ აჩვენებთ, რომ ის არ შეიძლება პერიოდიკური ბრძანებული ენების მოდელი, ან hiერაქტიკური სტრუქტურაციის მოდელი, თუ არ ეს ზომილებები იქნება საინტერესო, რომლებიც თავისუფლიოს მონაცემის პრაქტიკური წარმატებით და მნიშვნელოვანი პროლია, რომლებიც იერაქტიკური სტრუქტურაში იყო, რომლებიც იერაქტიკური ლენგლისტიკში იქნება, რომ ნახვ</abstract_ka>
      <abstract_it>I trasformatori stanno emergendo come il nuovo cavallo di battaglia di NLP, mostrando grande successo in tutte le attività. A differenza degli LSTMs, i trasformatori elaborano le sequenze di input interamente attraverso l'auto-attenzione. Il lavoro precedente ha suggerito che le capacità computazionali di auto-attenzione alle strutture gerarchiche di processo sono limitate. In questo lavoro, analizziamo matematicamente il potere computazionale dell'auto-attenzione per modellare linguaggi formali. Attraverso l'attenzione morbida e dura, mostriamo forti limitazioni teoriche delle capacità computazionali dell'auto-attenzione, trovando che non può modellare linguaggi periodici a stato finito, né struttura gerarchica, a meno che il numero di strati o teste non aumenti con la lunghezza dell'input. Queste limitazioni sembrano sorprendenti, dato il successo pratico dell'auto-attenzione e il ruolo di primo piano assegnato alla struttura gerarchica nella linguistica, suggerendo che il linguaggio naturale può essere ben approssimato con modelli troppo deboli per i linguaggi formali tipicamente assunti nella linguistica teorica.</abstract_it>
      <abstract_lt>Naujas NLP darbo arklys tampa transformatoriais, o uždaviniai labai sėkmingi. Priešingai nei LSTM, transformatoriai procesuoja įvedimo sekas visiškai per savarankišką dėmesį. Ankstesnis darbas parodė, kad savarankiško dėmesio procesų hierarchinėms struktūroms skaičiavimo pajėgumai yra riboti. In this work, we mathematically investigate the computational power of self-attention to model formal languages.  Kalbant apie minkštą ir griežtą dėmesį, mes parodome griežtus teorinius savarankiško dėmesio skaičiavimo gebėjimų apribojimus, nustatydami, kad jis negali modeliuoti periodinių terminuotos būsenos kalbų ar hierarchinės struktūros, nebent sluoksnių ar galvų skaičius didėja su įėjimo ilgiu. Šie apribojimai atrodo stebinantys, atsižvelgiant į praktinę savarankiško dėmesio sėkmę ir į svarbų vaidmenį, priskirtą kalbų hierarchinei struktūrai, ir tai rodo, kad natūralią kalbą galima gerai suderinti su modeliais, kurie yra per silpni formalioms kalboms, paprastai laikomoms teorinėje kalboje.</abstract_lt>
      <abstract_kk>Түрлендірушілер NLP жаңа жұмыс аты ретінде көрсетеді, тапсырмалардың арасында жақсы сәтті көрсетеді. LSTMs сияқты түрлендірушілер өзіңізге қатынас арқылы кіріс реттеулерін өзіңізге ауыстыру. Алдыңғы жұмыс иерархиялық құрылғыларды өзіңізге өзіңіздің есептеу мүмкіндіктері шектелген деп ойлады. Бұл жұмыста, біз математикалық түрлі тілдер үлгісіне өзіміздің есептеу қуатын зерттейміз. Біз қабаттар немесе басының ұзындығын өзіңізге өзіңіздің компьютерлік мүмкіндіктерінің теоретикалық шектеулерін көрсетедік. Ол периодикалық шектеу тілдерін, немесе иерархиялық құрылғыны үлгілей алмайды. Бұл шектеулері өзіңіздің өзіңіздің тәжірибесіне және лингвистикалық құрылымына таңдалған иерархиялық рөліне сәйкес келеді деп ойлайды. Табиғи тіл теориялық лингвистикалық тілдер үшін оқылмаған үлгілер үшін дұрыс жақсы жа</abstract_kk>
      <abstract_mk>Трансформирачите се појавуваат како новиот работен коњ на НЛП, кој покажува голем успех низ задачите. За разлика од ЛСТМ, трансформаторите процесираат вводни секвенци целосно преку самото внимание. Претходната работа покажа дека компјутерските способности на самото внимание на процесорските хиерархиски структури се ограничени. In this work, we mathematically investigate the computational power of self-attention to model formal languages.  Покрај меко и тешко внимание, покажуваме силни теоретски ограничувања на компјутерските способности на себеси внимание, откривајќи дека не може да се моделира периодични јазици со ограничена состојба, ниту хиерархична структура, освен ако бројот на слоеви или глави не се зголеми со должина Овие ограничувања се чинат изненадувачки со оглед на практичниот успех на самото внимание и истакнатата улога доделена на хиерархичната структура во јазикот, што укажува на тоа дека природниот јазик може да биде добро приближен со моделите кои се премногу слаби за формалните јазици кои обично се претпоставуваат во теор</abstract_mk>
      <abstract_ms>Penukar muncul sebagai kuda kerja baru NLP, menunjukkan sukses besar melalui tugas. Tidak seperti LSTM, pengubah proses urutan input secara keseluruhan melalui perhatian-diri. Previous work has suggested that the computational capabilities of self-attention to process hierarchical structures are limited.  Dalam kerja ini, kita secara matematik menyelidiki kuasa pengiraan perhatian diri kepada bahasa formal model. Melalui perhatian lembut dan keras, kita menunjukkan keterangan teori yang kuat kemampuan pengiraan perhatian diri, mencari bahawa ia tidak boleh model bahasa-keadaan tertentu periodik, atau struktur hierarki, kecuali bilangan lapisan atau kepala meningkat dengan panjang input. These limitations seem surprising given the practical success of self-attention and the prominent role assigned to hierarchical structure in linguistics, suggesting that natural language can be approximated well with models that are too weak for the formal languages typically assumed in theoretical linguistics.</abstract_ms>
      <abstract_ml>ട്രാന്‍സ്ഫോര്‍മാര്‍ NLP-ന്‍റെ പുതിയ പണികുതിരകളായി പുറപ്പെടുന്നുണ്ട്, ജോലികളില്‍ വെച്ച് വലിയ വിജയ LSTMs വ്യത്യസ്തമായാല്‍, മാറ്റങ്ങളുടെ പ്രക്രിയയുടെ ഇന്‍പുട്ട് സെക്കന്‍സ് പൂര്‍ണ്ണമായും സ്വയം ശ്രദ്ധിച് മുമ്പുള്ള ജോലി ആത്മശ്രദ്ധ പ്രവര്‍ത്തിപ്പിക്കുന്ന സ്വയം ശ്രദ്ധിക്കുന്നതിന്റെ കണക്കിട്ടുള്ള കഴിവുകള്‍ പര ഈ ജോലിയില്‍, നമ്മള്‍ ഗണിക്കണക്കില്‍ അന്വേഷിക്കുന്നു, ഫോള്‍മാലിക ഭാഷകളുടെ സ്വയം ശ്രദ്ധിക്കുന്ന ശക്തിയു മെളുപ്പത്തിലും കഠിനമായ ശ്രദ്ധിക്കുമ്പോള്‍ സ്വയം ശ്രദ്ധിച്ചുകൊണ്ടിരിക്കുന്ന കണക്കിന്റെ സാധ്യതകളുടെ ശക്തിയുള്ള പരിധികള്‍ നമ്മള്‍ കാണിക്കുന്നു. അതിന് സ്വയം ശ്രദ്ധിക്കുന്നതിന്റെ പ്രാകൃതിക വിജയത്തിന്റെയും ഭാഷകങ്ങളില്‍ ഹിയറാര്‍ച്ചിക്കല്‍ സ്ഥാപിക്കുന്ന സ്ഥാനത്തിന്റെയും പ്രധാനപ്പെട്ട വിജയത്തിന്റെയും പരിധികള്‍ അത്</abstract_ml>
      <abstract_no>Transformerer oppstår som den nye arbeidhesten i NLP, som viser stor suksess over oppgåver. I motsetjing av LSTMs, transformerer inndata-sekvensar heilt gjennom selvmerking. Førre arbeid har foreslått at datamaskinasjonskapasiteten for selvmerking til prosessering av hierarkiske strukturar er begrenset. I denne arbeiden er vi matematisk undersøk datamaskina for selvmerksomhet til formelt språk. På både måkt og vanskelig oppmerksomhet viser vi sterke teoretiske grenser av datamaskinen for selvmerksomhet, og finn vi at det ikkje kan modellere periodiske begrensningsspråk, eller hierarkiske struktur, dersom antallet lag eller hovud aukar med inndata lengde. Desse grensene ser overraskende gjeven praktiske suksess av selvmerking og den prominente rolla tilordna hierarkiske strukturen i språk, som tyder på at naturspråk kan bli omtrent godt med modelane som er for svake for dei formlege språka som normalt antar i teoretiske språk.</abstract_no>
      <abstract_mt>It-trasformaturi qegħdin jitfaċċaw bħala ż-żwiemel il-ġdid tal-NLP, li juru suċċess kbir fil-kompiti kollha. Unlike LSTMs, transformers process input sequences entirely through self-attention.  Xogħol preċedenti ssuġġerixxa li l-kapaċitajiet komputattivi ta’ awtonomija għall-istrutturi ġerarkiċi tal-proċess huma limitati. F’dan ix-xogħol, investigaw matematikament is-saħħa tal-komputazzjoni tal-awtonomija għal lingwi formali mudell. Minbarra kemm attenzjoni ratba kif ukoll iebsa, nuru limitazzjonijiet teoretiċi qawwija tal-kapaċitajiet komputattivi tal-awtonomija, u nsibu li ma tistax timmudella lingwi perjodiċi fi stat finit, u lanqas struttura ġerarkika, sakemm in-numru ta’ saffi jew irjus ma jiżdiedx bit-tul tal-input. Dawn il-limitazzjonijiet jidhru sorprendenti minħabba s-suċċess prattiku tal-awtonomija u r-rwol prominenti assenjat lill-istruttura ġerarkika fil-lingwistika, li jissuġġerixxu li l-lingwa naturali tista’ tiġi approssimatizzata tajjeb ma’ mudelli li huma dgħajfa wisq għall-lingwi formali li tipikament huma assunti fil-lingwistika teoretika.</abstract_mt>
      <abstract_pl>Transformatory stają się nowym koniem roboczym NLP, który odnosi ogromny sukces w różnych zadaniach. W przeciwieństwie do LSTMów transformatory przetwarzają sekwencje wejściowe całkowicie poprzez samą uwagę. Wcześniejsze prace sugerują, że możliwości obliczeniowe samodzielnej uwagi na struktury hierarchiczne procesów są ograniczone. W niniejszej pracy matematycznie badamy moc obliczeniową własnej uwagi na modelowe języki formalne. Wśród miękkiej i twardej uwagi pokazujemy silne teoretyczne ograniczenia obliczeniowe zdolności samoobserwacji, stwierdzając, że nie może ona modelować okresowych języków skończonych, ani struktury hierarchicznej, chyba że liczba warstw lub głów zwiększa się wraz z długością wejścia. Ograniczenia te wydają się zaskakujące biorąc pod uwagę praktyczny sukces uwagi na siebie oraz znaczącą rolę przypisywaną strukturze hierarchicznej w językoznawstwie, sugerując, że język naturalny można dobrze przybliżyć do modeli zbyt słabych dla języków formalnych typowo przyjmowanych w językoznawstwie teoretycznym.</abstract_pl>
      <abstract_sr>Transformeri se pojavljuju kao novi radni konj NLP-a, pokazujući veliki uspeh preko zadataka. Za razliku od LSTMs, transformatori procesiraju ulazne sekvence potpuno kroz samopouzdanje. Prethodni rad je predložio da su računalne sposobnosti samopouzdanja na proces hijerarhijske strukture ograničene. U ovom poslu, matematički istražujemo računalnu moć samopouzdanja na model formalnih jezika. Preko meke i teške pažnje pokazujemo jake teorijske ograničenje računalnih sposobnosti samopouzdanja, otkrivajući da ne može modelirati periodične krajnje države jezike, niti hijerarhijske strukture, osim ako broj slojeva ili glava ne povećava dužinu ulaza. Ove ograničenja izgledaju iznenađujuće s obzirom na praktični uspeh samopouzdanja i značajnu ulogu koja je dodijeljena hijerarhičkoj strukturi jezika, ukazujući na to da prirodni jezik može biti približen dobro sa modelima koji su previše slabi za formalne jezike koje se obično pretpostavljaju teoretičkom jeziku.</abstract_sr>
      <abstract_ro>Transformatorii apar ca noul cal de lucru al PNL, demonstrând un mare succes în toate sarcinile. Spre deosebire de LSTMs, transformatoarele procesează secvențele de intrare în întregime prin auto-atenție. Lucrările anterioare au sugerat că capacitățile computaționale de auto-atenție la structurile ierarhice ale proceselor sunt limitate. În această lucrare, investigăm matematic puterea computațională a auto-atenției la modelarea limbajelor formale. Atât prin atenție ușoară, cât și prin atenție grea, arătăm limitări teoretice puternice ale abilităților computaționale de auto-atenție, constatând că nu poate modela limbaje periodice cu stări finite, nici structura ierarhică, decât dacă numărul de straturi sau capete crește odată cu lungimea intrării. Aceste limitări par surprinzătoare având în vedere succesul practic al auto-atenției și rolul proeminent atribuit structurii ierarhice în lingvistică, sugerând că limbajul natural poate fi aproximat bine cu modele prea slabe pentru limbajele formale presupuse în mod obișnuit în lingvistica teoretică.</abstract_ro>
      <abstract_mn>Трансформаторууд NLP-ийн шинэ ажлын морь болж явж, ажил дахь гайхалтай амжилтыг харуулж байна. LSTMs шиг өөрчлөгчид өөртөө анхаарал хангалттай оролцоог шилжүүлдэг. Өмнөх ажлын ажил нь өөрийгөө анхаарлын үйл ажиллагаанд тооцоолох чадвар хязгаарлагддаг. Энэ ажлын хувьд бид математикийн хувьд өөрийгөө анхаарлын тооцооллын эрх мэдлийг официальн хэл загварын загварын загварын тулд судалж байна. Бид өөрсдийгөө анхаарлын тооцоолох чадварын теоретик хязгаарлалтыг харуулж байна. Энэ нь цаг хугацааны төгсгөл хэл болон төгсгөл бүтцийг загварчлах боломжгүй гэдгийг олж мэдсэн. Эдгээр хязгаар нь өөрийгөө анхаарлын амжилтын тулд, хэл хэлний хичээлийн бүтээгдэхүүнд зориулагдсан хамгийн чухал үүрэг нь гайхалтай мэт санагдаж байна. Байгалийн хэл нь теоретик хэлний хэлний хувьд ихэвчлэн хэлбэрт бага хэмжээний загваруудын тулд</abstract_mn>
      <abstract_si>NLP වල අලුත් වැඩ අශ්වයෙක් විදියට වෙනස් කරනවා, වැඩේ සැලසුම් වෙනුවෙන් ලොකු සාර්ථක පෙන්වනවා. LSTMs වගේ වෙනුවෙන්, ස්වයං අවධානයෙන් ප්‍රමාණය කරනවා ඇතුළත් අවධානයෙන්. මුලින් වැඩේ ප්‍රශ්නයක් තියෙනවා කියලා ස්වයංග්‍රහය අවධානය සඳහා පරීක්ෂණ ක්‍රියාත්මක ක්‍රියාත් මේ වැඩේ අපි ගණිතික ශක්තිය පරීක්ෂා කරන්නේ ස්වාමික අවධානයේ සාමාන්‍ය භාෂාවක් නිර්මාණය කරන්න. සාමාන්‍ය සහ අමාරු අවධානයක් දෙන්නම්, අපි ස්වාමාන්‍ය අවධානයේ පරීක්ෂණ ක්‍රියාත්මක ක්‍රියාත්මක සීමාවක් පෙන්වන්න පුළුවන් කියලා, හොයාගන්නේ  මේ සීමාව පේන විශ්වාසයක් වගේ ස්වයංග්‍රහයේ අවස්ථාවක් සහ භාෂාත්මක වලින් සාමාන්‍ය භාෂාත්මක වලින් ප්‍රභාවිත විශ්වාසයක් තියෙන්න පුළුවන් ව</abstract_si>
      <abstract_so>Transformers are emerging as the new workhorse of NLP, showing great success across tasks.  Isku duwan LSTMs, iskutallaabta soo beddelashada jidhka input-soo-gelinta si kamid ah u soo jeeda iskuulka. Shaqo hore wuxuu soo jeeday in awoodda xisaabta ah ee iskuul-xisaabinta ay sameynayso dhismaha hierarkiisa. Markaas waxan, waxaynu xisaab ku baaraynaa xoogga xisaabta ee iskuulka ah oo u qoran luuqadaha rasmiga ah. Dhaqdhaqaaq iyo qallafsan, waxaynu muujinnaa xadiiqada cilmiga ah ee awoodda xisaabta naftiisa, waxaynu ogaanaynaa inuusan sameyn karin luuqadaha xilliga ah oo dowladda, ama dhismaha hierarkiisa, haddii aan tirada qasnada ama madaxu kordhiso dhererka input. Xuduudahan waxaa la yaabaa in la yaabo suurtagalka iskuulka isboorsashada iyo qaybta caadiga ah ee loo qaybiyey dhismaha afka hierarkiisa, taas oo ka jeeda in luqada dabiiciga ah lagu koobi karo samooyin aad u itaal yar ee luuqadaha rasmiga ah sida caadiga ah loogu sameeyo afka theoretical.</abstract_so>
      <abstract_sv>Transformatorer växer fram som den nya arbetshästen för NLP, som visar stor framgång över olika uppgifter. Till skillnad från LSTM bearbetar transformatorer ingångssekvenser helt genom självuppmärksamhet. Tidigare arbete har föreslagit att beräkningsförmågan för självuppmärksamheten på processhierarkiska strukturer är begränsad. I detta arbete undersöker vi matematiskt självuppmärksamhetens beräkningskraft för modellering av formella språk. Genom både mjuk och hård uppmärksamhet visar vi starka teoretiska begränsningar av självuppmärksamhetens beräkningsförmåga och finner att det inte kan modellera periodiska ändliga tillståndsspråk, eller hierarkisk struktur, om inte antalet lager eller huvuden ökar med inmatningens längd. Dessa begränsningar förefaller förvånande med tanke på den praktiska framgången av självuppmärksamhet och den framträdande roll som tilldelats hierarkisk struktur i lingvistiken, vilket tyder på att naturligt språk kan närmas väl med modeller som är för svaga för de formella språk som vanligtvis antas inom teoretisk lingvistik.</abstract_sv>
      <abstract_ur>تبدیل کرنے والے NLP کی نوی کارگھوڑ کی طرح ظاہر ہوتے ہیں، کاموں میں بہت بڑی کامیابی دکھاتے ہیں. LSTMs کے بغیر، تغییر پھیلانے والے اپنے آپ کی توجه کے ذریعہ پوری طرح اپنا اپنا اپنا اپنا اپنا اپنا اپنا اپنا اپنا سفارش کر رہے ہیں. اگلے کام نے سفارش کی ہے کہ اپنے آپ کو سمجھنے کے لئے کمپیوٹریشن قابلیت محدود ہیں. اس کام میں ہم ریاضیکی طور پر اپنے آپ کی توجه کی کمپیوٹی طاقت کی مدل رسمی زبانوں پر تحقیق کرتے ہیں. ہم نرم اور سخت توجه کے درمیان مضبوط نظریہ محدودیت کو دکھاتے ہیں جو اپنے آپ کی کمپیوٹریسی قابلیت کی محدودیت کرتی ہیں، اس کو معلوم ہوتا ہے کہ یہ موجود مقررہ وطن کی زبانوں کی مدل نہیں کر سکتا، اور نہ سطح یا سروں کی تعداد کے مطابق اضافہ ہوتی ہے. یہ محدودیت تعجب کرنے والی نظر آتی ہیں کہ اپنے آپ کی توجه کے قابل کامیابی اور زبان شناسی کی ساختاری کے لئے مقرر کیا گیا ہے، اور یہ معلوم ہے کہ طبیعی زبان کی مدلکوں کے ساتھ بہت زیادہ کمزور ہو سکتی ہے جو معمولاً نظریہ زبان شناسی میں معلوم ہوتے ہیں۔</abstract_ur>
      <abstract_ta>மாற்றுபவர்கள் NLP இன் புதிய வேலைகுதிரையாக வருகிறார்கள், பணிகள் முழுவதும் பெரிய வெற்றியை காட்டுகிறது. LSTMs மாற்றங்கள் முழுமையாக உள்ளீட்டு வரிசைகள் முழுமையாக தானே கவனத்தை மூலம் மாற்றும். முந்தைய வேலை பரிந்துரைக்கப்பட்டுள்ளது அந்த கணக்கீட்டு தன்னை கவனத்தை செயல்படுத்துவதற்கு கட்டுப்படுத்தல் தான். இந்த வேலையில், நாம் கணிதத்தில் வடிவமைப்பு மொழிகளுக்கு தன்னை கவனத்திற்கான கணிதமான சக்தியை கணிக்க வேண்டும். மென்னிருப்பு மற்றும் கடினமான கவனத்திற்கு மேலும், நாம் தன்னுடைய கணிப்பான திடீரியல் எல்லைகளைக் காட்டுகிறோம். இது குறிப்பிட்ட நிலையான மொழிகளை மாற்ற முடியா இந்த எல்லைகள் ஆச்சரியமாக இருக்கும் என்பதால் தானே கவனத்தின் செயல்பாடு வெற்றி மற்றும் மொழிகளில் உயர்ந்த மொழிகளுக்கு பங்கிடப்பட்ட பெரிய பங்கு, இயற்கையான மொழி சுருக்கமாக மா</abstract_ta>
      <abstract_uz>Vazifalar vazifalar bilan juda ajoyib muvaffaqiyatli ko'rsatadi. @ info: whatsthis Oldingi vazifa esa, hierarchik strukturalarini jarayonligiga o'zimni o'zimga hisoblash imkoniyatini beradi. Bu ishda, matematika o'rganimiz, rasmlar tilga o'zining o'zimni o'zimni o'rganish maktabini o'rganamiz. Softa va qiyin murakkablik bilan biz o'zimni o'zimga hisoblash qobiliyatlarining katta teoretikal chegaralarini ko'rsamiz. Bu o'sha narsa davlat tillarini ko'paytirish mumkin, balки qatorlar yoki boshqalar soni ko'payishi mumkin. Bu chegaralar o'z o'zining o'zingizga muvaffaqiyatli va tillarda hierarchik tizimi yaratilgan muvaffaqiyatga qaramaydi, bu cheksiz tillar o'zgarishga juda yaxshi bo'lishi mumkin, o'ylaymaydi, asl tili teoretikal tillarda o'zgartirilgan modellar uchun juda yomon yo'q.</abstract_uz>
      <abstract_vi>Các robot biến hình đang phát triển thành ngựa làm việc mới của đài khôn ngoan. Không giống với LSTMs, máy biến đổi xử lý dãy nhập hoàn toàn nhờ chính mình. Những nghiên cứu trước cho thấy khả năng tính to án của bản thân chú ý đến cấu trúc cấp độ quy trình đã bị hạn chế. Trong công việc này, chúng tôi nghiên cứu to án học về sức mạnh tính của tự chú ý vào các ngôn ngữ chính thức. Bên cạnh sự chú ý mềm mại và mạnh mẽ, chúng tôi cho thấy những giới hạn lý thuyết mạnh mẽ của khả năng tính tập trung bản thân, tìm ra rằng nó không thể mô tả các ngôn ngữ xác định giới hạn, hay cấu trúc cấp dưới, trừ khi số lượng các lớp hoặc đầu tăng theo chiều dài nhập. Những giới hạn này có vẻ đáng ngạc nhiên vì khả năng tự trọng của mình và vai trò quan trọng gắn liền với hệ thống ngôn ngữ phân cấp, gợi ý rằng ngôn ngữ tự nhiên có thể được tương ứng tốt với các mô hình quá yếu cho ngôn ngữ văn học lý thuyết.</abstract_vi>
      <abstract_bg>Трансформаторите се появяват като нов работен кон на НЛО, показвайки голям успех в различните задачи. За разлика от ЛСТМ трансформаторите обработват входните последователности изцяло чрез самовнимание. Предишна работа предполага, че изчислителните възможности за самовнимание към процесните йерархични структури са ограничени. В тази работа математически изследваме изчислителната сила на самовниманието към моделните формални езици. При мекото и твърдото внимание показваме силни теоретични ограничения на изчислителните способности на самовниманието, като откриваме, че не може да моделира периодични езици с крайни състояния, нито йерархична структура, освен ако броят на слоевете или главите не се увеличава с дължината на входа. Тези ограничения изглеждат изненадващи предвид практическия успех на самовниманието и важната роля, възложена на йерархичната структура в лингвистиката, което предполага, че естественият език може да бъде прилаган добре с модели, които са твърде слаби за формалните езици, обикновено възприемани в теоретичната лингвистика.</abstract_bg>
      <abstract_hr>Transformeri se pojavljuju kao novi radni konj NLP-a, pokazujući veliki uspjeh u svim zadacima. Za razliku od LSTMs, transformatori procesiraju ulazne sekvence potpuno kroz samopouzdanje. Prethodni rad je predložio da su računalne sposobnosti samopouzdanja na proces hijerarhijske strukture ograničene. U ovom poslu, matematički istražujemo računalnu moć samopouzdanja na model formalnih jezika. Preko meke i teške pažnje pokazujemo jake teorijske ograničenje računalnih sposobnosti samopouzdanja, otkrivajući da ne može modelirati periodične ograničene jezike i hijerarhijske strukture, osim ako broj slojeva ili glava ne povećava s dužinom ulaza. Ove ograničenja izgledaju iznenađujuće s obzirom na praktični uspjeh samopouzdanja i značajnu ulogu koja je dodijeljena hijerarhičkoj strukturi jezika, što ukazuje na to da prirodni jezik može biti približen dobro sa modelima koji su previše slabi za formalne jezike obično pretpostavljene u teorijskom jeziku.</abstract_hr>
      <abstract_da>Transformere er ved at dukke op som den nye arbejdshest i NLP, der viser stor succes på tværs af opgaver. I modsætning til LSTMs behandler transformatorer input sekvenser udelukkende gennem selvopmærksomhed. Tidligere arbejde har antydet, at de beregningsmæssige muligheder for selvopmærksomhed på proces hierarkiske strukturer er begrænsede. I dette arbejde undersøger vi matematisk selvopmærksomhedens beregningskraft til modelsprog. På tværs af både blød og hård opmærksomhed viser vi stærke teoretiske begrænsninger af selvopmærksomhedens beregningsmæssige evner, idet vi konstaterer, at det ikke kan modellere periodiske finite-state sprog eller hierarkisk struktur, medmindre antallet af lag eller hoveder stiger med input længde. Disse begrænsninger synes overraskende i betragtning af selvopmærksomhedens praktiske succes og den fremtrædende rolle, der tildeles hierarkisk struktur i lingvistik, hvilket tyder på, at naturligt sprog kan tilnærmes godt med modeller, der er for svage til de formelle sprog, der typisk antages i teoretisk lingvistik.</abstract_da>
      <abstract_nl>Transformers komen naar voren als het nieuwe werkpaard van NLP, die grote successen laten zien in alle taken. In tegenstelling tot LSTMs verwerken transformatoren invoersequenties volledig door zelfaandacht. Eerder onderzoek heeft gesuggereerd dat de rekenmogelijkheden van zelfaandacht voor proceshiërarchische structuren beperkt zijn. In dit werk onderzoeken we wiskundig de rekenkracht van zelfaandacht voor formele modeltalen. In zowel zachte als harde aandacht tonen we sterke theoretische beperkingen van de rekenmogelijkheden van zelfaandacht, waarbij we vaststellen dat het geen periodieke eindige-state talen kan modelleren, noch hiërarchische structuur, tenzij het aantal lagen of hoofden toeneemt met invoerlengte. Deze beperkingen lijken verrassend gezien het praktische succes van zelfaandacht en de prominente rol die wordt toegekend aan hiërarchische structuur in de linguïstiek, wat suggereert dat natuurlijke taal goed kan worden benaderd met modellen die te zwak zijn voor de formele talen die typisch worden aangenomen in de theoretische linguïstiek.</abstract_nl>
      <abstract_id>Transformers muncul sebagai kuda kerja baru NLP, menunjukkan sukses besar di seluruh tugas. Tidak seperti LSTM, transformer memproses urutan input sepenuhnya melalui perhatian diri. Pekerjaan sebelumnya menyarankan bahwa kemampuan komputasi perhatian diri untuk proses struktur hierarkis terbatas. Dalam pekerjaan ini, kami secara matematis menyelidiki kekuatan perhitungan perhatian diri kepada bahasa formal model. Melalui perhatian lembut dan keras, kita menunjukkan batasan teori yang kuat dari kemampuan komputasi perhatian diri, menemukan bahwa ia tidak dapat model bahasa periodik keadaan-batas, atau struktur hierarkis, kecuali jumlah lapisan atau kepala meningkat dengan panjang masukan. Pembatasan ini tampaknya mengejutkan karena sukses praktis perhatian diri dan peran terkemuka yang ditugaskan untuk struktur hierarkis dalam bahasa, yang menunjukkan bahasa alam dapat mendekati dengan baik dengan model yang terlalu lemah untuk bahasa formal biasanya dianggap dalam bahasa teori.</abstract_id>
      <abstract_de>Transformatoren sind das neue Arbeitspferd von NLP und zeigen große Erfolge bei allen Aufgaben. Im Gegensatz zu LSTMs verarbeiten Transformatoren Eingangssequenzen ausschließlich durch Selbstachtung. Bisherige Arbeiten haben gezeigt, dass die Rechenleistungen der Selbstaufmerksamkeit auf hierarchische Prozessstrukturen begrenzt sind. In dieser Arbeit untersuchen wir mathematisch die Rechenleistung der Selbstaufmerksamkeit auf Modellformalsprachen. Sowohl bei weicher als auch bei harter Aufmerksamkeit zeigen wir starke theoretische Einschränkungen der Rechenfähigkeit der Selbstaufmerksamkeit, wobei festgestellt wird, dass sie weder periodische finite-state Sprachen noch hierarchische Strukturen modellieren kann, es sei denn, die Anzahl der Schichten oder Köpfe steigt mit der Eingabelänge. Diese Einschränkungen scheinen angesichts des praktischen Erfolgs der Selbstaufmerksamkeit und der herausragenden Rolle, die der hierarchischen Struktur in der Linguistik zugewiesen wird, überraschend, was darauf hindeutet, dass natürliche Sprache gut mit Modellen angenähert werden kann, die für die formalen Sprachen, die in der theoretischen Linguistik typischerweise angenommen werden, zu schwach sind.</abstract_de>
      <abstract_fa>تغییر دهندگان به عنوان اسب کاری جدید NLP روشن می شوند، که موفقیت بزرگی را در سر کار نشان می دهند. برخلاف LSTMs، تغییر دهنده‌ها از طریق توجه خودشان به طور کامل ردیابی ورودی را فرایند می‌کنند. کار قبلی پیشنهاد کرده است که توانایی محاسبات توجه به ساختارهای مختلف محدودیت شده است. در این کار، ما به ریاضی قدرت کامپیوتری خود را به مدل زبان رسمی تحقیق می کنیم. از طریق توجه نرم و سخت، محدودیت نظریه‌ای قوی از توانایی محاسبات توجه خود را نشان می‌دهیم، و پیدا می‌کنیم که نمی‌تواند زبان‌های محدودیت محدودیت محدودیت را مدل کند، و نه ساختار محدودیت، مگر اینکه تعداد لایه‌ها یا سرها با طول ورودی افزایش کند. این محدودیت به نظر تعجب کننده می‌شود با توجه به موفقیت عملی توجه خود و نقش بزرگی که به ساختار شیراریکی در زبان‌شناسی وابسته می‌شود، پیشنهاد می‌دهد که زبان طبیعی می‌تواند با مدل‌های زیادی ضعیف باشد که برای زبان‌های رسمی معمولاً در زبان‌شناسی تئوری می</abstract_fa>
      <abstract_tr>Transformat챌ylar NLP t채ze i힊aty bolup g철r체n첵채r, i힊i흫 체st체nde 철r채n ba힊arnygy g철rke첵채rler. LSTMs 첵aly, terjime edip giri힊 dizirleri 철z체ne 체ns berip i힊le첵채r. 횜흫ki i힊i흫 hijerarhi첵a d체z체mlerni흫 철z-철z체ne 체ns bermegi 체챌in hesaplamak ukyplary 챌arpndyryldy. Bu i힊de, biz matematiksel olarak 철z체ni흫 체ns체ni formal dillere g철r채 hesaplamak g체c체ni barla첵arys. Hem yumu힊y hem kyn 체ns beril첵채ris, biz 철z체ni흫 체ns체ni흫 kalkulary흫 g체첵챌li teori첵aly 챌yky힊laryny g철rke첵채ris we munu흫 periodi첵a be첵ik durmu힊 dillerini, hem i첵erarhi첵a struktury흫 sany 첵a kell채흫 giri힊 uzunlygy bilen k철pr채k bolmasa di첵ip kabul edip bilme첵채ris. Bu 챌yky힊lar 철z-철z체ne 체ns bermegi we lingwistiklerde hiyerar힊ik strukturyna berilen t채sirli t채sirli bolup g철r체n첵채r, tebigy dil formal diller 체챌in 철r채n zay캇f di첵ip kabul edil첵채n 철r채n t채sirli bir nusga g철r첵채r.</abstract_tr>
      <abstract_sw>Watafsiri wanajitokeza kama farasi mpya ya NLP, wakionyesha mafanikio makubwa katika kazi. Tofauti na LSTMs, mchakato wa mabadiliko utaratibu wa input kwa ujumla kupitia kujitazama. Kazi zilizopita imependekeza kwamba uwezo wa kompyuta wa kujihisia katika kuchukua miundombinu ya ubunifu ni mdogo. Katika kazi hii, tunachunguza nguvu ya kompyuta ya kujitazama katika lugha rasmi. Katika hali ngumu na ngumu, tunaonyesha vizuizi vikali vya nadharia ya uwezo wa kompyuta wa kujihisabu, kwa kutambua kwamba haiwezi kuunda lugha za kiserikali za kawaida, wala muundo wa kiunde, ikiwa ni pamoja na idadi ya vipeperushi au vichwa vinaongezeka kwa kiwango cha input. Uzuizi huu unaonekana kushangaza kufuatia mafanikio ya msingi wa kujitegemea na jukumu maarufu lililowekwa kwenye muundo wa miundombi katika lugha za lugha, ikipendekeza kuwa lugha ya asili inaweza kupatikana vizuri na mifano ambayo ni dhaifu sana kwa lugha rasmi zilizochukuliwa kawaida katika lugha za kitaaluma.</abstract_sw>
      <abstract_sq>Transformuesit po shfaqen si kali i ri i punës i NLP, duke treguar sukses të madh nëpërmjet detyrave. Ndryshe nga LSTMs, transformuesit procesojnë sekuencat e hyrjes tërësisht nëpërmjet vetë-vëmendjes. Puna e mëparshme ka sugjeruar se aftësitë kompjuterike të vetëvëmendjes ndaj strukturave hierarkike të procesit janë të kufizuara. Në këtë punë, ne matematikisht hetojmë fuqinë kompjuterike të vetëvëmendjes ndaj gjuhëve zyrtare. Përmes vëmendjes së butë dhe të fortë, ne tregojmë kufizime të forta teorike të aftësive llogaritare të vetë-vëmendjes, duke gjetur se ajo nuk mund të modelojë gjuhë periodike të shtetit të kufizuar, as strukturë hierarkike, përveç nëse numri i shtresave apo kokave rritet me gjatësinë e hyrjes. Këto kufizime duken befasuese duke patur parasysh suksesin praktik të vetëvëmendjes dhe rolin e shquar të caktuar strukturës hierarkike në gjuhën gjuhësore, duke sugjeruar se gjuha natyrore mund të përafërtohet mirë me modele që janë shumë të dobëta për gjuhët zyrtare tipikisht të pranuara në gjuhën teorike.</abstract_sq>
      <abstract_am>ትርጓሚዎች በአዲስ አዲስ የNLP ፈረስ እያወጡ በሥራው ላይ ታላቅ ድል አግኝተዋል፡፡ በተለየ LSTMs፣ ለውጦች ፕሮጀክት የድምፅ ውጤቶች በሙሉ ራሳቸውን በመጠየቅ ይደረጋሉ፡፡ የቀድሞው ሥራ የራሳቸውን አካባቢነት የሥልጣን ሥርዓት ለመሥራት ግንኙነት ነው፡፡ በዚህ ሥራ፣ የራሳችንን የሥልጣን አካባቢ የፊደል ቋንቋዎች እናሳውቃለን፡፡ Across both soft and hard attention, we show strong theoretical limitations of the computational abilities of self-attention, finding that it cannot model periodic finite-state languages, nor hierarchical structure, unless the number of layers or heads increases with input length.  እነዚህ ግንኙነት የራሳቸውን አካባቢ ስኬት እና ለቋንቋ ቋንቋዎች የተደረገው ትክክለኛ ክፍል፣ የፍጥረቱ ቋንቋ በአካባቢው ቋንቋዎች ላይ በጣም ደካማ ለመሆን ይችላል፡፡</abstract_am>
      <abstract_af>Transformeerders word opgekom as die nuwe werksperd van NLP, wat groot sukses oor werke vertoon. Ongelyks van LSTMs, transformeerders proses invoer sekwensies heeltemal deur self-aandag. Vorige werk het voorgestel dat die rekenaarske kapasiteite van self-aandag na proses hierarkies strukture beperk is. In hierdie werk, ons wiskundig ondersoek die rekenaar krag van self-aandag na model formele tale. Binne sagte en moeilike aandag, wys ons sterke teorieese beperkings van die rekenaasjonale moontlikhede van self-aandag, en vind dat dit nie periodieke beperking-staat tale of hierarkies struktuur kan model nie, tensy die aantal lage of koppe met invoer lengte vergroot word nie. Hierdie beperkinge lyk verwonderbaar gegee het die praktiese sukses van self-aandag en die prominente rol wat aan hierarkiese struktuur in lingwistike toegewys is, wat voorstel dat natuurlike taal goed kan aangewys word met modele wat te swak is vir die formele tale tipies in teorieese lingwistike aangeneem word.</abstract_af>
      <abstract_hy>Փոփոխակերպերը զարգանում են որպես ՆԼՊ-ի նոր աշխատաձի, որը մեծ հաջողություն է ցույց տալիս խնդիրների ընթացքում: Ի տարբերություն LSMT-ներին, վերափոխողները պրոցեսում են ներմուծման հաջորդականությունները ամբողջովին ինքնաուշադրության միջոցով: Նախորդ աշխատանքը առաջարկեց, որ ինքնաուշադրության հաշվարկների հնարավորությունները գործընթացի հիերարխիկ կառուցվածքների վրա սահմանափակված են: Այս աշխատանքի ընթացքում մենք մաթեմատիկապես ուսումնասիրում ենք ինքնաուշադրության հաշվարկների ուժը պաշտոնական լեզուների մոդելների վրա: Եթե ուշադրություն դարձնենք, ապա մենք ցույց ենք տալիս ինքնաուշադրության հաշվարկների հզոր տեսական սահմանափակումներ, որոնք ցույց են տալիս, որ այն չի կարող օրինակել պարբերական սահմանափակ վիճակում գտնվող լեզուներ, կամ հիերարխիկ կառուցվածք, եթե շերտերի կամ գլխավորների թիվը չբարձ Այս սահմանափակումները զարմանալի են թվում, հաշվի առնելով ինքնաուշադրության պրակտիկ հաջողությունը և լեզվաբանության հիերարխիկ կառուցվածքի նշանակալի դերը, որը առաջարկում է, որ բնական լեզուն կարող է լավ մոտենալ այնպիսի մոդելների հետ, որոնք չափազանց թույլ են պաշտոնական</abstract_hy>
      <abstract_az>Transformers NLP'in yeni işatı kimi ortaya çıxarır, işlərdə böyük başarılı göstərir. LSTMs kimi, özünün dikkatini ilə transformatçılar giriş sıralarını tamamlayır. Əvvəlki işin hiyerarşik yapıları işlədirmək üçün özünün təsirlərinin hesablama qabiliyyətlərinin sınırlı olduğunu göstərdi. Bu işdə, biz matematiksel olaraq özümüzün ünsiyyətini formal dillərin modelinin hesablama gücünü incidirik. İkisində də yumuşaq və ağır dikkati, özünün dikkatini hesablayan təriqətli qabiliyyətlərin çox möhkəm təriqətli sınırlarını göstərdik, periodik təriqətli dillərin və hiyerarşik quruluşlarının modellərini göstərməyəcəyini öyrəndik. Lakin səviyyənin və başların giriş uzunluğu ilə artır Bu limitlərin özünün təsirlərinin praktik başarısından və dillərin hiyerarşik strukturlarına verilən ən böyük rolünün təəccüblənməsi təəccüblü görünür ki, təbiətli dil teoriqli dillərdə çox zəif olan modellərlə yaxınlaşdırılabilir.</abstract_az>
      <abstract_bn>ট্রান্সফর্মাররা এনএলপির নতুন কার্যঘোড় হিসেবে উদ্ভাবন করছে, যারা কাজের বিভিন্ন সাফল্য দেখাচ্ছে। এলস্টিএমএস-এর অন্যান্য ভিন্ন ভিন্ন ভিন্ন ভিন্ন ভিন্ন ভিন্ন ভিন্ন মনোযোগের মাধ্যমে বিনিময়ে প্রক্ পূর্ববর্তী কাজ পরামর্শ প্রদান করেছে যে হিয়েরার্কিকাল কাঠামো প্রক্রিয়ার জন্য আত্মমনোযোগের সংক্রান্ত ক্ষমত এই কাজে আমরা গণতান্ত্রিকভাবে গণতান্ত্রিকভাবে তদন্ত করি নিজেদের আত্মমনোযোগের ক্ষমতা গণনা করা হয়েছে ফর্মি কঠিন এবং কঠিন মনোযোগ দিয়ে আমরা স্বয়ংক্রিয়ভাবে সংক্রান্ত ততিত্ত্বিক সীমাবদ্ধতা দেখাচ্ছি যে এটি নিয়মিত সংখ্যা রাষ্ট্রীয় ভাষা মডেল করতে পারে না, আর হিরেরাক এই সীমাবদ্ধ মনোযোগের ব্যাপারে বিস্ময়কর মনোযোগ এবং ভাষায় হিয়েরার্কিক কাঠামোর জন্য বিশাল ভূমিকা দায়িত্ব দেয়া হয়েছে, তার পরামর্শ দেয়া হচ্ছে যে প্রাকৃতিক ভাষার প্রাকৃত</abstract_bn>
      <abstract_ca>Els transformadors estan emergint com el nou cabell de treball de NLP, mostrant un gran èxit a través de les tasques. A diferència dels LSTMs, els transformadors processen seqüències d'entrada completament a través de l'autoatenció. La feina anterior ha suggerit que les capacitats computacionals de l'autoatenció a les estructures jeràrquiques de procés són limitades. En aquest treball, matemàticament investigam el poder computacional de l'autoatenció a les llengües formals modelades. Across both soft and hard attention, we show strong theoretical limitations of the computational abilities of self-attention, finding that it cannot model periodic finite-state languages, nor hierarchical structure, unless the number of layers or heads increases with input length.  These limitations seem surprising given the practical success of self-attention and the prominent role assigned to hierarchical structure in linguistics, suggesting that natural language can be approximated well with models that are too weak for the formal languages typically assumed in theoretical linguistics.</abstract_ca>
      <abstract_ko>'트랜스포머'는 NLP의 새로운 주축이 되고 있으며 각종 임무에서 큰 성공을 거두고 있다.LSTM과 달리 transformers는 입력 서열을 자기 관심으로 처리합니다.이전의 연구에 의하면 자기 관심 과정의 차원 구조의 계산 능력은 유한한 것으로 나타났다.이 작업에서 우리는 수학적 측면에서 모델 형식 언어에 대한 계산 능력을 연구했다.소프트 주의와 하드 주의에서 우리는 자기주의 계산 능력의 강력한 이론적 한계를 나타냈다. 이는 주기적인 유한한 상태 언어를 모의할 수 없고 차원 구조도 모의할 수 없다. 층이나 머리의 수량이 입력 길이가 증가함에 따라 증가하지 않는 한.자기 관심의 실제 성공과 언어학에서 등급 구조의 두드러진 작용을 감안하면 이러한 제한은 놀랄 만하다. 이는 자연언어가 이론언어학에서 일반적으로 가설하는 형식언어가 너무 약한 모델과 비슷하게 접근할 수 있음을 나타낸다.</abstract_ko>
      <abstract_cs>Transformátory se objevují jako nový pracovní kůň NLP, který ukazuje velký úspěch napříč úkoly. Na rozdíl od LSTMů transformátory zpracovávají vstupní sekvence výhradně prostřednictvím vlastní pozornosti. Předchozí práce naznačuje, že výpočetní schopnosti sebepozornosti na hierarchické struktury procesů jsou omezené. V této práci matematicky zkoumáme výpočetní sílu sebepozornosti k modelovým formálním jazykům. Napříč měkkou i tvrdou pozorností ukazujeme silná teoretická omezení výpočetních schopností sebepozornosti a zjišťujeme, že nemůže modelovat periodické jazyky konečných stavů, ani hierarchickou strukturu, pokud se počet vrstev nebo hlav nezvyšuje s délkou vstupu. Tato omezení se jeví překvapivě vzhledem k praktickému úspěchu sebepozornosti a významné roli hierarchické struktuře v lingvistice, což naznačuje, že přirozený jazyk lze dobře aproximovat s modely, které jsou příliš slabé pro formální jazyky typicky předpokládané v teoretické lingvistice.</abstract_cs>
      <abstract_bs>Transformeri se pojavljuju kao novi radni konj NLP-a, pokazujući veliki uspjeh preko zadataka. Za razliku od LSTMs, transformatori procesiraju ulazne sekvence potpuno kroz samopouzdanje. Prethodni rad je predložio da su računalne sposobnosti samopouzdanja na proces hijerarhijske strukture ograničene. U ovom poslu, matematički istražujemo računalnu moć samopouzdanja na model formalnih jezika. Preko meke i teške pažnje pokazujemo jake teorijske ograničenje računalnih sposobnosti samopouzdanja, otkrivajući da ne može modelirati periodične ograničene jezike i hijerarhijske strukture, osim ako broj slojeva ili glava ne povećava dužinu ulaza. Ove ograničenja izgledaju iznenađujuće s obzirom na praktični uspjeh samopouzdanja i značajnu ulogu koja je dodijeljena hijerarhičkoj strukturi na jeziku, ukazujući na to da prirodni jezik može biti približen dobro sa modelima koji su previše slabi za formalne jezike koje se obično pretpostavljaju teoretičkom jeziku.</abstract_bs>
      <abstract_et>Transformerid on kujunemas uue tööprogrammi uueks tööhobuseks, näidates ülesannete lõikes suurt edu. Erinevalt LSTMdest töötlevad trafod sisendjärjestusi täielikult enesetähelepanu kaudu. Varasemad tööd on näidanud, et eneseahelepanu arvutusvõimekus protsessi hierarhilistele struktuuridele on piiratud. Käesolevas töös uurime matemaatiliselt enesetähelepanu arvutusjõudu formaalsetele mudelkeeltele. Nii pehme kui ka kõva tähelepanu ulatuses näitame enesetähelepanu arvutusvõimete tugevaid teoreetilisi piiranguid, leides, et see ei saa modelleerida perioodilisi piiratud oleku keeli ega hierarhilist struktuuri, kui kihtide või peade arv sisendi pikkusega ei suurene. Need piirangud tunduvad üllatavad, arvestades isetähelepanu praktilist edu ja hierarhilise struktuuri silmapaistvat rolli lingvistikas, mis viitab sellele, et loomulikku keelt saab hästi ühtlustada mudelitega, mis on teoreetilises lingvistikas tavaliselt eeldatavate formaalsete keelte jaoks liiga nõrgad.</abstract_et>
      <abstract_fi>Muuntajat ovat nousemassa NLP:n uudeksi työvuoroksi, mikä osoittaa suurta menestystä eri tehtävissä. Toisin kuin LSTMs, muuntajat prosessoivat syöttöjaksoja täysin itsetunnon kautta. Aiempi työ on ehdottanut, että prosessihierarkisten rakenteiden itsehuomion laskennalliset mahdollisuudet ovat rajalliset. Tässä työssä tutkimme matemaattisesti itsetunnon laskennallista voimaa mallinnuskieliin. Sekä pehmeän että kovan huomion kautta näytämme vahvoja teoreettisia rajoituksia itsetunnon laskennallisille kyvyille, havaiten, että se ei voi mallintaa määräajoin rajatilakieliä eikä hierarkkista rakennetta, ellei kerrosten tai päiden määrä kasva syötteen pituuden myötä. Nämä rajoitukset vaikuttavat yllättäviltä, kun otetaan huomioon itsetunnon käytännön menestys ja hierarkian rakenteen merkittävä rooli kielitieteessä, mikä viittaa siihen, että luonnollista kieltä voidaan hyvin lähentää malleilla, jotka ovat liian heikkoja muodollisille kielille, jotka tyypillisesti oletetaan teoreettisessa kielitieteessä.</abstract_fi>
      <abstract_jv>Transformer sampeyan mbut dumadhi nganggo cah operasi sing gawe NLP, iso nguasah barang pengguna sing gawe barang seneng operasi. UTC Workspace %1% Nang gunggo iki, kéné mataten sakjane nguasakno perusahaan anyar nggawe nguasakno tentang karo ingkang supaya. politenessoffpolite"), and when there is a change ("assertivepoliteness limiting</abstract_jv>
      <abstract_ha>Transformers are emerging as new hesis of NLP, showing babban rabo mai girma a kan aikin. @ info: whatsthis Yin aikin da ya gabãni ya shauri cẽwa, abincin lissafi na bincike wa masu bincike wa aikin matsayin hierrchical ne wanda za'a ƙunsa. A cikin wannan aikin, za mu yi ƙidãya a lissafin zartar da ƙarfin kansa na zama masu bincike zuwa misalin ayuka masu rasmi. Ko cikin masu sauri da ƙwaƙasasshiya, Munã nũna kanana mai ƙarfin teoreoreki na abincin da ya lissafa kansa, kuma tuna cewa bã za ta iya motsar kamar lugha na-state na daidaici ko kuma da tsarin hiirarkiki ba, sai ƙidãyar ƙananan ko huɗu su ƙara da tsawo na inputi. Wannan ƙaddarar za ta yi mãmãki ko da babban rabo na masu fassarar kansa da rabon da aka sanar da shi zuwa matsayin hierrchical cikin linguistic, yana madaidaita cewa za'a karɓi lugha na natura da misãlai masu rauni ko da misãlai masu rauni wa lugha rasmi wanda aka ƙayyade a cikin linguistic na littafiki.</abstract_ha>
      <abstract_sk>Transformatorji se pojavljajo kot novi delovni konj NLP, ki kažejo velik uspeh pri vseh nalogah. Za razliko od LSTMs transformatorjev vhodne sekvence obdelujejo v celoti s samopozornostjo. Predhodno delo je predlagalo, da so računalniške zmogljivosti samopozornosti procesnim hierarhičnim strukturam omejene. V tem delu matematično raziskujemo računalniško moč samopozornosti modelnim formalnim jezikom. Skozi mehko in trdo pozornost kažemo močne teoretične omejitve računalniških sposobnosti samopozornosti, ugotovimo, da ne more modelirati periodičnih jezikov končnih stanj niti hierarhične strukture, razen če se število plasti ali glav z vhodno dolžino poveča. Te omejitve se zdijo presenetljive glede na praktični uspeh samopozornosti in pomembno vlogo hierarhične strukture v jezikoslovju, kar kaže, da je naravni jezik mogoče dobro približati modelom, ki so prešibki za formalne jezike, ki se običajno domnevajo v teoretičnem jezikoslovju.</abstract_sk>
      <abstract_he>המעברים מתגלים כסוס העבודה החדש של NLP, מראה הצלחה גדולה בכל משימות. בניגוד ל-LSTMs, משתנים מעבדים רצפי הכניסה לחלוטין דרך תשומת לב עצמית. העבודה הקודמת הציעה שיכולות החישוב של תשומת לב עצמית לתהליך מבנים היררכיים מוגבלות. בעבודה הזו, אנו חוקרים מתמטית את כוח החישוב של תשומת לב עצמית לשפה רשמית מודל. באמצעות תשומת לב רכה וקשה, אנו מראים מגבלות תיאורטיות חזקות של היכולות החישוביות של תשומת לב עצמית, למצוא שהוא לא יכול לדוגמא שפות תקופיות במצב מוגבל, או מבנה הייררכי, אלא אם מספר שכבות או ראשים מגדלים עם אורך הכניסה. These limitations seem surprising given the practical success of self-attention and the prominent role assigned to hierarchical structure in linguistics, suggesting that natural language can be approximated well with models that are too weak for the formal languages typically assumed in theoretical linguistics.</abstract_he>
      <abstract_bo>རྒྱུན་བཅོས་པ་ཚོས་NLP ཡི་ལས་ཀ་གསར་བ་ཞིག་གིས་མཐོང་བཞིན་པས་ལས་ཀ་སྒྲུབ་མང་པོ་བྱེད་ཀྱི་ཡོད། Unlike LSTMs, transformers process input sequences entirely through self-attention. སྔོན་གྱི་ལས་ཀ་ནི་གྲངས་འབྱོར་གྱི་སྒེར་གྱི་རྩིས་འཁོར་གྱི་ཆ་ཁྱད་ཆོས་ཚོས་རང་ཉིད་ཀྱིས་ལས་སྦྱོར་བའི་དབང་ཆ་ ང་ཚོས་རང་ཉིད་ཀྱི་ལས་ཀ་འདིའི་ནང་གི་གྲངས་རིག་གིས་སྐོར་གྱི་གྲངས་རིག་གིས་རང་ཉིད་ཀྱི་ཆོས་ཉིད་ཀྱི་དཔེ་ Across both soft and hard attention, we show strong theoretical limitations of the computational abilities of self-attention, finding that it cannot model periodic finite-state languages, nor hierarchical structure, unless the number of layers or heads increases with input length. སྐད་ཆ་འདི་དག་རང་ཉིད་ཀྱི་རང་ཉིད་ཡུལ་གྱི་གྲུབ་སྐྱོང་དང་སྐད་རིགས་ནང་གི་དབྱིབས་མཐུན་གྱི་རྩ་བའི་གོ་སྣང་དག་བརྗོད་བསྐྱེད་ཡོད།</abstract_bo>
      </paper>
    <paper id="14">
      <title>Acoustic-Prosodic and Lexical Cues to Deception and Trust : Deciphering How People Detect Lies</title>
      <author><first>Xi (Leslie)</first><last>Chen</last></author>
      <author><first>Sarah Ita</first><last>Levitan</last></author>
      <author><first>Michelle</first><last>Levine</last></author>
      <author><first>Marko</first><last>Mandic</last></author>
      <author><first>Julia</first><last>Hirschberg</last></author>
      <doi>10.1162/tacl_a_00311</doi>
      <abstract>Humans rarely perform better than chance at <a href="https://en.wikipedia.org/wiki/Lie_detection">lie detection</a>. To better understand human perception of deception, we created a game framework, LieCatcher, to collect ratings of perceived deception using a large corpus of deceptive and truthful interviews. We analyzed the acoustic-prosodic and linguistic characteristics of language trusted and mistrusted by raters and compared these to characteristics of actual truthful and deceptive language to understand how perception aligns with reality. With this data we built <a href="https://en.wikipedia.org/wiki/Statistical_classification">classifiers</a> to automatically distinguish <a href="https://en.wikipedia.org/wiki/Trust_(social_science)">trusted</a> from mistrusted speech, achieving an F1 of 66.1 %. We next evaluated whether the strategies raters said they used to discriminate between truthful and deceptive responses were in fact useful. Our results show that, although several prosodic and lexical features were consistently perceived as trustworthy, they were not reliable cues. Also, the <a href="https://en.wikipedia.org/wiki/Strategy">strategies</a> that judges reported using in deception detection were not helpful for the <a href="https://en.wikipedia.org/wiki/Task_(project_management)">task</a>. Our work sheds light on the nature of trusted language and provides insight into the challenging problem of human deception detection.</abstract>
      <pages>199–214</pages>
      <url hash="d0dfd22b">2020.tacl-1.14</url>
      <bibkey>chen-etal-2020-acoustic</bibkey>
    </paper>
    </volume>
</collection>