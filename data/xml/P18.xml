<collection id="P18">
  <volume id="1">
    <meta>
      <booktitle>Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</booktitle>
      <url hash="e268fd13">P18-1</url>
      <editor><first>Iryna</first><last>Gurevych</last></editor>
      <editor><first>Yusuke</first><last>Miyao</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Melbourne, Australia</address>
      <month>July</month>
      <year>2018</year>
    </meta>
    <frontmatter>
      <url hash="de9d99e0">P18-1000</url>
      <bibkey>acl-2018-association</bibkey>
    </frontmatter>
    <paper id="2">
      <title>A La Carte Embedding: Cheap but Effective Induction of Semantic Feature Vectors</title>
      <author><first>Mikhail</first><last>Khodak</last></author>
      <author><first>Nikunj</first><last>Saunshi</last></author>
      <author><first>Yingyu</first><last>Liang</last></author>
      <author><first>Tengyu</first><last>Ma</last></author>
      <author><first>Brandon</first><last>Stewart</last></author>
      <author><first>Sanjeev</first><last>Arora</last></author>
      <pages>12&#8211;22</pages>
      <abstract>Motivations like domain adaptation, transfer learning, and feature learning have fueled interest in inducing embeddings for rare or unseen words, n-grams, synsets, and other textual features. This paper introduces a la carte embedding, a simple and general alternative to the usual word2vec-based approaches for building such representations that is based upon recent theoretical results for GloVe-like embeddings. Our method relies mainly on a linear transformation that is efficiently learnable using pretrained word vectors and linear regression. This transform is applicable on the fly in the future when a new text feature or rare word is encountered, even if only a single usage example is available. We introduce a new dataset showing how the a la carte method requires fewer examples of words in context to learn high-quality embeddings and we obtain state-of-the-art results on a nonce task and some unsupervised document classification tasks.</abstract>
      <url hash="1b4df139">P18-1002</url>
      <video href="https://vimeo.com/285807785" />
      <attachment type="presentation" hash="6983a612">P18-1002.Presentation.pdf</attachment>
      <doi>10.18653/v1/P18-1002</doi>
      <bibkey>khodak-etal-2018-la</bibkey>
      <pwccode url="https://github.com/NLPrinceton/ALaCarte" additional="false">NLPrinceton/ALaCarte</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mpqa-opinion-corpus">MPQA Opinion Corpus</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mr">MR</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/subj">SUBJ</pwcdataset>
    </paper>
    <paper id="3">
      <title>Unsupervised Learning of Distributional Relation Vectors</title>
      <author><first>Shoaib</first><last>Jameel</last></author>
      <author><first>Zied</first><last>Bouraoui</last></author>
      <author><first>Steven</first><last>Schockaert</last></author>
      <pages>23&#8211;33</pages>
      <abstract>Word embedding models such as GloVe rely on co-occurrence statistics to learn vector representations of word meaning. While we may similarly expect that co-occurrence statistics can be used to capture rich information about the relationships between different words, existing approaches for modeling such relationships are based on manipulating pre-trained word vectors. In this paper, we introduce a novel method which directly learns relation vectors from co-occurrence statistics. To this end, we first introduce a variant of GloVe, in which there is an explicit connection between word vectors and PMI weighted co-occurrence vectors. We then show how relation vectors can be naturally embedded into the resulting vector space.</abstract>
      <url hash="b2280377">P18-1003</url>
      <video href="https://vimeo.com/285807793" />
      <doi>10.18653/v1/P18-1003</doi>
      <bibkey>jameel-etal-2018-unsupervised</bibkey>
    </paper>
    <paper id="4">
      <title>Explicit Retrofitting of Distributional Word Vectors</title>
      <author><first>Goran</first><last>Glava&#353;</last></author>
      <author><first>Ivan</first><last>Vuli&#263;</last></author>
      <pages>34&#8211;45</pages>
      <abstract>Semantic specialization of distributional word vectors, referred to as retrofitting, is a process of fine-tuning word vectors using external lexical knowledge in order to better embed some semantic relation. Existing retrofitting models integrate linguistic constraints directly into learning objectives and, consequently, specialize only the vectors of words from the constraints. In this work, in contrast, we transform external lexico-semantic relations into training examples which we use to learn an explicit retrofitting model (ER). The ER model allows us to learn a global specialization function and specialize the vectors of words unobserved in the training data as well. We report large gains over original distributional vector spaces in (1) intrinsic word similarity evaluation and on (2) two downstream tasks &#8722; lexical simplification and dialog state tracking. Finally, we also successfully specialize vector spaces of new languages (i.e., unseen in the training data) by coupling ER with shared multilingual distributional vector spaces.</abstract>
      <url hash="86bcce8c">P18-1004</url>
      <attachment type="software" hash="f73a7321">P18-1004.Software.zip</attachment>
      <video href="https://vimeo.com/285807800" />
      <attachment type="presentation" hash="cb23b48b">P18-1004.Presentation.pdf</attachment>
      <doi>10.18653/v1/P18-1004</doi>
      <bibkey>glavas-vulic-2018-explicit</bibkey>
    </paper>
    <paper id="6">
      <title>Triangular Architecture for Rare Language Translation</title>
      <author><first>Shuo</first><last>Ren</last></author>
      <author><first>Wenhu</first><last>Chen</last></author>
      <author><first>Shujie</first><last>Liu</last></author>
      <author><first>Mu</first><last>Li</last></author>
      <author><first>Ming</first><last>Zhou</last></author>
      <author><first>Shuai</first><last>Ma</last></author>
      <pages>56&#8211;65</pages>
      <abstract>Neural Machine Translation (NMT) performs poor on the low-resource language pair (X,Z), especially when Z is a rare language. By introducing another rich language Y, we propose a novel triangular training architecture (TA-NMT) to leverage bilingual data (Y,Z) (may be small) and (X,Y) (can be rich) to improve the translation performance of low-resource pairs. In this triangular architecture, Z is taken as the intermediate latent variable, and translation models of Z are jointly optimized with an unified bidirectional EM algorithm under the goal of maximizing the translation likelihood of (X,Y). Empirical results demonstrate that our method significantly improves the translation quality of rare languages on MultiUN and IWSLT2012 datasets, and achieves even better performance combining back-translation methods.</abstract>
      <url hash="dde2ad38">P18-1006</url>
      <video href="https://vimeo.com/285807823" />
      <doi>10.18653/v1/P18-1006</doi>
      <bibkey>ren-etal-2018-triangular</bibkey>
    </paper>
    <paper id="7">
      <title>Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates</title>
      <author><first>Taku</first><last>Kudo</last></author>
      <pages>66&#8211;75</pages>
      <abstract>Subword units are an effective way to alleviate the open vocabulary problems in neural machine translation (NMT). While sentences are usually converted into unique subword sequences, subword segmentation is potentially ambiguous and multiple segmentations are possible even with the same vocabulary. The question addressed in this paper is whether it is possible to harness the segmentation ambiguity as a noise to improve the robustness of NMT. We present a simple regularization method, subword regularization, which trains the model with multiple subword segmentations probabilistically sampled during training. In addition, for better subword sampling, we propose a new subword segmentation algorithm based on a unigram language model. We experiment with multiple corpora and report consistent improvements especially on low resource and out-of-domain settings.</abstract>
      <url hash="485f9836">P18-1007</url>
      <video href="https://vimeo.com/285807834" />
      <doi>10.18653/v1/P18-1007</doi>
      <bibkey>kudo-2018-subword</bibkey>
      <pwccode url="https://github.com/google/sentencepiece" additional="true">google/sentencepiece</pwccode>
    </paper>
    <paper id="8">
      <title>The Best of Both Worlds: Combining Recent Advances in Neural Machine Translation</title>
      <author><first>Mia Xu</first><last>Chen</last></author>
      <author><first>Orhan</first><last>Firat</last></author>
      <author><first>Ankur</first><last>Bapna</last></author>
      <author><first>Melvin</first><last>Johnson</last></author>
      <author><first>Wolfgang</first><last>Macherey</last></author>
      <author><first>George</first><last>Foster</last></author>
      <author><first>Llion</first><last>Jones</last></author>
      <author><first>Mike</first><last>Schuster</last></author>
      <author><first>Noam</first><last>Shazeer</last></author>
      <author><first>Niki</first><last>Parmar</last></author>
      <author><first>Ashish</first><last>Vaswani</last></author>
      <author><first>Jakob</first><last>Uszkoreit</last></author>
      <author><first>Lukasz</first><last>Kaiser</last></author>
      <author><first>Zhifeng</first><last>Chen</last></author>
      <author><first>Yonghui</first><last>Wu</last></author>
      <author><first>Macduff</first><last>Hughes</last></author>
      <pages>76&#8211;86</pages>
      <abstract>The past year has witnessed rapid advances in sequence-to-sequence (seq2seq) modeling for Machine Translation (MT). The classic RNN-based approaches to MT were first out-performed by the convolutional seq2seq model, which was then out-performed by the more recent Transformer model. Each of these new approaches consists of a fundamental architecture accompanied by a set of modeling and training techniques that are in principle applicable to other seq2seq architectures. In this paper, we tease apart the new architectures and their accompanying techniques in two ways. First, we identify several key modeling and training techniques, and apply them to the RNN architecture, yielding a new RNMT+ model that outperforms all of the three fundamental architectures on the benchmark WMT&#8217;14 English to French and English to German tasks. Second, we analyze the properties of each fundamental seq2seq architecture and devise new hybrid architectures intended to combine their strengths. Our hybrid models obtain further improvements, outperforming the RNMT+ model on both benchmark datasets.</abstract>
      <url hash="3701fe51">P18-1008</url>
      <attachment type="note" hash="90615a6e">P18-1008.Notes.pdf</attachment>
      <video href="https://vimeo.com/285807844" />
      <attachment type="presentation" hash="b42dff13">P18-1008.Presentation.pdf</attachment>
      <doi>10.18653/v1/P18-1008</doi>
      <bibkey>chen-etal-2018-best</bibkey>
      <pwccode url="https://github.com/tensorflow/lingvo" additional="true">tensorflow/lingvo</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/wmt-2014">WMT 2014</pwcdataset>
    </paper>
    <paper id="9">
      <title>Ultra-Fine Entity Typing</title>
      <author><first>Eunsol</first><last>Choi</last></author>
      <author><first>Omer</first><last>Levy</last></author>
      <author><first>Yejin</first><last>Choi</last></author>
      <author><first>Luke</first><last>Zettlemoyer</last></author>
      <pages>87&#8211;96</pages>
      <abstract>We introduce a new entity typing task: given a sentence with an entity mention, the goal is to predict a set of free-form phrases (e.g. skyscraper, songwriter, or criminal) that describe appropriate types for the target entity. This formulation allows us to use a new type of distant supervision at large scale: head words, which indicate the type of the noun phrases they appear in. We show that these ultra-fine types can be crowd-sourced, and introduce new evaluation sets that are much more diverse and fine-grained than existing benchmarks. We present a model that can predict ultra-fine types, and is trained using a multitask objective that pools our new head-word supervision with prior supervision from entity linking. Experimental results demonstrate that our model is effective in predicting entity types at varying granularity; it achieves state of the art performance on an existing fine-grained entity typing benchmark, and sets baselines for our newly-introduced datasets.</abstract>
      <url hash="4eb503b8">P18-1009</url>
      <video href="https://vimeo.com/285807855" />
      <attachment type="presentation" hash="fd14fd46">P18-1009.Presentation.pdf</attachment>
      <doi>10.18653/v1/P18-1009</doi>
      <bibkey>choi-etal-2018-ultra</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/open-entity-1">Open Entity</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/figer">FIGER</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/ontonotes-5-0">OntoNotes 5.0</pwcdataset>
    </paper>
    <paper id="12">
      <title>Towards Understanding the Geometry of Knowledge Graph Embeddings</title>
      <author><first /><last>Chandrahas</last></author>
      <author><first>Aditya</first><last>Sharma</last></author>
      <author><first>Partha</first><last>Talukdar</last></author>
      <pages>122&#8211;131</pages>
      <abstract>Knowledge Graph (KG) embedding has emerged as a very active area of research over the last few years, resulting in the development of several embedding methods. These KG embedding methods represent KG entities and relations as vectors in a high-dimensional space. Despite this popularity and effectiveness of KG embeddings in various tasks (e.g., link prediction), geometric understanding of such embeddings (i.e., arrangement of entity and relation vectors in vector space) is unexplored &#8211; we fill this gap in the paper. We initiate a study to analyze the geometry of KG embeddings and correlate it with task performance and other hyperparameters. To the best of our knowledge, this is the first study of its kind. Through extensive experiments on real-world datasets, we discover several insights. For example, we find that there are sharp differences between the geometry of embeddings learnt by different classes of KG embeddings methods. We hope that this initial study will inspire other follow-up research on this important but unexplored problem.</abstract>
      <url hash="72d664e7">P18-1012</url>
      <attachment type="note" hash="066cbc01">P18-1012.Notes.pdf</attachment>
      <attachment type="software" hash="497eeb3b">P18-1012.Software.zip</attachment>
      <video href="https://vimeo.com/285807892" />
      <attachment type="presentation" hash="a27603ef">P18-1012.Presentation.pdf</attachment>
      <doi>10.18653/v1/P18-1012</doi>
      <bibkey>chandrahas-etal-2018-towards</bibkey>
      <pwccode url="https://github.com/malllabiisc/kg-geometry" additional="false">malllabiisc/kg-geometry</pwccode>
    </paper>
    <paper id="13">
      <title>A Unified Model for Extractive and Abstractive Summarization using Inconsistency Loss</title>
      <author><first>Wan-Ting</first><last>Hsu</last></author>
      <author><first>Chieh-Kai</first><last>Lin</last></author>
      <author><first>Ming-Ying</first><last>Lee</last></author>
      <author><first>Kerui</first><last>Min</last></author>
      <author><first>Jing</first><last>Tang</last></author>
      <author><first>Min</first><last>Sun</last></author>
      <pages>132&#8211;141</pages>
      <abstract>We propose a unified model combining the strength of extractive and abstractive summarization. On the one hand, a simple extractive model can obtain sentence-level attention with high ROUGE scores but less readable. On the other hand, a more complicated abstractive model can obtain word-level dynamic attention to generate a more readable paragraph. In our model, sentence-level attention is used to modulate the word-level attention such that words in less attended sentences are less likely to be generated. Moreover, a novel inconsistency loss function is introduced to penalize the inconsistency between two levels of attentions. By end-to-end training our model with the inconsistency loss and original losses of extractive and abstractive models, we achieve state-of-the-art ROUGE scores while being the most informative and readable summarization on the CNN/Daily Mail dataset in a solid human evaluation.</abstract>
      <url hash="ec9453f7">P18-1013</url>
      <attachment type="note" hash="d48e7fb8">P18-1013.Notes.pdf</attachment>
      <video href="https://vimeo.com/285800568" />
      <attachment type="presentation" hash="76a2ba78">P18-1013.Presentation.pdf</attachment>
      <doi>10.18653/v1/P18-1013</doi>
      <bibkey>hsu-etal-2018-unified</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/cnn-daily-mail-1">CNN/Daily Mail</pwcdataset>
    </paper>
    <paper id="14">
      <title>Extractive Summarization with <fixed-case>SWAP</fixed-case>-<fixed-case>NET</fixed-case>: Sentences and Words from Alternating Pointer Networks</title>
      <author><first>Aishwarya</first><last>Jadhav</last></author>
      <author><first>Vaibhav</first><last>Rajan</last></author>
      <pages>142&#8211;151</pages>
      <abstract>We present a new neural sequence-to-sequence model for extractive summarization called SWAP-NET (Sentences and Words from Alternating Pointer Networks). Extractive summaries comprising a salient subset of input sentences, often also contain important key words. Guided by this principle, we design SWAP-NET that models the interaction of key words and salient sentences using a new two-level pointer network based architecture. SWAP-NET identifies both salient sentences and key words in an input document, and then combines them to form the extractive summary. Experiments on large scale benchmark corpora demonstrate the efficacy of SWAP-NET that outperforms state-of-the-art extractive summarizers.</abstract>
      <url hash="a279f5a5">P18-1014</url>
      <video href="https://vimeo.com/285800574" />
      <doi>10.18653/v1/P18-1014</doi>
      <bibkey>jadhav-rajan-2018-extractive</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/cnn-daily-mail-1">CNN/Daily Mail</pwcdataset>
    </paper>
    <paper id="15">
      <title>Retrieve, Rerank and Rewrite: Soft Template Based Neural Summarization</title>
      <author><first>Ziqiang</first><last>Cao</last></author>
      <author><first>Wenjie</first><last>Li</last></author>
      <author><first>Sujian</first><last>Li</last></author>
      <author><first>Furu</first><last>Wei</last></author>
      <pages>152&#8211;161</pages>
      <abstract>Most previous seq2seq summarization systems purely depend on the source text to generate summaries, which tends to work unstably. Inspired by the traditional template-based summarization approaches, this paper proposes to use existing summaries as soft templates to guide the seq2seq model. To this end, we use a popular IR platform to Retrieve proper summaries as candidate templates. Then, we extend the seq2seq framework to jointly conduct template Reranking and template-aware summary generation (Rewriting). Experiments show that, in terms of informativeness, our model significantly outperforms the state-of-the-art methods, and even soft templates themselves demonstrate high competitiveness. In addition, the import of high-quality external summaries improves the stability and readability of generated summaries.</abstract>
      <url hash="6e7d1c95">P18-1015</url>
      <video href="https://vimeo.com/285800584" />
      <attachment type="presentation" hash="1bc7c53f">P18-1015.Presentation.pdf</attachment>
      <doi>10.18653/v1/P18-1015</doi>
      <bibkey>cao-etal-2018-retrieve</bibkey>
    </paper>
    <paper id="16">
      <title>Simple and Effective Text Simplification Using Semantic and Neural Methods</title>
      <author><first>Elior</first><last>Sulem</last></author>
      <author><first>Omri</first><last>Abend</last></author>
      <author><first>Ari</first><last>Rappoport</last></author>
      <pages>162&#8211;173</pages>
      <abstract>Sentence splitting is a major simplification operator. Here we present a simple and efficient splitting algorithm based on an automatic semantic parser. After splitting, the text is amenable for further fine-tuned simplification operations. In particular, we show that neural Machine Translation can be effectively used in this situation. Previous application of Machine Translation for simplification suffers from a considerable disadvantage in that they are over-conservative, often failing to modify the source in any way. Splitting based on semantic parsing, as proposed here, alleviates this issue. Extensive automatic and human evaluation shows that the proposed method compares favorably to the state-of-the-art in combined lexical and structural simplification.</abstract>
      <url hash="97aaf464">P18-1016</url>
      <video href="https://vimeo.com/285800596" />
      <attachment type="presentation" hash="494058e6">P18-1016.Presentation.pdf</attachment>
      <doi>10.18653/v1/P18-1016</doi>
      <bibkey>sulem-etal-2018-simple</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/turkcorpus">TurkCorpus</pwcdataset>
    </paper>
    <paper id="19">
      <title>A Corpus with Multi-Level Annotations of Patients, Interventions and Outcomes to Support Language Processing for Medical Literature</title>
      <author><first>Benjamin</first><last>Nye</last></author>
      <author><first>Junyi Jessy</first><last>Li</last></author>
      <author><first>Roma</first><last>Patel</last></author>
      <author><first>Yinfei</first><last>Yang</last></author>
      <author><first>Iain</first><last>Marshall</last></author>
      <author><first>Ani</first><last>Nenkova</last></author>
      <author><first>Byron</first><last>Wallace</last></author>
      <pages>197&#8211;207</pages>
      <abstract>We present a corpus of 5,000 richly annotated abstracts of medical articles describing clinical randomized controlled trials. Annotations include demarcations of text spans that describe the Patient population enrolled, the Interventions studied and to what they were Compared, and the Outcomes measured (the &#8216;PICO&#8217; elements). These spans are further annotated at a more granular level, e.g., individual interventions within them are marked and mapped onto a structured medical vocabulary. We acquired annotations from a diverse set of workers with varying levels of expertise and cost. We describe our data collection process and the corpus itself in detail. We then outline a set of challenging NLP tasks that would aid searching of the medical literature and the practice of evidence-based medicine.</abstract>
      <url hash="18e0ad8b">P18-1019</url>
      <attachment type="note" hash="689da319">P18-1019.Notes.pdf</attachment>
      <video href="https://vimeo.com/285800638" />
      <doi>10.18653/v1/P18-1019</doi>
      <bibkey>nye-etal-2018-corpus</bibkey>
      <pwccode url="" additional="true" />
    </paper>
    <paper id="21">
      <title>Neural Argument Generation Augmented with Externally Retrieved Evidence</title>
      <author><first>Xinyu</first><last>Hua</last></author>
      <author><first>Lu</first><last>Wang</last></author>
      <pages>219&#8211;230</pages>
      <abstract>High quality arguments are essential elements for human reasoning and decision-making processes. However, effective argument construction is a challenging task for both human and machines. In this work, we study a novel task on automatically generating arguments of a different stance for a given statement. We propose an encoder-decoder style neural network-based argument generation model enriched with externally retrieved evidence from Wikipedia. Our model first generates a set of talking point phrases as intermediate representation, followed by a separate decoder producing the final argument based on both input and the keyphrases. Experiments on a large-scale dataset collected from Reddit show that our model constructs arguments with more topic-relevant content than popular sequence-to-sequence generation models according to automatic evaluation and human assessments.</abstract>
      <url hash="c970231c">P18-1021</url>
      <attachment type="note" hash="54d82446">P18-1021.Notes.pdf</attachment>
      <video href="https://vimeo.com/285800652" />
      <attachment type="presentation" hash="34dd3704">P18-1021.Presentation.pdf</attachment>
      <doi>10.18653/v1/P18-1021</doi>
      <bibkey>hua-wang-2018-neural</bibkey>
    </paper>
    <paper id="23">
      <title>Retrieval of the Best Counterargument without Prior Topic Knowledge</title>
      <author><first>Henning</first><last>Wachsmuth</last></author>
      <author><first>Shahbaz</first><last>Syed</last></author>
      <author><first>Benno</first><last>Stein</last></author>
      <pages>241&#8211;251</pages>
      <abstract>Given any argument on any controversial topic, how to counter it? This question implies the challenging retrieval task of finding the best counterargument. Since prior knowledge of a topic cannot be expected in general, we hypothesize the best counterargument to invoke the same aspects as the argument while having the opposite stance. To operationalize our hypothesis, we simultaneously model the similarity and dissimilarity of pairs of arguments, based on the words and embeddings of the arguments&#8217; premises and conclusions. A salient property of our model is its independence from the topic at hand, i.e., it applies to arbitrary arguments. We evaluate different model variations on millions of argument pairs derived from the web portal idebate.org. Systematic ranking experiments suggest that our hypothesis is true for many arguments: For 7.6 candidates with opposing stance on average, we rank the best counterargument highest with 60% accuracy. Even among all 2801 test set pairs as candidates, we still find the best one about every third time.</abstract>
      <url hash="db19754b">P18-1023</url>
      <attachment type="note" hash="0fb40c59">P18-1023.Notes.pdf</attachment>
      <video href="https://vimeo.com/285800671" />
      <doi>10.18653/v1/P18-1023</doi>
      <bibkey>wachsmuth-etal-2018-retrieval</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/conceptnet">ConceptNet</pwcdataset>
    </paper>
    <paper id="24">
      <title><fixed-case>L</fixed-case>ink<fixed-case>NB</fixed-case>ed: Multi-Graph Representation Learning with Entity Linkage</title>
      <author><first>Rakshit</first><last>Trivedi</last></author>
      <author><first>Bunyamin</first><last>Sisman</last></author>
      <author><first>Xin Luna</first><last>Dong</last></author>
      <author><first>Christos</first><last>Faloutsos</last></author>
      <author><first>Jun</first><last>Ma</last></author>
      <author><first>Hongyuan</first><last>Zha</last></author>
      <pages>252&#8211;262</pages>
      <abstract>Knowledge graphs have emerged as an important model for studying complex multi-relational data. This has given rise to the construction of numerous large scale but incomplete knowledge graphs encoding information extracted from various resources. An effective and scalable approach to jointly learn over multiple graphs and eventually construct a unified graph is a crucial next step for the success of knowledge-based inference for many downstream applications. To this end, we propose LinkNBed, a deep relational learning framework that learns entity and relationship representations across multiple graphs. We identify entity linkage across graphs as a vital component to achieve our goal. We design a novel objective that leverage entity linkage and build an efficient multi-task training procedure. Experiments on link prediction and entity linkage demonstrate substantial improvements over the state-of-the-art relational learning approaches.</abstract>
      <url hash="e2d20a58">P18-1024</url>
      <attachment type="note" hash="51be0eb5">P18-1024.Notes.pdf</attachment>
      <attachment type="poster" hash="d0b18275">P18-1024.Poster.pdf</attachment>
      <doi>10.18653/v1/P18-1024</doi>
      <bibkey>trivedi-etal-2018-linknbed</bibkey>
    </paper>
    <paper id="25">
      <title>Probabilistic Embedding of Knowledge Graphs with Box Lattice Measures</title>
      <author><first>Luke</first><last>Vilnis</last></author>
      <author><first>Xiang</first><last>Li</last></author>
      <author><first>Shikhar</first><last>Murty</last></author>
      <author><first>Andrew</first><last>McCallum</last></author>
      <pages>263&#8211;272</pages>
      <abstract>Embedding methods which enforce a partial order or lattice structure over the concept space, such as Order Embeddings (OE), are a natural way to model transitive relational data (e.g. entailment graphs). However, OE learns a deterministic knowledge base, limiting expressiveness of queries and the ability to use uncertainty for both prediction and learning (e.g. learning from expectations). Probabilistic extensions of OE have provided the ability to somewhat calibrate these denotational probabilities while retaining the consistency and inductive bias of ordered models, but lack the ability to model the negative correlations found in real-world knowledge. In this work we show that a broad class of models that assign probability measures to OE can never capture negative correlation, which motivates our construction of a novel box lattice and accompanying probability measure to capture anti-correlation and even disjoint concepts, while still providing the benefits of probabilistic modeling, such as the ability to perform rich joint and conditional queries over arbitrary sets of concepts, and both learning from and predicting calibrated uncertainty. We show improvements over previous approaches in modeling the Flickr and WordNet entailment graphs, and investigate the power of the model.</abstract>
      <url hash="f02da105">P18-1025</url>
      <attachment type="note" hash="904610ed">P18-1025.Notes.pdf</attachment>
      <doi>10.18653/v1/P18-1025</doi>
      <bibkey>vilnis-etal-2018-probabilistic</bibkey>
    </paper>
    <paper id="27">
      <title>Sharp Nearby, Fuzzy Far Away: How Neural Language Models Use Context</title>
      <author><first>Urvashi</first><last>Khandelwal</last></author>
      <author><first>He</first><last>He</last></author>
      <author><first>Peng</first><last>Qi</last></author>
      <author><first>Dan</first><last>Jurafsky</last></author>
      <pages>284&#8211;294</pages>
      <abstract>We know very little about how neural language models (LM) use prior linguistic context. In this paper, we investigate the role of context in an LSTM LM, through ablation studies. Specifically, we analyze the increase in perplexity when prior context words are shuffled, replaced, or dropped. On two standard datasets, Penn Treebank and WikiText-2, we find that the model is capable of using about 200 tokens of context on average, but sharply distinguishes nearby context (recent 50 tokens) from the distant history. The model is highly sensitive to the order of words within the most recent sentence, but ignores word order in the long-range context (beyond 50 tokens), suggesting the distant past is modeled only as a rough semantic field or topic. We further find that the neural caching model (Grave et al., 2017b) especially helps the LSTM to copy words from within this distant context. Overall, our analysis not only provides a better understanding of how neural LMs use their context, but also sheds light on recent success from cache-based models.</abstract>
      <url hash="37871b8f">P18-1027</url>
      <attachment type="note" hash="56cdf472">P18-1027.Notes.pdf</attachment>
      <attachment type="poster" hash="66f40af2">P18-1027.Poster.pdf</attachment>
      <doi>10.18653/v1/P18-1027</doi>
      <bibkey>khandelwal-etal-2018-sharp</bibkey>
      <pwccode url="https://github.com/urvashik/lm-context-analysis" additional="false">urvashik/lm-context-analysis</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/penn-treebank">Penn Treebank</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikitext-2">WikiText-2</pwcdataset>
    </paper>
    <paper id="28">
      <title>Bridging <fixed-case>CNN</fixed-case>s, <fixed-case>RNN</fixed-case>s, and Weighted Finite-State Machines</title>
      <author><first>Roy</first><last>Schwartz</last></author>
      <author><first>Sam</first><last>Thomson</last></author>
      <author><first>Noah A.</first><last>Smith</last></author>
      <pages>295&#8211;305</pages>
      <abstract>Recurrent and convolutional neural networks comprise two distinct families of models that have proven to be useful for encoding natural language utterances. In this paper we present SoPa, a new model that aims to bridge these two approaches. SoPa combines neural representation learning with weighted finite-state automata (WFSAs) to learn a soft version of traditional surface patterns. We show that SoPa is an extension of a one-layer CNN, and that such CNNs are equivalent to a restricted version of SoPa, and accordingly, to a restricted form of WFSA. Empirically, on three text classification tasks, SoPa is comparable or better than both a BiLSTM (RNN) baseline and a CNN baseline, and is particularly useful in small data settings.</abstract>
      <url hash="7fef99b1">P18-1028</url>
      <attachment type="note" hash="e7af223c">P18-1028.Notes.pdf</attachment>
      <attachment type="poster" hash="55f54ce9">P18-1028.Poster.pdf</attachment>
      <doi>10.18653/v1/P18-1028</doi>
      <bibkey>schwartz-etal-2018-bridging</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="29">
      <title>Zero-shot Learning of Classifiers from Natural Language Quantification</title>
      <author><first>Shashank</first><last>Srivastava</last></author>
      <author><first>Igor</first><last>Labutov</last></author>
      <author><first>Tom</first><last>Mitchell</last></author>
      <pages>306&#8211;316</pages>
      <abstract>Humans can efficiently learn new concepts using language. We present a framework through which a set of explanations of a concept can be used to learn a classifier without access to any labeled examples. We use semantic parsing to map explanations to probabilistic assertions grounded in latent class labels and observed attributes of unlabeled data, and leverage the differential semantics of linguistic quantifiers (e.g., &#8216;usually&#8217; vs &#8216;always&#8217;) to drive model training. Experiments on three domains show that the learned classifiers outperform previous approaches for learning with limited data, and are comparable with fully supervised classifiers trained from a small number of labeled examples.</abstract>
      <url hash="9843471d">P18-1029</url>
      <attachment type="poster" hash="a2efd3d3">P18-1029.Poster.pdf</attachment>
      <doi>10.18653/v1/P18-1029</doi>
      <bibkey>srivastava-etal-2018-zero</bibkey>
    </paper>
    <paper id="30">
      <title>Sentence-State <fixed-case>LSTM</fixed-case> for Text Representation</title>
      <author><first>Yue</first><last>Zhang</last></author>
      <author><first>Qi</first><last>Liu</last></author>
      <author><first>Linfeng</first><last>Song</last></author>
      <pages>317&#8211;327</pages>
      <abstract>Bi-directional LSTMs are a powerful tool for text representation. On the other hand, they have been shown to suffer various limitations due to their sequential nature. We investigate an alternative LSTM structure for encoding text, which consists of a parallel state for each word. Recurrent steps are used to perform local and global information exchange between words simultaneously, rather than incremental reading of a sequence of words. Results on various classification and sequence labelling benchmarks show that the proposed model has strong representation power, giving highly competitive performances compared to stacked BiLSTM models with similar parameter numbers.</abstract>
      <url hash="19e47b6b">P18-1030</url>
      <attachment type="poster" hash="76675ee7">P18-1030.Poster.pdf</attachment>
      <doi>10.18653/v1/P18-1030</doi>
      <bibkey>zhang-etal-2018-sentence</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/conll-2003">CoNLL-2003</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mr">MR</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/penn-treebank">Penn Treebank</pwcdataset>
    </paper>
    <paper id="33">
      <title>Improving Text-to-<fixed-case>SQL</fixed-case> Evaluation Methodology</title>
      <author><first>Catherine</first><last>Finegan-Dollak</last></author>
      <author><first>Jonathan K.</first><last>Kummerfeld</last></author>
      <author><first>Li</first><last>Zhang</last></author>
      <author><first>Karthik</first><last>Ramanathan</last></author>
      <author><first>Sesh</first><last>Sadasivam</last></author>
      <author><first>Rui</first><last>Zhang</last></author>
      <author><first>Dragomir</first><last>Radev</last></author>
      <pages>351&#8211;360</pages>
      <abstract>To be informative, an evaluation must measure how well systems generalize to realistic unseen data. We identify limitations of and propose improvements to current evaluations of text-to-SQL systems. First, we compare human-generated and automatically generated questions, characterizing properties of queries necessary for real-world applications. To facilitate evaluation on multiple datasets, we release standardized and improved versions of seven existing datasets and one new text-to-SQL dataset. Second, we show that the current division of data into training and test sets measures robustness to variations in the way questions are asked, but only partially tests how well systems generalize to new queries; therefore, we propose a complementary dataset split for evaluation of future work. Finally, we demonstrate how the common practice of anonymizing variables during evaluation removes an important challenge of the task. Our observations highlight key difficulties, and our methodology enables effective measurement of future development.</abstract>
      <url hash="e3675b6c">P18-1033</url>
      <attachment type="poster" hash="e2b4c4d4">P18-1033.Poster.pdf</attachment>
      <doi>10.18653/v1/P18-1033</doi>
      <bibkey>finegan-dollak-etal-2018-improving</bibkey>
      <pwccode url="https://github.com/jkkummerfeld/text2sql-data" additional="false">jkkummerfeld/text2sql-data</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/atis">ATIS</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikisql">WikiSQL</pwcdataset>
    </paper>
    <paper id="34">
      <title>Semantic Parsing with Syntax- and Table-Aware <fixed-case>SQL</fixed-case> Generation</title>
      <author><first>Yibo</first><last>Sun</last></author>
      <author><first>Duyu</first><last>Tang</last></author>
      <author><first>Nan</first><last>Duan</last></author>
      <author><first>Jianshu</first><last>Ji</last></author>
      <author><first>Guihong</first><last>Cao</last></author>
      <author><first>Xiaocheng</first><last>Feng</last></author>
      <author><first>Bing</first><last>Qin</last></author>
      <author><first>Ting</first><last>Liu</last></author>
      <author><first>Ming</first><last>Zhou</last></author>
      <pages>361&#8211;372</pages>
      <abstract>We present a generative model to map natural language questions into SQL queries. Existing neural network based approaches typically generate a SQL query word-by-word, however, a large portion of the generated results is incorrect or not executable due to the mismatch between question words and table contents. Our approach addresses this problem by considering the structure of table and the syntax of SQL language. The quality of the generated SQL query is significantly improved through (1) learning to replicate content from column names, cells or SQL keywords; and (2) improving the generation of WHERE clause by leveraging the column-cell relation. Experiments are conducted on WikiSQL, a recently released dataset with the largest question- SQL pairs. Our approach significantly improves the state-of-the-art execution accuracy from 69.0% to 74.4%.</abstract>
      <url hash="7030323c">P18-1034</url>
      <doi>10.18653/v1/P18-1034</doi>
      <bibkey>sun-etal-2018-semantic</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/wikisql">WikiSQL</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikitablequestions">WikiTableQuestions</pwcdataset>
    </paper>
    <paper id="35">
      <title>Multitask Parsing Across Semantic Representations</title>
      <author><first>Daniel</first><last>Hershcovich</last></author>
      <author><first>Omri</first><last>Abend</last></author>
      <author><first>Ari</first><last>Rappoport</last></author>
      <pages>373&#8211;385</pages>
      <abstract>The ability to consolidate information of different types is at the core of intelligence, and has tremendous practical value in allowing learning for one task to benefit from generalizations learned for others. In this paper we tackle the challenging task of improving semantic parsing performance, taking UCCA parsing as a test case, and AMR, SDP and Universal Dependencies (UD) parsing as auxiliary tasks. We experiment on three languages, using a uniform transition-based system and learning architecture for all parsing tasks. Despite notable conceptual, formal and domain differences, we show that multitask learning significantly improves UCCA parsing in both in-domain and out-of-domain settings.</abstract>
      <url hash="979ba132">P18-1035</url>
      <attachment type="note" hash="88638e3d">P18-1035.Notes.pdf</attachment>
      <attachment type="software" hash="0a6133b9">P18-1035.Software.zip</attachment>
      <attachment type="poster" hash="41d61e3a">P18-1035.Poster.pdf</attachment>
      <doi>10.18653/v1/P18-1035</doi>
      <bibkey>hershcovich-etal-2018-multitask</bibkey>
      <pwccode url="https://github.com/danielhers/tupa" additional="false">danielhers/tupa</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/universal-dependencies">Universal Dependencies</pwcdataset>
    </paper>
    <paper id="37">
      <title><fixed-case>AMR</fixed-case> Parsing as Graph Prediction with Latent Alignment</title>
      <author><first>Chunchuan</first><last>Lyu</last></author>
      <author><first>Ivan</first><last>Titov</last></author>
      <pages>397&#8211;407</pages>
      <abstract>Abstract meaning representations (AMRs) are broad-coverage sentence-level semantic representations. AMRs represent sentences as rooted labeled directed acyclic graphs. AMR parsing is challenging partly due to the lack of annotated alignments between nodes in the graphs and words in the corresponding sentences. We introduce a neural parser which treats alignments as latent variables within a joint probabilistic model of concepts, relations and alignments. As exact inference requires marginalizing over alignments and is infeasible, we use the variational autoencoding framework and a continuous relaxation of the discrete alignments. We show that joint modeling is preferable to using a pipeline of align and parse. The parser achieves the best reported results on the standard benchmark (74.4% on LDC2016E25).</abstract>
      <url hash="294da3fc">P18-1037</url>
      <attachment type="note" hash="c83e63ee">P18-1037.Notes.pdf</attachment>
      <attachment type="poster" hash="1f7295ae">P18-1037.Poster.pdf</attachment>
      <doi>10.18653/v1/P18-1037</doi>
      <bibkey>lyu-titov-2018-amr</bibkey>
      <pwccode url="https://github.com/ChunchuanLv/AMR_AS_GRAPH_PREDICTION" additional="true">ChunchuanLv/AMR_AS_GRAPH_PREDICTION</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/ldc2017t10">LDC2017T10</pwcdataset>
    </paper>
    <paper id="40">
      <title>Discourse Representation Structure Parsing</title>
      <author><first>Jiangming</first><last>Liu</last></author>
      <author><first>Shay B.</first><last>Cohen</last></author>
      <author><first>Mirella</first><last>Lapata</last></author>
      <pages>429&#8211;439</pages>
      <abstract>We introduce an open-domain neural semantic parser which generates formal meaning representations in the style of Discourse Representation Theory (DRT; Kamp and Reyle 1993). We propose a method which transforms Discourse Representation Structures (DRSs) to trees and develop a structure-aware model which decomposes the decoding process into three stages: basic DRS structure prediction, condition prediction (i.e., predicates and relations), and referent prediction (i.e., variables). Experimental results on the Groningen Meaning Bank (GMB) show that our model outperforms competitive baselines by a wide margin.</abstract>
      <url hash="6ed2dc08">P18-1040</url>
      <attachment type="note" hash="ec9bc9bf">P18-1040.Notes.pdf</attachment>
      <attachment type="poster" hash="36c2081b">P18-1040.Poster.pdf</attachment>
      <doi>10.18653/v1/P18-1040</doi>
      <bibkey>liu-etal-2018-discourse</bibkey>
      <pwccode url="https://github.com/EdinburghNLP/EncDecDRSparsing" additional="false">EdinburghNLP/EncDecDRSparsing</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/groningen-meaning-bank">Groningen Meaning Bank</pwcdataset>
    </paper>
    <paper id="41">
      <title>Baseline Needs More Love: On Simple Word-Embedding-Based Models and Associated Pooling Mechanisms</title>
      <author><first>Dinghan</first><last>Shen</last></author>
      <author><first>Guoyin</first><last>Wang</last></author>
      <author><first>Wenlin</first><last>Wang</last></author>
      <author><first>Martin Renqiang</first><last>Min</last></author>
      <author><first>Qinliang</first><last>Su</last></author>
      <author><first>Yizhe</first><last>Zhang</last></author>
      <author><first>Chunyuan</first><last>Li</last></author>
      <author><first>Ricardo</first><last>Henao</last></author>
      <author><first>Lawrence</first><last>Carin</last></author>
      <pages>440&#8211;450</pages>
      <abstract>Many deep learning architectures have been proposed to model the <i>compositionality</i> in text sequences, requiring substantial number of parameters and expensive computations. However, there has not been a rigorous evaluation regarding the added value of sophisticated compositional functions. In this paper, we conduct a point-by-point comparative study between Simple Word-Embedding-based Models (SWEMs), consisting of parameter-free pooling operations, relative to word-embedding-based RNN/CNN models. Surprisingly, SWEMs exhibit comparable or even superior performance in the majority of cases considered. Based upon this understanding, we propose two additional pooling strategies over learned word embeddings: (<i>i</i>) a max-pooling operation for improved interpretability; and (<i>ii</i>) a hierarchical pooling operation, which preserves spatial (<tex-math>n</tex-math>-gram) information within text sequences. We present experiments on 17 datasets encompassing three tasks: (<i>i</i>) (long) document classification; (<i>ii</i>) text sequence matching; and (<i>iii</i>) short text tasks, including classification and tagging.</abstract>
      <url hash="3f1e82a7">P18-1041</url>
      <attachment type="note" hash="43873eb1">P18-1041.Notes.pdf</attachment>
      <attachment type="poster" hash="cfd8cbfc">P18-1041.Poster.pdf</attachment>
      <doi>10.18653/v1/P18-1041</doi>
      <bibkey>shen-etal-2018-baseline</bibkey>
      <pwccode url="https://github.com/dinghanshen/SWEM" additional="true">dinghanshen/SWEM</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/ag-news">AG News</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/conll-2003">CoNLL-2003</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/dbpedia">DBpedia</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mr">MR</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/quora-question-pairs">Quora Question Pairs</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/subj">SUBJ</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikiqa">WikiQA</pwcdataset>
    </paper>
    <paper id="42">
      <title><fixed-case>P</fixed-case>ara<fixed-case>NMT</fixed-case>-50<fixed-case>M</fixed-case>: Pushing the Limits of Paraphrastic Sentence Embeddings with Millions of Machine Translations</title>
      <author><first>John</first><last>Wieting</last></author>
      <author><first>Kevin</first><last>Gimpel</last></author>
      <pages>451&#8211;462</pages>
      <abstract>We describe ParaNMT-50M, a dataset of more than 50 million English-English sentential paraphrase pairs. We generated the pairs automatically by using neural machine translation to translate the non-English side of a large parallel corpus, following Wieting et al. (2017). Our hope is that ParaNMT-50M can be a valuable resource for paraphrase generation and can provide a rich source of semantic knowledge to improve downstream natural language understanding tasks. To show its utility, we use ParaNMT-50M to train paraphrastic sentence embeddings that outperform all supervised systems on every SemEval semantic textual similarity competition, in addition to showing how it can be used for paraphrase generation.</abstract>
      <url hash="849c9422">P18-1042</url>
      <attachment type="note" hash="16faadcb">P18-1042.Notes.pdf</attachment>
      <doi>10.18653/v1/P18-1042</doi>
      <bibkey>wieting-gimpel-2018-paranmt</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/paranmt-50m">PARANMT-50M</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mrpc">MRPC</pwcdataset>
    </paper>
    <paper id="44">
      <title>Neural Adversarial Training for Semi-supervised <fixed-case>J</fixed-case>apanese Predicate-argument Structure Analysis</title>
      <author><first>Shuhei</first><last>Kurita</last></author>
      <author><first>Daisuke</first><last>Kawahara</last></author>
      <author><first>Sadao</first><last>Kurohashi</last></author>
      <pages>474&#8211;484</pages>
      <abstract>Japanese predicate-argument structure (PAS) analysis involves zero anaphora resolution, which is notoriously difficult. To improve the performance of Japanese PAS analysis, it is straightforward to increase the size of corpora annotated with PAS. However, since it is prohibitively expensive, it is promising to take advantage of a large amount of raw corpora. In this paper, we propose a novel Japanese PAS analysis model based on semi-supervised adversarial training with a raw corpus. In our experiments, our model outperforms existing state-of-the-art models for Japanese PAS analysis.</abstract>
      <url hash="05355ad9">P18-1044</url>
      <attachment type="poster" hash="58be7b56">P18-1044.Poster.pdf</attachment>
      <doi>10.18653/v1/P18-1044</doi>
      <bibkey>kurita-etal-2018-neural</bibkey>
    </paper>
    <paper id="48">
      <title>Self-regulation: Employing a Generative Adversarial Network to Improve Event Detection</title>
      <author><first>Yu</first><last>Hong</last></author>
      <author><first>Wenxuan</first><last>Zhou</last></author>
      <author><first>Jingli</first><last>Zhang</last></author>
      <author><first>Guodong</first><last>Zhou</last></author>
      <author><first>Qiaoming</first><last>Zhu</last></author>
      <pages>515&#8211;526</pages>
      <abstract>Due to the ability of encoding and mapping semantic information into a high-dimensional latent feature space, neural networks have been successfully used for detecting events to a certain extent. However, such a feature space can be easily contaminated by spurious features inherent in event detection. In this paper, we propose a self-regulated learning approach by utilizing a generative adversarial network to generate spurious features. On the basis, we employ a recurrent network to eliminate the fakes. Detailed experiments on the ACE 2005 and TAC-KBP 2015 corpora show that our proposed method is highly effective and adaptable.</abstract>
      <url hash="45849e79">P18-1048</url>
      <attachment type="poster" hash="031b57a5">P18-1048.Poster.pdf</attachment>
      <doi>10.18653/v1/P18-1048</doi>
      <bibkey>hong-etal-2018-self</bibkey>
      <pwccode url="https://github.com/JoeZhouWenxuan/Self-regulation-Employing-a-Generative-Adversarial-Network-to-Improve-Event-Detection" additional="false">JoeZhouWenxuan/Self-regulation-Employing-a-Generative-Adversarial-Network-to-Improve-Event-Detection</pwccode>
    </paper>
    <paper id="50">
      <title>Temporal Event Knowledge Acquisition via Identifying Narratives</title>
      <author><first>Wenlin</first><last>Yao</last></author>
      <author><first>Ruihong</first><last>Huang</last></author>
      <pages>537&#8211;547</pages>
      <abstract>Inspired by the double temporality characteristic of narrative texts, we propose a novel approach for acquiring rich temporal &#8220;before/after&#8221; event knowledge across sentences in narrative stories. The double temporality states that a narrative story often describes a sequence of events following the chronological order and therefore, the temporal order of events matches with their textual order. We explored narratology principles and built a weakly supervised approach that identifies 287k narrative paragraphs from three large corpora. We then extracted rich temporal event knowledge from these narrative paragraphs. Such event knowledge is shown useful to improve temporal relation classification and outperforms several recent neural network models on the narrative cloze task.</abstract>
      <url hash="98299090">P18-1050</url>
      <attachment type="poster" hash="576a3c18">P18-1050.Poster.pdf</attachment>
      <doi>10.18653/v1/P18-1050</doi>
      <bibkey>yao-huang-2018-temporal</bibkey>
    </paper>
    <paper id="51">
      <title>Textual Deconvolution Saliency (<fixed-case>TDS</fixed-case>) : a deep tool box for linguistic analysis</title>
      <author><first>Laurent</first><last>Vanni</last></author>
      <author><first>Melanie</first><last>Ducoffe</last></author>
      <author><first>Carlos</first><last>Aguilar</last></author>
      <author><first>Frederic</first><last>Precioso</last></author>
      <author><first>Damon</first><last>Mayaffre</last></author>
      <pages>548&#8211;557</pages>
      <abstract>In this paper, we propose a new strategy, called Text Deconvolution Saliency (TDS), to visualize linguistic information detected by a CNN for text classification. We extend Deconvolution Networks to text in order to present a new perspective on text analysis to the linguistic community. We empirically demonstrated the efficiency of our Text Deconvolution Saliency on corpora from three different languages: English, French, and Latin. For every tested dataset, our Text Deconvolution Saliency automatically encodes complex linguistic patterns based on co-occurrences and possibly on grammatical and syntax analysis.</abstract>
      <url hash="ca4dc533">P18-1051</url>
      <attachment type="poster" hash="de019e9d">P18-1051.Poster.pdf</attachment>
      <doi>10.18653/v1/P18-1051</doi>
      <bibkey>vanni-etal-2018-textual</bibkey>
    </paper>
    <paper id="52">
      <title>Coherence Modeling of Asynchronous Conversations: A Neural Entity Grid Approach</title>
      <author><first>Shafiq</first><last>Joty</last></author>
      <author><first>Muhammad Tasnim</first><last>Mohiuddin</last></author>
      <author><first>Dat</first><last>Tien Nguyen</last></author>
      <pages>558&#8211;568</pages>
      <abstract>We propose a novel coherence model for written asynchronous conversations (e.g., forums, emails), and show its applications in coherence assessment and thread reconstruction tasks. We conduct our research in two steps. First, we propose improvements to the recently proposed neural entity grid model by lexicalizing its entity transitions. Then, we extend the model to asynchronous conversations by incorporating the underlying conversational structure in the entity grid representation and feature computation. Our model achieves state of the art results on standard coherence assessment tasks in monologue and conversations outperforming existing models. We also demonstrate its effectiveness in reconstructing thread structures.</abstract>
      <url hash="6f314aa2">P18-1052</url>
      <attachment type="note" hash="84b87727">P18-1052.Notes.pdf</attachment>
      <attachment type="poster" hash="ef1ce91b">P18-1052.Poster.pdf</attachment>
      <doi>10.18653/v1/P18-1052</doi>
      <bibkey>joty-etal-2018-coherence</bibkey>
    </paper>
    <paper id="53">
      <title>Deep Reinforcement Learning for <fixed-case>C</fixed-case>hinese Zero Pronoun Resolution</title>
      <author><first>Qingyu</first><last>Yin</last></author>
      <author><first>Yu</first><last>Zhang</last></author>
      <author><first>Wei-Nan</first><last>Zhang</last></author>
      <author><first>Ting</first><last>Liu</last></author>
      <author><first>William Yang</first><last>Wang</last></author>
      <pages>569&#8211;578</pages>
      <abstract>Recent neural network models for Chinese zero pronoun resolution gain great performance by capturing semantic information for zero pronouns and candidate antecedents, but tend to be short-sighted, operating solely by making local decisions. They typically predict coreference links between the zero pronoun and one single candidate antecedent at a time while ignoring their influence on future decisions. Ideally, modeling useful information of preceding potential antecedents is crucial for classifying later zero pronoun-candidate antecedent pairs, a need which leads traditional models of zero pronoun resolution to draw on reinforcement learning. In this paper, we show how to integrate these goals, applying deep reinforcement learning to deal with the task. With the help of the reinforcement learning agent, our system learns the policy of selecting antecedents in a sequential manner, where useful information provided by earlier predicted antecedents could be utilized for making later coreference decisions. Experimental results on OntoNotes 5.0 show that our approach substantially outperforms the state-of-the-art methods under three experimental settings.</abstract>
      <url hash="592fc513">P18-1053</url>
      <attachment type="poster" hash="005f5204">P18-1053.Poster.pdf</attachment>
      <doi>10.18653/v1/P18-1053</doi>
      <bibkey>yin-etal-2018-deep</bibkey>
      <pwccode url="https://github.com/qyyin/Reinforce4ZP" additional="false">qyyin/Reinforce4ZP</pwccode>
    </paper>
    <paper id="54">
      <title>Entity-Centric Joint Modeling of <fixed-case>J</fixed-case>apanese Coreference Resolution and Predicate Argument Structure Analysis</title>
      <author><first>Tomohide</first><last>Shibata</last></author>
      <author><first>Sadao</first><last>Kurohashi</last></author>
      <pages>579&#8211;589</pages>
      <abstract>Predicate argument structure analysis is a task of identifying structured events. To improve this field, we need to identify a salient entity, which cannot be identified without performing coreference resolution and predicate argument structure analysis simultaneously. This paper presents an entity-centric joint model for Japanese coreference resolution and predicate argument structure analysis. Each entity is assigned an embedding, and when the result of both analyses refers to an entity, the entity embedding is updated. The analyses take the entity embedding into consideration to access the global information of entities. Our experimental results demonstrate the proposed method can improve the performance of the inter-sentential zero anaphora resolution drastically, which is a notoriously difficult task in predicate argument structure analysis.</abstract>
      <url hash="aedf42d8">P18-1054</url>
      <attachment type="poster" hash="7366537b">P18-1054.Poster.pdf</attachment>
      <doi>10.18653/v1/P18-1054</doi>
      <bibkey>shibata-kurohashi-2018-entity</bibkey>
    </paper>
    <paper id="55">
      <title>Constraining <fixed-case>MG</fixed-case>bank: Agreement, <fixed-case>L</fixed-case>-Selection and Supertagging in <fixed-case>M</fixed-case>inimalist <fixed-case>G</fixed-case>rammars</title>
      <author><first>John</first><last>Torr</last></author>
      <pages>590&#8211;600</pages>
      <abstract>This paper reports on two strategies that have been implemented for improving the efficiency and precision of wide-coverage Minimalist Grammar (MG) parsing. The first extends the formalism presented in Torr and Stabler (2016) with a mechanism for enforcing fine-grained selectional restrictions and agreements. The second is a method for factoring computationally costly null heads out from bottom-up MG parsing; this has the additional benefit of rendering the formalism fully compatible for the first time with highly efficient Markovian supertaggers. These techniques aided in the task of generating MGbank, the first wide-coverage corpus of Minimalist Grammar derivation trees.</abstract>
      <url hash="44c8a1e9">P18-1055</url>
      <attachment type="poster" hash="3afbbaf4">P18-1055.Poster.pdf</attachment>
      <doi>10.18653/v1/P18-1055</doi>
      <bibkey>torr-2018-constraining</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/penn-treebank">Penn Treebank</pwcdataset>
    </paper>
    <paper id="58">
      <title>Give Me More Feedback: Annotating Argument Persuasiveness and Related Attributes in Student Essays</title>
      <author><first>Winston</first><last>Carlile</last></author>
      <author><first>Nishant</first><last>Gurrapadi</last></author>
      <author><first>Zixuan</first><last>Ke</last></author>
      <author><first>Vincent</first><last>Ng</last></author>
      <pages>621&#8211;631</pages>
      <abstract>While argument persuasiveness is one of the most important dimensions of argumentative essay quality, it is relatively little studied in automated essay scoring research. Progress on scoring argument persuasiveness is hindered in part by the scarcity of annotated corpora. We present the first corpus of essays that are simultaneously annotated with argument components, argument persuasiveness scores, and attributes of argument components that impact an argument&#8217;s persuasiveness. This corpus could trigger the development of novel computational models concerning argument persuasiveness that provide useful feedback to students on why their arguments are (un)persuasive in addition to how persuasive they are.</abstract>
      <url hash="5d09a40c">P18-1058</url>
      <attachment type="poster" hash="86116c43">P18-1058.Poster.pdf</attachment>
      <doi>10.18653/v1/P18-1058</doi>
      <bibkey>carlile-etal-2018-give</bibkey>
    </paper>
    <paper id="59">
      <title>Inherent Biases in Reference-based Evaluation for Grammatical Error Correction</title>
      <author><first>Leshem</first><last>Choshen</last></author>
      <author><first>Omri</first><last>Abend</last></author>
      <pages>632&#8211;642</pages>
      <abstract>The prevalent use of too few references for evaluating text-to-text generation is known to bias estimates of their quality (henceforth, low coverage bias or LCB). This paper shows that overcoming LCB in Grammatical Error Correction (GEC) evaluation cannot be attained by re-scaling or by increasing the number of references in any feasible range, contrary to previous suggestions. This is due to the long-tailed distribution of valid corrections for a sentence. Concretely, we show that LCB incentivizes GEC systems to avoid correcting even when they can generate a valid correction. Consequently, existing systems obtain comparable or superior performance compared to humans, by making few but targeted changes to the input. Similar effects on Text Simplification further support our claims.</abstract>
      <url hash="69dfc25d">P18-1059</url>
      <attachment type="note" hash="ed5c9ea8">P18-1059.Notes.pdf</attachment>
      <attachment type="poster" hash="1aff1383">P18-1059.Poster.pdf</attachment>
      <doi>10.18653/v1/P18-1059</doi>
      <bibkey>choshen-abend-2018-inherent</bibkey>
      <pwccode url="https://github.com/borgr/IBGEC" additional="false">borgr/IBGEC</pwccode>
    </paper>
    <paper id="62">
      <title>Unsupervised Abstractive Meeting Summarization with Multi-Sentence Compression and Budgeted Submodular Maximization</title>
      <author><first>Guokan</first><last>Shang</last></author>
      <author><first>Wensi</first><last>Ding</last></author>
      <author><first>Zekun</first><last>Zhang</last></author>
      <author><first>Antoine</first><last>Tixier</last></author>
      <author><first>Polykarpos</first><last>Meladianos</last></author>
      <author><first>Michalis</first><last>Vazirgiannis</last></author>
      <author><first>Jean-Pierre</first><last>Lorr&#233;</last></author>
      <pages>664&#8211;674</pages>
      <abstract>We introduce a novel graph-based framework for abstractive meeting speech summarization that is fully unsupervised and does not rely on any annotations. Our work combines the strengths of multiple recent approaches while addressing their weaknesses. Moreover, we leverage recent advances in word embeddings and graph degeneracy applied to NLP to take exterior semantic knowledge into account, and to design custom diversity and informativeness measures. Experiments on the AMI and ICSI corpus show that our system improves on the state-of-the-art. Code and data are publicly available, and our system can be interactively tested.</abstract>
      <url hash="dff6f316">P18-1062</url>
      <attachment type="note" hash="56489435">P18-1062.Notes.pdf</attachment>
      <attachment type="poster" hash="74fe29d4">P18-1062.Poster.pdf</attachment>
      <doi>10.18653/v1/P18-1062</doi>
      <bibkey>shang-etal-2018-unsupervised</bibkey>
      <pwccode url="https://bitbucket.org/dascim/acl2018_abssumm" additional="true">dascim/acl2018_abssumm</pwccode>
    </paper>
    <paper id="63">
      <title>Fast Abstractive Summarization with Reinforce-Selected Sentence Rewriting</title>
      <author><first>Yen-Chun</first><last>Chen</last></author>
      <author><first>Mohit</first><last>Bansal</last></author>
      <pages>675&#8211;686</pages>
      <abstract>Inspired by how humans summarize long documents, we propose an accurate and fast summarization model that first selects salient sentences and then rewrites them abstractively (i.e., compresses and paraphrases) to generate a concise overall summary. We use a novel sentence-level policy gradient method to bridge the non-differentiable computation between these two neural networks in a hierarchical way, while maintaining language fluency. Empirically, we achieve the new state-of-the-art on all metrics (including human evaluation) on the CNN/Daily Mail dataset, as well as significantly higher abstractiveness scores. Moreover, by first operating at the sentence-level and then the word-level, we enable parallel decoding of our neural generative model that results in substantially faster (10-20x) inference speed as well as 4x faster training convergence than previous long-paragraph encoder-decoder models. We also demonstrate the generalization of our model on the test-only DUC-2002 dataset, where we achieve higher scores than a state-of-the-art model.</abstract>
      <url hash="6b8d76b4">P18-1063</url>
      <attachment type="note" hash="ac22d6e9">P18-1063.Notes.pdf</attachment>
      <doi>10.18653/v1/P18-1063</doi>
      <bibkey>chen-bansal-2018-fast</bibkey>
      <pwccode url="https://github.com/ChenRocks/fast_abs_rl" additional="true">ChenRocks/fast_abs_rl</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/cnn-daily-mail-1">CNN/Daily Mail</pwcdataset>
    </paper>
    <paper id="64">
      <title>Soft Layer-Specific Multi-Task Summarization with Entailment and Question Generation</title>
      <author><first>Han</first><last>Guo</last></author>
      <author><first>Ramakanth</first><last>Pasunuru</last></author>
      <author><first>Mohit</first><last>Bansal</last></author>
      <pages>687&#8211;697</pages>
      <abstract>An accurate abstractive summary of a document should contain all its salient information and should be logically entailed by the input document. We improve these important aspects of abstractive summarization via multi-task learning with the auxiliary tasks of question generation and entailment generation, where the former teaches the summarization model how to look for salient questioning-worthy details, and the latter teaches the model how to rewrite a summary which is a directed-logical subset of the input document. We also propose novel multi-task architectures with high-level (semantic) layer-specific sharing across multiple encoder and decoder layers of the three tasks, as well as soft-sharing mechanisms (and show performance ablations and analysis examples of each contribution). Overall, we achieve statistically significant improvements over the state-of-the-art on both the CNN/DailyMail and Gigaword datasets, as well as on the DUC-2002 transfer setup. We also present several quantitative and qualitative analysis studies of our model&#8217;s learned saliency and entailment skills.</abstract>
      <url hash="589c0b7e">P18-1064</url>
      <attachment type="note" hash="8b74f48d">P18-1064.Notes.pdf</attachment>
      <doi>10.18653/v1/P18-1064</doi>
      <bibkey>guo-etal-2018-soft</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/cnn-daily-mail-1">CNN/Daily Mail</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
    </paper>
    <paper id="68">
      <title>Coarse-to-Fine Decoding for Neural Semantic Parsing</title>
      <author><first>Li</first><last>Dong</last></author>
      <author><first>Mirella</first><last>Lapata</last></author>
      <pages>731&#8211;742</pages>
      <abstract>Semantic parsing aims at mapping natural language utterances into structured meaning representations. In this work, we propose a structure-aware neural architecture which decomposes the semantic parsing process into two stages. Given an input utterance, we first generate a rough sketch of its meaning, where low-level information (such as variable names and arguments) is glossed over. Then, we fill in missing details by taking into account the natural language input and the sketch itself. Experimental results on four datasets characteristic of different domains and meaning representations show that our approach consistently improves performance, achieving competitive results despite the use of relatively simple decoders.</abstract>
      <url hash="7bd8406e">P18-1068</url>
      <video href="https://vimeo.com/285800848" />
      <attachment type="presentation" hash="9acc3901">P18-1068.Presentation.pdf</attachment>
      <doi>10.18653/v1/P18-1068</doi>
      <bibkey>dong-lapata-2018-coarse</bibkey>
      <pwccode url="https://github.com/donglixp/coarse2fine" additional="true">donglixp/coarse2fine</pwccode>
    </paper>
    <paper id="69">
      <title>Confidence Modeling for Neural Semantic Parsing</title>
      <author><first>Li</first><last>Dong</last></author>
      <author><first>Chris</first><last>Quirk</last></author>
      <author><first>Mirella</first><last>Lapata</last></author>
      <pages>743&#8211;753</pages>
      <abstract>In this work we focus on confidence modeling for neural semantic parsers which are built upon sequence-to-sequence models. We outline three major causes of uncertainty, and design various metrics to quantify these factors. These metrics are then used to estimate confidence scores that indicate whether model predictions are likely to be correct. Beyond confidence estimation, we identify which parts of the input contribute to uncertain predictions allowing users to interpret their model, and verify or refine its input. Experimental results show that our confidence model significantly outperforms a widely used method that relies on posterior probability, and improves the quality of interpretation compared to simply relying on attention scores.</abstract>
      <url hash="3c667826">P18-1069</url>
      <attachment type="presentation" hash="7d790e0c">P18-1069.Presentation.pdf</attachment>
      <doi>10.18653/v1/P18-1069</doi>
      <bibkey>dong-etal-2018-confidence</bibkey>
      <pwccode url="https://github.com/donglixp/confidence" additional="false">donglixp/confidence</pwccode>
    </paper>
    <paper id="72">
      <title>On the Limitations of Unsupervised Bilingual Dictionary Induction</title>
      <author><first>Anders</first><last>S&#248;gaard</last></author>
      <author><first>Sebastian</first><last>Ruder</last></author>
      <author><first>Ivan</first><last>Vuli&#263;</last></author>
      <pages>778&#8211;788</pages>
      <abstract>Unsupervised machine translation - i.e., not assuming any cross-lingual supervision signal, whether a dictionary, translations, or comparable corpora - seems impossible, but nevertheless, Lample et al. (2017) recently proposed a fully unsupervised machine translation (MT) model. The model relies heavily on an adversarial, unsupervised cross-lingual word embedding technique for bilingual dictionary induction (Conneau et al., 2017), which we examine here. Our results identify the limitations of current unsupervised MT: unsupervised bilingual dictionary induction performs much worse on morphologically rich languages that are not dependent marking, when monolingual corpora from different domains or different embedding algorithms are used. We show that a simple trick, exploiting a weak supervision signal from identical words, enables more robust induction and establish a near-perfect correlation between unsupervised bilingual dictionary induction performance and a previously unexplored graph similarity metric.</abstract>
      <url hash="c8eb480e">P18-1072</url>
      <video href="https://vimeo.com/285800939" />
      <attachment type="presentation" hash="ea9846e9">P18-1072.Presentation.pdf</attachment>
      <doi>10.18653/v1/P18-1072</doi>
      <bibkey>sogaard-etal-2018-limitations</bibkey>
    </paper>
    <paper id="73">
      <title>A robust self-learning method for fully unsupervised cross-lingual mappings of word embeddings</title>
      <author><first>Mikel</first><last>Artetxe</last></author>
      <author><first>Gorka</first><last>Labaka</last></author>
      <author><first>Eneko</first><last>Agirre</last></author>
      <pages>789&#8211;798</pages>
      <abstract>Recent work has managed to learn cross-lingual word embeddings without parallel data by mapping monolingual embeddings to a shared space through adversarial training. However, their evaluation has focused on favorable conditions, using comparable corpora or closely-related languages, and we show that they often fail in more realistic scenarios. This work proposes an alternative approach based on a fully unsupervised initialization that explicitly exploits the structural similarity of the embeddings, and a robust self-learning algorithm that iteratively improves this solution. Our method succeeds in all tested scenarios and obtains the best published results in standard datasets, even surpassing previous supervised systems. Our implementation is released as an open source project at <url>https://github.com/artetxem/vecmap</url>.</abstract>
      <url hash="2c9893fe">P18-1073</url>
      <video href="https://vimeo.com/285800964" />
      <attachment type="presentation" hash="d366ecf5">P18-1073.Presentation.pdf</attachment>
      <doi>10.18653/v1/P18-1073</doi>
      <bibkey>artetxe-etal-2018-robust</bibkey>
      <pwccode url="https://github.com/artetxem/vecmap" additional="false">artetxem/vecmap</pwccode>
    </paper>
    <paper id="74">
      <title>A Multi-lingual Multi-task Architecture for Low-resource Sequence Labeling</title>
      <author><first>Ying</first><last>Lin</last></author>
      <author><first>Shengqi</first><last>Yang</last></author>
      <author><first>Veselin</first><last>Stoyanov</last></author>
      <author><first>Heng</first><last>Ji</last></author>
      <pages>799&#8211;809</pages>
      <abstract>We propose a multi-lingual multi-task architecture to develop supervised models with a minimal amount of labeled data for sequence labeling. In this new architecture, we combine various transfer models using two layers of parameter sharing. On the first layer, we construct the basis of the architecture to provide universal word representation and feature extraction capability for all models. On the second level, we adopt different parameter sharing strategies for different transfer schemes. This architecture proves to be particularly effective for low-resource settings, when there are less than 200 training sentences for the target task. Using Name Tagging as a target task, our approach achieved 4.3%-50.5% absolute F-score gains compared to the mono-lingual single-task baseline model.</abstract>
      <url hash="22c83937">P18-1074</url>
      <video href="https://vimeo.com/285800977" />
      <attachment type="presentation" hash="4fce3202">P18-1074.Presentation.pdf</attachment>
      <doi>10.18653/v1/P18-1074</doi>
      <bibkey>lin-etal-2018-multi-lingual</bibkey>
      <pwccode url="https://github.com/limteng-rpi/mlmt" additional="false">limteng-rpi/mlmt</pwccode>
    </paper>
    <paper id="77">
      <title>Multi-Relational Question Answering from Narratives: Machine Reading and Reasoning in Simulated Worlds</title>
      <author><first>Igor</first><last>Labutov</last></author>
      <author><first>Bishan</first><last>Yang</last></author>
      <author><first>Anusha</first><last>Prakash</last></author>
      <author><first>Amos</first><last>Azaria</last></author>
      <pages>833&#8211;844</pages>
      <abstract>Question Answering (QA), as a research field, has primarily focused on either knowledge bases (KBs) or free text as a source of knowledge. These two sources have historically shaped the kinds of questions that are asked over these sources, and the methods developed to answer them. In this work, we look towards a practical use-case of QA over user-instructed knowledge that uniquely combines elements of both structured QA over knowledge bases, and unstructured QA over narrative, introducing the task of multi-relational QA over personal narrative. As a first step towards this goal, we make three key contributions: (i) we generate and release TextWorldsQA, a set of five diverse datasets, where each dataset contains dynamic narrative that describes entities and relations in a simulated world, paired with variably compositional questions over that knowledge, (ii) we perform a thorough evaluation and analysis of several state-of-the-art QA models and their variants at this task, and (iii) we release a lightweight Python-based framework we call TextWorlds for easily generating arbitrary additional worlds and narrative, with the goal of allowing the community to create and share a growing collection of diverse worlds as a test-bed for this task.</abstract>
      <url hash="1dd73dda">P18-1077</url>
      <video href="https://vimeo.com/285801036" />
      <doi>10.18653/v1/P18-1077</doi>
      <bibkey>labutov-etal-2018-multi</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
    </paper>
    <paper id="78">
      <title>Simple and Effective Multi-Paragraph Reading Comprehension</title>
      <author><first>Christopher</first><last>Clark</last></author>
      <author><first>Matt</first><last>Gardner</last></author>
      <pages>845&#8211;855</pages>
      <abstract>We introduce a method of adapting neural paragraph-level question answering models to the case where entire documents are given as input. Most current question answering models cannot scale to document or multi-document input, and naively applying these models to each paragraph independently often results in them being distracted by irrelevant text. We show that it is possible to significantly improve performance by using a modified training scheme that teaches the model to ignore non-answer containing paragraphs. Our method involves sampling multiple paragraphs from each document, and using an objective function that requires the model to produce globally correct output. We additionally identify and improve upon a number of other design decisions that arise when working with document-level data. Experiments on TriviaQA and SQuAD shows our method advances the state of the art, including a 10 point gain on TriviaQA.</abstract>
      <url hash="91135ad0">P18-1078</url>
      <video href="https://vimeo.com/285801068" />
      <attachment type="presentation" hash="49d5fe76">P18-1078.Presentation.pdf</attachment>
      <doi>10.18653/v1/P18-1078</doi>
      <bibkey>clark-gardner-2018-simple</bibkey>
      <pwccode url="https://github.com/allenai/document-qa" additional="false">allenai/document-qa</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/triviaqa">TriviaQA</pwcdataset>
    </paper>
    <paper id="79">
      <title>Semantically Equivalent Adversarial Rules for Debugging <fixed-case>NLP</fixed-case> models</title>
      <author><first>Marco Tulio</first><last>Ribeiro</last></author>
      <author><first>Sameer</first><last>Singh</last></author>
      <author><first>Carlos</first><last>Guestrin</last></author>
      <pages>856&#8211;865</pages>
      <abstract>Complex machine learning models for NLP are often brittle, making different predictions for input instances that are extremely similar semantically. To automatically detect this behavior for individual instances, we present semantically equivalent adversaries (SEAs) &#8211; semantic-preserving perturbations that induce changes in the model&#8217;s predictions. We generalize these adversaries into semantically equivalent adversarial rules (SEARs) &#8211; simple, universal replacement rules that induce adversaries on many instances. We demonstrate the usefulness and flexibility of SEAs and SEARs by detecting bugs in black-box state-of-the-art models for three domains: machine comprehension, visual question-answering, and sentiment analysis. Via user studies, we demonstrate that we generate high-quality local adversaries for more instances than humans, and that SEARs induce four times as many mistakes as the bugs discovered by human experts. SEARs are also actionable: retraining models using data augmentation significantly reduces bugs, while maintaining accuracy.</abstract>
      <url hash="c0777e5b">P18-1079</url>
      <attachment type="note" hash="53e951a8">P18-1079.Notes.pdf</attachment>
      <video href="https://vimeo.com/285801102" />
      <attachment type="presentation" hash="e54b2174">P18-1079.Presentation.pdf</attachment>
      <doi>10.18653/v1/P18-1079</doi>
      <bibkey>ribeiro-etal-2018-semantically</bibkey>
      <pwccode url="https://github.com/marcotcr/sears" additional="false">marcotcr/sears</pwccode>
    </paper>
    <paper id="80">
      <title>Style Transfer Through Back-Translation</title>
      <author><first>Shrimai</first><last>Prabhumoye</last></author>
      <author><first>Yulia</first><last>Tsvetkov</last></author>
      <author><first>Ruslan</first><last>Salakhutdinov</last></author>
      <author><first>Alan W</first><last>Black</last></author>
      <pages>866&#8211;876</pages>
      <abstract>Style transfer is the task of rephrasing the text to contain specific stylistic properties without changing the intent or affect within the context. This paper introduces a new method for automatic style transfer. We first learn a latent representation of the input sentence which is grounded in a language translation model in order to better preserve the meaning of the sentence while reducing stylistic properties. Then adversarial generation techniques are used to make the output match the desired style. We evaluate this technique on three different style transformations: sentiment, gender and political slant. Compared to two state-of-the-art style transfer modeling techniques we show improvements both in automatic evaluation of style transfer and in manual evaluation of meaning preservation and fluency.</abstract>
      <url hash="2cc39ade">P18-1080</url>
      <attachment type="note" hash="61438667">P18-1080.Notes.pdf</attachment>
      <video href="https://vimeo.com/285801126" />
      <doi>10.18653/v1/P18-1080</doi>
      <bibkey>prabhumoye-etal-2018-style</bibkey>
      <pwccode url="https://github.com/shrimai/Style-Transfer-Through-Back-Translation" additional="true">shrimai/Style-Transfer-Through-Back-Translation</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/gyafc">GYAFC</pwcdataset>
    </paper>
    <paper id="82">
      <title>Hierarchical Neural Story Generation</title>
      <author><first>Angela</first><last>Fan</last></author>
      <author><first>Mike</first><last>Lewis</last></author>
      <author><first>Yann</first><last>Dauphin</last></author>
      <pages>889&#8211;898</pages>
      <abstract>We explore story generation: creative systems that can build coherent and fluent passages of text about a topic. We collect a large dataset of 300K human-written stories paired with writing prompts from an online forum. Our dataset enables hierarchical story generation, where the model first generates a premise, and then transforms it into a passage of text. We gain further improvements with a novel form of model fusion that improves the relevance of the story to the prompt, and adding a new gated multi-scale self-attention mechanism to model long-range context. Experiments show large improvements over strong baselines on both automated and human evaluations. Human judges prefer stories generated by our approach to those from a strong non-hierarchical model by a factor of two to one.</abstract>
      <url hash="7e02eb3a">P18-1082</url>
      <attachment type="note" hash="dbeeda67">P18-1082.Notes.pdf</attachment>
      <video href="https://vimeo.com/285801163" />
      <doi>10.18653/v1/P18-1082</doi>
      <bibkey>fan-etal-2018-hierarchical</bibkey>
      <pwccode url="https://github.com/pytorch/fairseq" additional="true">pytorch/fairseq</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/writingprompts">WritingPrompts</pwcdataset>
    </paper>
    <paper id="83">
      <title>No Metrics Are Perfect: Adversarial Reward Learning for Visual Storytelling</title>
      <author><first>Xin</first><last>Wang</last></author>
      <author><first>Wenhu</first><last>Chen</last></author>
      <author><first>Yuan-Fang</first><last>Wang</last></author>
      <author><first>William Yang</first><last>Wang</last></author>
      <pages>899&#8211;909</pages>
      <abstract>Though impressive results have been achieved in visual captioning, the task of generating abstract stories from photo streams is still a little-tapped problem. Different from captions, stories have more expressive language styles and contain many imaginary concepts that do not appear in the images. Thus it poses challenges to behavioral cloning algorithms. Furthermore, due to the limitations of automatic metrics on evaluating story quality, reinforcement learning methods with hand-crafted rewards also face difficulties in gaining an overall performance boost. Therefore, we propose an Adversarial REward Learning (AREL) framework to learn an implicit reward function from human demonstrations, and then optimize policy search with the learned reward function. Though automatic evaluation indicates slight performance boost over state-of-the-art (SOTA) methods in cloning expert behaviors, human evaluation shows that our approach achieves significant improvement in generating more human-like stories than SOTA systems.</abstract>
      <url hash="6ac65c3a">P18-1083</url>
      <attachment type="note" hash="37a1a967">P18-1083.Notes.pdf</attachment>
      <video href="https://vimeo.com/285801215" />
      <attachment type="presentation" hash="451a8173">P18-1083.Presentation.pdf</attachment>
      <doi>10.18653/v1/P18-1083</doi>
      <bibkey>wang-etal-2018-metrics</bibkey>
      <pwccode url="https://github.com/littlekobe/AREL" additional="true">littlekobe/AREL</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/vist">VIST</pwcdataset>
    </paper>
    <paper id="87">
      <title>Transformation Networks for Target-Oriented Sentiment Classification</title>
      <author><first>Xin</first><last>Li</last></author>
      <author><first>Lidong</first><last>Bing</last></author>
      <author><first>Wai</first><last>Lam</last></author>
      <author><first>Bei</first><last>Shi</last></author>
      <pages>946&#8211;956</pages>
      <abstract>Target-oriented sentiment classification aims at classifying sentiment polarities over individual opinion targets in a sentence. RNN with attention seems a good fit for the characteristics of this task, and indeed it achieves the state-of-the-art performance. After re-examining the drawbacks of attention mechanism and the obstacles that block CNN to perform well in this classification task, we propose a new model that achieves new state-of-the-art results on a few benchmarks. Instead of attention, our model employs a CNN layer to extract salient features from the transformed word representations originated from a bi-directional RNN layer. Between the two layers, we propose a component which first generates target-specific representations of words in the sentence, and then incorporates a mechanism for preserving the original contextual information from the RNN layer.</abstract>
      <url hash="9448aef8">P18-1087</url>
      <video href="https://vimeo.com/285801308" />
      <attachment type="presentation" hash="9b11eb50">P18-1087.Presentation.pdf</attachment>
      <doi>10.18653/v1/P18-1087</doi>
      <bibkey>li-etal-2018-transformation</bibkey>
      <pwccode url="https://github.com/lixin4ever/TNet" additional="false">lixin4ever/TNet</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/semeval-2014-task-4-sub-task-2">SemEval 2014 Task 4 Sub Task 2</pwcdataset>
    </paper>
    <paper id="88">
      <title>Target-Sensitive Memory Networks for Aspect Sentiment Classification</title>
      <author><first>Shuai</first><last>Wang</last></author>
      <author><first>Sahisnu</first><last>Mazumder</last></author>
      <author><first>Bing</first><last>Liu</last></author>
      <author><first>Mianwei</first><last>Zhou</last></author>
      <author><first>Yi</first><last>Chang</last></author>
      <pages>957&#8211;967</pages>
      <abstract>Aspect sentiment classification (ASC) is a fundamental task in sentiment analysis. Given an aspect/target and a sentence, the task classifies the sentiment polarity expressed on the target in the sentence. Memory networks (MNs) have been used for this task recently and have achieved state-of-the-art results. In MNs, attention mechanism plays a crucial role in detecting the sentiment context for the given target. However, we found an important problem with the current MNs in performing the ASC task. Simply improving the attention mechanism will not solve it. The problem is referred to as target-sensitive sentiment, which means that the sentiment polarity of the (detected) context is dependent on the given target and it cannot be inferred from the context alone. To tackle this problem, we propose the target-sensitive memory networks (TMNs). Several alternative techniques are designed for the implementation of TMNs and their effectiveness is experimentally evaluated.</abstract>
      <url hash="f26bf47a">P18-1088</url>
      <video href="https://vimeo.com/285801326" />
      <doi>10.18653/v1/P18-1088</doi>
      <bibkey>wang-etal-2018-target</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/semeval-2014-task-4-sub-task-2">SemEval 2014 Task 4 Sub Task 2</pwcdataset>
    </paper>
    <paper id="89">
      <title>Identifying Transferable Information Across Domains for Cross-domain Sentiment Classification</title>
      <author><first>Raksha</first><last>Sharma</last></author>
      <author><first>Pushpak</first><last>Bhattacharyya</last></author>
      <author><first>Sandipan</first><last>Dandapat</last></author>
      <author><first>Himanshu Sharad</first><last>Bhatt</last></author>
      <pages>968&#8211;978</pages>
      <abstract>Getting manually labeled data in each domain is always an expensive and a time consuming task. Cross-domain sentiment analysis has emerged as a demanding concept where a labeled source domain facilitates a sentiment classifier for an unlabeled target domain. However, polarity orientation (positive or negative) and the significance of a word to express an opinion often differ from one domain to another domain. Owing to these differences, cross-domain sentiment classification is still a challenging task. In this paper, we propose that words that do not change their polarity and significance represent the transferable (usable) information across domains for cross-domain sentiment classification. We present a novel approach based on &#967;2 test and cosine-similarity between context vector of words to identify polarity preserving significant words across domains. Furthermore, we show that a weighted ensemble of the classifiers enhances the cross-domain classification performance.</abstract>
      <url hash="ae17fbe8">P18-1089</url>
      <video href="https://vimeo.com/285801356" />
      <attachment type="presentation" hash="898824f8">P18-1089.Presentation.pdf</attachment>
      <doi>10.18653/v1/P18-1089</doi>
      <bibkey>sharma-etal-2018-identifying</bibkey>
    </paper>
    <paper id="90">
      <title>Unpaired Sentiment-to-Sentiment Translation: A Cycled Reinforcement Learning Approach</title>
      <author><first>Jingjing</first><last>Xu</last></author>
      <author><first>Xu</first><last>Sun</last></author>
      <author><first>Qi</first><last>Zeng</last></author>
      <author><first>Xiaodong</first><last>Zhang</last></author>
      <author><first>Xuancheng</first><last>Ren</last></author>
      <author><first>Houfeng</first><last>Wang</last></author>
      <author><first>Wenjie</first><last>Li</last></author>
      <pages>979&#8211;988</pages>
      <abstract>The goal of sentiment-to-sentiment &#8220;translation&#8221; is to change the underlying sentiment of a sentence while keeping its content. The main challenge is the lack of parallel data. To solve this problem, we propose a cycled reinforcement learning method that enables training on unpaired data by collaboration between a neutralization module and an emotionalization module. We evaluate our approach on two review datasets, Yelp and Amazon. Experimental results show that our approach significantly outperforms the state-of-the-art systems. Especially, the proposed method substantially improves the content preservation performance. The BLEU score is improved from 1.64 to 22.46 and from 0.56 to 14.06 on the two datasets, respectively.</abstract>
      <url hash="2d7a3c3c">P18-1090</url>
      <video href="https://vimeo.com/285801380" />
      <attachment type="presentation" hash="802c5b78">P18-1090.Presentation.pdf</attachment>
      <doi>10.18653/v1/P18-1090</doi>
      <bibkey>xu-etal-2018-unpaired</bibkey>
      <pwccode url="https://github.com/lancopku/unpaired-sentiment-translation" additional="false">lancopku/unpaired-sentiment-translation</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/gyafc">GYAFC</pwcdataset>
    </paper>
    <paper id="92">
      <title>Working Memory Networks: Augmenting Memory Networks with a Relational Reasoning Module</title>
      <author><first>Juan</first><last>Pavez</last></author>
      <author><first>H&#233;ctor</first><last>Allende</last></author>
      <author><first>H&#233;ctor</first><last>Allende-Cid</last></author>
      <pages>1000&#8211;1009</pages>
      <abstract>During the last years, there has been a lot of interest in achieving some kind of complex reasoning using deep neural networks. To do that, models like Memory Networks (MemNNs) have combined external memory storages and attention mechanisms. These architectures, however, lack of more complex reasoning mechanisms that could allow, for instance, relational reasoning. Relation Networks (RNs), on the other hand, have shown outstanding results in relational reasoning tasks. Unfortunately, their computational cost grows quadratically with the number of memories, something prohibitive for larger problems. To solve these issues, we introduce the Working Memory Network, a MemNN architecture with a novel working memory storage and reasoning module. Our model retains the relational reasoning abilities of the RN while reducing its computational complexity from quadratic to linear. We tested our model on the text QA dataset bAbI and the visual QA dataset NLVR. In the jointly trained bAbI-10k, we set a new state-of-the-art, achieving a mean error of less than 0.5%. Moreover, a simple ensemble of two of our models solves all 20 tasks in the joint version of the benchmark.</abstract>
      <url hash="7844f570">P18-1092</url>
      <video href="https://vimeo.com/285802130" />
      <attachment type="presentation" hash="be04a2ae">P18-1092.Presentation.pdf</attachment>
      <doi>10.18653/v1/P18-1092</doi>
      <bibkey>pavez-etal-2018-working</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/nlvr">NLVR</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/babi-1">bAbI</pwcdataset>
    </paper>
    <paper id="94">
      <title>Adversarial Contrastive Estimation</title>
      <author><first>Avishek Joey</first><last>Bose</last></author>
      <author><first>Huan</first><last>Ling</last></author>
      <author><first>Yanshuai</first><last>Cao</last></author>
      <pages>1021&#8211;1032</pages>
      <abstract>Learning by contrasting positive and negative samples is a general strategy adopted by many methods. Noise contrastive estimation (NCE) for word embeddings and translating embeddings for knowledge graphs are examples in NLP employing this approach. In this work, we view contrastive learning as an abstraction of all such methods and augment the negative sampler into a mixture distribution containing an adversarially learned sampler. The resulting adaptive sampler finds harder negative examples, which forces the main model to learn a better representation of the data. We evaluate our proposal on learning word embeddings, order embeddings and knowledge graph embeddings and observe both faster convergence and improved results on multiple metrics.</abstract>
      <url hash="e34a625d">P18-1094</url>
      <video href="https://vimeo.com/285802169" />
      <attachment type="presentation" hash="56402058">P18-1094.Presentation.pdf</attachment>
      <doi>10.18653/v1/P18-1094</doi>
      <bibkey>bose-etal-2018-adversarial</bibkey>
    </paper>
    <paper id="96">
      <title>Strong Baselines for Neural Semi-Supervised Learning under Domain Shift</title>
      <author><first>Sebastian</first><last>Ruder</last></author>
      <author><first>Barbara</first><last>Plank</last></author>
      <pages>1044&#8211;1054</pages>
      <abstract>Novel neural models have been proposed in recent years for learning under domain shift. Most models, however, only evaluate on a single task, on proprietary datasets, or compare to weak baselines, which makes comparison of models difficult. In this paper, we re-evaluate classic general-purpose bootstrapping approaches in the context of neural networks under domain shifts vs. recent neural approaches and propose a novel multi-task tri-training method that reduces the time and space complexity of classic tri-training. Extensive experiments on two benchmarks for part-of-speech tagging and sentiment analysis are negative: while our novel method establishes a new state-of-the-art for sentiment analysis, it does not fare consistently the best. More importantly, we arrive at the somewhat surprising conclusion that classic tri-training, with some additions, outperforms the state-of-the-art for NLP. Hence classic approaches constitute an important and strong baseline.</abstract>
      <url hash="31a011c8">P18-1096</url>
      <video href="https://vimeo.com/285802189" />
      <attachment type="presentation" hash="54b036c6">P18-1096.Presentation.pdf</attachment>
      <doi>10.18653/v1/P18-1096</doi>
      <bibkey>ruder-plank-2018-strong</bibkey>
      <pwccode url="https://github.com/bplank/semi-supervised-baselines" additional="true">bplank/semi-supervised-baselines</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/multi-domain-sentiment-dataset-v2-0">Multi-Domain Sentiment Dataset v2.0</pwcdataset>
    </paper>
    <paper id="98">
      <title>A Neural Architecture for Automated <fixed-case>ICD</fixed-case> Coding</title>
      <author><first>Pengtao</first><last>Xie</last></author>
      <author><first>Eric</first><last>Xing</last></author>
      <pages>1066&#8211;1076</pages>
      <abstract>The International Classification of Diseases (ICD) provides a hierarchy of diagnostic codes for classifying diseases. Medical coding &#8211; which assigns a subset of ICD codes to a patient visit &#8211; is a mandatory process that is crucial for patient care and billing. Manual coding is time-consuming, expensive, and error prone. In this paper, we build a neural architecture for automated coding. It takes the diagnosis descriptions (DDs) of a patient as inputs and selects the most relevant ICD codes. This architecture contains four major ingredients: (1) tree-of-sequences LSTM encoding of code descriptions (CDs), (2) adversarial learning for reconciling the different writing styles of DDs and CDs, (3) isotonic constraints for incorporating the importance order among the assigned codes, and (4) attentional matching for performing many-to-one and one-to-many mappings from DDs to CDs. We demonstrate the effectiveness of the proposed methods on a clinical datasets with 59K patient visits.</abstract>
      <url hash="f270be80">P18-1098</url>
      <video href="https://vimeo.com/285802228" />
      <doi>10.18653/v1/P18-1098</doi>
      <bibkey>xie-xing-2018-neural</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/mimic-iii">MIMIC-III</pwcdataset>
    </paper>
    <paper id="99">
      <title>Domain Adaptation with Adversarial Training and Graph Embeddings</title>
      <author><first>Firoj</first><last>Alam</last></author>
      <author><first>Shafiq</first><last>Joty</last></author>
      <author><first>Muhammad</first><last>Imran</last></author>
      <pages>1077&#8211;1087</pages>
      <abstract>The success of deep neural networks (DNNs) is heavily dependent on the availability of labeled data. However, obtaining labeled data is a big challenge in many real-world problems. In such scenarios, a DNN model can leverage labeled and unlabeled data from a related domain, but it has to deal with the shift in data distributions between the source and the target domains. In this paper, we study the problem of classifying social media posts during a crisis event (e.g., Earthquake). For that, we use labeled and unlabeled data from past similar events (e.g., Flood) and unlabeled data for the current event. We propose a novel model that performs adversarial learning based domain adaptation to deal with distribution drifts and graph based semi-supervised learning to leverage unlabeled data within a single unified deep learning framework. Our experiments with two real-world crisis datasets collected from Twitter demonstrate significant improvements over several baselines.</abstract>
      <url hash="fb35582a">P18-1099</url>
      <video href="https://vimeo.com/285802247" />
      <attachment type="presentation" hash="21393ceb">P18-1099.Presentation.pdf</attachment>
      <doi>10.18653/v1/P18-1099</doi>
      <bibkey>alam-etal-2018-domain</bibkey>
      <pwccode url="https://github.com/firojalam/domain-adaptation" additional="false">firojalam/domain-adaptation</pwccode>
    </paper>
    <paper id="100">
      <title><fixed-case>TDNN</fixed-case>: A Two-stage Deep Neural Network for Prompt-independent Automated Essay Scoring</title>
      <author><first>Cancan</first><last>Jin</last></author>
      <author><first>Ben</first><last>He</last></author>
      <author><first>Kai</first><last>Hui</last></author>
      <author><first>Le</first><last>Sun</last></author>
      <pages>1088&#8211;1097</pages>
      <abstract>Existing automated essay scoring (AES) models rely on rated essays for the target prompt as training data. Despite their successes in prompt-dependent AES, how to effectively predict essay ratings under a prompt-independent setting remains a challenge, where the rated essays for the target prompt are not available. To close this gap, a two-stage deep neural network (TDNN) is proposed. In particular, in the first stage, using the rated essays for non-target prompts as the training data, a shallow model is learned to select essays with an extreme quality for the target prompt, serving as pseudo training data; in the second stage, an end-to-end hybrid deep model is proposed to learn a prompt-dependent rating model consuming the pseudo training data from the first step. Evaluation of the proposed TDNN on the standard ASAP dataset demonstrates a promising improvement for the prompt-independent AES task.</abstract>
      <url hash="d8a38a28">P18-1100</url>
      <video href="https://vimeo.com/285802257" />
      <attachment type="presentation" hash="55de4182">P18-1100.Presentation.pdf</attachment>
      <doi>10.18653/v1/P18-1100</doi>
      <bibkey>jin-etal-2018-tdnn</bibkey>
    </paper>
    <paper id="101">
      <title>Unsupervised Discrete Sentence Representation Learning for Interpretable Neural Dialog Generation</title>
      <author><first>Tiancheng</first><last>Zhao</last></author>
      <author><first>Kyusong</first><last>Lee</last></author>
      <author><first>Maxine</first><last>Eskenazi</last></author>
      <pages>1098&#8211;1107</pages>
      <abstract>The encoder-decoder dialog model is one of the most prominent methods used to build dialog systems in complex domains. Yet it is limited because it cannot output interpretable actions as in traditional systems, which hinders humans from understanding its generation process. We present an unsupervised discrete sentence representation learning method that can integrate with any existing encoder-decoder dialog models for interpretable response generation. Building upon variational autoencoders (VAEs), we present two novel models, DI-VAE and DI-VST that improve VAEs and can discover interpretable semantics via either auto encoding or context predicting. Our methods have been validated on real-world dialog datasets to discover semantic representations and enhance encoder-decoder models with interpretable generation.</abstract>
      <url hash="6213fd88">P18-1101</url>
      <attachment type="note" hash="07b2934e">P18-1101.Notes.pdf</attachment>
      <video href="https://vimeo.com/285802293" />
      <attachment type="presentation" hash="dc9393ab">P18-1101.Presentation.pdf</attachment>
      <doi>10.18653/v1/P18-1101</doi>
      <bibkey>zhao-etal-2018-unsupervised</bibkey>
      <pwccode url="https://github.com/snakeztc/NeuralDialog-LAED" additional="true">snakeztc/NeuralDialog-LAED</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/penn-treebank">Penn Treebank</pwcdataset>
    </paper>
    <paper id="104">
      <title><fixed-case>M</fixed-case>oji<fixed-case>T</fixed-case>alk: Generating Emotional Responses at Scale</title>
      <author><first>Xianda</first><last>Zhou</last></author>
      <author><first>William Yang</first><last>Wang</last></author>
      <pages>1128&#8211;1137</pages>
      <abstract>Generating emotional language is a key step towards building empathetic natural language processing agents. However, a major challenge for this line of research is the lack of large-scale labeled training data, and previous studies are limited to only small sets of human annotated sentiment labels. Additionally, explicitly controlling the emotion and sentiment of generated text is also difficult. In this paper, we take a more radical approach: we exploit the idea of leveraging Twitter data that are naturally labeled with emojis. We collect a large corpus of Twitter conversations that include emojis in the response and assume the emojis convey the underlying emotions of the sentence. We investigate several conditional variational autoencoders training on these conversations, which allow us to use emojis to control the emotion of the generated text. Experimentally, we show in our quantitative and qualitative analyses that the proposed models can successfully generate high-quality abstractive conversation responses in accordance with designated emotions.</abstract>
      <url hash="a375a91c">P18-1104</url>
      <attachment type="note" hash="5de7d454">P18-1104.Notes.pdf</attachment>
      <video href="https://vimeo.com/285802339" />
      <doi>10.18653/v1/P18-1104</doi>
      <bibkey>zhou-wang-2018-mojitalk</bibkey>
      <pwccode url="" additional="true" />
    </paper>
    <paper id="106">
      <title>A Framework for Representing Language Acquisition in a Population Setting</title>
      <author><first>Jordan</first><last>Kodner</last></author>
      <author><first>Christopher</first><last>Cerezo Falco</last></author>
      <pages>1149&#8211;1159</pages>
      <abstract>Language variation and change are driven both by individuals&#8217; internal cognitive processes and by the social structures through which language propagates. A wide range of computational frameworks have been proposed to connect these drivers. We compare the strengths and weaknesses of existing approaches and propose a new analytic framework which combines previous network models&#8217; ability to capture realistic social structure with practically and more elegant computational properties. The framework privileges the process of language acquisition and embeds learners in a social network but is modular so that population structure can be combined with different acquisition models. We demonstrate two applications for the framework: a test of practical concerns that arise when modeling acquisition in a population setting and an application of the framework to recent work on phonological mergers in progress.</abstract>
      <url hash="58a3f809">P18-1106</url>
      <attachment type="software" hash="d0a3fcc6">P18-1106.Software.zip</attachment>
      <video href="https://vimeo.com/285802370" />
      <attachment type="presentation" hash="4a2204a0">P18-1106.Presentation.pdf</attachment>
      <doi>10.18653/v1/P18-1106</doi>
      <bibkey>kodner-cerezo-falco-2018-framework</bibkey>
    </paper>
    <paper id="107">
      <title>Prefix Lexicalization of Synchronous <fixed-case>CFG</fixed-case>s using Synchronous <fixed-case>TAG</fixed-case></title>
      <author><first>Logan</first><last>Born</last></author>
      <author><first>Anoop</first><last>Sarkar</last></author>
      <pages>1160&#8211;1170</pages>
      <abstract>We show that an epsilon-free, chain-free synchronous context-free grammar (SCFG) can be converted into a weakly equivalent synchronous tree-adjoining grammar (STAG) which is prefix lexicalized. This transformation at most doubles the grammar&#8217;s rank and cubes its size, but we show that in practice the size increase is only quadratic. Our results extend Greibach normal form from CFGs to SCFGs and prove new formal properties about SCFG, a formalism with many applications in natural language processing.</abstract>
      <url hash="d7c9450c">P18-1107</url>
      <attachment type="note" hash="2cd2d024">P18-1107.Notes.pdf</attachment>
      <video href="https://vimeo.com/285802428" />
      <doi>10.18653/v1/P18-1107</doi>
      <bibkey>born-sarkar-2018-prefix</bibkey>
    </paper>
    <paper id="108">
      <title>Straight to the Tree: Constituency Parsing with Neural Syntactic Distance</title>
      <author><first>Yikang</first><last>Shen</last></author>
      <author><first>Zhouhan</first><last>Lin</last></author>
      <author><first>Athul Paul</first><last>Jacob</last></author>
      <author><first>Alessandro</first><last>Sordoni</last></author>
      <author><first>Aaron</first><last>Courville</last></author>
      <author><first>Yoshua</first><last>Bengio</last></author>
      <pages>1171&#8211;1180</pages>
      <abstract>In this work, we propose a novel constituency parsing scheme. The model first predicts a real-valued scalar, named syntactic distance, for each split position in the sentence. The topology of grammar tree is then determined by the values of syntactic distances. Compared to traditional shift-reduce parsing schemes, our approach is free from the potentially disastrous compounding error. It is also easier to parallelize and much faster. Our model achieves the state-of-the-art single model F1 score of 92.1 on PTB and 86.4 on CTB dataset, which surpasses the previous single model results by a large margin.</abstract>
      <url hash="3be18c62">P18-1108</url>
      <video href="https://vimeo.com/285802441" />
      <attachment type="presentation" hash="91cee460">P18-1108.Presentation.pdf</attachment>
      <doi>10.18653/v1/P18-1108</doi>
      <bibkey>shen-etal-2018-straight</bibkey>
      <pwccode url="" additional="true" />
    </paper>
    <paper id="109">
      <title><fixed-case>G</fixed-case>aussian Mixture Latent Vector Grammars</title>
      <author><first>Yanpeng</first><last>Zhao</last></author>
      <author><first>Liwen</first><last>Zhang</last></author>
      <author><first>Kewei</first><last>Tu</last></author>
      <pages>1181&#8211;1189</pages>
      <abstract>We introduce Latent Vector Grammars (LVeGs), a new framework that extends latent variable grammars such that each nonterminal symbol is associated with a continuous vector space representing the set of (infinitely many) subtypes of the nonterminal. We show that previous models such as latent variable grammars and compositional vector grammars can be interpreted as special cases of LVeGs. We then present Gaussian Mixture LVeGs (GM-LVeGs), a new special case of LVeGs that uses Gaussian mixtures to formulate the weights of production rules over subtypes of nonterminals. A major advantage of using Gaussian mixtures is that the partition function and the expectations of subtype rules can be computed using an extension of the inside-outside algorithm, which enables efficient inference and learning. We apply GM-LVeGs to part-of-speech tagging and constituency parsing and show that GM-LVeGs can achieve competitive accuracies.</abstract>
      <url hash="511eecb1">P18-1109</url>
      <attachment type="note" hash="b17292c4">P18-1109.Notes.zip</attachment>
      <video href="https://vimeo.com/285802453" />
      <doi>10.18653/v1/P18-1109</doi>
      <bibkey>zhao-etal-2018-gaussian</bibkey>
      <pwccode url="https://github.com/zhaoyanpeng/lveg" additional="false">zhaoyanpeng/lveg</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/universal-dependencies">Universal Dependencies</pwcdataset>
    </paper>
    <paper id="110">
      <title>Extending a Parser to Distant Domains Using a Few Dozen Partially Annotated Examples</title>
      <author><first>Vidur</first><last>Joshi</last></author>
      <author><first>Matthew</first><last>Peters</last></author>
      <author><first>Mark</first><last>Hopkins</last></author>
      <pages>1190&#8211;1199</pages>
      <abstract>We revisit domain adaptation for parsers in the neural era. First we show that recent advances in word representations greatly diminish the need for domain adaptation when the target domain is syntactically similar to the source domain. As evidence, we train a parser on the Wall Street Journal alone that achieves over 90% F1 on the Brown corpus. For more syntactically distant domains, we provide a simple way to adapt a parser using only dozens of partial annotations. For instance, we increase the percentage of error-free geometry-domain parses in a held-out set from 45% to 73% using approximately five dozen training examples. In the process, we demonstrate a new state-of-the-art single model result on the Wall Street Journal test set of 94.3%. This is an absolute increase of 1.7% over the previous state-of-the-art of 92.6%.</abstract>
      <url hash="19c1154b">P18-1110</url>
      <video href="https://vimeo.com/285802475" />
      <attachment type="presentation" hash="924f3f73">P18-1110.Presentation.pdf</attachment>
      <doi>10.18653/v1/P18-1110</doi>
      <bibkey>joshi-etal-2018-extending</bibkey>
      <pwccode url="https://github.com/vidurj/parser-adaptation" additional="false">vidurj/parser-adaptation</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/penn-treebank">Penn Treebank</pwcdataset>
    </paper>
    <paper id="112">
      <title>Searching for the <fixed-case>X</fixed-case>-Factor: Exploring Corpus Subjectivity for Word Embeddings</title>
      <author><first>Maksim</first><last>Tkachenko</last></author>
      <author><first>Chong Cher</first><last>Chia</last></author>
      <author><first>Hady</first><last>Lauw</last></author>
      <pages>1212&#8211;1221</pages>
      <abstract>We explore the notion of subjectivity, and hypothesize that word embeddings learnt from input corpora of varying levels of subjectivity behave differently on natural language processing tasks such as classifying a sentence by sentiment, subjectivity, or topic. Through systematic comparative analyses, we establish this to be the case indeed. Moreover, based on the discovery of the outsized role that sentiment words play on subjectivity-sensitive tasks such as sentiment classification, we develop a novel word embedding SentiVec which is infused with sentiment information from a lexical resource, and is shown to outperform baselines on such tasks.</abstract>
      <url hash="7d489cae">P18-1112</url>
      <video href="https://vimeo.com/285803381" />
      <attachment type="presentation" hash="df034068">P18-1112.Presentation.pdf</attachment>
      <doi>10.18653/v1/P18-1112</doi>
      <bibkey>tkachenko-etal-2018-searching</bibkey>
    </paper>
    <paper id="113">
      <title>Word Embedding and <fixed-case>W</fixed-case>ord<fixed-case>N</fixed-case>et Based Metaphor Identification and Interpretation</title>
      <author><first>Rui</first><last>Mao</last></author>
      <author><first>Chenghua</first><last>Lin</last></author>
      <author><first>Frank</first><last>Guerin</last></author>
      <pages>1222&#8211;1231</pages>
      <abstract>Metaphoric expressions are widespread in natural language, posing a significant challenge for various natural language processing tasks such as Machine Translation. Current word embedding based metaphor identification models cannot identify the exact metaphorical words within a sentence. In this paper, we propose an unsupervised learning method that identifies and interprets metaphors at word-level without any preprocessing, outperforming strong baselines in the metaphor identification task. Our model extends to interpret the identified metaphors, paraphrasing them into their literal counterparts, so that they can be better translated by machines. We evaluated this with two popular translation systems for English to Chinese, showing that our model improved the systems significantly.</abstract>
      <url hash="9b17b75e">P18-1113</url>
      <video href="https://vimeo.com/285803402" />
      <attachment type="presentation" hash="b8a757d7">P18-1113.Presentation.pdf</attachment>
      <doi>10.18653/v1/P18-1113</doi>
      <bibkey>mao-etal-2018-word</bibkey>
    </paper>
    <paper id="117">
      <title>Context-Aware Neural Machine Translation Learns Anaphora Resolution</title>
      <author><first>Elena</first><last>Voita</last></author>
      <author><first>Pavel</first><last>Serdyukov</last></author>
      <author><first>Rico</first><last>Sennrich</last></author>
      <author><first>Ivan</first><last>Titov</last></author>
      <pages>1264&#8211;1274</pages>
      <abstract>Standard machine translation systems process sentences in isolation and hence ignore extra-sentential information, even though extended context can both prevent mistakes in ambiguous cases and improve translation coherence. We introduce a context-aware neural machine translation model designed in such way that the flow of information from the extended context to the translation model can be controlled and analyzed. We experiment with an English-Russian subtitles dataset, and observe that much of what is captured by our model deals with improving pronoun translation. We measure correspondences between induced attention distributions and coreference relations and observe that the model implicitly captures anaphora. It is consistent with gains for sentences where pronouns need to be gendered in translation. Beside improvements in anaphoric cases, the model also improves in overall BLEU, both over its context-agnostic version (+0.7) and over simple concatenation of the context and source sentences (+0.6).</abstract>
      <url hash="23a4186e">P18-1117</url>
      <attachment type="note" hash="c525e2ae">P18-1117.Notes.pdf</attachment>
      <video href="https://vimeo.com/288152860" />
      <attachment type="presentation" hash="a170e3d2">P18-1117.Presentation.pdf</attachment>
      <doi>10.18653/v1/P18-1117</doi>
      <bibkey>voita-etal-2018-context</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/opensubtitles">OpenSubtitles</pwcdataset>
    </paper>
    <paper id="118">
      <title>Document Context Neural Machine Translation with Memory Networks</title>
      <author><first>Sameen</first><last>Maruf</last></author>
      <author><first>Gholamreza</first><last>Haffari</last></author>
      <pages>1275&#8211;1284</pages>
      <abstract>We present a document-level neural machine translation model which takes both source and target document context into account using memory networks. We model the problem as a structured prediction problem with interdependencies among the observed and hidden variables, i.e., the source sentences and their unobserved target translations in the document. The resulting structured prediction problem is tackled with a neural translation model equipped with two memory components, one each for the source and target side, to capture the documental interdependencies. We train the model end-to-end, and propose an iterative decoding algorithm based on block coordinate descent. Experimental results of English translations from French, German, and Estonian documents show that our model is effective in exploiting both source and target document context, and statistically significantly outperforms the previous work in terms of BLEU and METEOR.</abstract>
      <url hash="d06ccb47">P18-1118</url>
      <video href="https://vimeo.com/288152852" />
      <attachment type="presentation" hash="1f8713ac">P18-1118.Presentation.pdf</attachment>
      <doi>10.18653/v1/P18-1118</doi>
      <bibkey>maruf-haffari-2018-document</bibkey>
    </paper>
    <paper id="120">
      <title>Learning Prototypical Goal Activities for Locations</title>
      <author><first>Tianyu</first><last>Jiang</last></author>
      <author><first>Ellen</first><last>Riloff</last></author>
      <pages>1297&#8211;1307</pages>
      <abstract>People go to different places to engage in activities that reflect their goals. For example, people go to restaurants to eat, libraries to study, and churches to pray. We refer to an activity that represents a common reason why people typically go to a location as a prototypical goal activity (goal-act). Our research aims to learn goal-acts for specific locations using a text corpus and semi-supervised learning. First, we extract activities and locations that co-occur in goal-oriented syntactic patterns. Next, we create an activity profile matrix and apply a semi-supervised label propagation algorithm to iteratively revise the activity strengths for different locations using a small set of labeled data. We show that this approach outperforms several baseline methods when judged against goal-acts identified by human annotators.</abstract>
      <url hash="eabdfcec">P18-1120</url>
      <video href="https://vimeo.com/285803482" />
      <doi>10.18653/v1/P18-1120</doi>
      <bibkey>jiang-riloff-2018-learning</bibkey>
    </paper>
    <paper id="121">
      <title>Guess Me if You Can: Acronym Disambiguation for Enterprises</title>
      <author><first>Yang</first><last>Li</last></author>
      <author><first>Bo</first><last>Zhao</last></author>
      <author><first>Ariel</first><last>Fuxman</last></author>
      <author><first>Fangbo</first><last>Tao</last></author>
      <pages>1308&#8211;1317</pages>
      <abstract>Acronyms are abbreviations formed from the initial components of words or phrases. In enterprises, people often use acronyms to make communications more efficient. However, acronyms could be difficult to understand for people who are not familiar with the subject matter (new employees, etc.), thereby affecting productivity. To alleviate such troubles, we study how to automatically resolve the true meanings of acronyms in a given context. Acronym disambiguation for enterprises is challenging for several reasons. First, acronyms may be highly ambiguous since an acronym used in the enterprise could have multiple internal and external meanings. Second, there are usually no comprehensive knowledge bases such as Wikipedia available in enterprises. Finally, the system should be generic to work for any enterprise. In this work we propose an end-to-end framework to tackle all these challenges. The framework takes the enterprise corpus as input and produces a high-quality acronym disambiguation system as output. Our disambiguation models are trained via distant supervised learning, without requiring any manually labeled training examples. Therefore, our proposed framework can be deployed to any enterprise to support high-quality acronym disambiguation. Experimental results on real world data justified the effectiveness of our system.</abstract>
      <url hash="1cac7b2f">P18-1121</url>
      <video href="https://vimeo.com/285803502" />
      <doi>10.18653/v1/P18-1121</doi>
      <bibkey>li-etal-2018-guess</bibkey>
    </paper>
    <paper id="122">
      <title>A Multi-Axis Annotation Scheme for Event Temporal Relations</title>
      <author><first>Qiang</first><last>Ning</last></author>
      <author><first>Hao</first><last>Wu</last></author>
      <author><first>Dan</first><last>Roth</last></author>
      <pages>1318&#8211;1328</pages>
      <abstract>Existing temporal relation (TempRel) annotation schemes often have low inter-annotator agreements (IAA) even between experts, suggesting that the current annotation task needs a better definition. This paper proposes a new multi-axis modeling to better capture the temporal structure of events. In addition, we identify that event end-points are a major source of confusion in annotation, so we also propose to annotate TempRels based on start-points only. A pilot expert annotation effort using the proposed scheme shows significant improvement in IAA from the conventional 60&#8217;s to 80&#8217;s (Cohen&#8217;s Kappa). This better-defined annotation scheme further enables the use of crowdsourcing to alleviate the labor intensity for each annotator. We hope that this work can foster more interesting studies towards event understanding.</abstract>
      <url hash="36286775">P18-1122</url>
      <video href="https://vimeo.com/285803517" />
      <attachment type="presentation" hash="fc863ee3">P18-1122.Presentation.pdf</attachment>
      <doi>10.18653/v1/P18-1122</doi>
      <attachment type="notes" hash="da424c28">P18-1122.Notes.pdf</attachment>
      <bibkey>ning-etal-2018-multi</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/matres">MATRES</pwcdataset>
    </paper>
    <paper id="124">
      <title><fixed-case>D</fixed-case>ial<fixed-case>SQL</fixed-case>: Dialogue Based Structured Query Generation</title>
      <author><first>Izzeddin</first><last>Gur</last></author>
      <author><first>Semih</first><last>Yavuz</last></author>
      <author><first>Yu</first><last>Su</last></author>
      <author><first>Xifeng</first><last>Yan</last></author>
      <pages>1339&#8211;1349</pages>
      <abstract>The recent advance in deep learning and semantic parsing has significantly improved the translation accuracy of natural language questions to structured queries. However, further improvement of the existing approaches turns out to be quite challenging. Rather than solely relying on algorithmic innovations, in this work, we introduce DialSQL, a dialogue-based structured query generation framework that leverages human intelligence to boost the performance of existing algorithms via user interaction. DialSQL is capable of identifying potential errors in a generated SQL query and asking users for validation via simple multi-choice questions. User feedback is then leveraged to revise the query. We design a generic simulator to bootstrap synthetic training dialogues and evaluate the performance of DialSQL on the WikiSQL dataset. Using SQLNet as a black box query generation tool, DialSQL improves its performance from 61.3% to 69.0% using only 2.4 validation questions per dialogue.</abstract>
      <url hash="4f6f5044">P18-1124</url>
      <video href="https://vimeo.com/285803553" />
      <doi>10.18653/v1/P18-1124</doi>
      <bibkey>gur-etal-2018-dialsql</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/wikisql">WikiSQL</pwcdataset>
    </paper>
    <paper id="126">
      <title>Are <fixed-case>BLEU</fixed-case> and Meaning Representation in Opposition?</title>
      <author><first>Ond&#345;ej</first><last>C&#237;fka</last></author>
      <author><first>Ond&#345;ej</first><last>Bojar</last></author>
      <pages>1362&#8211;1371</pages>
      <abstract>One of possible ways of obtaining continuous-space sentence representations is by training neural machine translation (NMT) systems. The recent attention mechanism however removes the single point in the neural network from which the source sentence representation can be extracted. We propose several variations of the attentive NMT architecture bringing this meeting point back. Empirical evaluation suggests that the better the translation quality, the worse the learned sentence representations serve in a wide range of classification and similarity tasks.</abstract>
      <url hash="716d3b10">P18-1126</url>
      <attachment type="note" hash="e9e3aa09">P18-1126.Notes.pdf</attachment>
      <video href="https://vimeo.com/285803604" />
      <attachment type="presentation" hash="ad3c314b">P18-1126.Presentation.pdf</attachment>
      <doi>10.18653/v1/P18-1126</doi>
      <bibkey>cifka-bojar-2018-bleu</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/coco">COCO</pwcdataset>
    </paper>
    <paper id="128">
      <title>The Hitchhiker&#8217;s Guide to Testing Statistical Significance in Natural Language Processing</title>
      <author><first>Rotem</first><last>Dror</last></author>
      <author><first>Gili</first><last>Baumer</last></author>
      <author><first>Segev</first><last>Shlomov</last></author>
      <author><first>Roi</first><last>Reichart</last></author>
      <pages>1383&#8211;1392</pages>
      <abstract>Statistical significance testing is a standard statistical tool designed to ensure that experimental results are not coincidental. In this opinion/ theoretical paper we discuss the role of statistical significance testing in Natural Language Processing (NLP) research. We establish the fundamental concepts of significance testing and discuss the specific aspects of NLP tasks, experimental setups and evaluation measures that affect the choice of significance tests in NLP research. Based on this discussion we propose a simple practical protocol for statistical significance test selection in NLP setups and accompany this protocol with a brief survey of the most relevant tests. We then survey recent empirical papers published in ACL and TACL during 2017 and show that while our community assigns great value to experimental results, statistical significance testing is often ignored or misused. We conclude with a brief discussion of open issues that should be properly addressed so that this important tool can be applied. in NLP research in a statistically sound manner.</abstract>
      <url hash="218b47e3">P18-1128</url>
      <video href="https://vimeo.com/285803636" />
      <attachment type="presentation" hash="65448f0c">P18-1128.Presentation.pdf</attachment>
      <doi>10.18653/v1/P18-1128</doi>
      <bibkey>dror-etal-2018-hitchhikers</bibkey>
      <pwccode url="https://github.com/rtmdrr/testSignificanceNLP" additional="false">rtmdrr/testSignificanceNLP</pwccode>
    </paper>
    <paper id="130">
      <title>Stack-Pointer Networks for Dependency Parsing</title>
      <author><first>Xuezhe</first><last>Ma</last></author>
      <author><first>Zecong</first><last>Hu</last></author>
      <author><first>Jingzhou</first><last>Liu</last></author>
      <author><first>Nanyun</first><last>Peng</last></author>
      <author><first>Graham</first><last>Neubig</last></author>
      <author><first>Eduard</first><last>Hovy</last></author>
      <pages>1403&#8211;1414</pages>
      <abstract>We introduce a novel architecture for dependency parsing: stack-pointer networks (StackPtr). Combining pointer networks (Vinyals et al., 2015) with an internal stack, the proposed model first reads and encodes the whole sentence, then builds the dependency tree top-down (from root-to-leaf) in a depth-first fashion. The stack tracks the status of the depth-first search and the pointer networks select one child for the word at the top of the stack at each step. The StackPtr parser benefits from the information of whole sentence and all previously derived subtree structures, and removes the left-to-right restriction in classical transition-based parsers. Yet the number of steps for building any (non-projective) parse tree is linear in the length of the sentence just as other transition-based parsers, yielding an efficient decoding algorithm with <tex-math>O(n^2)</tex-math> time complexity. We evaluate our model on 29 treebanks spanning 20 languages and different dependency annotation schemas, and achieve state-of-the-art performances on 21 of them</abstract>
      <url hash="d8e9fb26">P18-1130</url>
      <attachment type="note" hash="d32d7eab">P18-1130.Notes.zip</attachment>
      <video href="https://vimeo.com/285803695" />
      <attachment type="presentation" hash="71eaa359">P18-1130.Presentation.pdf</attachment>
      <doi>10.18653/v1/P18-1130</doi>
      <bibkey>ma-etal-2018-stack</bibkey>
      <pwccode url="https://github.com/XuezheMax/NeuroNLP2" additional="true">XuezheMax/NeuroNLP2</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/penn-treebank">Penn Treebank</pwcdataset>
    </paper>
    <paper id="131">
      <title><fixed-case>T</fixed-case>witter <fixed-case>U</fixed-case>niversal <fixed-case>D</fixed-case>ependency Parsing for <fixed-case>A</fixed-case>frican-<fixed-case>A</fixed-case>merican and Mainstream <fixed-case>A</fixed-case>merican <fixed-case>E</fixed-case>nglish</title>
      <author><first>Su Lin</first><last>Blodgett</last></author>
      <author><first>Johnny</first><last>Wei</last></author>
      <author><first>Brendan</first><last>O&#8217;Connor</last></author>
      <pages>1415&#8211;1425</pages>
      <abstract>Due to the presence of both Twitter-specific conventions and non-standard and dialectal language, Twitter presents a significant parsing challenge to current dependency parsing tools. We broaden English dependency parsing to handle social media English, particularly social media African-American English (AAE), by developing and annotating a new dataset of 500 tweets, 250 of which are in AAE, within the Universal Dependencies 2.0 framework. We describe our standards for handling Twitter- and AAE-specific features and evaluate a variety of cross-domain strategies for improving parsing with no, or very little, in-domain labeled data, including a new data synthesis approach. We analyze these methods&#8217; impact on performance disparities between AAE and Mainstream American English tweets, and assess parsing accuracy for specific AAE lexical and syntactic features. Our annotated data and a parsing model are available at: <url>http://slanglab.cs.umass.edu/TwitterAAE/</url>.</abstract>
      <url hash="5e63686d">P18-1131</url>
      <video href="https://vimeo.com/285803712" />
      <doi>10.18653/v1/P18-1131</doi>
      <bibkey>blodgett-etal-2018-twitter</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/universal-dependencies">Universal Dependencies</pwcdataset>
    </paper>
    <paper id="132">
      <title><fixed-case>LSTM</fixed-case>s Can Learn Syntax-Sensitive Dependencies Well, But Modeling Structure Makes Them Better</title>
      <author><first>Adhiguna</first><last>Kuncoro</last></author>
      <author><first>Chris</first><last>Dyer</last></author>
      <author><first>John</first><last>Hale</last></author>
      <author><first>Dani</first><last>Yogatama</last></author>
      <author><first>Stephen</first><last>Clark</last></author>
      <author><first>Phil</first><last>Blunsom</last></author>
      <pages>1426&#8211;1436</pages>
      <abstract>Language exhibits hierarchical structure, but recent work using a subject-verb agreement diagnostic argued that state-of-the-art language models, LSTMs, fail to learn long-range syntax sensitive dependencies. Using the same diagnostic, we show that, in fact, LSTMs do succeed in learning such dependencies&#8212;provided they have enough capacity. We then explore whether models that have access to explicit syntactic information learn agreement more effectively, and how the way in which this structural information is incorporated into the model impacts performance. We find that the mere presence of syntactic information does not improve accuracy, but when model architecture is determined by syntax, number agreement is improved. Further, we find that the choice of how syntactic structure is built affects how well number agreement is learned: top-down construction outperforms left-corner and bottom-up variants in capturing non-local structural dependencies.</abstract>
      <url hash="c089094b">P18-1132</url>
      <video href="https://vimeo.com/285803729" />
      <attachment type="presentation" hash="e7116c52">P18-1132.Presentation.pdf</attachment>
      <doi>10.18653/v1/P18-1132</doi>
      <bibkey>kuncoro-etal-2018-lstms</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/penn-treebank">Penn Treebank</pwcdataset>
    </paper>
    <paper id="133">
      <title><fixed-case>S</fixed-case>equicity: Simplifying Task-oriented Dialogue Systems with Single Sequence-to-Sequence Architectures</title>
      <author><first>Wenqiang</first><last>Lei</last></author>
      <author><first>Xisen</first><last>Jin</last></author>
      <author><first>Min-Yen</first><last>Kan</last></author>
      <author><first>Zhaochun</first><last>Ren</last></author>
      <author><first>Xiangnan</first><last>He</last></author>
      <author><first>Dawei</first><last>Yin</last></author>
      <pages>1437&#8211;1447</pages>
      <abstract>Existing solutions to task-oriented dialogue systems follow pipeline designs which introduces architectural complexity and fragility. We propose a novel, holistic, extendable framework based on a single sequence-to-sequence (seq2seq) model which can be optimized with supervised or reinforcement learning. A key contribution is that we design text spans named belief spans to track dialogue believes, allowing task-oriented dialogue systems to be modeled in a seq2seq way. Based on this, we propose a simplistic Two Stage CopyNet instantiation which emonstrates good scalability: significantly reducing model complexity in terms of number of parameters and training time by a magnitude. It significantly outperforms state-of-the-art pipeline-based methods on large datasets and retains a satisfactory entity match rate on out-of-vocabulary (OOV) cases where pipeline-designed competitors totally fail.</abstract>
      <url hash="2e8cd0d9">P18-1133</url>
      <attachment type="poster" hash="f4812129">P18-1133.Poster.pdf</attachment>
      <doi>10.18653/v1/P18-1133</doi>
      <bibkey>lei-etal-2018-sequicity</bibkey>
      <pwccode url="https://github.com/WING-NUS/sequicity" additional="false">WING-NUS/sequicity</pwccode>
    </paper>
    <paper id="136">
      <title><fixed-case>M</fixed-case>em2<fixed-case>S</fixed-case>eq: Effectively Incorporating Knowledge Bases into End-to-End Task-Oriented Dialog Systems</title>
      <author><first>Andrea</first><last>Madotto</last></author>
      <author><first>Chien-Sheng</first><last>Wu</last></author>
      <author><first>Pascale</first><last>Fung</last></author>
      <pages>1468&#8211;1478</pages>
      <abstract>End-to-end task-oriented dialog systems usually suffer from the challenge of incorporating knowledge bases. In this paper, we propose a novel yet simple end-to-end differentiable model called memory-to-sequence (Mem2Seq) to address this issue. Mem2Seq is the first neural generative model that combines the multi-hop attention over memories with the idea of pointer network. We empirically show how Mem2Seq controls each generation step, and how its multi-hop attention mechanism helps in learning correlations between memories. In addition, our model is quite general without complicated task-specific designs. As a result, we show that Mem2Seq can be trained faster and attain the state-of-the-art performance on three different task-oriented dialog datasets.</abstract>
      <url hash="62ed4a18">P18-1136</url>
      <attachment type="note" hash="0ce2f0d5">P18-1136.Notes.zip</attachment>
      <attachment type="poster" hash="472baeb2">P18-1136.Poster.pdf</attachment>
      <doi>10.18653/v1/P18-1136</doi>
      <bibkey>madotto-etal-2018-mem2seq</bibkey>
      <pwccode url="https://github.com/HLTCHKUST/Mem2Seq" additional="false">HLTCHKUST/Mem2Seq</pwccode>
    </paper>
    <paper id="137">
      <title>Tailored Sequence to Sequence Models to Different Conversation Scenarios</title>
      <author><first>Hainan</first><last>Zhang</last></author>
      <author><first>Yanyan</first><last>Lan</last></author>
      <author><first>Jiafeng</first><last>Guo</last></author>
      <author><first>Jun</first><last>Xu</last></author>
      <author><first>Xueqi</first><last>Cheng</last></author>
      <pages>1479&#8211;1488</pages>
      <abstract>Sequence to sequence (Seq2Seq) models have been widely used for response generation in the area of conversation. However, the requirements for different conversation scenarios are distinct. For example, customer service requires the generated responses to be specific and accurate, while chatbot prefers diverse responses so as to attract different users. The current Seq2Seq model fails to meet these diverse requirements, by using a general average likelihood as the optimization criteria. As a result, it usually generates safe and commonplace responses, such as &#8216;I don&#8217;t know&#8217;. In this paper, we propose two tailored optimization criteria for Seq2Seq to different conversation scenarios, i.e., the maximum generated likelihood for specific-requirement scenario, and the conditional value-at-risk for diverse-requirement scenario. Experimental results on the Ubuntu dialogue corpus (Ubuntu service scenario) and Chinese Weibo dataset (social chatbot scenario) show that our proposed models not only satisfies diverse requirements for different scenarios, but also yields better performances against traditional Seq2Seq models in terms of both metric-based and human evaluations.</abstract>
      <url hash="6f641bd6">P18-1137</url>
      <attachment type="poster" hash="53b19f7b">P18-1137.Poster.pdf</attachment>
      <doi>10.18653/v1/P18-1137</doi>
      <bibkey>zhang-etal-2018-tailored</bibkey>
    </paper>
    <paper id="140">
      <title>Sentiment Adaptive End-to-End Dialog Systems</title>
      <author><first>Weiyan</first><last>Shi</last></author>
      <author><first>Zhou</first><last>Yu</last></author>
      <pages>1509&#8211;1519</pages>
      <abstract>End-to-end learning framework is useful for building dialog systems for its simplicity in training and efficiency in model updating. However, current end-to-end approaches only consider user semantic inputs in learning and under-utilize other user information. Therefore, we propose to include user sentiment obtained through multimodal information (acoustic, dialogic and textual), in the end-to-end learning framework to make systems more user-adaptive and effective. We incorporated user sentiment information in both supervised and reinforcement learning settings. In both settings, adding sentiment information reduced the dialog length and improved the task success rate on a bus information search task. This work is the first attempt to incorporate multimodal user information in the adaptive end-to-end dialog system training framework and attained state-of-the-art performance.</abstract>
      <url hash="98c0c5a3">P18-1140</url>
      <attachment type="note" hash="e92aef6b">P18-1140.Notes.pdf</attachment>
      <attachment type="poster" hash="3d17eec6">P18-1140.Poster.pdf</attachment>
      <doi>10.18653/v1/P18-1140</doi>
      <bibkey>shi-yu-2018-sentiment</bibkey>
    </paper>
    <paper id="141">
      <title>Embedding Learning Through Multilingual Concept Induction</title>
      <author><first>Philipp</first><last>Dufter</last></author>
      <author><first>Mengjie</first><last>Zhao</last></author>
      <author><first>Martin</first><last>Schmitt</last></author>
      <author><first>Alexander</first><last>Fraser</last></author>
      <author><first>Hinrich</first><last>Sch&#252;tze</last></author>
      <pages>1520&#8211;1530</pages>
      <abstract>We present a new method for estimating vector space representations of words: embedding learning by concept induction. We test this method on a highly parallel corpus and learn semantic representations of words in 1259 different languages in a single common space. An extensive experimental evaluation on crosslingual word similarity and sentiment analysis indicates that concept-based multilingual embedding learning performs better than previous approaches.</abstract>
      <url hash="c51983ff">P18-1141</url>
      <attachment type="poster" hash="9df0b544">P18-1141.Poster.pdf</attachment>
      <doi>10.18653/v1/P18-1141</doi>
      <bibkey>dufter-etal-2018-embedding</bibkey>
    </paper>
    <paper id="143">
      <title>Language Modeling for Code-Mixing: The Role of Linguistic Theory based Synthetic Data</title>
      <author><first>Adithya</first><last>Pratapa</last></author>
      <author><first>Gayatri</first><last>Bhat</last></author>
      <author><first>Monojit</first><last>Choudhury</last></author>
      <author><first>Sunayana</first><last>Sitaram</last></author>
      <author><first>Sandipan</first><last>Dandapat</last></author>
      <author><first>Kalika</first><last>Bali</last></author>
      <pages>1543&#8211;1553</pages>
      <abstract>Training language models for Code-mixed (CM) language is known to be a difficult problem because of lack of data compounded by the increased confusability due to the presence of more than one language. We present a computational technique for creation of grammatically valid artificial CM data based on the Equivalence Constraint Theory. We show that when training examples are sampled appropriately from this synthetic data and presented in certain order (aka training curriculum) along with monolingual and real CM data, it can significantly reduce the perplexity of an RNN-based language model. We also show that randomly generated CM data does not help in decreasing the perplexity of the LMs.</abstract>
      <url hash="3db4548c">P18-1143</url>
      <attachment type="poster" hash="25427e2d">P18-1143.Poster.pdf</attachment>
      <doi>10.18653/v1/P18-1143</doi>
      <bibkey>pratapa-etal-2018-language</bibkey>
    </paper>
    <paper id="148">
      <title>Improving Entity Linking by Modeling Latent Relations between Mentions</title>
      <author><first>Phong</first><last>Le</last></author>
      <author><first>Ivan</first><last>Titov</last></author>
      <pages>1595&#8211;1604</pages>
      <abstract>Entity linking involves aligning textual mentions of named entities to their corresponding entries in a knowledge base. Entity linking systems often exploit relations between textual mentions in a document (e.g., coreference) to decide if the linking decisions are compatible. Unlike previous approaches, which relied on supervised systems or heuristics to predict these relations, we treat relations as latent variables in our neural entity-linking model. We induce the relations without any supervision while optimizing the entity-linking system in an end-to-end fashion. Our multi-relational model achieves the best reported scores on the standard benchmark (AIDA-CoNLL) and substantially outperforms its relation-agnostic version. Its training also converges much faster, suggesting that the injected structural bias helps to explain regularities in the training data.</abstract>
      <url hash="a2903fef">P18-1148</url>
      <attachment type="poster" hash="e1a13300">P18-1148.Poster.pdf</attachment>
      <doi>10.18653/v1/P18-1148</doi>
      <bibkey>le-titov-2018-improving</bibkey>
      <pwccode url="https://github.com/lephong/mulrel-nel" additional="true">lephong/mulrel-nel</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/aida-conll-yago">AIDA CoNLL-YAGO</pwcdataset>
    </paper>
    <paper id="149">
      <title>Dating Documents using Graph Convolution Networks</title>
      <author><first>Shikhar</first><last>Vashishth</last></author>
      <author><first>Shib Sankar</first><last>Dasgupta</last></author>
      <author><first>Swayambhu Nath</first><last>Ray</last></author>
      <author><first>Partha</first><last>Talukdar</last></author>
      <pages>1605&#8211;1615</pages>
      <abstract>Document date is essential for many important tasks, such as document retrieval, summarization, event detection, etc. While existing approaches for these tasks assume accurate knowledge of the document date, this is not always available, especially for arbitrary documents from the Web. Document Dating is a challenging problem which requires inference over the temporal structure of the document. Prior document dating systems have largely relied on handcrafted features while ignoring such document-internal structures. In this paper, we propose NeuralDater, a Graph Convolutional Network (GCN) based document dating approach which jointly exploits syntactic and temporal graph structures of document in a principled way. To the best of our knowledge, this is the first application of deep learning for the problem of document dating. Through extensive experiments on real-world datasets, we find that NeuralDater significantly outperforms state-of-the-art baseline by 19% absolute (45% relative) accuracy points.</abstract>
      <url hash="2b0827e7">P18-1149</url>
      <attachment type="poster" hash="b06e97d6">P18-1149.Poster.pdf</attachment>
      <doi>10.18653/v1/P18-1149</doi>
      <bibkey>vashishth-etal-2018-dating</bibkey>
      <pwccode url="https://github.com/malllabiisc/NeuralDater" additional="false">malllabiisc/NeuralDater</pwccode>
    </paper>
    <paper id="150">
      <title>A Graph-to-Sequence Model for <fixed-case>AMR</fixed-case>-to-Text Generation</title>
      <author><first>Linfeng</first><last>Song</last></author>
      <author><first>Yue</first><last>Zhang</last></author>
      <author><first>Zhiguo</first><last>Wang</last></author>
      <author><first>Daniel</first><last>Gildea</last></author>
      <pages>1616&#8211;1626</pages>
      <abstract>The problem of AMR-to-text generation is to recover a text representing the same meaning as an input AMR graph. The current state-of-the-art method uses a sequence-to-sequence model, leveraging LSTM for encoding a linearized AMR structure. Although being able to model non-local semantic information, a sequence LSTM can lose information from the AMR graph structure, and thus facing challenges with large-graphs, which result in long sequences. We introduce a neural graph-to-sequence model, using a novel LSTM structure for directly encoding graph-level semantics. On a standard benchmark, our model shows superior results to existing methods in the literature.</abstract>
      <url hash="d7756eee">P18-1150</url>
      <attachment type="poster" hash="9a59ef3f">P18-1150.Poster.pdf</attachment>
      <doi>10.18653/v1/P18-1150</doi>
      <bibkey>song-etal-2018-graph</bibkey>
      <pwccode url="https://github.com/freesunshine0316/neural-graph-to-seq-mp" additional="false">freesunshine0316/neural-graph-to-seq-mp</pwccode>
    </paper>
    <paper id="152">
      <title>Learning to Write with Cooperative Discriminators</title>
      <author><first>Ari</first><last>Holtzman</last></author>
      <author><first>Jan</first><last>Buys</last></author>
      <author><first>Maxwell</first><last>Forbes</last></author>
      <author><first>Antoine</first><last>Bosselut</last></author>
      <author><first>David</first><last>Golub</last></author>
      <author><first>Yejin</first><last>Choi</last></author>
      <pages>1638&#8211;1649</pages>
      <abstract>Despite their local fluency, long-form text generated from RNNs is often generic, repetitive, and even self-contradictory. We propose a unified learning framework that collectively addresses all the above issues by composing a committee of discriminators that can guide a base RNN generator towards more globally coherent generations. More concretely, discriminators each specialize in a different principle of communication, such as Grice&#8217;s maxims, and are collectively combined with the base RNN generator through a composite decoding objective. Human evaluation demonstrates that text generated by our model is preferred over that of baselines by a large margin, significantly enhancing the overall coherence, style, and information of the generations.</abstract>
      <url hash="f03f8fca">P18-1152</url>
      <attachment type="note" hash="f5f38790">P18-1152.Notes.pdf</attachment>
      <attachment type="poster" hash="d08d7c44">P18-1152.Poster.pdf</attachment>
      <doi>10.18653/v1/P18-1152</doi>
      <bibkey>holtzman-etal-2018-learning</bibkey>
      <pwccode url="" additional="true" />
      <pwcdataset url="https://paperswithcode.com/dataset/bookcorpus">BookCorpus</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
    </paper>
    <paper id="153">
      <title>A Neural Approach to Pun Generation</title>
      <author><first>Zhiwei</first><last>Yu</last></author>
      <author><first>Jiwei</first><last>Tan</last></author>
      <author><first>Xiaojun</first><last>Wan</last></author>
      <pages>1650&#8211;1660</pages>
      <abstract>Automatic pun generation is an interesting and challenging text generation task. Previous efforts rely on templates or laboriously manually annotated pun datasets, which heavily constrains the quality and diversity of generated puns. Since sequence-to-sequence models provide an effective technique for text generation, it is promising to investigate these models on the pun generation task. In this paper, we propose neural network models for homographic pun generation, and they can generate puns without requiring any pun data for training. We first train a conditional neural language model from a general text corpus, and then generate puns from the language model with an elaborately designed decoding algorithm. Automatic and human evaluations show that our models are able to generate homographic puns of good readability and quality.</abstract>
      <url hash="2f251a9b">P18-1153</url>
      <attachment type="poster" hash="1efcef19">P18-1153.Poster.pdf</attachment>
      <doi>10.18653/v1/P18-1153</doi>
      <bibkey>yu-etal-2018-neural</bibkey>
    </paper>
    <paper id="154">
      <title>Learning to Generate Move-by-Move Commentary for Chess Games from Large-Scale Social Forum Data</title>
      <author><first>Harsh</first><last>Jhamtani</last></author>
      <author><first>Varun</first><last>Gangal</last></author>
      <author><first>Eduard</first><last>Hovy</last></author>
      <author><first>Graham</first><last>Neubig</last></author>
      <author><first>Taylor</first><last>Berg-Kirkpatrick</last></author>
      <pages>1661&#8211;1671</pages>
      <abstract>This paper examines the problem of generating natural language descriptions of chess games. We introduce a new large-scale chess commentary dataset and propose methods to generate commentary for individual moves in a chess game. The introduced dataset consists of more than 298K chess move-commentary pairs across 11K chess games. We highlight how this task poses unique research challenges in natural language generation: the data contain a large variety of styles of commentary and frequently depend on pragmatic context. We benchmark various baselines and propose an end-to-end trainable neural model which takes into account multiple pragmatic aspects of the game state that may be commented upon to describe a given chess move. Through a human study on predictions for a subset of the data which deals with direct move descriptions, we observe that outputs from our models are rated similar to ground truth commentary texts in terms of correctness and fluency.</abstract>
      <url hash="bb90921d">P18-1154</url>
      <attachment type="note" hash="c93df42b">P18-1154.Notes.pdf</attachment>
      <doi>10.18653/v1/P18-1154</doi>
      <bibkey>jhamtani-etal-2018-learning</bibkey>
      <pwccode url="https://github.com/harsh19/ChessCommentaryGeneration" additional="false">harsh19/ChessCommentaryGeneration</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/robocup">RoboCup</pwcdataset>
    </paper>
    <paper id="155">
      <title>From Credit Assignment to Entropy Regularization: Two New Algorithms for Neural Sequence Prediction</title>
      <author><first>Zihang</first><last>Dai</last></author>
      <author><first>Qizhe</first><last>Xie</last></author>
      <author><first>Eduard</first><last>Hovy</last></author>
      <pages>1672&#8211;1682</pages>
      <abstract>In this work, we study the credit assignment problem in reward augmented maximum likelihood (RAML) learning, and establish a theoretical equivalence between the token-level counterpart of RAML and the entropy regularized reinforcement learning. Inspired by the connection, we propose two sequence prediction algorithms, one extending RAML with fine-grained credit assignment and the other improving Actor-Critic with a systematic entropy regularization. On two benchmark datasets, we show the proposed algorithms outperform RAML and Actor-Critic respectively, providing new alternatives to sequence prediction.</abstract>
      <url hash="29ae0821">P18-1155</url>
      <attachment type="note" hash="c2f87f4a">P18-1155.Notes.pdf</attachment>
      <attachment type="poster" hash="4889b3a6">P18-1155.Poster.pdf</attachment>
      <doi>10.18653/v1/P18-1155</doi>
      <bibkey>dai-etal-2018-credit</bibkey>
      <pwccode url="https://github.com/zihangdai/ERAC-VAML" additional="false">zihangdai/ERAC-VAML</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/coco">COCO</pwcdataset>
    </paper>
    <paper id="156">
      <title><fixed-case>D</fixed-case>uo<fixed-case>RC</fixed-case>: Towards Complex Language Understanding with Paraphrased Reading Comprehension</title>
      <author><first>Amrita</first><last>Saha</last></author>
      <author><first>Rahul</first><last>Aralikatte</last></author>
      <author><first>Mitesh M.</first><last>Khapra</last></author>
      <author><first>Karthik</first><last>Sankaranarayanan</last></author>
      <pages>1683&#8211;1693</pages>
      <abstract>We propose DuoRC, a novel dataset for Reading Comprehension (RC) that motivates several new challenges for neural approaches in language understanding beyond those offered by existing RC datasets. DuoRC contains 186,089 unique question-answer pairs created from a collection of 7680 pairs of movie plots where each pair in the collection reflects two versions of the same movie - one from Wikipedia and the other from IMDb - written by two different authors. We asked crowdsourced workers to create questions from one version of the plot and a different set of workers to extract or synthesize answers from the other version. This unique characteristic of DuoRC where questions and answers are created from different versions of a document narrating the same underlying story, ensures by design, that there is very little lexical overlap between the questions created from one version and the segments containing the answer in the other version. Further, since the two versions have different levels of plot detail, narration style, vocabulary, etc., answering questions from the second version requires deeper language understanding and incorporating external background knowledge. Additionally, the narrative style of passages arising from movie plots (as opposed to typical descriptive passages in existing datasets) exhibits the need to perform complex reasoning over events across multiple sentences. Indeed, we observe that state-of-the-art neural RC models which have achieved near human performance on the SQuAD dataset, even when coupled with traditional NLP techniques to address the challenges presented in DuoRC exhibit very poor performance (F1 score of 37.42% on DuoRC v/s 86% on SQuAD dataset). This opens up several interesting research avenues wherein DuoRC could complement other RC datasets to explore novel neural approaches for studying language understanding.</abstract>
      <url hash="bc8fe943">P18-1156</url>
      <attachment type="note" hash="303960eb">P18-1156.Notes.pdf</attachment>
      <doi>10.18653/v1/P18-1156</doi>
      <bibkey>saha-etal-2018-duorc</bibkey>
      <pwccode url="https://github.com/duorc/duorc" additional="false">duorc/duorc</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/duorc">DuoRC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/ms-marco">MS MARCO</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/movieqa">MovieQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/newsqa">NewsQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/triviaqa">TriviaQA</pwcdataset>
    </paper>
    <paper id="157">
      <title>Stochastic Answer Networks for Machine Reading Comprehension</title>
      <author><first>Xiaodong</first><last>Liu</last></author>
      <author><first>Yelong</first><last>Shen</last></author>
      <author><first>Kevin</first><last>Duh</last></author>
      <author><first>Jianfeng</first><last>Gao</last></author>
      <pages>1694&#8211;1704</pages>
      <abstract>We propose a simple yet robust stochastic answer network (SAN) that simulates multi-step reasoning in machine reading comprehension. Compared to previous work such as ReasoNet which used reinforcement learning to determine the number of steps, the unique feature is the use of a kind of stochastic prediction dropout on the answer module (final layer) of the neural network during the training. We show that this simple trick improves robustness and achieves results competitive to the state-of-the-art on the Stanford Question Answering Dataset (SQuAD), the Adversarial SQuAD, and the Microsoft MAchine Reading COmprehension Dataset (MS MARCO).</abstract>
      <url hash="621e73b1">P18-1157</url>
      <attachment type="poster" hash="49fa4b4c">P18-1157.Poster.pdf</attachment>
      <doi>10.18653/v1/P18-1157</doi>
      <bibkey>liu-etal-2018-stochastic</bibkey>
      <pwccode url="" additional="true" />
      <pwcdataset url="https://paperswithcode.com/dataset/ms-marco">MS MARCO</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
    </paper>
    <paper id="159">
      <title>Joint Training of Candidate Extraction and Answer Selection for Reading Comprehension</title>
      <author><first>Zhen</first><last>Wang</last></author>
      <author><first>Jiachen</first><last>Liu</last></author>
      <author><first>Xinyan</first><last>Xiao</last></author>
      <author><first>Yajuan</first><last>Lyu</last></author>
      <author><first>Tian</first><last>Wu</last></author>
      <pages>1715&#8211;1724</pages>
      <abstract>While sophisticated neural-based techniques have been developed in reading comprehension, most approaches model the answer in an independent manner, ignoring its relations with other answer candidates. This problem can be even worse in open-domain scenarios, where candidates from multiple passages should be combined to answer a single question. In this paper, we formulate reading comprehension as an extract-then-select two-stage procedure. We first extract answer candidates from passages, then select the final answer by combining information from all the candidates. Furthermore, we regard candidate extraction as a latent variable and train the two-stage process jointly with reinforcement learning. As a result, our approach has improved the state-of-the-art performance significantly on two challenging open-domain reading comprehension datasets. Further analysis demonstrates the effectiveness of our model components, especially the information fusion of all the candidates and the joint training of the extract-then-select procedure.</abstract>
      <url hash="7d3fa636">P18-1159</url>
      <doi>10.18653/v1/P18-1159</doi>
      <bibkey>wang-etal-2018-joint</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/quasar-1">QUASAR</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/quasar-t">QUASAR-T</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/searchqa">SearchQA</pwcdataset>
    </paper>
    <paper id="160">
      <title>Efficient and Robust Question Answering from Minimal Context over Documents</title>
      <author><first>Sewon</first><last>Min</last></author>
      <author><first>Victor</first><last>Zhong</last></author>
      <author><first>Richard</first><last>Socher</last></author>
      <author><first>Caiming</first><last>Xiong</last></author>
      <pages>1725&#8211;1735</pages>
      <abstract>Neural models for question answering (QA) over documents have achieved significant performance improvements. Although effective, these models do not scale to large corpora due to their complex modeling of interactions between the document and the question. Moreover, recent work has shown that such models are sensitive to adversarial inputs. In this paper, we study the minimal context required to answer the question, and find that most questions in existing datasets can be answered with a small set of sentences. Inspired by this observation, we propose a simple sentence selector to select the minimal set of sentences to feed into the QA model. Our overall system achieves significant reductions in training (up to 15 times) and inference times (up to 13 times), with accuracy comparable to or better than the state-of-the-art on SQuAD, NewsQA, TriviaQA and SQuAD-Open. Furthermore, our experimental results and analyses show that our approach is more robust to adversarial inputs.</abstract>
      <url hash="b23f8e0b">P18-1160</url>
      <attachment type="note" hash="8403d897">P18-1160.Notes.pdf</attachment>
      <attachment type="poster" hash="08666191">P18-1160.Poster.pdf</attachment>
      <doi>10.18653/v1/P18-1160</doi>
      <bibkey>min-etal-2018-efficient</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/newsqa">NewsQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/triviaqa">TriviaQA</pwcdataset>
    </paper>
    <paper id="163">
      <title>Towards Robust Neural Machine Translation</title>
      <author><first>Yong</first><last>Cheng</last></author>
      <author><first>Zhaopeng</first><last>Tu</last></author>
      <author><first>Fandong</first><last>Meng</last></author>
      <author><first>Junjie</first><last>Zhai</last></author>
      <author id="yang-liu-ict"><first>Yang</first><last>Liu</last></author>
      <pages>1756&#8211;1766</pages>
      <abstract>Small perturbations in the input can severely distort intermediate representations and thus impact translation quality of neural machine translation (NMT) models. In this paper, we propose to improve the robustness of NMT models with adversarial stability training. The basic idea is to make both the encoder and decoder in NMT models robust against input perturbations by enabling them to behave similarly for the original input and its perturbed counterpart. Experimental results on Chinese-English, English-German and English-French translation tasks show that our approaches can not only achieve significant improvements over strong NMT systems but also improve the robustness of NMT models.</abstract>
      <url hash="1ef05b3f">P18-1163</url>
      <attachment type="poster" hash="67dbd8df">P18-1163.Poster.pdf</attachment>
      <doi>10.18653/v1/P18-1163</doi>
      <bibkey>cheng-etal-2018-towards</bibkey>
    </paper>
    <paper id="165">
      <title>Reliability and Learnability of Human Bandit Feedback for Sequence-to-Sequence Reinforcement Learning</title>
      <author><first>Julia</first><last>Kreutzer</last></author>
      <author><first>Joshua</first><last>Uyheng</last></author>
      <author><first>Stefan</first><last>Riezler</last></author>
      <pages>1777&#8211;1788</pages>
      <abstract>We present a study on reinforcement learning (RL) from human bandit feedback for sequence-to-sequence learning, exemplified by the task of bandit neural machine translation (NMT). We investigate the reliability of human bandit feedback, and analyze the influence of reliability on the learnability of a reward estimator, and the effect of the quality of reward estimates on the overall RL task. Our analysis of cardinal (5-point ratings) and ordinal (pairwise preferences) feedback shows that their intra- and inter-annotator &#945;-agreement is comparable. Best reliability is obtained for standardized cardinal feedback, and cardinal feedback is also easiest to learn and generalize from. Finally, improvements of over 1 BLEU can be obtained by integrating a regression-based reward estimator trained on cardinal feedback for 800 translations into RL for NMT. This shows that RL is possible even from small amounts of fairly reliable human feedback, pointing to a great potential for applications at larger scale.</abstract>
      <url hash="0014d6fe">P18-1165</url>
      <attachment type="note" hash="23a2f450">P18-1165.Notes.pdf</attachment>
      <attachment type="poster" hash="6acbfbdc">P18-1165.Poster.pdf</attachment>
      <doi>10.18653/v1/P18-1165</doi>
      <bibkey>kreutzer-etal-2018-reliability</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/humanmt">HumanMT</pwcdataset>
    </paper>
    <paper id="166">
      <title>Accelerating Neural Transformer via an Average Attention Network</title>
      <author><first>Biao</first><last>Zhang</last></author>
      <author><first>Deyi</first><last>Xiong</last></author>
      <author><first>Jinsong</first><last>Su</last></author>
      <pages>1789&#8211;1798</pages>
      <abstract>With parallelizable attention networks, the neural Transformer is very fast to train. However, due to the auto-regressive architecture and self-attention in the decoder, the decoding procedure becomes slow. To alleviate this issue, we propose an average attention network as an alternative to the self-attention network in the decoder of the neural Transformer. The average attention network consists of two layers, with an average layer that models dependencies on previous positions and a gating layer that is stacked over the average layer to enhance the expressiveness of the proposed attention network. We apply this network on the decoder part of the neural Transformer to replace the original target-side self-attention model. With masking tricks and dynamic programming, our model enables the neural Transformer to decode sentences over four times faster than its original version with almost no loss in training time and translation performance. We conduct a series of experiments on WMT17 translation tasks, where on 6 different language pairs, we obtain robust and consistent speed-ups in decoding.</abstract>
      <url hash="58caccb5">P18-1166</url>
      <attachment type="poster" hash="98201bc5">P18-1166.Poster.pdf</attachment>
      <doi>10.18653/v1/P18-1166</doi>
      <bibkey>zhang-etal-2018-accelerating</bibkey>
      <pwccode url="https://github.com/bzhangXMU/transformer-aan" additional="false">bzhangXMU/transformer-aan</pwccode>
    </paper>
    <paper id="167">
      <title>How Much Attention Do You Need? A Granular Analysis of Neural Machine Translation Architectures</title>
      <author><first>Tobias</first><last>Domhan</last></author>
      <pages>1799&#8211;1808</pages>
      <abstract>With recent advances in network architectures for Neural Machine Translation (NMT) recurrent models have effectively been replaced by either convolutional or self-attentional approaches, such as in the Transformer. While the main innovation of the Transformer architecture is its use of self-attentional layers, there are several other aspects, such as attention with multiple heads and the use of many attention layers, that distinguish the model from previous baselines. In this work we take a fine-grained look at the different architectures for NMT. We introduce an Architecture Definition Language (ADL) allowing for a flexible combination of common building blocks. Making use of this language we show in experiments that one can bring recurrent and convolutional models very close to the Transformer performance by borrowing concepts from the Transformer architecture, but not using self-attention. Additionally, we find that self-attention is much more important on the encoder side than on the decoder side, where it can be replaced by a RNN or CNN without a loss in performance in most settings. Surprisingly, even a model without any target side self-attention performs well.</abstract>
      <url hash="e1efa495">P18-1167</url>
      <attachment type="poster" hash="5c46ecd0">P18-1167.Poster.pdf</attachment>
      <doi>10.18653/v1/P18-1167</doi>
      <bibkey>domhan-2018-much</bibkey>
      <pwccode url="https://github.com/awslabs/sockeye" additional="false">awslabs/sockeye</pwccode>
    </paper>
    <paper id="168">
      <title>Weakly Supervised Semantic Parsing with Abstract Examples</title>
      <author><first>Omer</first><last>Goldman</last></author>
      <author><first>Veronica</first><last>Latcinnik</last></author>
      <author><first>Ehud</first><last>Nave</last></author>
      <author><first>Amir</first><last>Globerson</last></author>
      <author><first>Jonathan</first><last>Berant</last></author>
      <pages>1809&#8211;1819</pages>
      <abstract>Training semantic parsers from weak supervision (denotations) rather than strong supervision (programs) complicates training in two ways. First, a large search space of potential programs needs to be explored at training time to find a correct program. Second, spurious programs that accidentally lead to a correct denotation add noise to training. In this work we propose that in closed worlds with clear semantic types, one can substantially alleviate these problems by utilizing an abstract representation, where tokens in both the language utterance and program are lifted to an abstract form. We show that these abstractions can be defined with a handful of lexical rules and that they result in sharing between different examples that alleviates the difficulties in training. To test our approach, we develop the first semantic parser for CNLVR, a challenging visual reasoning dataset, where the search space is large and overcoming spuriousness is critical, because denotations are either TRUE or FALSE, and thus random programs are likely to lead to a correct denotation. Our method substantially improves performance, and reaches 82.5% accuracy, a 14.7% absolute accuracy improvement compared to the best reported accuracy so far.</abstract>
      <url hash="b5babb1e">P18-1168</url>
      <attachment type="note" hash="baccc7cb">P18-1168.Notes.pdf</attachment>
      <video href="https://vimeo.com/285804766" />
      <attachment type="presentation" hash="3cb92f75">P18-1168.Presentation.pdf</attachment>
      <doi>10.18653/v1/P18-1168</doi>
      <bibkey>goldman-etal-2018-weakly</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/clevr">CLEVR</pwcdataset>
    </paper>
    <paper id="169">
      <title>Improving a Neural Semantic Parser by Counterfactual Learning from Human Bandit Feedback</title>
      <author><first>Carolin</first><last>Lawrence</last></author>
      <author><first>Stefan</first><last>Riezler</last></author>
      <pages>1820&#8211;1830</pages>
      <abstract>Counterfactual learning from human bandit feedback describes a scenario where user feedback on the quality of outputs of a historic system is logged and used to improve a target system. We show how to apply this learning framework to neural semantic parsing. From a machine learning perspective, the key challenge lies in a proper reweighting of the estimator so as to avoid known degeneracies in counterfactual learning, while still being applicable to stochastic gradient optimization. To conduct experiments with human users, we devise an easy-to-use interface to collect human feedback on semantic parses. Our work is the first to show that semantic parsers can be improved significantly by counterfactual learning from logged human feedback data.</abstract>
      <url hash="0fd15fd7">P18-1169</url>
      <attachment type="note" hash="675806ad">P18-1169.Notes.pdf</attachment>
      <video href="https://vimeo.com/285804785" />
      <attachment type="presentation" hash="b88316ad">P18-1169.Presentation.pdf</attachment>
      <doi>10.18653/v1/P18-1169</doi>
      <bibkey>lawrence-riezler-2018-improving</bibkey>
      <pwccode url="https://github.com/carolinlawrence/nematus" additional="false">carolinlawrence/nematus</pwccode>
    </paper>
    <paper id="170">
      <title><fixed-case>AMR</fixed-case> dependency parsing with a typed semantic algebra</title>
      <author><first>Jonas</first><last>Groschwitz</last></author>
      <author><first>Matthias</first><last>Lindemann</last></author>
      <author><first>Meaghan</first><last>Fowlie</last></author>
      <author><first>Mark</first><last>Johnson</last></author>
      <author><first>Alexander</first><last>Koller</last></author>
      <pages>1831&#8211;1841</pages>
      <abstract>We present a semantic parser for Abstract Meaning Representations which learns to parse strings into tree representations of the compositional structure of an AMR graph. This allows us to use standard neural techniques for supertagging and dependency tree parsing, constrained by a linguistically principled type system. We present two approximative decoding algorithms, which achieve state-of-the-art accuracy and outperform strong baselines.</abstract>
      <url hash="4db29791">P18-1170</url>
      <attachment type="note" hash="3dae5e04">P18-1170.Notes.pdf</attachment>
      <video href="https://vimeo.com/285804795" />
      <attachment type="presentation" hash="c5087060">P18-1170.Presentation.pdf</attachment>
      <doi>10.18653/v1/P18-1170</doi>
      <bibkey>groschwitz-etal-2018-amr</bibkey>
    </paper>
    <paper id="172">
      <title>Batch <fixed-case>IS</fixed-case> <fixed-case>NOT</fixed-case> Heavy: Learning Word Representations From All Samples</title>
      <author><first>Xin</first><last>Xin</last></author>
      <author><first>Fajie</first><last>Yuan</last></author>
      <author><first>Xiangnan</first><last>He</last></author>
      <author><first>Joemon M.</first><last>Jose</last></author>
      <pages>1853&#8211;1862</pages>
      <abstract>Stochastic Gradient Descent (SGD) with negative sampling is the most prevalent approach to learn word representations. However, it is known that sampling methods are biased especially when the sampling distribution deviates from the true data distribution. Besides, SGD suffers from dramatic fluctuation due to the one-sample learning scheme. In this work, we propose AllVec that uses batch gradient learning to generate word representations from all training samples. Remarkably, the time complexity of AllVec remains at the same level as SGD, being determined by the number of positive samples rather than all samples. We evaluate AllVec on several benchmark tasks. Experiments show that AllVec outperforms sampling-based SGD methods with comparable efficiency, especially for small training corpora.</abstract>
      <url hash="4797c076">P18-1172</url>
      <video href="https://vimeo.com/285804833" />
      <doi>10.18653/v1/P18-1172</doi>
      <bibkey>xin-etal-2018-batch</bibkey>
    </paper>
    <paper id="173">
      <title>Backpropagating through Structured Argmax using a <fixed-case>SPIGOT</fixed-case></title>
      <author><first>Hao</first><last>Peng</last></author>
      <author><first>Sam</first><last>Thomson</last></author>
      <author><first>Noah A.</first><last>Smith</last></author>
      <pages>1863&#8211;1873</pages>
      <abstract>We introduce structured projection of intermediate gradients (SPIGOT), a new method for backpropagating through neural networks that include hard-decision structured predictions (e.g., parsing) in intermediate layers. SPIGOT requires no marginal inference, unlike structured attention networks and reinforcement learning-inspired solutions. Like so-called straight-through estimators, SPIGOT defines gradient-like quantities associated with intermediate nondifferentiable operations, allowing backpropagation before and after them; SPIGOT&#8217;s proxy aims to ensure that, after a parameter update, the intermediate structure will remain well-formed. We experiment on two structured NLP pipelines: syntactic-then-semantic dependency parsing, and semantic parsing followed by sentiment classification. We show that training with SPIGOT leads to a larger improvement on the downstream task than a modularly-trained pipeline, the straight-through estimator, and structured attention, reaching a new state of the art on semantic dependency parsing.</abstract>
      <url hash="e1fc2bc1">P18-1173</url>
      <attachment type="note" hash="ec8041e5">P18-1173.Notes.pdf</attachment>
      <video href="https://vimeo.com/285804853" />
      <attachment type="presentation" hash="bc2aed51">P18-1173.Presentation.pdf</attachment>
      <doi>10.18653/v1/P18-1173</doi>
      <bibkey>peng-etal-2018-backpropagating</bibkey>
      <pwccode url="https://github.com/Noahs-ARK/SPIGOT" additional="false">Noahs-ARK/SPIGOT</pwccode>
    </paper>
    <paper id="174">
      <title>Learning How to Actively Learn: A Deep Imitation Learning Approach</title>
      <author><first>Ming</first><last>Liu</last></author>
      <author><first>Wray</first><last>Buntine</last></author>
      <author><first>Gholamreza</first><last>Haffari</last></author>
      <pages>1874&#8211;1883</pages>
      <abstract>Heuristic-based active learning (AL) methods are limited when the data distribution of the underlying learning problems vary. We introduce a method that learns an AL &#8220;policy&#8221; using &#8220;imitation learning&#8221; (IL). Our IL-based approach makes use of an efficient and effective &#8220;algorithmic expert&#8221;, which provides the policy learner with good actions in the encountered AL situations. The AL strategy is then learned with a feedforward network, mapping situations to most informative query datapoints. We evaluate our method on two different tasks: text classification and named entity recognition. Experimental results show that our IL-based AL strategy is more effective than strong previous methods using heuristics and reinforcement learning.</abstract>
      <url hash="b7e43836">P18-1174</url>
      <video href="https://vimeo.com/285804866" />
      <attachment type="presentation" hash="67c94725">P18-1174.Presentation.pdf</attachment>
      <doi>10.18653/v1/P18-1174</doi>
      <bibkey>liu-etal-2018-learning-actively</bibkey>
      <pwccode url="https://github.com/Grayming/ALIL" additional="false">Grayming/ALIL</pwccode>
    </paper>
    <paper id="175">
      <title>Training Classifiers with Natural Language Explanations</title>
      <author><first>Braden</first><last>Hancock</last></author>
      <author><first>Paroma</first><last>Varma</last></author>
      <author><first>Stephanie</first><last>Wang</last></author>
      <author><first>Martin</first><last>Bringmann</last></author>
      <author><first>Percy</first><last>Liang</last></author>
      <author><first>Christopher</first><last>R&#233;</last></author>
      <pages>1884&#8211;1895</pages>
      <abstract>Training accurate classifiers requires many labels, but each label provides only limited information (one bit for binary classification). In this work, we propose BabbleLabble, a framework for training classifiers in which an annotator provides a natural language explanation for each labeling decision. A semantic parser converts these explanations into programmatic labeling functions that generate noisy labels for an arbitrary amount of unlabeled data, which is used to train a classifier. On three relation extraction tasks, we find that users are able to train classifiers with comparable F1 scores from 5-100 faster by providing explanations instead of just labels. Furthermore, given the inherent imperfection of labeling functions, we find that a simple rule-based semantic parser suffices.</abstract>
      <url hash="4de9b3f0">P18-1175</url>
      <video href="https://vimeo.com/285804886" />
      <attachment type="presentation" hash="b5d3bb6f">P18-1175.Presentation.pdf</attachment>
      <doi>10.18653/v1/P18-1175</doi>
      <bibkey>hancock-etal-2018-training</bibkey>
      <pwccode url="https://worksheets.codalab.org/worksheets/0x900e7e41deaa4ec5b2fe41dc50594548" additional="true">worksheets/0x900e7e41</pwccode>
    </paper>
    <paper id="177">
      <title>Harvesting Paragraph-level Question-Answer Pairs from <fixed-case>W</fixed-case>ikipedia</title>
      <author><first>Xinya</first><last>Du</last></author>
      <author><first>Claire</first><last>Cardie</last></author>
      <pages>1907&#8211;1917</pages>
      <abstract>We study the task of generating from Wikipedia articles question-answer pairs that cover content beyond a single sentence. We propose a neural network approach that incorporates coreference knowledge via a novel gating mechanism. As compared to models that only take into account sentence-level information (Heilman and Smith, 2010; Du et al., 2017; Zhou et al., 2017), we find that the linguistic knowledge introduced by the coreference representation aids question generation significantly, producing models that outperform the current state-of-the-art. We apply our system (composed of an answer span extraction system and the passage-level QG system) to the 10,000 top ranking Wikipedia articles and create a corpus of over one million question-answer pairs. We provide qualitative analysis for the this large-scale generated corpus from Wikipedia.</abstract>
      <url hash="ac441db1">P18-1177</url>
      <attachment type="note" hash="b881d2c1">P18-1177.Notes.pdf</attachment>
      <video href="https://vimeo.com/285804906" />
      <doi>10.18653/v1/P18-1177</doi>
      <bibkey>du-cardie-2018-harvesting</bibkey>
      <pwccode url="https://github.com/xinyadu/harvestingQA" additional="false">xinyadu/harvestingQA</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/simplequestions">SimpleQuestions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/webquestions">WebQuestions</pwcdataset>
    </paper>
    <paper id="179">
      <title>Language Generation via <fixed-case>DAG</fixed-case> Transduction</title>
      <author><first>Yajie</first><last>Ye</last></author>
      <author><first>Weiwei</first><last>Sun</last></author>
      <author><first>Xiaojun</first><last>Wan</last></author>
      <pages>1928&#8211;1937</pages>
      <abstract>A DAG automaton is a formal device for manipulating graphs. By augmenting a DAG automaton with transduction rules, a DAG transducer has potential applications in fundamental NLP tasks. In this paper, we propose a novel DAG transducer to perform graph-to-program transformation. The target structure of our transducer is a program licensed by a declarative programming language rather than linguistic structures. By executing such a program, we can easily get a surface string. Our transducer is designed especially for natural language generation (NLG) from type-logical semantic graphs. Taking Elementary Dependency Structures, a format of English Resource Semantics, as input, our NLG system achieves a BLEU-4 score of 68.07. This remarkable result demonstrates the feasibility of applying a DAG transducer to resolve NLG, as well as the effectiveness of our design.</abstract>
      <url hash="079820d7">P18-1179</url>
      <attachment type="software" hash="dce33220">P18-1179.Software.zip</attachment>
      <video href="https://vimeo.com/288152765" />
      <attachment type="presentation" hash="a251bae4">P18-1179.Presentation.pdf</attachment>
      <doi>10.18653/v1/P18-1179</doi>
      <bibkey>ye-etal-2018-language</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/penn-treebank">Penn Treebank</pwcdataset>
    </paper>
    <paper id="180">
      <title>A Distributional and Orthographic Aggregation Model for <fixed-case>E</fixed-case>nglish Derivational Morphology</title>
      <author><first>Daniel</first><last>Deutsch</last></author>
      <author><first>John</first><last>Hewitt</last></author>
      <author><first>Dan</first><last>Roth</last></author>
      <pages>1938&#8211;1947</pages>
      <abstract>Modeling derivational morphology to generate words with particular semantics is useful in many text generation tasks, such as machine translation or abstractive question answering. In this work, we tackle the task of derived word generation. That is, we attempt to generate the word &#8220;runner&#8221; for &#8220;someone who runs.&#8221; We identify two key problems in generating derived words from root words and transformations. We contribute a novel aggregation model of derived word generation that learns derivational transformations both as orthographic functions using sequence-to-sequence models and as functions in distributional word embedding space. The model then learns to choose between the hypothesis of each system. We also present two ways of incorporating corpus information into derived word generation.</abstract>
      <url hash="99e82c2f">P18-1180</url>
      <video href="https://vimeo.com/288152732" />
      <attachment type="presentation" hash="5339490c">P18-1180.Presentation.pdf</attachment>
      <doi>10.18653/v1/P18-1180</doi>
      <bibkey>deutsch-etal-2018-distributional</bibkey>
      <pwccode url="https://github.com/danieldeutsch/acl2018" additional="false">danieldeutsch/acl2018</pwccode>
    </paper>
    <paper id="182">
      <title><fixed-case>N</fixed-case>eural<fixed-case>REG</fixed-case>: An end-to-end approach to referring expression generation</title>
      <author><first>Thiago</first><last>Castro Ferreira</last></author>
      <author><first>Diego</first><last>Moussallem</last></author>
      <author><first>&#193;kos</first><last>K&#225;d&#225;r</last></author>
      <author><first>Sander</first><last>Wubben</last></author>
      <author><first>Emiel</first><last>Krahmer</last></author>
      <pages>1959&#8211;1969</pages>
      <abstract>Traditionally, Referring Expression Generation (REG) models first decide on the form and then on the content of references to discourse entities in text, typically relying on features such as salience and grammatical function. In this paper, we present a new approach (NeuralREG), relying on deep neural networks, which makes decisions about form and content in one go without explicit feature extraction. Using a delexicalized version of the WebNLG corpus, we show that the neural model substantially improves over two strong baselines.</abstract>
      <url hash="c2d580db">P18-1182</url>
      <video href="https://vimeo.com/285804944" />
      <attachment type="presentation" hash="681dd798">P18-1182.Presentation.pdf</attachment>
      <doi>10.18653/v1/P18-1182</doi>
      <bibkey>castro-ferreira-etal-2018-neuralreg</bibkey>
      <pwccode url="https://github.com/ThiagoCF05/NeuralREG" additional="false">ThiagoCF05/NeuralREG</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/webnlg">WebNLG</pwcdataset>
    </paper>
    <paper id="183">
      <title>Stock Movement Prediction from Tweets and Historical Prices</title>
      <author><first>Yumo</first><last>Xu</last></author>
      <author><first>Shay B.</first><last>Cohen</last></author>
      <pages>1970&#8211;1979</pages>
      <abstract>Stock movement prediction is a challenging problem: the market is highly stochastic, and we make temporally-dependent predictions from chaotic data. We treat these three complexities and present a novel deep generative model jointly exploiting text and price signals for this task. Unlike the case with discriminative or topic modeling, our model introduces recurrent, continuous latent variables for a better treatment of stochasticity, and uses neural variational inference to address the intractable posterior inference. We also provide a hybrid objective with temporal auxiliary to flexibly capture predictive dependencies. We demonstrate the state-of-the-art performance of our proposed model on a new stock movement prediction dataset which we collected.</abstract>
      <url hash="2955e628">P18-1183</url>
      <attachment type="note" hash="7aaf4b5b">P18-1183.Notes.pdf</attachment>
      <video href="https://vimeo.com/285804956" />
      <attachment type="presentation" hash="8198efe9">P18-1183.Presentation.pdf</attachment>
      <doi>10.18653/v1/P18-1183</doi>
      <bibkey>xu-cohen-2018-stock</bibkey>
      <pwccode url="https://github.com/yumoxu/stocknet-dataset" additional="false">yumoxu/stocknet-dataset</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/stocknet-1">StockNet</pwcdataset>
    </paper>
    <paper id="186">
      <title>Multimodal Named Entity Disambiguation for Noisy Social Media Posts</title>
      <author><first>Seungwhan</first><last>Moon</last></author>
      <author><first>Leonardo</first><last>Neves</last></author>
      <author><first>Vitor</first><last>Carvalho</last></author>
      <pages>2000&#8211;2008</pages>
      <abstract>We introduce the new Multimodal Named Entity Disambiguation (MNED) task for multimodal social media posts such as Snapchat or Instagram captions, which are composed of short captions with accompanying images. Social media posts bring significant challenges for disambiguation tasks because 1) ambiguity not only comes from polysemous entities, but also from inconsistent or incomplete notations, 2) very limited context is provided with surrounding words, and 3) there are many emerging entities often unseen during training. To this end, we build a new dataset called SnapCaptionsKB, a collection of Snapchat image captions submitted to public and crowd-sourced stories, with named entity mentions fully annotated and linked to entities in an external knowledge base. We then build a deep zeroshot multimodal network for MNED that 1) extracts contexts from both text and image, and 2) predicts correct entity in the knowledge graph embeddings space, allowing for zeroshot disambiguation of entities unseen in training set as well. The proposed model significantly outperforms the state-of-the-art text-only NED models, showing efficacy and potentials of the MNED task.</abstract>
      <url hash="b8465ef6">P18-1186</url>
      <video href="https://vimeo.com/285804998" />
      <doi>10.18653/v1/P18-1186</doi>
      <bibkey>moon-etal-2018-multimodal-named</bibkey>
    </paper>
    <paper id="187">
      <title>Semi-supervised User Geolocation via Graph Convolutional Networks</title>
      <author><first>Afshin</first><last>Rahimi</last></author>
      <author><first>Trevor</first><last>Cohn</last></author>
      <author><first>Timothy</first><last>Baldwin</last></author>
      <pages>2009&#8211;2019</pages>
      <abstract>Social media user geolocation is vital to many applications such as event detection. In this paper, we propose GCN, a multiview geolocation model based on Graph Convolutional Networks, that uses both text and network context. We compare GCN to the state-of-the-art, and to two baselines we propose, and show that our model achieves or is competitive with the state-of-the-art over three benchmark geolocation datasets when sufficient supervision is available. We also evaluate GCN under a minimal supervision scenario, and show it outperforms baselines. We find that highway network gates are essential for controlling the amount of useful neighbourhood expansion in GCN.</abstract>
      <url hash="844c0d8d">P18-1187</url>
      <attachment type="note" hash="cbc4c8cf">P18-1187.Notes.pdf</attachment>
      <video href="https://vimeo.com/285805016" />
      <attachment type="presentation" hash="42c65759">P18-1187.Presentation.pdf</attachment>
      <doi>10.18653/v1/P18-1187</doi>
      <bibkey>rahimi-etal-2018-semi</bibkey>
    </paper>
    <paper id="192">
      <title>Syntax for Semantic Role Labeling, To Be, Or Not To Be</title>
      <author><first>Shexia</first><last>He</last></author>
      <author><first>Zuchao</first><last>Li</last></author>
      <author><first>Hai</first><last>Zhao</last></author>
      <author><first>Hongxiao</first><last>Bai</last></author>
      <pages>2061&#8211;2071</pages>
      <abstract>Semantic role labeling (SRL) is dedicated to recognizing the predicate-argument structure of a sentence. Previous studies have shown syntactic information has a remarkable contribution to SRL performance. However, such perception was challenged by a few recent neural SRL models which give impressive performance without a syntactic backbone. This paper intends to quantify the importance of syntactic information to dependency SRL in deep learning framework. We propose an enhanced argument labeling model companying with an extended korder argument pruning algorithm for effectively exploiting syntactic information. Our model achieves state-of-the-art results on the CoNLL-2008, 2009 benchmarks for both English and Chinese, showing the quantitative significance of syntax to neural SRL together with a thorough empirical survey over existing models.</abstract>
      <url hash="534f47b9">P18-1192</url>
      <video href="https://vimeo.com/285805245" />
      <attachment type="presentation" hash="5c24471c">P18-1192.Presentation.pdf</attachment>
      <doi>10.18653/v1/P18-1192</doi>
      <bibkey>he-etal-2018-syntax</bibkey>
      <pwccode url="https://github.com/bcmi220/srl_syn_pruning" additional="false">bcmi220/srl_syn_pruning</pwccode>
    </paper>
    <paper id="193">
      <title>Situated Mapping of Sequential Instructions to Actions with Single-step Reward Observation</title>
      <author><first>Alane</first><last>Suhr</last></author>
      <author><first>Yoav</first><last>Artzi</last></author>
      <pages>2072&#8211;2082</pages>
      <abstract>We propose a learning approach for mapping context-dependent sequential instructions to actions. We address the problem of discourse and state dependencies with an attention-based model that considers both the history of the interaction and the state of the world. To train from start and goal states without access to demonstrations, we propose SESTRA, a learning algorithm that takes advantage of single-step reward observations and immediate expected reward maximization. We evaluate on the SCONE domains, and show absolute accuracy improvements of 9.8%-25.3% across the domains over approaches that use high-level logical representations.</abstract>
      <url hash="aa684a8d">P18-1193</url>
      <attachment type="note" hash="0940b6e3">P18-1193.Notes.pdf</attachment>
      <video href="https://vimeo.com/285805263" />
      <attachment type="presentation" hash="f1031f11">P18-1193.Presentation.pdf</attachment>
      <doi>10.18653/v1/P18-1193</doi>
      <bibkey>suhr-artzi-2018-situated</bibkey>
      <pwccode url="https://github.com/clic-lab/scone" additional="false">clic-lab/scone</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/atis">ATIS</pwcdataset>
    </paper>
    <paper id="194">
      <title>Marrying Up Regular Expressions with Neural Networks: A Case Study for Spoken Language Understanding</title>
      <author><first>Bingfeng</first><last>Luo</last></author>
      <author><first>Yansong</first><last>Feng</last></author>
      <author><first>Zheng</first><last>Wang</last></author>
      <author><first>Songfang</first><last>Huang</last></author>
      <author><first>Rui</first><last>Yan</last></author>
      <author><first>Dongyan</first><last>Zhao</last></author>
      <pages>2083&#8211;2093</pages>
      <abstract>The success of many natural language processing (NLP) tasks is bound by the number and quality of annotated data, but there is often a shortage of such training data. In this paper, we ask the question: &#8220;Can we combine a neural network (NN) with regular expressions (RE) to improve supervised learning for NLP?&#8221;. In answer, we develop novel methods to exploit the rich expressiveness of REs at different levels within a NN, showing that the combination significantly enhances the learning effectiveness when a small number of training examples are available. We evaluate our approach by applying it to spoken language understanding for intent detection and slot filling. Experimental results show that our approach is highly effective in exploiting the available training data, giving a clear boost to the RE-unaware NN.</abstract>
      <url hash="0d536607">P18-1194</url>
      <video href="https://vimeo.com/285805276" />
      <attachment type="presentation" hash="3f1c3534">P18-1194.Presentation.pdf</attachment>
      <doi>10.18653/v1/P18-1194</doi>
      <bibkey>luo-etal-2018-marrying</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/atis">ATIS</pwcdataset>
    </paper>
    <paper id="197">
      <title>To Attend or not to Attend: A Case Study on Syntactic Structures for Semantic Relatedness</title>
      <author><first>Amulya</first><last>Gupta</last></author>
      <author><first>Zhu</first><last>Zhang</last></author>
      <pages>2116&#8211;2125</pages>
      <abstract>With the recent success of Recurrent Neural Networks (RNNs) in Machine Translation (MT), attention mechanisms have become increasingly popular. The purpose of this paper is two-fold; firstly, we propose a novel attention model on Tree Long Short-Term Memory Networks (Tree-LSTMs), a tree-structured generalization of standard LSTM. Secondly, we study the interaction between attention and syntactic structures, by experimenting with three LSTM variants: bidirectional-LSTMs, Constituency Tree-LSTMs, and Dependency Tree-LSTMs. Our models are evaluated on two semantic relatedness tasks: semantic relatedness scoring for sentence pairs (SemEval 2012, Task 6 and SemEval 2014, Task 1) and paraphrase detection for question pairs (Quora, 2017).</abstract>
      <url hash="3ee1c326">P18-1197</url>
      <video href="https://vimeo.com/285805320" />
      <attachment type="presentation" hash="5d870502">P18-1197.Presentation.pdf</attachment>
      <doi>10.18653/v1/P18-1197</doi>
      <bibkey>gupta-zhang-2018-attend</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/sick">SICK</pwcdataset>
    </paper>
    <paper id="198">
      <title>What you can cram into a single $&amp;!#* vector: Probing sentence embeddings for linguistic properties</title>
      <author><first>Alexis</first><last>Conneau</last></author>
      <author><first>German</first><last>Kruszewski</last></author>
      <author><first>Guillaume</first><last>Lample</last></author>
      <author><first>Lo&#239;c</first><last>Barrault</last></author>
      <author><first>Marco</first><last>Baroni</last></author>
      <pages>2126&#8211;2136</pages>
      <abstract>Although much effort has recently been devoted to training high-quality sentence embeddings, we still have a poor understanding of what they are capturing. &#8220;Downstream&#8221; tasks, often based on sentence classification, are commonly used to evaluate the quality of sentence representations. The complexity of the tasks makes it however difficult to infer what kind of information is present in the representations. We introduce here 10 probing tasks designed to capture simple linguistic features of sentences, and we use them to study embeddings generated by three different encoders trained in eight distinct ways, uncovering intriguing properties of both encoders and training methods.</abstract>
      <url hash="4eb8eb26">P18-1198</url>
      <attachment type="note" hash="002202d0">P18-1198.Notes.pdf</attachment>
      <video href="https://vimeo.com/285805339" />
      <attachment type="presentation" hash="fc75646f">P18-1198.Presentation.pdf</attachment>
      <doi>10.18653/v1/P18-1198</doi>
      <bibkey>conneau-etal-2018-cram</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/bookcorpus">BookCorpus</pwcdataset>
    </paper>
    <paper id="199">
      <title>Robust Distant Supervision Relation Extraction via Deep Reinforcement Learning</title>
      <author><first>Pengda</first><last>Qin</last></author>
      <author><first>Weiran</first><last>Xu</last></author>
      <author><first>William Yang</first><last>Wang</last></author>
      <pages>2137&#8211;2147</pages>
      <abstract>Distant supervision has become the standard method for relation extraction. However, even though it is an efficient method, it does not come at no cost&#8212;The resulted distantly-supervised training samples are often very noisy. To combat the noise, most of the recent state-of-the-art approaches focus on selecting one-best sentence or calculating soft attention weights over the set of the sentences of one specific entity pair. However, these methods are suboptimal, and the false positive problem is still a key stumbling bottleneck for the performance. We argue that those incorrectly-labeled candidate sentences must be treated with a hard decision, rather than being dealt with soft attention weights. To do this, our paper describes a radical solution&#8212;We explore a deep reinforcement learning strategy to generate the false-positive indicator, where we automatically recognize false positives for each relation type without any supervised information. Unlike the removal operation in the previous studies, we redistribute them into the negative examples. The experimental results show that the proposed strategy significantly improves the performance of distant supervision comparing to state-of-the-art systems.</abstract>
      <url hash="02ef10e4">P18-1199</url>
      <video href="https://vimeo.com/285805358" />
      <attachment type="presentation" hash="88fe3a10">P18-1199.Presentation.pdf</attachment>
      <doi>10.18653/v1/P18-1199</doi>
      <bibkey>qin-etal-2018-robust</bibkey>
      <pwccode url="" additional="true" />
    </paper>
    <paper id="200">
      <title>Interpretable and Compositional Relation Learning by Joint Training with an Autoencoder</title>
      <author><first>Ryo</first><last>Takahashi</last></author>
      <author><first>Ran</first><last>Tian</last></author>
      <author><first>Kentaro</first><last>Inui</last></author>
      <pages>2148&#8211;2159</pages>
      <abstract>Embedding models for entities and relations are extremely useful for recovering missing facts in a knowledge base. Intuitively, a relation can be modeled by a matrix mapping entity vectors. However, relations reside on low dimension sub-manifolds in the parameter space of arbitrary matrices &#8211; for one reason, composition of two relations M1, M2 may match a third M3 (e.g. composition of relations currency_of_country and country_of_film usually matches currency_of_film_budget), which imposes compositional constraints to be satisfied by the parameters (i.e. M1*M2=M3). In this paper we investigate a dimension reduction technique by training relations jointly with an autoencoder, which is expected to better capture compositional constraints. We achieve state-of-the-art on Knowledge Base Completion tasks with strongly improved Mean Rank, and show that joint training with an autoencoder leads to interpretable sparse codings of relations, helps discovering compositional constraints and benefits from compositional training. Our source code is released at <url>github.com/tianran/glimvec</url>.</abstract>
      <url hash="80607165">P18-1200</url>
      <video href="https://vimeo.com/285805371" />
      <attachment type="presentation" hash="2654fbe5">P18-1200.Presentation.pdf</attachment>
      <doi>10.18653/v1/P18-1200</doi>
      <bibkey>takahashi-etal-2018-interpretable</bibkey>
      <pwccode url="https://github.com/tianran/glimvec" additional="false">tianran/glimvec</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/fb15k-237">FB15k-237</pwcdataset>
    </paper>
    <paper id="201">
      <title>Zero-Shot Transfer Learning for Event Extraction</title>
      <author><first>Lifu</first><last>Huang</last></author>
      <author><first>Heng</first><last>Ji</last></author>
      <author><first>Kyunghyun</first><last>Cho</last></author>
      <author><first>Ido</first><last>Dagan</last></author>
      <author><first>Sebastian</first><last>Riedel</last></author>
      <author><first>Clare</first><last>Voss</last></author>
      <pages>2160&#8211;2170</pages>
      <abstract>Most previous supervised event extraction methods have relied on features derived from manual annotations, and thus cannot be applied to new event types without extra annotation effort. We take a fresh look at event extraction and model it as a generic grounding problem: mapping each event mention to a specific type in a target event ontology. We design a transferable architecture of structural and compositional neural networks to jointly represent and map event mentions and types into a shared semantic space. Based on this new framework, we can select, for each event mention, the event type which is semantically closest in this space as its type. By leveraging manual annotations available for a small set of existing event types, our framework can be applied to new unseen event types without additional manual annotations. When tested on 23 unseen event types, our zero-shot framework, without manual annotations, achieved performance comparable to a supervised model trained from 3,000 sentences annotated with 500 event mentions.</abstract>
      <url hash="17ea6d7e">P18-1201</url>
      <video href="https://vimeo.com/285805384" />
      <attachment type="presentation" hash="400132b5">P18-1201.Presentation.pdf</attachment>
      <doi>10.18653/v1/P18-1201</doi>
      <bibkey>huang-etal-2018-zero</bibkey>
      <pwccode url="https://github.com/wilburOne/ZeroShotEvent" additional="false">wilburOne/ZeroShotEvent</pwccode>
    </paper>
    <paper id="205">
      <title>Personalizing Dialogue Agents: <fixed-case>I</fixed-case> have a dog, do you have pets too?</title>
      <author><first>Saizheng</first><last>Zhang</last></author>
      <author><first>Emily</first><last>Dinan</last></author>
      <author><first>Jack</first><last>Urbanek</last></author>
      <author><first>Arthur</first><last>Szlam</last></author>
      <author><first>Douwe</first><last>Kiela</last></author>
      <author><first>Jason</first><last>Weston</last></author>
      <pages>2204&#8211;2213</pages>
      <abstract>Chit-chat models are known to have several problems: they lack specificity, do not display a consistent personality and are often not very captivating. In this work we present the task of making chit-chat more engaging by conditioning on profile information. We collect data and train models to (i)condition on their given profile information; and (ii) information about the person they are talking to, resulting in improved dialogues, as measured by next utterance prediction. Since (ii) is initially unknown our model is trained to engage its partner with personal topics, and we show the resulting dialogue can be used to predict profile information about the interlocutors.</abstract>
      <url hash="29a5ea91">P18-1205</url>
      <attachment type="note" hash="be1fc5e2">P18-1205.Notes.pdf</attachment>
      <video href="https://vimeo.com/285805443" />
      <doi>10.18653/v1/P18-1205</doi>
      <bibkey>zhang-etal-2018-personalizing</bibkey>
      <pwccode url="https://github.com/facebookresearch/ParlAI" additional="true">facebookresearch/ParlAI</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/persona-chat-1">PERSONA-CHAT</pwcdataset>
    </paper>
    <paper id="206">
      <title>Efficient Large-Scale Neural Domain Classification with Personalized Attention</title>
      <author><first>Young-Bum</first><last>Kim</last></author>
      <author><first>Dongchan</first><last>Kim</last></author>
      <author><first>Anjishnu</first><last>Kumar</last></author>
      <author><first>Ruhi</first><last>Sarikaya</last></author>
      <pages>2214&#8211;2224</pages>
      <abstract>In this paper, we explore the task of mapping spoken language utterances to one of thousands of natural language understanding domains in intelligent personal digital assistants (IPDAs). This scenario is observed in mainstream IPDAs in industry that allow third parties to develop thousands of new domains to augment built-in first party domains to rapidly increase domain coverage and overall IPDA capabilities. We propose a scalable neural model architecture with a shared encoder, a novel attention mechanism that incorporates personalization information and domain-specific classifiers that solves the problem efficiently. Our architecture is designed to efficiently accommodate incremental domain additions achieving two orders of magnitude speed up compared to full model retraining. We consider the practical constraints of real-time production systems, and design to minimize memory footprint and runtime latency. We demonstrate that incorporating personalization significantly improves domain classification accuracy in a setting with thousands of overlapping domains.</abstract>
      <url hash="e76e0cce">P18-1206</url>
      <video href="https://vimeo.com/285805460" />
      <doi>10.18653/v1/P18-1206</doi>
      <bibkey>kim-etal-2018-efficient</bibkey>
    </paper>
    <paper id="208">
      <title>Multimodal Language Analysis in the Wild: <fixed-case>CMU</fixed-case>-<fixed-case>MOSEI</fixed-case> Dataset and Interpretable Dynamic Fusion Graph</title>
      <author><first>AmirAli</first><last>Bagher Zadeh</last></author>
      <author><first>Paul Pu</first><last>Liang</last></author>
      <author><first>Soujanya</first><last>Poria</last></author>
      <author><first>Erik</first><last>Cambria</last></author>
      <author><first>Louis-Philippe</first><last>Morency</last></author>
      <pages>2236&#8211;2246</pages>
      <abstract>Analyzing human multimodal language is an emerging area of research in NLP. Intrinsically this language is multimodal (heterogeneous), sequential and asynchronous; it consists of the language (words), visual (expressions) and acoustic (paralinguistic) modalities all in the form of asynchronous coordinated sequences. From a resource perspective, there is a genuine need for large scale datasets that allow for in-depth studies of this form of language. In this paper we introduce CMU Multimodal Opinion Sentiment and Emotion Intensity (CMU-MOSEI), the largest dataset of sentiment analysis and emotion recognition to date. Using data from CMU-MOSEI and a novel multimodal fusion technique called the Dynamic Fusion Graph (DFG), we conduct experimentation to exploit how modalities interact with each other in human multimodal language. Unlike previously proposed fusion techniques, DFG is highly interpretable and achieves competative performance when compared to the previous state of the art.</abstract>
      <url hash="9fae6d62">P18-1208</url>
      <video href="https://vimeo.com/285805491" />
      <doi>10.18653/v1/P18-1208</doi>
      <bibkey>bagher-zadeh-etal-2018-multimodal</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/cmu-mosei">CMU-MOSEI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/iemocap">IEMOCAP</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="212">
      <title>Joint Reasoning for Temporal and Causal Relations</title>
      <author><first>Qiang</first><last>Ning</last></author>
      <author><first>Zhili</first><last>Feng</last></author>
      <author><first>Hao</first><last>Wu</last></author>
      <author><first>Dan</first><last>Roth</last></author>
      <pages>2278&#8211;2288</pages>
      <abstract>Understanding temporal and causal relations between events is a fundamental natural language understanding task. Because a cause must occur earlier than its effect, temporal and causal relations are closely related and one relation often dictates the value of the other. However, limited attention has been paid to studying these two relations jointly. This paper presents a joint inference framework for them using constrained conditional models (CCMs). Specifically, we formulate the joint problem as an integer linear programming (ILP) problem, enforcing constraints that are inherent in the nature of time and causality. We show that the joint inference framework results in statistically significant improvement in the extraction of both temporal and causal relations from text.</abstract>
      <url hash="730fc58d">P18-1212</url>
      <attachment type="note" hash="2c49c1d9">P18-1212.Notes.pdf</attachment>
      <video href="https://vimeo.com/285805571" />
      <attachment type="presentation" hash="d3a66831">P18-1212.Presentation.pdf</attachment>
      <doi>10.18653/v1/P18-1212</doi>
      <bibkey>ning-etal-2018-joint</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/tcr">TCR</pwcdataset>
    </paper>
    <paper id="214">
      <title>A Deep Relevance Model for Zero-Shot Document Filtering</title>
      <author><first>Chenliang</first><last>Li</last></author>
      <author><first>Wei</first><last>Zhou</last></author>
      <author><first>Feng</first><last>Ji</last></author>
      <author><first>Yu</first><last>Duan</last></author>
      <author><first>Haiqing</first><last>Chen</last></author>
      <pages>2300&#8211;2310</pages>
      <abstract>In the era of big data, focused analysis for diverse topics with a short response time becomes an urgent demand. As a fundamental task, information filtering therefore becomes a critical necessity. In this paper, we propose a novel deep relevance model for zero-shot document filtering, named DAZER. DAZER estimates the relevance between a document and a category by taking a small set of seed words relevant to the category. With pre-trained word embeddings from a large external corpus, DAZER is devised to extract the relevance signals by modeling the hidden feature interactions in the word embedding space. The relevance signals are extracted through a gated convolutional process. The gate mechanism controls which convolution filters output the relevance signals in a category dependent manner. Experiments on two document collections of two different tasks (i.e., topic categorization and sentiment analysis) demonstrate that DAZER significantly outperforms the existing alternative solutions, including the state-of-the-art deep relevance ranking models.</abstract>
      <url hash="18dd4a29">P18-1214</url>
      <attachment type="poster" hash="4ea6dd13">P18-1214.Poster.pdf</attachment>
      <doi>10.18653/v1/P18-1214</doi>
      <bibkey>li-etal-2018-deep</bibkey>
      <pwccode url="https://github.com/WHUIR/DAZER" additional="false">WHUIR/DAZER</pwccode>
    </paper>
    <paper id="216">
      <title>Joint Embedding of Words and Labels for Text Classification</title>
      <author><first>Guoyin</first><last>Wang</last></author>
      <author><first>Chunyuan</first><last>Li</last></author>
      <author><first>Wenlin</first><last>Wang</last></author>
      <author><first>Yizhe</first><last>Zhang</last></author>
      <author><first>Dinghan</first><last>Shen</last></author>
      <author><first>Xinyuan</first><last>Zhang</last></author>
      <author><first>Ricardo</first><last>Henao</last></author>
      <author><first>Lawrence</first><last>Carin</last></author>
      <pages>2321&#8211;2331</pages>
      <abstract>Word embeddings are effective intermediate representations for capturing semantic regularities between words, when learning the representations of text sequences. We propose to view text classification as a label-word joint embedding problem: each label is embedded in the same space with the word vectors. We introduce an attention framework that measures the compatibility of embeddings between text sequences and labels. The attention is learned on a training set of labeled samples to ensure that, given a text sequence, the relevant words are weighted higher than the irrelevant ones. Our method maintains the interpretability of word embeddings, and enjoys a built-in ability to leverage alternative sources of information, in addition to input text sequences. Extensive results on the several large text datasets show that the proposed framework outperforms the state-of-the-art methods by a large margin, in terms of both accuracy and speed.</abstract>
      <url hash="b24d6768">P18-1216</url>
      <attachment type="poster" hash="487f1e1c">P18-1216.Poster.pdf</attachment>
      <doi>10.18653/v1/P18-1216</doi>
      <bibkey>wang-etal-2018-joint-embedding</bibkey>
      <pwccode url="https://github.com/guoyinwang/LEAM" additional="true">guoyinwang/LEAM</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/ag-news">AG News</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/dbpedia">DBpedia</pwcdataset>
    </paper>
    <paper id="218">
      <title>Document Similarity for Texts of Varying Lengths via Hidden Topics</title>
      <author><first>Hongyu</first><last>Gong</last></author>
      <author><first>Tarek</first><last>Sakakini</last></author>
      <author><first>Suma</first><last>Bhat</last></author>
      <author><first>JinJun</first><last>Xiong</last></author>
      <pages>2341&#8211;2351</pages>
      <abstract>Measuring similarity between texts is an important task for several applications. Available approaches to measure document similarity are inadequate for document pairs that have non-comparable lengths, such as a long document and its summary. This is because of the lexical, contextual and the abstraction gaps between a long document of rich details and its concise summary of abstract information. In this paper, we present a document matching approach to bridge this gap, by comparing the texts in a common space of hidden topics. We evaluate the matching algorithm on two matching tasks and find that it consistently and widely outperforms strong baselines. We also highlight the benefits of the incorporation of domain knowledge to text matching.</abstract>
      <url hash="1704f036">P18-1218</url>
      <attachment type="note" hash="c26d3cba">P18-1218.Notes.pdf</attachment>
      <attachment type="poster" hash="fc30e039">P18-1218.Poster.pdf</attachment>
      <doi>10.18653/v1/P18-1218</doi>
      <bibkey>gong-etal-2018-document</bibkey>
      <pwccode url="https://github.com/HongyuGong/Document-Similarity-via-Hidden-Topics" additional="false">HongyuGong/Document-Similarity-via-Hidden-Topics</pwccode>
    </paper>
    <paper id="220">
      <title>Multi-Input Attention for Unsupervised <fixed-case>OCR</fixed-case> Correction</title>
      <author><first>Rui</first><last>Dong</last></author>
      <author><first>David</first><last>Smith</last></author>
      <pages>2363&#8211;2372</pages>
      <abstract>We propose a novel approach to OCR post-correction that exploits repeated texts in large corpora both as a source of noisy target outputs for unsupervised training and as a source of evidence when decoding. A sequence-to-sequence model with attention is applied for single-input correction, and a new decoder with multi-input attention averaging is developed to search for consensus among multiple sequences. We design two ways of training the correction model without human annotation, either training to match noisily observed textual variants or bootstrapping from a uniform error model. On two corpora of historical newspapers and books, we show that these unsupervised techniques cut the character and word error rates nearly in half on single inputs and, with the addition of multi-input decoding, can rival supervised methods.</abstract>
      <url hash="8b4bc11a">P18-1220</url>
      <attachment type="poster" hash="1c12cfda">P18-1220.Poster.pdf</attachment>
      <doi>10.18653/v1/P18-1220</doi>
      <bibkey>dong-smith-2018-multi</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/new-york-times-annotated-corpus">New York Times Annotated Corpus</pwcdataset>
    </paper>
    <paper id="221">
      <title>Building Language Models for Text with Named Entities</title>
      <author><first>Md Rizwan</first><last>Parvez</last></author>
      <author><first>Saikat</first><last>Chakraborty</last></author>
      <author><first>Baishakhi</first><last>Ray</last></author>
      <author><first>Kai-Wei</first><last>Chang</last></author>
      <pages>2373&#8211;2383</pages>
      <abstract>Text in many domains involves a significant amount of named entities. Predicting the entity names is often challenging for a language model as they appear less frequent on the training corpus. In this paper, we propose a novel and effective approach to building a language model which can learn the entity names by leveraging their entity type information. We also introduce two benchmark datasets based on recipes and Java programming codes, on which we evaluate the proposed model. Experimental results show that our model achieves 52.2% better perplexity in recipe generation and 22.06% on code generation than state-of-the-art language models.</abstract>
      <url hash="fa7b49d3">P18-1221</url>
      <attachment type="poster" hash="07c058d8">P18-1221.Poster.pdf</attachment>
      <doi>10.18653/v1/P18-1221</doi>
      <bibkey>parvez-etal-2018-building</bibkey>
      <pwccode url="https://github.com/uclanlp/NamedEntityLanguageModel" additional="true">uclanlp/NamedEntityLanguageModel</pwccode>
    </paper>
    <paper id="222">
      <title>hyperdoc2vec: Distributed Representations of Hypertext Documents</title>
      <author><first>Jialong</first><last>Han</last></author>
      <author><first>Yan</first><last>Song</last></author>
      <author><first>Wayne Xin</first><last>Zhao</last></author>
      <author><first>Shuming</first><last>Shi</last></author>
      <author><first>Haisong</first><last>Zhang</last></author>
      <pages>2384&#8211;2394</pages>
      <abstract>Hypertext documents, such as web pages and academic papers, are of great importance in delivering information in our daily life. Although being effective on plain documents, conventional text embedding methods suffer from information loss if directly adapted to hyper-documents. In this paper, we propose a general embedding approach for hyper-documents, namely, hyperdoc2vec, along with four criteria characterizing necessary information that hyper-document embedding models should preserve. Systematic comparisons are conducted between hyperdoc2vec and several competitors on two tasks, i.e., paper classification and citation recommendation, in the academic paper domain. Analyses and experiments both validate the superiority of hyperdoc2vec to other models w.r.t. the four criteria.</abstract>
      <url hash="ff52d423">P18-1222</url>
      <attachment type="poster" hash="4620f72d">P18-1222.Poster.pdf</attachment>
      <doi>10.18653/v1/P18-1222</doi>
      <bibkey>han-etal-2018-hyperdoc2vec</bibkey>
    </paper>
    <paper id="223">
      <title>Entity-Duet Neural Ranking: Understanding the Role of Knowledge Graph Semantics in Neural Information Retrieval</title>
      <author><first>Zhenghao</first><last>Liu</last></author>
      <author><first>Chenyan</first><last>Xiong</last></author>
      <author><first>Maosong</first><last>Sun</last></author>
      <author><first>Zhiyuan</first><last>Liu</last></author>
      <pages>2395&#8211;2405</pages>
      <abstract>This paper presents the Entity-Duet Neural Ranking Model (EDRM), which introduces knowledge graphs to neural search systems. EDRM represents queries and documents by their words and entity annotations. The semantics from knowledge graphs are integrated in the distributed representations of their entities, while the ranking is conducted by interaction-based neural ranking networks. The two components are learned end-to-end, making EDRM a natural combination of entity-oriented search and neural information retrieval. Our experiments on a commercial search log demonstrate the effectiveness of EDRM. Our analyses reveal that knowledge graph semantics significantly improve the generalization ability of neural ranking models.</abstract>
      <url hash="f73bf004">P18-1223</url>
      <attachment type="poster" hash="d7ca52f5">P18-1223.Poster.pdf</attachment>
      <doi>10.18653/v1/P18-1223</doi>
      <bibkey>liu-etal-2018-entity</bibkey>
      <pwccode url="https://github.com/thunlp/EntityDuetNeuralRanking" additional="false">thunlp/EntityDuetNeuralRanking</pwccode>
    </paper>
    <paper id="224">
      <title>Neural Natural Language Inference Models Enhanced with External Knowledge</title>
      <author><first>Qian</first><last>Chen</last></author>
      <author><first>Xiaodan</first><last>Zhu</last></author>
      <author><first>Zhen-Hua</first><last>Ling</last></author>
      <author><first>Diana</first><last>Inkpen</last></author>
      <author><first>Si</first><last>Wei</last></author>
      <pages>2406&#8211;2417</pages>
      <abstract>Modeling natural language inference is a very challenging task. With the availability of large annotated data, it has recently become feasible to train complex models such as neural-network-based inference models, which have shown to achieve the state-of-the-art performance. Although there exist relatively large annotated data, can machines learn all knowledge needed to perform natural language inference (NLI) from these data? If not, how can neural-network-based NLI models benefit from external knowledge and how to build NLI models to leverage it? In this paper, we enrich the state-of-the-art neural natural language inference models with external knowledge. We demonstrate that the proposed models improve neural NLI models to achieve the state-of-the-art performance on the SNLI and MultiNLI datasets.</abstract>
      <url hash="a0d73f46">P18-1224</url>
      <attachment type="poster" hash="6b09cd55">P18-1224.Poster.pdf</attachment>
      <doi>10.18653/v1/P18-1224</doi>
      <bibkey>chen-etal-2018-neural-natural</bibkey>
      <pwccode url="https://github.com/lukecq1231/kim" additional="false">lukecq1231/kim</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
    </paper>
    <paper id="225">
      <title><fixed-case>A</fixed-case>dv<fixed-case>E</fixed-case>ntu<fixed-case>R</fixed-case>e: Adversarial Training for Textual Entailment with Knowledge-Guided Examples</title>
      <author><first>Dongyeop</first><last>Kang</last></author>
      <author><first>Tushar</first><last>Khot</last></author>
      <author><first>Ashish</first><last>Sabharwal</last></author>
      <author><first>Eduard</first><last>Hovy</last></author>
      <pages>2418&#8211;2428</pages>
      <abstract>We consider the problem of learning textual entailment models with limited supervision (5K-10K training examples), and present two complementary approaches for it. First, we propose knowledge-guided adversarial example generators for incorporating large lexical resources in entailment models via only a handful of rule templates. Second, to make the entailment model&#8212;a discriminator&#8212;more robust, we propose the first GAN-style approach for training it using a natural language example generator that iteratively adjusts to the discriminator&#8217;s weaknesses. We demonstrate effectiveness using two entailment datasets, where the proposed methods increase accuracy by 4.7% on SciTail and by 2.8% on a 1% sub-sample of SNLI. Notably, even a single hand-written rule, negate, improves the accuracy of negation examples in SNLI by 6.1%.</abstract>
      <url hash="dbea3450">P18-1225</url>
      <attachment type="note" hash="b7b9cd8c">P18-1225.Notes.pdf</attachment>
      <attachment type="poster" hash="b74606da">P18-1225.Poster.pdf</attachment>
      <doi>10.18653/v1/P18-1225</doi>
      <bibkey>kang-etal-2018-adventure</bibkey>
      <pwccode url="https://github.com/dykang/adventure" additional="false">dykang/adventure</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/sick">SICK</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
    </paper>
    <paper id="226">
      <title>Subword-level Word Vector Representations for <fixed-case>K</fixed-case>orean</title>
      <author><first>Sungjoon</first><last>Park</last></author>
      <author><first>Jeongmin</first><last>Byun</last></author>
      <author><first>Sion</first><last>Baek</last></author>
      <author><first>Yongseok</first><last>Cho</last></author>
      <author><first>Alice</first><last>Oh</last></author>
      <pages>2429&#8211;2438</pages>
      <abstract>Research on distributed word representations is focused on widely-used languages such as English. Although the same methods can be used for other languages, language-specific knowledge can enhance the accuracy and richness of word vector representations. In this paper, we look at improving distributed word representations for Korean using knowledge about the unique linguistic structure of Korean. Specifically, we decompose Korean words into the jamo-level, beyond the character-level, allowing a systematic use of subword information. To evaluate the vectors, we develop Korean test sets for word similarity and analogy and make them publicly available. The results show that our simple method outperforms word2vec and character-level Skip-Grams on semantic and syntactic similarity and analogy tasks and contributes positively toward downstream NLP tasks such as sentiment analysis.</abstract>
      <url hash="577ef475">P18-1226</url>
      <attachment type="poster" hash="fa8f448f">P18-1226.Poster.pdf</attachment>
      <doi>10.18653/v1/P18-1226</doi>
      <bibkey>park-etal-2018-subword</bibkey>
      <pwccode url="https://github.com/SungjoonPark/KoreanWordVectors" additional="false">SungjoonPark/KoreanWordVectors</pwccode>
    </paper>
    <paper id="229">
      <title>End-to-End Reinforcement Learning for Automatic Taxonomy Induction</title>
      <author><first>Yuning</first><last>Mao</last></author>
      <author><first>Xiang</first><last>Ren</last></author>
      <author><first>Jiaming</first><last>Shen</last></author>
      <author><first>Xiaotao</first><last>Gu</last></author>
      <author><first>Jiawei</first><last>Han</last></author>
      <pages>2462&#8211;2472</pages>
      <abstract>We present a novel end-to-end reinforcement learning approach to automatic taxonomy induction from a set of terms. While prior methods treat the problem as a two-phase task (<i>i.e.</i>,, detecting hypernymy pairs followed by organizing these pairs into a tree-structured hierarchy), we argue that such two-phase methods may suffer from error propagation, and cannot effectively optimize metrics that capture the holistic structure of a taxonomy. In our approach, the representations of term pairs are learned using multiple sources of information and used to determine <i>which</i> term to select and <i>where</i> to place it on the taxonomy via a policy network. All components are trained in an end-to-end manner with cumulative rewards, measured by a holistic tree metric over the training taxonomies. Experiments on two public datasets of different domains show that our approach outperforms prior state-of-the-art taxonomy induction methods up to 19.6% on ancestor F1.</abstract>
      <url hash="944ba0d3">P18-1229</url>
      <attachment type="poster" hash="6f2fea71">P18-1229.Poster.pdf</attachment>
      <doi>10.18653/v1/P18-1229</doi>
      <bibkey>mao-etal-2018-end</bibkey>
      <pwccode url="https://github.com/morningmoni/TaxoRL" additional="false">morningmoni/TaxoRL</pwccode>
    </paper>
    <paper id="230">
      <title>Incorporating Glosses into Neural Word Sense Disambiguation</title>
      <author><first>Fuli</first><last>Luo</last></author>
      <author><first>Tianyu</first><last>Liu</last></author>
      <author><first>Qiaolin</first><last>Xia</last></author>
      <author><first>Baobao</first><last>Chang</last></author>
      <author><first>Zhifang</first><last>Sui</last></author>
      <pages>2473&#8211;2482</pages>
      <abstract>Word Sense Disambiguation (WSD) aims to identify the correct meaning of polysemous words in the particular context. Lexical resources like WordNet which are proved to be of great help for WSD in the knowledge-based methods. However, previous neural networks for WSD always rely on massive labeled data (context), ignoring lexical resources like glosses (sense definitions). In this paper, we integrate the context and glosses of the target word into a unified framework in order to make full use of both labeled data and lexical knowledge. Therefore, we propose GAS: a gloss-augmented WSD neural network which jointly encodes the context and glosses of the target word. GAS models the semantic relationship between the context and the gloss in an improved memory network framework, which breaks the barriers of the previous supervised methods and knowledge-based methods. We further extend the original gloss of word sense via its semantic relations in WordNet to enrich the gloss information. The experimental results show that our model outperforms the state-of-the-art systems on several English all-words WSD datasets.</abstract>
      <url hash="0d463b96">P18-1230</url>
      <attachment type="poster" hash="80ef7a69">P18-1230.Poster.pdf</attachment>
      <doi>10.18653/v1/P18-1230</doi>
      <bibkey>luo-etal-2018-incorporating</bibkey>
      <pwccode url="https://github.com/jimiyulu/WSD_MemNN" additional="false">jimiyulu/WSD_MemNN</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/semeval-2013">SemEval 2013</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/word-sense-disambiguation-a-unified">Word Sense Disambiguation: a Unified Evaluation Framework and Empirical Comparison</pwcdataset>
    </paper>
    <paper id="231">
      <title>Bilingual Sentiment Embeddings: Joint Projection of Sentiment Across Languages</title>
      <author><first>Jeremy</first><last>Barnes</last></author>
      <author><first>Roman</first><last>Klinger</last></author>
      <author><first>Sabine</first><last>Schulte im Walde</last></author>
      <pages>2483&#8211;2493</pages>
      <abstract>Sentiment analysis in low-resource languages suffers from a lack of annotated corpora to estimate high-performing models. Machine translation and bilingual word embeddings provide some relief through cross-lingual sentiment approaches. However, they either require large amounts of parallel data or do not sufficiently capture sentiment information. We introduce Bilingual Sentiment Embeddings (BLSE), which jointly represent sentiment information in a source and target language. This model only requires a small bilingual lexicon, a source-language corpus annotated for sentiment, and monolingual word embeddings for each language. We perform experiments on three language combinations (Spanish, Catalan, Basque) for sentence-level cross-lingual sentiment classification and find that our model significantly outperforms state-of-the-art methods on four out of six experimental setups, as well as capturing complementary information to machine translation. Our analysis of the resulting embedding space provides evidence that it represents sentiment information in the resource-poor target language without any annotated data in that language.</abstract>
      <url hash="d2fa428e">P18-1231</url>
      <attachment type="poster" hash="da0422f2">P18-1231.Poster.pdf</attachment>
      <doi>10.18653/v1/P18-1231</doi>
      <bibkey>barnes-etal-2018-bilingual</bibkey>
      <pwccode url="https://github.com/jbarnesspain/blse" additional="false">jbarnesspain/blse</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/multibooked">MultiBooked</pwcdataset>
    </paper>
    <paper id="232">
      <title>Learning Domain-Sensitive and Sentiment-Aware Word Embeddings</title>
      <author><first>Bei</first><last>Shi</last></author>
      <author><first>Zihao</first><last>Fu</last></author>
      <author><first>Lidong</first><last>Bing</last></author>
      <author><first>Wai</first><last>Lam</last></author>
      <pages>2494&#8211;2504</pages>
      <abstract>Word embeddings have been widely used in sentiment classification because of their efficacy for semantic representations of words. Given reviews from different domains, some existing methods for word embeddings exploit sentiment information, but they cannot produce domain-sensitive embeddings. On the other hand, some other existing methods can generate domain-sensitive word embeddings, but they cannot distinguish words with similar contexts but opposite sentiment polarity. We propose a new method for learning domain-sensitive and sentiment-aware embeddings that simultaneously capture the information of sentiment semantics and domain sensitivity of individual words. Our method can automatically determine and produce domain-common embeddings and domain-specific embeddings. The differentiation of domain-common and domain-specific words enables the advantage of data augmentation of common semantics from multiple domains and capture the varied semantics of specific words from different domains at the same time. Experimental results show that our model provides an effective way to learn domain-sensitive and sentiment-aware word embeddings which benefit sentiment classification at both sentence level and lexicon term level.</abstract>
      <url hash="31b59c66">P18-1232</url>
      <attachment type="poster" hash="859e9259">P18-1232.Poster.pdf</attachment>
      <doi>10.18653/v1/P18-1232</doi>
      <bibkey>shi-etal-2018-learning-domain</bibkey>
    </paper>
    <paper id="233">
      <title>Cross-Domain Sentiment Classification with Target Domain Specific Information</title>
      <author><first>Minlong</first><last>Peng</last></author>
      <author><first>Qi</first><last>Zhang</last></author>
      <author><first>Yu-gang</first><last>Jiang</last></author>
      <author><first>Xuanjing</first><last>Huang</last></author>
      <pages>2505&#8211;2513</pages>
      <abstract>The task of adopting a model with good performance to a target domain that is different from the source domain used for training has received considerable attention in sentiment analysis. Most existing approaches mainly focus on learning representations that are domain-invariant in both the source and target domains. Few of them pay attention to domain-specific information, which should also be informative. In this work, we propose a method to simultaneously extract domain specific and invariant representations and train a classifier on each of the representation, respectively. And we introduce a few target domain labeled data for learning domain-specific information. To effectively utilize the target domain labeled data, we train the domain invariant representation based classifier with both the source and target domain labeled data and train the domain-specific representation based classifier with only the target domain labeled data. These two classifiers then boost each other in a co-training style. Extensive sentiment analysis experiments demonstrated that the proposed method could achieve better performance than state-of-the-art methods.</abstract>
      <url hash="4df14a55">P18-1233</url>
      <attachment type="poster" hash="46fd8259">P18-1233.Poster.pdf</attachment>
      <doi>10.18653/v1/P18-1233</doi>
      <bibkey>peng-etal-2018-cross</bibkey>
    </paper>
    <paper id="234">
      <title>Aspect Based Sentiment Analysis with Gated Convolutional Networks</title>
      <author><first>Wei</first><last>Xue</last></author>
      <author><first>Tao</first><last>Li</last></author>
      <pages>2514&#8211;2523</pages>
      <abstract>Aspect based sentiment analysis (ABSA) can provide more detailed information than general sentiment analysis, because it aims to predict the sentiment polarities of the given aspects or entities in text. We summarize previous approaches into two subtasks: aspect-category sentiment analysis (ACSA) and aspect-term sentiment analysis (ATSA). Most previous approaches employ long short-term memory and attention mechanisms to predict the sentiment polarity of the concerned targets, which are often complicated and need more training time. We propose a model based on convolutional neural networks and gating mechanisms, which is more accurate and efficient. First, the novel Gated Tanh-ReLU Units can selectively output the sentiment features according to the given aspect or entity. The architecture is much simpler than attention layer used in the existing models. Second, the computations of our model could be easily parallelized during training, because convolutional layers do not have time dependency as in LSTM layers, and gating units also work independently. The experiments on SemEval datasets demonstrate the efficiency and effectiveness of our models.</abstract>
      <url hash="e9f14bdf">P18-1234</url>
      <attachment type="poster" hash="64d7b17e">P18-1234.Poster.pdf</attachment>
      <doi>10.18653/v1/P18-1234</doi>
      <bibkey>xue-li-2018-aspect</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/semeval-2014-task-4-sub-task-2">SemEval 2014 Task 4 Sub Task 2</pwcdataset>
    </paper>
    <paper id="235">
      <title>A Helping Hand: Transfer Learning for Deep Sentiment Analysis</title>
      <author><first>Xin</first><last>Dong</last></author>
      <author><first>Gerard</first><last>de Melo</last></author>
      <pages>2524&#8211;2534</pages>
      <abstract>Deep convolutional neural networks excel at sentiment polarity classification, but tend to require substantial amounts of training data, which moreover differs quite significantly between domains. In this work, we present an approach to feed generic cues into the training process of such networks, leading to better generalization abilities given limited training data. We propose to induce sentiment embeddings via supervision on extrinsic data, which are then fed into the model via a dedicated memory-based component. We observe significant gains in effectiveness on a range of different datasets in seven different languages.</abstract>
      <url hash="a3156bf3">P18-1235</url>
      <doi>10.18653/v1/P18-1235</doi>
      <bibkey>dong-de-melo-2018-helping</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multi-domain-sentiment-dataset-v2-0">Multi-Domain Sentiment Dataset v2.0</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="236">
      <title>Cold-Start Aware User and Product Attention for Sentiment Classification</title>
      <author><first>Reinald Kim</first><last>Amplayo</last></author>
      <author><first>Jihyeok</first><last>Kim</last></author>
      <author><first>Sua</first><last>Sung</last></author>
      <author><first>Seung-won</first><last>Hwang</last></author>
      <pages>2535&#8211;2544</pages>
      <abstract>The use of user/product information in sentiment analysis is important, especially for cold-start users/products, whose number of reviews are very limited. However, current models do not deal with the cold-start problem which is typical in review websites. In this paper, we present Hybrid Contextualized Sentiment Classifier (HCSC), which contains two modules: (1) a fast word encoder that returns word vectors embedded with short and long range dependency features; and (2) Cold-Start Aware Attention (CSAA), an attention mechanism that considers the existence of cold-start problem when attentively pooling the encoded word vectors. HCSC introduces shared vectors that are constructed from similar users/products, and are used when the original distinct vectors do not have sufficient information (i.e. cold-start). This is decided by a frequency-guided selective gate vector. Our experiments show that in terms of RMSE, HCSC performs significantly better when compared with on famous datasets, despite having less complexity, and thus can be trained much faster. More importantly, our model performs significantly better than previous models when the training data is sparse and has cold-start problems.</abstract>
      <url hash="733af07a">P18-1236</url>
      <attachment type="poster" hash="17ffede8">P18-1236.Poster.pdf</attachment>
      <doi>10.18653/v1/P18-1236</doi>
      <bibkey>amplayo-etal-2018-cold</bibkey>
      <pwccode url="https://github.com/rktamplayo/HCSC" additional="false">rktamplayo/HCSC</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
    </paper>
    <paper id="237">
      <title>Modeling Deliberative Argumentation Strategies on <fixed-case>W</fixed-case>ikipedia</title>
      <author><first>Khalid</first><last>Al-Khatib</last></author>
      <author><first>Henning</first><last>Wachsmuth</last></author>
      <author><first>Kevin</first><last>Lang</last></author>
      <author><first>Jakob</first><last>Herpel</last></author>
      <author><first>Matthias</first><last>Hagen</last></author>
      <author><first>Benno</first><last>Stein</last></author>
      <pages>2545&#8211;2555</pages>
      <abstract>This paper studies how the argumentation strategies of participants in deliberative discussions can be supported computationally. Our ultimate goal is to predict the best next deliberative move of each participant. In this paper, we present a model for deliberative discussions and we illustrate its operationalization. Previous models have been built manually based on a small set of discussions, resulting in a level of abstraction that is not suitable for move recommendation. In contrast, we derive our model statistically from several types of metadata that can be used for move description. Applied to six million discussions from Wikipedia talk pages, our approach results in a model with 13 categories along three dimensions: discourse acts, argumentative relations, and frames. On this basis, we automatically generate a corpus with about 200,000 turns, labeled for the 13 categories. We then operationalize the model with three supervised classifiers and provide evidence that the proposed categories can be predicted.</abstract>
      <url hash="b5fb5711">P18-1237</url>
      <attachment type="poster" hash="b8a38ac9">P18-1237.Poster.pdf</attachment>
      <doi>10.18653/v1/P18-1237</doi>
      <bibkey>al-khatib-etal-2018-modeling</bibkey>
    </paper>
    <paper id="241">
      <title>Attacking Visual Language Grounding with Adversarial Examples: A Case Study on Neural Image Captioning</title>
      <author><first>Hongge</first><last>Chen</last></author>
      <author><first>Huan</first><last>Zhang</last></author>
      <author><first>Pin-Yu</first><last>Chen</last></author>
      <author><first>Jinfeng</first><last>Yi</last></author>
      <author><first>Cho-Jui</first><last>Hsieh</last></author>
      <pages>2587&#8211;2597</pages>
      <abstract>Visual language grounding is widely studied in modern neural image captioning systems, which typically adopts an encoder-decoder framework consisting of two principal components: a convolutional neural network (CNN) for image feature extraction and a recurrent neural network (RNN) for language caption generation. To study the robustness of language grounding to adversarial perturbations in machine vision and perception, we propose Show-and-Fool, a novel algorithm for crafting adversarial examples in neural image captioning. The proposed algorithm provides two evaluation approaches, which check if we can mislead neural image captioning systems to output some randomly chosen captions or keywords. Our extensive experiments show that our algorithm can successfully craft visually-similar adversarial examples with randomly targeted captions or keywords, and the adversarial examples can be made highly transferable to other image captioning systems. Consequently, our approach leads to new robustness implications of neural image captioning and novel insights in visual language grounding.</abstract>
      <url hash="c989f15f">P18-1241</url>
      <attachment type="note" hash="be1d5ca1">P18-1241.Notes.pdf</attachment>
      <attachment type="poster" hash="db8aedb4">P18-1241.Poster.pdf</attachment>
      <doi>10.18653/v1/P18-1241</doi>
      <bibkey>chen-etal-2018-attacking</bibkey>
      <pwccode url="https://github.com/huanzhang12/ImageCaptioningAttack" additional="true">huanzhang12/ImageCaptioningAttack</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/coco">COCO</pwcdataset>
    </paper>
    <paper id="243">
      <title>Interactive Language Acquisition with One-shot Visual Concept Learning through a Conversational Game</title>
      <author><first>Haichao</first><last>Zhang</last></author>
      <author><first>Haonan</first><last>Yu</last></author>
      <author><first>Wei</first><last>Xu</last></author>
      <pages>2609&#8211;2619</pages>
      <abstract>Building intelligent agents that can communicate with and learn from humans in natural language is of great value. Supervised language learning is limited by the ability of capturing mainly the statistics of training data, and is hardly adaptive to new scenarios or flexible for acquiring new knowledge without inefficient retraining or catastrophic forgetting. We highlight the perspective that conversational interaction serves as a natural interface both for language learning and for novel knowledge acquisition and propose a joint imitation and reinforcement approach for grounded language learning through an interactive conversational game. The agent trained with this approach is able to actively acquire information by asking questions about novel objects and use the just-learned knowledge in subsequent conversations in a one-shot fashion. Results compared with other methods verified the effectiveness of the proposed approach.</abstract>
      <url hash="d90fc70e">P18-1243</url>
      <attachment type="note" hash="b29a3271">P18-1243.Notes.pdf</attachment>
      <attachment type="poster" hash="0723ffa8">P18-1243.Poster.pdf</attachment>
      <doi>10.18653/v1/P18-1243</doi>
      <bibkey>zhang-etal-2018-interactive</bibkey>
      <pwccode url="https://github.com/PaddlePaddle/XWorld" additional="false">PaddlePaddle/XWorld</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/visual-question-answering">Visual Question Answering</pwcdataset>
    </paper>
    <paper id="245">
      <title>A Structured Variational Autoencoder for Contextual Morphological Inflection</title>
      <author><first>Lawrence</first><last>Wolf-Sonkin</last></author>
      <author><first>Jason</first><last>Naradowsky</last></author>
      <author><first>Sabrina J.</first><last>Mielke</last></author>
      <author><first>Ryan</first><last>Cotterell</last></author>
      <pages>2631&#8211;2641</pages>
      <abstract>Statistical morphological inflectors are typically trained on fully supervised, type-level data. One remaining open research question is the following: How can we effectively exploit raw, token-level data to improve their performance? To this end, we introduce a novel generative latent-variable model for the semi-supervised learning of inflection generation. To enable posterior inference over the latent variables, we derive an efficient variational inference procedure based on the wake-sleep algorithm. We experiment on 23 languages, using the Universal Dependencies corpora in a simulated low-resource setting, and find improvements of over 10% absolute accuracy in some cases.</abstract>
      <url hash="2d430d56">P18-1245</url>
      <attachment type="note" hash="b69ddf80">P18-1245.Notes.pdf</attachment>
      <attachment type="poster" hash="de4ec9a8">P18-1245.Poster.pdf</attachment>
      <doi>10.18653/v1/P18-1245</doi>
      <revision id="1" href="P18-1245v1" hash="fd0a3ce8" />
      <revision id="2" href="P18-1245v2" hash="2d430d56" date="2020-09-01">Name change for one of the authors.</revision>
      <bibkey>wolf-sonkin-etal-2018-structured</bibkey>
      <pwccode url="" additional="true" />
    </paper>
    <paper id="248">
      <title>Global Transition-based Non-projective Dependency Parsing</title>
      <author><first>Carlos</first><last>G&#243;mez-Rodr&#237;guez</last></author>
      <author><first>Tianze</first><last>Shi</last></author>
      <author><first>Lillian</first><last>Lee</last></author>
      <pages>2664&#8211;2675</pages>
      <abstract>Shi, Huang, and Lee (2017a) obtained state-of-the-art results for English and Chinese dependency parsing by combining dynamic-programming implementations of transition-based dependency parsers with a minimal set of bidirectional LSTM features. However, their results were limited to projective parsing. In this paper, we extend their approach to support non-projectivity by providing the first practical implementation of the MH&#8324; algorithm, an <tex-math>O(n^4)</tex-math> mildly nonprojective dynamic-programming parser with very high coverage on non-projective treebanks. To make MH&#8324; compatible with minimal transition-based feature sets, we introduce a transition-based interpretation of it in which parser items are mapped to sequences of transitions. We thus obtain the first implementation of global decoding for non-projective transition-based parsing, and demonstrate empirically that it is effective than its projective counterpart in parsing a number of highly non-projective languages.</abstract>
      <url hash="d4b19e2e">P18-1248</url>
      <attachment type="note" hash="680ba58a">P18-1248.Notes.pdf</attachment>
      <doi>10.18653/v1/P18-1248</doi>
      <bibkey>gomez-rodriguez-etal-2018-global</bibkey>
      <pwccode url="https://github.com/tzshi/mh4-parser-acl18" additional="false">tzshi/mh4-parser-acl18</pwccode>
    </paper>
    <paper id="251">
      <title>Composing Finite State Transducers on <fixed-case>GPU</fixed-case>s</title>
      <author><first>Arturo</first><last>Argueta</last></author>
      <author><first>David</first><last>Chiang</last></author>
      <pages>2697&#8211;2705</pages>
      <abstract>Weighted finite state transducers (FSTs) are frequently used in language processing to handle tasks such as part-of-speech tagging and speech recognition. There has been previous work using multiple CPU cores to accelerate finite state algorithms, but limited attention has been given to parallel graphics processing unit (GPU) implementations. In this paper, we introduce the first (to our knowledge) GPU implementation of the FST composition operation, and we also discuss the optimizations used to achieve the best performance on this architecture. We show that our approach obtains speedups of up to 6 times over our serial implementation and 4.5 times over OpenFST.</abstract>
      <url hash="6c259bad">P18-1251</url>
      <attachment type="poster" hash="350e2f17">P18-1251.Poster.pdf</attachment>
      <doi>10.18653/v1/P18-1251</doi>
      <bibkey>argueta-chiang-2018-composing</bibkey>
    </paper>
    <paper id="254">
      <title>Finding syntax in human encephalography with beam search</title>
      <author><first>John</first><last>Hale</last></author>
      <author><first>Chris</first><last>Dyer</last></author>
      <author><first>Adhiguna</first><last>Kuncoro</last></author>
      <author><first>Jonathan</first><last>Brennan</last></author>
      <pages>2727&#8211;2736</pages>
      <abstract>Recurrent neural network grammars (RNNGs) are generative models of (tree , string ) pairs that rely on neural networks to evaluate derivational choices. Parsing with them using beam search yields a variety of incremental complexity metrics such as word surprisal and parser action count. When used as regressors against human electrophysiological responses to naturalistic text, they derive two amplitude effects: an early peak and a P600-like later peak. By contrast, a non-syntactic neural language model yields no reliable effects. Model comparisons attribute the early peak to syntactic composition within the RNNG. This pattern of results recommends the RNNG+beam search combination as a mechanistic model of the syntactic processing that occurs during normal human language comprehension.</abstract>
      <url hash="c7db071e">P18-1254</url>
      <video href="https://vimeo.com/288152828" />
      <doi>10.18653/v1/P18-1254</doi>
      <bibkey>hale-etal-2018-finding</bibkey>
    </paper>
    <paper id="256">
      <title>Let&#8217;s do it &#8220;again&#8221;: A First Computational Approach to Detecting Adverbial Presupposition Triggers</title>
      <author><first>Andre</first><last>Cianflone</last></author>
      <author><first>Yulan</first><last>Feng</last></author>
      <author><first>Jad</first><last>Kabbara</last></author>
      <author><first>Jackie Chi Kit</first><last>Cheung</last></author>
      <pages>2747&#8211;2755</pages>
      <abstract>We introduce the novel task of predicting adverbial presupposition triggers, which is useful for natural language generation tasks such as summarization and dialogue systems. We introduce two new corpora, derived from the Penn Treebank and the Annotated English Gigaword dataset and investigate the use of a novel attention mechanism tailored to this task. Our attention mechanism augments a baseline recurrent neural network without the need for additional trainable parameters, minimizing the added computational cost of our mechanism. We demonstrate that this model statistically outperforms our baselines.</abstract>
      <url hash="b016d196">P18-1256</url>
      <video href="https://vimeo.com/288152712" />
      <attachment type="presentation" hash="cee37898">P18-1256.Presentation.pdf</attachment>
      <doi>10.18653/v1/P18-1256</doi>
      <bibkey>cianflone-etal-2018-lets</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/penn-treebank">Penn Treebank</pwcdataset>
    </paper>
  </volume>
  <volume id="2">
    <meta>
      <booktitle>Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</booktitle>
      <url hash="b0dd545f">P18-2</url>
      <editor><first>Iryna</first><last>Gurevych</last></editor>
      <editor><first>Yusuke</first><last>Miyao</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Melbourne, Australia</address>
      <month>July</month>
      <year>2018</year>
    </meta>
    <frontmatter>
      <url hash="3983da9c">P18-2000</url>
      <bibkey>acl-2018-association-linguistics</bibkey>
    </frontmatter>
    <paper id="2">
      <title>Restricted Recurrent Neural Tensor Networks: Exploiting Word Frequency and Compositionality</title>
      <author><first>Alexandre</first><last>Salle</last></author>
      <author><first>Aline</first><last>Villavicencio</last></author>
      <pages>8&#8211;13</pages>
      <abstract>Increasing the capacity of recurrent neural networks (RNN) usually involves augmenting the size of the hidden layer, with significant increase of computational cost. Recurrent neural tensor networks (RNTN) increase capacity using distinct hidden layer weights for each word, but with greater costs in memory usage. In this paper, we introduce restricted recurrent neural tensor networks (r-RNTN) which reserve distinct hidden layer weights for frequent vocabulary words while sharing a single set of weights for infrequent words. Perplexity evaluations show that for fixed hidden layer sizes, r-RNTNs improve language model performance over RNNs using only a small fraction of the parameters of unrestricted RNTNs. These results hold for r-RNTNs using Gated Recurrent Units and Long Short-Term Memory.</abstract>
      <url hash="8176249b">P18-2002</url>
      <attachment type="poster" hash="472315bc">P18-2002.Poster.pdf</attachment>
      <doi>10.18653/v1/P18-2002</doi>
      <bibkey>salle-villavicencio-2018-restricted</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/penn-treebank">Penn Treebank</pwcdataset>
    </paper>
    <paper id="3">
      <title>Deep <fixed-case>RNN</fixed-case>s Encode Soft Hierarchical Syntax</title>
      <author><first>Terra</first><last>Blevins</last></author>
      <author><first>Omer</first><last>Levy</last></author>
      <author><first>Luke</first><last>Zettlemoyer</last></author>
      <pages>14&#8211;19</pages>
      <abstract>We present a set of experiments to demonstrate that deep recurrent neural networks (RNNs) learn internal representations that capture soft hierarchical notions of syntax from highly varied supervision. We consider four syntax tasks at different depths of the parse tree; for each word, we predict its part of speech as well as the first (parent), second (grandparent) and third level (great-grandparent) constituent labels that appear above it. These predictions are made from representations produced at different depths in networks that are pretrained with one of four objectives: dependency parsing, semantic role labeling, machine translation, or language modeling. In every case, we find a correspondence between network depth and syntactic depth, suggesting that a soft syntactic hierarchy emerges. This effect is robust across all conditions, indicating that the models encode significant amounts of syntax even in the absence of an explicit syntactic training supervision.</abstract>
      <url hash="0f479934">P18-2003</url>
      <attachment type="note" hash="8d6e7827">P18-2003.Notes.pdf</attachment>
      <attachment type="poster" hash="500aa059">P18-2003.Poster.pdf</attachment>
      <doi>10.18653/v1/P18-2003</doi>
      <bibkey>blevins-etal-2018-deep</bibkey>
    </paper>
    <paper id="4">
      <title>Word Error Rate Estimation for Speech Recognition: e-<fixed-case>WER</fixed-case></title>
      <author><first>Ahmed</first><last>Ali</last></author>
      <author><first>Steve</first><last>Renals</last></author>
      <pages>20&#8211;24</pages>
      <abstract>Measuring the performance of automatic speech recognition (ASR) systems requires manually transcribed data in order to compute the word error rate (WER), which is often time-consuming and expensive. In this paper, we propose a novel approach to estimate WER, or e-WER, which does not require a gold-standard transcription of the test set. Our e-WER framework uses a comprehensive set of features: ASR recognised text, character recognition results to complement recognition output, and internal decoder features. We report results for the two features; black-box and glass-box using unseen 24 Arabic broadcast programs. Our system achieves 16.9% WER root mean squared error (RMSE) across 1,400 sentences. The estimated overall WER e-WER was 25.3% for the three hours test set, while the actual WER was 28.5%.</abstract>
      <url hash="f2dd0be2">P18-2004</url>
      <doi>10.18653/v1/P18-2004</doi>
      <bibkey>ali-renals-2018-word</bibkey>
      <pwccode url="https://github.com/qcri/e-wer" additional="false">qcri/e-wer</pwccode>
    </paper>
    <paper id="6">
      <title><fixed-case>H</fixed-case>ot<fixed-case>F</fixed-case>lip: White-Box Adversarial Examples for Text Classification</title>
      <author><first>Javid</first><last>Ebrahimi</last></author>
      <author><first>Anyi</first><last>Rao</last></author>
      <author><first>Daniel</first><last>Lowd</last></author>
      <author><first>Dejing</first><last>Dou</last></author>
      <pages>31&#8211;36</pages>
      <abstract>We propose an efficient method to generate white-box adversarial examples to trick a character-level neural classifier. We find that only a few manipulations are needed to greatly decrease the accuracy. Our method relies on an atomic flip operation, which swaps one token for another, based on the gradients of the one-hot input vectors. Due to efficiency of our method, we can perform adversarial training which makes the model more robust to attacks at test time. With the use of a few semantics-preserving constraints, we demonstrate that HotFlip can be adapted to attack a word-level classifier as well.</abstract>
      <url hash="e6785de5">P18-2006</url>
      <attachment type="poster" hash="29a957ca">P18-2006.Poster.pdf</attachment>
      <doi>10.18653/v1/P18-2006</doi>
      <bibkey>ebrahimi-etal-2018-hotflip</bibkey>
      <pwccode url="" additional="true" />
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="8">
      <title>Active learning for deep semantic parsing</title>
      <author><first>Long</first><last>Duong</last></author>
      <author><first>Hadi</first><last>Afshar</last></author>
      <author><first>Dominique</first><last>Estival</last></author>
      <author><first>Glen</first><last>Pink</last></author>
      <author><first>Philip</first><last>Cohen</last></author>
      <author><first>Mark</first><last>Johnson</last></author>
      <pages>43&#8211;48</pages>
      <abstract>Semantic parsing requires training data that is expensive and slow to collect. We apply active learning to both traditional and &#8220;overnight&#8221; data collection approaches. We show that it is possible to obtain good training hyperparameters from seed data which is only a small fraction of the full dataset. We show that uncertainty sampling based on least confidence score is competitive in traditional data collection but not applicable for overnight collection. We propose several active learning strategies for overnight data collection and show that different example selection strategies per domain perform best.</abstract>
      <url hash="b656078b">P18-2008</url>
      <attachment type="note" hash="f564b07c">P18-2008.Notes.pdf</attachment>
      <attachment type="poster" hash="4fc2ddc4">P18-2008.Poster.pdf</attachment>
      <doi>10.18653/v1/P18-2008</doi>
      <bibkey>duong-etal-2018-active</bibkey>
    </paper>
    <paper id="10">
      <title>Unsupervised Semantic Frame Induction using Triclustering</title>
      <author><first>Dmitry</first><last>Ustalov</last></author>
      <author><first>Alexander</first><last>Panchenko</last></author>
      <author><first>Andrey</first><last>Kutuzov</last></author>
      <author><first>Chris</first><last>Biemann</last></author>
      <author><first>Simone Paolo</first><last>Ponzetto</last></author>
      <pages>55&#8211;62</pages>
      <abstract>We use dependency triples automatically extracted from a Web-scale corpus to perform unsupervised semantic frame induction. We cast the frame induction problem as a triclustering problem that is a generalization of clustering for triadic data. Our replicable benchmarks demonstrate that the proposed graph-based approach, Triframes, shows state-of-the art results on this task on a FrameNet-derived dataset and performing on par with competitive methods on a verb class clustering task.</abstract>
      <url hash="b8b75ecf">P18-2010</url>
      <attachment type="note" hash="da1f2e3d">P18-2010.Notes.pdf</attachment>
      <attachment type="poster" hash="e784f2fc">P18-2010.Poster.pdf</attachment>
      <doi>10.18653/v1/P18-2010</doi>
      <bibkey>ustalov-etal-2018-unsupervised-semantic</bibkey>
      <pwccode url="https://github.com/uhh-lt/triframes" additional="false">uhh-lt/triframes</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/framenet">FrameNet</pwcdataset>
    </paper>
    <paper id="11">
      <title>Identification of Alias Links among Participants in Narratives</title>
      <author><first>Sangameshwar</first><last>Patil</last></author>
      <author><first>Sachin</first><last>Pawar</last></author>
      <author><first>Swapnil</first><last>Hingmire</last></author>
      <author><first>Girish</first><last>Palshikar</last></author>
      <author><first>Vasudeva</first><last>Varma</last></author>
      <author><first>Pushpak</first><last>Bhattacharyya</last></author>
      <pages>63&#8211;68</pages>
      <abstract>Identification of distinct and independent participants (entities of interest) in a narrative is an important task for many NLP applications. This task becomes challenging because these participants are often referred to using multiple aliases. In this paper, we propose an approach based on linguistic knowledge for identification of aliases mentioned using proper nouns, pronouns or noun phrases with common noun headword. We use Markov Logic Network (MLN) to encode the linguistic knowledge for identification of aliases. We evaluate on four diverse history narratives of varying complexity. Our approach performs better than the state-of-the-art approach as well as a combination of standard named entity recognition and coreference resolution techniques.</abstract>
      <url hash="60f00ff0">P18-2011</url>
      <attachment type="software" hash="d73bf43d">P18-2011.Software.zip</attachment>
      <doi>10.18653/v1/P18-2011</doi>
      <bibkey>patil-etal-2018-identification</bibkey>
    </paper>
    <paper id="12">
      <title>Named Entity Recognition With Parallel Recurrent Neural Networks</title>
      <author><first>Andrej</first><last>&#381;ukov-Gregori&#269;</last></author>
      <author><first>Yoram</first><last>Bachrach</last></author>
      <author><first>Sam</first><last>Coope</last></author>
      <pages>69&#8211;74</pages>
      <abstract>We present a new architecture for named entity recognition. Our model employs multiple independent bidirectional LSTM units across the same input and promotes diversity among them by employing an inter-model regularization term. By distributing computation across multiple smaller LSTMs we find a significant reduction in the total number of parameters. We find our architecture achieves state-of-the-art performance on the CoNLL 2003 NER dataset.</abstract>
      <url hash="9fe267c4">P18-2012</url>
      <attachment type="poster" hash="94ab9fe8">P18-2012.Poster.pdf</attachment>
      <doi>10.18653/v1/P18-2012</doi>
      <bibkey>zukov-gregoric-etal-2018-named</bibkey>
    </paper>
    <paper id="14">
      <title>A Walk-based Model on Entity Graphs for Relation Extraction</title>
      <author><first>Fenia</first><last>Christopoulou</last></author>
      <author><first>Makoto</first><last>Miwa</last></author>
      <author><first>Sophia</first><last>Ananiadou</last></author>
      <pages>81&#8211;88</pages>
      <abstract>We present a novel graph-based neural network model for relation extraction. Our model treats multiple pairs in a sentence simultaneously and considers interactions among them. All the entities in a sentence are placed as nodes in a fully-connected graph structure. The edges are represented with position-aware contexts around the entity pairs. In order to consider different relation paths between two entities, we construct up to <tex-math>l</tex-math>-length walks between each pair. The resulting walks are merged and iteratively used to update the edge representations into longer walks representations. We show that the model achieves performance comparable to the state-of-the-art systems on the ACE 2005 dataset without using any external tools.</abstract>
      <url hash="4130a5a8">P18-2014</url>
      <attachment type="poster" hash="aef0b4c7">P18-2014.Poster.pdf</attachment>
      <doi>10.18653/v1/P18-2014</doi>
      <bibkey>christopoulou-etal-2018-walk</bibkey>
      <pwccode url="https://github.com/fenchri/walk-based-re" additional="false">fenchri/walk-based-re</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/ace-2005">ACE 2005</pwcdataset>
    </paper>
    <paper id="16">
      <title>Automatic Extraction of Commonsense <fixed-case>L</fixed-case>ocated<fixed-case>N</fixed-case>ear Knowledge</title>
      <author><first>Frank F.</first><last>Xu</last></author>
      <author><first>Bill Yuchen</first><last>Lin</last></author>
      <author><first>Kenny</first><last>Zhu</last></author>
      <pages>96&#8211;101</pages>
      <abstract>LocatedNear relation is a kind of commonsense knowledge describing two physical objects that are typically found near each other in real life. In this paper, we study how to automatically extract such relationship through a sentence-level relation classifier and aggregating the scores of entity pairs from a large corpus. Also, we release two benchmark datasets for evaluation and future research.</abstract>
      <url hash="0f947f58">P18-2016</url>
      <attachment type="poster" hash="23e53390">P18-2016.Poster.pdf</attachment>
      <doi>10.18653/v1/P18-2016</doi>
      <bibkey>xu-etal-2018-automatic</bibkey>
      <pwccode url="https://github.com/adapt-sjtu/commonsense-locatednear" additional="false">adapt-sjtu/commonsense-locatednear</pwccode>
    </paper>
    <paper id="20">
      <title>A Named Entity Recognition Shootout for <fixed-case>G</fixed-case>erman</title>
      <author><first>Martin</first><last>Riedl</last></author>
      <author><first>Sebastian</first><last>Pad&#243;</last></author>
      <pages>120&#8211;125</pages>
      <abstract>We ask how to practically build a model for German named entity recognition (NER) that performs at the state of the art for both contemporary and historical texts, i.e., a big-data and a small-data scenario. The two best-performing model families are pitted against each other (linear-chain CRFs and BiLSTM) to observe the trade-off between expressiveness and data requirements. BiLSTM outperforms the CRF when large datasets are available and performs inferior for the smallest dataset. BiLSTMs profit substantially from transfer learning, which enables them to be trained on multiple corpora, resulting in a new state-of-the-art model for German NER on two contemporary German corpora (CoNLL 2003 and GermEval 2014) and two historic corpora.</abstract>
      <url hash="52c050cf">P18-2020</url>
      <attachment type="poster" hash="31fab8f2">P18-2020.Poster.pdf</attachment>
      <doi>10.18653/v1/P18-2020</doi>
      <bibkey>riedl-pado-2018-named</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/conll-2003">CoNLL-2003</pwcdataset>
    </paper>
    <paper id="21">
      <title>A dataset for identifying actionable feedback in collaborative software development</title>
      <author><first>Benjamin S.</first><last>Meyers</last></author>
      <author><first>Nuthan</first><last>Munaiah</last></author>
      <author><first>Emily</first><last>Prud&#8217;hommeaux</last></author>
      <author><first>Andrew</first><last>Meneely</last></author>
      <author><first>Josephine</first><last>Wolff</last></author>
      <author><first>Cecilia</first><last>Ovesdotter Alm</last></author>
      <author><first>Pradeep</first><last>Murukannaiah</last></author>
      <pages>126&#8211;131</pages>
      <abstract>Software developers and testers have long struggled with how to elicit proactive responses from their coworkers when reviewing code for security vulnerabilities and errors. For a code review to be successful, it must not only identify potential problems but also elicit an active response from the colleague responsible for modifying the code. To understand the factors that contribute to this outcome, we analyze a novel dataset of more than one million code reviews for the Google Chromium project, from which we extract linguistic features of feedback that elicited responsive actions from coworkers. Using a manually-labeled subset of reviewer comments, we trained a highly accurate classifier to identify acted-upon comments (AUC = 0.85). Our results demonstrate the utility of our dataset, the feasibility of using NLP for this new task, and the potential of NLP to improve our understanding of how communications between colleagues can be authored to elicit positive, proactive responses.</abstract>
      <url hash="94cccafb">P18-2021</url>
      <doi>10.18653/v1/P18-2021</doi>
      <bibkey>meyers-etal-2018-dataset</bibkey>
    </paper>
    <paper id="23">
      <title>Analogical Reasoning on <fixed-case>C</fixed-case>hinese Morphological and Semantic Relations</title>
      <author><first>Shen</first><last>Li</last></author>
      <author><first>Zhe</first><last>Zhao</last></author>
      <author><first>Renfen</first><last>Hu</last></author>
      <author><first>Wensi</first><last>Li</last></author>
      <author><first>Tao</first><last>Liu</last></author>
      <author><first>Xiaoyong</first><last>Du</last></author>
      <pages>138&#8211;143</pages>
      <abstract>Analogical reasoning is effective in capturing linguistic regularities. This paper proposes an analogical reasoning task on Chinese. After delving into Chinese lexical knowledge, we sketch 68 implicit morphological relations and 28 explicit semantic relations. A big and balanced dataset CA8 is then built for this task, including 17813 questions. Furthermore, we systematically explore the influences of vector representations, context features, and corpora on analogical reasoning. With the experiments, CA8 is proved to be a reliable benchmark for evaluating Chinese word embeddings.</abstract>
      <url hash="4d421fb0">P18-2023</url>
      <attachment type="poster" hash="712fcb42">P18-2023.Poster.pdf</attachment>
      <doi>10.18653/v1/P18-2023</doi>
      <bibkey>li-etal-2018-analogical</bibkey>
      <pwccode url="https://github.com/Embedding/Chinese-Word-Vectors" additional="true">Embedding/Chinese-Word-Vectors</pwccode>
    </paper>
    <paper id="24">
      <title>Construction of a <fixed-case>C</fixed-case>hinese Corpus for the Analysis of the Emotionality of Metaphorical Expressions</title>
      <author><first>Dongyu</first><last>Zhang</last></author>
      <author><first>Hongfei</first><last>Lin</last></author>
      <author><first>Liang</first><last>Yang</last></author>
      <author><first>Shaowu</first><last>Zhang</last></author>
      <author><first>Bo</first><last>Xu</last></author>
      <pages>144&#8211;150</pages>
      <abstract>Metaphors are frequently used to convey emotions. However, there is little research on the construction of metaphor corpora annotated with emotion for the analysis of emotionality of metaphorical expressions. Furthermore, most studies focus on English, and few in other languages, particularly Sino-Tibetan languages such as Chinese, for emotion analysis from metaphorical texts, although there are likely to be many differences in emotional expressions of metaphorical usages across different languages. We therefore construct a significant new corpus on metaphor, with 5,605 manually annotated sentences in Chinese. We present an annotation scheme that contains annotations of linguistic metaphors, emotional categories (joy, anger, sadness, fear, love, disgust and surprise), and intensity. The annotation agreement analyses for multiple annotators are described. We also use the corpus to explore and analyze the emotionality of metaphors. To the best of our knowledge, this is the first relatively large metaphor corpus with an annotation of emotions in Chinese.</abstract>
      <url hash="d10cc2be">P18-2024</url>
      <attachment type="note" hash="73e25375">P18-2024.Notes.pdf</attachment>
      <attachment type="poster" hash="1e3a5dc6">P18-2024.Poster.pdf</attachment>
      <doi>10.18653/v1/P18-2024</doi>
      <bibkey>zhang-etal-2018-construction</bibkey>
    </paper>
    <paper id="28">
      <title>A Language Model based Evaluator for Sentence Compression</title>
      <author><first>Yang</first><last>Zhao</last></author>
      <author><first>Zhiyuan</first><last>Luo</last></author>
      <author><first>Akiko</first><last>Aizawa</last></author>
      <pages>170&#8211;175</pages>
      <abstract>We herein present a language-model-based evaluator for deletion-based sentence compression and view this task as a series of deletion-and-evaluation operations using the evaluator. More specifically, the evaluator is a syntactic neural language model that is first built by learning the syntactic and structural collocation among words. Subsequently, a series of trial-and-error deletion operations are conducted on the source sentences via a reinforcement learning framework to obtain the best target compression. An empirical study shows that the proposed model can effectively generate more readable compression, comparable or superior to several strong baselines. Furthermore, we introduce a 200-sentence test set for a large-scale dataset, setting a new baseline for the future research.</abstract>
      <url hash="ffcf2bc9">P18-2028</url>
      <attachment type="note" hash="6a9e6826">P18-2028.Notes.pdf</attachment>
      <attachment type="poster" hash="da96ba36">P18-2028.Poster.pdf</attachment>
      <doi>10.18653/v1/P18-2028</doi>
      <bibkey>zhao-etal-2018-language</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/google">Google</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sentence-compression">Sentence Compression</pwcdataset>
    </paper>
    <paper id="29">
      <title>Identifying and Understanding User Reactions to Deceptive and Trusted Social News Sources</title>
      <author><first>Maria</first><last>Glenski</last></author>
      <author><first>Tim</first><last>Weninger</last></author>
      <author><first>Svitlana</first><last>Volkova</last></author>
      <pages>176&#8211;181</pages>
      <abstract>In the age of social news, it is important to understand the types of reactions that are evoked from news sources with various levels of credibility. In the present work we seek to better understand how users react to trusted and deceptive news sources across two popular, and very different, social media platforms. To that end, (1) we develop a model to classify user reactions into one of nine types, such as answer, elaboration, and question, etc, and (2) we measure the speed and the type of reaction for trusted and deceptive news sources for 10.8M Twitter posts and 6.2M Reddit comments. We show that there are significant differences in the speed and the type of reactions between trusted and deceptive news sources on Twitter, but far smaller differences on Reddit.</abstract>
      <url hash="c1539742">P18-2029</url>
      <doi>10.18653/v1/P18-2029</doi>
      <bibkey>glenski-etal-2018-identifying</bibkey>
    </paper>
    <paper id="31">
      <title>Fighting Offensive Language on Social Media with Unsupervised Text Style Transfer</title>
      <author><first>Cicero</first><last>Nogueira dos Santos</last></author>
      <author><first>Igor</first><last>Melnyk</last></author>
      <author><first>Inkit</first><last>Padhi</last></author>
      <pages>189&#8211;194</pages>
      <abstract>We introduce a new approach to tackle the problem of offensive language in online social media. Our approach uses unsupervised text style transfer to translate offensive sentences into non-offensive ones. We propose a new method for training encoder-decoders using non-parallel data that combines a collaborative classifier, attention and the cycle consistency loss. Experimental results on data from Twitter and Reddit show that our method outperforms a state-of-the-art text style transfer system in two out of three quantitative metrics and produces reliable non-offensive transferred sentences.</abstract>
      <url hash="16bdb32a">P18-2031</url>
      <doi>10.18653/v1/P18-2031</doi>
      <bibkey>nogueira-dos-santos-etal-2018-fighting</bibkey>
    </paper>
    <paper id="33">
      <title>Task-oriented Dialogue System for Automatic Diagnosis</title>
      <author><first>Zhongyu</first><last>Wei</last></author>
      <author><first>Qianlong</first><last>Liu</last></author>
      <author><first>Baolin</first><last>Peng</last></author>
      <author><first>Huaixiao</first><last>Tou</last></author>
      <author><first>Ting</first><last>Chen</last></author>
      <author><first>Xuanjing</first><last>Huang</last></author>
      <author><first>Kam-fai</first><last>Wong</last></author>
      <author><first>Xiangying</first><last>Dai</last></author>
      <pages>201&#8211;207</pages>
      <abstract>In this paper, we make a move to build a dialogue system for automatic diagnosis. We first build a dataset collected from an online medical forum by extracting symptoms from both patients&#8217; self-reports and conversational data between patients and doctors. Then we propose a task-oriented dialogue system framework to make diagnosis for patients automatically, which can converse with patients to collect additional symptoms beyond their self-reports. Experimental results on our dataset show that additional symptoms extracted from conversation can greatly improve the accuracy for disease identification and our dialogue system is able to collect these symptoms automatically and make a better diagnosis.</abstract>
      <url hash="c46781de">P18-2033</url>
      <attachment type="poster" hash="75477db9">P18-2033.Poster.pdf</attachment>
      <doi>10.18653/v1/P18-2033</doi>
      <bibkey>wei-etal-2018-task</bibkey>
    </paper>
    <paper id="34">
      <title>Transfer Learning for Context-Aware Question Matching in Information-seeking Conversations in <fixed-case>E</fixed-case>-commerce</title>
      <author><first>Minghui</first><last>Qiu</last></author>
      <author><first>Liu</first><last>Yang</last></author>
      <author><first>Feng</first><last>Ji</last></author>
      <author><first>Wei</first><last>Zhou</last></author>
      <author><first>Jun</first><last>Huang</last></author>
      <author><first>Haiqing</first><last>Chen</last></author>
      <author><first>Bruce</first><last>Croft</last></author>
      <author><first>Wei</first><last>Lin</last></author>
      <pages>208&#8211;213</pages>
      <abstract>Building multi-turn information-seeking conversation systems is an important and challenging research topic. Although several advanced neural text matching models have been proposed for this task, they are generally not efficient for industrial applications. Furthermore, they rely on a large amount of labeled data, which may not be available in real-world applications. To alleviate these problems, we study transfer learning for multi-turn information seeking conversations in this paper. We first propose an efficient and effective multi-turn conversation model based on convolutional neural networks. After that, we extend our model to adapt the knowledge learned from a resource-rich domain to enhance the performance. Finally, we deployed our model in an industrial chatbot called AliMe Assist and observed a significant improvement over the existing online model.</abstract>
      <url hash="e049b288">P18-2034</url>
      <doi>10.18653/v1/P18-2034</doi>
      <bibkey>qiu-etal-2018-transfer</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/ubuntu-dialogue-corpus">UDC</pwcdataset>
    </paper>
    <paper id="35">
      <title>A Multi-task Approach to Learning Multilingual Representations</title>
      <author><first>Karan</first><last>Singla</last></author>
      <author><first>Dogan</first><last>Can</last></author>
      <author><first>Shrikanth</first><last>Narayanan</last></author>
      <pages>214&#8211;220</pages>
      <abstract>We present a novel multi-task modeling approach to learning multilingual distributed representations of text. Our system learns word and sentence embeddings jointly by training a multilingual skip-gram model together with a cross-lingual sentence similarity model. Our architecture can transparently use both monolingual and sentence aligned bilingual corpora to learn multilingual embeddings, thus covering a vocabulary significantly larger than the vocabulary of the bilingual corpora alone. Our model shows competitive performance in a standard cross-lingual document classification task. We also show the effectiveness of our method in a limited resource scenario.</abstract>
      <url hash="c6ed25ef">P18-2035</url>
      <attachment type="poster" hash="2f0a3d4c">P18-2035.Poster.pdf</attachment>
      <doi>10.18653/v1/P18-2035</doi>
      <bibkey>singla-etal-2018-multi</bibkey>
    </paper>
    <paper id="36">
      <title>Characterizing Departures from Linearity in Word Translation</title>
      <author><first>Ndapa</first><last>Nakashole</last></author>
      <author><first>Raphael</first><last>Flauger</last></author>
      <pages>221&#8211;227</pages>
      <abstract>We investigate the behavior of maps learned by machine translation methods. The maps translate words by projecting between word embedding spaces of different languages. We locally approximate these maps using linear maps, and find that they vary across the word embedding space. This demonstrates that the underlying maps are non-linear. Importantly, we show that the locally linear maps vary by an amount that is tightly correlated with the distance between the neighborhoods on which they are trained. Our results can be used to test non-linear methods, and to drive the design of more accurate maps for word translation.</abstract>
      <url hash="7879c35a">P18-2036</url>
      <doi>10.18653/v1/P18-2036</doi>
      <bibkey>nakashole-flauger-2018-characterizing</bibkey>
    </paper>
    <paper id="38">
      <title>Hybrid semi-<fixed-case>M</fixed-case>arkov <fixed-case>CRF</fixed-case> for Neural Sequence Labeling</title>
      <author><first>Zhixiu</first><last>Ye</last></author>
      <author><first>Zhen-Hua</first><last>Ling</last></author>
      <pages>235&#8211;240</pages>
      <abstract>This paper proposes hybrid semi-Markov conditional random fields (SCRFs) for neural sequence labeling in natural language processing. Based on conventional conditional random fields (CRFs), SCRFs have been designed for the tasks of assigning labels to segments by extracting features from and describing transitions between segments instead of words. In this paper, we improve the existing SCRF methods by employing word-level and segment-level information simultaneously. First, word-level labels are utilized to derive the segment scores in SCRFs. Second, a CRF output layer and an SCRF output layer are integrated into a unified neural network and trained jointly. Experimental results on CoNLL 2003 named entity recognition (NER) shared task show that our model achieves state-of-the-art performance when no external knowledge is used.</abstract>
      <url hash="3c31afbe">P18-2038</url>
      <attachment type="poster" hash="578bbfcb">P18-2038.Poster.pdf</attachment>
      <doi>10.18653/v1/P18-2038</doi>
      <bibkey>ye-ling-2018-hybrid</bibkey>
      <pwccode url="https://github.com/ZhixiuYe/HSCRF-pytorch" additional="false">ZhixiuYe/HSCRF-pytorch</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/conll-2003">CoNLL-2003</pwcdataset>
    </paper>
    <paper id="42">
      <title>Paper Abstract Writing through Editing Mechanism</title>
      <author><first>Qingyun</first><last>Wang</last></author>
      <author><first>Zhihao</first><last>Zhou</last></author>
      <author><first>Lifu</first><last>Huang</last></author>
      <author><first>Spencer</first><last>Whitehead</last></author>
      <author><first>Boliang</first><last>Zhang</last></author>
      <author><first>Heng</first><last>Ji</last></author>
      <author><first>Kevin</first><last>Knight</last></author>
      <pages>260&#8211;265</pages>
      <abstract>We present a paper abstract writing system based on an attentive neural sequence-to-sequence model that can take a title as input and automatically generate an abstract. We design a novel Writing-editing Network that can attend to both the title and the previously generated abstract drafts and then iteratively revise and polish the abstract. With two series of Turing tests, where the human judges are asked to distinguish the system-generated abstracts from human-written ones, our system passes Turing tests by junior domain experts at a rate up to 30% and by non-expert at a rate up to 80%.</abstract>
      <url hash="aa8fa1d5">P18-2042</url>
      <attachment type="poster" hash="6dcb3459">P18-2042.Poster.pdf</attachment>
      <doi>10.18653/v1/P18-2042</doi>
      <bibkey>wang-etal-2018-paper</bibkey>
      <pwccode url="https://github.com/EagleW/ACL_titles_abstracts_dataset" additional="true">EagleW/ACL_titles_abstracts_dataset</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/acl-title-and-abstract-dataset">ACL Title and Abstract Dataset</pwcdataset>
    </paper>
    <paper id="43">
      <title>Conditional Generators of Words Definitions</title>
      <author><first>Artyom</first><last>Gadetsky</last></author>
      <author><first>Ilya</first><last>Yakubovskiy</last></author>
      <author><first>Dmitry</first><last>Vetrov</last></author>
      <pages>266&#8211;271</pages>
      <abstract>We explore recently introduced definition modeling technique that provided the tool for evaluation of different distributed vector representations of words through modeling dictionary definitions of words. In this work, we study the problem of word ambiguities in definition modeling and propose a possible solution by employing latent variable modeling and soft attention mechanisms. Our quantitative and qualitative evaluation and analysis of the model shows that taking into account words&#8217; ambiguity and polysemy leads to performance improvement.</abstract>
      <url hash="9a268e3f">P18-2043</url>
      <doi>10.18653/v1/P18-2043</doi>
      <bibkey>gadetsky-etal-2018-conditional</bibkey>
      <pwccode url="https://github.com/agadetsky/pytorch-definitions" additional="false">agadetsky/pytorch-definitions</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/wikitext-103">WikiText-103</pwcdataset>
    </paper>
    <paper id="45">
      <title>Narrative Modeling with Memory Chains and Semantic Supervision</title>
      <author id="fei-liu-unimelb"><first>Fei</first><last>Liu</last></author>
      <author><first>Trevor</first><last>Cohn</last></author>
      <author><first>Timothy</first><last>Baldwin</last></author>
      <pages>278&#8211;284</pages>
      <abstract>Story comprehension requires a deep semantic understanding of the narrative, making it a challenging task. Inspired by previous studies on ROC Story Cloze Test, we propose a novel method, tracking various semantic aspects with external neural memory chains while encouraging each to focus on a particular semantic aspect. Evaluated on the task of story ending prediction, our model demonstrates superior performance to a collection of competitive baselines, setting a new state of the art.</abstract>
      <url hash="f70ad355">P18-2045</url>
      <doi>10.18653/v1/P18-2045</doi>
      <bibkey>liu-etal-2018-narrative</bibkey>
      <pwccode url="https://github.com/liufly/narrative-modeling" additional="false">liufly/narrative-modeling</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/rocstories">ROCStories</pwcdataset>
    </paper>
    <paper id="46">
      <title>Injecting Relational Structural Representation in Neural Networks for Question Similarity</title>
      <author><first>Antonio</first><last>Uva</last></author>
      <author><first>Daniele</first><last>Bonadiman</last></author>
      <author><first>Alessandro</first><last>Moschitti</last></author>
      <pages>285&#8211;291</pages>
      <abstract>Effectively using full syntactic parsing information in Neural Networks (NNs) for solving relational tasks, e.g., question similarity, is still an open problem. In this paper, we propose to inject structural representations in NNs by (i) learning a model with Tree Kernels (TKs) on relatively few pairs of questions (few thousands) as gold standard (GS) training data is typically scarce, (ii) predicting labels on a very large corpus of question pairs, and (iii) pre-training NNs on such large corpus. The results on Quora and SemEval question similarity datasets show that NNs using our approach can learn more accurate models, especially after fine tuning on GS.</abstract>
      <url hash="a35c44f7">P18-2046</url>
      <attachment type="poster" hash="51c92357">P18-2046.Poster.pdf</attachment>
      <doi>10.18653/v1/P18-2046</doi>
      <bibkey>uva-etal-2018-injecting</bibkey>
      <pwccode url="https://github.com/aseveryn/deep-qa" additional="false">aseveryn/deep-qa</pwccode>
    </paper>
    <paper id="47">
      <title>A Simple and Effective Approach to Coverage-Aware Neural Machine Translation</title>
      <author><first>Yanyang</first><last>Li</last></author>
      <author><first>Tong</first><last>Xiao</last></author>
      <author><first>Yinqiao</first><last>Li</last></author>
      <author><first>Qiang</first><last>Wang</last></author>
      <author><first>Changming</first><last>Xu</last></author>
      <author><first>Jingbo</first><last>Zhu</last></author>
      <pages>292&#8211;297</pages>
      <abstract>We offer a simple and effective method to seek a better balance between model confidence and length preference for Neural Machine Translation (NMT). Unlike the popular length normalization and coverage models, our model does not require training nor reranking the limited n-best outputs. Moreover, it is robust to large beam sizes, which is not well studied in previous work. On the Chinese-English and English-German translation tasks, our approach yields +0.4 1.5 BLEU improvements over the state-of-the-art baselines.</abstract>
      <url hash="e918abee">P18-2047</url>
      <attachment type="note" hash="0070d0c9">P18-2047.Notes.pdf</attachment>
      <attachment type="poster" hash="7a740af2">P18-2047.Poster.pdf</attachment>
      <doi>10.18653/v1/P18-2047</doi>
      <bibkey>li-etal-2018-simple</bibkey>
    </paper>
    <paper id="48">
      <title>Dynamic Sentence Sampling for Efficient Training of Neural Machine Translation</title>
      <author><first>Rui</first><last>Wang</last></author>
      <author><first>Masao</first><last>Utiyama</last></author>
      <author><first>Eiichiro</first><last>Sumita</last></author>
      <pages>298&#8211;304</pages>
      <abstract>Traditional Neural machine translation (NMT) involves a fixed training procedure where each sentence is sampled once during each epoch. In reality, some sentences are well-learned during the initial few epochs; however, using this approach, the well-learned sentences would continue to be trained along with those sentences that were not well learned for 10-30 epochs, which results in a wastage of time. Here, we propose an efficient method to dynamically sample the sentences in order to accelerate the NMT training. In this approach, a weight is assigned to each sentence based on the measured difference between the training costs of two iterations. Further, in each epoch, a certain percentage of sentences are dynamically sampled according to their weights. Empirical results based on the NIST Chinese-to-English and the WMT English-to-German tasks show that the proposed method can significantly accelerate the NMT training and improve the NMT performance.</abstract>
      <url hash="d75801fe">P18-2048</url>
      <attachment type="poster" hash="fa706ade">P18-2048.Poster.pdf</attachment>
      <doi>10.18653/v1/P18-2048</doi>
      <bibkey>wang-etal-2018-dynamic</bibkey>
    </paper>
    <paper id="50">
      <title>Extreme Adaptation for Personalized Neural Machine Translation</title>
      <author><first>Paul</first><last>Michel</last></author>
      <author><first>Graham</first><last>Neubig</last></author>
      <pages>312&#8211;318</pages>
      <abstract>Every person speaks or writes their own flavor of their native language, influenced by a number of factors: the content they tend to talk about, their gender, their social status, or their geographical origin. When attempting to perform Machine Translation (MT), these variations have a significant effect on how the system should perform translation, but this is not captured well by standard one-size-fits-all models. In this paper, we propose a simple and parameter-efficient adaptation technique that only requires adapting the bias of the output softmax to each particular user of the MT system, either directly or through a factored approximation. Experiments on TED talks in three languages demonstrate improvements in translation accuracy, and better reflection of speaker traits in the target text.</abstract>
      <url hash="2d3c0570">P18-2050</url>
      <attachment type="note" hash="a9c4c831">P18-2050.Notes.pdf</attachment>
      <doi>10.18653/v1/P18-2050</doi>
      <bibkey>michel-neubig-2018-extreme</bibkey>
      <pwccode url="https://github.com/neulab/extreme-adaptation-for-personalized-translation" additional="false">neulab/extreme-adaptation-for-personalized-translation</pwccode>
    </paper>
    <paper id="52">
      <title>Learning from Chunk-based Feedback in Neural Machine Translation</title>
      <author><first>Pavel</first><last>Petrushkov</last></author>
      <author><first>Shahram</first><last>Khadivi</last></author>
      <author><first>Evgeny</first><last>Matusov</last></author>
      <pages>326&#8211;331</pages>
      <abstract>We empirically investigate learning from partial feedback in neural machine translation (NMT), when partial feedback is collected by asking users to highlight a correct chunk of a translation. We propose a simple and effective way of utilizing such feedback in NMT training. We demonstrate how the common machine translation problem of domain mismatch between training and deployment can be reduced solely based on chunk-level user feedback. We conduct a series of simulation experiments to test the effectiveness of the proposed method. Our results show that chunk-level feedback outperforms sentence based feedback by up to 2.61% BLEU absolute.</abstract>
      <url hash="dd57969e">P18-2052</url>
      <doi>10.18653/v1/P18-2052</doi>
      <bibkey>petrushkov-etal-2018-learning</bibkey>
    </paper>
    <paper id="53">
      <title>Bag-of-Words as Target for Neural Machine Translation</title>
      <author><first>Shuming</first><last>Ma</last></author>
      <author><first>Xu</first><last>Sun</last></author>
      <author><first>Yizhong</first><last>Wang</last></author>
      <author><first>Junyang</first><last>Lin</last></author>
      <pages>332&#8211;338</pages>
      <abstract>A sentence can be translated into more than one correct sentences. However, most of the existing neural machine translation models only use one of the correct translations as the targets, and the other correct sentences are punished as the incorrect sentences in the training stage. Since most of the correct translations for one sentence share the similar bag-of-words, it is possible to distinguish the correct translations from the incorrect ones by the bag-of-words. In this paper, we propose an approach that uses both the sentences and the bag-of-words as targets in the training stage, in order to encourage the model to generate the potentially correct sentences that are not appeared in the training set. We evaluate our model on a Chinese-English translation dataset, and experiments show our model outperforms the strong baselines by the BLEU score of 4.55.</abstract>
      <url hash="3212a6d8">P18-2053</url>
      <doi>10.18653/v1/P18-2053</doi>
      <bibkey>ma-etal-2018-bag</bibkey>
      <pwccode url="https://github.com/lancopku/bag-of-words" additional="false">lancopku/bag-of-words</pwccode>
    </paper>
    <paper id="54">
      <title>Improving Beam Search by Removing Monotonic Constraint for Neural Machine Translation</title>
      <author><first>Raphael</first><last>Shu</last></author>
      <author><first>Hideki</first><last>Nakayama</last></author>
      <pages>339&#8211;344</pages>
      <abstract>To achieve high translation performance, neural machine translation models usually rely on the beam search algorithm for decoding sentences. The beam search finds good candidate translations by considering multiple hypotheses of translations simultaneously. However, as the algorithm produces hypotheses in a monotonic left-to-right order, a hypothesis can not be revisited once it is discarded. We found such monotonicity forces the algorithm to sacrifice some good decoding paths. To mitigate this problem, we relax the monotonic constraint of the beam search by maintaining all found hypotheses in a single priority queue and using a universal score function for hypothesis selection. The proposed algorithm allows discarded hypotheses to be recovered in a later step. Despite its simplicity, we show that the proposed decoding algorithm enhances the quality of selected hypotheses and improve the translations even for high-performance models in English-Japanese translation task.</abstract>
      <url hash="931dada6">P18-2054</url>
      <attachment type="note" hash="b5911513">P18-2054.Notes.pdf</attachment>
      <attachment type="poster" hash="084eed8b">P18-2054.Poster.pdf</attachment>
      <doi>10.18653/v1/P18-2054</doi>
      <bibkey>shu-nakayama-2018-improving</bibkey>
    </paper>
    <paper id="55">
      <title>Leveraging distributed representations and lexico-syntactic fixedness for token-level prediction of the idiomaticity of <fixed-case>E</fixed-case>nglish verb-noun combinations</title>
      <author><first>Milton</first><last>King</last></author>
      <author><first>Paul</first><last>Cook</last></author>
      <pages>345&#8211;350</pages>
      <abstract>Verb-noun combinations (VNCs) - e.g., blow the whistle, hit the roof, and see stars - are a common type of English idiom that are ambiguous with literal usages. In this paper we propose and evaluate models for classifying VNC usages as idiomatic or literal, based on a variety of approaches to forming distributed representations. Our results show that a model based on averaging word embeddings performs on par with, or better than, a previously-proposed approach based on skip-thoughts. Idiomatic usages of VNCs are known to exhibit lexico-syntactic fixedness. We further incorporate this information into our models, demonstrating that this rich linguistic knowledge is complementary to the information carried by distributed representations.</abstract>
      <url hash="6e35cbd3">P18-2055</url>
      <video href="https://vimeo.com/285803899" />
      <attachment type="presentation" hash="ca6c4126">P18-2055.Presentation.pdf</attachment>
      <doi>10.18653/v1/P18-2055</doi>
      <bibkey>king-cook-2018-leveraging</bibkey>
    </paper>
    <paper id="56">
      <title>Using pseudo-senses for improving the extraction of synonyms from word embeddings</title>
      <author><first>Olivier</first><last>Ferret</last></author>
      <pages>351&#8211;357</pages>
      <abstract>The methods proposed recently for specializing word embeddings according to a particular perspective generally rely on external knowledge. In this article, we propose Pseudofit, a new method for specializing word embeddings according to semantic similarity without any external knowledge. Pseudofit exploits the notion of pseudo-sense for building several representations for each word and uses these representations for making the initial embeddings more generic. We illustrate the interest of Pseudofit for acquiring synonyms and study several variants of Pseudofit according to this perspective.</abstract>
      <url hash="e5612bc4">P18-2056</url>
      <video href="https://vimeo.com/285803915" />
      <attachment type="presentation" hash="14c20242">P18-2056.Presentation.pdf</attachment>
      <doi>10.18653/v1/P18-2056</doi>
      <bibkey>ferret-2018-using</bibkey>
    </paper>
    <paper id="57">
      <title>Hearst Patterns Revisited: Automatic Hypernym Detection from Large Text Corpora</title>
      <author><first>Stephen</first><last>Roller</last></author>
      <author><first>Douwe</first><last>Kiela</last></author>
      <author><first>Maximilian</first><last>Nickel</last></author>
      <pages>358&#8211;363</pages>
      <abstract>Methods for unsupervised hypernym detection may broadly be categorized according to two paradigms: pattern-based and distributional methods. In this paper, we study the performance of both approaches on several hypernymy tasks and find that simple pattern-based methods consistently outperform distributional methods on common benchmark datasets. Our results show that pattern-based models provide important contextual constraints which are not yet captured in distributional methods.</abstract>
      <url hash="8b88daf6">P18-2057</url>
      <video href="https://vimeo.com/285803929" />
      <attachment type="presentation" hash="0d24e3ad">P18-2057.Presentation.pdf</attachment>
      <doi>10.18653/v1/P18-2057</doi>
      <bibkey>roller-etal-2018-hearst</bibkey>
      <pwccode url="https://github.com/facebookresearch/hypernymysuite" additional="true">facebookresearch/hypernymysuite</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/hyperlex">HyperLex</pwcdataset>
    </paper>
    <paper id="59">
      <title>Sparse and Constrained Attention for Neural Machine Translation</title>
      <author><first>Chaitanya</first><last>Malaviya</last></author>
      <author><first>Pedro</first><last>Ferreira</last></author>
      <author><first>Andr&#233; F. T.</first><last>Martins</last></author>
      <pages>370&#8211;376</pages>
      <abstract>In neural machine translation, words are sometimes dropped from the source or generated repeatedly in the translation. We explore novel strategies to address the coverage problem that change only the attention transformation. Our approach allocates fertilities to source words, used to bound the attention each word can receive. We experiment with various sparse and constrained attention transformations and propose a new one, constrained sparsemax, shown to be differentiable and sparse. Empirical evaluation is provided in three languages pairs.</abstract>
      <url hash="f5145462">P18-2059</url>
      <attachment type="note" hash="2896928d">P18-2059.Notes.pdf</attachment>
      <video href="https://vimeo.com/285803960" />
      <attachment type="presentation" hash="268cc0cb">P18-2059.Presentation.pdf</attachment>
      <doi>10.18653/v1/P18-2059</doi>
      <bibkey>malaviya-etal-2018-sparse</bibkey>
    </paper>
    <paper id="60">
      <title>Neural Hidden <fixed-case>M</fixed-case>arkov Model for Machine Translation</title>
      <author><first>Weiyue</first><last>Wang</last></author>
      <author><first>Derui</first><last>Zhu</last></author>
      <author><first>Tamer</first><last>Alkhouli</last></author>
      <author><first>Zixuan</first><last>Gan</last></author>
      <author><first>Hermann</first><last>Ney</last></author>
      <pages>377&#8211;382</pages>
      <abstract>Attention-based neural machine translation (NMT) models selectively focus on specific source positions to produce a translation, which brings significant improvements over pure encoder-decoder sequence-to-sequence models. This work investigates NMT while replacing the attention component. We study a neural hidden Markov model (HMM) consisting of neural network-based alignment and lexicon models, which are trained jointly using the forward-backward algorithm. We show that the attention component can be effectively replaced by the neural network alignment model and the neural HMM approach is able to provide comparable performance with the state-of-the-art attention-based models on the WMT 2017 German&#8596;English and Chinese&#8594;English translation tasks.</abstract>
      <url hash="b850755f">P18-2060</url>
      <video href="https://vimeo.com/285803972" />
      <attachment type="presentation" hash="12408b7e">P18-2060.Presentation.pdf</attachment>
      <doi>10.18653/v1/P18-2060</doi>
      <bibkey>wang-etal-2018-neural-hidden</bibkey>
    </paper>
    <paper id="62">
      <title>Orthographic Features for Bilingual Lexicon Induction</title>
      <author><first>Parker</first><last>Riley</last></author>
      <author><first>Daniel</first><last>Gildea</last></author>
      <pages>390&#8211;394</pages>
      <abstract>Recent embedding-based methods in bilingual lexicon induction show good results, but do not take advantage of orthographic features, such as edit distance, which can be helpful for pairs of related languages. This work extends embedding-based methods to incorporate these features, resulting in significant accuracy gains for related languages.</abstract>
      <url hash="87654925">P18-2062</url>
      <video href="https://vimeo.com/285804010" />
      <attachment type="presentation" hash="25b66482">P18-2062.Presentation.pdf</attachment>
      <doi>10.18653/v1/P18-2062</doi>
      <bibkey>riley-gildea-2018-orthographic</bibkey>
    </paper>
    <paper id="63">
      <title>Neural Cross-Lingual Coreference Resolution And Its Application To Entity Linking</title>
      <author><first>Gourab</first><last>Kundu</last></author>
      <author><first>Avi</first><last>Sil</last></author>
      <author><first>Radu</first><last>Florian</last></author>
      <author><first>Wael</first><last>Hamza</last></author>
      <pages>395&#8211;400</pages>
      <abstract>We propose an entity-centric neural crosslingual coreference model that builds on multi-lingual embeddings and language independent features. We perform both intrinsic and extrinsic evaluations of our model. In the intrinsic evaluation, we show that our model, when trained on English and tested on Chinese and Spanish, achieves competitive results to the models trained directly on Chinese and Spanish respectively. In the extrinsic evaluation, we show that our English model helps achieve superior entity linking accuracy on Chinese and Spanish test sets than the top 2015 TAC system without using any annotated data from Chinese or Spanish.</abstract>
      <url hash="2a012f3d">P18-2063</url>
      <video href="https://vimeo.com/285804022" />
      <doi>10.18653/v1/P18-2063</doi>
      <bibkey>kundu-etal-2018-neural</bibkey>
    </paper>
    <paper id="65">
      <title>Neural Open Information Extraction</title>
      <author><first>Lei</first><last>Cui</last></author>
      <author><first>Furu</first><last>Wei</last></author>
      <author><first>Ming</first><last>Zhou</last></author>
      <pages>407&#8211;413</pages>
      <abstract>Conventional Open Information Extraction (Open IE) systems are usually built on hand-crafted patterns from other NLP tools such as syntactic parsing, yet they face problems of error propagation. In this paper, we propose a neural Open IE approach with an encoder-decoder framework. Distinct from existing methods, the neural Open IE approach learns highly confident arguments and relation tuples bootstrapped from a state-of-the-art Open IE system. An empirical study on a large benchmark dataset shows that the neural Open IE system significantly outperforms several baselines, while maintaining comparable computational efficiency.</abstract>
      <url hash="9b5d8082">P18-2065</url>
      <video href="https://vimeo.com/285804053" />
      <doi>10.18653/v1/P18-2065</doi>
      <bibkey>cui-etal-2018-neural</bibkey>
    </paper>
    <paper id="66">
      <title>Document Embedding Enhanced Event Detection with Hierarchical and Supervised Attention</title>
      <author><first>Yue</first><last>Zhao</last></author>
      <author><first>Xiaolong</first><last>Jin</last></author>
      <author><first>Yuanzhuo</first><last>Wang</last></author>
      <author><first>Xueqi</first><last>Cheng</last></author>
      <pages>414&#8211;419</pages>
      <abstract>Document-level information is very important for event detection even at sentence level. In this paper, we propose a novel Document Embedding Enhanced Bi-RNN model, called DEEB-RNN, to detect events in sentences. This model first learns event detection oriented embeddings of documents through a hierarchical and supervised attention based RNN, which pays word-level attention to event triggers and sentence-level attention to those sentences containing events. It then uses the learned document embedding to enhance another bidirectional RNN model to identify event triggers and their types in sentences. Through experiments on the ACE-2005 dataset, we demonstrate the effectiveness and merits of the proposed DEEB-RNN model via comparison with state-of-the-art methods.</abstract>
      <url hash="ea9082d4">P18-2066</url>
      <video href="https://vimeo.com/285804071" />
      <attachment type="presentation" hash="a3568399">P18-2066.Presentation.pdf</attachment>
      <doi>10.18653/v1/P18-2066</doi>
      <bibkey>zhao-etal-2018-document</bibkey>
    </paper>
    <paper id="68">
      <title>Improving Slot Filling in Spoken Language Understanding with Joint Pointer and Attention</title>
      <author><first>Lin</first><last>Zhao</last></author>
      <author><first>Zhe</first><last>Feng</last></author>
      <pages>426&#8211;431</pages>
      <abstract>We present a generative neural network model for slot filling based on a sequence-to-sequence (Seq2Seq) model together with a pointer network, in the situation where only sentence-level slot annotations are available in the spoken dialogue data. This model predicts slot values by jointly learning to copy a word which may be out-of-vocabulary (OOV) from an input utterance through a pointer network, or generate a word within the vocabulary through an attentional Seq2Seq model. Experimental results show the effectiveness of our slot filling model, especially at addressing the OOV problem. Additionally, we integrate the proposed model into a spoken language understanding system and achieve the state-of-the-art performance on the benchmark data.</abstract>
      <url hash="bc18a364">P18-2068</url>
      <video href="https://vimeo.com/285804096" />
      <doi>10.18653/v1/P18-2068</doi>
      <bibkey>zhao-feng-2018-improving</bibkey>
    </paper>
    <paper id="70">
      <title>Modeling discourse cohesion for discourse parsing via memory network</title>
      <author><first>Yanyan</first><last>Jia</last></author>
      <author><first>Yuan</first><last>Ye</last></author>
      <author><first>Yansong</first><last>Feng</last></author>
      <author><first>Yuxuan</first><last>Lai</last></author>
      <author><first>Rui</first><last>Yan</last></author>
      <author><first>Dongyan</first><last>Zhao</last></author>
      <pages>438&#8211;443</pages>
      <abstract>Identifying long-span dependencies between discourse units is crucial to improve discourse parsing performance. Most existing approaches design sophisticated features or exploit various off-the-shelf tools, but achieve little success. In this paper, we propose a new transition-based discourse parser that makes use of memory networks to take discourse cohesion into account. The automatically captured discourse cohesion benefits discourse parsing, especially for long span scenarios. Experiments on the RST discourse treebank show that our method outperforms traditional featured based methods, and the memory based discourse cohesion can improve the overall parsing performance significantly.</abstract>
      <url hash="fbae99e7">P18-2070</url>
      <video href="https://vimeo.com/285804139" />
      <attachment type="presentation" hash="286ad6c4">P18-2070.Presentation.pdf</attachment>
      <doi>10.18653/v1/P18-2070</doi>
      <bibkey>jia-etal-2018-modeling</bibkey>
    </paper>
    <paper id="71">
      <title><fixed-case>S</fixed-case>ci<fixed-case>DTB</fixed-case>: Discourse Dependency <fixed-case>T</fixed-case>ree<fixed-case>B</fixed-case>ank for Scientific Abstracts</title>
      <author><first>An</first><last>Yang</last></author>
      <author><first>Sujian</first><last>Li</last></author>
      <pages>444&#8211;449</pages>
      <abstract>Annotation corpus for discourse relations benefits NLP tasks such as machine translation and question answering. In this paper, we present SciDTB, a domain-specific discourse treebank annotated on scientific articles. Different from widely-used RST-DT and PDTB, SciDTB uses dependency trees to represent discourse structure, which is flexible and simplified to some extent but do not sacrifice structural integrity. We discuss the labeling framework, annotation workflow and some statistics about SciDTB. Furthermore, our treebank is made as a benchmark for evaluating discourse dependency parsers, on which we provide several baselines as fundamental work.</abstract>
      <url hash="c27798c9">P18-2071</url>
      <video href="https://vimeo.com/285804155" />
      <attachment type="presentation" hash="c8b5e1e4">P18-2071.Presentation.pdf</attachment>
      <doi>10.18653/v1/P18-2071</doi>
      <bibkey>yang-li-2018-scidtb</bibkey>
      <pwccode url="https://github.com/PKU-TANGENT/SciDTB" additional="false">PKU-TANGENT/SciDTB</pwccode>
    </paper>
    <paper id="75">
      <title>Policy Gradient as a Proxy for Dynamic Oracles in Constituency Parsing</title>
      <author><first>Daniel</first><last>Fried</last></author>
      <author><first>Dan</first><last>Klein</last></author>
      <pages>469&#8211;476</pages>
      <abstract>Dynamic oracles provide strong supervision for training constituency parsers with exploration, but must be custom defined for a given parser&#8217;s transition system. We explore using a policy gradient method as a parser-agnostic alternative. In addition to directly optimizing for a tree-level metric such as F1, policy gradient has the potential to reduce exposure bias by allowing exploration during training; moreover, it does not require a dynamic oracle for supervision. On four constituency parsers in three languages, the method substantially outperforms static oracle likelihood training in almost all settings. For parsers where a dynamic oracle is available (including a novel oracle which we define for the transition system of Dyer et al., 2016), policy gradient typically recaptures a substantial fraction of the performance gain afforded by the dynamic oracle.</abstract>
      <url hash="276b507e">P18-2075</url>
      <attachment type="note" hash="3f9c4b00">P18-2075.Notes.pdf</attachment>
      <video href="https://vimeo.com/285804205" />
      <attachment type="presentation" hash="fb304379">P18-2075.Presentation.pdf</attachment>
      <doi>10.18653/v1/P18-2075</doi>
      <bibkey>fried-klein-2018-policy</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/penn-treebank">Penn Treebank</pwcdataset>
    </paper>
    <paper id="76">
      <title>Linear-time Constituency Parsing with <fixed-case>RNN</fixed-case>s and Dynamic Programming</title>
      <author><first>Juneki</first><last>Hong</last></author>
      <author><first>Liang</first><last>Huang</last></author>
      <pages>477&#8211;483</pages>
      <abstract>Recently, span-based constituency parsing has achieved competitive accuracies with extremely simple models by using bidirectional RNNs to model &#8220;spans&#8221;. However, the minimal span parser of Stern et al. (2017a) which holds the current state of the art accuracy is a chart parser running in cubic time, <tex-math>O(n^3)</tex-math>, which is too slow for longer sentences and for applications beyond sentence boundaries such as end-to-end discourse parsing and joint sentence boundary detection and parsing. We propose a linear-time constituency parser with RNNs and dynamic programming using graph-structured stack and beam search, which runs in time <tex-math>O(n b^2)</tex-math> where <tex-math>b</tex-math> is the beam size. We further speed this up to <tex-math>O(n b log b)</tex-math> by integrating cube pruning. Compared with chart parsing baselines, this linear-time parser is substantially faster for long sentences on the Penn Treebank and orders of magnitude faster for discourse parsing, and achieves the highest F1 accuracy on the Penn Treebank among single model end-to-end systems.</abstract>
      <url hash="2c6f10ce">P18-2076</url>
      <video href="https://vimeo.com/285804218" />
      <attachment type="presentation" hash="b6eb9f20">P18-2076.Presentation.pdf</attachment>
      <doi>10.18653/v1/P18-2076</doi>
      <bibkey>hong-huang-2018-linear</bibkey>
    </paper>
    <paper id="77">
      <title>Simpler but More Accurate Semantic Dependency Parsing</title>
      <author><first>Timothy</first><last>Dozat</last></author>
      <author><first>Christopher D.</first><last>Manning</last></author>
      <pages>484&#8211;490</pages>
      <abstract>While syntactic dependency annotations concentrate on the surface or functional structure of a sentence, semantic dependency annotations aim to capture between-word relationships that are more closely related to the meaning of a sentence, using graph-structured representations. We extend the LSTM-based syntactic parser of Dozat and Manning (2017) to train on and generate these graph structures. The resulting system on its own achieves state-of-the-art performance, beating the previous, substantially more complex state-of-the-art system by 0.6% labeled F1. Adding linguistically richer input representations pushes the margin even higher, allowing us to beat it by 1.9% labeled F1.</abstract>
      <url hash="45c6e9dd">P18-2077</url>
      <video href="https://vimeo.com/285804230" />
      <doi>10.18653/v1/P18-2077</doi>
      <bibkey>dozat-manning-2018-simpler</bibkey>
      <pwccode url="" additional="true" />
      <pwcdataset url="https://paperswithcode.com/dataset/universal-dependencies">Universal Dependencies</pwcdataset>
    </paper>
    <paper id="79">
      <title>Automatic Academic Paper Rating Based on Modularized Hierarchical Convolutional Neural Network</title>
      <author><first>Pengcheng</first><last>Yang</last></author>
      <author><first>Xu</first><last>Sun</last></author>
      <author><first>Wei</first><last>Li</last></author>
      <author><first>Shuming</first><last>Ma</last></author>
      <pages>496&#8211;502</pages>
      <abstract>As more and more academic papers are being submitted to conferences and journals, evaluating all these papers by professionals is time-consuming and can cause inequality due to the personal factors of the reviewers. In this paper, in order to assist professionals in evaluating academic papers, we propose a novel task: automatic academic paper rating (AAPR), which automatically determine whether to accept academic papers. We build a new dataset for this task and propose a novel modularized hierarchical convolutional neural network to achieve automatic academic paper rating. Evaluation results show that the proposed model outperforms the baselines by a large margin. The dataset and code are available at <url>https://github.com/lancopku/AAPR</url>
      </abstract>
      <url hash="fb54e8bb">P18-2079</url>
      <attachment type="poster" hash="9a441e77">P18-2079.Poster.pdf</attachment>
      <doi>10.18653/v1/P18-2079</doi>
      <bibkey>yang-etal-2018-automatic</bibkey>
      <pwccode url="https://github.com/lancopku/AAPR" additional="false">lancopku/AAPR</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/arxiv-academic-paper-dataset">Arxiv Academic Paper Dataset</pwcdataset>
    </paper>
    <paper id="80">
      <title>Automated essay scoring with string kernels and word embeddings</title>
      <author><first>M&#259;d&#259;lina</first><last>Cozma</last></author>
      <author><first>Andrei</first><last>Butnaru</last></author>
      <author><first>Radu Tudor</first><last>Ionescu</last></author>
      <pages>503&#8211;509</pages>
      <abstract>In this work, we present an approach based on combining string kernels and word embeddings for automatic essay scoring. String kernels capture the similarity among strings based on counting common character n-grams, which are a low-level yet powerful type of feature, demonstrating state-of-the-art results in various text classification tasks such as Arabic dialect identification or native language identification. To our best knowledge, we are the first to apply string kernels to automatically score essays. We are also the first to combine them with a high-level semantic feature representation, namely the bag-of-super-word-embeddings. We report the best performance on the Automated Student Assessment Prize data set, in both in-domain and cross-domain settings, surpassing recent state-of-the-art deep learning approaches.</abstract>
      <url hash="7f48bd06">P18-2080</url>
      <attachment type="poster" hash="ff656b27">P18-2080.Poster.pdf</attachment>
      <doi>10.18653/v1/P18-2080</doi>
      <bibkey>cozma-etal-2018-automated</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/asap">ASAP</pwcdataset>
    </paper>
    <paper id="81">
      <title>Party Matters: Enhancing Legislative Embeddings with Author Attributes for Vote Prediction</title>
      <author><first>Anastassia</first><last>Kornilova</last></author>
      <author><first>Daniel</first><last>Argyle</last></author>
      <author><first>Vladimir</first><last>Eidelman</last></author>
      <pages>510&#8211;515</pages>
      <abstract>Predicting how Congressional legislators will vote is important for understanding their past and future behavior. However, previous work on roll-call prediction has been limited to single session settings, thus not allowing for generalization across sessions. In this paper, we show that text alone is insufficient for modeling voting outcomes in new contexts, as session changes lead to changes in the underlying data generation process. We propose a novel neural method for encoding documents alongside additional metadata, achieving an average of a 4% boost in accuracy over the previous state-of-the-art.</abstract>
      <url hash="a4969c90">P18-2081</url>
      <doi>10.18653/v1/P18-2081</doi>
      <bibkey>kornilova-etal-2018-party</bibkey>
    </paper>
    <paper id="82">
      <title>Dynamic and Static Topic Model for Analyzing Time-Series Document Collections</title>
      <author><first>Rem</first><last>Hida</last></author>
      <author><first>Naoya</first><last>Takeishi</last></author>
      <author><first>Takehisa</first><last>Yairi</last></author>
      <author><first>Koichi</first><last>Hori</last></author>
      <pages>516&#8211;520</pages>
      <abstract>For extracting meaningful topics from texts, their structures should be considered properly. In this paper, we aim to analyze structured time-series documents such as a collection of news articles and a series of scientific papers, wherein topics evolve along time depending on multiple topics in the past and are also related to each other at each time. To this end, we propose a dynamic and static topic model, which simultaneously considers the dynamic structures of the temporal topic evolution and the static structures of the topic hierarchy at each time. We show the results of experiments on collections of scientific papers, in which the proposed method outperformed conventional models. Moreover, we show an example of extracted topic structures, which we found helpful for analyzing research activities.</abstract>
      <url hash="ef0434a1">P18-2082</url>
      <attachment type="note" hash="c187f1c1">P18-2082.Notes.pdf</attachment>
      <doi>10.18653/v1/P18-2082</doi>
      <bibkey>hida-etal-2018-dynamic</bibkey>
    </paper>
    <paper id="83">
      <title><fixed-case>P</fixed-case>hrase<fixed-case>CTM</fixed-case>: Correlated Topic Modeling on Phrases within <fixed-case>M</fixed-case>arkov Random Fields</title>
      <author><first>Weijing</first><last>Huang</last></author>
      <pages>521&#8211;526</pages>
      <abstract>Recent emerged phrase-level topic models are able to provide topics of phrases, which are easy to read for humans. But these models are lack of the ability to capture the correlation structure among the discovered numerous topics. We propose a novel topic model PhraseCTM and a two-stage method to find out the correlated topics at phrase level. In the first stage, we train PhraseCTM, which models the generation of words and phrases simultaneously by linking the phrases and component words within Markov Random Fields when they are semantically coherent. In the second stage, we generate the correlation of topics from PhraseCTM. We evaluate our method by a quantitative experiment and a human study, showing the correlated topic modeling on phrases is a good and practical way to interpret the underlying themes of a corpus.</abstract>
      <url hash="03c382e5">P18-2083</url>
      <attachment type="poster" hash="813b2aea">P18-2083.Poster.pdf</attachment>
      <doi>10.18653/v1/P18-2083</doi>
      <bibkey>huang-2018-phrasectm</bibkey>
    </paper>
    <paper id="85">
      <title>Learning with Structured Representations for Negation Scope Extraction</title>
      <author><first>Hao</first><last>Li</last></author>
      <author><first>Wei</first><last>Lu</last></author>
      <pages>533&#8211;539</pages>
      <abstract>We report an empirical study on the task of negation scope extraction given the negation cue. Our key observation is that certain useful information such as features related to negation cue, long-distance dependencies as well as some latent structural information can be exploited for such a task. We design approaches based on conditional random fields (CRF), semi-Markov CRF, as well as latent-variable CRF models to capture such information. Extensive experiments on several standard datasets demonstrate that our approaches are able to achieve better results than existing approaches reported in the literature.</abstract>
      <url hash="f5cc64d9">P18-2085</url>
      <attachment type="note" hash="99660e41">P18-2085.Notes.pdf</attachment>
      <attachment type="poster" hash="9afa0b3b">P18-2085.Poster.pdf</attachment>
      <doi>10.18653/v1/P18-2085</doi>
      <bibkey>li-lu-2018-learning</bibkey>
    </paper>
    <paper id="86">
      <title>End-Task Oriented Textual Entailment via Deep Explorations of Inter-Sentence Interactions</title>
      <author><first>Wenpeng</first><last>Yin</last></author>
      <author><first>Dan</first><last>Roth</last></author>
      <author><first>Hinrich</first><last>Sch&#252;tze</last></author>
      <pages>540&#8211;545</pages>
      <abstract>This work deals with SciTail, a natural entailment challenge derived from a multi-choice question answering problem. The premises and hypotheses in SciTail were generated with no awareness of each other, and did not specifically aim at the entailment task. This makes it more challenging than other entailment data sets and more directly useful to the end-task &#8211; question answering. We propose DEISTE (deep explorations of inter-sentence interactions for textual entailment) for this entailment task. Given word-to-word interactions between the premise-hypothesis pair (P, H), DEISTE consists of: (i) a parameter-dynamic convolution to make important words in P and H play a dominant role in learnt representations; and (ii) a position-aware attentive convolution to encode the representation and position information of the aligned word pairs. Experiments show that DEISTE gets &#8776;5% improvement over prior state of the art and that the pretrained DEISTE on SciTail generalizes well on RTE-5.</abstract>
      <url hash="8c822319">P18-2086</url>
      <attachment type="poster" hash="137a3fd2">P18-2086.Poster.pdf</attachment>
      <doi>10.18653/v1/P18-2086</doi>
      <bibkey>yin-etal-2018-end</bibkey>
      <pwccode url="https://github.com/yinwenpeng/SciTail" additional="false">yinwenpeng/SciTail</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
    </paper>
    <paper id="88">
      <title>A Rank-Based Similarity Metric for Word Embeddings</title>
      <author><first>Enrico</first><last>Santus</last></author>
      <author><first>Hongmin</first><last>Wang</last></author>
      <author><first>Emmanuele</first><last>Chersoni</last></author>
      <author><first>Yue</first><last>Zhang</last></author>
      <pages>552&#8211;557</pages>
      <abstract>Word Embeddings have recently imposed themselves as a standard for representing word meaning in NLP. Semantic similarity between word pairs has become the most common evaluation benchmark for these representations, with vector cosine being typically used as the only similarity metric. In this paper, we report experiments with a rank-based metric for WE, which performs comparably to vector cosine in similarity estimation and outperforms it in the recently-introduced and challenging task of outlier detection, thus suggesting that rank-based measures can improve clustering quality.</abstract>
      <url hash="d5f8631f">P18-2088</url>
      <attachment type="software" hash="3a761742">P18-2088.Software.zip</attachment>
      <attachment type="poster" hash="bea8cc93">P18-2088.Poster.pdf</attachment>
      <doi>10.18653/v1/P18-2088</doi>
      <bibkey>santus-etal-2018-rank</bibkey>
    </paper>
    <paper id="89">
      <title>Addressing Noise in Multidialectal Word Embeddings</title>
      <author><first>Alexander</first><last>Erdmann</last></author>
      <author><first>Nasser</first><last>Zalmout</last></author>
      <author><first>Nizar</first><last>Habash</last></author>
      <pages>558&#8211;565</pages>
      <abstract>Word embeddings are crucial to many natural language processing tasks. The quality of embeddings relies on large non-noisy corpora. Arabic dialects lack large corpora and are noisy, being linguistically disparate with no standardized spelling. We make three contributions to address this noise. First, we describe simple but effective adaptations to word embedding tools to maximize the informative content leveraged in each training sentence. Second, we analyze methods for representing disparate dialects in one embedding space, either by mapping individual dialects into a shared space or learning a joint model of all dialects. Finally, we evaluate via dictionary induction, showing that two metrics not typically reported in the task enable us to analyze our contributions&#8217; effects on low and high frequency words. In addition to boosting performance between 2-53%, we specifically improve on noisy, low frequency forms without compromising accuracy on high frequency forms.</abstract>
      <url hash="83043a7d">P18-2089</url>
      <attachment type="poster" hash="3a5d760c">P18-2089.Poster.pdf</attachment>
      <doi>10.18653/v1/P18-2089</doi>
      <bibkey>erdmann-etal-2018-addressing</bibkey>
    </paper>
    <paper id="90">
      <title><fixed-case>GNEG</fixed-case>: Graph-Based Negative Sampling for word2vec</title>
      <author><first>Zheng</first><last>Zhang</last></author>
      <author><first>Pierre</first><last>Zweigenbaum</last></author>
      <pages>566&#8211;571</pages>
      <abstract>Negative sampling is an important component in word2vec for distributed word representation learning. We hypothesize that taking into account global, corpus-level information and generating a different noise distribution for each target word better satisfies the requirements of negative examples for each training word than the original frequency-based distribution. In this purpose we pre-compute word co-occurrence statistics from the corpus and apply to it network algorithms such as random walk. We test this hypothesis through a set of experiments whose results show that our approach boosts the word analogy task by about 5% and improves the performance on word similarity tasks by about 1% compared to the skip-gram negative sampling baseline.</abstract>
      <url hash="58fa9761">P18-2090</url>
      <attachment type="poster" hash="e2e9a2ea">P18-2090.Poster.pdf</attachment>
      <doi>10.18653/v1/P18-2090</doi>
      <bibkey>zhang-zweigenbaum-2018-gneg</bibkey>
    </paper>
    <paper id="91">
      <title>Unsupervised Learning of Style-sensitive Word Vectors</title>
      <author><first>Reina</first><last>Akama</last></author>
      <author><first>Kento</first><last>Watanabe</last></author>
      <author><first>Sho</first><last>Yokoi</last></author>
      <author><first>Sosuke</first><last>Kobayashi</last></author>
      <author><first>Kentaro</first><last>Inui</last></author>
      <pages>572&#8211;578</pages>
      <abstract>This paper presents the first study aimed at capturing stylistic similarity between words in an unsupervised manner. We propose extending the continuous bag of words (CBOW) embedding model (Mikolov et al., 2013b) to learn style-sensitive word vectors using a wider context window under the assumption that the style of all the words in an utterance is consistent. In addition, we introduce a novel task to predict lexical stylistic similarity and to create a benchmark dataset for this task. Our experiment with this dataset supports our assumption and demonstrates that the proposed extensions contribute to the acquisition of style-sensitive word embeddings.</abstract>
      <url hash="5a611550">P18-2091</url>
      <attachment type="poster" hash="00212502">P18-2091.Poster.pdf</attachment>
      <doi>10.18653/v1/P18-2091</doi>
      <bibkey>akama-etal-2018-unsupervised</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/japanese-word-similarity">Japanese Word Similarity</pwcdataset>
    </paper>
    <paper id="94">
      <title>Double Embeddings and <fixed-case>CNN</fixed-case>-based Sequence Labeling for Aspect Extraction</title>
      <author><first>Hu</first><last>Xu</last></author>
      <author><first>Bing</first><last>Liu</last></author>
      <author><first>Lei</first><last>Shu</last></author>
      <author><first>Philip S.</first><last>Yu</last></author>
      <pages>592&#8211;598</pages>
      <abstract>One key task of fine-grained sentiment analysis of product reviews is to extract product aspects or features that users have expressed opinions on. This paper focuses on supervised aspect extraction using deep learning. Unlike other highly sophisticated supervised deep learning models, this paper proposes a novel and yet simple CNN model employing two types of pre-trained embeddings for aspect extraction: general-purpose embeddings and domain-specific embeddings. Without using any additional supervision, this model achieves surprisingly good results, outperforming state-of-the-art sophisticated existing methods. To our knowledge, this paper is the first to report such double embeddings based CNN model for aspect extraction and achieve very good results.</abstract>
      <url hash="10fa390c">P18-2094</url>
      <attachment type="poster" hash="de9486c0">P18-2094.Poster.pdf</attachment>
      <doi>10.18653/v1/P18-2094</doi>
      <bibkey>xu-etal-2018-double</bibkey>
      <pwccode url="" additional="true" />
      <pwcdataset url="https://paperswithcode.com/dataset/semeval-2014-task-4-sub-task-2">SemEval 2014 Task 4 Sub Task 2</pwcdataset>
    </paper>
    <paper id="95">
      <title>Will it Blend? Blending Weak and Strong Labeled Data in a Neural Network for Argumentation Mining</title>
      <author><first>Eyal</first><last>Shnarch</last></author>
      <author><first>Carlos</first><last>Alzate</last></author>
      <author><first>Lena</first><last>Dankin</last></author>
      <author><first>Martin</first><last>Gleize</last></author>
      <author><first>Yufang</first><last>Hou</last></author>
      <author><first>Leshem</first><last>Choshen</last></author>
      <author><first>Ranit</first><last>Aharonov</last></author>
      <author><first>Noam</first><last>Slonim</last></author>
      <pages>599&#8211;605</pages>
      <abstract>The process of obtaining high quality labeled data for natural language understanding tasks is often slow, error-prone, complicated and expensive. With the vast usage of neural networks, this issue becomes more notorious since these networks require a large amount of labeled data to produce satisfactory results. We propose a methodology to blend high quality but scarce strong labeled data with noisy but abundant weak labeled data during the training of neural networks. Experiments in the context of topic-dependent evidence detection with two forms of weak labeled data show the advantages of the blending scheme. In addition, we provide a manually annotated data set for the task of topic-dependent evidence detection. We believe that blending weak and strong labeled data is a general notion that may be applicable to many language understanding tasks, and can especially assist researchers who wish to train a network but have a small amount of high quality labeled data for their task of interest.</abstract>
      <url hash="71493971">P18-2095</url>
      <attachment type="poster" hash="e0a93a64">P18-2095.Poster.pdf</attachment>
      <doi>10.18653/v1/P18-2095</doi>
      <bibkey>shnarch-etal-2018-will</bibkey>
    </paper>
    <paper id="96">
      <title>Investigating Audio, Video, and Text Fusion Methods for End-to-End Automatic Personality Prediction</title>
      <author><first>Onno</first><last>Kampman</last></author>
      <author><first>Elham</first><last>J. Barezi</last></author>
      <author><first>Dario</first><last>Bertero</last></author>
      <author><first>Pascale</first><last>Fung</last></author>
      <pages>606&#8211;611</pages>
      <abstract>We propose a tri-modal architecture to predict Big Five personality trait scores from video clips with different channels for audio, text, and video data. For each channel, stacked Convolutional Neural Networks are employed. The channels are fused both on decision-level and by concatenating their respective fully connected layers. It is shown that a multimodal fusion approach outperforms each single modality channel, with an improvement of 9.4% over the best individual modality (video). Full backpropagation is also shown to be better than a linear combination of modalities, meaning complex interactions between modalities can be leveraged to build better models. Furthermore, we can see the prediction relevance of each modality for each trait. The described model can be used to increase the emotional intelligence of virtual agents.</abstract>
      <url hash="3a7d7e36">P18-2096</url>
      <attachment type="poster" hash="6d98f7bb">P18-2096.Poster.pdf</attachment>
      <doi>10.18653/v1/P18-2096</doi>
      <bibkey>kampman-etal-2018-investigating</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/2d-3d-match-dataset">2D-3D Match Dataset</pwcdataset>
    </paper>
    <paper id="98">
      <title>Parser Training with Heterogeneous Treebanks</title>
      <author><first>Sara</first><last>Stymne</last></author>
      <author><first>Miryam</first><last>de Lhoneux</last></author>
      <author><first>Aaron</first><last>Smith</last></author>
      <author><first>Joakim</first><last>Nivre</last></author>
      <pages>619&#8211;625</pages>
      <abstract>How to make the most of multiple heterogeneous treebanks when training a monolingual dependency parser is an open question. We start by investigating previously suggested, but little evaluated, strategies for exploiting multiple treebanks based on concatenating training sets, with or without fine-tuning. We go on to propose a new method based on treebank embeddings. We perform experiments for several languages and show that in many cases fine-tuning and treebank embeddings lead to substantial improvements over single treebanks or concatenation, with average gains of 2.0&#8211;3.5 LAS points. We argue that treebank embeddings should be preferred due to their conceptual simplicity, flexibility and extensibility.</abstract>
      <url hash="ea4a5ed7">P18-2098</url>
      <attachment type="poster" hash="0aba7069">P18-2098.Poster.pdf</attachment>
      <doi>10.18653/v1/P18-2098</doi>
      <bibkey>stymne-etal-2018-parser</bibkey>
      <pwccode url="https://github.com/UppsalaNLP/uuparser" additional="false">UppsalaNLP/uuparser</pwccode>
    </paper>
    <paper id="99">
      <title>Generalized chart constraints for efficient <fixed-case>PCFG</fixed-case> and <fixed-case>TAG</fixed-case> parsing</title>
      <author><first>Stefan</first><last>Gr&#252;newald</last></author>
      <author><first>Sophie</first><last>Henning</last></author>
      <author><first>Alexander</first><last>Koller</last></author>
      <pages>626&#8211;631</pages>
      <abstract>Chart constraints, which specify at which string positions a constituent may begin or end, have been shown to speed up chart parsers for PCFGs. We generalize chart constraints to more expressive grammar formalisms and describe a neural tagger which predicts chart constraints at very high precision. Our constraints accelerate both PCFG and TAG parsing, and combine effectively with other pruning techniques (coarse-to-fine and supertagging) for an overall speedup of two orders of magnitude, while improving accuracy.</abstract>
      <url hash="c608ca06">P18-2099</url>
      <attachment type="note" hash="58f7b6fe">P18-2099.Notes.pdf</attachment>
      <doi>10.18653/v1/P18-2099</doi>
      <bibkey>grunewald-etal-2018-generalized</bibkey>
    </paper>
    <paper id="100">
      <title>Exploring Semantic Properties of Sentence Embeddings</title>
      <author><first>Xunjie</first><last>Zhu</last></author>
      <author><first>Tingfeng</first><last>Li</last></author>
      <author><first>Gerard</first><last>de Melo</last></author>
      <pages>632&#8211;637</pages>
      <abstract>Neural vector representations are ubiquitous throughout all subfields of NLP. While word vectors have been studied in much detail, thus far only little light has been shed on the properties of sentence embeddings. In this paper, we assess to what extent prominent sentence embedding methods exhibit select semantic properties. We propose a framework that generate triplets of sentences to explore how changes in the syntactic structure or semantics of a given sentence affect the similarities obtained between their sentence embeddings.</abstract>
      <url hash="00c485fc">P18-2100</url>
      <video href="https://vimeo.com/285805830" />
      <attachment type="presentation" hash="4b73b0b7">P18-2100.Presentation.pdf</attachment>
      <doi>10.18653/v1/P18-2100</doi>
      <bibkey>zhu-etal-2018-exploring</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/sick">SICK</pwcdataset>
    </paper>
    <paper id="103">
      <title>Breaking <fixed-case>NLI</fixed-case> Systems with Sentences that Require Simple Lexical Inferences</title>
      <author><first>Max</first><last>Glockner</last></author>
      <author><first>Vered</first><last>Shwartz</last></author>
      <author><first>Yoav</first><last>Goldberg</last></author>
      <pages>650&#8211;655</pages>
      <abstract>We create a new NLI test set that shows the deficiency of state-of-the-art models in inferences that require lexical and world knowledge. The new examples are simpler than the SNLI test set, containing sentences that differ by at most one word from sentences in the training set. Yet, the performance on the new test set is substantially worse across systems trained on SNLI, demonstrating that these systems are limited in their generalization ability, failing to capture many simple inferences.</abstract>
      <url hash="bc639d88">P18-2103</url>
      <video href="https://vimeo.com/285805874" />
      <attachment type="presentation" hash="b8e85827">P18-2103.Presentation.pdf</attachment>
      <doi>10.18653/v1/P18-2103</doi>
      <bibkey>glockner-etal-2018-breaking</bibkey>
      <pwccode url="https://github.com/BIU-NLP/Breaking_NLI" additional="true">BIU-NLP/Breaking_NLI</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
    </paper>
    <paper id="106">
      <title>Polyglot Semantic Role Labeling</title>
      <author><first>Phoebe</first><last>Mulcaire</last></author>
      <author><first>Swabha</first><last>Swayamdipta</last></author>
      <author><first>Noah A.</first><last>Smith</last></author>
      <pages>667&#8211;672</pages>
      <abstract>Previous approaches to multilingual semantic dependency parsing treat languages independently, without exploiting the similarities between semantic structures across languages. We experiment with a new approach where we combine resources from different languages in the CoNLL 2009 shared task to build a single polyglot semantic dependency parser. Notwithstanding the absence of parallel data, and the dissimilarity in annotations between languages, our approach results in improvement in parsing performance on several languages over a monolingual baseline. Analysis of the polyglot models&#8217; performance provides a new understanding of the similarities and differences between languages in the shared task.</abstract>
      <url hash="4478c6c1">P18-2106</url>
      <video href="https://vimeo.com/285805925" />
      <doi>10.18653/v1/P18-2106</doi>
      <bibkey>mulcaire-etal-2018-polyglot</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/universal-dependencies">Universal Dependencies</pwcdataset>
    </paper>
    <paper id="111">
      <title>Personalized Language Model for Query Auto-Completion</title>
      <author><first>Aaron</first><last>Jaech</last></author>
      <author><first>Mari</first><last>Ostendorf</last></author>
      <pages>700&#8211;705</pages>
      <abstract>Query auto-completion is a search engine feature whereby the system suggests completed queries as the user types. Recently, the use of a recurrent neural network language model was suggested as a method of generating query completions. We show how an adaptable language model can be used to generate personalized completions and how the model can use online updating to make predictions for users not seen during training. The personalized predictions are significantly better than a baseline that uses no user information.</abstract>
      <url hash="327a60d3">P18-2111</url>
      <video href="https://vimeo.com/285806005" />
      <attachment type="presentation" hash="01cab86b">P18-2111.Presentation.pdf</attachment>
      <doi>10.18653/v1/P18-2111</doi>
      <bibkey>jaech-ostendorf-2018-personalized</bibkey>
      <pwccode url="https://github.com/ajaech/query_completion" additional="true">ajaech/query_completion</pwccode>
    </paper>
    <paper id="112">
      <title>Personalized Review Generation By Expanding Phrases and Attending on Aspect-Aware Representations</title>
      <author><first>Jianmo</first><last>Ni</last></author>
      <author><first>Julian</first><last>McAuley</last></author>
      <pages>706&#8211;711</pages>
      <abstract>In this paper, we focus on the problem of building assistive systems that can help users to write reviews. We cast this problem using an encoder-decoder framework that generates personalized reviews by expanding short phrases (e.g. review summaries, product titles) provided as input to the system. We incorporate aspect-level information via an aspect encoder that learns aspect-aware user and item representations. An attention fusion layer is applied to control generation by attending on the outputs of multiple encoders. Experimental results show that our model successfully learns representations capable of generating coherent and diverse reviews. In addition, the learned aspect-aware representations discover those aspects that users are more inclined to discuss and bias the generated text toward their personalized aspect preferences.</abstract>
      <url hash="e77e3be8">P18-2112</url>
      <video href="https://vimeo.com/285806016" />
      <attachment type="presentation" hash="727aa1de">P18-2112.Presentation.pdf</attachment>
      <doi>10.18653/v1/P18-2112</doi>
      <bibkey>ni-mcauley-2018-personalized</bibkey>
      <pwccode url="https://github.com/nijianmo/textExpansion" additional="false">nijianmo/textExpansion</pwccode>
    </paper>
    <paper id="113">
      <title>Learning Simplifications for Specific Target Audiences</title>
      <author><first>Carolina</first><last>Scarton</last></author>
      <author><first>Lucia</first><last>Specia</last></author>
      <pages>712&#8211;718</pages>
      <abstract>Text simplification (TS) is a monolingual text-to-text transformation task where an original (complex) text is transformed into a target (simpler) text. Most recent work is based on sequence-to-sequence neural models similar to those used for machine translation (MT). Different from MT, TS data comprises more elaborate transformations, such as sentence splitting. It can also contain multiple simplifications of the same original text targeting different audiences, such as school grade levels. We explore these two features of TS to build models tailored for specific grade levels. Our approach uses a standard sequence-to-sequence architecture where the original sequence is annotated with information about the target audience and/or the (predicted) type of simplification operation. We show that it outperforms state-of-the-art TS approaches (up to 3 and 12 BLEU and SARI points, respectively), including when training data for the specific complex-simple combination of grade levels is not available, i.e. zero-shot learning.</abstract>
      <url hash="3b8ffe03">P18-2113</url>
      <attachment type="note" hash="9413a55f">P18-2113.Notes.pdf</attachment>
      <video href="https://vimeo.com/285806034" />
      <attachment type="presentation" hash="8a7c4d46">P18-2113.Presentation.pdf</attachment>
      <doi>10.18653/v1/P18-2113</doi>
      <bibkey>scarton-specia-2018-learning</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/newsela">Newsela</pwcdataset>
    </paper>
    <paper id="114">
      <title>Split and Rephrase: Better Evaluation and Stronger Baselines</title>
      <author><first>Roee</first><last>Aharoni</last></author>
      <author><first>Yoav</first><last>Goldberg</last></author>
      <pages>719&#8211;724</pages>
      <abstract>Splitting and rephrasing a complex sentence into several shorter sentences that convey the same meaning is a challenging problem in NLP. We show that while vanilla seq2seq models can reach high scores on the proposed benchmark (Narayan et al., 2017), they suffer from memorization of the training set which contains more than 89% of the unique simple sentences from the validation and test sets. To aid this, we present a new train-development-test data split and neural models augmented with a copy-mechanism, outperforming the best reported baseline by 8.68 BLEU and fostering further progress on the task.</abstract>
      <url hash="8cb9bc5d">P18-2114</url>
      <attachment type="note" hash="44942396">P18-2114.Notes.pdf</attachment>
      <video href="https://vimeo.com/285806055" />
      <attachment type="presentation" hash="2feab392">P18-2114.Presentation.pdf</attachment>
      <doi>10.18653/v1/P18-2114</doi>
      <bibkey>aharoni-goldberg-2018-split</bibkey>
      <pwccode url="https://github.com/biu-nlp/sprp-acl2018" additional="false">biu-nlp/sprp-acl2018</pwccode>
    </paper>
    <paper id="115">
      <title>Autoencoder as Assistant Supervisor: Improving Text Representation for <fixed-case>C</fixed-case>hinese Social Media Text Summarization</title>
      <author><first>Shuming</first><last>Ma</last></author>
      <author><first>Xu</first><last>Sun</last></author>
      <author><first>Junyang</first><last>Lin</last></author>
      <author><first>Houfeng</first><last>Wang</last></author>
      <pages>725&#8211;731</pages>
      <abstract>Most of the current abstractive text summarization models are based on the sequence-to-sequence model (Seq2Seq). The source content of social media is long and noisy, so it is difficult for Seq2Seq to learn an accurate semantic representation. Compared with the source content, the annotated summary is short and well written. Moreover, it shares the same meaning as the source content. In this work, we supervise the learning of the representation of the source content with that of the summary. In implementation, we regard a summary autoencoder as an assistant supervisor of Seq2Seq. Following previous work, we evaluate our model on a popular Chinese social media dataset. Experimental results show that our model achieves the state-of-the-art performances on the benchmark dataset.</abstract>
      <url hash="4ad04cad">P18-2115</url>
      <video href="https://vimeo.com/285806074" />
      <doi>10.18653/v1/P18-2115</doi>
      <bibkey>ma-etal-2018-autoencoder</bibkey>
      <pwccode url="https://github.com/lancopku/superAE" additional="false">lancopku/superAE</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/lcsts">LCSTS</pwcdataset>
    </paper>
    <paper id="117">
      <title>On the Practical Computational Power of Finite Precision <fixed-case>RNN</fixed-case>s for Language Recognition</title>
      <author><first>Gail</first><last>Weiss</last></author>
      <author><first>Yoav</first><last>Goldberg</last></author>
      <author><first>Eran</first><last>Yahav</last></author>
      <pages>740&#8211;745</pages>
      <abstract>While Recurrent Neural Networks (RNNs) are famously known to be Turing complete, this relies on infinite precision in the states and unbounded computation time. We consider the case of RNNs with finite precision whose computation time is linear in the input length. Under these limitations, we show that different RNN variants have different computational power. In particular, we show that the LSTM and the Elman-RNN with ReLU activation are strictly stronger than the RNN with a squashing activation and the GRU. This is achieved because LSTMs and ReLU-RNNs can easily implement counting behavior. We show empirically that the LSTM does indeed learn to effectively use the counting mechanism.</abstract>
      <url hash="72d15c8c">P18-2117</url>
      <attachment type="note" hash="cded70e3">P18-2117.Notes.pdf</attachment>
      <video href="https://vimeo.com/285806108" />
      <attachment type="presentation" hash="aff1a378">P18-2117.Presentation.pdf</attachment>
      <doi>10.18653/v1/P18-2117</doi>
      <bibkey>weiss-etal-2018-practical</bibkey>
    </paper>
    <paper id="121">
      <title>Pretraining Sentiment Classifiers with Unlabeled Dialog Data</title>
      <author><first>Toru</first><last>Shimizu</last></author>
      <author><first>Nobuyuki</first><last>Shimizu</last></author>
      <author><first>Hayato</first><last>Kobayashi</last></author>
      <pages>764&#8211;770</pages>
      <abstract>The huge cost of creating labeled training data is a common problem for supervised learning tasks such as sentiment classification. Recent studies showed that pretraining with unlabeled data via a language model can improve the performance of classification models. In this paper, we take the concept a step further by using a conditional language model, instead of a language model. Specifically, we address a sentiment classification task for a tweet analysis service as a case study and propose a pretraining strategy with unlabeled dialog data (tweet-reply pairs) via an encoder-decoder model. Experimental results show that our strategy can improve the performance of sentiment classifiers and outperform several state-of-the-art strategies including language model pretraining.</abstract>
      <url hash="21ab1cd9">P18-2121</url>
      <attachment type="note" hash="17c6b5cb">P18-2121.Notes.pdf</attachment>
      <video href="https://vimeo.com/285806152" />
      <attachment type="presentation" hash="75b31d7f">P18-2121.Presentation.pdf</attachment>
      <doi>10.18653/v1/P18-2121</doi>
      <bibkey>shimizu-etal-2018-pretraining</bibkey>
    </paper>
    <paper id="122">
      <title>Disambiguating False-Alarm Hashtag Usages in Tweets for Irony Detection</title>
      <author><first>Hen-Hsen</first><last>Huang</last></author>
      <author><first>Chiao-Chen</first><last>Chen</last></author>
      <author><first>Hsin-Hsi</first><last>Chen</last></author>
      <pages>771&#8211;777</pages>
      <abstract>The reliability of self-labeled data is an important issue when the data are regarded as ground-truth for training and testing learning-based models. This paper addresses the issue of false-alarm hashtags in the self-labeled data for irony detection. We analyze the ambiguity of hashtag usages and propose a novel neural network-based model, which incorporates linguistic information from different aspects, to disambiguate the usage of three hashtags that are widely used to collect the training data for irony detection. Furthermore, we apply our model to prune the self-labeled training data. Experimental results show that the irony detection model trained on the less but cleaner training instances outperforms the models trained on all data.</abstract>
      <url hash="435fcc0e">P18-2122</url>
      <video href="https://vimeo.com/285806165" />
      <attachment type="presentation" hash="24d535b5">P18-2122.Presentation.pdf</attachment>
      <doi>10.18653/v1/P18-2122</doi>
      <bibkey>huang-etal-2018-disambiguating</bibkey>
    </paper>
    </volume>
  <volume id="3">
    <meta>
      <booktitle>Proceedings of <fixed-case>ACL</fixed-case> 2018, Student Research Workshop</booktitle>
      <url hash="2cee3423">P18-3</url>
      <editor><first>Vered</first><last>Shwartz</last></editor>
      <editor><first>Jeniya</first><last>Tabassum</last></editor>
      <editor><first>Rob</first><last>Voigt</last></editor>
      <editor><first>Wanxiang</first><last>Che</last></editor>
      <editor><first>Marie-Catherine</first><last>de Marneffe</last></editor>
      <editor><first>Malvina</first><last>Nissim</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Melbourne, Australia</address>
      <month>July</month>
      <year>2018</year>
    </meta>
    <frontmatter>
      <url hash="13195672">P18-3000</url>
      <bibkey>acl-2018-acl</bibkey>
    </frontmatter>
    <paper id="6">
      <title>Recognizing Complex Entity Mentions: A Review and Future Directions</title>
      <author><first>Xiang</first><last>Dai</last></author>
      <pages>37&#8211;44</pages>
      <abstract>Standard named entity recognizers can effectively recognize entity mentions that consist of contiguous tokens and do not overlap with each other. However, in practice, there are many domains, such as the biomedical domain, in which there are nested, overlapping, and discontinuous entity mentions. These complex mentions cannot be directly recognized by conventional sequence tagging models because they may break the assumptions based on which sequence tagging techniques are built. We review the existing methods which are revised to tackle complex entity mentions and categorize them as tokenlevel and sentence-level approaches. We then identify the research gap, and discuss some directions that we are exploring.</abstract>
      <url hash="ac6b2a5a">P18-3006</url>
      <doi>10.18653/v1/P18-3006</doi>
      <bibkey>dai-2018-recognizing</bibkey>
    </paper>
    <paper id="7">
      <title>Automatic Detection of Cross-Disciplinary Knowledge Associations</title>
      <author><first>Menasha</first><last>Thilakaratne</last></author>
      <author><first>Katrina</first><last>Falkner</last></author>
      <author><first>Thushari</first><last>Atapattu</last></author>
      <pages>45&#8211;51</pages>
      <abstract>Detecting interesting, cross-disciplinary knowledge associations hidden in scientific publications can greatly assist scientists to formulate and validate scientifically sensible novel research hypotheses. This will also introduce new areas of research that can be successfully linked with their research discipline. Currently, this process is mostly performed manually by exploring the scientific publications, requiring a substantial amount of time and effort. Due to the exponential growth of scientific literature, it has become almost impossible for a scientist to keep track of all research advances. As a result, scientists tend to deal with fragments of the literature according to their specialisation. Consequently, important and hidden associations among these fragmented knowledge that can be linked to produce significant scientific discoveries remain unnoticed. This doctoral work aims to develop a novel knowledge discovery approach that suggests most promising research pathways by analysing the existing scientific literature.</abstract>
      <url hash="4617598c">P18-3007</url>
      <doi>10.18653/v1/P18-3007</doi>
      <bibkey>thilakaratne-etal-2018-automatic</bibkey>
    </paper>
    <paper id="8">
      <title>Language Identification and Named Entity Recognition in <fixed-case>H</fixed-case>inglish Code Mixed Tweets</title>
      <author><first>Kushagra</first><last>Singh</last></author>
      <author><first>Indira</first><last>Sen</last></author>
      <author><first>Ponnurangam</first><last>Kumaraguru</last></author>
      <pages>52&#8211;58</pages>
      <abstract>While growing code-mixed content on Online Social Networks(OSN) provides a fertile ground for studying various aspects of code-mixing, the lack of automated text analysis tools render such studies challenging. To meet this challenge, a family of tools for analyzing code-mixed data such as language identifiers, parts-of-speech (POS) taggers, chunkers have been developed. Named Entity Recognition (NER) is an important text analysis task which is not only informative by itself, but is also needed for downstream NLP tasks such as semantic role labeling. In this work, we present an exploration of automatic NER of code-mixed data. We compare our method with existing off-the-shelf NER tools for social media content,and find that our systems outperforms the best baseline by 33.18 % (F1 score).</abstract>
      <url hash="1be4423d">P18-3008</url>
      <doi>10.18653/v1/P18-3008</doi>
      <bibkey>singh-etal-2018-language</bibkey>
    </paper>
    <paper id="10">
      <title><fixed-case>S</fixed-case>uper<fixed-case>NMT</fixed-case>: Neural Machine Translation with Semantic Supersenses and Syntactic Supertags</title>
      <author><first>Eva</first><last>Vanmassenhove</last></author>
      <author><first>Andy</first><last>Way</last></author>
      <pages>67&#8211;73</pages>
      <abstract>In this paper we incorporate semantic supersensetags and syntactic supertag features into EN&#8211;FR and EN&#8211;DE factored NMT systems. In experiments on various test sets, we observe that such features (and particularly when combined) help the NMT model training to converge faster and improve the model quality according to the BLEU scores.</abstract>
      <url hash="919e76dc">P18-3010</url>
      <doi>10.18653/v1/P18-3010</doi>
      <bibkey>vanmassenhove-way-2018-supernmt</bibkey>
    </paper>
    <paper id="12">
      <title>Biomedical Document Retrieval for Clinical Decision Support System</title>
      <author><first>Jainisha</first><last>Sankhavara</last></author>
      <pages>84&#8211;90</pages>
      <abstract>The availability of huge amount of biomedical literature have opened up new possibilities to apply Information Retrieval and NLP for mining documents from them. In this work, we are focusing on biomedical document retrieval from literature for clinical decision support systems. We compare statistical and NLP based approaches of query reformulation for biomedical document retrieval. Also, we have modeled the biomedical document retrieval as a learning to rank problem. We report initial results for statistical and NLP based query reformulation approaches and learning to rank approach with future direction of research.</abstract>
      <url hash="762414d8">P18-3012</url>
      <doi>10.18653/v1/P18-3012</doi>
      <bibkey>sankhavara-2018-biomedical</bibkey>
    </paper>
    <paper id="13">
      <title>A Computational Approach to Feature Extraction for Identification of Suicidal Ideation in Tweets</title>
      <author><first>Ramit</first><last>Sawhney</last></author>
      <author><first>Prachi</first><last>Manchanda</last></author>
      <author><first>Raj</first><last>Singh</last></author>
      <author><first>Swati</first><last>Aggarwal</last></author>
      <pages>91&#8211;98</pages>
      <abstract>Technological advancements in the World Wide Web and social networks in particular coupled with an increase in social media usage has led to a positive correlation between the exhibition of Suicidal ideation on websites such as Twitter and cases of suicide. This paper proposes a novel supervised approach for detecting suicidal ideation in content on Twitter. A set of features is proposed for training both linear and ensemble classifiers over a dataset of manually annotated tweets. The performance of the proposed methodology is compared against four baselines that utilize varying approaches to validate its utility. The results are finally summarized by reflecting on the effect of the inclusion of the proposed features one by one for suicidal ideation detection.</abstract>
      <url hash="9ef9c565">P18-3013</url>
      <doi>10.18653/v1/P18-3013</doi>
      <bibkey>sawhney-etal-2018-computational</bibkey>
    </paper>
    <paper id="14">
      <title><fixed-case>BCSAT</fixed-case> : A Benchmark Corpus for Sentiment Analysis in <fixed-case>T</fixed-case>elugu Using Word-level Annotations</title>
      <author><first>Sreekavitha</first><last>Parupalli</last></author>
      <author><first>Vijjini</first><last>Anvesh Rao</last></author>
      <author><first>Radhika</first><last>Mamidi</last></author>
      <pages>99&#8211;104</pages>
      <abstract>The presented work aims at generating a systematically annotated corpus that can support the enhancement of sentiment analysis tasks in Telugu using word-level sentiment annotations. From OntoSenseNet, we extracted 11,000 adjectives, 253 adverbs, 8483 verbs and sentiment annotation is being done by language experts. We discuss the methodology followed for the polarity annotations and validate the developed resource. This work aims at developing a benchmark corpus, as an extension to SentiWordNet, and baseline accuracy for a model where lexeme annotations are applied for sentiment predictions. The fundamental aim of this paper is to validate and study the possibility of utilizing machine learning algorithms, word-level sentiment annotations in the task of automated sentiment identification. Furthermore, accuracy is improved by annotating the bi-grams extracted from the target corpus.</abstract>
      <url hash="40d93ed4">P18-3014</url>
      <doi>10.18653/v1/P18-3014</doi>
      <bibkey>parupalli-etal-2018-bcsat</bibkey>
    </paper>
    <paper id="15">
      <title>Reinforced Extractive Summarization with Question-Focused Rewards</title>
      <author><first>Kristjan</first><last>Arumae</last></author>
      <author id="fei-liu-utdallas"><first>Fei</first><last>Liu</last></author>
      <pages>105&#8211;111</pages>
      <abstract>We investigate a new training paradigm for extractive summarization. Traditionally, human abstracts are used to derive goldstandard labels for extraction units. However, the labels are often inaccurate, because human abstracts and source documents cannot be easily aligned at the word level. In this paper we convert human abstracts to a set of Cloze-style comprehension questions. System summaries are encouraged to preserve salient source content useful for answering questions and share common words with the abstracts. We use reinforcement learning to explore the space of possible extractive summaries and introduce a question-focused reward function to promote concise, fluent, and informative summaries. Our experiments show that the proposed method is effective. It surpasses state-of-the-art systems on the standard summarization dataset.</abstract>
      <url hash="88a0637d">P18-3015</url>
      <attachment type="presentation" hash="f715c27b">P18-3015.Presentation.pdf</attachment>
      <doi>10.18653/v1/P18-3015</doi>
      <bibkey>arumae-liu-2018-reinforced</bibkey>
    </paper>
    <paper id="16">
      <title>Graph-based Filtering of Out-of-Vocabulary Words for Encoder-Decoder Models</title>
      <author><first>Satoru</first><last>Katsumata</last></author>
      <author><first>Yukio</first><last>Matsumura</last></author>
      <author><first>Hayahide</first><last>Yamagishi</last></author>
      <author><first>Mamoru</first><last>Komachi</last></author>
      <pages>112&#8211;119</pages>
      <abstract>Encoder-decoder models typically only employ words that are frequently used in the training corpus because of the computational costs and/or to exclude noisy words. However, this vocabulary set may still include words that interfere with learning in encoder-decoder models. This paper proposes a method for selecting more suitable words for learning encoders by utilizing not only frequency, but also co-occurrence information, which we capture using the HITS algorithm. The proposed method is applied to two tasks: machine translation and grammatical error correction. For Japanese-to-English translation, this method achieved a BLEU score that was 0.56 points more than that of a baseline. It also outperformed the baseline method for English grammatical error correction, with an F-measure that was 1.48 points higher.</abstract>
      <url hash="f73122db">P18-3016</url>
      <doi>10.18653/v1/P18-3016</doi>
      <bibkey>katsumata-etal-2018-graph</bibkey>
      <pwccode url="https://github.com/Katsumata420/HITS_Ranking" additional="false">Katsumata420/HITS_Ranking</pwccode>
    </paper>
    <paper id="20">
      <title>Mixed Feelings: Natural Text Generation with Variable, Coexistent Affective Categories</title>
      <author><first>Lee</first><last>Kezar</last></author>
      <pages>141&#8211;145</pages>
      <abstract>Conversational agents, having the goal of natural language generation, must rely on language models which can integrate emotion into their responses. Recent projects outline models which can produce emotional sentences, but unlike human language, they tend to be restricted to one affective category out of a few. To my knowledge, none allow for the intentional coexistence of multiple emotions on the word or sentence level. Building on prior research which allows for variation in the intensity of a singular emotion, this research proposal outlines an LSTM (Long Short-Term Memory) language model which allows for variation in multiple emotions simultaneously.</abstract>
      <url hash="cd9c5409">P18-3020</url>
      <doi>10.18653/v1/P18-3020</doi>
      <bibkey>kezar-2018-mixed</bibkey>
    </paper>
    <paper id="21">
      <title>Automatic Spelling Correction for Resource-Scarce Languages using Deep Learning</title>
      <author><first>Pravallika</first><last>Etoori</last></author>
      <author><first>Manoj</first><last>Chinnakotla</last></author>
      <author><first>Radhika</first><last>Mamidi</last></author>
      <pages>146&#8211;152</pages>
      <abstract>Spelling correction is a well-known task in Natural Language Processing (NLP). Automatic spelling correction is important for many NLP applications like web search engines, text summarization, sentiment analysis etc. Most approaches use parallel data of noisy and correct word mappings from different sources as training data for automatic spelling correction. Indic languages are resource-scarce and do not have such parallel data due to low volume of queries and non-existence of such prior implementations. In this paper, we show how to build an automatic spelling corrector for resource-scarce languages. We propose a sequence-to-sequence deep learning model which trains end-to-end. We perform experiments on synthetic datasets created for Indic languages, Hindi and Telugu, by incorporating the spelling mistakes committed at character level. A comparative evaluation shows that our model is competitive with the existing spell checking and correction techniques for Indic languages.</abstract>
      <url hash="b40301f6">P18-3021</url>
      <doi>10.18653/v1/P18-3021</doi>
      <bibkey>etoori-etal-2018-automatic</bibkey>
    </paper>
    </volume>
  <volume id="4">
    <meta>
      <booktitle>Proceedings of <fixed-case>ACL</fixed-case> 2018, System Demonstrations</booktitle>
      <url hash="fdafb184">P18-4</url>
      <editor id="fei-liu-utdallas"><first>Fei</first><last>Liu</last></editor>
      <editor><first>Thamar</first><last>Solorio</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Melbourne, Australia</address>
      <month>July</month>
      <year>2018</year>
    </meta>
    <frontmatter>
      <url hash="70e31f86">P18-4000</url>
      <bibkey>acl-2018-acl-2018</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Platforms for Non-speakers Annotating Names in Any Language</title>
      <author><first>Ying</first><last>Lin</last></author>
      <author><first>Cash</first><last>Costello</last></author>
      <author><first>Boliang</first><last>Zhang</last></author>
      <author><first>Di</first><last>Lu</last></author>
      <author><first>Heng</first><last>Ji</last></author>
      <author><first>James</first><last>Mayfield</last></author>
      <author><first>Paul</first><last>McNamee</last></author>
      <pages>1&#8211;6</pages>
      <abstract>We demonstrate two annotation platforms that allow an English speaker to annotate names for any language without knowing the language. These platforms provided high-quality &#8217;&#8216;silver standard&#8221; annotations for low-resource language name taggers (Zhang et al., 2017) that achieved state-of-the-art performance on two surprise languages (Oromo and Tigrinya) at LoreHLT20171 and ten languages at TAC-KBP EDL2017 (Ji et al., 2017). We discuss strengths and limitations and compare other methods of creating silver- and gold-standard annotations using native speakers. We will make our tools publicly available for research use.</abstract>
      <url hash="583b5877">P18-4001</url>
      <doi>10.18653/v1/P18-4001</doi>
      <bibkey>lin-etal-2018-platforms</bibkey>
    </paper>
    <paper id="2">
      <title><fixed-case>N</fixed-case>ovel<fixed-case>P</fixed-case>erspective: Identifying Point of View Characters</title>
      <author><first>Lyndon</first><last>White</last></author>
      <author><first>Roberto</first><last>Togneri</last></author>
      <author><first>Wei</first><last>Liu</last></author>
      <author><first>Mohammed</first><last>Bennamoun</last></author>
      <pages>7&#8211;12</pages>
      <abstract>We present NovelPerspective: a tool to allow consumers to subset their digital literature, based on point of view (POV) character. Many novels have multiple main characters each with their own storyline running in parallel. A well-known example is George R. R. Martin&#8217;s novel: &#8220;A Game of Thrones&#8221;, and others from that series. Our tool detects the main character that each section is from the POV of, and allows the user to generate a new ebook with only those sections. This gives consumers new options in how they consume their media; allowing them to pursue the storylines sequentially, or skip chapters about characters they find boring. We present two heuristic-based baselines, and two machine learning based methods for the detection of the main character.</abstract>
      <url hash="94542e3d">P18-4002</url>
      <doi>10.18653/v1/P18-4002</doi>
      <bibkey>white-etal-2018-novelperspective</bibkey>
      <pwccode url="https://github.com/oxinabox/NovelPerspective" additional="false">oxinabox/NovelPerspective</pwccode>
    </paper>
    <paper id="4">
      <title><fixed-case>H</fixed-case>arri<fixed-case>GT</fixed-case>: A Tool for Linking News to Science</title>
      <author><first>James</first><last>Ravenscroft</last></author>
      <author><first>Amanda</first><last>Clare</last></author>
      <author><first>Maria</first><last>Liakata</last></author>
      <pages>19&#8211;24</pages>
      <abstract>Being able to reliably link scientific works to the newspaper articles that discuss them could provide a breakthrough in the way we rationalise and measure the impact of science on our society. Linking these articles is challenging because the language used in the two domains is very different, and the gathering of online resources to align the two is a substantial information retrieval endeavour. We present HarriGT, a semi-automated tool for building corpora of news articles linked to the scientific papers that they discuss. Our aim is to facilitate future development of information-retrieval tools for newspaper/scientific work citation linking. HarriGT retrieves newspaper articles from an archive containing 17 years of UK web content. It also integrates with 3 large external citation networks, leveraging named entity extraction, and document classification to surface relevant examples of scientific literature to the user. We also provide a tuned candidate ranking algorithm to highlight potential links between scientific papers and newspaper articles to the user, in order of likelihood. HarriGT is provided as an open source tool (<url>http://harrigt.xyz</url>).</abstract>
      <url hash="4949cc01">P18-4004</url>
      <doi>10.18653/v1/P18-4004</doi>
      <bibkey>ravenscroft-etal-2018-harrigt</bibkey>
    </paper>
    <paper id="6">
      <title><fixed-case>YEDDA</fixed-case>: A Lightweight Collaborative Text Span Annotation Tool</title>
      <author><first>Jie</first><last>Yang</last></author>
      <author><first>Yue</first><last>Zhang</last></author>
      <author><first>Linwei</first><last>Li</last></author>
      <author><first>Xingxuan</first><last>Li</last></author>
      <pages>31&#8211;36</pages>
      <abstract>In this paper, we introduce Yedda, a lightweight but efficient and comprehensive open-source tool for text span annotation. Yedda provides a systematic solution for text span annotation, ranging from collaborative user annotation to administrator evaluation and analysis. It overcomes the low efficiency of traditional text annotation tools by annotating entities through both command line and shortcut keys, which are configurable with custom labels. Yedda also gives intelligent recommendations by learning the up-to-date annotated text. An administrator client is developed to evaluate annotation quality of multiple annotators and generate detailed comparison report for each annotator pair. Experiments show that the proposed system can reduce the annotation time by half compared with existing annotation tools. And the annotation time can be further compressed by 16.47% through intelligent recommendation.</abstract>
      <url hash="d9aeab5a">P18-4006</url>
      <doi>10.18653/v1/P18-4006</doi>
      <bibkey>yang-etal-2018-yedda</bibkey>
      <pwccode url="https://github.com/jiesutd/YEDDA" additional="false">jiesutd/YEDDA</pwccode>
    </paper>
    <paper id="7">
      <title><fixed-case>N</fixed-case>ext<fixed-case>G</fixed-case>en <fixed-case>AML</fixed-case>: Distributed Deep Learning based Language Technologies to Augment Anti Money Laundering Investigation</title>
      <author><first>Jingguang</first><last>Han</last></author>
      <author><first>Utsab</first><last>Barman</last></author>
      <author><first>Jeremiah</first><last>Hayes</last></author>
      <author><first>Jinhua</first><last>Du</last></author>
      <author><first>Edward</first><last>Burgin</last></author>
      <author><first>Dadong</first><last>Wan</last></author>
      <pages>37&#8211;42</pages>
      <abstract>Most of the current anti money laundering (AML) systems, using handcrafted rules, are heavily reliant on existing structured databases, which are not capable of effectively and efficiently identifying hidden and complex ML activities, especially those with dynamic and time-varying characteristics, resulting in a high percentage of false positives. Therefore, analysts are engaged for further investigation which significantly increases human capital cost and processing time. To alleviate these issues, this paper presents a novel framework for the next generation AML by applying and visualizing deep learning-driven natural language processing (NLP) technologies in a distributed and scalable manner to augment AML monitoring and investigation. The proposed distributed framework performs news and tweet sentiment analysis, entity recognition, relation extraction, entity linking and link analysis on different data sources (e.g. news articles and tweets) to provide additional evidence to human investigators for final decision-making. Each NLP module is evaluated on a task-specific data set, and the overall experiments are performed on synthetic and real-world datasets. Feedback from AML practitioners suggests that our system can reduce approximately 30% time and cost compared to their previous manual approaches of AML investigation.</abstract>
      <url hash="a35622bf">P18-4007</url>
      <doi>10.18653/v1/P18-4007</doi>
      <bibkey>han-etal-2018-nextgen</bibkey>
    </paper>
    <paper id="8">
      <title><fixed-case>NLP</fixed-case> Web Services for Resource-Scarce Languages</title>
      <author><first>Martin</first><last>Puttkammer</last></author>
      <author><first>Roald</first><last>Eiselen</last></author>
      <author><first>Justin</first><last>Hocking</last></author>
      <author><first>Frederik</first><last>Koen</last></author>
      <pages>43&#8211;49</pages>
      <abstract>In this paper, we present a project where existing text-based core technologies were ported to Java-based web services from various architectures. These technologies were developed over a period of eight years through various government funded projects for 10 resource-scarce languages spoken in South Africa. We describe the API and a simple web front-end capable of completing various predefined tasks.</abstract>
      <url hash="3172ed96">P18-4008</url>
      <doi>10.18653/v1/P18-4008</doi>
      <bibkey>puttkammer-etal-2018-nlp</bibkey>
    </paper>
    <paper id="12">
      <title><fixed-case>SANTO</fixed-case>: A Web-based Annotation Tool for Ontology-driven Slot Filling</title>
      <author><first>Matthias</first><last>Hartung</last></author>
      <author><first>Hendrik</first><last>ter Horst</last></author>
      <author><first>Frank</first><last>Grimm</last></author>
      <author><first>Tim</first><last>Diekmann</last></author>
      <author><first>Roman</first><last>Klinger</last></author>
      <author><first>Philipp</first><last>Cimiano</last></author>
      <pages>68&#8211;73</pages>
      <abstract>Supervised machine learning algorithms require training data whose generation for complex relation extraction tasks tends to be difficult. Being optimized for relation extraction at sentence level, many annotation tools lack in facilitating the annotation of relational structures that are widely spread across the text. This leads to non-intuitive and cumbersome visualizations, making the annotation process unnecessarily time-consuming. We propose SANTO, an easy-to-use, domain-adaptive annotation tool specialized for complex slot filling tasks which may involve problems of cardinality and referential grounding. The web-based architecture enables fast and clearly structured annotation for multiple users in parallel. Relational structures are formulated as templates following the conceptualization of an underlying ontology. Further, import and export procedures of standard formats enable interoperability with external sources and tools.</abstract>
      <url hash="54cd5626">P18-4012</url>
      <doi>10.18653/v1/P18-4012</doi>
      <bibkey>hartung-etal-2018-santo</bibkey>
    </paper>
    <paper id="13">
      <title><fixed-case>NCRF</fixed-case>++: An Open-source Neural Sequence Labeling Toolkit</title>
      <author><first>Jie</first><last>Yang</last></author>
      <author><first>Yue</first><last>Zhang</last></author>
      <pages>74&#8211;79</pages>
      <abstract>This paper describes NCRF++, a toolkit for neural sequence labeling. NCRF++ is designed for quick implementation of different neural sequence labeling models with a CRF inference layer. It provides users with an inference for building the custom model structure through configuration file with flexible neural feature design and utilization. Built on PyTorch <url>http://pytorch.org/</url>, the core operations are calculated in batch, making the toolkit efficient with the acceleration of GPU. It also includes the implementations of most state-of-the-art neural sequence labeling models such as LSTM-CRF, facilitating reproducing and refinement on those methods.</abstract>
      <url hash="ab15861e">P18-4013</url>
      <doi>10.18653/v1/P18-4013</doi>
      <bibkey>yang-zhang-2018-ncrf</bibkey>
      <pwccode url="https://github.com/jiesutd/NCRFpp" additional="true">jiesutd/NCRFpp</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/conll-2003">CoNLL-2003</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/penn-treebank">Penn Treebank</pwcdataset>
    </paper>
    <paper id="15">
      <title>A Web-scale system for scientific knowledge exploration</title>
      <author><first>Zhihong</first><last>Shen</last></author>
      <author><first>Hao</first><last>Ma</last></author>
      <author><first>Kuansan</first><last>Wang</last></author>
      <pages>87&#8211;92</pages>
      <abstract>To enable efficient exploration of Web-scale scientific knowledge, it is necessary to organize scientific publications into a hierarchical concept structure. In this work, we present a large-scale system to (1) identify hundreds of thousands of scientific concepts, (2) tag these identified concepts to hundreds of millions of scientific publications by leveraging both text and graph structure, and (3) build a six-level concept hierarchy with a subsumption-based model. The system builds the most comprehensive cross-domain scientific concept ontology published to date, with more than 200 thousand concepts and over one million relationships.</abstract>
      <url hash="c78323cc">P18-4015</url>
      <doi>10.18653/v1/P18-4015</doi>
      <bibkey>shen-etal-2018-web</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/microsoft-academic-graph">Microsoft Academic Graph</pwcdataset>
    </paper>
    <paper id="17">
      <title>The <fixed-case>SUMMA</fixed-case> Platform: A Scalable Infrastructure for Multi-lingual Multi-media Monitoring</title>
      <author><first>Ulrich</first><last>Germann</last></author>
      <author><first>Ren&#257;rs</first><last>Liepins</last></author>
      <author><first>Guntis</first><last>Barzdins</last></author>
      <author><first>Didzis</first><last>Gosko</last></author>
      <author><first>Sebasti&#227;o</first><last>Miranda</last></author>
      <author><first>David</first><last>Nogueira</last></author>
      <pages>99&#8211;104</pages>
      <abstract>The open-source SUMMA Platform is a highly scalable distributed architecture for monitoring a large number of media broadcasts in parallel, with a lag behind actual broadcast time of at most a few minutes. The Platform offers a fully automated media ingestion pipeline capable of recording live broadcasts, detection and transcription of spoken content, translation of all text (original or transcribed) into English, recognition and linking of Named Entities, topic detection, clustering and cross-lingual multi-document summarization of related media items, and last but not least, extraction and storage of factual claims in these news items. Browser-based graphical user interfaces provide humans with aggregated information as well as structured access to individual news items stored in the Platform&#8217;s database. This paper describes the intended use cases and provides an overview over the system&#8217;s implementation.</abstract>
      <url hash="3f40aac1">P18-4017</url>
      <doi>10.18653/v1/P18-4017</doi>
      <bibkey>germann-etal-2018-summa</bibkey>
    </paper>
    <paper id="21">
      <title><fixed-case>D</fixed-case>eep<fixed-case>P</fixed-case>avlov: Open-Source Library for Dialogue Systems</title>
      <author><first>Mikhail</first><last>Burtsev</last></author>
      <author><first>Alexander</first><last>Seliverstov</last></author>
      <author><first>Rafael</first><last>Airapetyan</last></author>
      <author><first>Mikhail</first><last>Arkhipov</last></author>
      <author><first>Dilyara</first><last>Baymurzina</last></author>
      <author><first>Nickolay</first><last>Bushkov</last></author>
      <author><first>Olga</first><last>Gureenkova</last></author>
      <author><first>Taras</first><last>Khakhulin</last></author>
      <author><first>Yuri</first><last>Kuratov</last></author>
      <author><first>Denis</first><last>Kuznetsov</last></author>
      <author><first>Alexey</first><last>Litinsky</last></author>
      <author><first>Varvara</first><last>Logacheva</last></author>
      <author><first>Alexey</first><last>Lymar</last></author>
      <author><first>Valentin</first><last>Malykh</last></author>
      <author><first>Maxim</first><last>Petrov</last></author>
      <author><first>Vadim</first><last>Polulyakh</last></author>
      <author><first>Leonid</first><last>Pugachev</last></author>
      <author><first>Alexey</first><last>Sorokin</last></author>
      <author><first>Maria</first><last>Vikhreva</last></author>
      <author><first>Marat</first><last>Zaynutdinov</last></author>
      <pages>122&#8211;127</pages>
      <abstract>Adoption of messaging communication and voice assistants has grown rapidly in the last years. This creates a demand for tools that speed up prototyping of feature-rich dialogue systems. An open-source library DeepPavlov is tailored for development of conversational agents. The library prioritises efficiency, modularity, and extensibility with the goal to make it easier to develop dialogue systems from scratch and with limited data available. It supports modular as well as end-to-end approaches to implementation of conversational agents. Conversational agent consists of skills and every skill can be decomposed into components. Components are usually models which solve typical NLP tasks such as intent classification, named entity recognition or pre-trained word vectors. Sequence-to-sequence chit-chat skill, question answering skill or task-oriented skill can be assembled from components provided in the library.</abstract>
      <url hash="48c7d3e7">P18-4021</url>
      <doi>10.18653/v1/P18-4021</doi>
      <bibkey>burtsev-etal-2018-deeppavlov</bibkey>
    </paper>
    <paper id="23">
      <title>A Flexible, Efficient and Accurate Framework for Community Question Answering Pipelines</title>
      <author><first>Salvatore</first><last>Romeo</last></author>
      <author><first>Giovanni</first><last>Da San Martino</last></author>
      <author><first>Alberto</first><last>Barr&#243;n-Cede&#241;o</last></author>
      <author><first>Alessandro</first><last>Moschitti</last></author>
      <pages>134&#8211;139</pages>
      <abstract>Although deep neural networks have been proving to be excellent tools to deliver state-of-the-art results, when data is scarce and the tackled tasks involve complex semantic inference, deep linguistic processing and traditional structure-based approaches, such as tree kernel methods, are an alternative solution. Community Question Answering is a research area that benefits from deep linguistic analysis to improve the experience of the community of forum users. In this paper, we present a UIMA framework to distribute the computation of cQA tasks over computer clusters such that traditional systems can scale to large datasets and deliver fast processing.</abstract>
      <url hash="ff1f4cb6">P18-4023</url>
      <doi>10.18653/v1/P18-4023</doi>
      <bibkey>romeo-etal-2018-flexible</bibkey>
    </paper>
    <paper id="24">
      <title>Moon <fixed-case>IME</fixed-case>: Neural-based <fixed-case>C</fixed-case>hinese <fixed-case>P</fixed-case>inyin Aided Input Method with Customizable Association</title>
      <author><first>Yafang</first><last>Huang</last></author>
      <author><first>Zuchao</first><last>Li</last></author>
      <author><first>Zhuosheng</first><last>Zhang</last></author>
      <author><first>Hai</first><last>Zhao</last></author>
      <pages>140&#8211;145</pages>
      <abstract>Chinese pinyin input method engine (IME) lets user conveniently input Chinese into a computer by typing pinyin through the common keyboard. In addition to offering high conversion quality, modern pinyin IME is supposed to aid user input with extended association function. However, existing solutions for such functions are roughly based on oversimplified matching algorithms at word-level, whose resulting products provide limited extension associated with user inputs. This work presents the Moon IME, a pinyin IME that integrates the attention-based neural machine translation (NMT) model and Information Retrieval (IR) to offer amusive and customizable association ability. The released IME is implemented on Windows via text services framework.</abstract>
      <url hash="2e7e123f">P18-4024</url>
      <doi>10.18653/v1/P18-4024</doi>
      <bibkey>huang-etal-2018-moon</bibkey>
    </paper>
  </volume>
  <volume id="5">
    <meta>
      <booktitle>Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics: Tutorial Abstracts</booktitle>
      <url hash="57997b8b">P18-5</url>
      <editor><first>Yoav</first><last>Artzi</last></editor>
      <editor><first>Jacob</first><last>Eisenstein</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Melbourne, Australia</address>
      <month>July</month>
      <year>2018</year>
    </meta>
    <frontmatter>
      <url hash="96e71385">P18-5000</url>
      <bibkey>acl-2018-association-linguistics-tutorial</bibkey>
    </frontmatter>
    <paper id="4">
      <title>Connecting Language and Vision to Actions</title>
      <author><first>Peter</first><last>Anderson</last></author>
      <author><first>Abhishek</first><last>Das</last></author>
      <author><first>Qi</first><last>Wu</last></author>
      <pages>10&#8211;14</pages>
      <abstract>A long-term goal of AI research is to build intelligent agents that can see the rich visual environment around us, communicate this understanding in natural language to humans and other agents, and act in a physical or embodied environment. To this end, recent advances at the intersection of language and vision have made incredible progress &#8211; from being able to generate natural language descriptions of images/videos, to answering questions about them, to even holding free-form conversations about visual content! However, while these agents can passively describe images or answer (a sequence of) questions about them, they cannot act in the world (what if I cannot answer a question from my current view, or I am asked to move or manipulate something?). Thus, the challenge now is to extend this progress in language and vision to embodied agents that take actions and actively interact with their visual environments. To reduce the entry barrier for new researchers, this tutorial will provide an overview of the growing number of multimodal tasks and datasets that combine textual and visual understanding. We will comprehensively review existing state-of-the-art approaches to selected tasks such as image captioning, visual question answering (VQA) and visual dialog, presenting the key architectural building blocks (such as co-attention) and novel algorithms (such as cooperative/adversarial games) used to train models for these tasks. We will then discuss some of the current and upcoming challenges of combining language, vision and actions, and introduce some recently-released interactive 3D simulation environments designed for this purpose.</abstract>
      <url hash="00308d8f">P18-5004</url>
      <doi>10.18653/v1/P18-5004</doi>
      <bibkey>anderson-etal-2018-connecting</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/visual-question-answering">Visual Question Answering</pwcdataset>
    </paper>
    <paper id="5">
      <title>Beyond Multiword Expressions: Processing Idioms and Metaphors</title>
      <author><first>Valia</first><last>Kordoni</last></author>
      <pages>15&#8211;16</pages>
      <abstract>Idioms and metaphors are characteristic to all areas of human activity and to all types of discourse. Their processing is a rapidly growing area in NLP, since they have become a big challenge for NLP systems. Their omnipresence in language has been established in a number of corpus studies and the role they play in human reasoning has also been confirmed in psychological experiments. This makes idioms and metaphors an important research area for computational and cognitive linguistics, and their automatic identification and interpretation indispensable for any semantics-oriented NLP application. This tutorial aims to provide attendees with a clear notion of the linguistic characteristics of idioms and metaphors, computational models of idioms and metaphors using state-of-the-art NLP techniques, their relevance for the intersection of deep learning and natural language processing, what methods and resources are available to support their use, and what more could be done in the future. Our target audience are researchers and practitioners in machine learning, parsing (syntactic and semantic) and language technology, not necessarily experts in idioms and metaphors, who are interested in tasks that involve or could benefit from considering idioms and metaphors as a pervasive phenomenon in human language and communication.</abstract>
      <url hash="d42161bc">P18-5005</url>
      <doi>10.18653/v1/P18-5005</doi>
      <bibkey>kordoni-2018-beyond</bibkey>
    </paper>
    <paper id="7">
      <title>Deep Reinforcement Learning for <fixed-case>NLP</fixed-case></title>
      <author><first>William Yang</first><last>Wang</last></author>
      <author><first>Jiwei</first><last>Li</last></author>
      <author><first>Xiaodong</first><last>He</last></author>
      <pages>19&#8211;21</pages>
      <abstract>Many Natural Language Processing (NLP) tasks (including generation, language grounding, reasoning, information extraction, coreference resolution, and dialog) can be formulated as deep reinforcement learning (DRL) problems. However, since language is often discrete and the space for all sentences is infinite, there are many challenges for formulating reinforcement learning problems of NLP tasks. In this tutorial, we provide a gentle introduction to the foundation of deep reinforcement learning, as well as some practical DRL solutions in NLP. We describe recent advances in designing deep reinforcement learning for NLP, with a special focus on generation, dialogue, and information extraction. Finally, we discuss why they succeed, and when they may fail, aiming at providing some practical advice about deep reinforcement learning for solving real-world NLP problems.</abstract>
      <url hash="44e9e9ab">P18-5007</url>
      <doi>10.18653/v1/P18-5007</doi>
      <bibkey>wang-etal-2018-deep</bibkey>
    </paper>
    </volume>
</collection>