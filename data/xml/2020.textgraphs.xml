<collection id="2020.textgraphs">
  <volume id="1" ingest-date="2020-11-29">
    <meta>
      <booktitle>Proceedings of the Graph-based Methods for Natural Language Processing (TextGraphs)</booktitle>
      <editor><first>Dmitry</first><last>Ustalov</last></editor>
      <editor><first>Swapna</first><last>Somasundaran</last></editor>
      <editor><first>Alexander</first><last>Panchenko</last></editor>
      <editor><first>Fragkiskos D.</first><last>Malliaros</last></editor>
      <editor><first>Ioana</first><last>Hulpu&#537;</last></editor>
      <editor><first>Peter</first><last>Jansen</last></editor>
      <editor><first>Abhik</first><last>Jana</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Barcelona, Spain (Online)</address>
      <month>December</month>
      <year>2020</year>
    </meta>
    <frontmatter>
      <url hash="00d04105">2020.textgraphs-1.0</url>
      <bibkey>textgraphs-2020-graph</bibkey>
    </frontmatter>
    <paper id="1">
      <title>A survey of embedding models of entities and relationships for knowledge graph completion</title>
      <author><first>Dat Quoc</first><last>Nguyen</last></author>
      <pages>1&#8211;14</pages>
      <abstract>Knowledge graphs (KGs) of real-world facts about entities and their relationships are useful resources for a variety of natural language processing tasks. However, because knowledge graphs are typically incomplete, it is useful to perform knowledge graph completion or link prediction, i.e. predict whether a relationship not in the knowledge graph is likely to be true. This paper serves as a comprehensive survey of embedding models of entities and relationships for knowledge graph completion, summarizing up-to-date experimental results on standard benchmark datasets and pointing out potential future research directions.</abstract>
      <url hash="645a9fe7">2020.textgraphs-1.1</url>
      <attachment type="OptionalSupplementaryMaterial" hash="c48b2a8f">2020.textgraphs-1.1.OptionalSupplementaryMaterial.pdf</attachment>
      <bibkey>nguyen-2020-survey</bibkey>
      <doi>10.18653/v1/2020.textgraphs-1.1</doi>
      <pwccode url="" additional="true" />
      <pwcdataset url="https://paperswithcode.com/dataset/fb15k">FB15k</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/fb15k-237">FB15k-237</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/nell">NELL</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wn18">WN18</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wn18rr">WN18RR</pwcdataset>
    </paper>
    <paper id="5">
      <title>Contextual <fixed-case>BERT</fixed-case>: Conditioning the Language Model Using a Global State</title>
      <author><first>Timo I.</first><last>Denk</last></author>
      <author><first>Ana</first><last>Peleteiro Ramallo</last></author>
      <pages>46&#8211;50</pages>
      <abstract>BERT is a popular language model whose main pre-training task is to fill in the blank, i.e., predicting a word that was masked out of a sentence, based on the remaining words. In some applications, however, having an additional context can help the model make the right prediction, e.g., by taking the domain or the time of writing into account. This motivates us to advance the BERT architecture by adding a global state for conditioning on a fixed-sized context. We present our two novel approaches and apply them to an industry use-case, where we complete fashion outfits with missing articles, conditioned on a specific customer. An experimental comparison to other methods from the literature shows that our methods improve personalization significantly.</abstract>
      <url hash="20609025">2020.textgraphs-1.5</url>
      <bibkey>denk-peleteiro-ramallo-2020-contextual</bibkey>
      <doi>10.18653/v1/2020.textgraphs-1.5</doi>
    </paper>
    <paper id="13">
      <title>Explanation Regeneration via Multi-Hop <fixed-case>ILP</fixed-case> Inference over Knowledge Base</title>
      <author><first>Aayushee</first><last>Gupta</last></author>
      <author><first>Gopalakrishnan</first><last>Srinivasaraghavan</last></author>
      <pages>109&#8211;114</pages>
      <abstract>Textgraphs 2020 Workshop organized a shared task on &#8216;Explanation Regeneration&#8217; that required reconstructing gold explanations for elementary science questions. This work describes our submission to the task which is based on multiple components: a BERT baseline ranking, an Integer Linear Program (ILP) based re-scoring and a regression model for re-ranking the explanation facts. Our system achieved a Mean Average Precision score of 0.3659.</abstract>
      <url hash="41fbef76">2020.textgraphs-1.13</url>
      <attachment type="OptionalSupplementaryMaterial" hash="20936250">2020.textgraphs-1.13.OptionalSupplementaryMaterial.pdf</attachment>
      <bibkey>gupta-srinivasaraghavan-2020-explanation</bibkey>
      <doi>10.18653/v1/2020.textgraphs-1.13</doi>
    </paper>
    </volume>
</collection>