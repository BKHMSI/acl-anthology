<collection id="2020.conll">
  <volume id="1" ingest-date="2020-11-05">
    <meta>
      <booktitle>Proceedings of the 24th Conference on Computational Natural Language Learning</booktitle>
      <editor><first>Raquel</first><last>Fern&#225;ndez</last></editor>
      <editor><first>Tal</first><last>Linzen</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Online</address>
      <month>November</month>
      <year>2020</year>
    </meta>
    <frontmatter>
      <url hash="92d9bf6e">2020.conll-1.0</url>
      <bibkey>conll-2020-natural</bibkey>
    </frontmatter>
    <paper id="3">
      <title>Neural Proof Nets</title>
      <author><first>Konstantinos</first><last>Kogkalidis</last></author>
      <author><first>Michael</first><last>Moortgat</last></author>
      <author><first>Richard</first><last>Moot</last></author>
      <pages>26&#8211;40</pages>
      <abstract>Linear logic and the linear &#955;-calculus have a long standing tradition in the study of natural language form and meaning. Among the proof calculi of linear logic, proof nets are of particular interest, offering an attractive geometric representation of derivations that is unburdened by the bureaucratic complications of conventional prooftheoretic formats. Building on recent advances in set-theoretic learning, we propose a neural variant of proof nets based on Sinkhorn networks, which allows us to translate parsing as the problem of extracting syntactic primitives and permuting them into alignment. Our methodology induces a batch-efficient, end-to-end differentiable architecture that actualizes a formally grounded yet highly efficient neuro-symbolic parser. We test our approach on &#198;Thel, a dataset of type-logical derivations for written Dutch, where it manages to correctly transcribe raw text sentences into proofs and terms of the linear &#955;-calculus with an accuracy of as high as 70%.</abstract>
      <url hash="dfb23f26">2020.conll-1.3</url>
      <doi>10.18653/v1/2020.conll-1.3</doi>
      <bibkey>kogkalidis-etal-2020-neural</bibkey>
      <pwccode url="https://github.com/konstantinosKokos/neural-proof-nets" additional="false">konstantinosKokos/neural-proof-nets</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/aethel">aethel</pwcdataset>
    </paper>
    <paper id="6">
      <title>On the Frailty of Universal <fixed-case>POS</fixed-case> Tags for Neural <fixed-case>UD</fixed-case> Parsers</title>
      <author><first>Mark</first><last>Anderson</last></author>
      <author><first>Carlos</first><last>G&#243;mez-Rodr&#237;guez</last></author>
      <pages>69&#8211;96</pages>
      <abstract>We present an analysis on the effect UPOS accuracy has on parsing performance. Results suggest that leveraging UPOS tags as fea-tures for neural parsers requires a prohibitively high tagging accuracy and that the use of gold tags offers a non-linear increase in performance, suggesting some sort of exceptionality. We also investigate what aspects of predicted UPOS tags impact parsing accuracy the most, highlighting some potentially meaningful linguistic facets of the problem.</abstract>
      <url hash="27be3738">2020.conll-1.6</url>
      <doi>10.18653/v1/2020.conll-1.6</doi>
      <bibkey>anderson-gomez-rodriguez-2020-frailty</bibkey>
    </paper>
    <paper id="11">
      <title>Bridging Information-Seeking Human Gaze and Machine Reading Comprehension</title>
      <author><first>Jonathan</first><last>Malmaud</last></author>
      <author><first>Roger</first><last>Levy</last></author>
      <author><first>Yevgeni</first><last>Berzak</last></author>
      <pages>142&#8211;152</pages>
      <abstract>In this work, we analyze how human gaze during reading comprehension is conditioned on the given reading comprehension question, and whether this signal can be beneficial for machine reading comprehension. To this end, we collect a new eye-tracking dataset with a large number of participants engaging in a multiple choice reading comprehension task. Our analysis of this data reveals increased fixation times over parts of the text that are most relevant for answering the question. Motivated by this finding, we propose making automated reading comprehension more human-like by mimicking human information-seeking reading behavior during reading comprehension. We demonstrate that this approach leads to performance gains on multiple choice question answering in English for a state-of-the-art reading comprehension model.</abstract>
      <url hash="42856397">2020.conll-1.11</url>
      <doi>10.18653/v1/2020.conll-1.11</doi>
      <bibkey>malmaud-etal-2020-bridging</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/onestopenglish">OneStopEnglish</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/onestopqa">OneStopQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/race">RACE</pwcdataset>
    </paper>
    <paper id="12">
      <title>A Corpus of Very Short Scientific Summaries</title>
      <author><first>Yifan</first><last>Chen</last></author>
      <author><first>Tamara</first><last>Polajnar</last></author>
      <author><first>Colin</first><last>Batchelor</last></author>
      <author><first>Simone</first><last>Teufel</last></author>
      <pages>153&#8211;164</pages>
      <abstract>We present a new summarisation task, taking scientific articles and producing journal table-of-contents entries in the chemistry domain. These are one- or two-sentence author-written summaries that present the key findings of a paper. This is a first look at this summarisation task with an open access publication corpus consisting of titles and abstracts, as input texts, and short author-written advertising blurbs, as the ground truth. We introduce the dataset and evaluate it with state-of-the-art summarisation methods.</abstract>
      <url hash="aee8def9">2020.conll-1.12</url>
      <doi>10.18653/v1/2020.conll-1.12</doi>
      <bibkey>chen-etal-2020-corpus</bibkey>
      <pwccode url="https://github.com/atulkum/pointer_summarizer" additional="false">atulkum/pointer_summarizer</pwccode>
    </paper>
    <paper id="16">
      <title>Identifying Incorrect Labels in the <fixed-case>C</fixed-case>o<fixed-case>NLL</fixed-case>-2003 Corpus</title>
      <author><first>Frederick</first><last>Reiss</last></author>
      <author><first>Hong</first><last>Xu</last></author>
      <author><first>Bryan</first><last>Cutler</last></author>
      <author><first>Karthik</first><last>Muthuraman</last></author>
      <author><first>Zachary</first><last>Eichenberger</last></author>
      <pages>215&#8211;226</pages>
      <abstract>The CoNLL-2003 corpus for English-language named entity recognition (NER) is one of the most influential corpora for NER model research. A large number of publications, including many landmark works, have used this corpus as a source of ground truth for NER tasks. In this paper, we examine this corpus and identify over 1300 incorrect labels (out of 35089 in the corpus). In particular, the number of incorrect labels in the test fold is comparable to the number of errors that state-of-the-art models make when running inference over this corpus. We describe the process by which we identified these incorrect labels, using novel variants of techniques from semi-supervised learning. We also summarize the types of errors that we found, and we revisit several recent results in NER in light of the corrected data. Finally, we show experimentally that our corrections to the corpus have a positive impact on three state-of-the-art models.</abstract>
      <url hash="b111e911">2020.conll-1.16</url>
      <doi>10.18653/v1/2020.conll-1.16</doi>
      <bibkey>reiss-etal-2020-identifying</bibkey>
      <pwccode url="https://github.com/codait/text-extensions-for-pandas" additional="true">codait/text-extensions-for-pandas</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/conll">CoNLL++</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/conll-2003">CoNLL-2003</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/rcv1">RCV1</pwcdataset>
    </paper>
    <paper id="20">
      <title>Cross-lingual Embeddings Reveal Universal and Lineage-Specific Patterns in Grammatical Gender Assignment</title>
      <author><first>Hartger</first><last>Veeman</last></author>
      <author><first>Marc</first><last>Allassonni&#232;re-Tang</last></author>
      <author><first>Aleksandrs</first><last>Berdicevskis</last></author>
      <author><first>Ali</first><last>Basirat</last></author>
      <pages>265&#8211;275</pages>
      <abstract>Grammatical gender is assigned to nouns differently in different languages. Are all factors that influence gender assignment idiosyncratic to languages or are there any that are universal? Using cross-lingual aligned word embeddings, we perform two experiments to address these questions about language typology and human cognition. In both experiments, we predict the gender of nouns in language X using a classifier trained on the nouns of language Y, and take the classifier&#8217;s accuracy as a measure of transferability of gender systems. First, we show that for 22 Indo-European languages the transferability decreases as the phylogenetic distance increases. This correlation supports the claim that some gender assignment factors are idiosyncratic, and as the languages diverge, the proportion of shared inherited idiosyncrasies diminishes. Second, we show that when the classifier is trained on two Afro-Asiatic languages and tested on the same 22 Indo-European languages (or vice versa), its performance is still significantly above the chance baseline, thus showing that universal factors exist and, moreover, can be captured by word embeddings. When the classifier is tested across families and on inanimate nouns only, the performance is still above baseline, indicating that the universal factors are not limited to biological sex.</abstract>
      <url hash="65b7a793">2020.conll-1.20</url>
      <doi>10.18653/v1/2020.conll-1.20</doi>
      <bibkey>veeman-etal-2020-cross</bibkey>
    </paper>
    <paper id="21">
      <title>Modelling Lexical Ambiguity with Density Matrices</title>
      <author><first>Francois</first><last>Meyer</last></author>
      <author><first>Martha</first><last>Lewis</last></author>
      <pages>276&#8211;290</pages>
      <abstract>Words can have multiple senses. Compositional distributional models of meaning have been argued to deal well with finer shades of meaning variation known as polysemy, but are not so well equipped to handle word senses that are etymologically unrelated, or homonymy. Moving from vectors to density matrices allows us to encode a probability distribution over different senses of a word, and can also be accommodated within a compositional distributional model of meaning. In this paper we present three new neural models for learning density matrices from a corpus, and test their ability to discriminate between word senses on a range of compositional datasets. When paired with a particular composition method, our best model outperforms existing vector-based compositional models as well as strong sentence encoders.</abstract>
      <url hash="a6b0e982">2020.conll-1.21</url>
      <attachment type="OptionalSupplementaryMaterial" hash="1cb732f8">2020.conll-1.21.OptionalSupplementaryMaterial.zip</attachment>
      <doi>10.18653/v1/2020.conll-1.21</doi>
      <bibkey>meyer-lewis-2020-modelling</bibkey>
    </paper>
    <paper id="25">
      <title>Word Representations Concentrate and This is Good News!</title>
      <author><first>Romain</first><last>Couillet</last></author>
      <author><first>Yagmur Gizem</first><last>Cinar</last></author>
      <author><first>Eric</first><last>Gaussier</last></author>
      <author><first>Muhammad</first><last>Imran</last></author>
      <pages>325&#8211;334</pages>
      <abstract>This article establishes that, unlike the legacy tf*idf representation, recent natural language representations (word embedding vectors) tend to exhibit a so-called <i>concentration of measure phenomenon</i>, in the sense that, as the representation size <tex-math>p</tex-math> and database size <tex-math>n</tex-math> are both large, their behavior is similar to that of large dimensional Gaussian random vectors. This phenomenon may have important consequences as machine learning algorithms for natural language data could be amenable to improvement, thereby providing new theoretical insights into the field of natural language processing.</abstract>
      <url hash="559aa2de">2020.conll-1.25</url>
      <doi>10.18653/v1/2020.conll-1.25</doi>
      <bibkey>couillet-etal-2020-word</bibkey>
      <pwccode url="https://github.com/ygcinar/nlp-concentration" additional="false">ygcinar/nlp-concentration</pwccode>
    </paper>
    <paper id="26">
      <title>&#8220;<fixed-case>L</fixed-case>az<fixed-case>I</fixed-case>mpa&#8221;: Lazy and Impatient neural agents learn to communicate efficiently</title>
      <author><first>Mathieu</first><last>Rita</last></author>
      <author><first>Rahma</first><last>Chaabouni</last></author>
      <author><first>Emmanuel</first><last>Dupoux</last></author>
      <pages>335&#8211;343</pages>
      <abstract>Previous work has shown that artificial neural agents naturally develop surprisingly non-efficient codes. This is illustrated by the fact that in a referential game involving a speaker and a listener neural networks optimizing accurate transmission over a discrete channel, the emergent messages fail to achieve an optimal length. Furthermore, frequent messages tend to be longer than infrequent ones, a pattern contrary to the Zipf Law of Abbreviation (ZLA) observed in all natural languages. Here, we show that near-optimal and ZLA-compatible messages can emerge, but only if both the speaker and the listener are modified. We hence introduce a new communication system, &#8220;LazImpa&#8221;, where the speaker is made increasingly lazy, i.e., avoids long messages, and the listener impatient, i.e., seeks to guess the intended content as soon as possible.</abstract>
      <url hash="38192e0a">2020.conll-1.26</url>
      <attachment type="OptionalSupplementaryMaterial" hash="df2dd4d9">2020.conll-1.26.OptionalSupplementaryMaterial.zip</attachment>
      <doi>10.18653/v1/2020.conll-1.26</doi>
      <bibkey>rita-etal-2020-lazimpa</bibkey>
    </paper>
    <paper id="36">
      <title>Analysing Word Representation from the Input and Output Embeddings in Neural Network Language Models</title>
      <author><first>Steven</first><last>Derby</last></author>
      <author><first>Paul</first><last>Miller</last></author>
      <author><first>Barry</first><last>Devereux</last></author>
      <pages>442&#8211;454</pages>
      <abstract>Researchers have recently demonstrated that tying the neural weights between the input look-up table and the output classification layer can improve training and lower perplexity on sequence learning tasks such as language modelling. Such a procedure is possible due to the design of the softmax classification layer, which previous work has shown to comprise a viable set of semantic representations for the model vocabulary, and these these output embeddings are known to perform well on word similarity benchmarks. In this paper, we make meaningful comparisons between the input and output embeddings and other SOTA distributional models to gain a better understanding of the types of information they represent. We also construct a new set of word embeddings using the output embeddings to create locally-optimal approximations for the intermediate representations from the language model. These locally-optimal embeddings demonstrate excellent performance across all our evaluations.</abstract>
      <url hash="ecbb9454">2020.conll-1.36</url>
      <attachment type="OptionalSupplementaryMaterial" hash="3590f41e">2020.conll-1.36.OptionalSupplementaryMaterial.pdf</attachment>
      <doi>10.18653/v1/2020.conll-1.36</doi>
      <bibkey>derby-etal-2020-analysing</bibkey>
      <pwccode url="https://github.com/stevend94/conll2020" additional="false">stevend94/conll2020</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/mpqa-opinion-corpus">MPQA Opinion Corpus</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/penn-treebank">Penn Treebank</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="40">
      <title>Don&#8217;t Parse, Insert: Multilingual Semantic Parsing with Insertion Based Decoding</title>
      <author><first>Qile</first><last>Zhu</last></author>
      <author><first>Haidar</first><last>Khan</last></author>
      <author><first>Saleh</first><last>Soltan</last></author>
      <author><first>Stephen</first><last>Rawls</last></author>
      <author><first>Wael</first><last>Hamza</last></author>
      <pages>496&#8211;506</pages>
      <abstract>Semantic parsing is one of the key components of natural language understanding systems. A successful parse transforms an input utterance to an action that is easily understood by the system. Many algorithms have been proposed to solve this problem, from conventional rule-based or statistical slot-filling systems to shift-reduce based neural parsers. For complex parsing tasks, the state-of-the-art method is based on an autoregressive sequence to sequence model that generates the parse directly. This model is slow at inference time, generating parses in O(n) decoding steps (n is the length of the target sequence). In addition, we demonstrate that this method performs poorly in zero-shot cross-lingual transfer learning settings. In this paper, we propose a non-autoregressive parser which is based on the insertion transformer to overcome these two issues. Our approach 1) speeds up decoding by 3x while outperforming the autoregressive model and 2) significantly improves cross-lingual transfer in the low-resource setting by 37% compared to autoregressive baseline. We test our approach on three wellknown monolingual datasets: ATIS, SNIPS and TOP. For cross-lingual semantic parsing, we use the MultiATIS++ and the multilingual TOP datasets.</abstract>
      <url hash="aba91e7a">2020.conll-1.40</url>
      <doi>10.18653/v1/2020.conll-1.40</doi>
      <bibkey>zhu-etal-2020-dont</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/snips">SNIPS</pwcdataset>
    </paper>
    <paper id="49">
      <title>Cloze Distillation: Improving Neural Language Models with Human Next-Word Prediction</title>
      <author><first>Tiwalayo</first><last>Eisape</last></author>
      <author><first>Noga</first><last>Zaslavsky</last></author>
      <author><first>Roger</first><last>Levy</last></author>
      <pages>609&#8211;619</pages>
      <abstract>Contemporary autoregressive language models (LMs) trained purely on corpus data have been shown to capture numerous features of human incremental processing. However, past work has also suggested dissociations between corpus probabilities and human next-word predictions. Here we evaluate several state-of-the-art language models for their match to human next-word predictions and to reading time behavior from eye movements. We then propose a novel method for distilling the linguistic information implicit in human linguistic predictions into pre-trained LMs: Cloze Distillation. We apply this method to a baseline neural LM and show potential improvement in reading time prediction and generalization to held-out human cloze data.</abstract>
      <url hash="3828b793">2020.conll-1.49</url>
      <doi>10.18653/v1/2020.conll-1.49</doi>
      <bibkey>eisape-etal-2020-cloze</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/wikitext-103">WikiText-103</pwcdataset>
    </paper>
    <paper id="52">
      <title>From Dataset Recycling to Multi-Property Extraction and Beyond</title>
      <author><first>Tomasz</first><last>Dwojak</last></author>
      <author><first>Micha&#322;</first><last>Pietruszka</last></author>
      <author><first>&#321;ukasz</first><last>Borchmann</last></author>
      <author><first>Jakub</first><last>Ch&#322;&#281;dowski</last></author>
      <author><first>Filip</first><last>Grali&#324;ski</last></author>
      <pages>641&#8211;651</pages>
      <abstract>This paper investigates various Transformer architectures on the WikiReading Information Extraction and Machine Reading Comprehension dataset. The proposed dual-source model outperforms the current state-of-the-art by a large margin. Next, we introduce WikiReading Recycled - a newly developed public dataset, and the task of multiple-property extraction. It uses the same data as WikiReading but does not inherit its predecessor&#8217;s identified disadvantages. In addition, we provide a human-annotated test set with diagnostic subsets for a detailed analysis of model performance.</abstract>
      <url hash="ed5e0dcc">2020.conll-1.52</url>
      <doi>10.18653/v1/2020.conll-1.52</doi>
      <bibkey>dwojak-etal-2020-dataset</bibkey>
      <pwccode url="https://github.com/applicaai/multi-property-extraction" additional="false">applicaai/multi-property-extraction</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/wikireading-recycled">WikiReading Recycled</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikireading">WikiReading</pwcdataset>
    </paper>
    </volume>
  <volume id="shared" ingest-date="2020-11-05">
    <meta>
      <booktitle>Proceedings of the CoNLL 2020 Shared Task: Cross-Framework Meaning Representation Parsing</booktitle>
      <editor><first>Stephan</first><last>Oepen</last></editor>
      <editor><first>Omri</first><last>Abend</last></editor>
      <editor><first>Lasha</first><last>Abzianidze</last></editor>
      <editor><first>Johan</first><last>Bos</last></editor>
      <editor><first>Jan</first><last>Haji&#269;</last></editor>
      <editor><first>Daniel</first><last>Hershcovich</last></editor>
      <editor><first>Bin</first><last>Li</last></editor>
      <editor><first>Tim</first><last>O'Gorman</last></editor>
      <editor><first>Nianwen</first><last>Xue</last></editor>
      <editor><first>Daniel</first><last>Zeman</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Online</address>
      <month>November</month>
      <year>2020</year>
    </meta>
    <frontmatter>
      <url hash="3e727ac8">2020.conll-shared.0</url>
      <bibkey>conll-2020-conll</bibkey>
    </frontmatter>
    <paper id="4">
      <title>Hitachi at <fixed-case>MRP</fixed-case> 2020: Text-to-Graph-Notation Transducer</title>
      <author><first>Hiroaki</first><last>Ozaki</last></author>
      <author><first>Gaku</first><last>Morio</last></author>
      <author><first>Yuta</first><last>Koreeda</last></author>
      <author><first>Terufumi</first><last>Morishita</last></author>
      <author><first>Toshinori</first><last>Miyoshi</last></author>
      <pages>40&#8211;52</pages>
      <abstract>This paper presents our proposed parser for the shared task on Meaning Representation Parsing (MRP 2020) at CoNLL, where participant systems were required to parse five types of graphs in different languages. We propose to unify these tasks as a text-to-graph-notation transduction in which we convert an input text into a graph notation. To this end, we designed a novel Plain Graph Notation (PGN) that handles various graphs universally. Then, our parser predicts a PGN-based sequence by leveraging Transformers and biaffine attentions. Notably, our parser can handle any PGN-formatted graphs with fewer framework-specific modifications. As a result, ensemble versions of the parser tied for 1st place in both cross-framework and cross-lingual tracks.</abstract>
      <url hash="2a4dc896">2020.conll-shared.4</url>
      <doi>10.18653/v1/2020.conll-shared.4</doi>
      <bibkey>ozaki-etal-2020-hitachi</bibkey>
    </paper>
    <paper id="7">
      <title><fixed-case>HUJI</fixed-case>-<fixed-case>KU</fixed-case> at <fixed-case>MRP</fixed-case> 2020: Two Transition-based Neural Parsers</title>
      <author><first>Ofir</first><last>Arviv</last></author>
      <author><first>Ruixiang</first><last>Cui</last></author>
      <author><first>Daniel</first><last>Hershcovich</last></author>
      <pages>73&#8211;82</pages>
      <abstract>This paper describes the HUJI-KU system submission to the shared task on CrossFramework Meaning Representation Parsing (MRP) at the 2020 Conference for Computational Language Learning (CoNLL), employing TUPA and the HIT-SCIR parser, which were, respectively, the baseline system and winning system in the 2019 MRP shared task. Both are transition-based parsers using BERT contextualized embeddings. We generalized TUPA to support the newly-added MRP frameworks and languages, and experimented with multitask learning with the HIT-SCIR parser. We reached 4th place in both the crossframework and cross-lingual tracks.</abstract>
      <url hash="6a26281e">2020.conll-shared.7</url>
      <doi>10.18653/v1/2020.conll-shared.7</doi>
      <bibkey>arviv-etal-2020-huji</bibkey>
    </paper>
    </volume>
</collection>