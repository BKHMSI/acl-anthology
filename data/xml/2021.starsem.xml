<collection id="2021.starsem">
  <volume id="1" ingest-date="2021-07-25">
    <meta>
      <booktitle>Proceedings of *SEM 2021: The Tenth Joint Conference on Lexical and Computational Semantics</booktitle>
      <editor><first>Lun-Wei</first><last>Ku</last></editor>
      <editor><first>Vivi</first><last>Nastase</last></editor>
      <editor><first>Ivan</first><last>Vuli&#263;</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Online</address>
      <month>August</month>
      <year>2021</year>
      <url hash="b7c2739b">2021.starsem-1</url>
    </meta>
    <frontmatter>
      <url hash="2ecd08a5">2021.starsem-1.0</url>
      <bibkey>sem-2021-sem</bibkey>
    </frontmatter>
    <paper id="2">
      <title>Can Transformer Language Models Predict Psychometric Properties?</title>
      <author><first>Antonio</first><last>Laverghetta Jr.</last></author>
      <author><first>Animesh</first><last>Nighojkar</last></author>
      <author><first>Jamshidbek</first><last>Mirzakhalov</last></author>
      <author><first>John</first><last>Licato</last></author>
      <pages>12&#8211;25</pages>
      <abstract>Transformer-based language models (LMs) continue to advance state-of-the-art performance on NLP benchmark tasks, including tasks designed to mimic human-inspired &#8220;commonsense&#8221; competencies. To better understand the degree to which LMs can be said to have certain linguistic reasoning skills, researchers are beginning to adapt the tools and concepts of the field of psychometrics. But to what extent can the benefits flow in the other direction? I.e., can LMs be of use in predicting what the psychometric properties of test items will be when those items are given to human participants? We gather responses from numerous human participants and LMs (transformer- and non-transformer-based) on a broad diagnostic test of linguistic competencies. We then use the responses to calculate standard psychometric properties of the items in the diagnostic test, using the human responses and the LM responses separately. We then determine how well these two sets of predictions match. We find cases in which transformer-based LMs predict psychometric properties consistently well in certain categories but consistently poorly in others, thus providing new insights into fundamental similarities and differences between human and LM reasoning.</abstract>
      <url hash="c9268154">2021.starsem-1.2</url>
      <doi>10.18653/v1/2021.starsem-1.2</doi>
      <bibkey>laverghetta-jr-etal-2021-transformer</bibkey>
      <pwccode url="https://github.com/Advancing-Machine-Human-Reasoning-Lab/transformer-psychometrics" additional="false">Advancing-Machine-Human-Reasoning-Lab/transformer-psychometrics</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/anli">ANLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/chaosnli">ChaosNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/taxinli">TaxiNLI</pwcdataset>
    </paper>
    <paper id="4">
      <title>A Study on Using Semantic Word Associations to Predict the Success of a Novel</title>
      <author><first>Syeda</first><last>Jannatus Saba</last></author>
      <author><first>Biddut Sarker</first><last>Bijoy</last></author>
      <author><first>Henry</first><last>Gorelick</last></author>
      <author><first>Sabir</first><last>Ismail</last></author>
      <author><first>Md Saiful</first><last>Islam</last></author>
      <author><first>Mohammad Ruhul</first><last>Amin</last></author>
      <pages>38&#8211;51</pages>
      <abstract>Many new books get published every year, and only a fraction of them become popular among the readers. So the prediction of a book success can be a very useful parameter for publishers to make a reliable decision. This article presents the study of semantic word associations using the word embedding of book content for a set of Roget&#8217;s thesaurus concepts for book success prediction. In this work, we discuss the method to represent a book as a spectrum of concepts based on the association score between its content embedding and a global embedding (i.e. fastText) for a set of semantically linked word clusters. We show that the semantic word associations outperform the previous methods for book success prediction. In addition, we present that semantic word associations also provide better results than using features like the frequency of word groups in Roget&#8217;s thesaurus, LIWC (a popular tool for linguistic inquiry and word count), NRC (word association emotion lexicon), and part of speech (PoS). Our study reports that concept associations based on Roget&#8217;s Thesaurus using word embedding of individual novel resulted in the state-of-the-art performance of 0.89 average weighted F1-score for book success prediction. Finally, we present a set of dominant themes that contribute towards the popularity of a book for a specific genre.</abstract>
      <url hash="c3d53777">2021.starsem-1.4</url>
      <attachment type="OptionalSupplementaryMaterial" hash="4dd3ad88">2021.starsem-1.4.OptionalSupplementaryMaterial.zip</attachment>
      <doi>10.18653/v1/2021.starsem-1.4</doi>
      <bibkey>jannatus-saba-etal-2021-study</bibkey>
    </paper>
    <paper id="9">
      <title><fixed-case>P</fixed-case>ars<fixed-case>FEVER</fixed-case>: a Dataset for <fixed-case>F</fixed-case>arsi Fact Extraction and Verification</title>
      <author><first>Majid</first><last>Zarharan</last></author>
      <author><first>Mahsa</first><last>Ghaderan</last></author>
      <author><first>Amin</first><last>Pourdabiri</last></author>
      <author><first>Zahra</first><last>Sayedi</last></author>
      <author><first>Behrouz</first><last>Minaei-Bidgoli</last></author>
      <author><first>Sauleh</first><last>Eetemadi</last></author>
      <author><first>Mohammad Taher</first><last>Pilehvar</last></author>
      <pages>99&#8211;104</pages>
      <abstract>Training and evaluation of automatic fact extraction and verification techniques require large amounts of annotated data which might not be available for low-resource languages. This paper presents ParsFEVER: the first publicly available Farsi dataset for fact extraction and verification. We adopt the construction procedure of the standard English dataset for the task, i.e., FEVER, and improve it for the case of low-resource languages. Specifically, claims are extracted from sentences that are carefully selected to be more informative. The dataset comprises nearly 23K manually-annotated claims. Over 65% of the claims in ParsFEVER are many-hop (require evidence from multiple sources), making the dataset a challenging benchmark (only 13% of the claims in FEVER are many-hop). Also, despite having a smaller training set (around one-ninth of that in Fever), a model trained on ParsFEVER attains similar downstream performance, indicating the quality of the dataset. We release the dataset and the annotation guidelines at https://github.com/Zarharan/ParsFEVER.</abstract>
      <url hash="8f67e828">2021.starsem-1.9</url>
      <attachment type="OptionalSupplementaryMaterial" hash="da2a6bcd">2021.starsem-1.9.OptionalSupplementaryMaterial.zip</attachment>
      <doi>10.18653/v1/2021.starsem-1.9</doi>
      <bibkey>zarharan-etal-2021-parsfever</bibkey>
      <pwccode url="https://github.com/zarharan/parsfever" additional="false">zarharan/parsfever</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/fever">FEVER</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/hover">HoVer</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/liar">LIAR</pwcdataset>
    </paper>
    <paper id="10">
      <title><fixed-case>B</fixed-case>i<fixed-case>Q</fixed-case>u<fixed-case>AD</fixed-case>: Towards <fixed-case>QA</fixed-case> based on deeper text understanding</title>
      <author><first>Frank</first><last>Grimm</last></author>
      <author><first>Philipp</first><last>Cimiano</last></author>
      <pages>105&#8211;115</pages>
      <abstract>Recent question answering and machine reading benchmarks frequently reduce the task to one of pinpointing spans within a certain text passage that answers the given question. Typically, these systems are not required to actually understand the text on a deeper level that allows for more complex reasoning on the information contained. We introduce a new dataset called BiQuAD that requires deeper comprehension in order to answer questions in both extractive and deductive fashion. The dataset consist of 4,190 closed-domain texts and a total of 99,149 question-answer pairs. The texts are synthetically generated soccer match reports that verbalize the main events of each match. All texts are accompanied by a structured Datalog program that represents a (logical) model of its information. We show that state-of-the-art QA models do not perform well on the challenging long form contexts and reasoning requirements posed by the dataset. In particular, transformer based state-of-the-art models achieve F1-scores of only 39.0. We demonstrate how these synthetic datasets align structured knowledge with natural text and aid model introspection when approaching complex text understanding.</abstract>
      <url hash="1cb2ee65">2021.starsem-1.10</url>
      <doi>10.18653/v1/2021.starsem-1.10</doi>
      <bibkey>grimm-cimiano-2021-biquad</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/drop">DROP</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/medhop">MedHop</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/newsqa">NewsQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/openbookqa">OpenBookQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/qasc">QASC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/race">RACE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/triviaqa">TriviaQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikihop">WikiHop</pwcdataset>
    </paper>
    <paper id="11">
      <title>Evaluating <fixed-case>U</fixed-case>niversal <fixed-case>D</fixed-case>ependency Parser Recovery of Predicate Argument Structure via <fixed-case>C</fixed-case>omp<fixed-case>C</fixed-case>hain Analysis</title>
      <author><first>Sagar</first><last>Indurkhya</last></author>
      <author><first>Beracah</first><last>Yankama</last></author>
      <author><first>Robert C.</first><last>Berwick</last></author>
      <pages>116&#8211;128</pages>
      <abstract>Accurate recovery of predicate-argument structure from a Universal Dependency (UD) parse is central to downstream tasks such as extraction of semantic roles or event representations. This study introduces compchains, a categorization of the hierarchy of predicate dependency relations present within a UD parse. Accuracy of compchain classification serves as a proxy for measuring accurate recovery of predicate-argument structure from sentences with embedding. We analyzed the distribution of compchains in three UD English treebanks, EWT, GUM and LinES, revealing that these treebanks are sparse with respect to sentences with predicate-argument structure that includes predicate-argument embedding. We evaluated the CoNLL 2018 Shared Task UDPipe (v1.2) baseline (dependency parsing) models as compchain classifiers for the EWT, GUMS and LinES UD treebanks. Our results indicate that these three baseline models exhibit poorer performance on sentences with predicate-argument structure with more than one level of embedding; we used compchains to characterize the errors made by these parsers and present examples of erroneous parses produced by the parser that were identified using compchains. We also analyzed the distribution of compchains in 58 non-English UD treebanks and then used compchains to evaluate the CoNLL&#8217;18 Shared Task baseline model for each of these treebanks. Our analysis shows that performance with respect to compchain classification is only weakly correlated with the official evaluation metrics (LAS, MLAS and BLEX). We identify gaps in the distribution of compchains in several of the UD treebanks, thus providing a roadmap for how these treebanks may be supplemented. We conclude by discussing how compchains provide a new perspective on the sparsity of training data for UD parsers, as well as the accuracy of the resulting UD parses.</abstract>
      <url hash="a48eab5f">2021.starsem-1.11</url>
      <doi>10.18653/v1/2021.starsem-1.11</doi>
      <bibkey>indurkhya-etal-2021-evaluating</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/universal-dependencies">Universal Dependencies</pwcdataset>
    </paper>
    <paper id="14">
      <title>Disentangling Online Chats with <fixed-case>DAG</fixed-case>-structured <fixed-case>LSTM</fixed-case>s</title>
      <author><first>Duccio</first><last>Pappadopulo</last></author>
      <author><first>Lisa</first><last>Bauer</last></author>
      <author><first>Marco</first><last>Farina</last></author>
      <author><first>Ozan</first><last>&#304;rsoy</last></author>
      <author><first>Mohit</first><last>Bansal</last></author>
      <pages>152&#8211;159</pages>
      <abstract>Many modern messaging systems allow fast and synchronous textual communication among many users. The resulting sequence of messages hides a more complicated structure in which independent sub-conversations are interwoven with one another. This poses a challenge for any task aiming to understand the content of the chat logs or gather information from them. The ability to disentangle these conversations is then tantamount to the success of many downstream tasks such as summarization and question answering. Structured information accompanying the text such as user turn, user mentions, timestamps, is used as a cue by the participants themselves who need to follow the conversation and has been shown to be important for disentanglement. DAG-LSTMs, a generalization of Tree-LSTMs that can handle directed acyclic dependencies, are a natural way to incorporate such information and its non-sequential nature. In this paper, we apply DAG-LSTMs to the conversation disentanglement task. We perform our experiments on the Ubuntu IRC dataset. We show that the novel model we propose achieves state of the art status on the task of recovering reply-to relations and it is competitive on other disentanglement metrics.</abstract>
      <url hash="fb1be83f">2021.starsem-1.14</url>
      <doi>10.18653/v1/2021.starsem-1.14</doi>
      <bibkey>pappadopulo-etal-2021-disentangling</bibkey>
    </paper>
    <paper id="15">
      <title>Toward Diverse Precondition Generation</title>
      <author><first>Heeyoung</first><last>Kwon</last></author>
      <author><first>Nathanael</first><last>Chambers</last></author>
      <author><first>Niranjan</first><last>Balasubramanian</last></author>
      <pages>160&#8211;172</pages>
      <abstract>A typical goal for language understanding is to logically connect the events of a discourse, but often connective events are not described due to their commonsense nature. In order to address this deficit, we focus here on generating precondition events. Precondition generation can be framed as a sequence-to-sequence problem: given a target event, generate a possible precondition. However, in most real-world scenarios, an event can have several preconditions, which is not always suitable for standard seq2seq frameworks. We propose DiP, the Diverse Precondition generation system that can generate unique and diverse preconditions. DiP consists of three stages of the generative process &#8211; an event sampler, a candidate generator, and a post-processor. The event sampler provides control codes (precondition triggers) which the candidate generator uses to focus its generation. Post-processing further improves the results through re-ranking and filtering. Unlike other conditional generation systems, DiP automatically generates control codes without training on diverse examples. Analysis reveals that DiP improves the diversity of preconditions significantly compared to a beam search baseline. Also, manual evaluation shows that DiP generates more preconditions than a strong nucleus sampling baseline.</abstract>
      <url hash="0afad549">2021.starsem-1.15</url>
      <doi>10.18653/v1/2021.starsem-1.15</doi>
      <bibkey>kwon-etal-2021-toward</bibkey>
    </paper>
    <paper id="19">
      <title>Incorporating <fixed-case>EDS</fixed-case> Graph for <fixed-case>AMR</fixed-case> Parsing</title>
      <author><first>Ziyi</first><last>Shou</last></author>
      <author><first>Fangzhen</first><last>Lin</last></author>
      <pages>202&#8211;211</pages>
      <abstract>AMR (Abstract Meaning Representation) and EDS (Elementary Dependency Structures) are two popular meaning representations in NLP/NLU. AMR is more abstract and conceptual, while EDS is more low level, closer to the lexical structures of the given sentences. It is thus not surprising that EDS parsing is easier than AMR parsing. In this work, we consider using information from EDS parsing to help improve the performance of AMR parsing. We adopt a transition-based parser and propose to add EDS graphs as additional semantic features using a graph encoder composed of LSTM layer and GCN layer. Our experimental results show that the additional information from EDS parsing indeed gives a boost to the performance of the base AMR parser used in our experiments.</abstract>
      <url hash="92dfd058">2021.starsem-1.19</url>
      <doi>10.18653/v1/2021.starsem-1.19</doi>
      <bibkey>shou-lin-2021-incorporating</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/penn-treebank">Penn Treebank</pwcdataset>
    </paper>
    <paper id="21">
      <title>Neural Metaphor Detection with Visibility Embeddings</title>
      <author><first>Gitit</first><last>Kehat</last></author>
      <author><first>James</first><last>Pustejovsky</last></author>
      <pages>222&#8211;228</pages>
      <abstract>We present new results for the problem of sequence metaphor labeling, using the recently developed Visibility Embeddings. We show that concatenating such embeddings to the input of a BiLSTM obtains consistent and significant improvements at almost no cost, and we present further improved results when visibility embeddings are combined with BERT.</abstract>
      <url hash="90afa474">2021.starsem-1.21</url>
      <doi>10.18653/v1/2021.starsem-1.21</doi>
      <bibkey>kehat-pustejovsky-2021-neural</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/visual-genome">Visual Genome</pwcdataset>
    </paper>
    <paper id="22">
      <title>Inducing Language-Agnostic Multilingual Representations</title>
      <author><first>Wei</first><last>Zhao</last></author>
      <author><first>Steffen</first><last>Eger</last></author>
      <author><first>Johannes</first><last>Bjerva</last></author>
      <author><first>Isabelle</first><last>Augenstein</last></author>
      <pages>229&#8211;240</pages>
      <abstract>Cross-lingual representations have the potential to make NLP techniques available to the vast majority of languages in the world. However, they currently require large pretraining corpora or access to typologically similar languages. In this work, we address these obstacles by removing language identity signals from multilingual embeddings. We examine three approaches for this: (i) re-aligning the vector spaces of target languages (all together) to a pivot source language; (ii) removing language-specific means and variances, which yields better discriminativeness of embeddings as a by-product; and (iii) increasing input similarity across languages by removing morphological contractions and sentence reordering. We evaluate on XNLI and reference-free MT evaluation across 19 typologically diverse languages. Our findings expose the limitations of these approaches&#8212;unlike vector normalization, vector space re-alignment and text normalization do not achieve consistent gains across encoders and languages. Due to the approaches&#8217; additive effects, their combination decreases the cross-lingual transfer gap by 8.9 points (m-BERT) and 18.2 points (XLM-R) on average across all tasks and languages, however.</abstract>
      <url hash="b003b43c">2021.starsem-1.22</url>
      <attachment type="OptionalSupplementaryMaterial" hash="4468620b">2021.starsem-1.22.OptionalSupplementaryMaterial.pdf</attachment>
      <doi>10.18653/v1/2021.starsem-1.22</doi>
      <bibkey>zhao-etal-2021-inducing</bibkey>
      <pwccode url="https://github.com/AIPHES/Language-Agnostic-Contextualized-Encoders" additional="false">AIPHES/Language-Agnostic-Contextualized-Encoders</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/xnli">XNLI</pwcdataset>
    </paper>
    <paper id="25">
      <title>Spurious Correlations in Cross-Topic Argument Mining</title>
      <author><first>Terne Sasha</first><last>Thorn Jakobsen</last></author>
      <author><first>Maria</first><last>Barrett</last></author>
      <author><first>Anders</first><last>S&#248;gaard</last></author>
      <pages>263&#8211;277</pages>
      <abstract>Recent work in cross-topic argument mining attempts to learn models that generalise across topics rather than merely relying on within-topic spurious correlations. We examine the effectiveness of this approach by analysing the output of single-task and multi-task models for cross-topic argument mining, through a combination of linear approximations of their decision boundaries, manual feature grouping, challenge examples, and ablations across the input vocabulary. Surprisingly, we show that cross-topic models still rely mostly on spurious correlations and only generalise within closely related topics, e.g., a model trained only on closed-class words and a few common open-class words outperforms a state-of-the-art cross-topic model on distant target topics.</abstract>
      <url hash="e26daf8d">2021.starsem-1.25</url>
      <doi>10.18653/v1/2021.starsem-1.25</doi>
      <bibkey>thorn-jakobsen-etal-2021-spurious</bibkey>
      <pwccode url="https://github.com/terne/spurious_correlations_in_argmin" additional="false">terne/spurious_correlations_in_argmin</pwccode>
    </paper>
    <paper id="27">
      <title>Overcoming Poor Word Embeddings with Word Definitions</title>
      <author><first>Christopher</first><last>Malon</last></author>
      <pages>288&#8211;293</pages>
      <abstract>Modern natural language understanding models depend on pretrained subword embeddings, but applications may need to reason about words that were never or rarely seen during pretraining. We show that examples that depend critically on a rarer word are more challenging for natural language inference models. Then we explore how a model could learn to use definitions, provided in natural text, to overcome this handicap. Our model&#8217;s understanding of a definition is usually weaker than a well-modeled word embedding, but it recovers most of the performance gap from using a completely untrained word.</abstract>
      <url hash="a823cd7c">2021.starsem-1.27</url>
      <doi>10.18653/v1/2021.starsem-1.27</doi>
      <bibkey>malon-2021-overcoming</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
    </paper>
    </volume>
</collection>