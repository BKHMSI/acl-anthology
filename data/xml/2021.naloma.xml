<collection id="2021.naloma">
  <volume id="1" ingest-date="2021-10-27">
    <meta>
      <booktitle>Proceedings of the 1st and 2nd Workshops on Natural Logic Meets Machine Learning (NALOMA)</booktitle>
      <editor><first>Aikaterini-Lida</first><last>Kalouli</last></editor>
      <editor><first>Lawrence S.</first><last>Moss</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Groningen, the Netherlands (online)</address>
      <month>June</month>
      <year>2021</year>
      <url hash="68bf91c3">2021.naloma-1</url>
    </meta>
    <frontmatter>
      <url hash="b8097cd2">2021.naloma-1.0</url>
      <bibkey>naloma-2021-workshops</bibkey>
    </frontmatter>
    <paper id="3">
      <title>Attentive Tree-structured Network for Monotonicity Reasoning</title>
      <author><first>Zeming</first><last>Chen</last></author>
      <pages>12&#8211;21</pages>
      <abstract>Many state-of-art neural models designed for monotonicity reasoning perform poorly on downward inference. To address this shortcoming, we developed an attentive tree-structured neural network. It consists of a tree-based long-short-term-memory network (Tree-LSTM) with soft attention. It is designed to model the syntactic parse tree information from the sentence pair of a reasoning task. A self-attentive aggregator is used for aligning the representations of the premise and the hypothesis. We present our model and evaluate it using the Monotonicity Entailment Dataset (MED). We show and attempt to explain that our model outperforms existing models on MED.</abstract>
      <url hash="cfbaf3f9">2021.naloma-1.3</url>
      <bibkey>chen-2021-attentive</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/help">HELP</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/med">MED</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
    </paper>
    <paper id="4">
      <title>Transferring Representations of Logical Connectives</title>
      <author><first>Aaron</first><last>Traylor</last></author>
      <author><first>Ellie</first><last>Pavlick</last></author>
      <author><first>Roman</first><last>Feiman</last></author>
      <pages>22&#8211;25</pages>
      <abstract>In modern natural language processing pipelines, it is common practice to &#8220;pretrain&#8221; a generative language model on a large corpus of text, and then to &#8220;finetune&#8221; the created representations by continuing to train them on a discriminative textual inference task. However, it is not immediately clear whether the logical meaning necessary to model logical entailment is captured by language models in this paradigm. We examine this pretrain-finetune recipe with language models trained on a synthetic propositional language entailment task, and present results on test sets probing models&#8217; knowledge of axioms of first order logic.</abstract>
      <url hash="8ba0ccc2">2021.naloma-1.4</url>
      <bibkey>traylor-etal-2021-transferring</bibkey>
    </paper>
    <paper id="5">
      <title>Monotonic Inference for Underspecified Episodic Logic</title>
      <author><first>Gene</first><last>Kim</last></author>
      <author><first>Mandar</first><last>Juvekar</last></author>
      <author><first>Lenhart</first><last>Schubert</last></author>
      <pages>26&#8211;40</pages>
      <abstract>We present a method of making natural logic inferences from Unscoped Logical Form of Episodic Logic. We establish a correspondence between inference rules of scope resolved Episodic Logic and the natural logic treatment by S&#225;nchez Valencia (1991a), and hence demonstrate the ability to handle foundational natural logic inferences from prior literature as well as more general nested monotonicity inferences.</abstract>
      <url hash="7d7fd64d">2021.naloma-1.5</url>
      <bibkey>kim-etal-2021-monotonic</bibkey>
    </paper>
    <paper id="7">
      <title><fixed-case>B</fixed-case>ayesian Classification and Inference in a Probabilistic Type Theory with Records</title>
      <author><first>Staffan</first><last>Larsson</last></author>
      <author><first>Robin</first><last>Cooper</last></author>
      <pages>51&#8211;59</pages>
      <abstract>We propose a probabilistic account of semantic inference and classification formulated in terms of probabilistic type theory with records, building on Cooper et. al. (2014) and Cooper et. al. (2015). We suggest probabilistic type theoretic formulations of Naive Bayes Classifiers and Bayesian Networks. A central element of these constructions is a type-theoretic version of a random variable. We illustrate this account with a simple language game combining probabilistic classification of perceptual input with probabilistic (semantic) inference.</abstract>
      <url hash="6d329532">2021.naloma-1.7</url>
      <bibkey>larsson-cooper-2021-bayesian</bibkey>
    </paper>
    </volume>
</collection>