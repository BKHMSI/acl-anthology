<collection id="2021.argmining">
  <volume id="1" ingest-date="2021-10-28">
    <meta>
      <booktitle>Proceedings of the 8th Workshop on Argument Mining</booktitle>
      <editor><first>Khalid</first><last>Al-Khatib</last></editor>
      <editor><first>Yufang</first><last>Hou</last></editor>
      <editor><first>Manfred</first><last>Stede</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Punta Cana, Dominican Republic</address>
      <month>November</month>
      <year>2021</year>
    </meta>
    <frontmatter>
      <url hash="f5428747">2021.argmining-1.0</url>
      <bibkey>argmining-2021-argument</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Argument Mining on <fixed-case>T</fixed-case>witter: A Case Study on the Planned Parenthood Debate</title>
      <author><first>Muhammad Mahad Afzal</first><last>Bhatti</last></author>
      <author><first>Ahsan Suheer</first><last>Ahmad</last></author>
      <author><first>Joonsuk</first><last>Park</last></author>
      <pages>1&#8211;11</pages>
      <abstract>Twitter is a popular platform to share opinions and claims, which may be accompanied by the underlying rationale. Such information can be invaluable to policy makers, marketers and social scientists, to name a few. However, the effort to mine arguments on Twitter has been limited, mainly because a tweet is typically too short to contain an argument &#8212; both a claim and a premise. In this paper, we propose a novel problem formulation to mine arguments from Twitter: We formulate argument mining on Twitter as a text classification task to identify tweets that serve as premises for a hashtag that represents a claim of interest. To demonstrate the efficacy of this formulation, we mine arguments for and against funding Planned Parenthood expressed in tweets. We first present a new dataset of 24,100 tweets containing hashtag #StandWithPP or #DefundPP, manually labeled as SUPPORT WITH REASON, SUPPORT WITHOUT REASON, and NO EXPLICIT SUPPORT. We then train classifiers to determine the types of tweets, achieving the best performance of 71% F1. Our results manifest claim-specific keywords as the most informative features, which in turn reveal prominent arguments for and against funding Planned Parenthood.</abstract>
      <url hash="903d86b5">2021.argmining-1.1</url>
      <bibkey>bhatti-etal-2021-argument</bibkey>
      <doi>10.18653/v1/2021.argmining-1.1</doi>
    </paper>
    <paper id="3">
      <title>Explainable Unsupervised Argument Similarity Rating with <fixed-case>A</fixed-case>bstract <fixed-case>M</fixed-case>eaning <fixed-case>R</fixed-case>epresentation and Conclusion Generation</title>
      <author><first>Juri</first><last>Opitz</last></author>
      <author><first>Philipp</first><last>Heinisch</last></author>
      <author><first>Philipp</first><last>Wiesenbach</last></author>
      <author><first>Philipp</first><last>Cimiano</last></author>
      <author><first>Anette</first><last>Frank</last></author>
      <pages>24&#8211;35</pages>
      <abstract>When assessing the similarity of arguments, researchers typically use approaches that do not provide interpretable evidence or justifications for their ratings. Hence, the features that determine argument similarity remain elusive. We address this issue by introducing <i>novel argument similarity metrics</i> that aim at high performance and explainability. We show that Abstract Meaning Representation (AMR) graphs can be useful for representing arguments, and that novel AMR graph metrics can offer explanations for argument similarity ratings. We start from the hypothesis that <i>similar premises</i> often lead to <i>similar conclusions</i>&#8212;and extend an approach for <i>AMR-based argument similarity rating</i> by estimating, in addition, the similarity of <i>conclusions</i> that we automatically infer from the arguments used as premises. We show that AMR similarity metrics make argument similarity judgements more <i>interpretable</i> and may even support <i>argument quality judgements</i>. Our approach provides significant performance improvements over strong baselines in a <i>fully unsupervised</i> setting. Finally, we make first steps to address the problem of reference-less evaluation of argumentative conclusion generations.</abstract>
      <url hash="81c5accd">2021.argmining-1.3</url>
      <bibkey>opitz-etal-2021-explainable</bibkey>
      <doi>10.18653/v1/2021.argmining-1.3</doi>
      <pwccode url="https://github.com/heidelberg-nlp/amr-argument-sim" additional="false">heidelberg-nlp/amr-argument-sim</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/conceptnet">ConceptNet</pwcdataset>
    </paper>
    <paper id="7">
      <title>Assessing the Sufficiency of Arguments through Conclusion Generation</title>
      <author><first>Timon</first><last>Gurcke</last></author>
      <author><first>Milad</first><last>Alshomary</last></author>
      <author><first>Henning</first><last>Wachsmuth</last></author>
      <pages>67&#8211;77</pages>
      <abstract>The premises of an argument give evidence or other reasons to support a conclusion. However, the amount of support required depends on the generality of a conclusion, the nature of the individual premises, and similar. An argument whose premises make its conclusion rationally worthy to be drawn is called sufficient in argument quality research. Previous work tackled sufficiency assessment as a standard text classification problem, not modeling the inherent relation of premises and conclusion. In this paper, we hypothesize that the conclusion of a sufficient argument can be generated from its premises. To study this hypothesis, we explore the potential of assessing sufficiency based on the output of large-scale pre-trained language models. Our best model variant achieves an F1-score of .885, outperforming the previous state-of-the-art and being on par with human experts. While manual evaluation reveals the quality of the generated conclusions, their impact remains low ultimately.</abstract>
      <url hash="be61af83">2021.argmining-1.7</url>
      <bibkey>gurcke-etal-2021-assessing</bibkey>
      <doi>10.18653/v1/2021.argmining-1.7</doi>
      <pwccode url="https://github.com/webis-de/argmining-21" additional="false">webis-de/argmining-21</pwccode>
    </paper>
    <paper id="11">
      <title><fixed-case>B</fixed-case>ayesian Argumentation-Scheme Networks: <fixed-case>A</fixed-case> Probabilistic Model of Argument Validity Facilitated by Argumentation Schemes</title>
      <author><first>Takahiro</first><last>Kondo</last></author>
      <author><first>Koki</first><last>Washio</last></author>
      <author><first>Katsuhiko</first><last>Hayashi</last></author>
      <author><first>Yusuke</first><last>Miyao</last></author>
      <pages>112&#8211;124</pages>
      <abstract>We propose a methodology for representing the reasoning structure of arguments using Bayesian networks and predicate logic facilitated by argumentation schemes. We express the meaning of text segments using predicate logic and map the boolean values of predicate logic expressions to nodes in a Bayesian network. The reasoning structure among text segments is described with a directed acyclic graph. While our formalism is highly expressive and capable of describing the informal logic of human arguments, it is too open-ended to actually build a network for an argument. It is not at all obvious which segment of argumentative text should be considered as a node in a Bayesian network, and how to decide the dependencies among nodes. To alleviate the difficulty, we provide abstract network fragments, called idioms, which represent typical argument justification patterns derived from argumentation schemes. The network construction process is decomposed into idiom selection, idiom instantiation, and idiom combination. We define 17 idioms in total by referring to argumentation schemes as well as analyzing actual arguments and fitting idioms to them. We also create a dataset consisting of pairs of an argumentative text and a corresponding Bayesian network. Our dataset contains about 2,400 pairs, which is large in the research area of argumentation schemes.</abstract>
      <url hash="4203bb2f">2021.argmining-1.11</url>
      <bibkey>kondo-etal-2021-bayesian</bibkey>
      <doi>10.18653/v1/2021.argmining-1.11</doi>
    </paper>
    <paper id="13">
      <title>Predicting Moderation of Deliberative Arguments: Is Argument Quality the Key?</title>
      <author><first>Neele</first><last>Falk</last></author>
      <author><first>Iman</first><last>Jundi</last></author>
      <author><first>Eva Maria</first><last>Vecchi</last></author>
      <author><first>Gabriella</first><last>Lapesa</last></author>
      <pages>133&#8211;141</pages>
      <abstract>Human moderation is commonly employed in deliberative contexts (argumentation and discussion targeting a shared decision on an issue relevant to a group, e.g., citizens arguing on how to employ a shared budget). As the scale of discussion enlarges in online settings, the overall discussion quality risks to drop and moderation becomes more important to assist participants in having a cooperative and productive interaction. The scale also makes it more important to employ NLP methods for(semi-)automatic moderation, e.g. to prioritize when moderation is most needed. In this work, we make the first steps towards (semi-)automatic moderation by using state-of-the-art classification models to predict which posts require moderation, showing that while the task is undoubtedly difficult, performance is significantly above baseline. We further investigate whether argument quality is a key indicator of the need for moderation, showing that surprisingly, high quality arguments also trigger moderation. We make our code and data publicly available.</abstract>
      <url hash="15d70475">2021.argmining-1.13</url>
      <bibkey>falk-etal-2021-predicting</bibkey>
      <doi>10.18653/v1/2021.argmining-1.13</doi>
    </paper>
    <paper id="17">
      <title>Matching The Statements: A Simple and Accurate Model for Key Point Analysis</title>
      <author><first>Hoang</first><last>Phan</last></author>
      <author><first>Long</first><last>Nguyen</last></author>
      <author><first>Long</first><last>Nguyen</last></author>
      <author><first>Khanh</first><last>Doan</last></author>
      <pages>165&#8211;174</pages>
      <abstract>Key Point Analysis (KPA) is one of the most essential tasks in building an Opinion Summarization system, which is capable of generating key points for a collection of arguments toward a particular topic. Furthermore, KPA allows quantifying the coverage of each summary by counting its matched arguments. With the aim of creating high-quality summaries, it is necessary to have an in-depth understanding of each individual argument as well as its universal semantic in a specified context. In this paper, we introduce a promising model, named Matching the Statements (MTS) that incorporates the discussed topic information into arguments/key points comprehension to fully understand their meanings, thus accurately performing ranking and retrieving best-match key points for an input argument. Our approach has achieved the 4th place in Track 1 of the Quantitative Summarization &#8211; Key Point Analysis Shared Task by IBM, yielding a competitive performance of 0.8956 (3rd) and 0.9632 (7th) strict and relaxed mean Average Precision, respectively.</abstract>
      <url hash="a68e2c45">2021.argmining-1.17</url>
      <bibkey>phan-etal-2021-matching</bibkey>
      <doi>10.18653/v1/2021.argmining-1.17</doi>
      <pwccode url="https://github.com/viethoang1512/kpa" additional="false">viethoang1512/kpa</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/argkp-2021">ArgKP-2021</pwcdataset>
    </paper>
    </volume>
</collection>