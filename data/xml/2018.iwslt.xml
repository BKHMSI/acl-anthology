<collection id="2018.iwslt">
  <volume id="1" ingest-date="2022-02-18">
    <meta>
      <booktitle>Proceedings of the 15th International Conference on Spoken Language Translation</booktitle>
      <publisher>International Conference on Spoken Language Translation</publisher>
      <address>Brussels</address>
      <month>October 29-30</month>
      <year>2018</year>
      <url hash="289091ee">2018.iwslt-1</url>
      <editor><first>Marco</first><last>Turchi</last></editor>
      <editor><first>Jan</first><last>Niehues</last></editor>
      <editor><first>Marcello</first><last>Frederico</last></editor>
    </meta>
    <frontmatter>
      <url hash="25b3ea73">2018.iwslt-1.0</url>
      <bibkey>iwslt-2018-international</bibkey>
    </frontmatter>
    <paper id="2">
      <title>Unsupervised Parallel Sentence Extraction from Comparable Corpora</title>
      <author><first>Viktor</first><last>Hangya</last></author>
      <author><first>Fabienne</first><last>Braune</last></author>
      <author><first>Yuliya</first><last>Kalasouskaya</last></author>
      <author><first>Alexander</first><last>Fraser</last></author>
      <pages>7-13</pages>
      <abstract>Mining parallel sentences from comparable corpora is of great interest for many downstream tasks. In the BUCC 2017 shared task, systems performed well by training on gold standard parallel sentences. However, we often want to mine parallel sentences without bilingual supervision. We present a simple approach relying on bilingual word embeddings trained in an unsupervised fashion. We incorporate orthographic similarity in order to handle words with similar surface forms. In addition, we propose a dynamic threshold method to decide if a candidate sentence-pair is parallel which eliminates the need to fine tune a static value for different datasets. Since we do not employ any language specific engineering our approach is highly generic. We show that our approach is effective, on three language-pairs, without the use of any bilingual signal which is important because parallel sentence mining is most useful in low resource scenarios.</abstract>
      <url hash="cb9f14e0">2018.iwslt-1.2</url>
      <bibkey>hangya-etal-2018-unsupervised</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/bucc">BUCC</pwcdataset>
    </paper>
    <paper id="4">
      <title>Analyzing Knowledge Distillation in Neural Machine Translation</title>
      <author><first>Dakun</first><last>Zhang</last></author>
      <author><first>Josep</first><last>Crego</last></author>
      <author><first>Jean</first><last>Senellart</last></author>
      <pages>23-30</pages>
      <abstract>Knowledge distillation has recently been successfully applied to neural machine translation. It allows for building shrunk networks while the resulting systems retain most of the quality of the original model. Despite the fact that many authors report on the benefits of knowledge distillation, few have discussed the actual reasons why it works, especially in the context of neural MT. In this paper, we conduct several experiments aimed at understanding why and how distillation impacts accuracy on an English-German translation task. We show that translation complexity is actually reduced when building a distilled/synthesised bi-text when compared to the reference bi-text. We further remove noisy data from synthesised translations and merge filtered synthesised data together with original reference, thus achieving additional gains in terms of accuracy.</abstract>
      <url hash="99d49824">2018.iwslt-1.4</url>
      <bibkey>zhang-etal-2018-analyzing</bibkey>
    </paper>
    <paper id="7">
      <title>Multi-Source Neural Machine Translation with Data Augmentation</title>
      <author><first>Yuta</first><last>Nishimura</last></author>
      <author><first>Katsuhito</first><last>Sudoh</last></author>
      <author><first>Graham</first><last>Neubig</last></author>
      <author><first>Satoshi</first><last>Nakamura</last></author>
      <pages>48-53</pages>
      <award>Best Student Paper</award>
      <abstract>Multi-source translation systems translate from multiple languages to a single target language. By using information from these multiple sources, these systems achieve large gains in accuracy. To train these systems, it is necessary to have corpora with parallel text in multiple sources and the target language. However, these corpora are rarely complete in practice due to the difficulty of providing human translations in all of the relevant languages. In this paper, we propose a data augmentation approach to fill such incomplete parts using multi-source neural machine translation (NMT). In our experiments, results varied over different language combinations but significant gains were observed when using a source language similar to the target language.</abstract>
      <url hash="9ab44f77">2018.iwslt-1.7</url>
      <bibkey>nishimura-etal-2018-multisource</bibkey>
    </paper>
    <paper id="10">
      <title>The <fixed-case>USTC</fixed-case>-<fixed-case>NEL</fixed-case> Speech Translation system at <fixed-case>IWSLT</fixed-case> 2018</title>
      <author><first>Dan</first><last>Liu</last></author>
      <author><first>Junhua</first><last>Liu</last></author>
      <author><first>Wu</first><last>Guo</last></author>
      <author><first>Shifu</first><last>Xiong</last></author>
      <author><first>Zhiqiang</first><last>Ma</last></author>
      <author><first>Rui</first><last>Song</last></author>
      <author><first>Chongliang</first><last>Wu</last></author>
      <author><first>Quan</first><last>Liu</last></author>
      <pages>70-75</pages>
      <abstract>This paper describes the USTC-NEL (short for &#8221;National Engineering Laboratory for Speech and Language Information Processing University of science and technology of china&#8221;) system to the speech translation task of the IWSLT Evaluation 2018. The system is a conventional pipeline system which contains 3 modules: speech recognition, post-processing and machine translation. We train a group of hybrid-HMM models for our speech recognition, and for machine translation we train transformer based neural machine translation models with speech recognition output style text as input. Experiments conducted on the IWSLT 2018 task indicate that, compared to baseline system from KIT, our system achieved 14.9 BLEU improvement.</abstract>
      <url hash="64a80ff2">2018.iwslt-1.10</url>
      <bibkey>liu-etal-2018-ustc</bibkey>
    </paper>
    <paper id="11">
      <title>The <fixed-case>ADAPT</fixed-case> System Description for the <fixed-case>IWSLT</fixed-case> 2018 <fixed-case>B</fixed-case>asque to <fixed-case>E</fixed-case>nglish Translation Task</title>
      <author><first>Alberto</first><last>Poncelas</last></author>
      <author><first>Andy</first><last>Way</last></author>
      <author><first>Kepa</first><last>Sarasola</last></author>
      <pages>76-82</pages>
      <abstract>In this paper we present the ADAPT system built for the Basque to English Low Resource MT Evaluation Campaign. Basque is a low-resourced, morphologically-rich language. This poses a challenge for Neural Machine Translation models which usually achieve better performance when trained with large sets of data. Accordingly, we used synthetic data to improve the translation quality produced by a model built using only authentic data. Our proposal uses back-translated data to: (a) create new sentences, so the system can be trained with more data; and (b) translate sentences that are close to the test set, so the model can be fine-tuned to the document to be translated.</abstract>
      <url hash="42692a7e">2018.iwslt-1.11</url>
      <bibkey>poncelas-etal-2018-adapt</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/wmt-2016">WMT 2016</pwcdataset>
    </paper>
    <paper id="13">
      <title>The <fixed-case>M</fixed-case>e<fixed-case>MAD</fixed-case> Submission to the <fixed-case>IWSLT</fixed-case> 2018 Speech Translation Task</title>
      <author><first>Umut</first><last>Sulubacak</last></author>
      <author><first>J&#246;rg</first><last>Tiedemann</last></author>
      <author><first>Aku</first><last>Rouhe</last></author>
      <author><first /><last>Stig-ArneGr&#246;nroos</last></author>
      <author><first>Mikko</first><last>Kurimo</last></author>
      <pages>89-94</pages>
      <abstract>This paper describes the MeMAD project entry to the IWSLT Speech Translation Shared Task, addressing the translation of English audio into German text. Between the pipeline and end-to-end model tracks, we participated only in the former, with three contrastive systems. We tried also the latter, but were not able to finish our end-to-end model in time. All of our systems start by transcribing the audio into text through an automatic speech recognition (ASR) model trained on the TED-LIUM English Speech Recognition Corpus (TED-LIUM). Afterwards, we feed the transcripts into English-German text-based neural machine translation (NMT) models. Our systems employ three different translation models trained on separate training sets compiled from the English-German part of the TED Speech Translation Corpus (TED-TRANS) and the OPENSUBTITLES2018 section of the OPUS collection. In this paper, we also describe the experiments leading up to our final systems. Our experiments indicate that using OPENSUBTITLES2018 in training significantly improves translation performance. We also experimented with various preand postprocessing routines for the NMT module, but we did not have much success with these. Our best-scoring system attains a BLEU score of 16.45 on the test set for this year&#8217;s task.</abstract>
      <url hash="9b99e769">2018.iwslt-1.13</url>
      <bibkey>sulubacak-etal-2018-memad</bibkey>
    </paper>
    <paper id="17">
      <title><fixed-case>S</fixed-case>amsung and <fixed-case>U</fixed-case>niversity of <fixed-case>E</fixed-case>dinburgh&#8217;s System for the <fixed-case>IWSLT</fixed-case> 2018 Low Resource <fixed-case>MT</fixed-case> Task</title>
      <author><first>Philip</first><last>Williams</last></author>
      <author><first>Marcin</first><last>Chochowski</last></author>
      <author><first>Pawel</first><last>Przybysz</last></author>
      <author><first>Rico</first><last>Sennrich</last></author>
      <author><first>Barry</first><last>Haddow</last></author>
      <author><first>Alexandra</first><last>Birch</last></author>
      <pages>118-123</pages>
      <abstract>This paper describes the joint submission to the IWSLT 2018 Low Resource MT task by Samsung R&amp;D Institute, Poland, and the University of Edinburgh. We focused on supplementing the very limited in-domain Basque-English training data with out-of-domain data, with synthetic data, and with data for other language pairs. We also experimented with a variety of model architectures and features, which included the development of extensions to the Nematus toolkit. Our submission was ultimately produced by a system combination in which we reranked translations from our strongest individual system using multiple weaker systems.</abstract>
      <url hash="6fe9958f">2018.iwslt-1.17</url>
      <bibkey>williems-etal-2018-samsung</bibkey>
    </paper>
    <paper id="18">
      <title>The <fixed-case>AFRL</fixed-case> <fixed-case>IWSLT</fixed-case> 2018 Systems: What Worked, What Didn&#8217;t</title>
      <author><first>Brian</first><last>Ore</last></author>
      <author><first>Eric</first><last>Hansen</last></author>
      <author><first>Katherine</first><last>Young</last></author>
      <author><first>Grant</first><last>Erdmann</last></author>
      <author><first>Jeremy</first><last>Gwinnup</last></author>
      <pages>124-130</pages>
      <abstract>This report summarizes the Air Force Research Laboratory (AFRL) machine translation (MT) and automatic speech recognition (ASR) systems submitted to the spoken language translation (SLT) and low-resource MT tasks as part of the IWSLT18 evaluation campaign.</abstract>
      <url hash="b409fe59">2018.iwslt-1.18</url>
      <bibkey>afrl-etal-2018-ore</bibkey>
    </paper>
    <paper id="21">
      <title><fixed-case>CUNI</fixed-case> <fixed-case>B</fixed-case>asque-to-<fixed-case>E</fixed-case>nglish Submission in <fixed-case>IWSLT</fixed-case>18</title>
      <author><first>Tom</first><last>Kocmi</last></author>
      <author><first>Du&#353;an</first><last>Vari&#353;</last></author>
      <author><first>Ond&#345;ej</first><last>Bojar</last></author>
      <pages>142-146</pages>
      <abstract>We present our submission to the IWSLT18 Low Resource task focused on the translation from Basque-to-English. Our submission is based on the current state-of-the-art self-attentive neural network architecture, Transformer. We further improve this strong baseline by exploiting available monolingual data using the back-translation technique. We also present further improvements gained by a transfer learning, a technique that trains a model using a high-resource language pair (Czech-English) and then fine-tunes the model using the target low-resource language pair (Basque-English).</abstract>
      <url hash="742d6232">2018.iwslt-1.21</url>
      <bibkey>kocmi-etal-2018b-cuni</bibkey>
    </paper>
    <paper id="26">
      <title>Data Selection with Feature Decay Algorithms Using an Approximated Target Side</title>
      <author><first>Alberto</first><last>Poncelas</last></author>
      <author><first>Gideon</first><last>Maillette de Buy Wenniger</last></author>
      <author><first>Andy</first><last>Way</last></author>
      <pages>173-180</pages>
      <abstract>Data selection techniques applied to neural machine translation (NMT) aim to increase the performance of a model by retrieving a subset of sentences for use as training data. One of the possible data selection techniques are transductive learning methods, which select the data based on the test set, i.e. the document to be translated. A limitation of these methods to date is that using the source-side test set does not by itself guarantee that sentences are selected with correct translations, or translations that are suitable given the test-set domain. Some corpora, such as subtitle corpora, may contain parallel sentences with inaccurate translations caused by localization or length restrictions. In order to try to fix this problem, in this paper we propose to use an approximated target-side in addition to the source-side when selecting suitable sentence-pairs for training a model. This approximated target-side is built by pre-translating the source-side. In this work, we explore the performance of this general idea for one specific data selection approach called Feature Decay Algorithms (FDA). We train German-English NMT models on data selected by using the test set (source), the approximated target side, and a mixture of both. Our findings reveal that models built using a combination of outputs of FDA (using the test set and an approximated target side) perform better than those solely using the test set. We obtain a statistically significant improvement of more than 1.5 BLEU points over a model trained with all data, and more than 0.5 BLEU points over a strong FDA baseline that uses source-side information only.</abstract>
      <url hash="3b09452f">2018.iwslt-1.26</url>
      <bibkey>poncelas-etal-2018-data</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/wmt-2015">WMT 2015</pwcdataset>
    </paper>
    <paper id="27">
      <title>Multi-paraphrase Augmentation to Leverage Neural Caption Translation</title>
      <author><first>Johanes</first><last>Effendi</last></author>
      <author><first>Sakriani</first><last>Sakti</last></author>
      <author><first>Katsuhito</first><last>Sudoh</last></author>
      <author><first>Satoshi</first><last>Nakamura</last></author>
      <pages>181-188</pages>
      <abstract>Paraphrasing has been proven to improve translation quality in machine translation (MT) and has been widely studied alongside with the development of statistical MT (SMT). In this paper, we investigate and utilize neural paraphrasing to improve translation quality in neural MT (NMT), which has not yet been much explored. Our first contribution is to propose a new way of creating a multi-paraphrase corpus through visual description. After that, we also proposed to construct neural paraphrase models which initiate expert models and utilize them to leverage NMT. Here, we diffuse the image information by using image-based paraphrasing without using the image itself. Our proposed image-based multi-paraphrase augmentation strategies showed improvement against a vanilla NMT baseline.</abstract>
      <url hash="f7f681b3">2018.iwslt-1.27</url>
      <bibkey>effendi-etal-2018-multi</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/coco">COCO</pwcdataset>
    </paper>
    </volume>
</collection>