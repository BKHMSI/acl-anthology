<collection id="2021.newsum">
  <volume id="1" ingest-date="2021-10-28">
    <meta>
      <booktitle>Proceedings of the Third Workshop on New Frontiers in Summarization</booktitle>
      <editor><first>Giuseppe</first><last>Carenini</last></editor>
      <editor><first>Jackie Chi Kit</first><last>Cheung</last></editor>
      <editor><first>Yue</first><last>Dong</last></editor>
      <editor id="fei-liu-utdallas"><first>Fei</first><last>Liu</last></editor>
      <editor><first>Lu</first><last>Wang</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Online and in Dominican Republic</address>
      <month>November</month>
      <year>2021</year>
    </meta>
    <frontmatter>
      <url hash="50975032">2021.newsum-1.0</url>
      <bibkey>newsum-2021-new</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Sentence-level Planning for Especially Abstractive Summarization</title>
      <author><first>Andreas</first><last>Marfurt</last></author>
      <author><first>James</first><last>Henderson</last></author>
      <pages>1&#8211;14</pages>
      <abstract>Abstractive summarization models heavily rely on copy mechanisms, such as the pointer network or attention, to achieve good performance, measured by textual overlap with reference summaries. As a result, the generated summaries stay close to the formulations in the source document. We propose the *sentence planner* model to generate more abstractive summaries. It includes a hierarchical decoder that first generates a representation for the next summary sentence, and then conditions the word generator on this representation. Our generated summaries are more abstractive and at the same time achieve high ROUGE scores when compared to human reference summaries. We verify the effectiveness of our design decisions with extensive evaluations.</abstract>
      <url hash="6c478295">2021.newsum-1.1</url>
      <bibkey>marfurt-henderson-2021-sentence</bibkey>
      <doi>10.18653/v1/2021.newsum-1.1</doi>
      <pwccode url="https://github.com/idiap/sentence-planner" additional="false">idiap/sentence-planner</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/cnn-daily-mail-1">CNN/Daily Mail</pwcdataset>
    </paper>
    <paper id="2">
      <title>Template-aware Attention Model for Earnings Call Report Generation</title>
      <author><first>Yangchen</first><last>Huang</last></author>
      <author><first>Prashant K.</first><last>Dhingra</last></author>
      <author><first>Seyed Danial</first><last>Mohseni Taheri</last></author>
      <pages>15&#8211;24</pages>
      <abstract>Earning calls are among important resources for investors and analysts for updating their price targets. Firms usually publish corresponding transcripts soon after earnings events. However, raw transcripts are often too long and miss the coherent structure. To enhance the clarity, analysts write well-structured reports for some important earnings call events by analyzing them, requiring time and effort. In this paper, we propose TATSum (Template-Aware aTtention model for Summarization), a generalized neural summarization approach for structured report generation, and evaluate its performance in the earnings call domain. We build a large corpus with thousands of transcripts and reports using historical earnings events. We first generate a candidate set of reports from the corpus as potential soft templates which do not impose actual rules on the output. Then, we employ an encoder model with margin-ranking loss to rank the candidate set and select the best quality template. Finally, the transcript and the selected soft template are used as input in a seq2seq framework for report generation. Empirical results on the earnings call dataset show that our model significantly outperforms state-of-the-art models in terms of informativeness and structure.</abstract>
      <url hash="7a331d05">2021.newsum-1.2</url>
      <bibkey>huang-etal-2021-template</bibkey>
      <doi>10.18653/v1/2021.newsum-1.2</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/cnn-daily-mail-1">CNN/Daily Mail</pwcdataset>
    </paper>
    <paper id="6">
      <title>Evaluation of Summarization Systems across Gender, Age, and Race</title>
      <author><first>Anna</first><last>J&#248;rgensen</last></author>
      <author><first>Anders</first><last>S&#248;gaard</last></author>
      <pages>51&#8211;56</pages>
      <abstract>Summarization systems are ultimately evaluated by human annotators and raters. Usually, annotators and raters do not reflect the demographics of end users, but are recruited through student populations or crowdsourcing platforms with skewed demographics. For two different evaluation scenarios &#8211; evaluation against gold summaries and system output ratings &#8211; we show that summary evaluation is sensitive to protected attributes. This can severely bias system development and evaluation, leading us to build models that cater for some groups rather than others.</abstract>
      <url hash="29131b06">2021.newsum-1.6</url>
      <bibkey>jorgensen-sogaard-2021-evaluation</bibkey>
      <doi>10.18653/v1/2021.newsum-1.6</doi>
    </paper>
    <paper id="8">
      <title>Capturing Speaker Incorrectness: Speaker-Focused Post-Correction for Abstractive Dialogue Summarization</title>
      <author><first>Dongyub</first><last>Lee</last></author>
      <author><first>Jungwoo</first><last>Lim</last></author>
      <author><first>Taesun</first><last>Whang</last></author>
      <author><first>Chanhee</first><last>Lee</last></author>
      <author><first>Seungwoo</first><last>Cho</last></author>
      <author><first>Mingun</first><last>Park</last></author>
      <author><first>Heuiseok</first><last>Lim</last></author>
      <pages>65&#8211;73</pages>
      <abstract>In this paper, we focus on improving the quality of the summary generated by neural abstractive dialogue summarization systems. Even though pre-trained language models generate well-constructed and promising results, it is still challenging to summarize the conversation of multiple participants since the summary should include a description of the overall situation and the actions of each speaker. This paper proposes self-supervised strategies for speaker-focused post-correction in abstractive dialogue summarization. Specifically, our model first discriminates which type of speaker correction is required in a draft summary and then generates a revised summary according to the required type. Experimental results show that our proposed method adequately corrects the draft summaries, and the revised summaries are significantly improved in both quantitative and qualitative evaluations.</abstract>
      <url hash="72e1d5f4">2021.newsum-1.8</url>
      <bibkey>lee-etal-2021-capturing</bibkey>
      <doi>10.18653/v1/2021.newsum-1.8</doi>
    </paper>
    <paper id="11">
      <title>Context or No Context? A preliminary exploration of human-in-the-loop approach for Incremental Temporal Summarization in meetings</title>
      <author><first>Nicole</first><last>Beckage</last></author>
      <author><first>Shachi</first><last>H Kumar</last></author>
      <author><first>Saurav</first><last>Sahay</last></author>
      <author><first>Ramesh</first><last>Manuvinakurike</last></author>
      <pages>96&#8211;106</pages>
      <abstract>Incremental meeting temporal summarization, summarizing relevant information of partial multi-party meeting dialogue, is emerging as the next challenge in summarization research. Here we examine the extent to which human abstractive summaries of the preceding increments (context) can be combined with extractive meeting dialogue to generate abstractive summaries. We find that previous context improves ROUGE scores. Our findings further suggest that contexts begin to outweigh the dialogue. Using keyphrase extraction and semantic role labeling (SRL), we find that SRL captures relevant information without overwhelming the the model architecture. By compressing the previous contexts by ~70%, we achieve better ROUGE scores over our baseline models. Collectively, these results suggest that context matters, as does the way in which context is presented to the model.</abstract>
      <url hash="8b52edd0">2021.newsum-1.11</url>
      <bibkey>beckage-etal-2021-context</bibkey>
      <doi>10.18653/v1/2021.newsum-1.11</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/cnn-daily-mail-1">CNN/Daily Mail</pwcdataset>
    </paper>
    <paper id="12">
      <title>Are We Summarizing the Right Way? A Survey of Dialogue Summarization Data Sets</title>
      <author><first>Don</first><last>Tuggener</last></author>
      <author><first>Margot</first><last>Mieskes</last></author>
      <author><first>Jan</first><last>Deriu</last></author>
      <author><first>Mark</first><last>Cieliebak</last></author>
      <pages>107&#8211;118</pages>
      <abstract>Dialogue summarization is a long-standing task in the field of NLP, and several data sets with dialogues and associated human-written summaries of different styles exist. However, it is unclear for which type of dialogue which type of summary is most appropriate. For this reason, we apply a linguistic model of dialogue types to derive matching summary items and NLP tasks. This allows us to map existing dialogue summarization data sets into this model and identify gaps and potential directions for future work. As part of this process, we also provide an extensive overview of existing dialogue summarization data sets.</abstract>
      <url hash="86503840">2021.newsum-1.12</url>
      <bibkey>tuggener-etal-2021-summarizing</bibkey>
      <doi>10.18653/v1/2021.newsum-1.12</doi>
    </paper>
    <paper id="13">
      <title>Modeling Endorsement for Multi-Document Abstractive Summarization</title>
      <author><first>Logan</first><last>Lebanoff</last></author>
      <author><first>Bingqing</first><last>Wang</last></author>
      <author><first>Zhe</first><last>Feng</last></author>
      <author id="fei-liu-utdallas"><first>Fei</first><last>Liu</last></author>
      <pages>119&#8211;130</pages>
      <abstract>A crucial difference between single- and multi-document summarization is how salient content manifests itself in the document(s). While such content may appear at the beginning of a single document, essential information is frequently reiterated in a set of documents related to a particular topic, resulting in an endorsement effect that increases information salience. In this paper, we model the cross-document endorsement effect and its utilization in multiple document summarization. Our method generates a synopsis from each document, which serves as an endorser to identify salient content from other documents. Strongly endorsed text segments are used to enrich a neural encoder-decoder model to consolidate them into an abstractive summary. The method has a great potential to learn from fewer examples to identify salient content, which alleviates the need for costly retraining when the set of documents is dynamically adjusted. Through extensive experiments on benchmark multi-document summarization datasets, we demonstrate the effectiveness of our proposed method over strong published baselines. Finally, we shed light on future research directions and discuss broader challenges of this task using a case study.</abstract>
      <url hash="23d249a2">2021.newsum-1.13</url>
      <bibkey>lebanoff-etal-2021-modeling</bibkey>
      <doi>10.18653/v1/2021.newsum-1.13</doi>
      <pwccode url="https://github.com/ucfnlp/endorser-summ" additional="false">ucfnlp/endorser-summ</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/wcep">WCEP</pwcdataset>
    </paper>
    <paper id="14">
      <title><fixed-case>SUBSUME</fixed-case>: A Dataset for Subjective Summary Extraction from <fixed-case>W</fixed-case>ikipedia Documents</title>
      <author><first>Nishant</first><last>Yadav</last></author>
      <author><first>Matteo</first><last>Brucato</last></author>
      <author><first>Anna</first><last>Fariha</last></author>
      <author><first>Oscar</first><last>Youngquist</last></author>
      <author><first>Julian</first><last>Killingback</last></author>
      <author><first>Alexandra</first><last>Meliou</last></author>
      <author><first>Peter</first><last>Haas</last></author>
      <pages>131&#8211;141</pages>
      <abstract>Many applications require generation of summaries tailored to the user&#8217;s information needs, i.e., their intent. Methods that express intent via explicit user queries fall short when query interpretation is subjective. Several datasets exist for summarization with objective intents where, for each document and intent (e.g., &#8220;weather&#8221;), a single summary suffices for all users. No datasets exist, however, for subjective intents (e.g., &#8220;interesting places&#8221;) where different users will provide different summaries. We present SUBSUME, the first dataset for evaluation of SUBjective SUMmary Extraction systems. SUBSUME contains 2,200 (document, intent, summary) triplets over 48 Wikipedia pages, with ten intents of varying subjectivity, provided by 103 individuals over Mechanical Turk. We demonstrate statistically that the intents in SUBSUME vary systematically in subjectivity. To indicate SUBSUME&#8217;s usefulness, we explore a collection of baseline algorithms for subjective extractive summarization and show that (i) as expected, example-based approaches better capture subjective intents than query-based ones, and (ii) there is ample scope for improving upon the baseline algorithms, thereby motivating further research on this challenging problem.</abstract>
      <url hash="6d84def1">2021.newsum-1.14</url>
      <bibkey>yadav-etal-2021-subsume</bibkey>
      <doi>10.18653/v1/2021.newsum-1.14</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/subsume">SubSumE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/cnn-daily-mail-1">CNN/Daily Mail</pwcdataset>
    </paper>
    </volume>
</collection>