<collection id="2021.insights">
  <volume id="1" ingest-date="2021-11-02">
    <meta>
      <booktitle>Proceedings of the Second Workshop on Insights from Negative Results in NLP</booktitle>
      <editor><first>Jo&#227;o</first><last>Sedoc</last></editor>
      <editor><first>Anna</first><last>Rogers</last></editor>
      <editor><first>Anna</first><last>Rumshisky</last></editor>
      <editor><first>Shabnam</first><last>Tafreshi</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Online and Punta Cana, Dominican Republic</address>
      <month>November</month>
      <year>2021</year>
    </meta>
    <frontmatter>
      <url hash="18678a7a">2021.insights-1.0</url>
      <bibkey>insights-2021-insights</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Corrected <fixed-case>CBOW</fixed-case> Performs as well as Skip-gram</title>
      <author><first>Ozan</first><last>&#304;rsoy</last></author>
      <author><first>Adrian</first><last>Benton</last></author>
      <author><first>Karl</first><last>Stratos</last></author>
      <pages>1&#8211;8</pages>
      <abstract>Mikolov et al. (2013a) observed that continuous bag-of-words (CBOW) word embeddings tend to underperform Skip-gram (SG) embeddings, and this finding has been reported in subsequent works. We find that these observations are driven not by fundamental differences in their training objectives, but more likely on faulty negative sampling CBOW implementations in popular libraries such as the official implementation, word2vec.c, and Gensim. We show that after correcting a bug in the CBOW gradient update, one can learn CBOW word embeddings that are fully competitive with SG on various intrinsic and extrinsic tasks, while being many times faster to train.</abstract>
      <url hash="aeb2d85a">2021.insights-1.1</url>
      <bibkey>irsoy-etal-2021-corrected</bibkey>
      <doi>10.18653/v1/2021.insights-1.1</doi>
      <pwccode url="https://github.com/bloomberg/koan" additional="false">bloomberg/koan</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/c4">C4</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/qnli">QNLI</pwcdataset>
    </paper>
    <paper id="3">
      <title><fixed-case>BERT</fixed-case> Cannot Align Characters</title>
      <author><first>Antonis</first><last>Maronikolakis</last></author>
      <author><first>Philipp</first><last>Dufter</last></author>
      <author><first>Hinrich</first><last>Sch&#252;tze</last></author>
      <pages>16&#8211;22</pages>
      <abstract>In previous work, it has been shown that BERT can adequately align cross-lingual sentences on the word level. Here we investigate whether BERT can also operate as a char-level aligner. The languages examined are English, Fake English, German and Greek. We show that the closer two languages are, the better BERT can align them on the character level. BERT indeed works well in English to Fake English alignment, but this does not generalize to natural languages to the same extent. Nevertheless, the proximity of two languages does seem to be a factor. English is more related to German than to Greek and this is reflected in how well BERT aligns them; English to German is better than English to Greek. We examine multiple setups and show that the similarity matrices for natural languages show weaker relations the further apart two languages are.</abstract>
      <url hash="993e58e5">2021.insights-1.3</url>
      <bibkey>maronikolakis-etal-2021-bert</bibkey>
      <doi>10.18653/v1/2021.insights-1.3</doi>
    </paper>
    <paper id="4">
      <title>Two Heads are Better than One? Verification of Ensemble Effect in Neural Machine Translation</title>
      <author><first>Chanjun</first><last>Park</last></author>
      <author><first>Sungjin</first><last>Park</last></author>
      <author><first>Seolhwa</first><last>Lee</last></author>
      <author><first>Taesun</first><last>Whang</last></author>
      <author><first>Heuiseok</first><last>Lim</last></author>
      <pages>23&#8211;28</pages>
      <abstract>In the field of natural language processing, ensembles are broadly known to be effective in improving performance. This paper analyzes how ensemble of neural machine translation (NMT) models affect performance improvement by designing various experimental setups (i.e., intra-, inter-ensemble, and non-convergence ensemble). To an in-depth examination, we analyze each ensemble method with respect to several aspects such as different attention models and vocab strategies. Experimental results show that ensembling is not always resulting in performance increases and give noteworthy negative findings.</abstract>
      <url hash="507d99cc">2021.insights-1.4</url>
      <bibkey>park-etal-2021-two</bibkey>
      <doi>10.18653/v1/2021.insights-1.4</doi>
    </paper>
    <paper id="8">
      <title>Comparing <fixed-case>E</fixed-case>uclidean and Hyperbolic Embeddings on the <fixed-case>W</fixed-case>ord<fixed-case>N</fixed-case>et Nouns Hypernymy Graph</title>
      <author><first>Sameer</first><last>Bansal</last></author>
      <author><first>Adrian</first><last>Benton</last></author>
      <pages>49&#8211;53</pages>
      <abstract>Nickel and Kiela (2017) present a new method for embedding tree nodes in the Poincare ball, and suggest that these hyperbolic embeddings are far more effective than Euclidean embeddings at embedding nodes in large, hierarchically structured graphs like the WordNet nouns hypernymy tree. This is especially true in low dimensions (Nickel and Kiela, 2017, Table 1). In this work, we seek to reproduce their experiments on embedding and reconstructing the WordNet nouns hypernymy graph. Counter to what they report, we find that Euclidean embeddings are able to represent this tree at least as well as Poincare embeddings, when allowed at least 50 dimensions. We note that this does not diminish the significance of their work given the impressive performance of hyperbolic embeddings in very low-dimensional settings. However, given the wide influence of their work, our aim here is to present an updated and more accurate comparison between the Euclidean and hyperbolic embeddings.</abstract>
      <url hash="fa0346b5">2021.insights-1.8</url>
      <bibkey>bansal-benton-2021-comparing</bibkey>
      <doi>10.18653/v1/2021.insights-1.8</doi>
    </paper>
    <paper id="12">
      <title>The Highs and Lows of Simple Lexical Domain Adaptation Approaches for Neural Machine Translation</title>
      <author><first>Nikolay</first><last>Bogoychev</last></author>
      <author><first>Pinzhen</first><last>Chen</last></author>
      <pages>74&#8211;80</pages>
      <abstract>Machine translation systems are vulnerable to domain mismatch, especially in a low-resource scenario. Out-of-domain translations are often of poor quality and prone to hallucinations, due to exposure bias and the decoder acting as a language model. We adopt two approaches to alleviate this problem: lexical shortlisting restricted by IBM statistical alignments, and hypothesis reranking based on similarity. The methods are computationally cheap and show success on low-resource out-of-domain test sets. However, the methods lose advantage when there is sufficient data or too great domain mismatch. This is due to both the IBM model losing its advantage over the implicitly learned neural alignment, and issues with subword segmentation of unseen words.</abstract>
      <url hash="93b16635">2021.insights-1.12</url>
      <bibkey>bogoychev-chen-2021-highs</bibkey>
      <doi>10.18653/v1/2021.insights-1.12</doi>
      <pwccode url="https://github.com/marian-nmt/marian" additional="false">marian-nmt/marian</pwccode>
    </paper>
    <paper id="13">
      <title>Backtranslation in Neural Morphological Inflection</title>
      <author><first>Ling</first><last>Liu</last></author>
      <author><first>Mans</first><last>Hulden</last></author>
      <pages>81&#8211;88</pages>
      <abstract>Backtranslation is a common technique for leveraging unlabeled data in low-resource scenarios in machine translation. The method is directly applicable to morphological inflection generation if unlabeled word forms are available. This paper evaluates the potential of backtranslation for morphological inflection using data from six languages with labeled data drawn from the SIGMORPHON shared task resource and unlabeled data from different sources. Our core finding is that backtranslation can offer modest improvements in low-resource scenarios, but only if the unlabeled data is very clean and has been filtered by the same annotation standards as the labeled data.</abstract>
      <url hash="f560fdcd">2021.insights-1.13</url>
      <bibkey>liu-hulden-2021-backtranslation</bibkey>
      <doi>10.18653/v1/2021.insights-1.13</doi>
    </paper>
    <paper id="19">
      <title>Challenging the Semi-Supervised <fixed-case>VAE</fixed-case> Framework for Text Classification</title>
      <author><first>Ghazi</first><last>Felhi</last></author>
      <author><first>Joseph</first><last>Le Roux</last></author>
      <author><first>Djam&#233;</first><last>Seddah</last></author>
      <pages>136&#8211;143</pages>
      <abstract>Semi-Supervised Variational Autoencoders (SSVAEs) are widely used models for data efficient learning. In this paper, we question the adequacy of the standard design of sequence SSVAEs for the task of text classification as we exhibit two sources of overcomplexity for which we provide simplifications. These simplifications to SSVAEs preserve their theoretical soundness while providing a number of practical advantages in the semi-supervised setup where the result of training is a text classifier. These simplifications are the removal of (i) the Kullback-Liebler divergence from its objective and (ii) the fully unobserved latent variable from its probabilistic model. These changes relieve users from choosing a prior for their latent variables, make the model smaller and faster, and allow for a better flow of information into the latent variables. We compare the simplified versions to standard SSVAEs on 4 text classification tasks. On top of the above-mentioned simplification, experiments show a speed-up of 26%, while keeping equivalent classification scores. The code to reproduce our experiments is public.</abstract>
      <url hash="f94e6d7d">2021.insights-1.19</url>
      <bibkey>felhi-etal-2021-challenging</bibkey>
      <doi>10.18653/v1/2021.insights-1.19</doi>
      <pwccode url="https://github.com/ghazi-f/challenging-ssvaes" additional="false">ghazi-f/challenging-ssvaes</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/ag-news">AG News</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
    </paper>
    <paper id="20">
      <title>Active Learning for Argument Strength Estimation</title>
      <author><first>Nataliia</first><last>Kees</last></author>
      <author><first>Michael</first><last>Fromm</last></author>
      <author><first>Evgeniy</first><last>Faerman</last></author>
      <author><first>Thomas</first><last>Seidl</last></author>
      <pages>144&#8211;150</pages>
      <abstract>High-quality arguments are an essential part of decision-making. Automatically predicting the quality of an argument is a complex task that recently got much attention in argument mining. However, the annotation effort for this task is exceptionally high. Therefore, we test uncertainty-based active learning (AL) methods on two popular argument-strength data sets to estimate whether sample-efficient learning can be enabled. Our extensive empirical evaluation shows that uncertainty-based acquisition functions can not surpass the accuracy reached with the random acquisition on these data sets.</abstract>
      <url hash="8e0ea365">2021.insights-1.20</url>
      <attachment type="Software" hash="a1fb3587">2021.insights-1.20.Software.zip</attachment>
      <bibkey>kees-etal-2021-active</bibkey>
      <doi>10.18653/v1/2021.insights-1.20</doi>
      <pwccode url="https://github.com/nkees/active-learning-argument-strength" additional="false">nkees/active-learning-argument-strength</pwccode>
    </paper>
  </volume>
</collection>