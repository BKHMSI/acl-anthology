<collection id="Q19">
  <volume id="1">
    <meta>
      <booktitle>Transactions of the Association for Computational Linguistics, Volume 7</booktitle>
      <editor><last>Lee</last><first>Lillian</first></editor>
      <editor><last>Johnson</last><first>Mark</first></editor>
      <editor><last>Roark</last><first>Brian</first></editor>
      <editor><last>Nenkova</last><first>Ani</first></editor>
      <publisher>MIT Press</publisher>
      <address>Cambridge, MA</address>
      <year>2019</year>
    </meta>
    <frontmatter>
      <bibkey>tacl-2019-transactions</bibkey>
    </frontmatter>
    <paper id="2">
      <title>Semantic Neural Machine Translation Using <fixed-case>AMR</fixed-case></title>
      <author><first>Linfeng</first><last>Song</last></author>
      <author><first>Daniel</first><last>Gildea</last></author>
      <author><first>Yue</first><last>Zhang</last></author>
      <author><first>Zhiguo</first><last>Wang</last></author>
      <author><first>Jinsong</first><last>Su</last></author>
      <doi>10.1162/tacl_a_00252</doi>
      <abstract>It is intuitive that semantic representations can be useful for machine translation, mainly because they can help in enforcing meaning preservation and handling data sparsity (many sentences correspond to one meaning) of machine translation models. On the other hand, little work has been done on leveraging semantics for neural machine translation (NMT). In this work, we study the usefulness of AMR (abstract meaning representation) on NMT. Experiments on a standard English-to-German dataset show that incorporating AMR as additional knowledge can significantly improve a strong attention-based sequence-to-sequence neural translation model.</abstract>
      <pages>19&#8211;31</pages>
      <url hash="5a3f3312">Q19-1002</url>
      <bibkey>song-etal-2019-semantic</bibkey>
      <pwccode url="https://github.com/freesunshine0316/semantic-nmt" additional="false">freesunshine0316/semantic-nmt</pwccode>
    </paper>
    <paper id="3">
      <title>Joint Transition-Based Models for Morpho-Syntactic Parsing: Parsing Strategies for <fixed-case>MRL</fixed-case>s and a Case Study from <fixed-case>M</fixed-case>odern <fixed-case>H</fixed-case>ebrew</title>
      <author><first>Amir</first><last>More</last></author>
      <author><first>Amit</first><last>Seker</last></author>
      <author><first>Victoria</first><last>Basmova</last></author>
      <author><first>Reut</first><last>Tsarfaty</last></author>
      <doi>10.1162/tacl_a_00253</doi>
      <abstract>In standard NLP pipelines, morphological analysis and disambiguation (MA&amp;D) precedes syntactic and semantic downstream tasks. However, for languages with complex and ambiguous word-internal structure, known as morphologically rich languages (MRLs), it has been hypothesized that syntactic context may be crucial for accurate MA&amp;D, and vice versa. In this work we empirically confirm this hypothesis for Modern Hebrew, an MRL with complex morphology and severe word-level ambiguity, in a novel transition-based framework. Specifically, we propose a joint morphosyntactic transition-based framework which formally unifies two distinct transition systems, morphological and syntactic, into a single transition-based system with joint training and joint inference. We empirically show that MA&amp;D results obtained in the joint settings outperform MA&amp;D results obtained by the respective standalone components, and that end-to-end parsing results obtained by our joint system present a new state of the art for Hebrew dependency parsing.</abstract>
      <pages>33&#8211;48</pages>
      <video href="https://vimeo.com/384777366" />
      <url hash="f3f27f3a">Q19-1003</url>
      <bibkey>more-etal-2019-joint</bibkey>
    </paper>
    <paper id="6">
      <title>Synchronous Bidirectional Neural Machine Translation</title>
      <author><first>Long</first><last>Zhou</last></author>
      <author><first>Jiajun</first><last>Zhang</last></author>
      <author><first>Chengqing</first><last>Zong</last></author>
      <doi>10.1162/tacl_a_00256</doi>
      <abstract>Existing approaches to neural machine translation (NMT) generate the target language sequence token-by-token from left to right. However, this kind of unidirectional decoding framework cannot make full use of the target-side future contexts which can be produced in a right-to-left decoding direction, and thus suffers from the issue of unbalanced outputs. In this paper, we introduce a synchronous bidirectional&#8211;neural machine translation (SB-NMT) that predicts its outputs using left-to-right and right-to-left decoding simultaneously and interactively, in order to leverage both of the history and future information at the same time. Specifically, we first propose a new algorithm that enables synchronous bidirectional decoding in a single model. Then, we present an interactive decoding model in which left-to-right (right-to-left) generation does not only depend on its previously generated outputs, but also relies on future contexts predicted by right-to-left (left-to-right) decoding. We extensively evaluate the proposed SB-NMT model on large-scale NIST Chinese&#8211;English, WMT14 English&#8211;German, and WMT18 Russian&#8211;English translation tasks. Experimental results demonstrate that our model achieves significant improvements over the strong Transformer model by 3.92, 1.49, and 1.04 BLEU points, respectively, and obtains the state-of-the-art performance on Chinese&#8211;English and English&#8211;German translation tasks.</abstract>
      <pages>91&#8211;105</pages>
      <video href="https://vimeo.com/385255892" />
      <url hash="14f3e215">Q19-1006</url>
      <bibkey>zhou-etal-2019-synchronous</bibkey>
      <pwccode url="https://github.com/wszlong/sb-nmt" additional="true">wszlong/sb-nmt</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/wmt-2014">WMT 2014</pwcdataset>
    </paper>
    <paper id="9">
      <title><fixed-case>GILE</fixed-case>: A Generalized Input-Label Embedding for Text Classification</title>
      <author><first>Nikolaos</first><last>Pappas</last></author>
      <author><first>James</first><last>Henderson</last></author>
      <doi>10.1162/tacl_a_00259</doi>
      <abstract>Neural text classification models typically treat output labels as categorical variables that lack description and semantics. This forces their parametrization to be dependent on the label set size, and, hence, they are unable to scale to large label sets and generalize to unseen ones. Existing joint input-label text models overcome these issues by exploiting label descriptions, but they are unable to capture complex label relationships, have rigid parametrization, and their gains on unseen labels happen often at the expense of weak performance on the labels seen during training. In this paper, we propose a new input-label model that generalizes over previous such models, addresses their limitations, and does not compromise performance on seen labels. The model consists of a joint nonlinear input-label embedding with controllable capacity and a joint-space-dependent classification unit that is trained with cross-entropy loss to optimize classification performance. We evaluate models on full-resource and low- or zero-resource text classification of multilingual news and biomedical text with a large label set. Our model outperforms monolingual and multilingual models that do not leverage label semantics and previous joint input-label space models in both scenarios.</abstract>
      <pages>139&#8211;155</pages>
      <url hash="6d5277f7">Q19-1009</url>
      <bibkey>pappas-henderson-2019-gile</bibkey>
      <pwccode url="https://github.com/idiap/gile" additional="false">idiap/gile</pwccode>
    </paper>
    <paper id="10">
      <title>Autosegmental Input Strictly Local Functions</title>
      <author><first>Jane</first><last>Chandlee</last></author>
      <author><first>Adam</first><last>Jardine</last></author>
      <doi>10.1162/tacl_a_00260</doi>
      <abstract>Autosegmental representations (ARs; Goldsmith, 1976) are claimed to enable local analyses of otherwise non-local phenomena Odden (1994). Focusing on the domain of tone, we investigate this ability of ARs using a computationally well-defined notion of locality extended from Chandlee (2014). The result is a more nuanced understanding of the way in which ARs interact with phonological locality.</abstract>
      <pages>157&#8211;168</pages>
      <url hash="b3f1bc1c">Q19-1010</url>
      <bibkey>chandlee-jardine-2019-autosegmental</bibkey>
    </paper>
    <paper id="11">
      <title><fixed-case>SECTOR</fixed-case>: A Neural Model for Coherent Topic Segmentation and Classification</title>
      <author><first>Sebastian</first><last>Arnold</last></author>
      <author><first>Rudolf</first><last>Schneider</last></author>
      <author><first>Philippe</first><last>Cudr&#233;-Mauroux</last></author>
      <author><first>Felix A.</first><last>Gers</last></author>
      <author><first>Alexander</first><last>L&#246;ser</last></author>
      <doi>10.1162/tacl_a_00261</doi>
      <abstract>When searching for information, a human reader first glances over a document, spots relevant sections, and then focuses on a few sentences for resolving her intention. However, the high variance of document structure complicates the identification of the salient topic of a given section at a glance. To tackle this challenge, we present SECTOR, a model to support machine reading systems by segmenting documents into coherent sections and assigning topic labels to each section. Our deep neural network architecture learns a latent topic embedding over the course of a document. This can be leveraged to classify local topics from plain text and segment a document at topic shifts. In addition, we contribute WikiSection, a publicly available data set with 242k labeled sections in English and German from two distinct domains: diseases and cities. From our extensive evaluation of 20 architectures, we report a highest score of 71.6% F1 for the segmentation and classification of 30 topics from the English city domain, scored by our SECTOR long short-term memory model with Bloom filter embeddings and bidirectional segmentation. This is a significant improvement of 29.5 points F1 over state-of-the-art CNN classifiers with baseline segmentation.</abstract>
      <pages>169&#8211;184</pages>
      <url hash="3b77c57d">Q19-1011</url>
      <video href="https://vimeo.com/384478902" />
      <attachment type="presentation" hash="e0aa7535">Q19-1011.Presentation.pdf</attachment>
      <bibkey>arnold-etal-2019-sector</bibkey>
      <pwccode url="https://github.com/sebastianarnold/SECTOR" additional="true">sebastianarnold/SECTOR</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/wikisection">WikiSection</pwcdataset>
    </paper>
    <paper id="12">
      <title>Complex Program Induction for Querying Knowledge Bases in the Absence of Gold Programs</title>
      <author><first>Amrita</first><last>Saha</last></author>
      <author><first>Ghulam Ahmed</first><last>Ansari</last></author>
      <author><first>Abhishek</first><last>Laddha</last></author>
      <author><first>Karthik</first><last>Sankaranarayanan</last></author>
      <author><first>Soumen</first><last>Chakrabarti</last></author>
      <doi>10.1162/tacl_a_00262</doi>
      <abstract>Recent years have seen increasingly complex question-answering on knowledge bases (KBQA) involving logical, quantitative, and comparative reasoning over KB subgraphs. Neural Program Induction (NPI) is a pragmatic approach toward modularizing the reasoning process by translating a complex natural language query into a multi-step executable program. While NPI has been commonly trained with the &#8216;&#8216;gold&#8217;&#8217; program or its sketch, for realistic KBQA applications such gold programs are expensive to obtain. There, practically only natural language queries and the corresponding answers can be provided for training. The resulting combinatorial explosion in program space, along with extremely sparse rewards, makes NPI for KBQA ambitious and challenging. We present Complex Imperative Program Induction from Terminal Rewards (CIPITR), an advanced neural programmer that mitigates reward sparsity with auxiliary rewards, and restricts the program space to semantically correct programs using high-level constraints, KB schema, and inferred answer type. CIPITR solves complex KBQA considerably more accurately than key-value memory networks and neural symbolic machines (NSM). For moderately complex queries requiring 2- to 5-step programs, CIPITR scores at least 3&#215; higher F1 than the competing systems. On one of the hardest class of programs (comparative reasoning) with 5&#8211;10 steps, CIPITR outperforms NSM by a factor of 89 and memory networks by 9 times.</abstract>
      <pages>185&#8211;200</pages>
      <url hash="3cd83ac9">Q19-1012</url>
      <bibkey>saha-etal-2019-complex</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/csqa">CSQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/webquestions">WebQuestions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/webquestionssp">WebQuestionsSP</pwcdataset>
    </paper>
    <paper id="14">
      <title><fixed-case>DREAM</fixed-case>: A Challenge Data Set and Models for Dialogue-Based Reading Comprehension</title>
      <author><first>Kai</first><last>Sun</last></author>
      <author><first>Dian</first><last>Yu</last></author>
      <author><first>Jianshu</first><last>Chen</last></author>
      <author><first>Dong</first><last>Yu</last></author>
      <author><first>Yejin</first><last>Choi</last></author>
      <author><first>Claire</first><last>Cardie</last></author>
      <doi>10.1162/tacl_a_00264</doi>
      <abstract>We present DREAM, the first dialogue-based multiple-choice reading comprehension data set. Collected from English as a Foreign Language examinations designed by human experts to evaluate the comprehension level of Chinese learners of English, our data set contains 10,197 multiple-choice questions for 6,444 dialogues. In contrast to existing reading comprehension data sets, DREAM is the first to focus on in-depth multi-turn multi-party dialogue understanding. DREAM is likely to present significant challenges for existing reading comprehension systems: 84% of answers are non-extractive, 85% of questions require reasoning beyond a single sentence, and 34% of questions also involve commonsense knowledge. We apply several popular neural reading comprehension models that primarily exploit surface information within the text and find them to, at best, just barely outperform a rule-based approach. We next investigate the effects of incorporating dialogue structure and different kinds of general world knowledge into both rule-based and (neural and non-neural) machine learning-based reading comprehension models. Experimental results on the DREAM data set show the effectiveness of dialogue structure and general world knowledge. DREAM is available at https://dataset.org/dream/.</abstract>
      <pages>217&#8211;231</pages>
      <url hash="aee4406b">Q19-1014</url>
      <bibkey>sun-etal-2019-dream</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/dream">DREAM</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/coqa">CoQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/conceptnet">ConceptNet</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/narrativeqa">NarrativeQA</pwcdataset>
    </paper>
    <paper id="22">
      <title>Syntax-aware Semantic Role Labeling without Parsing</title>
      <author><first>Rui</first><last>Cai</last></author>
      <author><first>Mirella</first><last>Lapata</last></author>
      <doi>10.1162/tacl_a_00272</doi>
      <abstract>In this paper we focus on learning dependency aware representations for semantic role labeling without recourse to an external parser. The backbone of our model is an LSTM-based semantic role labeler jointly trained with two auxiliary tasks: predicting the dependency label of a word and whether there exists an arc linking it to the predicate. The auxiliary tasks provide syntactic information that is specific to semantic role labeling and are learned from training data (dependency annotations) without relying on existing dependency parsers, which can be noisy (e.g., on out-of-domain data or infrequent constructions). Experimental results on the CoNLL-2009 benchmark dataset show that our model outperforms the state of the art in English, and consistently improves performance in other languages, including Chinese, German, and Spanish.</abstract>
      <pages>343&#8211;356</pages>
      <video href="https://vimeo.com/384772555" />
      <url hash="c8f6bcf4">Q19-1022</url>
      <bibkey>cai-lapata-2019-syntax</bibkey>
      <pwccode url="https://github.com/RuiCaiNLP/SRL_DEP" additional="false">RuiCaiNLP/SRL_DEP</pwccode>
    </paper>
    <paper id="25">
      <title>No Word is an <fixed-case>I</fixed-case>sland&#8212;<fixed-case>A</fixed-case> Transformation Weighting Model for Semantic Composition</title>
      <author><first>Corina</first><last>Dima</last></author>
      <author><first>Dani&#235;l</first><last>de Kok</last></author>
      <author><first>Neele</first><last>Witte</last></author>
      <author><first>Erhard</first><last>Hinrichs</last></author>
      <doi>10.1162/tacl_a_00275</doi>
      <abstract>Composition models of distributional semantics are used to construct phrase representations from the representations of their words. Composition models are typically situated on two ends of a spectrum. They either have a small number of parameters but compose all phrases in the same way, or they perform word-specific compositions at the cost of a far larger number of parameters. In this paper we propose transformation weighting (TransWeight), a composition model that consistently outperforms existing models on nominal compounds, adjective-noun phrases, and adverb-adjective phrases in English, German, and Dutch. TransWeight drastically reduces the number of parameters needed compared with the best model in the literature by composing similar words in the same way.</abstract>
      <pages>437&#8211;451</pages>
      <video href="https://vimeo.com/384772326" />
      <url hash="f819b871">Q19-1025</url>
      <bibkey>dima-etal-2019-word</bibkey>
    </paper>
    </volume>
</collection>