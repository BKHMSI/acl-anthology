<collection id="2020.codi">
  <volume id="1" ingest-date="2020-11-06">
    <meta>
      <booktitle>Proceedings of the First Workshop on Computational Approaches to Discourse</booktitle>
      <editor><first>Chlo&#233;</first><last>Braud</last></editor>
      <editor><first>Christian</first><last>Hardmeier</last></editor>
      <editor><first>Junyi Jessy</first><last>Li</last></editor>
      <editor><first>Annie</first><last>Louis</last></editor>
      <editor><first>Michael</first><last>Strube</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Online</address>
      <month>November</month>
      <year>2020</year>
    </meta>
    <frontmatter>
      <url hash="f32c0e1b">2020.codi-1.0</url>
      <bibkey>codi-2020-approaches</bibkey>
    </frontmatter>
    <paper id="6">
      <title>Exploring Coreference Features in Heterogeneous Data</title>
      <author><first>Ekaterina</first><last>Lapshinova-Koltunski</last></author>
      <author><first>Kerstin</first><last>Kunz</last></author>
      <pages>53&#8211;64</pages>
      <abstract>The present paper focuses on variation phenomena in coreference chains. We address the hypothesis that the degree of structural variation between chain elements depends on language-specific constraints and preferences and, even more, on the communicative situation of language production. We define coreference features that also include reference to abstract entities and events. These features are inspired through several sources &#8211; cognitive parameters, pragmatic factors and typological status. We pay attention to the distributions of these features in a dataset containing English and German texts of spoken and written discourse mode, which can be classified into seven different registers. We apply text classification and feature selection to find out how these variational dimensions (language, mode and register) impact on coreference features. Knowledge on the variation under analysis is valuable for contrastive linguistics, translation studies and multilingual natural language processing (NLP), e.g. machine translation or cross-lingual coreference resolution.</abstract>
      <url hash="8d1943cb">2020.codi-1.6</url>
      <doi>10.18653/v1/2020.codi-1.6</doi>
      <video href="https://slideslive.com/38939691" />
      <bibkey>lapshinova-koltunski-kunz-2020-exploring</bibkey>
    </paper>
    <paper id="8">
      <title><fixed-case>DSNDM</fixed-case>: Deep <fixed-case>S</fixed-case>iamese Neural Discourse Model with Attention for Text Pairs Categorization and Ranking</title>
      <author><first>Alexander</first><last>Chernyavskiy</last></author>
      <author><first>Dmitry</first><last>Ilvovsky</last></author>
      <pages>76&#8211;85</pages>
      <abstract>In this paper, the utility and advantages of the discourse analysis for text pairs categorization and ranking are investigated. We consider two tasks in which discourse structure seems useful and important: automatic verification of political statements, and ranking in question answering systems. We propose a neural network based approach to learn the match between pairs of discourse tree structures. To this end, the neural TreeLSTM model is modified to effectively encode discourse trees and DSNDM model based on it is suggested to analyze pairs of texts. In addition, the integration of the attention mechanism in the model is proposed. Moreover, different ranking approaches are investigated for the second task. In the paper, the comparison with state-of-the-art methods is given. Experiments illustrate that combination of neural networks and discourse structure in DSNDM is effective since it reaches top results in the assigned tasks. The evaluation also demonstrates that discourse analysis improves quality for the processing of longer texts.</abstract>
      <url hash="763c07db">2020.codi-1.8</url>
      <doi>10.18653/v1/2020.codi-1.8</doi>
      <video href="https://slideslive.com/38939693" />
      <bibkey>chernyavskiy-ilvovsky-2020-dsndm</bibkey>
    </paper>
    <paper id="10">
      <title>Joint Modeling of Arguments for Event Understanding</title>
      <author><first>Yunmo</first><last>Chen</last></author>
      <author><first>Tongfei</first><last>Chen</last></author>
      <author><first>Benjamin</first><last>Van Durme</last></author>
      <pages>96&#8211;101</pages>
      <abstract>We recognize the task of event argument linking in documents as similar to that of intent slot resolution in dialogue, providing a Transformer-based model that extends from a recently proposed solution to resolve references to slots. The approach allows for joint consideration of argument candidates given a detected event, which we illustrate leads to state-of-the-art performance in multi-sentence argument linking.</abstract>
      <url hash="b66d3c30">2020.codi-1.10</url>
      <doi>10.18653/v1/2020.codi-1.10</doi>
      <video href="https://slideslive.com/38939698" />
      <bibkey>chen-etal-2020-joint-modeling</bibkey>
    </paper>
    <paper id="14">
      <title>Extending Implicit Discourse Relation Recognition to the <fixed-case>PDTB</fixed-case>-3</title>
      <author><first>Li</first><last>Liang</last></author>
      <author><first>Zheng</first><last>Zhao</last></author>
      <author><first>Bonnie</first><last>Webber</last></author>
      <pages>135&#8211;147</pages>
      <abstract>The PDTB-3 contains many more Implicit discourse relations than the previous PDTB-2. This is in part because implicit relations have now been annotated within sentences as well as between them. In addition, some now co-occur with explicit discourse relations, instead of standing on their own. Here we show that while this can complicate the problem of identifying the location of implicit discourse relations, it can in turn simplify the problem of identifying their senses. We present data to support this claim, as well as methods that can serve as a non-trivial baseline for future state-of-the-art recognizers for implicit discourse relations.</abstract>
      <url hash="d2d7553c">2020.codi-1.14</url>
      <doi>10.18653/v1/2020.codi-1.14</doi>
      <video href="https://slideslive.com/38939702" />
      <bibkey>liang-etal-2020-extending</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/penn-treebank">Penn Treebank</pwcdataset>
    </paper>
    </volume>
</collection>