<?xml version='1.0' encoding='utf-8'?>
<collection id="2021.adaptnlp">
  <volume id="1" ingest-date="2021-04-19">
    <meta>
      <booktitle>Proceedings of the Second Workshop on Domain Adaptation for NLP</booktitle>
      <editor><first>Eyal</first><last>Ben-David</last></editor>
      <editor><first>Shay</first><last>Cohen</last></editor>
      <editor><first>Ryan</first><last>McDonald</last></editor>
      <editor><first>Barbara</first><last>Plank</last></editor>
      <editor><first>Roi</first><last>Reichart</last></editor>
      <editor><first>Guy</first><last>Rotman</last></editor>
      <editor><first>Yftah</first><last>Ziser</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Kyiv, Ukraine</address>
      <month>April</month>
      <year>2021</year>
    </meta>
    <frontmatter>
      <url hash="7c3fd8ad">2021.adaptnlp-1.0</url>
      <bibkey>adaptnlp-2021-domain</bibkey>
    </frontmatter>
    <paper id="6">
      <title>Challenges in Annotating and Parsing Spoken, Code-switched, Frisian-Dutch Data<fixed-case>F</fixed-case>risian-<fixed-case>D</fixed-case>utch Data</title>
      <author><first>Anouck</first><last>Braggaar</last></author>
      <author><first>Rob</first><last>van der Goot</last></author>
      <pages>50–58</pages>
      <abstract>While high performance have been obtained for high-resource languages, performance on low-resource languages lags behind. In this paper we focus on the <a href="https://en.wikipedia.org/wiki/Parsing">parsing</a> of the low-resource language Frisian. We use a sample of code-switched, spontaneously spoken data, which proves to be a challenging setup. We propose to train a <a href="https://en.wikipedia.org/wiki/Parsing">parser</a> specifically tailored towards the target domain, by selecting instances from multiple <a href="https://en.wikipedia.org/wiki/Treebank">treebanks</a>. Specifically, we use Latent Dirichlet Allocation (LDA), with word and character N-grams. We use a deep biaffine parser initialized with mBERT. The best single source treebank (nl_alpino) resulted in an <a href="https://en.wikipedia.org/wiki/Greatest_common_divisor">LAS</a> of 54.7 whereas our data selection outperformed the single best transfer treebank and led to 55.6 <a href="https://en.wikipedia.org/wiki/Greatest_common_divisor">LAS</a> on the test data. Additional experiments consisted of removing <a href="https://en.wikipedia.org/wiki/Diacritic">diacritics</a> from our Frisian data, creating more similar training data by cropping sentences and running our best <a href="https://en.wikipedia.org/wiki/Statistical_model">model</a> using XLM-R. These experiments did not lead to a better performance.</abstract>
      <url hash="10615d45">2021.adaptnlp-1.6</url>
      <bibkey>braggaar-van-der-goot-2021-challenges</bibkey>
      <pwccode url="https://github.com/anouck96/parsingfrisian" additional="false">anouck96/parsingfrisian</pwccode>
    <title_ar>التحديات في شرح وتحليل البيانات المنطوقة والمبدلة بالشفرة والفريزية الهولندية</title_ar>
      <title_pt>Desafios na anotação e análise de dados falados, trocados por código, frísios-holandeses</title_pt>
      <title_es>Desafíos en la anotación y el análisis de datos hablados, con cambio de código y entre frisón y holandés</title_es>
      <title_zh>注与解析口语、代码切换、弗里斯兰语-荷兰语数挑战</title_zh>
      <title_ja>フリジア語-オランダ語データのコードスイッチによるアノテーションと解析の課題</title_ja>
      <title_hi>एनोटेटिंग और पार्सिंग बोली गई, कोड-स्विच्ड, फ्रिसियन-डच डेटा में चुनौतियां</title_hi>
      <title_ga>Dúshláin maidir le Anótáil agus Parsáil Sonraí Labhartha, Cóid-aistrithe, Freaslainnise-Ollainnis</title_ga>
      <title_el>Προκλήσεις σε σχολιασμό και ανάλυση ομιλούμενων, αλλαγή κώδικα, Φριζιανά-ολλανδικά δεδομένα</title_el>
      <title_ka>Name</title_ka>
      <title_it>Sfide nell'annotazione e nell'analisi dei dati parlati, commutazione di codice, Friso-Olandese</title_it>
      <title_kk>Жазбалау және талдау тізбектері, код ауыстырылған, фрис- голландша деректері</title_kk>
      <title_hu>Kihívások a beszéd jegyzetelésében és értelmezésében, kódkapcsolású, fríz-holland adatok</title_hu>
      <title_lt>Anotacijos ir analizavimo iššūkiai, kodų keitimas, prancūzų ir olandų duomenys</title_lt>
      <title_ml>കോഡ്- മാറ്റി, ഫ്രിസിഷ്യന്‍ ഡേറ്റാName</title_ml>
      <title_mt>Sfidi fl-Annotazzjoni u l-Analiżi tal-Konflitti, Kodiċi mibdula, Data Franċiża-Olandiża</title_mt>
      <title_ms>Challenges in Annotating and Parsing Spoken, Code-switched, Frisian-Dutch Data</title_ms>
      <title_no>Utfordringar i annotasjon og tolking av spoken, kodbytt, frisk- nederlandsk data</title_no>
      <title_mk>Предизвики во анатирање и анализирање на зборови, промена на код, фризиско-холандски податоци</title_mk>
      <title_mn>Аннотаци болон талбарлах хэмжээний шаардлага, Код-өөрчлөгдсөн, Фризиан-Датч өгөгдлийн шаардлага</title_mn>
      <title_sr>Izazovi u Annotating and Parsing Spoken, Code-switched, Frisian-Dutch Data</title_sr>
      <title_pl>Wyzwania w komentowaniu i analizie mówionych, przełączaniu kodu, fryzyjsko-holenderskich danych</title_pl>
      <title_si>Name</title_si>
      <title_so>Challenges in Annotation and Parsing, Cod-switched, Frisian-Dutch Data</title_so>
      <title_sv>Utmaningar i att kommentera och tolka tal, kodväxlad, frisisk-holländska data</title_sv>
      <title_ro>Provocări în adnotarea și analizarea datelor vorbite, schimbarea codului, datele frisone-olandeze</title_ro>
      <title_ta>Name</title_ta>
      <title_ur>Annotating and Parsing Spoken, Code-switched, Frisian-Dutch Data میں چالنج</title_ur>
      <title_uz>Name</title_uz>
      <title_vi>Bài thi đấu giải thích và giải thích bằng ngôn ngữ</title_vi>
      <title_bg>Предизвикателства при анотирането и анализирането на говорени, кодово-променени, фризийско-холандски данни</title_bg>
      <title_nl>Uitdagingen in Annoteren en Parsen Gesproken, Code-switched, Fries-Nederlandse data</title_nl>
      <title_hr>Izazovi u Annotating and Parsing Spoken, Code-switched, Frisian-Dutch Data</title_hr>
      <title_da>Udfordringer i notering og fortolkning af talte, kodeskiftede, frisisk-hollandske data</title_da>
      <title_de>Herausforderungen beim Annotieren und Parsen gesprochener, kodierter, friesisch-niederländischer Daten</title_de>
      <title_id>tantangan dalam Annotasi dan Analisasi Bicara, Kode-switched, Data Frisian-Belanda</title_id>
      <title_fa>challenges in Annotating and Parsing Spoken, Code-switched, Frisian-Dutch Data</title_fa>
      <title_tr>Annotating and Parsing Spoken, Kod-switched, Frisian-Dutch Data</title_tr>
      <title_sw>Changamoto katika Mjadala wa Kutangaza na Kuchapisha, Kubadilishwa kwa Code, Taarifa za UFrisian-Dutch</title_sw>
      <title_af>Opdragte in Annotating and Parsing Spoken, Code-switched, Frisian-Dutch Data</title_af>
      <title_sq>Sfidat në njoftimin dhe analizimin e të dhënave të folura, të ndryshuara me kod, të dhënave frizi-hollandeze</title_sq>
      <title_am>ምርጫዎች</title_am>
      <title_az>Annotating and Parsing Spoken, Code-switched, Frisian-Dutch Data</title_az>
      <title_hy>Խնդիրները նշում և վերլուծում խոսքերի, կոդի փոխակերպման, ֆրիսիական-հոլանդական տվյալների մեջ</title_hy>
      <title_bn>বিজ্ঞাপন এবং পার্সিং স্পুকেন, কোড- পরিবর্তন, ফ্রিসিয়ান-ডাচ তথ্যের চ্যালেঞ্জ</title_bn>
      <title_bs>Izazovi u Annotating and Parsing Spoken, Code-switched, Frisian-Dutch Data</title_bs>
      <title_ca>Els reptes en anotar i analitzar les dades parlades, canviades de codi, francés-holandeses</title_ca>
      <title_ko>구어, 코드 변환, 프리스 네덜란드어 데이터 주석과 해석의 도전</title_ko>
      <title_cs>Výzvy v komentování a analýze mluvených dat, přepínání kódu, frízsko-holandská data</title_cs>
      <title_fi>Haasteet puhuttujen merkintöjen ja analysoinnin, koodinvaihtoisten, friisialaisten ja hollantilaisten tietojen osalta</title_fi>
      <title_et>Väljakutsed räägitud, koodiga vahetatud, friisi-hollandi andmete märgistamisel ja parsimisel</title_et>
      <title_he>אתגרים בהעטפות ומחקרים מדברים, מחליפים קודים, מידע פריזי-הולנדי</title_he>
      <title_sk>Izzivi pri označevanju in razčlenjanju govorjenih, preklopljenih s kodami, frizijsko-nizozemskih podatkov</title_sk>
      <title_ha>Challenges in Annotating and Parsing Spoken, Code-switched, Frisian-Dutch Data</title_ha>
      <title_jv>Delokan Nanggang-Ngerawat lan Pansing Pikno, kode-bisa, Dong-Olayan Jejarang</title_jv>
      <title_bo>གསལ་བཤད་དང་ཞིབ་བཤེར་གྱི་ནང་དུ་ཁོང་ལ་དགོས་པ།</title_bo>
      <abstract_ar>في حين تم الحصول على أداء عالٍ للغات عالية الموارد ، فإن الأداء في اللغات منخفضة الموارد يتأخر. في هذه الورقة نركز على تحليل اللغة الفريزية منخفضة الموارد. نحن نستخدم عينة من البيانات المنطوقة تلقائيًا بتبديل الشفرة ، والتي تثبت أنها إعداد صعب. نقترح تدريب المحلل اللغوي المصمم خصيصًا نحو المجال المستهدف ، عن طريق اختيار مثيلات من بنوك شجرية متعددة. على وجه التحديد ، نستخدم تخصيص Latent Dirichlet (LDA) ، مع كلمة وحرف N-grams. نحن نستخدم محلل لغوي عميق للبيافيني تمت تهيئته بـ mBERT. نتج عن أفضل بنك شجرة أحادي المصدر (nl_alpino) معدل LAS قدره 54.7 في حين تفوق اختيارنا على البيانات على أفضل بنك شجرة واحد وأدى إلى 55.6 LAS في بيانات الاختبار. اشتملت التجارب الإضافية على إزالة علامات التشكيل من بياناتنا الفريزية ، وإنشاء بيانات تدريب أكثر تشابهًا عن طريق اقتصاص الجمل وتشغيل أفضل نموذج لدينا باستخدام XLM-R. لم تؤد هذه التجارب إلى أداء أفضل.</abstract_ar>
      <abstract_es>Si bien se ha obtenido un alto rendimiento para los lenguajes de recursos altos, el rendimiento en idiomas de bajos recursos va a la zaga. En este artículo nos centramos en el análisis del idioma frisón de bajos recursos. Utilizamos una muestra de datos de cambio de código y hablados espontáneamente, lo que demuestra ser una configuración desafiante. Proponemos entrenar un analizador específicamente diseñado para el dominio de destino, mediante la selección de instancias de varios bancos de árboles. Específicamente, utilizamos la asignación de Dirichlet latente (LDA), con N-gramas de palabras y caracteres. Utilizamos un analizador biafín profundo inicializado con mBert. El mejor banco de árboles de una sola fuente (nl_alpino) dio como resultado un LAS de 54,7, mientras que nuestra selección de datos superó al mejor banco de árboles de transferencia y condujo a 55,6 LAS en los datos de prueba. Los experimentos adicionales consistieron en eliminar los signos diacríticos de nuestros datos en frisón, crear datos de entrenamiento más similares recortando oraciones y ejecutando nuestro mejor modelo con XLM-R. Estos experimentos no condujeron a un mejor rendimiento.</abstract_es>
      <abstract_pt>Embora o alto desempenho tenha sido obtido para linguagens de alto recurso, o desempenho em linguagens de baixo recurso fica para trás. Neste artigo, focamos na análise da linguagem de poucos recursos Frisian. Usamos uma amostra de dados comutados por código e falados espontaneamente, o que prova ser uma configuração desafiadora. Propomos treinar um analisador específico para o domínio de destino, selecionando instâncias de vários bancos de árvores. Especificamente, usamos a Alocação de Dirichlet Latente (LDA), com N-grams de palavras e caracteres. Usamos um analisador biaffine profundo inicializado com mBERT. O melhor banco de árvore de fonte única (nl_alpino) resultou em um LAS de 54,7, enquanto nossa seleção de dados superou o melhor banco de árvore de transferência e levou a 55,6 LAS nos dados de teste. Experimentos adicionais consistiram em remover diacríticos de nossos dados frísios, criando dados de treinamento mais semelhantes cortando frases e executando nosso melhor modelo usando XLM-R. Esses experimentos não levaram a um melhor desempenho.</abstract_pt>
      <abstract_ja>高リソース言語では高いパフォーマンスが得られていますが、低リソース言語ではパフォーマンスが遅れています。本稿では、低資源言語フリジア語の構文解析に焦点を当てる。私たちはコードスイッチされた自発的に話されたデータのサンプルを使用しています。これは難しいセットアップであることが証明されています。複数のツリーバンクからインスタンスを選択して、ターゲットドメインに特化したパーサーをトレーニングすることを提案します。具体的には、単語と文字Nグラムを含むLatent Dirichlet Allocation (LDA)を使用します。mBERTで初期化されたディープビアフィン構文解析器を使用します。最高の単一ソースツリーバンク（ nl_alpino ）は、54.7のLASをもたらしましたが、当社のデータ選択は、単一の最高の転送ツリーバンクを上回り、テストデータの55.6 LASにつながりました。追加の実験では、フリジアのデータからダイアクリティックを削除し、文章をトリミングしてより類似したトレーニングデータを作成し、XLM - Rを使用してベストモデルを実行しました。これらの実験は、より良いパフォーマンスにはつながりませんでした。</abstract_ja>
      <abstract_zh>虽高资言性高,而低资源言性后矣。 本文中,专注低资源语弗里斯兰语解析。 吾以代码切换、自发之数示例,此一挑战性之设也。 臣等请择树库实以练专解析器。 具体来说,以潜狄利克雷分(LDA),带单词符N-gram。 一用 mBERT 初始化深双affine解析器。 最佳单源树库(nl_alpino)之LAS为54.7,而吾数择优于传输树库,并致55.6 LAS于测试数据。 其他实验删弗里斯兰数变音符,裁句创练,用XLM-R行最佳。 此实验未有善者也。</abstract_zh>
      <abstract_hi>जबकि उच्च-संसाधन भाषाओं के लिए उच्च प्रदर्शन प्राप्त किया गया है, कम-संसाधन भाषाओं पर प्रदर्शन पीछे है। इस पेपर में हम कम-संसाधन भाषा फ्रिसियन के पार्सिंग पर ध्यान केंद्रित करते हैं। हम कोड-स्विच्ड, अनायास बोले जाने वाले डेटा के नमूने का उपयोग करते हैं, जो एक चुनौतीपूर्ण सेटअप साबित होता है। हम एक पार्सर को विशेष रूप से लक्ष्य डोमेन की ओर सिलवाया प्रशिक्षित करने का प्रस्ताव करते हैं, कई ट्रीबैंक से उदाहरणों का चयन करके। विशेष रूप से, हम अव्यक्त Dirichlet आवंटन (LDA) का उपयोग करें, शब्द और चरित्र एन-ग्राम के साथ। हम mBERT के साथ शुरू किए गए एक गहरे biaffine पार्सर का उपयोग करते हैं। सर्वश्रेष्ठ एकल स्रोत ट्रीबैंक (nl_alpino) के परिणामस्वरूप 54.7 का एलएएस हुआ, जबकि हमारे डेटा चयन ने एकल सर्वश्रेष्ठ ट्रांसफर ट्रीबैंक को पछाड़ दिया और परीक्षण डेटा पर 55.6 एलएएस का नेतृत्व किया। अतिरिक्त प्रयोगों में हमारे फ्रिसियन डेटा से डायक्रिटिक्स को हटाने, वाक्यों को क्रॉप करके और एक्सएलएम-आर का उपयोग करके हमारे सबसे अच्छे मॉडल को चलाने के द्वारा अधिक समान प्रशिक्षण डेटा बनाना शामिल था। इन प्रयोगों ने बेहतर प्रदर्शन नहीं किया।</abstract_hi>
      <abstract_ga>Cé go bhfuil ardfheidhmíocht bainte amach do theangacha ard-acmhainne, tá feidhmíocht ar theangacha íseal-acmhainne chun deiridh. Sa pháipéar seo dírímid ar pharsáil na Freaslainnise Freaslainnise. Bainimid úsáid as sampla de shonraí cód-aistrithe, a labhraítear go spontáineach, rud a chruthaíonn gur socrú dúshlánach é. Tá sé beartaithe againn parsálaí a oiliúint a bheidh saindeartha don spriocfhearann, trí chásanna a roghnú ó ilchúnna crann. Go sonrach, úsáidimid Leithdháileadh Dirichlet Folaigh (LDA), le focal agus carachtar N-gram. Bainimid úsáid as parsálaí domhain biaifín inisealaithe le mBERT. Bhí SAR de 54.7 mar thoradh ar an gcruach crann aonfhoinse is fearr (nl_alpino) ach d’fheidhmigh ár rogha sonraí níos fearr ná an banc crann aistrithe aonair is fearr agus ba é an toradh a bhí air ná 55.6 LAS ar na sonraí tástála. Is éard a bhí i dturgnaimh bhreise diacritics a bhaint as ár sonraí Freaslainnise, sonraí oiliúna níos cosúla a chruthú trí abairtí a ghearradh agus ár múnla is fearr a rith ag baint úsáide as XLM-R. Ní raibh feidhmíocht níos fearr mar thoradh ar na turgnaimh seo.</abstract_ga>
      <abstract_hu>Míg a nagy erőforrásokat igénylő nyelvek nagy teljesítményét érték el, az alacsony erőforrásokat igénylő nyelvek teljesítménye lemarad. Ebben a tanulmányban az alacsony erőforrású fríz nyelv elemzésére összpontosítunk. Kódkapcsolt, spontán beszélt adatokból álló mintát használunk, ami kihívást jelent. Javasoljuk, hogy egy speciálisan a céltartományra szabott elemzőt képezzünk, több fabank példányát választva. Konkrétan a Latent Dirichlet Allocation (LDA) szót és karaktert használjuk N-grammokkal. Egy mély biaffin elemzőt használunk, amelyet mBERT-vel inicializálunk. A legjobb egyforrású treebank (nl_alpino) 54,7 LAS értéket eredményezett, míg az adatválasztásunk meghaladta az egyetlen legjobb transzfer treebank értékét és 55,6 LAS értéket eredményezett a teszt adataiban. További kísérletek a diakritikusok eltávolítása a fríz adatokból, több hasonló edzési adat létrehozása a mondatok kivágásával és a legjobb modellünk XLM-R használatával történt. Ezek a kísérletek nem eredményeztek jobb teljesítményt.</abstract_hu>
      <abstract_el>Ενώ έχουν επιτευχθεί υψηλές επιδόσεις για γλώσσες υψηλής περιεκτικότητας, οι επιδόσεις στις γλώσσες χαμηλής περιεκτικότητας παραμένουν πίσω. Σε αυτή την εργασία εστιάζουμε στην ανάλυση της χαμηλής περιεκτικότητας στη φριζιανή γλώσσα. Χρησιμοποιούμε ένα δείγμα κωδικοποιημένων, αυθόρμητα μιλημένων δεδομένων, το οποίο αποδεικνύεται μια δύσκολη ρύθμιση. Προτείνουμε να εκπαιδεύσουμε έναν αναλυτή ειδικά προσαρμοσμένο στον τομέα προορισμού, επιλέγοντας περιπτώσεις από πολλαπλές τράπεζες δέντρων. Συγκεκριμένα, χρησιμοποιούμε τη Λατινική Κατανομή Διρίκλετ (με Ν-γράμματα λέξης και χαρακτήρων). Χρησιμοποιούμε έναν βαθύ αναλυτή μπιαφίνης αρχικοποιημένο με mBERT. Η καλύτερη τράπεζα δέντρων μίας πηγής (οδήγησε σε ένα LAS 54.7 ενώ η επιλογή δεδομένων μας ξεπερνούσε την καλύτερη τράπεζα δέντρων μεταφοράς και οδήγησε σε 55.6 LAS στα δεδομένα δοκιμής. Επιπρόσθετα πειράματα συνίσταντο στην αφαίρεση των διακοπτικών από τα φριζιανά δεδομένα μας, στη δημιουργία περισσότερων παρόμοιων εκπαιδευτικών δεδομένων με περικοπή προτάσεων και την εκτέλεση του καλύτερου μοντέλου μας χρησιμοποιώντας Αυτά τα πειράματα δεν οδήγησαν σε καλύτερη απόδοση.</abstract_el>
      <abstract_ka>მაგრამ უფრო დიდი რესურსის ენათებისთვის მიღებულია, მაგრამ მარტივი რესურსის ენათებისთვის გამოსახულება. ამ დოკუნტში ჩვენ ფრისიანი ქვემოთ რესურსის ენის პანსუზაციაზე დავყენებთ. ჩვენ გამოყენებთ კოდის შეცვლა, სონტანოდ საუბრილო მონაცემების მონაცემების გამოყენება, რომელიც გამოწყენება შესაძლებელი მონაცემები. ჩვენ გვეძლევა განსხვავება პანსერტის განსაკუთრებით განსხვავებული მისაღების დიომინზე, რამდენიმე საბრძანების განსხვავებით. განსაკუთრებით, ჩვენ შევყენებთ Latent Dirichlet Allocation (LDA) სიტყვებით და სიტყვებით N- გრამით. ჩვენ გამოყენებთ ძალიან ბიფინის პანელერი, რომელიც mBERT-ით инициаლიზებულია. ყველაზე საუკეთესო მხოლოდ ერთი მხოლოდ საბეჭდო (nl_alpino) დაიწყება 54.7-ის LAS-ში, თუმცა ჩვენი მონაცემები მონიშნული მონაცემები ერთი საუკეთესო საბეჭდო საბეჭდო და დამატებული ექსპერიმენტები იყო დიაკრიტიკური ჩვენი ფრისიანი მონაცემების წაშლადან, უფრო სხვადასხვა მონაცემების შექმნა და XLM-R გამოყენებით ჩვენი უკეთესი მოდელს. ეს ექსპერიმე</abstract_ka>
      <abstract_lt>Nors daug išteklių turinčių kalbų rezultatų pasiekta, mažai išteklių turinčių kalbų rezultatai atsilieka. Šiame dokumente daugiausia dėmesio skiriame mažai išteklių turinčios friziškos kalbos analizei. We use a sample of code-switched, spontaneously spoken data, which proves to be a challenging setup.  Siūlome apmokyti analizatorių, specialiai pritaikytą tikslinei sričiai, pasirinkdami atvejus iš kelių medžių. Konkrečiai mes naudojame Latent Dirichlet Allocation (LDA) su žodžiu ir simboliu N-gramais. Naudojame gilią biffino analizatorių, inicijuotą mBERT. Geriausias vieno šaltinio medžio pagrindas (nl_alpino) lėmė 54,7 LAS, o mūsų duomenų atranka viršijo vieną geriausią perdavimo pagrindą ir 55,6 LAS bandymų duomenimis. Papildomi eksperimentai buvo diakritikų pašalinimas iš mūsų frizijos duomenų, panašių mokymo duomenų sukūrimas paspaudžiant sakinius ir naudojant XLM-R naudojamą geriausią model į. Šie eksperimentai nesukėlė geresnių rezultatų.</abstract_lt>
      <abstract_mk>И покрај тоа што се постигнати високи резултати за јазиците со високи ресурси, резултатите на јазиците со ниски ресурси се задржуваат. Во овој весник се фокусираме на анализирањето на фризискиот јазик со ниски ресурси. Користиме примерок на спонтано зборувани податоци кои се покажуваат како предизвикувачки поставување. Предлагаме да обучуваме анализатор специфично приспособен кон доменот на метата, со избор на примери од повеќе дрвја. Specifically, we use Latent Dirichlet Allocation (LDA), with word and character N-grams.  Ние користиме длабок биафински анализатор иницијализиран со mBERT. Најдобрата група на дрвја од еден извор (nl_alpino) резултираше со LAS од 54,7 додека нашиот избор на податоци ја надмина најдобрата група на дрвја од трансфер и доведе до 55,6 LAS на тестовите податоци. Дополнителни експерименти се состојуваа од отстранувањето на дијакритичарите од нашите фризиски податоци, создавање на послични податоци за обука со собирање реченици и управување со нашиот најдобар модел користејќи XLM-R. Овие експерименти не доведоа до подобра резултат</abstract_mk>
      <abstract_kk>Ресурстардың жоғары тілдері үшін жоғары істеу керек болғанда, төмен ресурс тілдерінің істеу керек. Бұл қағазда Фризияндың төмен ресурстар тілін талдау үшін көздейміз. Біз код ауыстырылған, автоматты түрде сөйлейтін деректер үлгісін қолданамыз. Бұл қиын баптау деген сияқты. Біз бірнеше орындағы мәліметтерді таңдап, мақсатты доменге өзгертілген талдаушы оқытуды ұсынамыз. Сонымен қатар, біз "Latent Dirichlet Allocation" (LDA) сөз мен N- граммалар таңбалармен қолданамыз. МБЕРТ арқылы инициализацияланған үлкен биафин талдаушысын қолданамыз. Ең жақсы жалпы көздегі требанды (nl_ alpino) 54. 7 тізімінің LAS болды, бірақ деректерді таңдағанда бір ең жақсы транспорттау требанды жасап, сынақ деректерінде 55. 6 LAS болды. Қосымша тәжірибелер біздің Фрис деректерімізден диаткритикаларды алып тастау, сөздерді қиып, XLM-R арқылы ең жақсы моделімізді орындау арқылы ұқсас оқыту деректерін құрып жатқан. Бұл тәжір</abstract_kk>
      <abstract_it>Mentre le prestazioni elevate sono state ottenute per i linguaggi ad alto contenuto di risorse, le prestazioni sui linguaggi a basso contenuto di risorse rimangono indietro. In questo articolo ci concentriamo sull'analisi della lingua frisone a basso contenuto di risorse. Usiamo un campione di dati a commutazione di codice, parlati spontaneamente, che si rivela un setup impegnativo. Proponiamo di addestrare un parser su misura per il dominio di destinazione, selezionando istanze da più treebank. Nello specifico, utilizziamo Latent Dirichlet Allocation (LDA), con parole e caratteri N-grammi. Usiamo un analizzatore di biaffine profondo inizializzato con mBERT. Il miglior treebank single source (nl_alpino) ha portato a un LAS di 54,7 mentre la nostra selezione di dati ha superato il singolo treebank di trasferimento migliore e ha portato a 55,6 LAS sui dati di test. Ulteriori esperimenti consistevano nel rimuovere i diacritici dai nostri dati frisiani, creare dati di allenamento più simili ritagliando frasi ed eseguendo il nostro modello migliore utilizzando XLM-R. Questi esperimenti non hanno portato a prestazioni migliori.</abstract_it>
      <abstract_ms>Sementara prestasi tinggi telah dicapai untuk bahasa sumber tinggi, prestasi bahasa sumber rendah tertinggal. Dalam kertas ini kita fokus pada penghuraian bahasa Frisian sumber rendah. Kami menggunakan sampel data yang ditukar-kod, yang bercakap secara spontan, yang membuktikan menjadi seting yang mencabar. Kami cadangkan untuk melatih penghurai yang disesuaikan secara khusus ke arah domain sasaran, dengan memilih contoh dari garis pokok berbilang. Secara khusus, kita gunakan Allocation Latent Dirichlet (LDA), dengan perkataan dan aksara N-gram. Kami menggunakan penghurai biaffin yang diawalkan dengan mBERT. Pangkalan pokok sumber tunggal terbaik (nl_alpino) menghasilkan LAS 54.7 semasa pemilihan data kita melebihi pangkalan pokok pemindahan tunggal terbaik dan membawa ke 55.6 LAS pada data ujian. Eksperimen tambahan terdiri daripada membuang diakritik dari data Frisian kami, mencipta data latihan yang lebih serupa dengan menguap kalimat dan menjalankan model terbaik kami menggunakan XLM-R. Eksperimen ini tidak membawa kepada prestasi yang lebih baik.</abstract_ms>
      <abstract_mn>Хэдийгээр өндөр боловсролын хэл дээр ажиллагаа гаргасан ч, бага боловсролын хэл дээр ажиллагаа үлдсэн. Энэ цаасан дээр бид Фризийн бага боловсролын хэлний талаар анхаарлаа хандуулдаг. Бид кодыг өөрчлөгдсөн, сэтгэл хөдлөл өгөгдлийн жишээ хэрэглэдэг. Энэ нь хэцүү байдал юм. Бид хэд хэдэн загваруудын жишээг сонгож зорилготой зорилготой зорилготой хэлбэрээр хуваарч сургуульд сургуульд сургаж байна. Ялангуяа бид Latent Dirichlet Allocation (LDA), N-граммын үг болон дүрсийг ашиглаж байна. Бид mBERT-тай эхлэгдсэн гүн гүнзгий биефин хуваагч ашиглаж байна. Хамгийн шилдэг эх үүсвэрийн загвар (nl_alpino) нь 54.7-ын ЛАС болсон юм. Гэхдээ бидний мэдээллийн сонголт нь хамгийн сайн шилдэг шилжүүлэлтийн загварыг дамжуулж, шалгалтын өгөгдлийн талаар 55.6 ЛАС болсон юм. Тэгээд нэмэлт туршилтууд бидний Фризийн өгөгдлийн хувьд илүү төстэй суралцах өгөгдлийг бүтээж, XLM-R-г ашиглан бидний хамгийн сайн загварын загварыг ашиглаж байдаг.</abstract_mn>
      <abstract_ml>ഉയര്‍ന്ന വിഭവങ്ങളുടെ ഭാഷകള്‍ക്ക് ഉയര്‍ന്ന പ്രദര്‍ശനം ലഭിച്ചിരിക്കുമ്പോള്‍, കുറഞ്ഞ വിഭവഭാഷകളില്‍ പ്രകടനം  ഈ പത്രത്തില്‍ ഞങ്ങള്‍ കുറഞ്ഞ വിഭവങ്ങളുടെ ഭാഷ ഫ്രിസ്യന്‍ പാര്‍ജിങ്ങിനെ ശ്രദ്ധിക്കുന്നു. നമ്മള്‍ ഒരു കോഡ് മാറ്റിയിട്ടുണ്ട്, സ്വയമായി സംസാരിക്കുന്ന ഡേറ്റാ ഉപയോഗിക്കുന്നു. അത് ഒരു വിലാല്‍ക്കാലി നമ്മള്‍ ഒരു പരിശീലിപ്പിക്കാന്‍ പ്രത്യേകിച്ച് ലക്ഷ്യത്തിലേക്ക് പ്രത്യേകിച്ച് ടോമെയിനിലേക്ക് തിരഞ്ഞെടുക്ക പ്രത്യേകിച്ച്, നമ്മള്‍ ലാറ്റെന്റ് ഡിറിച്ചില്ലെറ്റ് ഒലോക്ഷന്‍ (LDA) ഉപയോഗിക്കുന്നു. വാക്കും അക്ഷരങ്ങളും N-  നമ്മള്‍ ഒരു ആഴമുള്ള ബീഫിന്‍ പരാജയപ്രകാരം ഉപയോഗിക്കുന്നു. The best single source treebank (nl_alpino) resulted in an LAS of 54.7 whereas our data selection outperformed the single best transfer treebank and led to 55.6 LAS on the test data.  കൂടുതല്‍ പരീക്ഷണങ്ങള്‍ നമ്മുടെ ഫ്രിസ്യന്‍ വിവരങ്ങളില്‍ നിന്ന് ഡയറിക്രിക്കുന്നവരെ നീക്കം ചെയ്യുന്നതിനായിരുന്നു. വിവരങ്ങള്‍ വാങ്ങുന്നതിനാല്‍ കൂ</abstract_ml>
      <abstract_no>Mens høg utviklinga er fått for høg ressursspråk, vil utviklinga på låg ressursspråk gå bak. I denne papiret fokuserer vi på tolking av den låg ressursspråket Frisian. Vi bruker eit prøve av kodbytt, spontane snakket data, som viser å vera eit vanskeleg oppsett. Vi foreslår å trena ei tolkar spesifikke tilpassa mot måldområdet ved å velja instansar frå fleire treebankar. Spesielt bruker vi Latent Dirichlet Allocation (LDA) med ord og teikn N- gramar. Vi bruker ein dyp biaffin- tolkar som er starta med mBERT. Den beste enkelte kjeldetrebanen (nl_alpino) resulterte i ein LAS med 54,7 mens datautvalet vårt utførte den enkelte beste overføringsbanken og førte til 55,6 LAS på test data. Ekstra eksperimenter best år av å fjerna diakritikk frå våre Frisiske data, oppretta meir liknande treningsdata ved å beskjera setningar og køyra våre beste modellen med XLM-R. Desse eksperimentene førte ikkje til ein bedre utvikling.</abstract_no>
      <abstract_mt>Filwaqt li nkisbu prestazzjoni għolja għal lingwi b’riżorsi għoljin, il-prestazzjoni fuq lingwi b’riżorsi baxxi għadha lura. F’dan id-dokument niffokaw fuq l-analiżi tal-lingwa friża b’riżorsi baxxi. Aħna nużaw kampjun ta’ dejta mibdula bil-kodiċi, li titkellem b’mod spontanju, li turi li hija struttura ta’ sfida. Aħna nipproponu li nħarrġu analizzatur imfassal speċifikament lejn id-dominju fil-mira, billi nagħżlu każijiet minn għelieqi multipli tas-siġar. Speċifikament, aħna nużaw l-Allokazzjoni Latent Dirichlet (LDA), bil-kelma u l-karattru N-grammi. Aħna nużaw parser tal-biffina fond inizjalizzat b’mBERT. L-aħjar bank tas-siġar tas-sors uniku (nl_alpino) irriżulta f’LAS ta’ 54.7 filwaqt li l-għażla tad-dejta tagħna qabżet l-aħjar bank tas-siġar tat-trasferiment uniku u wasslet għal 55.6 LAS fuq id-dejta tat-test. Esperimenti addizzjonali kienu jikkonsistu fit-tneħħija tad-dijakritiċi mid-dejta Franċiża tagħna, il-ħolqien ta’ dejta ta’ taħriġ aktar simili permezz tas-sentenzi tal-għelejjel u t-tħaddim tal-a ħjar mudell tagħna bl-użu ta’ XLM-R. Dawn l-esperimenti ma wasslux għal prestazzjoni aħjar.</abstract_mt>
      <abstract_sr>Iako su dobili visoke funkcije za jezike visokog resursa, nastup na jezicima niskog resursa ostaje iza sebe. U ovom papiru fokusiramo se na analizu jezika niskog resursa Frisiana. Koristimo uzorak zamjene šifre, spontano govorenih podataka, koji dokazuje da je izazovna postavka. Predlažemo da treniramo analizatora posebno prilagođenog prema ciljnom domenu, birajući instance iz višestrukih treebana. Posebno, koristimo Latent Dirichlet Allocation (LDA), sa rijeèima i karakterom N-grama. Koristimo duboki analizator biafina inicijalizovan sa mBERT-om. Najbolji jedinstveni izvor treeban (nl_alpino) rezultirao je LAS od 54,7, dok je naš izbor podataka izneo jedinstveni najbolji treeban i doveo do 55,6 LAS na test podataka. Dodatni eksperimenti su sastavljeni od uklanjanja diakritika iz naših Frisijskih podataka, stvaranja sličnih podataka za obuku usvajajući rečenice i vodeći naš najbolji model koristeći XLM-R. Ovi eksperimenti nisu doveli do boljih izvođenja.</abstract_sr>
      <abstract_si>උත්සන්ධ භාෂාව සඳහා උත්සන්ධ භාෂාව ලැබෙනවා නමුත්, අඩුම භාෂා භාෂාව අඩුම භාෂාව පි මේ පත්තරේ අපි ප්‍රශ්න භාෂාව අඩු ප්‍රශ්න භාෂාව විශ්වාස කරනවා. අපි කෝඩ් ස්විච්ච් කරපු නිර්මාණයක් පාවිච්චි කරනවා, ස්වයංගයෙන් කතා කරපු දත්ත, ඒක ප්‍රශ්න විශා අපි සැකසුම් කරනවා විශේෂයෙන් විශේෂයෙන් ලක්ෂණ ඩෝමින් වලට පරීක්ෂණය කරන්න, විශේෂයෙන් විශේෂයෙන් ස විශේෂයෙන්ම, අපි ලෙට්ට් ඩිරිච්ලෙට් අන්තිමාණය (LDA) පාවිච්චි කරනවා, N- ග්‍රාම්ස් වචන සහ අක්ෂර අපි ම්බෙර්ට් එක්ක පටන් ගත්ත ගොඩක් බියාෆින් විශේෂකයක් භාවිත කරනවා. හොඳම ප්‍රමාණයක් ත්‍රීබැන්ක් (nl_alpino) පරීක්ෂණ දත්තේ ලැස් 54.7 වලින් පරීක්ෂණය කරලා තියෙනවා නමුත් අපේ දත්ත තෝරණය පරීක්ෂණය කරන එකම හොඳම අතර පරීක්ෂණය සම්පූර්ණයෙන් අපේ ප්‍රිසියාන් දත්තෙන් පිළිගන්න බැරි විදිහට පරීක්ෂණ දත්ත සිද්ධ වුණා, ප්‍රශ්ණ දත්ත සිද්ධ ව</abstract_si>
      <abstract_pl>Podczas gdy uzyskano wysoką wydajność języków o wysokim zasobie, wydajność języków o niskim zasobie pozostaje w tyle. W niniejszym artykule skupiamy się na parsowaniu niskich zasobów języka fryzyjskiego. Wykorzystujemy próbkę przełączonych kodem, spontanicznie mówionych danych, co okazuje się trudną konfiguracją. Proponujemy szkolenie parsera specjalnie dopasowanego do domeny docelowej, wybierając instancje z wielu bank drzew. W szczególności używamy Latent Dirichlet Allocation (LDA), z N-gramami słowa i znaków. Używamy głębokiego parsera biafiny zainicjowanego mBERT. Najlepszy pojedynczy źródłowy bank drzew (nl_alpino) zaowocował LAS 54.7, podczas gdy nasz wybór danych przewyższył pojedynczy najlepszy bank drzew transferowych i doprowadził do 55.6 LAS na danych testowych. Dodatkowe eksperymenty polegały na usunięciu diakrytyki z naszych danych fryzyjskich, tworzeniu bardziej podobnych danych treningowych poprzez przycinanie zdań i uruchomieniu najlepszego modelu przy użyciu XLM-R. Eksperymenty te nie doprowadziły do lepszej wydajności.</abstract_pl>
      <abstract_so>Inta lagu helo muuqasho dheer oo luuqadaha sare ee luqadaha hoose ee rasmiga ah, waxyaabaha lagu sameeyo waxaa dib looga dhigaa. Warqadan waxaynu ku fiirsanaynaa baarlamaanka luqada Finnishka ee hoose. Waxaynu isticmaalnaa sameynta codsiga, oo si rasmi ah looga hadlo, taasoo caddaynaya inay tahay hab adag. Waxaynu soo jeedaynaa in aan ku tababarinno lambarada si gaar ah loogu talagalay guriga goalka, si aan u dooranno tusaalooyin kala duduwan qoraalka. Si gaar ah, waxaynu ugu isticmaalnaa Latent Dirichlet Allocation (LDA), hadal iyo xaraf N-gram. Waxaynu isticmaalnaa baaritaanka mool dheer oo lagu billaabiyey mBERT. Tan ugu wanaagsanayd treebank (nl_alpino) waxay sababtay LAS oo ka mid ah 54.7, iyadoo doorashadeedii macluumaadkayagu ay sameyn jireenka ugu wanaagsan ee treebank waxaana u keenay 55.6 LAS oo ku saabsan data imtixaanka. Imtixaanka dheeraadka ah waxaa ka mid ah dhaqdhaqaalaha macluumaadkayaga Frisian, waxayna sameyn jirrabaadyo la mid ah oo ka mid ah xafiiska beeraha iyo dhaqdhaqaalaha sameynta modelkeena ugu wanaagsan ee isticmaalaya XLM-R. Imtixaankaas ma uu sababin wax ka wanaagsan.</abstract_so>
      <abstract_ta>அதிக மூலத்தின் மொழிகளுக்கான அதிக செயல்பாடு பெற்றது இந்த காகிதத்தில் நாம் குறைந்த மூலத்தின் பாக்கியத்தை கவனம் செலுத்துகிறோம். We use a sample of code-switched, spontaneously spoken data, which proves to be a challenging setup.  இலக்கு களத்திற்கு குறிப்பிட்ட ஒரு பொருளை பயிற்சி செய்ய நாம் பரிந்துரைக்கிறோம், பல மூன்று கோடுகளில் இருந்து ந குறிப்பிட்டால், நாம் சொல்லும் எழுத்தும் N- கிராமுடன் அலங்காரத்தை பயன்படுத்துகிறோம். MBERT உடன் துவக்கப்பட்ட ஒரு ஆழமான பியாபின் பகுதியை பயன்படுத்துகிறோம். சிறந்த ஒற்றை மூலம் treebank (nl_ alpino) 54. 7 ல் ஒரு LAS வெளியேற்றினார். ஆனால் எங்கள் தரவு தேர்வு சிறந்த ஒற்றை மாற்று treebank செய்து சோதனை தரவில் 55. 6 LAS ஆக்க கூடுதல் சோதனைகள் எங்கள் ஃப்ரிசியன் தரவிலிருந்து டையாக்ரியன்களை நீக்குதல் இருந்தது, விளைச்சுட்டு வாக்கியங்களை உருவாக்கி எக்ஸ்எல்எம்- R பயன்படுத்</abstract_ta>
      <abstract_ro>În timp ce au fost obținute performanțe ridicate pentru limbile cu resurse ridicate, performanța pe limbile cu resurse reduse rămâne în urmă. În această lucrare ne concentrăm pe analizarea limbii frisone cu resurse reduse. Folosim un eșantion de date cu comutare de cod, vorbite spontan, ceea ce se dovedește a fi o configurație dificilă. Vă propunem să instruiți un parser special adaptat domeniului țintă, prin selectarea instanțelor din mai multe brake-uri. Mai exact, folosim Latent Dirichlet Allocation (LDA), cu cuvinte și caractere N-grame. Folosim un parser de biafine profund initializat cu mBERT. Cel mai bun treebank single source (nl_alpino) a rezultat într-un LAS de 54,7 în timp ce selecția noastră de date a depășit cel mai bun transfer treebank unic și a dus la 55,6 LAS pe datele de testare. Experimentele suplimentare au constat în eliminarea diacritică din datele noastre frisone, crearea mai multor date de antrenament similare prin decuparea frazelor și rularea celui mai bun model al nostru folosind XLM-R. Aceste experimente nu au dus la o performanță mai bună.</abstract_ro>
      <abstract_sv>Även om hög prestanda har uppnåtts för språk med hög resurs släpar prestandan på språk med låg resurs efter. I denna uppsats fokuserar vi på tolkningen av lågresursspråket frisiska. Vi använder ett urval av kodväxlade, spontant talade data, vilket visar sig vara en utmanande installation. Vi föreslår att träna en parser specifikt anpassad för måldomänen, genom att välja instanser från flera trädbanker. Specifikt använder vi Latent Dirichlet Allocation (LDA), med ord och tecken N-gram. Vi använder en djup biaffin parser initierad med mBERT. Den bästa singel source treebank (nl_alpino) resulterade i en LAS på 54,7 medan vårt dataval överträffade singel best transfer treebank och ledde till 55,6 LAS på testdata. Ytterligare experiment bestod av att ta bort diakritiker från våra frisiska data, skapa mer liknande träningsdata genom att beskära meningar och köra vår bästa modell med XLM-R. Dessa experiment ledde inte till en bättre prestanda.</abstract_sv>
      <abstract_ur>اچھی طرح عمدہ کامپیوتر بالا سروسیس زبانوں کے لئے حاصل کیا گیا ہے، کم سروسیس زبانوں کے پیچھے رہ جاتا ہے. اس کاغذ میں ہم کم سرمایہ کی زبان فریزین کے بارے میں تمرکز کرتے ہیں۔ ہم ایک نمونہ کا استعمال کرتے ہیں کوڈ-سوئٹ، اسپانیٹ سے بول دیے گئے ہیں، جو ایک مشکل سٹاپ ہے۔ ہم ایک پارچر کی ترینس کرنا چاہتے ہیں جو مخصوص طریقے سے موقع ڈومین کی طرف پھیلائی جاتی ہے، بہت سی ٹریب بانک سے موقعیتیں انتخاب کرتی ہیں۔ خاص طور پر، ہم Latent Dirichlet Allocation (LDA) کا استعمال کرتے ہیں، کلمات اور شخصت N-grams کے ساتھ۔ ہم نے mBERT کے ساتھ شروع کی ایک عمیق بیفن پارچر استعمال کیا۔ The best source treebank (nl_alpino) resulted in a LAS of 54.7 whereas our data selection outperformed the single best transfer treebank and led to 55.6 LAS on the test data. اور اضافہ آزمائش کی وجہ سے ہمارے فریس ڈیٹا سے دیاکریکٹی کو ہٹانے کے لئے تھا، کلمات کاٹنے کے ذریعہ اور ہمارے بہترین نمڈل کو XLM-R کے ذریعہ دوڑانے کے ذریعہ زیادہ برابر تربین ڈیٹا بناتے تھے. یہ آزمائش اچھی عمل</abstract_ur>
      <abstract_uz>Chunki juda katta manbalar tillari uchun bajarishga ega bo'lganda, kam manbaning tillarida bajarishni bajaradi. In this paper we focus on the parsing of the low-resource language Frisian.  Biz avtomatik gapiradigan maʼlumotlardan foydalanamiz. Bu muammolar moslamasi mumkin. Biz bir nechta treeborni tanlash uchun qo'shilgan kompyuterni taʼminlashni talab qilamiz. Koʻrsatilgan, biz Yaqinda ochilgan Dirichlet kompyuterdan (LDA), so'z va belgi N- gram bilan foydalanamiz. Biz mBERT bilan boshlangan juda qiyin biaffin parameterdan foydalanamiz. Eng eng eng eng yaxshi manba treebank (nl_ alpino) 54. 7 yordamida boshqa maʼlumot tanlanganmiz bir eng eng eng eng yaxshi koʻpaytirilgan treebank va sinab maʼlumot bilan 55. 6 LAS ga erishildi. Koʻproq tajribalar Frisiy маълумотларимиздан diakritiklarni olib tashlashdir, bir xil taʼminlov maʼlumotni o'rganish va XLM-R yordamida eng yaxshi modelmizni ishga tushirish mumkin. Bu imtiyozlar yaxshi bajarishga sababdi.</abstract_uz>
      <abstract_vi>Một số hiệu suất cao dành cho các ngôn ngữ giàu có, nhưng khả năng ngôn ngữ ít tài nguyên vẫn chậm trễ. Trong tờ giấy này chúng ta tập trung vào việc phân tích ngôn ngữ trù phú Frisian. Chúng tôi sử dụng một mẫu dữ liệu đã được mã hóa thay đổi, tự động nói ra, một thiết lập đầy thử thách. Chúng tôi đề nghị huấn luyện một phân tách đặc biệt về miền đích, bằng cách chọn các trường hợp từ đa dạng ba bóng. Cụ thể, chúng tôi dùng Latent Dirichhlet Allocation (LDAP), with word and character N-grams. Chúng tôi dùng phân tách cà phê lát được khởi tạo bằng mBERT. The best single source treeback (nl*u alpino) đã dẫn đến một LAS of 45.7 trong khi chúng tôi đã chọn dữ liệu chỉ ra duy nhất một lần truyền giá và dẫn đến 5005.6 LAS trên dữ liệu thí nghiệm. Các thí nghiệm khác bao gồm việc loại bỏ Diacritics khỏi dữ liệu Frisian, tạo ra dữ liệu đào tạo tương tự bằng việc xén câu và chạy mô hình tốt nhất của chúng ta bằng XLM-R. Những thí nghiệm này không mang lại hiệu quả tốt hơn.</abstract_vi>
      <abstract_hr>Iako je napravljena visoka učinkovitost na jezicima visokih resursa, učinkovitost na jezicima niskih resursa ostaje iza nje. U ovom papiru fokusiramo se na analizu jezika niskog resursa Frisianca. Koristimo uzorak zamjene šifre, spontano govorenih podataka, koji dokazuje da je izazovna postavka. Predlažemo trenirati analizatora posebno prilagođenog prema ciljnom domenu, birajući instancije iz višestrukih područja. Posebno, koristimo Latent Dirichlet Allocation (LDA), s riječima i karakterom N-grama. Koristimo duboki analizator biafina inicijaliziran s mBERT-om. Najbolji jedinstveni izvor treeban (nl_alpino) rezultirao je LAS od 54,7, dok je naš izbor podataka iznosio jedinstveni najbolji prijenos treeban i doveo do 55,6 LAS na testne podatke. Dodatni eksperimenti su sastavljeni od uklanjanja dijamanata iz naših Frisijskih podataka, stvaranja sličnih podataka za obuku prijevozom rečenica i vodeći naš najbolji model koristeći XLM-R. Ovi eksperimenti nisu doveli do boljih učinka.</abstract_hr>
      <abstract_da>Mens der er opnået høj ydeevne for sprog med høj ressource, er ydeevnen på sprog med lav ressource bagud. I denne artikel fokuserer vi på tolkningen af det lave ressourcesprog frisisk. Vi bruger en prøve af kodekoblingte, spontant talte data, hvilket viser sig at være en udfordrende opsætning. Vi foreslår at træne en fortolker specifikt skræddersyet til måldomænet ved at vælge forekomster fra flere træbanker. Specielt bruger vi Latent Dirichlet Allocation (LDA), med ord og tegn N-gram. Vi bruger en dyb biaffin parser initialiseret med mBERT. Den bedste single source treebank (nl_alpino) resulterede i en LAS på 54,7, mens vores datavalg overgik den enkelte bedste transfer treebank og førte til 55,6 LAS på testdata. Yderligere eksperimenter bestod i at fjerne diakritikere fra vores frisiske data, skabe mere lignende træningsdata ved at beskære sætninger og køre vores bedste model ved hjælp af XLM-R. Disse eksperimenter førte ikke til en bedre præstation.</abstract_da>
      <abstract_bg>Въпреки че е постигната висока производителност за езици с висок ресурс, производителността на езици с нисък ресурс изостава. В тази статия се фокусираме върху анализирането на нискоресурсния език фризийски. Използваме пример от кодово превключени, спонтанно говорени данни, което се оказва предизвикателна настройка. Предлагаме да се обучи анализатор, специално пригоден за целевия домейн, като се избират инстанции от множество дървесни ленти. По-конкретно, ние използваме латентно разпределение на дириклет (ЛДА), с дума и знак Н-грама. Използваме дълбок биафинов анализатор инициализиран с mBERT. Най-добрата единична дървесна банка (Нл_алпино) доведе до LAS от 54.7, докато нашият избор на данни надмина най-добрата единична дървесна банка за трансфер и доведе до 55.6 LAS на тестовите данни. Допълнителните експерименти се състояха в премахване на диакритиката от фризийските ни данни, създаване на повече подобни данни за обучение чрез изрязване на изречения и стартиране на най-добрия ни модел с помощта на Тези експерименти не доведоха до по-добро представяне.</abstract_bg>
      <abstract_nl>Hoewel er hoge prestaties zijn behaald voor talen met veel resources, blijven de prestaties op talen met weinig resources achter. In dit artikel richten we ons op het parsen van de low-resource taal Fries. We gebruiken een voorbeeld van code-switched, spontaan gesproken data, wat een uitdagende opstelling blijkt te zijn. We stellen voor om een parser te trainen die specifiek is afgestemd op het doeldomein, door instances uit meerdere boombanken te selecteren. Specifiek gebruiken we Latent Dirichlet Allocation (LDA), met woord en teken N-grammen. We gebruiken een diepe biaffine parser geïnitialiseerd met mBERT. De beste single source boombank (nl_alpino) resulteerde in een LAS van 54.7 terwijl onze dataselectie de beste transferboombank overtrof en leidde tot 55.6 LAS op de testdata. Aanvullende experimenten bestonden uit het verwijderen van diacritici uit onze Friese data, het creëren van meer vergelijkbare trainingsdata door zinnen bij te snijden en het uitvoeren van ons beste model met XLM-R. Deze experimenten leidden niet tot een betere prestatie.</abstract_nl>
      <abstract_de>Während hohe Leistung für ressourcenintensive Sprachen erzielt wurde, hinkt die Leistung bei ressourcenarmen Sprachen hinterher. In diesem Beitrag konzentrieren wir uns auf das Parsen der ressourcenarmen Sprache Friesisch. Wir verwenden ein Beispiel von code-geschalteten, spontan gesprochenen Daten, was sich als herausforderndes Setup erweist. Wir schlagen vor, einen Parser zu trainieren, der speziell auf die Zieldomäne zugeschnitten ist, indem Instanzen aus mehreren Baumbänken ausgewählt werden. Insbesondere verwenden wir Latent Dirichlet Allocation (LDA), mit Wort- und Zeichen-N-Gramm. Wir verwenden einen tiefen Biaffinparser, der mit mBERT initialisiert wurde. Die beste Single Source Treebank (nl_alpino) führte zu einem LAS von 54.7, während unsere Datenauswahl die beste Single Transfer Treebank übertraf und zu 55.6 LAS auf den Testdaten führte. Weitere Experimente bestanden darin, Diakritiken aus unseren friesischen Daten zu entfernen, mehr ähnliche Trainingsdaten durch Zuschneiden von Sätzen zu erstellen und unser bestes Modell mit XLM-R auszuführen. Diese Experimente führten nicht zu einer besseren Leistung.</abstract_de>
      <abstract_id>Sementara prestasi tinggi telah diperoleh untuk bahasa sumber daya tinggi, prestasi bahasa sumber daya rendah tertinggal. Dalam kertas ini kita fokus pada penghuraian bahasa Frisian sumber daya rendah. We use a sample of code-switched, spontaneously spoken data, which proves to be a challenging setup.  Kami mengusulkan untuk melatih parser khusus disesuaikan menuju domain sasaran, dengan memilih contoh dari beberapa batang pohon. Secara spesifik, kita menggunakan Allokasi Latent Dirichlet (LDA), dengan kata dan karakter N-gram. Kami menggunakan parser biaffin dalam yang diinisialisasikan dengan mBERT. Pangkalan pohon sumber tunggal terbaik (nl_alpino) menghasilkan LAS 54,7 sementara pemilihan data kami melebihi panggkalan pohon transfer terbaik tunggal dan membawa ke 55,6 LAS pada data tes. Eksperimen tambahan terdiri dari menghapus diakritik dari data Frisian kami, menciptakan data pelatihan yang lebih mirip dengan menambah kalimat dan menjalankan model terbaik kami menggunakan XLM-R. Eksperimen ini tidak menyebabkan prestasi yang lebih baik.</abstract_id>
      <abstract_fa>در حالی که عملکرد بالا برای زبانهای منابع بالا دریافت شده است، عملکرد روی زبانهای منابع کم باقی مانده است. در این کاغذ ما روی تجزیه کردن زبان کم منبع فریزین تمرکز می کنیم. ما از نمونه‌ای از داده‌های تغییر کد استفاده می‌کنیم، که ثابت می‌کند یک تنظیم مشکل است. ما پیشنهاد می‌کنیم که یک بازیگر ویژه‌ای را به سمت دامنه هدف آموزش دهیم، با انتخاب نمونه‌ها از بسته‌های متعدد درخت‌ها. به طور خاصی، ما با کلمه و شخصیت N-گرم استفاده می‌کنیم از تقسیم دیریکلت Latent (LDA). ما از یک تجزیه‌کننده‌ی بیفاین عمیق استفاده می‌کنیم که با mBERT شروع شده است. بهترین ترکیب ترکیب منبع تنها (nl_alpino) به نتیجه یک LAS از 54.7 به وجود آورد، در حالی که انتخاب داده‌های ما بهترین ترکیب ترکیب ترکیب تنها را برداشت و به 55.6 LAS در داده‌های آزمایش برداشت. آزمایشات اضافه از حذف دیاکریک‌ها از داده‌های فریسی ما بودند، و داده‌های آموزش مشابه‌تر از جمله‌های جمع کردن جمله‌ها و اجرای بهترین مدل‌های ما با استفاده از XLM-R. این آزمایشات به انجام بهتر رهبری نکردند.</abstract_fa>
      <abstract_sw>Wakati mafanikio makubwa yamekuwa yakipatikana kwa lugha za rasilimali za juu, utendaji wa lugha za rasilimali zilizobaki. Katika karatasi hii tunajikita kwenye kuimba lugha ya KiFrisia yenye rasilimali duni. Tunatumia mifano ya kubadilishwa kwa kodi, taarifa zinazozungumzwa kwa wenyewe, ambayo inaonyesha kuwa ni seti ya changamoto. Tunazipendekeza kuwafundisha mchambuzi hasa unaoongozwa kuelekea eneo la lengo, kwa kuchagua matukio kutoka kwenye viwanja vingi vya mitatu. Kwa ujumla, tunatumia Umoja wa Kusini wa Dirichlet (LDA), kwa maneno na tabia ya N-grams. Tunatumia mchambuzi wa kina wa biaffine ulioanzishwa na mBERT. Chanzo bora zaidi cha treebank (nl_alpino) kilisababisha LAS ya 54.7 wakati uchaguzi wetu wa takwimu ulifanya usafirishaji bora zaidi wa mitebank na ulipelekea LAS 55.6 kwenye takwimu za jaribio. Majaribio mengine yalikuwa ni pamoja na kuondoa wagonjwa kutoka kwenye takwimu zetu za KiFrisia, kutengeneza takwimu za mafunzo kama hizo kwa kutengeneza mifano yetu bora kwa kutumia XLM-R.</abstract_sw>
      <abstract_ko>고자원 언어는 이미 고성능을 얻었지만 저자원 언어의 성능은 뒤떨어졌다.본고는 주로 저자원 언어인 프리시안의 문법 분석을 연구한다.우리는 코드 전환, 자발적으로 나온 데이터 샘플을 사용했는데 이것은 도전적인 설정임을 증명한다.여러 개의 트리 라이브러리에서 실례를 선택해서 목표 영역에 대한 맞춤형 해상도를 훈련하는 것을 권장합니다.구체적으로 말하면, 우리는 잠재적인 디릭레 분배 (LDA) 와 단어와 문자의 N-gram을 사용한다.우리는 mBERT로 초기화된 심비아핀 해상도를 사용합니다.최적 단원 트리 라이브러리(nl alpino)의 LAS는 54.7이고 우리의 데이터 선택은 단일 최적 이동 트리 라이브러리보다 우수하며 테스트 데이터의 LAS는 55.6이다.다른 실험으로는 프리스식 데이터에서 변음 기호를 삭제하고 문장을 재단하여 비슷한 훈련 데이터를 만들고 XLM-R로 우리의 가장 좋은 모델을 실행하는 것이 포함된다. 이 실험들은 더 좋은 성능을 가져오지 못했다.</abstract_ko>
      <abstract_tr>Yüksek ressurs dilleri üçin ýokary ukyp edildi, iň-çeşme dilleriniň yzynda täsirler bar. Bu kagyzda biz Frisiýanyň iň köp ukyp dilini çözmek üçin üns berýäris. Biz köd üýtgedilmiş, spontaz gepleşilen maglumatyň örnekini ulanýarys. Bu kynçylyk düzümlenmesi üçin kanıtlaýar. Biz birden çoklu çubuktan örnekler seçmek üzere özellikle hedef domenya geçirilen bir ayıran öwrenmesini teklif ediyoruz. Adatça, biz Later Dirichlet Allocation (LDA), söz we karakter N-gramler bilen ullanyrys Biz mBERT bilen başlanýan bir çukur biafin çözümlerini ulanýarys. Iň gowy bir çeşme gatlaky (nl_alpino) 54.7 ýagdaýynda LAS sebäpli boldy. Maglumat saýlawymyz ýeke iň gowy gatlaky gatlakyny üstün etdi we testiň maglumatynda 55.6 LAS giddi. Ekstra deneyler Frisiýa maglumatlarymyzdan diakritikalary çykarmak bolupdyr, sözlerimizi kesip we iň gowy nusgasymyzy XLM-R ulanyp daşary çykarmak bilen meňzeşdirdi. Bu deneyler gowy bir hereket etmäge ýok edip bilmedi.</abstract_tr>
      <abstract_sq>Ndërsa paraqitja e lartë është arritur për gjuhët me burime të larta, paraqitja në gjuhët me burime të ulta mbetet prapa. Në këtë letër ne përqëndrohemi në analizimin e gjuhës me burime të ulëta friziane. We use a sample of code-switched, spontaneously spoken data, which proves to be a challenging setup.  Ne propozojmë të trajnojmë një analizues specifikisht të përshtatur drejt domenisë objektive, duke zgjedhur raste nga vende të shumta pemësh. Specifically, we use Latent Dirichlet Allocation (LDA), with word and character N-grams.  Ne përdorim një analizues biffine të thellë të inicializuar me mBERT. Banka më e mirë e një burimi (nl_alpino) rezultoi në një LAS 54.7 ndërsa zgjedhja jonë e të dhënave kaloi bankën më të mirë të transferimit dhe çoi në 55.6 LAS në të dhënat e testit. Eksperimente shtesë përbënin heqjen e diakritikëve nga të dhënat tona friziane, krijimin e të dhënave më të ngjashme të trajnimit duke mbledhur fjalë dhe duke drejtuar modelin tonë më të mirë duke përdorur XLM-R. Këto eksperimente nuk shpien në një performancë më të mirë.</abstract_sq>
      <abstract_af>Terwyl hoë prestasie vir hoë-hulpbronne tale ontvang is, het prestasie op lae-hulpbronne tale agter verlaat. In hierdie papier fokus ons op die verwerking van die lae hulpbron taal Frisian. Ons gebruik 'n voorbeeld van kode-geskuif, spontaneël gepraat data, wat bevestig dat 'n pragtige opstelling is. Ons voorstel om 'n analyseer spesifieke na die doel domein te trein deur voorbeelde te kies van veelvuldige treebanks. Spesifieke, ons gebruik Latent Dirichlet Allocation (LDA), met woord en karakter N- grame. Ons gebruik 'n diep biaffine ontwerker wat geïnisialiseer is met mBERT. Die beste enkele bron treebank (nl_alpino) het resultaat in 'n LAS van 54.7 terwyl ons data keuse uitgevoer het die enkele beste oordrag treebank en gelei na 55.6 LAS op die toets data. Addisionele eksperimente het bestuur van die verwyder van diakrities van ons Frisiese data, skep meer gelyke onderwerp data deur die kruip van teikens en die bestuur van ons beste model gebruik van XLM-R. Hierdie eksperimente het nie na 'n beter uitvoering gelei nie.</abstract_af>
      <abstract_am>ከፍተኛ የክፍለ ሀብት ቋንቋዎች ሲያገኙ፣ የዝናብ ቋንቋዎች ፍላጎት በኋላ ነው፡፡ በዚህ ገጽ የዋናው የፍሪሳውያንን ቋንቋ ማዘጋጀት ላይ እናስማማታለን፡፡ የኮድ ተለወጠን ምሳሌ እናስቀምጣለን፡፡ በተለየ አካባቢ አካላቢ ዶሜን በመምረጥ የተመሳሳይ ምርጫዎችን በመምረጥ እናሳውቃለን፡፡ በተለያይነት፣ Latent Dirichlet Allocation (LDA), በቃል እና በ-graph እናስቀምጣለን፡፡ በ mBERT የተጀመረ ጥልቅ የቢፊን ምርጫዎች እናስቀምጣለን፡፡ The best single source treebank (nl_alpino) resulted in an LAS of 54.7 whereas our data selection outperformed the single best transfer treebank and led to 55.6 LAS on the test data.  ጨዋታ ፈተናዎች ከፍሪሳዊ ዳታዎችን ለማስወግድ፣ እንደዚህ ብጤ የተሰናከረውን አስተማሪ ዳታዎችን በመፍጠር እና በXLM-R በመጠቀም የተሻለ ሞዴላዎችን ለመፈለግ ነው፡፡</abstract_am>
      <abstract_hy>Մինչդեռ բարձր արտադրողականությունը հասել է բարձր ռեսուրսների լեզուների համար, ցածր ռեսուրսների լեզուների արտադրողականությունը հետաքրքիր է: Այս թղթի մեջ մենք կենտրոնանում ենք ցածր ռեսուրսների լեզվի վերլուծության վրա: Մենք օգտագործում ենք կոդի փոխակերպված, ինքնաբուխ խոսվող տվյալների նմուշ, որը պարզվում է դժվար կառուցվածք է: Մենք առաջարկում ենք դասակարգչի ուսուցանումը, որն առանձնահատուկ է նպատակային բնագավառի ուղղությամբ, ընտրելով բազմաթիվ ծառերից ստացված օրինակներ: Մասնավորապես, մենք օգտագործում ենք Վերջին դիրիկլետի (ԼԴԱ) բառերի և N-գրամանների հետ: Մենք օգտագործում ենք mBER-ի միջոցով հիմնված խորը բիաֆինի վերլուծողը: Ամենալավ մեկ աղբյուրի ծառի հիմքը (nl_AlPino) հանգեցրեց 54.7-ի LAS-ին, մինչդեռ մեր տվյալների ընտրությունը գերազանցեց ամենալավ փոխանցման ծառի հիմքը և հանգեցրեց 55.6 LAS-ին փորձարկման տվյալների վրա: Ավելի փորձարկումներ կազմակերպեցին մեր ֆրիզիացի տվյալներից դիաքնրիկներին հեռացնելը, ավելի նմանատիպ ուսուցման տվյալներ ստեղծելով վերբերյալ նախադասությունները և մեր լավագույն մոդելը XLM-R օգտագործելով: Այս փորձարկումները ավելի լավ արդ</abstract_hy>
      <abstract_bn>যখন উচ্চ সম্পদের ভাষার জন্য উচ্চভাষা পাওয়া গেছে, তখন কম সম্পদ ভাষায় প্রদর্শন করা হয়েছে। এই পত্রিকায় আমরা নীচের সম্পদ ভাষা ফ্রিসিয়ানের পার্গিং দিয়ে মনোযোগ দিচ্ছি। আমরা কোড পরিবর্তনের উদাহরণ ব্যবহার করি, স্বয়ংক্রিয়ভাবে কথা বলা তথ্য ব্যবহার করি, যা একটি চ্যালেঞ্জের ব্যবস্থা প্রমাণ করে। আমরা বিশেষ করে লক্ষ্য ডোমেইনের দিকে একটি প্রশিক্ষণ প্রশিক্ষণ দিতে প্রস্তাব করছি, বেশ কয়েকটি ত্রিব্যাংক থেকে অনুষ্ঠিত হয়েছে। বিশেষ করে, আমরা ল্যাটেন্ট ডিরিচেলেট অ্যালোকেশন (এলডিএ) ব্যবহার করি, শব্দ এবং অক্ষর এন-গ্রাম দিয়ে। আমরা একটি গভীর বিয়াফিন প্যারেজার ব্যবহার করি যা এমবের্টের সাথে শুরু করা হয়েছে। সবচেয়ে ভালো সূত্র ট্রিবাঙ্ক (এনএল_আলপিনো) এর ফলে ৫৪. ৭ সালের একটি ল্যাসের ফলে আমাদের তথ্য নির্বাচনের মধ্যে সবচেয়ে ভালো ট্রেইব্যান্সফার্নারে আরো পরীক্ষার মধ্যে রয়েছে আমাদের ফ্রিসিয়ান ডাটা থেকে ডায়ারিকারীদের সরিয়ে নেয়ার জন্য, তারা বিভিন্ন ধরনের প্রশিক্ষণের তথ্য তৈরি করে এবং এক্সএলএম-</abstract_bn>
      <abstract_az>Yüksek performans yüksək kaynaqlar dillərinə verilən halda, düşük kaynaqlar dillərində performans geri qalar. Bu kağızda biz düşük ressurs dilini Frisian dilinin ayırmasına odaklanırıq. Biz kodu dəyişdirilmiş, spontane danışmış məlumatların nümunələrini istifadə edirik, bu isə çətin bir qurğu göstərir. Biz müəyyən edilmiş məqsəd domeinə təhsil edilən bir parçacın təhsil etməyi təklif edirik, çoxlu a ğaç çubuqlarından örnəkləri seçərək. Biz Latent Dirichlet Allocation (LDA) sözləri və karakterləri N-gramləri ilə istifadə edirik. Biz mBERT ilə başlanğıçlı bir biafin parçacısını istifadə edirik. Ən yaxşısı tək mənbə çubuğu (nl_alpino) 54.7-lik LAS olaraq gəldi, amma məlumatlarımız seçilməsi tək təkrar təkrar çubuğunu təkrar etdi və sınama məlumatlarında 55.6 LAS təkrar etdi. XLM-R vasitəsilə ən yaxşı modellərimizi istifadə etmək üçün daha çox bənzər təhsil məlumatları yaratmaq və XLM-R vasitəsilə ən yaxşı modellərimizi istifadə etmək məqsədilə idi. Bu experimentlər daha yaxşı təhsil etməyə yol vermədi.</abstract_az>
      <abstract_cs>Zatímco u jazyků s vysokými zdroji bylo dosaženo vysokého výkonu, výkon u jazyků s nízkými zdroji zaostává. V tomto článku se zaměřujeme na analýzu nízkoprostrojového jazyka fríštiny. Používáme vzorek kódově přepínaných, spontánně mluvených dat, což se ukáže jako náročné nastavení. Navrhujeme trénovat parser speciálně přizpůsobený cílové doméně výběrem instancí z více stromových bank. Konkrétně používáme Latent Dirichlet Allocation (LDA), s N-gramy slova a znaků. Používáme hluboký biafinový parser inicializovaný mBERT. Nejlepší single source stromová banka (nl_alpino) vyústila v LAS 54.7, zatímco náš výběr dat předčil nejlepší přenosovou stromovou banku a vedl k 55.6 LAS na testovacích datech. Další experimenty spočívaly v odstranění diakritiky z frízských dat, vytvoření podobnějších tréninkových dat oříznutím vět a spuštění našeho nejlepšího modelu pomocí XLM-R. Tyto experimenty nevedly k lepšímu výkonu.</abstract_cs>
      <abstract_et>Kuigi suure ressursiga keelte puhul on saavutatud suur jõudlus, jääb vähese ressursiga keelte puhul maha. Käesolevas töös keskendume parsimisele madala ressursiga keele friisi keel. Me kasutame koodiga vahetatud spontaanselt kõnelevate andmete näidist, mis osutub keeruliseks seadistuseks. Me teeme ettepaneku koolitada spetsiaalselt sihtdomeenile kohandatud parser, valides eksemplarid mitmest puupunktist. Täpsemalt kasutame Latent Dirichlet Allocation (LDA), sõna- ja märgiga N-grammid. Me kasutame sügavat biafiini parserit, mis on initsialiseeritud mBERT-ga. Parima ühe allika puupanga (nl_alpino) tulemuseks oli LAS 54,7, samas kui meie andmete valik ületas ühe parima ülekande puupanga ja viis 55,6 LAS testi andmetel. Täiendavad eksperimendid hõlmasid diakriitika eemaldamist meie friisi andmetest, sarnasemate treeningandmete loomist lausete kärpimise teel ja parima mudeli kasutamist XLM-R abil. Need eksperimendid ei viinud parema tulemuseni.</abstract_et>
      <abstract_ca>Tot i que s'han aconseguit alts resultats en llengües d'alt recurso, els resultats en llengües de baix recurso es retarden. En aquest article ens centrem en l'analització del francès amb baix recursos. Utilitzem una mostra de dades parlades espontàniament canviades de codi, que resulta ser una configuració difícil. We propose to train a parser specifically tailored towards the target domain, by selecting instances from multiple treebanks.  En concret, utilitzem l'Allocació Latent Dirichlet (LDA), amb paraula i caràcter N-grams. Utilitzem un analitzador biaffin profund inicializat amb mBERT. El millor banc d'arbres d'una sola font (nl_alpino) va resultar en un LAS de 54,7 mentre que la nostra selecció de dades va superar el millor banc d'arbres de transfer ència i va portar a 55,6 LAS en les dades de prova. Els experiments adicionals van consistir en eliminar diacrítics de les nostres dades frises, crear dades de formació més similars recollint frases i executant el nostre millor model fent servir XLM-R. Aquests experiments no van portar a millor rendiment.</abstract_ca>
      <abstract_fi>Vaikka suuriresurssisten kielten suorituskyky on saavutettu, vähäresurssisten kielten suorituskyky on jäljessä. Tässä artikkelissa keskitymme jäsentämiseen vähäresurssinen kieli friisi. Käytämme koodinvaihtoista, spontaanisti puhuttua dataa, joka osoittautuu haastavaksi kokoonpanoksi. Ehdotamme, että koulutetaan kohdeverkkotunnukselle räätälöity jäsentäjä valitsemalla esiintymiä useista puupankeista. Erityisesti käytämme Latent Dirichlet Allocation (LDA), jossa on sana ja merkki N-grammia. Käytämme syvää biafiininparseria, joka on alustettu mBERT:llä. Paras yhden lähteen puupankki (nl_alpino) tuotti LAS:n 54,7, kun taas meidän tietovalikoimamme ylitti yksittäisen parhaan siirtopuupankin ja johti 55,6 LAS:iin testitiedoissa. Lisäkokeet koostuivat diakriitikkojen poistamisesta friisilaisista tiedoista, samankaltaisten harjoitustietojen luomisesta lauseita leikkaamalla ja parhaan mallimme ajamisesta XLM-R:llä. Nämä kokeet eivät johtaneet parempaan suorituskykyyn.</abstract_fi>
      <abstract_bs>Iako je napravljena visoka učinkovitost za jezike visokog resursa, nastup na jezicima niskog resursa ostaje iza sebe. U ovom papiru fokusiramo se na analizu jezika niskog resursa Frisianca. Koristimo uzorak zamjene šifre, spontano govorenih podataka, koji dokazuje da je izazovna postavka. Predlažemo da treniramo analizatora posebno prilagođenog prema ciljnom domenu, birajući instance iz višestrukih područja. Posebno, koristimo Latent Dirichlet Allocation (LDA), sa riječima i karakterom N-grama. Koristimo duboki analizator biafina inicijaliziran sa mBERT-om. Najbolji jedinstveni izvor treeban (nl_alpino) rezultirao je LAS od 54,7, dok je naš izbor podataka iznosio jedinstveni najbolji prevoz treeban i doveo do 55,6 LAS na testne podatke. Dodatni eksperimenti su sastavljeni od uklanjanja diakritika iz naših Frijskih podataka, stvaranja sličnih podataka obuke usvajanjem rečenica i vodeći naš najbolji model koristeći XLM-R. Ovi eksperimenti nisu doveli do boljih izvođenja.</abstract_bs>
      <abstract_jv>ssoffpolite"), and when there is a change ("assertive Nang pepul iki, kita dipunduh langkung urip nggambar kelas Jejaran Awak dhéwé nggunakake sampler kanggo kode bisa-bisa dadi, dadi mesthi o nggambar barang seneng pisan. We proposal to vlan a PASSMENDAR PASSMENDAR PASSMENDAR IGNOPAL PASSMENDAR olar" is the abbreviation for "Line", Col is the abbreviation for "Column Two Label Ndheke supaya karo hal-hal sing ngrusak diakritik tentang ning awak dhéwé dolanan ping dolanan, nggawe data sing luwih nggawe dolanan nggawe kesempatan kanggo ngwala macem nggawe model sing luwih nggambar XLM-R. Perintah sing iki dadi sing bisa pasar awak dhéwé.</abstract_jv>
      <abstract_ha>Akwai da za'a iya samar da sauri wa harshen masu sarki, aiki na bakin harshen-wuri-resource. Ga wannan karatun, Munã fokus wa paring of the lower-resource language Frisian. Tuna yi amfani da wani misali da aka canza kodi, da aka yi magana farat ɗaya, da za'a iya kasa zama tsarin mai tsõratar. Tunamaɗa mu kõre wani parser wanda aka ƙayyade shi zuwa duk aka goa, kuma za mu zãɓi misãlai daga bakin turu masu yawa. A ƙayyade, Munã yi amfani da Allocation na Naƙasan Dirikla (LDA), da maganar da takardar N-gram. Tuna amfani da wani parse mai ƙari wa biffine wanda aka fara da mBERT. Tan da mafi kyaun source trebank (nl_alpino) ta ƙara wata MAS na 54.7 alhãli kuwa zaɓallinmu na zaɓe ta kowacan transfer ta Treebank kuma ya zaɓi 55.6 lass a kan data ta jarraba. Wata jarrabo na ƙaranci na sami da ta tafiyar da diagon daga data masu Frisian, suna samun data masu kama da kwatanku da za'a yi danganta da cire-garwaya kuma ya yi tafiyar da misãlai masu kyãwo da amfani da XLM-R. Wannan jarrabõ ba ta ƙara wani mafiya tsari ba.</abstract_ha>
      <abstract_sk>Medtem ko je bila dosežena visoka zmogljivost jezikov z visokimi viri, zmogljivost jezikov z nizkimi viri zaostaja. V tem prispevku se osredotočamo na razčlenitev jezika z nizkimi viri frizijščine. Uporabljamo vzorec spontano govorjenih podatkov, ki se izkažejo za zahtevno nastavitev. Predlagamo usposabljanje razčlenjevalnika, ki je posebej prilagojen ciljni domeni, z izbiro primerkov iz več drevesnih zbirk. Natančneje uporabljamo Latent Dirichlet Allocation (LDA), z besedami in znaki N-grami. Uporabljamo globok biafin razčlenjevalec inicializiran z mBERT. Najboljša enovirna drevesna plošča (nl_alpino) je imela LAS 54,7, medtem ko je naša izbira podatkov presegla najboljšo prenosno drevesno ploščo in privedla do 55,6 LAS na testnih podatkih. Dodatni eksperimenti so vključevali odstranitev diakritikov iz frizijskih podatkov, ustvarjanje podobnih podatkov o treningu z obrezovanjem stavkov in izvajanje našega najboljšega modela z uporabo XLM-R. Ti eksperimenti niso privedli do boljše zmogljivosti.</abstract_sk>
      <abstract_he>While high performance have been obtained for high-resource languages, performance on low-resource languages lags behind.  בעיתון הזה אנו מתמקדים במחקר של שפת משאבים נמוכים פריזית. אנחנו משתמשים בדגימה של נתונים שנחלפו קודים, שדיברו באופן ספונטני, מה שמוכיח להיות התקנה מאתגרת. אנו מציעים לאמן מעבד מתוכנן במיוחד לכיוון תחום המטרה, על ידי לבחור מקרים ממספר עצים. במיוחד, אנו משתמשים בהחלטת דיריקלט לאנט (LDA), עם מילה ודמות N-גרם. אנחנו משתמשים במחקר ביאפין עמוק שנתחיל עם mBERT. בנק העץ המקור היחיד הטוב ביותר (nl_alpino) הוביל לאס.אס של 54.7 בזמן שבבחירת הנתונים שלנו עברה את בנק העץ היחיד הטוב ביותר והוביל ל-55.6 לאס.אס על הנתונים המבחנים. ניסויים נוספים כוללו להסיר מחתונים מהנתונים הפריסיים שלנו, ליצור נתונים אימונים דומים יותר על ידי גיבוי משפטים ולפעיל את המודל הטוב ביותר שלנו באמצעות XLM-R. ניסויים אלה לא הובילו להופעה טובה יותר.</abstract_he>
      <abstract_bo>རྒྱུ་དངོས་ཐོག ང་ཚོས་ཤོག་བྱང་འདིའི་ནང་དུ་རྒྱ་ནག་མི་མང་ཆེ་བའི་སྐད་རིགས་ཕྱོགས་སྟོན་པ་ཚོར་བློ་གཏོང་ནི་ ང་ཚོས་མཚོན་རྟགས་ལ་བསྒྱུར་བཅོས་བྱས་པའི་མིག་གྲངས་ཀྱི་དཔེ་བརྗོད་ཞིག་སྤྱོད་ཀྱི་ཡོད། We propose to train a parser specifically tailored towards the target domain, by selecting instances from multiple treebanks. དམིགས་འཛུགས་ཀྱིས། ང་ཚོས་Latent Dirichlet Allocation (LDA)དང་ཐ་སྙད་དང་ཡིག་འབྲུ་N-gramདང་བེད་སྤྱོད་པ We use a deep biaffine parser initialized with mBERT. གསལ་ཤོག་མའི་ཐོག་མའི་འཇུག་སྣོད་གསལ་པོ(nl_alpino)དེ་འདྲ་བཤུ་ཐུབ་པ་ཡིན། ང་ཚོའི་གནས་སྡུད་འདེམས་ཀྱིས་གསལ་བཤད་ཀྱི་གནས་སྟངས་མང་ཤོས་མའི་འཇུག་སྣོད དབྱེ་ཚིག་གི་བརྟག་དཔྱད་ཆ་རྣམས་ང་ཚོའི་Frisian གནད་སྡུད་ཕྱིར་འཐེན་བྱས་ན་ཏེ།</abstract_bo>
      </paper>
    <paper id="9">
      <title>Addressing Zero-Resource Domains Using Document-Level Context in Neural Machine Translation</title>
      <author><first>Dario</first><last>Stojanovski</last></author>
      <author><first>Alexander</first><last>Fraser</last></author>
      <pages>80–93</pages>
      <abstract>Achieving satisfying performance in <a href="https://en.wikipedia.org/wiki/Machine_translation">machine translation</a> on domains for which there is no training data is challenging. Traditional supervised domain adaptation is not suitable for addressing such zero-resource domains because it relies on in-domain parallel data. We show that when in-domain parallel data is not available, access to document-level context enables better capturing of domain generalities compared to only having access to a single sentence. Having access to more information provides a more reliable domain estimation. We present two document-level Transformer models which are capable of using large context sizes and we compare these <a href="https://en.wikipedia.org/wiki/Computer_simulation">models</a> against strong Transformer baselines. We obtain improvements for the two zero-resource domains we study. We additionally provide an analysis where we vary the amount of context and look at the case where in-domain data is available.</abstract>
      <url hash="063fdfa1">2021.adaptnlp-1.9</url>
      <bibkey>stojanovski-fraser-2021-addressing</bibkey>
    </paper>
    <paper id="12">
      <title>BERTologiCoMix : How does Code-Mixing interact with Multilingual BERT?<fixed-case>BERT</fixed-case>ologi<fixed-case>C</fixed-case>o<fixed-case>M</fixed-case>ix: How does Code-Mixing interact with Multilingual <fixed-case>BERT</fixed-case>?</title>
      <author><first>Sebastin</first><last>Santy</last></author>
      <author><first>Anirudh</first><last>Srinivasan</last></author>
      <author><first>Monojit</first><last>Choudhury</last></author>
      <pages>111–121</pages>
      <abstract>Models such as mBERT and XLMR have shown success in solving Code-Mixed NLP tasks even though they were not exposed to such text during pretraining. Code-Mixed NLP models have relied on using <a href="https://en.wikipedia.org/wiki/Synthetic_genomics">synthetically generated data</a> along with <a href="https://en.wikipedia.org/wiki/Natural_language_processing">naturally occurring data</a> to improve their performance. Finetuning mBERT on such <a href="https://en.wikipedia.org/wiki/Data_(computing)">data</a> improves it’s code-mixed performance, but the benefits of using the different types of Code-Mixed data are n’t clear. In this paper, we study the impact of <a href="https://en.wikipedia.org/wiki/Finetuning">finetuning</a> with different types of code-mixed data and outline the changes that occur to the <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> during such <a href="https://en.wikipedia.org/wiki/Finetuning">finetuning</a>. Our findings suggest that using naturally occurring code-mixed data brings in the best performance improvement after <a href="https://en.wikipedia.org/wiki/Finetuning">finetuning</a> and that <a href="https://en.wikipedia.org/wiki/Finetuning">finetuning</a> with any type of code-mixed text improves the responsivity of it’s attention heads to code-mixed text inputs.</abstract>
      <url hash="34e7e485">2021.adaptnlp-1.12</url>
      <bibkey>santy-etal-2021-bertologicomix</bibkey>
    </paper>
    <paper id="13">
      <title>Locality Preserving Loss : Neighbors that Live together, Align together</title>
      <author><first>Ashwinkumar</first><last>Ganesan</last></author>
      <author><first>Francis</first><last>Ferraro</last></author>
      <author><first>Tim</first><last>Oates</last></author>
      <pages>122–139</pages>
      <abstract>We present a locality preserving loss (LPL) that improves the alignment between vector space embeddings while separating uncorrelated representations. Given two pretrained embedding manifolds, LPL optimizes a model to project an <a href="https://en.wikipedia.org/wiki/Embedding">embedding</a> and maintain its local neighborhood while aligning one <a href="https://en.wikipedia.org/wiki/Manifold">manifold</a> to another. This reduces the overall size of the dataset required to align the two in <a href="https://en.wikipedia.org/wiki/Task_(project_management)">tasks</a> such as crosslingual word alignment. We show that the LPL-based alignment between input vector spaces acts as a <a href="https://en.wikipedia.org/wiki/Regularization_(mathematics)">regularizer</a>, leading to better and consistent accuracy than the baseline, especially when the size of the training set is small. We demonstrate the effectiveness of LPL-optimized alignment on semantic text similarity (STS), natural language inference (SNLI), multi-genre language inference (MNLI) and cross-lingual word alignment (CLA) showing consistent improvements, finding up to 16 % improvement over our baseline in lower resource settings.</abstract>
      <url hash="7ff3f115">2021.adaptnlp-1.13</url>
      <bibkey>ganesan-etal-2021-locality</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
    </paper>
    <paper id="15">
      <title>Trajectory-Based Meta-Learning for Out-Of-Vocabulary Word Embedding Learning</title>
      <author><first>Gordon</first><last>Buck</last></author>
      <author><first>Andreas</first><last>Vlachos</last></author>
      <pages>146–155</pages>
      <abstract>Word embedding learning methods require a large number of occurrences of a word to accurately learn its embedding. However, out-of-vocabulary (OOV) words which do not appear in the training corpus emerge frequently in the smaller downstream data. Recent work formulated OOV embedding learning as a few-shot regression problem and demonstrated that <a href="https://en.wikipedia.org/wiki/Meta-learning">meta-learning</a> can improve results obtained. However, the <a href="https://en.wikipedia.org/wiki/Algorithm">algorithm</a> used, model-agnostic meta-learning (MAML) is known to be unstable and perform worse when a large number of gradient steps are used for parameter updates. In this work, we propose the use of Leap, a meta-learning algorithm which leverages the entire trajectory of the learning process instead of just the beginning and the end points, and thus ameliorates these two issues. In our experiments on a benchmark OOV embedding learning dataset and in an extrinsic evaluation, Leap performs comparably or better than MAML. We go on to examine which contexts are most beneficial to learn an OOV embedding from, and propose that the choice of contexts may matter more than the meta-learning employed.</abstract>
      <url hash="86928266">2021.adaptnlp-1.15</url>
      <bibkey>buck-vlachos-2021-trajectory</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/wikitext-103">WikiText-103</pwcdataset>
    </paper>
    <paper id="17">
      <title>An Empirical Study of Compound PCFGs<fixed-case>PCFG</fixed-case>s</title>
      <author><first>Yanpeng</first><last>Zhao</last></author>
      <author><first>Ivan</first><last>Titov</last></author>
      <pages>166–171</pages>
      <abstract>Compound probabilistic context-free grammars (C-PCFGs) have recently established a new state of the art for phrase-structure grammar induction. However, due to the high <a href="https://en.wikipedia.org/wiki/Time_complexity">time-complexity</a> of chart-based representation and inference, it is difficult to investigate them comprehensively. In this work, we rely on a fast implementation of C-PCFGs to conduct evaluation complementary to that of (CITATION). We highlight three key findings : (1) C-PCFGs are data-efficient, (2) C-PCFGs make the best use of global sentence-level information in preterminal rule probabilities, and (3) the best configurations of C-PCFGs on <a href="https://en.wikipedia.org/wiki/English_language">English</a> do not always generalize to morphology-rich languages.</abstract>
      <url hash="b99b0c8f">2021.adaptnlp-1.17</url>
      <bibkey>zhao-titov-2021-empirical</bibkey>
      <pwccode url="https://github.com/zhaoyanpeng/cpcfg" additional="true">zhaoyanpeng/cpcfg</pwccode>
    </paper>
    <paper id="20">
      <title>Effective Distant Supervision for Temporal Relation Extraction</title>
      <author><first>Xinyu</first><last>Zhao</last></author>
      <author><first>Shih-Ting</first><last>Lin</last></author>
      <author><first>Greg</first><last>Durrett</last></author>
      <pages>195–203</pages>
      <abstract>A principal barrier to training temporal relation extraction models in new domains is the lack of varied, high quality examples and the challenge of collecting more. We present a <a href="https://en.wikipedia.org/wiki/Methodology">method</a> of automatically collecting distantly-supervised examples of temporal relations. We scrape and automatically label event pairs where the temporal relations are made explicit in text, then mask out those explicit cues, forcing a <a href="https://en.wikipedia.org/wiki/Statistical_model">model</a> trained on this <a href="https://en.wikipedia.org/wiki/Data">data</a> to learn other signals. We demonstrate that a pre-trained Transformer model is able to transfer from the weakly labeled examples to human-annotated benchmarks in both zero-shot and few-shot settings, and that the masking scheme is important in improving generalization.</abstract>
      <url hash="97258218">2021.adaptnlp-1.20</url>
      <bibkey>zhao-etal-2021-effective</bibkey>
      <pwccode url="https://github.com/xyz-zy/xdomain-temprel" additional="true">xyz-zy/xdomain-temprel</pwccode>
    </paper>
    <paper id="21">
      <title>Zero-Shot Cross-Lingual Dependency Parsing through Contextual Embedding Transformation</title>
      <author><first>Haoran</first><last>Xu</last></author>
      <author><first>Philipp</first><last>Koehn</last></author>
      <pages>204–213</pages>
      <abstract>Linear embedding transformation has been shown to be effective for zero-shot cross-lingual transfer tasks and achieve surprisingly promising results. However, cross-lingual embedding space mapping is usually studied in static word-level embeddings, where a space transformation is derived by aligning representations of translation pairs that are referred from dictionaries. We move further from this line and investigate a contextual embedding alignment approach which is sense-level and dictionary-free. To enhance the quality of the <a href="https://en.wikipedia.org/wiki/Map_(mathematics)">mapping</a>, we also provide a deep view of properties of contextual embeddings, i.e., the <a href="https://en.wikipedia.org/wiki/Anisotropy">anisotropy problem</a> and its solution. Experiments on zero-shot dependency parsing through the concept-shared space built by our embedding transformation substantially outperform state-of-the-art methods using multilingual embeddings.</abstract>
      <url hash="063c51f4">2021.adaptnlp-1.21</url>
      <bibkey>xu-koehn-2021-zero</bibkey>
      <pwccode url="https://github.com/fe1ixxu/ZeroShot-CrossLing-Parsing" additional="false">fe1ixxu/ZeroShot-CrossLing-Parsing</pwccode>
    </paper>
    <paper id="22">
      <title>Gradual Fine-Tuning for Low-Resource Domain Adaptation</title>
      <author><first>Haoran</first><last>Xu</last></author>
      <author><first>Seth</first><last>Ebner</last></author>
      <author><first>Mahsa</first><last>Yarmohammadi</last></author>
      <author><first>Aaron Steven</first><last>White</last></author>
      <author><first>Benjamin</first><last>Van Durme</last></author>
      <author><first>Kenton</first><last>Murray</last></author>
      <pages>214–221</pages>
      <abstract>Fine-tuning is known to improve NLP models by adapting an initial <a href="https://en.wikipedia.org/wiki/Statistical_model">model</a> trained on more plentiful but less domain-salient examples to data in a target domain. Such <a href="https://en.wikipedia.org/wiki/Domain_adaptation">domain adaptation</a> is typically done using one stage of <a href="https://en.wikipedia.org/wiki/Fine-tuning">fine-tuning</a>. We demonstrate that gradually fine-tuning in a multi-step process can yield substantial further gains and can be applied without modifying the <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> or learning objective.</abstract>
      <url hash="84f585ac">2021.adaptnlp-1.22</url>
      <bibkey>xu-etal-2021-gradual</bibkey>
      <pwccode url="https://github.com/fe1ixxu/Gradual-Finetune" additional="false">fe1ixxu/Gradual-Finetune</pwccode>
    </paper>
    <paper id="25">
      <title>Semantic Parsing of Brief and Multi-Intent Natural Language Utterances</title>
      <author><first>Logan</first><last>Lebanoff</last></author>
      <author><first>Charles</first><last>Newton</last></author>
      <author><first>Victor</first><last>Hung</last></author>
      <author><first>Beth</first><last>Atkinson</last></author>
      <author><first>John</first><last>Killilea</last></author>
      <author id="fei-liu-utdallas"><first>Fei</first><last>Liu</last></author>
      <pages>255–262</pages>
      <abstract>Many military communication domains involve rapidly conveying situation awareness with few words. Converting natural language utterances to logical forms in these domains is challenging, as these utterances are brief and contain multiple intents. In this paper, we present a first effort toward building a weakly-supervised semantic parser to transform brief, multi-intent natural utterances into logical forms. Our findings suggest a new projection and reduction method that iteratively performs projection from natural to canonical utterances followed by reduction of natural utterances is the most effective. We conduct extensive experiments on two military and a general-domain dataset and provide a new baseline for future research toward accurate parsing of multi-intent utterances.</abstract>
      <url hash="b17dcdb4">2021.adaptnlp-1.25</url>
      <bibkey>lebanoff-etal-2021-semantic</bibkey>
    </paper>
    </volume>
</collection>