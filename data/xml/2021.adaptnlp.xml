<collection id="2021.adaptnlp">
  <volume id="1" ingest-date="2021-04-19">
    <meta>
      <booktitle>Proceedings of the Second Workshop on Domain Adaptation for NLP</booktitle>
      <editor><first>Eyal</first><last>Ben-David</last></editor>
      <editor><first>Shay</first><last>Cohen</last></editor>
      <editor><first>Ryan</first><last>McDonald</last></editor>
      <editor><first>Barbara</first><last>Plank</last></editor>
      <editor><first>Roi</first><last>Reichart</last></editor>
      <editor><first>Guy</first><last>Rotman</last></editor>
      <editor><first>Yftah</first><last>Ziser</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Kyiv, Ukraine</address>
      <month>April</month>
      <year>2021</year>
    </meta>
    <frontmatter>
      <url hash="7c3fd8ad">2021.adaptnlp-1.0</url>
      <bibkey>adaptnlp-2021-domain</bibkey>
    </frontmatter>
    <paper id="6">
      <title>Challenges in Annotating and Parsing Spoken, Code-switched, <fixed-case>F</fixed-case>risian-<fixed-case>D</fixed-case>utch Data</title>
      <author><first>Anouck</first><last>Braggaar</last></author>
      <author><first>Rob</first><last>van der Goot</last></author>
      <pages>50&#8211;58</pages>
      <abstract>While high performance have been obtained for high-resource languages, performance on low-resource languages lags behind. In this paper we focus on the parsing of the low-resource language Frisian. We use a sample of code-switched, spontaneously spoken data, which proves to be a challenging setup. We propose to train a parser specifically tailored towards the target domain, by selecting instances from multiple treebanks. Specifically, we use Latent Dirichlet Allocation (LDA), with word and character N-grams. We use a deep biaffine parser initialized with mBERT. The best single source treebank (nl_alpino) resulted in an LAS of 54.7 whereas our data selection outperformed the single best transfer treebank and led to 55.6 LAS on the test data. Additional experiments consisted of removing diacritics from our Frisian data, creating more similar training data by cropping sentences and running our best model using XLM-R. These experiments did not lead to a better performance.</abstract>
      <url hash="10615d45">2021.adaptnlp-1.6</url>
      <bibkey>braggaar-van-der-goot-2021-challenges</bibkey>
      <pwccode url="https://github.com/anouck96/parsingfrisian" additional="false">anouck96/parsingfrisian</pwccode>
    </paper>
    <paper id="9">
      <title>Addressing Zero-Resource Domains Using Document-Level Context in Neural Machine Translation</title>
      <author><first>Dario</first><last>Stojanovski</last></author>
      <author><first>Alexander</first><last>Fraser</last></author>
      <pages>80&#8211;93</pages>
      <abstract>Achieving satisfying performance in machine translation on domains for which there is no training data is challenging. Traditional supervised domain adaptation is not suitable for addressing such zero-resource domains because it relies on in-domain parallel data. We show that when in-domain parallel data is not available, access to document-level context enables better capturing of domain generalities compared to only having access to a single sentence. Having access to more information provides a more reliable domain estimation. We present two document-level Transformer models which are capable of using large context sizes and we compare these models against strong Transformer baselines. We obtain improvements for the two zero-resource domains we study. We additionally provide an analysis where we vary the amount of context and look at the case where in-domain data is available.</abstract>
      <url hash="063fdfa1">2021.adaptnlp-1.9</url>
      <bibkey>stojanovski-fraser-2021-addressing</bibkey>
    </paper>
    <paper id="12">
      <title><fixed-case>BERT</fixed-case>ologi<fixed-case>C</fixed-case>o<fixed-case>M</fixed-case>ix: How does Code-Mixing interact with Multilingual <fixed-case>BERT</fixed-case>?</title>
      <author><first>Sebastin</first><last>Santy</last></author>
      <author><first>Anirudh</first><last>Srinivasan</last></author>
      <author><first>Monojit</first><last>Choudhury</last></author>
      <pages>111&#8211;121</pages>
      <abstract>Models such as mBERT and XLMR have shown success in solving Code-Mixed NLP tasks even though they were not exposed to such text during pretraining. Code-Mixed NLP models have relied on using synthetically generated data along with naturally occurring data to improve their performance. Finetuning mBERT on such data improves it&#8217;s code-mixed performance, but the benefits of using the different types of Code-Mixed data aren&#8217;t clear. In this paper, we study the impact of finetuning with different types of code-mixed data and outline the changes that occur to the model during such finetuning. Our findings suggest that using naturally occurring code-mixed data brings in the best performance improvement after finetuning and that finetuning with any type of code-mixed text improves the responsivity of it&#8217;s attention heads to code-mixed text inputs.</abstract>
      <url hash="34e7e485">2021.adaptnlp-1.12</url>
      <bibkey>santy-etal-2021-bertologicomix</bibkey>
    </paper>
    <paper id="13">
      <title>Locality Preserving Loss: Neighbors that Live together, Align together</title>
      <author><first>Ashwinkumar</first><last>Ganesan</last></author>
      <author><first>Francis</first><last>Ferraro</last></author>
      <author><first>Tim</first><last>Oates</last></author>
      <pages>122&#8211;139</pages>
      <abstract>We present a locality preserving loss (LPL) that improves the alignment between vector space embeddings while separating uncorrelated representations. Given two pretrained embedding manifolds, LPL optimizes a model to project an embedding and maintain its local neighborhood while aligning one manifold to another. This reduces the overall size of the dataset required to align the two in tasks such as crosslingual word alignment. We show that the LPL-based alignment between input vector spaces acts as a regularizer, leading to better and consistent accuracy than the baseline, especially when the size of the training set is small. We demonstrate the effectiveness of LPL-optimized alignment on semantic text similarity (STS), natural language inference (SNLI), multi-genre language inference (MNLI) and cross-lingual word alignment (CLA) showing consistent improvements, finding up to 16% improvement over our baseline in lower resource settings.</abstract>
      <url hash="7ff3f115">2021.adaptnlp-1.13</url>
      <bibkey>ganesan-etal-2021-locality</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
    </paper>
    <paper id="15">
      <title>Trajectory-Based Meta-Learning for Out-Of-Vocabulary Word Embedding Learning</title>
      <author><first>Gordon</first><last>Buck</last></author>
      <author><first>Andreas</first><last>Vlachos</last></author>
      <pages>146&#8211;155</pages>
      <abstract>Word embedding learning methods require a large number of occurrences of a word to accurately learn its embedding. However, out-of-vocabulary (OOV) words which do not appear in the training corpus emerge frequently in the smaller downstream data. Recent work formulated OOV embedding learning as a few-shot regression problem and demonstrated that meta-learning can improve results obtained. However, the algorithm used, model-agnostic meta-learning (MAML) is known to be unstable and perform worse when a large number of gradient steps are used for parameter updates. In this work, we propose the use of Leap, a meta-learning algorithm which leverages the entire trajectory of the learning process instead of just the beginning and the end points, and thus ameliorates these two issues. In our experiments on a benchmark OOV embedding learning dataset and in an extrinsic evaluation, Leap performs comparably or better than MAML. We go on to examine which contexts are most beneficial to learn an OOV embedding from, and propose that the choice of contexts may matter more than the meta-learning employed.</abstract>
      <url hash="86928266">2021.adaptnlp-1.15</url>
      <bibkey>buck-vlachos-2021-trajectory</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/wikitext-103">WikiText-103</pwcdataset>
    </paper>
    <paper id="17">
      <title>An Empirical Study of Compound <fixed-case>PCFG</fixed-case>s</title>
      <author><first>Yanpeng</first><last>Zhao</last></author>
      <author><first>Ivan</first><last>Titov</last></author>
      <pages>166&#8211;171</pages>
      <abstract>Compound probabilistic context-free grammars (C-PCFGs) have recently established a new state of the art for phrase-structure grammar induction. However, due to the high time-complexity of chart-based representation and inference, it is difficult to investigate them comprehensively. In this work, we rely on a fast implementation of C-PCFGs to conduct evaluation complementary to that of (CITATION). We highlight three key findings: (1) C-PCFGs are data-efficient, (2) C-PCFGs make the best use of global sentence-level information in preterminal rule probabilities, and (3) the best configurations of C-PCFGs on English do not always generalize to morphology-rich languages.</abstract>
      <url hash="b99b0c8f">2021.adaptnlp-1.17</url>
      <bibkey>zhao-titov-2021-empirical</bibkey>
      <pwccode url="https://github.com/zhaoyanpeng/cpcfg" additional="true">zhaoyanpeng/cpcfg</pwccode>
    </paper>
    <paper id="20">
      <title>Effective Distant Supervision for Temporal Relation Extraction</title>
      <author><first>Xinyu</first><last>Zhao</last></author>
      <author><first>Shih-Ting</first><last>Lin</last></author>
      <author><first>Greg</first><last>Durrett</last></author>
      <pages>195&#8211;203</pages>
      <abstract>A principal barrier to training temporal relation extraction models in new domains is the lack of varied, high quality examples and the challenge of collecting more. We present a method of automatically collecting distantly-supervised examples of temporal relations. We scrape and automatically label event pairs where the temporal relations are made explicit in text, then mask out those explicit cues, forcing a model trained on this data to learn other signals. We demonstrate that a pre-trained Transformer model is able to transfer from the weakly labeled examples to human-annotated benchmarks in both zero-shot and few-shot settings, and that the masking scheme is important in improving generalization.</abstract>
      <url hash="97258218">2021.adaptnlp-1.20</url>
      <bibkey>zhao-etal-2021-effective</bibkey>
      <pwccode url="https://github.com/xyz-zy/xdomain-temprel" additional="true">xyz-zy/xdomain-temprel</pwccode>
    </paper>
    <paper id="21">
      <title>Zero-Shot Cross-Lingual Dependency Parsing through Contextual Embedding Transformation</title>
      <author><first>Haoran</first><last>Xu</last></author>
      <author><first>Philipp</first><last>Koehn</last></author>
      <pages>204&#8211;213</pages>
      <abstract>Linear embedding transformation has been shown to be effective for zero-shot cross-lingual transfer tasks and achieve surprisingly promising results. However, cross-lingual embedding space mapping is usually studied in static word-level embeddings, where a space transformation is derived by aligning representations of translation pairs that are referred from dictionaries. We move further from this line and investigate a contextual embedding alignment approach which is sense-level and dictionary-free. To enhance the quality of the mapping, we also provide a deep view of properties of contextual embeddings, i.e., the anisotropy problem and its solution. Experiments on zero-shot dependency parsing through the concept-shared space built by our embedding transformation substantially outperform state-of-the-art methods using multilingual embeddings.</abstract>
      <url hash="063c51f4">2021.adaptnlp-1.21</url>
      <bibkey>xu-koehn-2021-zero</bibkey>
      <pwccode url="https://github.com/fe1ixxu/ZeroShot-CrossLing-Parsing" additional="false">fe1ixxu/ZeroShot-CrossLing-Parsing</pwccode>
    </paper>
    <paper id="22">
      <title>Gradual Fine-Tuning for Low-Resource Domain Adaptation</title>
      <author><first>Haoran</first><last>Xu</last></author>
      <author><first>Seth</first><last>Ebner</last></author>
      <author><first>Mahsa</first><last>Yarmohammadi</last></author>
      <author><first>Aaron Steven</first><last>White</last></author>
      <author><first>Benjamin</first><last>Van Durme</last></author>
      <author><first>Kenton</first><last>Murray</last></author>
      <pages>214&#8211;221</pages>
      <abstract>Fine-tuning is known to improve NLP models by adapting an initial model trained on more plentiful but less domain-salient examples to data in a target domain. Such domain adaptation is typically done using one stage of fine-tuning. We demonstrate that gradually fine-tuning in a multi-step process can yield substantial further gains and can be applied without modifying the model or learning objective.</abstract>
      <url hash="84f585ac">2021.adaptnlp-1.22</url>
      <bibkey>xu-etal-2021-gradual</bibkey>
      <pwccode url="https://github.com/fe1ixxu/Gradual-Finetune" additional="false">fe1ixxu/Gradual-Finetune</pwccode>
    </paper>
    <paper id="25">
      <title>Semantic Parsing of Brief and Multi-Intent Natural Language Utterances</title>
      <author><first>Logan</first><last>Lebanoff</last></author>
      <author><first>Charles</first><last>Newton</last></author>
      <author><first>Victor</first><last>Hung</last></author>
      <author><first>Beth</first><last>Atkinson</last></author>
      <author><first>John</first><last>Killilea</last></author>
      <author id="fei-liu-utdallas"><first>Fei</first><last>Liu</last></author>
      <pages>255&#8211;262</pages>
      <abstract>Many military communication domains involve rapidly conveying situation awareness with few words. Converting natural language utterances to logical forms in these domains is challenging, as these utterances are brief and contain multiple intents. In this paper, we present a first effort toward building a weakly-supervised semantic parser to transform brief, multi-intent natural utterances into logical forms. Our findings suggest a new &#8220;projection and reduction&#8221; method that iteratively performs projection from natural to canonical utterances followed by reduction of natural utterances is the most effective. We conduct extensive experiments on two military and a general-domain dataset and provide a new baseline for future research toward accurate parsing of multi-intent utterances.</abstract>
      <url hash="b17dcdb4">2021.adaptnlp-1.25</url>
      <bibkey>lebanoff-etal-2021-semantic</bibkey>
    </paper>
    </volume>
</collection>