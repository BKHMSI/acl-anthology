<collection id="2020.signlang">
  <volume id="1">
    <meta>
      <booktitle>Proceedings of the LREC2020 9th Workshop on the Representation and Processing of Sign Languages: Sign Language Resources in the Service of the Language Community, Technological Challenges and Application Perspectives</booktitle>
      <editor><first>Eleni</first><last>Efthimiou</last></editor>
      <editor><first>Stavroula-Evita</first><last>Fotinea</last></editor>
      <editor><first>Thomas</first><last>Hanke</last></editor>
      <editor><first>Julie A.</first><last>Hochgesang</last></editor>
      <editor><first>Jette</first><last>Kristoffersen</last></editor>
      <editor><first>Johanna</first><last>Mesch</last></editor>
      <publisher>European Language Resources Association (ELRA)</publisher>
      <address>Marseille, France</address>
      <month>May</month>
      <year>2020</year>
      <isbn>979-10-95546-54-2</isbn>
    </meta>
    <frontmatter>
      <url hash="c53a1350">2020.signlang-1.0</url>
      <bibkey>signlang-2020-lrec2020</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Back and Forth between Theory and Application: Shared Phonological Coding Between <fixed-case>ASL</fixed-case> <fixed-case>S</fixed-case>ignbank and <fixed-case>ASL</fixed-case>-<fixed-case>LEX</fixed-case></title>
      <author><first>Amelia</first><last>Becker</last></author>
      <author><first>Donovan</first><last>Catt</last></author>
      <author><first>Julie A.</first><last>Hochgesang</last></author>
      <pages>1&#8211;6</pages>
      <abstract>The development of signed language lexical databases, digital organizations that describe different phonological features of and attempt to establish relationships between signs has resulted in a renewed interest in the phonological descriptions used to uniquely identify and organize the lexicons of respective sign languages (van der Kooij, 2002; Fenlon et al., 2016; Brentari et al., 2018). Throughout the mutually shared coding process involved in organizing two lexical databases, ASL Signbank (Hochgesang, Crasborn and Lillo-Martin, 2020) and ASL-LEX (Caselli et al., 2016), issues have arisen that require revisiting how phonological features and categories are to be applied and even decided upon, and which would adequately distinguish lexical contrast for respective sign languages. The paper concludes by exploring the inverse of the theory-to-database relationship. Examples are given of theoretical implications and research questions that arise from consequences of language resource building. These are presented as evidence that not only does theory impact organization of databases but that the process of database creation can also inform our theories.</abstract>
      <url hash="e3e77ee8">2020.signlang-1.1</url>
      <language>eng</language>
      <bibkey>becker-etal-2020-back</bibkey>
    </paper>
    <paper id="4">
      <title>Measuring Lexical Similarity across Sign Languages in <fixed-case>G</fixed-case>lobal <fixed-case>S</fixed-case>ignbank</title>
      <author><first>Carl</first><last>B&#246;rstell</last></author>
      <author><first>Onno</first><last>Crasborn</last></author>
      <author><first>Lori</first><last>Whynot</last></author>
      <pages>21&#8211;26</pages>
      <abstract>Lexicostatistics is the main method used in previous work measuring linguistic distances between sign languages. As a method, it disregards any possible structural/grammatical similarity, instead focusing exclusively on lexical items, but it is time consuming as it requires some comparable phonological coding (i.e. form description) as well as concept matching (i.e. meaning description) of signs across the sign languages to be compared. In this paper, we present a novel approach for measuring lexical similarity across any two sign languages using the Global Signbank platform, a lexical database of uniformly coded signs. The method involves a feature-by-feature comparison of all matched phonological features. This method can be used in two distinct ways: 1) automatically comparing the amount of lexical overlap between two sign languages (with a more detailed feature-description than previous lexicostatistical methods); 2) finding exact form-matches across languages that are either matched or mismatched in meaning (i.e. true or false friends). We show the feasability of this method by comparing three languages (datasets) in Global Signbank, and are currently expanding both the size of these three as well as the total number of datasets.</abstract>
      <url hash="5785ec2e">2020.signlang-1.4</url>
      <language>eng</language>
      <bibkey>borstell-etal-2020-measuring</bibkey>
    </paper>
    <paper id="6">
      <title><fixed-case>PE</fixed-case>2<fixed-case>LGP</fixed-case> Animator: A Tool To Animate A <fixed-case>P</fixed-case>ortuguese <fixed-case>S</fixed-case>ign <fixed-case>L</fixed-case>anguage Avatar</title>
      <author><first>Pedro</first><last>Cabral</last></author>
      <author><first>Matilde</first><last>Gon&#231;alves</last></author>
      <author><first>Hugo</first><last>Nicolau</last></author>
      <author><first>Lu&#237;sa</first><last>Coheur</last></author>
      <author><first>Ruben</first><last>Santos</last></author>
      <pages>33&#8211;38</pages>
      <abstract>Software for the production of sign languages is much less common than for spoken languages. Such software usually relies on 3D humanoid avatars to produce signs which, inevitably, necessitates the use of animation. One barrier to the use of popular animation tools is their complexity and steep learning curve, which can be hard to master for inexperienced users. Here, we present PE2LGP, an authoring system that features a 3D avatar that signs Portuguese Sign Language. Our Animator is designed specifically to craft sign language animations using a key frame method, and is meant to be easy to use and learn to users without animation skills. We conducted a preliminary evaluation of the Animator, where we animated seven Portuguese Sign Language sentences and asked four sign language users to evaluate their quality. This evaluation revealed that the system, in spite of its simplicity, is indeed capable of producing comprehensible messages.</abstract>
      <url hash="40e84936">2020.signlang-1.6</url>
      <language>eng</language>
      <bibkey>cabral-etal-2020-pe2lgp</bibkey>
    </paper>
    <paper id="7">
      <title>Translating an <fixed-case>A</fixed-case>esop&#8217;s Fable to <fixed-case>F</fixed-case>ilipino <fixed-case>S</fixed-case>ign <fixed-case>L</fixed-case>anguage through 3<fixed-case>D</fixed-case> Animation</title>
      <author><first>Mark</first><last>Cueto</last></author>
      <author><first>Winnie</first><last>He</last></author>
      <author><first>Rei</first><last>Untiveros</last></author>
      <author><first>Josh</first><last>Zu&#241;iga</last></author>
      <author><first>Joanna Pauline</first><last>Rivera</last></author>
      <pages>39&#8211;44</pages>
      <abstract>According to the National Statistics Office (2003) in the 2000 Population Census, the deaf community in the Philippines numbered to about 121,000 deaf and hard of hearing Filipinos. Deaf and hard of hearing Filipinos in these communities use the Filipino Sign Language (FSL) as the main method of manual communication. Deaf and hard of hearing children experience difficulty in developing reading and writing skills through traditional methods of teaching used primarily for hearing children. This study aims to translate an Aesop&#8217;s fable to Filipino Sign Language with the use of 3D animation resulting to a video output. The video created contains a 3D animated avatar performing the sign translations to FSL (mainly focusing on hand gestures which includes hand shape, palm orientation, location, and movement) on screen beside their English text equivalent and related images. The final output was then evaluated by FSL deaf signers. Evaluation results showed that the final output can potentially be used as a learning material. In order to make it more effective as a learning material, it is very important to consider the animation&#8217;s appearance, speed, naturalness, and accuracy. In this paper, the common action units were also listed for easier construction of animations of the signs.</abstract>
      <url hash="46955659">2020.signlang-1.7</url>
      <language>eng</language>
      <bibkey>cueto-etal-2020-translating</bibkey>
    </paper>
    <paper id="9">
      <title>Elicitation and Corpus of Spontaneous Sign Language Discourse Representation Diagrams</title>
      <author><first>Michael</first><last>Filhol</last></author>
      <pages>53&#8211;60</pages>
      <abstract>While Sign Languages have no standard written form, many signers do capture their language in some form of spontaneous graphical form. We list a few use cases (discourse preparation, deverbalising for translation, etc.) and give examples of diagrams. After hypothesising that they contain regular patterns of significant value, we propose to build a corpus of such productions. The main contribution of this paper is the specification of the elicitation protocol, explaining the variables that are likely to affect the diagrams collected. We conclude with a report on the current state of a collection following this protocol, and a few observations on the collected contents. A first prospect is the standardisation of a scheme to represent SL discourse in a way that would make them sharable. A subsequent longer-term prospect is for this scheme to be owned by users and with time be shaped into a script for their language.</abstract>
      <url hash="98b9400b">2020.signlang-1.9</url>
      <language>eng</language>
      <bibkey>filhol-2020-elicitation</bibkey>
    </paper>
    <paper id="11">
      <title>Signing as Input for a Dictionary Query: Matching Signs Based on Joint Positions of the Dominant Hand</title>
      <author><first>Manolis</first><last>Fragkiadakis</last></author>
      <author><first>Victoria</first><last>Nyst</last></author>
      <author><first>Peter</first><last>van der Putten</last></author>
      <pages>69&#8211;74</pages>
      <abstract>This study presents a new methodology to search sign language lexica, using a full sign as input for a query. Thus, a dictionary user can look up information about a sign by signing the sign to a webcam. The recorded sign is then compared to potential matching signs in the lexicon. As such, it provides a new way of searching sign language dictionaries to complement existing methods based on (spoken language) glosses or phonological features, like handshape or location. The method utilizes OpenPose to extract the body and finger joint positions. Dynamic Time Warping (DTW) is used to quantify the variation of the trajectory of the dominant hand and the average trajectories of the fingers. Ten people with various degrees of sign language proficiency have participated in this study. Each subject viewed a set of 20 signs from the newly compiled Ghanaian sign language lexicon and was asked to replicate the signs. The results show that DTW can predict the matching sign with 87% and 74% accuracy at the Top-10 and Top-5 ranking level respectively by using only the trajectory of the dominant hand. Additionally, more proficient signers obtain 90% accuracy at the Top-10 ranking. The methodology has the potential to be used also as a variation measurement tool to quantify the difference in signing between different signers or sign languages in general.</abstract>
      <url hash="da33243a">2020.signlang-1.11</url>
      <language>eng</language>
      <bibkey>fragkiadakis-etal-2020-signing</bibkey>
    </paper>
    <paper id="14">
      <title>An Isolated-Signing <fixed-case>RGBD</fixed-case> Dataset of 100 <fixed-case>A</fixed-case>merican <fixed-case>S</fixed-case>ign <fixed-case>L</fixed-case>anguage Signs Produced by Fluent <fixed-case>ASL</fixed-case> Signers</title>
      <author><first>Saad</first><last>Hassan</last></author>
      <author><first>Larwan</first><last>Berke</last></author>
      <author><first>Elahe</first><last>Vahdani</last></author>
      <author><first>Longlong</first><last>Jing</last></author>
      <author><first>Yingli</first><last>Tian</last></author>
      <author><first>Matt</first><last>Huenerfauth</last></author>
      <pages>89&#8211;94</pages>
      <abstract>We have collected a new dataset consisting of color and depth videos of fluent American Sign Language (ASL) signers performing sequences of 100 ASL signs from a Kinect v2 sensor. This directed dataset had originally been collected as part of an ongoing collaborative project, to aid in the development of a sign-recognition system for identifying occurrences of these 100 signs in video. The set of words consist of vocabulary items that would commonly be learned in a first-year ASL course offered at a university, although the specific set of signs selected for inclusion in the dataset had been motivated by project-related factors. Given increasing interest among sign-recognition and other computer-vision researchers in red-green-blue-depth (RBGD) video, we release this dataset for use by the research community. In addition to the RGB video files, we share depth and HD face data as well as additional features of face, hands, and body produced through post-processing of this data.</abstract>
      <url hash="12d76e40">2020.signlang-1.14</url>
      <language>eng</language>
      <bibkey>hassan-etal-2020-isolated</bibkey>
    </paper>
    <paper id="16">
      <title>Sign Language Motion Capture Dataset for Data-driven Synthesis</title>
      <author><first>Pavel</first><last>Jedli&#269;ka</last></author>
      <author><first>Zden&#283;k</first><last>Kr&#328;oul</last></author>
      <author><first>Jakub</first><last>Kanis</last></author>
      <author><first>Milo&#353;</first><last>&#381;elezn&#253;</last></author>
      <pages>101&#8211;106</pages>
      <abstract>This paper presents a new 3D motion capture dataset of Czech Sign Language (CSE). Its main purpose is to provide the data for further analysis and data-based automatic synthesis of CSE utterances. The content of the data in the given limited domain of weather forecasts was carefully selected by the CSE linguists to provide the necessary utterances needed to produce any new weather forecast. The dataset was recorded using the state-of-the-art motion capture (MoCap) technology to provide the most precise trajectories of the motion. In general, MoCap is a device capable of accurate recording of motion directly in 3D space. The data contains trajectories of body, arms, hands and face markers recorded at once to provide consistent data without the need for the time alignment.</abstract>
      <url hash="b43c520f">2020.signlang-1.16</url>
      <language>eng</language>
      <bibkey>jedlicka-etal-2020-sign</bibkey>
    </paper>
    <paper id="20">
      <title>Recognition of Static Features in Sign Language Using Key-Points</title>
      <author><first>Ioannis</first><last>Koulierakis</last></author>
      <author><first>Georgios</first><last>Siolas</last></author>
      <author><first>Eleni</first><last>Efthimiou</last></author>
      <author><first>Evita</first><last>Fotinea</last></author>
      <author><first>Andreas-Georgios</first><last>Stafylopatis</last></author>
      <pages>123&#8211;126</pages>
      <abstract>In this paper we report on a research effort focusing on recognition of static features of sign formation in single sign videos. Three sequential models have been developed for handshape, palm orientation and location of sign formation respectively, which make use of key-points extracted via OpenPose software. The models have been applied to a Danish and a Greek Sign Language dataset, providing results around 96%. Moreover, during the reported research, a method has been developed for identifying the time-frame of real signing in the video, which allows to ignore transition frames during sign recognition processing.</abstract>
      <url hash="ba17a60c">2020.signlang-1.20</url>
      <language>eng</language>
      <bibkey>koulierakis-etal-2020-recognition</bibkey>
    </paper>
    <paper id="22">
      <title>Machine Learning for Enhancing Dementia Screening in Ageing Deaf Signers of <fixed-case>B</fixed-case>ritish <fixed-case>S</fixed-case>ign <fixed-case>L</fixed-case>anguage</title>
      <author><first>Xing</first><last>Liang</last></author>
      <author><first>Bencie</first><last>Woll</last></author>
      <author><first>Kapetanios</first><last>Epaminondas</last></author>
      <author><first>Anastasia</first><last>Angelopoulou</last></author>
      <author><first>Reda</first><last>Al-Batat</last></author>
      <pages>135&#8211;138</pages>
      <abstract>Ageing trend in populations is correlated with increased prevalence of acquired cognitive impairments such as dementia. Although there is no cure for dementia, a timely diagnosis helps in obtaining necessary support and appropriate medication. With this in mind, researchers are working urgently to develop effective technological tools that can help doctors undertake early identification of cognitive disorder. In this paper, we introduce an automatic dementia screening system for ageing Deaf signers of British Sign Language (BSL), using Convolutional Neural Networks (CNN), by analysing the sign space envelope and facial expression of BSL signers using normal 2D videos from BSL corpus. Our approach firstly establishes an accurate real-time hand trajectory tracking model together with a real-time landmark facial motion analysis model to identify differences in sign space envelope and facial movement as the keys to identifying language changes associated with dementia. Based on the differences in patterns obtained from facial and trajectory motion data, CNN models (ResNet50/VGG16) are fine-tuned using Keras deep learning models to incrementally identify and improve dementia recognition rates. We report the results for two methods using different modalities (sign trajectory and facial motion), together with the performance comparisons between different deep learning CNN models in ResNet50 and VGG16. The experiments show the effectiveness of our deep learning based approach in terms of sign space tracking, facial motion tracking and early stage dementia performance assessment tasks. The results are validated against cognitive assessment scores as of our ground truth data with a test set performance of 87.88%. The proposed system has potential for economical, simple, flexible, and adaptable assessment of other acquired neurological impairments associated with motor changes, such as stroke and Parkinson&#8217;s disease in both hearing and Deaf people.</abstract>
      <url hash="c6fb2ac4">2020.signlang-1.22</url>
      <language>eng</language>
      <bibkey>liang-etal-2020-machine</bibkey>
    </paper>
    <paper id="23">
      <title>Machine Translation from Spoken Language to Sign Language using Pre-trained Language Model as Encoder</title>
      <author><first>Taro</first><last>Miyazaki</last></author>
      <author><first>Yusuke</first><last>Morita</last></author>
      <author><first>Masanori</first><last>Sano</last></author>
      <pages>139&#8211;144</pages>
      <abstract>Sign language is the first language for those who were born deaf or lost their hearing in early childhood, so such individuals require services provided with sign language. To achieve flexible open-domain services with sign language, machine translations into sign language are needed. Machine translations generally require large-scale training corpora, but there are only small corpora for sign language. To overcome this data-shortage scenario, we developed a method that involves using a pre-trained language model of spoken language as the initial model of the encoder of the machine translation model. We evaluated our method by comparing it to baseline methods, including phrase-based machine translation, using only 130,000 phrase pairs of training data. Our method outperformed the baseline method, and we found that one of the reasons of translation error is from pointing, which is a special feature used in sign language. We also conducted trials to improve the translation quality for pointing. The results are somewhat disappointing, so we believe that there is still room for improving translation quality, especially for pointing.</abstract>
      <url hash="e68807df">2020.signlang-1.23</url>
      <language>eng</language>
      <bibkey>miyazaki-etal-2020-machine</bibkey>
    </paper>
    <paper id="27">
      <title>Automatic Classification of Handshapes in <fixed-case>R</fixed-case>ussian <fixed-case>S</fixed-case>ign <fixed-case>L</fixed-case>anguage</title>
      <author><first>Medet</first><last>Mukushev</last></author>
      <author><first>Alfarabi</first><last>Imashev</last></author>
      <author><first>Vadim</first><last>Kimmelman</last></author>
      <author><first>Anara</first><last>Sandygulova</last></author>
      <pages>165&#8211;170</pages>
      <abstract>Handshapes are one of the basic parameters of signs, and any phonological or phonetic analysis of a sign language must account for handshapes. Many sign languages have been carefully analysed by sign language linguists to create handshape inventories. This has theoretical implications, but also applied use, as it is important due to the need of generating corpora for sign languages that can be searched, filtered, sorted by different sign components (such as handshapes, orientation, location, movement, etc.). However, it is a very time-consuming process, thus only a handful of sign languages have such inventories. This work proposes a process of automatically generating such inventories for sign languages by applying automatic hand detection, cropping, and clustering techniques. We applied our proposed method to a commonly used resource: the Spreadthesign online dictionary (www.spreadthesign.com), in particular to Russian Sign Language (RSL). We then manually verified the data to be able to perform classification. Thus, the proposed pipeline can serve as an alternative approach to manual annotation, and can help linguists in answering numerous research questions in relation to handshape frequencies in sign languages.</abstract>
      <url hash="a96553f5">2020.signlang-1.27</url>
      <language>eng</language>
      <bibkey>mukushev-etal-2020-automatic</bibkey>
    </paper>
    <paper id="30">
      <title><fixed-case>B</fixed-case>osphorus<fixed-case>S</fixed-case>ign22k Sign Language Recognition Dataset</title>
      <author><first>O&#287;ulcan</first><last>&#214;zdemir</last></author>
      <author><first>Ahmet Alp</first><last>K&#305;nd&#305;ro&#287;lu</last></author>
      <author><first>Necati</first><last>Cihan Camg&#246;z</last></author>
      <author><first>Lale</first><last>Akarun</last></author>
      <pages>181&#8211;188</pages>
      <abstract>Sign Language Recognition is a challenging research domain. It has recently seen several advancements with the increased availability of data. In this paper, we introduce the BosphorusSign22k, a publicly available large scale sign language dataset aimed at computer vision, video recognition and deep learning research communities. The primary objective of this dataset is to serve as a new benchmark in Turkish Sign Language Recognition for its vast lexicon, the high number of repetitions by native signers, high recording quality, and the unique syntactic properties of the signs it encompasses. We also provide state-of-the-art human pose estimates to encourage other tasks such as Sign Language Production. We survey other publicly available datasets and expand on how BosphorusSign22k can contribute to future research that is being made possible through the widespread availability of similar Sign Language resources. We have conducted extensive experiments and present baseline results to underpin future research on our dataset.</abstract>
      <url hash="dc25c0b2">2020.signlang-1.30</url>
      <language>eng</language>
      <bibkey>ozdemir-etal-2020-bosphorussign22k</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/bosphorussign22k">BosphorusSign22k</pwcdataset>
    </paper>
    <paper id="34">
      <title>Video-to-<fixed-case>H</fixed-case>am<fixed-case>N</fixed-case>o<fixed-case>S</fixed-case>ys Automated Annotation System</title>
      <author><first>Victor</first><last>Skobov</last></author>
      <author><first>Yves</first><last>Lepage</last></author>
      <pages>209&#8211;216</pages>
      <abstract>The Hamburg Notation System (HamNoSys) was developed for movement annotation of any sign language (SL) and can be used to produce signing animations for a virtual avatar with the JASigning platform. This provides the potential to use HamNoSys, i.e., strings of characters, as a representation of an SL corpus instead of video material. Processing strings of characters instead of images can significantly contribute to sign language research. However, the complexity of HamNoSys makes it difficult to annotate without a lot of time and effort. Therefore annotation has to be automatized. This work proposes a conceptually new approach to this problem. It includes a new tree representation of the HamNoSys grammar that serves as a basis for the generation of grammatical training data and classification of complex movements using machine learning. Our automatic annotation system relies on HamNoSys grammar structure and can potentially be used on already existing SL corpora. It is retrainable for specific settings such as camera angles, speed, and gestures. Our approach is conceptually different from other SL recognition solutions and offers a developed methodology for future research.</abstract>
      <url hash="9448fa21">2020.signlang-1.34</url>
      <language>eng</language>
      <bibkey>skobov-lepage-2020-video</bibkey>
    </paper>
    <paper id="35">
      <title>Cross-Lingual Keyword Search for Sign Language</title>
      <author><first>Nazif Can</first><last>Tamer</last></author>
      <author><first>Murat</first><last>Sara&#231;lar</last></author>
      <pages>217&#8211;223</pages>
      <abstract>Sign language research most often relies on exhaustively annotated and segmented data, which is scarce even for the most studied sign languages. However, parallel corpora consisting of sign language interpreting are rarely explored. By utilizing such data for the task of keyword search, this work aims to enable information retrieval from sign language with the queries from the translated written language. With the written language translations as labels, we train a weakly supervised keyword search model for sign language and further improve the retrieval performance with two context modeling strategies. In our experiments, we compare the gloss retrieval and cross language retrieval performance on RWTH-PHOENIX-Weather 2014T dataset.</abstract>
      <url hash="6b2680c3">2020.signlang-1.35</url>
      <language>eng</language>
      <bibkey>tamer-saraclar-2020-cross</bibkey>
    </paper>
    </volume>
</collection>