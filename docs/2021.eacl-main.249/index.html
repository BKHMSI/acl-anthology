<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Crisscrossed Captions : Extended Intramodal and Intermodal Semantic Similarity Judgments for MS-COCOMS-COCO - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css><meta content="Crisscrossed Captions : Extended Intramodal and Intermodal Semantic Similarity Judgments for MS-COCOMS-COCO" name=citation_title><meta content="Zarana Parekh" name=citation_author><meta content="Jason Baldridge" name=citation_author><meta content="Daniel Cer" name=citation_author><meta content="Austin Waters" name=citation_author><meta content="Yinfei Yang" name=citation_author><meta content="Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume" name=citation_conference_title><meta content="2021/4" name=citation_publication_date><meta content="https://aclanthology.org/2021.eacl-main.249.pdf" name=citation_pdf_url><meta content="2855" name=citation_firstpage><meta content="2870" name=citation_lastpage><meta content="10.18653/v1/2021.eacl-main.249" name=citation_doi><meta property="og:title" content="Crisscrossed Captions : Extended Intramodal and Intermodal Semantic Similarity Judgments for MS-COCOMS-COCO"><meta property="og:image" content="https://aclanthology.org/thumb/2021.eacl-main.249.jpg"><meta property="og:image:alt" content="First page of paper PDF."><meta property="og:type" content="article"><meta property="og:site_name" content="ACL Anthology"><meta property="og:url" content="https://aclanthology.org/2021.eacl-main.249"><meta property="og:description" content="Zarana Parekh, Jason Baldridge, Daniel Cer, Austin Waters, Yinfei Yang. Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume. 2021."><link rel=canonical href=https://aclanthology.org/2021.eacl-main.249></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><div><h2 id=title><a id=en_title href=https://aclanthology.org/2021.eacl-main.249.pdf>Crisscrossed Captions : Extended Intramodal and Intermodal Semantic Similarity Judgments for MS-COCO<span class=acl-fixed-case>MS</span>-<span class=acl-fixed-case>COCO</span></a>
<a id=af_title style=display:none href=https://aclanthology.org/2021.eacl-main.249.pdf>Crisscrossed Titels: uitgebreide Intramodaal en Intermodaal Semantiese Ligtigheid Verordeninge vir MS- COCO</a>
<a id=am_title style=display:none href=https://aclanthology.org/2021.eacl-main.249.pdf>የስሕተት አርእስቶች: Extended Intramodal and Intermodal Semantic Similarity Judges for MS-CO</a>
<a id=ar_title style=display:none href=https://aclanthology.org/2021.eacl-main.249.pdf>التعليقات المتقاطعة: أحكام التشابه الدلالي متعددة الوسائط والداخلية الموسعة لـ MS-COCO</a>
<a id=az_title style=display:none href=https://aclanthology.org/2021.eacl-main.249.pdf>Crisscrossed Captions: Extended Intramodal and Intermodal Semantic Similarity Judgments for MS-COCO</a>
<a id=bg_title style=display:none href=https://aclanthology.org/2021.eacl-main.249.pdf>Крискрозирани надписи: Разширени решения за интрамодална и интермодална семантична сходство за МС-КОКО</a>
<a id=bn_title style=display:none href=https://aclanthology.org/2021.eacl-main.249.pdf>সমালোচনার শিরোনাম: MS-CO এর জন্য বিস্তৃত ইন্ট্রামোডাল এবং ইন্টারমডাল সেম্যান্ডিক সেমান্টিক বিচার</a>
<a id=bo_title style=display:none href=https://aclanthology.org/2021.eacl-main.249.pdf>Crisscrossed Captions: Extended Intramodal and Intermodal Semantic Similarity Judgments for MS-COCO</a>
<a id=bs_title style=display:none href=https://aclanthology.org/2021.eacl-main.249.pdf>Raskrsnuti kapiji: Prošireni sudovi o intramodalnoj i intermodalnoj semantičkoj sličnosti za MS-COCO</a>
<a id=ca_title style=display:none href=https://aclanthology.org/2021.eacl-main.249.pdf>Capcions transversals: Sentences extenses de Semància Semàtica Intramodal i Intermodal per MS-COCO</a>
<a id=cs_title style=display:none href=https://aclanthology.org/2021.eacl-main.249.pdf>Krizové titulky: Rozšířené rozsudky o intramodální a intermodální sémantické podobnosti pro MS-COCO</a>
<a id=da_title style=display:none href=https://aclanthology.org/2021.eacl-main.249.pdf>Krydsrullede billedtekster: Udvidede domme om intramodal og intermodal semantisk lighed for MS-COCO</a>
<a id=de_title style=display:none href=https://aclanthology.org/2021.eacl-main.249.pdf>Crissscrossed Untertitel: Erweiterte intramodale und intermodale semantische Ähnlichkeitsurteile für MS-COCO</a>
<a id=el_title style=display:none href=https://aclanthology.org/2021.eacl-main.249.pdf>Επικεφαλής τίτλος: Εκτεταμένες αποφάσεις ενδοmodaler και διατροπικής σημασιολογικής ομοιότητας για MS-COCO</a>
<a id=es_title style=display:none href=https://aclanthology.org/2021.eacl-main.249.pdf>Subtítulos entrecruzados: Juicios de similitud semántica intramodal e intermodal ampliados para MS-COCO</a>
<a id=et_title style=display:none href=https://aclanthology.org/2021.eacl-main.249.pdf>Kriisilised pealkirjad: MS-COCO laiendatud intramodaalse ja intermodaalse semantilise sarnasuse kohtuotsused</a>
<a id=fa_title style=display:none href=https://aclanthology.org/2021.eacl-main.249.pdf>کاپیتان‌های کریسکروس: قضاوت‌های شبیه‌انگیزی داخلی و منطقه‌ای برای MS-COCO گسترده شده</a>
<a id=fi_title style=display:none href=https://aclanthology.org/2021.eacl-main.249.pdf>Crisscrossed captions: Extended intramodal and intermodal semantic similarity tuomiot MS-COCO</a>
<a id=fl_title style=display:none href=https://aclanthology.org/2021.eacl-main.249.pdf></a>
<a id=fr_title style=display:none href=https://aclanthology.org/2021.eacl-main.249.pdf>Sous-titres entrecroisés : Jugements de similitude sémantique intramodale et intermodale étendus pour MS-COCO</a>
<a id=ga_title style=display:none href=https://aclanthology.org/2021.eacl-main.249.pdf>Fotheidil Crisscrossed: Breithiúnais Chomhchosúlachta Shéimeantach Intramodal agus Idirmhódúil Leathnaithe do MS-COCO</a>
<a id=ha_title style=display:none href=https://aclanthology.org/2021.eacl-main.249.pdf>KCharselect unicode block name</a>
<a id=he_title style=display:none href=https://aclanthology.org/2021.eacl-main.249.pdf>תורגם ע"י Crisscrossed Captions: Extended Intramodal and Intermodal Semantic Similarity Judgments for MS-COCO</a>
<a id=hi_title style=display:none href=https://aclanthology.org/2021.eacl-main.249.pdf>Crisscrossed कैप्शन: एमएस-कोको के लिए विस्तारित इंट्रामोडल और इंटरमोडल सिमेंटिक समानता निर्णय</a>
<a id=hr_title style=display:none href=https://aclanthology.org/2021.eacl-main.249.pdf>Raskrsnuti načini: Prošireni rasuđivanje intrramodalne i intermodalne semantičke sličnosti za MS-COCO</a>
<a id=hu_title style=display:none href=https://aclanthology.org/2021.eacl-main.249.pdf>Válogatott feliratok: kiterjesztett intramodális és intermodális szemantikus hasonlósági ítéletek MS-COCO esetében</a>
<a id=hy_title style=display:none href=https://aclanthology.org/2021.eacl-main.249.pdf>Քրիսխրոսխային գլխարկներ. ՄՍ-ԿոԿՕ-ի համար ընդլայնված ինտրամոդալ և ինտերմոդալ սեմատիկ նման դատողություններ</a>
<a id=id_title style=display:none href=https://aclanthology.org/2021.eacl-main.249.pdf>Capsi Crisscrossed: Penghakiman Semantik Intramodal dan Intermodal Terluas untuk MS-COCO</a>
<a id=is_title style=display:none href=https://aclanthology.org/2021.eacl-main.249.pdf></a>
<a id=it_title style=display:none href=https://aclanthology.org/2021.eacl-main.249.pdf>Didascalie a croce: sentenze estese di somiglianza semantica intramodale e intermodale per MS-COCO</a>
<a id=ja_title style=display:none href=https://aclanthology.org/2021.eacl-main.249.pdf>Crisscrossed Captions: Extended Intramodal and Intermodal Semantic Similarity Judgments for MS - COCO</a>
<a id=jv_title style=display:none href=https://aclanthology.org/2021.eacl-main.249.pdf>kriScroosed Captions: Expanded Intramadal and intermodal semanti Similrity judgments for CS-COMO</a>
<a id=ka_title style=display:none href=https://aclanthology.org/2021.eacl-main.249.pdf>კრისკროსური შესახებ: გაფართებული ინტრამოდელური და ინტერმოდელური Semantic Similarity Judgments for MS- COCO</a>
<a id=kk_title style=display:none href=https://aclanthology.org/2021.eacl-main.249.pdf>Крискросс айдарлары: MS- COCO үшін интрамодалық және интермодалық Semantic Similarity Judgments</a>
<a id=ko_title style=display:none href=https://aclanthology.org/2021.eacl-main.249.pdf>교차 자막: MS-COCO의 확장 모드 내와 모드 간 의미 유사성 판단</a>
<a id=lt_title style=display:none href=https://aclanthology.org/2021.eacl-main.249.pdf>Krizių kryžminiai žodžiai: išplėsti valstybių narių – COCO – vidaus ir tarpmodalinio semestinio panašumo sprendimai</a>
<a id=mk_title style=display:none href=https://aclanthology.org/2021.eacl-main.249.pdf>Крискросирани наслови: Проширени интрамодални и интермодални семантични судии за сличност за MS-COCO</a>
<a id=ml_title style=display:none href=https://aclanthology.org/2021.eacl-main.249.pdf>ക്രിസ്ക്രോസ് ചെയ്ത പ്രമേയങ്ങള്‍: എസ്- കോക്കോയ്ക്ക് വേണ്ടി വിശാലമായ ഇന്റ്രാമോഡാലും ഇന്റര്‍മോഡാല്‍ സെമാന്റ</a>
<a id=mn_title style=display:none href=https://aclanthology.org/2021.eacl-main.249.pdf>Крискросс дурангууд: MS-COCO-ын интрамодал болон интермодал төстэй ижил төстэй шүүмжүүд</a>
<a id=ms_title style=display:none href=https://aclanthology.org/2021.eacl-main.249.pdf>Crisscrossed Captions: Extended Intramodal and Intermodal Semantic Similarity Judgments for MS-COCO</a>
<a id=mt_title style=display:none href=https://aclanthology.org/2021.eacl-main.249.pdf>Crisscrossed Captions: Extended Intramodal and Intermodal Semantic Similarity Judgments for MS-COCO</a>
<a id=nl_title style=display:none href=https://aclanthology.org/2021.eacl-main.249.pdf>Crissgescrossed ondertitels: Uitgebreide intramodale en intermodale semantische gelijkenisoordelen voor MS-COCO</a>
<a id=no_title style=display:none href=https://aclanthology.org/2021.eacl-main.249.pdf>Krysskrosserte tittel: Utvida intermodal og intermodal semantisk forskjellighetsjustering for MS- COCO</a>
<a id=pl_title style=display:none href=https://aclanthology.org/2021.eacl-main.249.pdf>Podpisy kryzysowe: Rozszerzone wyroki dotyczące podobieństwa semantycznego w odniesieniu do MS-COCO</a>
<a id=pt_title style=display:none href=https://aclanthology.org/2021.eacl-main.249.pdf>Legendas cruzadas: julgamentos estendidos de semelhança semântica intramodal e intermodal para MS-COCO</a>
<a id=ro_title style=display:none href=https://aclanthology.org/2021.eacl-main.249.pdf>Titluri clasice: Hotărâri extinse privind similitudinea semantică intramodală și intermodală pentru MS-COCO</a>
<a id=ru_title style=display:none href=https://aclanthology.org/2021.eacl-main.249.pdf>Перекрестные заголовки: расширенные внутримодальные и интермодальные суждения о семантическом сходстве для MS-COCO</a>
<a id=si_title style=display:none href=https://aclanthology.org/2021.eacl-main.249.pdf>Crisscrossed Captions: විස්තාරිත සිමාන්තික සිමාන්තික විශ්වාසය MS-COCO සඳහා</a>
<a id=sk_title style=display:none href=https://aclanthology.org/2021.eacl-main.249.pdf>Krizni napisi: Razširjene intramodalne in intermodalne semantične podobnosti sodbe za MS-COCO</a>
<a id=so_title style=display:none href=https://aclanthology.org/2021.eacl-main.249.pdf>Crisscross Captions: Extended Intramodal and Intermodal Semantic Similarity Judges for MS-CO</a>
<a id=sq_title style=display:none href=https://aclanthology.org/2021.eacl-main.249.pdf>Kapsionet e kryqëzuara: Gjykimet e zgjeruara të ngjashmërisë Semantike Intramodale dhe Intermodale për MS-COCO</a>
<a id=sr_title style=display:none href=https://aclanthology.org/2021.eacl-main.249.pdf>Raskrivena kapcija: Prošireni rasuđivanje intrramodalne i intermodalne semantičke sličnosti za MS-COCO</a>
<a id=sv_title style=display:none href=https://aclanthology.org/2021.eacl-main.249.pdf>Kryssformade bildtexter: Utvidgade domar om intramodal och intermodal semantisk likhet för MS-COCO</a>
<a id=sw_title style=display:none href=https://aclanthology.org/2021.eacl-main.249.pdf>Makala yaliyokosolewa: Mahakama ya Kitengo cha Intramodal na Kimataifa kwa ajili ya MS-CO</a>
<a id=ta_title style=display:none href=https://aclanthology.org/2021.eacl-main.249.pdf>சிகிரிஸ் தலைப்புகள்: MS- CO க்கான விரிவாக்கப்பட்ட Intramodal மற்றும் Intermodal Semantic Similarity தீர்ப்புகள்</a>
<a id=tr_title style=display:none href=https://aclanthology.org/2021.eacl-main.249.pdf>Crisscrossed Captions: Extended Intramodal and Intermodal Semantic Similarity Judgments for MS-COCO</a>
<a id=uk_title style=display:none href=https://aclanthology.org/2021.eacl-main.249.pdf></a>
<a id=ur_title style=display:none href=https://aclanthology.org/2021.eacl-main.249.pdf>کریسسکروس کیپٹینز: MS-COCO کے لئے پھیلائی Intramodal and Intermodal Semantic Similarity Judgments</a>
<a id=uz_title style=display:none href=https://aclanthology.org/2021.eacl-main.249.pdf>Name</a>
<a id=vi_title style=display:none href=https://aclanthology.org/2021.eacl-main.249.pdf>Các phim khủng khiếp: Các quyết định tương đồng truyền thống truyền thống và truyền thống.</a>
<a id=zh_title style=display:none href=https://aclanthology.org/2021.eacl-main.249.pdf>纵横之题:MS-COCO之广内联与多式联运语义相似性决</a></h2><p class=lead><a href=/people/z/zarana-parekh/>Zarana Parekh</a>,
<a href=/people/j/jason-baldridge/>Jason Baldridge</a>,
<a href=/people/d/daniel-cer/>Daniel Cer</a>,
<a href=/people/a/austin-waters/>Austin Waters</a>,
<a href=/people/y/yinfei-yang/>Yinfei Yang</a></p></div><hr><div class="row acl-paper-details"><div class="col col-lg-10 order-2"><div class="card bg-light mb-2 mb-lg-3" id=en_abstract><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>By supporting multi-modal retrieval training and evaluation, image captioning datasets have spurred remarkable progress on <a href=https://en.wikipedia.org/wiki/Representation_learning>representation learning</a>. Unfortunately, datasets have limited cross-modal associations : images are not paired with other images, captions are only paired with other captions of the same image, there are no negative associations and there are missing positive cross-modal associations. This undermines research into how inter-modality learning impacts intra-modality tasks. We address this gap with Crisscrossed Captions (CxC), an extension of the MS-COCO dataset with human semantic similarity judgments for 267,095 intra- and inter-modality pairs. We report baseline results on <a href=https://en.wikipedia.org/wiki/CxC>CxC</a> for strong existing unimodal and multimodal models. We also evaluate a multitask dual encoder trained on both image-caption and caption-caption pairs that crucially demonstrates CxC&#8217;s value for measuring the influence of intra- and inter-modality learning.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=af_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Deur te ondersteun multimodaal ontvang onderwerking en evaluering, het beeldtitelsette datastelle betekende vordering gevra op voorstelling leer. Ongelukkig, datastelle het beperk kruismodale assosiasies: beelde word nie paired met ander beelde nie, titels word slegs paired met ander titels van dieselfde beeld, daar is geen negatiewe assosiasies en daar is ontbreek positiewe kruismodale assosiasies. Hierdie onderwerp onderwerp onder die onderwerp na hoe intermodaliteit leer invloek intra-modaliteit-opdragte. Ons adres hierdie gap met Crisscrossed Captions (CxC), ân uitbreiding van die MS- COCO datastel met menslike semantiese gelykenis oordelings vir 267,095 intra- en intermodaliteit paar. Ons rapporteer basisline resultate op CxC vir sterk bestaande unimodale en multimodale modele. Ons evalueer ook 'n veelvuldige taak tweede enkoder wat op beeld-titel en caption-caption-paire opgelei is wat kruistelik CxC se waarde wys om die influens van intra- en intermodaliteit-leer te maak.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=am_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>በብዙ-modal ማስተማር እና ማስታወቂያውን በመደጋገም ምስል ማቀናጃ የዳታ ማቀናጃ ማስታወቂያውን በመግለጥ የሚያስደንቅ ግንኙነት አግኝቷል፡፡ በመከፋት፣ የዳታ ሰርቨሮች የክፍለ ሥርዓት ማኅበረሰብ ነው፤ ምስሎች ከሌሎች ምስሎች ጋር አይጋጠሙም፣ ምርጫዎች ግን በአንድ ምስል ላይ በተለያዩ አርእስቶች ብቻ ናቸው፣ ምንም የnegative ማኅበረሰብ የለውም፣ እናም የክፍለ ሥልጣን-ሞዴል ማኅበረሰቦች አይጎድሉም፡፡ ይሄ ትምህርት በሥልጣናዊ ትምህርት እንዴት በሥርዓት ላይ የሚደርስ ነው በማድረግ ያሳድጋል፡፡ ይህንን ክፍል በክሮስcross አርእስት (CxC)፣ የMS-CO ዳታዎችን ለመዘርጋት በሰው የsemantic ብጤነት ፍርድ ለ267,095 በ.አ እና በ. የCxC ውጤቶች በጽኑ unimodal እና multimodal ሞዴል ላይ እናስባለን፡፡ የምስል አርእስት እና የCxC ዋጋውን በመለኪያ እና በሥርዓት ትምህርት ላይ በሚያስፈልገው የሁለት ብልቲካባቢ የፊደል ኮድዶችን እናሳውቃለን፡፡</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ar_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>من خلال دعم التدريب على الاسترجاع متعدد الوسائط والتقييم ، حفزت مجموعات بيانات الصور التوضيحية تقدمًا ملحوظًا في تعلم التمثيل. لسوء الحظ ، تحتوي مجموعات البيانات على ارتباطات محدودة عبر الوسائط: لا يتم إقران الصور مع الصور الأخرى ، ويتم إقران التسميات التوضيحية فقط مع التسميات التوضيحية الأخرى للصورة نفسها ، ولا توجد ارتباطات سلبية ولا توجد ارتباطات موجبة عبر الوسائط. هذا يقوض البحث في كيفية تأثير التعلم متعدد الوسائط على المهام داخل الوسائط. نعالج هذه الفجوة باستخدام Crisscrossed Captions (CxC) ، وهو امتداد لمجموعة بيانات MS-COCO مع أحكام تشابه دلالي بشرية لـ 267095 زوجًا داخليًا ومتعدد الأساليب. نقوم بالإبلاغ عن نتائج خط الأساس على CxC للنماذج القوية الحالية أحادية الوسائط ومتعددة الوسائط. نقوم أيضًا بتقييم برنامج تشفير مزدوج متعدد المهام تم تدريبه على كل من أزواج التعليقات التوضيحية والصورة والتعليقات التوضيحية التي توضح بشكل حاسم قيمة CxC لقياس تأثير التعلم داخل الوسائط ومتعددها.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=az_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Çoxlu modal alma təcrübəsini və değerlendirməyi dəstəkləndirək, görüntü başlıqları verilən qurğuları göstəricisi öyrənməsi barəsində möhtərəm təcrübə göstərməsini tələb etdi. Necə olaraq, veri qurğuları çox modal bağlantıları var: görüntülər başqa şəkillərlə bağlanılmaz, başlıqlar yalnız aynı suretin başqa başlıqları ilə bağlanılır, negatif bağlantılar yoxdur və pozitif çox modal bağlantılar yoxdur. Bu, modaliyyət öyrənməsinin modaliyyət işlərinin necə etkisi olduğunu araştırmağı zəifləyir. Biz bu boşluğu Crisscrossed Captions (CxC) ilə çəkirik, MS-COCO veri qutusu 267,095 intra-modalitet çiftləri üçün insan semantik bənzər hökmləri ilə uzaqlaşdırılır. Biz CxC ilə əsas səhifələrin sonuçlarını çox modal və çox modal modellər üçün xəbər veririk. Biz həmçinin CxC'nin qiymətini intra-modalitat öyrənməsinin təsirini ölçüyə görə bilinmiş çoxlu işin dual kodlayıcıs ını də təhsil edirik.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=bg_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Подкрепяйки обучението и оценката за мултимодално извличане, наборите от данни за надписи на изображения стимулират забележителен напредък в обучението за представяне. За съжаление, наборите от данни имат ограничени междумодални асоциации: изображенията не са сдвоени с други изображения, надписите се сдвояват само с други надписи на едно и също изображение, няма отрицателни асоциации и липсват положителни междумодални асоциации. Това подкопава изследванията за това как интермодалното обучение оказва влияние върху задачите в рамките на модалността. Ние разглеждаме тази празнота с Крискросed Captions (CxC), разширение на набор от данни MS-COCO с преценки за човешка семантична прилика за 267 095 интрамодални и интермодални двойки. Докладваме базови резултати за СхС за силни съществуващи унимодални и мултимодални модели. Също така оценяваме многозадачен двоен кодер, обучен както върху двойки изображение-надпис, така и върху надпис-надпис, който демонстрира съществено стойността на CxC за измерване на влиянието на интрамодалното и интермодалното обучение.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=bn_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>মাল্টিমোডাল পুনরুদ্ধার প্রশিক্ষণ এবং মূল্যায়নের সমর্থন দিয়ে ছবির ক্যাপ্টেশন ডাটাসেটের প্রতিনিধিত্ব শিক্ষা নিয়ে চম দুর্ভাগ্যবশত, ডাটাসেট সীমিত ক্রস-মোডাল সংস্থা: ছবিগুলো অন্যান্য ছবির সাথে জোড়া করা হয়নি, শিরোনাম শুধুমাত্র একই ছবির অন্যান্য শিরোনাম, কোন ন নেতিব কিভাবে মোডিয়াল শিক্ষা বিভিন্ন মোডিয়াল কাজের উপর প্রভাব ফেলে দেয়া হয়েছে। ক্রিস্ক্রোস ক্রোস শিরোনামের (সিক্সিসি) দ্বারা আমরা এই বিভ্রান্ত বিষয়টিকে আলোচনা করি। এমএস- কমো ডাটাসেট বিস্তারিত বিস্তারিত মানুষের সামান আমরা বেসেলাইনের ফলাফল সিক্সিতে রিপোর্ট করি বিদ্যমান ইউনিমোডাল এবং বহুটিমোডাল মডেলের জন্য। আমরা একই সাথে চিত্রের শিরোনাম এবং শিরোনামের শিরোনামের দুইবার কোডার প্রশিক্ষিত একটি মাল্টিটিক্যাডারের মূল্য মূল্য দেখাচ্ছি যা প্রকাশিত সিক্সির</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=bo_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>སྣ་མང་ཅིག་གི་ལྟ་རྒྱབ་སྐྱོར་དང་གནད་དོན་རིམ་ལ་རྒྱབ་སྐྱོར་བྱེད་ཀྱི་ཡོད་པ། གཟུགས ཡིན་ནའང་། གནད་སྡུད་གཞུང་སྒྲིག་ཚུ་ལ་ཆུང་བ་བཟོས་མེད་པ་དང་བརྙན་རིས་གཞན་དང་བསྡོམས་མེད་པ། པར་རིས་འདི་གཅིག་མཚུངས་ཀྱི་པར འདིས་གནས་ཚུལ་བྱ་རིམ་གྱི་དབྱེ་རིམ་གྱི་ཐབས་ལམ་ནང་གི་ལས་འཚོལ་ཞིབ་བྱེད་ཀྱི་ཡོད། We address this gap with Crisscrossed Captions (CxC), an extension of the MS-COCO dataset with human semantic similarity judgments for 267,095 intra- and inter-modality pairs. ང་ཚོས་གནས་ཡུལ་མྱུར་བའི་དབྱིབས་མཐུན་ཅན་དང་སྣ་མང་མཐུན་པའི་མིག་དཔེ་བྱེད་པར་གཞི་རྟེན་འབྲེལ་བ་ཡིན། ང་ཚོས་བརྙན་རིས་དང་བགོ་བྲིས་ཁང་གཉིས་ཀྱི་ལས་འགུལ་གྱི་རྣམ་པ་གཉིས་ཀྱིས་དབྱེ་བ་ཞིབ་འཇུག་བྱེད་ཀྱི་ཡོད་ཚད་ལ་ངེས་འཛིན་བྱེད་པའི་ནང་དུ་འཇུག་ཟམ་འཛ</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=bs_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Podržavajući multimodalnu obuku i procjenu povratka, podaci podataka za snimanje slika izazvali su izuzetan napredak na učenju predstavljanja. Nažalost, podaci imaju ograničene krsnomodalne asocijacije: slike se ne povezuju sa drugim slikama, kapcije se povezuju samo sa drugim kapcima iste slike, nema negativnih asocijacija i nedostaju pozitivne krsnomodalne asocijacije. To podnosi istraživanje o tome kako međumodalno učenje utječe na zadatke unutar modaliteta. Mi obraćamo taj praznik sa kapcima Crisscrossed (CxC), produženjem kompleta podataka MS-COCO sa osuđivanjima ljudske semantičke sličnosti za 267.095 unutar i međumodalitetskih parova. Prijavljamo početne rezultate CxC za jake postojeće unimodalne i multimodalne modele. Također procjenjujemo dvostruki koder koji je obučen na parovima slike i kapcije, koji su ključno pokazivali vrijednost CxC-a za mjerenje utjecaja učenja unutar i međumodaliteta.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ca_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Sostenint formació i evaluació multimodals de recuperació, els conjunts de dades de capturació d'imatges han impulsat un progrés notable en l'aprenentatge de representació. Malauradament, els conjunts de dades tenen associacions transmodals limitades: les imatges no estan parellades amb altres imatges, els títulos només estan parellats amb altres títulos de la mateixa imatge, no hi ha associacions negatives i falten associacions transmodals positives. Això socava la recerca sobre com l'aprenentatge intermodalitat afecta les tasques intramodalitats. Afectem aquesta diferència amb Crisscrossed Captions (CxC), una extensió del conjunt de dades MS-COCO amb judicis semàntics de similitud humana per 267.095 parells intra- i inter-modalitat. Informem els resultats basals de la CxC per a models unimodals i multimodals importants. També evaluem un codificador doble multitasca entrenat en parelles imatge-títol i títol-títol que demostren crucialment el valor de CxC per mesurar l'influència de l'aprenentatge intra- i intermodalitat.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=cs_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Podporou multimodálního vyhledávání školení a vyhodnocování obrazových titulků podněcovaly pozoruhodný pokrok ve vzdělávání reprezentace. Bohužel datové sady mají omezené crossmodální asociace: obrázky nejsou spárovány s jinými obrázky, titulky jsou spárovány pouze s jinými titulky stejného obrázku, neexistují žádné negativní asociace a chybí zde kladné crossmodální asociace. To podkopává výzkum toho, jak intermodální učení ovlivňuje intramodální úkoly. Tuto mezeru řešíme pomocí Crisscrossed Titions (CxC), rozšíření MS-COCO datové sady o lidské sémantické podobnosti pro 267,095 intra- a intermodalitní páry. Podáváme základní výsledky CxC pro silné existující unimodální a multimodální modely. Vyhodnocujeme také multiúlohový duální snímač trénovaný jak na párech obrázků, tak na párech titulků, který zásadně demonstruje hodnotu CxC pro měření vlivu intra- a intermodálního učení.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=da_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Ved at understøtte multimodal træning og evaluering har datasæt med billedtekster fremmet bemærkelsesværdige fremskridt med repræsentationslæring. Desværre har datasæt begrænsede tværmodale associationer: Billeder parres ikke med andre billeder, billedtekster parres kun med andre billedtekster af det samme billede, der er ingen negative associationer, og der mangler positive tværmodale associationer. Dette underminerer forskningen i, hvordan intermodal læring påvirker intramodalitetsopgaver. Vi løser dette hul med Crisscrossed Captions (CxC), en udvidelse af MS-COCO datasættet med menneskelige semantiske lighedsvurderinger for 267.095 intra- og intermodalitetspar. Vi rapporterer baseline resultater på CxC for stærke eksisterende unimodale og multimodale modeller. Vi evaluerer også en multi-task dual encoder trænet på både billede-billedtekst og billedtekst-billedtekst par, der afgørende demonstrerer CxC's værdi for at måle indflydelsen af intra- og intermodalitet læring.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=de_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Durch die Unterst체tzung multimodaler Retrieval-Schulungen und -Evaluierungen haben Bildunterschriftsdatens채tze bemerkenswerte Fortschritte beim Lernen von Repr채sentationen angeregt. Leider haben Datens채tze begrenzte crossmodale Assoziationen: Bilder werden nicht mit anderen Bildern gepaart, Beschriftungen werden nur mit anderen Beschriftungen des gleichen Bildes gepaart, es gibt keine negativen Assoziationen und es fehlen positive crossmodale Assoziationen. Dies untergr채bt die Erforschung, wie sich intermodales Lernen auf intramodale Aufgaben auswirkt. Wir schlie횩en diese L체cke mit Crisscrossed Captions (CxC), einer Erweiterung des MS-COCO Datensatzes mit humanen semantischen 횆hnlichkeitsurteilen f체r 267,095 intra- und intermodale Paare. Wir berichten Baseline-Ergebnisse zu CxC f체r starke bestehende unimodale und multimodale Modelle. Wir evaluieren auch einen Multitask-Dual-Encoder, der sowohl f체r Bild-Untertitel- als auch f체r Untertitel-Untertitel-Paare trainiert ist, der den Wert von CxC f체r die Messung des Einflusses von intra- und intermodalem Lernen entscheidend demonstriert.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=el_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Υποστηρίζοντας την εκπαίδευση και την αξιολόγηση πολλαπλών τρόπων ανάκτησης, τα σύνολα δεδομένων λεζάντας εικόνων έχουν προκαλέσει αξιοσημείωτη πρόοδο στη μάθηση αναπαράστασης. Δυστυχώς, τα σύνολα δεδομένων έχουν περιορισμένες διασυνδέσεις: οι εικόνες δεν συνδυάζονται με άλλες εικόνες, οι λεζάντες συνδυάζονται μόνο με άλλες λεζάντες της ίδιας εικόνας, δεν υπάρχουν αρνητικές συσχετίσεις και λείπουν θετικές διασυνδέσεις. Αυτό υπονομεύει την έρευνα σχετικά με τον τρόπο με τον οποίο η διαπροπική μάθηση επηρεάζει τα καθήκοντα ενδοmodality. Αντιμετωπίζουμε αυτό το χάσμα με Crissscrooss Titions (CxC), μια επέκταση του συνόλου δεδομένων MS-COCO με ανθρώπινες σημασιολογικές ομοιότητες για 267,095 ζεύγη ενδο- και ενδοmodality. Αναφέρουμε βασικά αποτελέσματα για το CxC για ισχυρά υπάρχοντα μονοmodale και πολυmodale μοντέλα. Αξιολογούμε επίσης έναν πολυλειτουργικό διπλό κωδικοποιητή εκπαιδευμένο τόσο σε ζεύγη εικόνας-λεζάντας όσο και λεζάντας-λεζάντας που καταδεικνύει καθοριστικά την αξία του για τη μέτρηση της επιρροής της ενδο- και διαμεσολαβητικής μάθησης.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=es_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Al apoyar la capacitación y la evaluación de recuperación multimodal, los conjuntos de datos de subtítulos de imágenes han impulsado un progreso notable en el aprendizaje de la representación. Desafortunadamente, los conjuntos de datos tienen asociaciones intermodales limitadas: las imágenes no se emparejan con otras imágenes, los subtítulos solo se emparejan con otros subtítulos de la misma imagen, no hay asociaciones negativas y faltan asociaciones intermodales positivas. Esto socava la investigación sobre cómo el aprendizaje intermodal impacta en las tareas intramodales. Abordamos esta brecha con Crisscrossed Captions (CxC), una extensión del conjunto de datos MS-COCO con juicios de similitud semántica humana para 267.095 pares intra e intermodalidad. Presentamos los resultados de referencia en CxC para modelos unimodales y multimodales sólidos existentes. También evaluamos un codificador dual multitarea entrenado en pares de subtítulos de imagen y subtítulos que demuestra de manera crucial el valor de CxC para medir la influencia del aprendizaje intra e intermodal.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=et_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Mitmeliigilise päringu koolituse ja hindamise toetamisega on piltide pealdiste andmekogumid andnud märkimisväärseid edusamme esindusõppes. Kahjuks on andmekogumitel piiratud modaalseid seoseid: pilte ei paarita teiste piltidega, pealdisi paaritatakse ainult teiste sama pildi pealdistega, negatiivseid seoseid puudub ja positiivseid modaalseid seoseid puudub. See õõnestab teadusuuringuid selle kohta, kuidas intermodaalne õpe mõjutab modaalsusesiseseid ülesandeid. Selle lünga lahendame Crisscrossed Captions (CxC), mis on MS-COCO andmekogumi laiendus inimese semantilise sarnasuse otsustega 267 095 modaalsusesisese ja intermodaalsuse paari kohta. Teatame CxC baastulemustest tugevate olemasolevate ühe- ja mitmeliigiliste mudelite puhul. Samuti hindame mitmeülesandelist kahekordset kodeerijat, mis on koolitatud nii pildi-pealdise kui ka pealdise-pealdise paari jaoks, mis näitab oluliselt CxC väärtust intramodaalsuse ja intermodaalsuse õppe mõju mõõtmisel.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=fa_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>با پشتیبانی آموزش و ارزیابی بسیاری از مدل‌ها، مجموعه‌های داده‌های عنوان تصویر، پیشرفت فوق‌العاده در مورد یادگیری نمایش‌دهندگان را پیشنهاد داده است. متاسفانه، مجموعه‌های داده‌ها ارتباط‌های متفاوتی محدود دارند: تصویر با تصویر دیگر جفت نمی‌شوند، عنوان فقط با عنوان دیگر از یک تصویر جفت می‌شوند، ارتباط منفی وجود ندارد و ارتباط‌های متفاوتی مثبت وجود ندارد. این تحقیقات درباره اینکه یادگیری بین‌modalities چگونه بر وظیفه‌های بین‌modalities اثر می‌دهد. ما این فاصله را با کاپیتان‌های کریسسکروس (CxC) دریافت می‌کنیم، وسیله‌ای از مجموعه داده‌های MS-COCO با قضاوت‌های شبیه‌انگیز انسان برای جفت‌های ۲۶۷،095 درون و بین‌اندازی. ما نتیجه‌های پایه‌خط بر CxC گزارش می‌کنیم برای مدل‌های بی‌modal و multimodal قوی وجود دارد. ما همچنین یک قالب دوگانه‌ی چندین کار را ارزیابی می‌کنیم که روی جفت‌های عنوان تصویر و عنوان عنوان عنوان عنوان تصویر آموزش یافته شده است که به طور کلی ارزش CxC را برای اندازه‌گیری تاثیر یادگیری داخل و متوسط موادی نشان می‌دهد.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=fi_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Tukemalla multimodaalista hakukoulutusta ja -arviointia kuvatekstitysten aineistot ovat edistäneet huomattavaa edistystä edustuksen oppimisessa. Valitettavasti aineistoilla on rajallisia multimodaalisia yhteyksiä: kuvia ei pariteta muiden kuvien kanssa, tekstityksiä paritetaan vain saman kuvan muiden tekstitysten kanssa, negatiivisia yhteyksiä ei ole ja positiivisia multimodaalisia yhteyksiä puuttuu. Tämä heikentää tutkimusta siitä, miten intermodaalisuus-oppiminen vaikuttaa intermodaalisuuteen liittyviin tehtäviin. Korjaamme tätä aukkoa Crisscrossed Captions (CxC), MS-COCO-aineiston laajennus ihmisen semanttisen samankaltaisuuden arvioinnilla 267 095 intra- ja intermodaalisuusparille. Raportoimme CxC:n lähtötason tulokset vahvojen olemassa olevien unimodaalisten ja multimodaalisten mallien osalta. Arvioimme myös monikäyttöistä kaksoiskoodausta, joka on koulutettu sekä kuva-kuvateksti- että kuvateksti-kuvapariin, joka osoittaa ratkaisevasti CxC:n arvon intra- ja intermodaalisuuden oppimisen vaikutuksen mittaamisessa.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=fr_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>En prenant en charge la formation et l'évaluation de la récupération multimodale, les ensembles de données de sous-titrage d'images ont stimulé des progrès remarquables en matière d'apprentissage des représentations. Malheureusement, les ensembles de données ont des associations intermodales limitées : les images ne sont pas appariées avec d'autres images, les légendes sont uniquement associées à d'autres légendes de la même image, il n'y a pas d'association négative et il manque des associations intermodales positives. Cela mine la recherche sur l'impact de l'apprentissage intermodalité sur les tâches intra-modales. Nous comblons cette lacune avec Crisscrossed Captions (CxC), une extension de l'ensemble de données MS-COCO avec des jugements de similarité sémantique humaine pour 267 095 paires intra et intermodalité. Nous présentons des résultats de référence sur la CxC pour des modèles unimodaux et multimodaux solides existants. Nous évaluons également un codeur double multitâche entraîné à la fois sur des paires image-légende et légende-légende qui démontre de manière cruciale la valeur de CxC pour mesurer l'influence de l'apprentissage intra et intermodalité.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ga_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Trí thacú le hoiliúint agus le meastóireacht ilmhódúil aisghabhála, spreag tacair shonraí um theidealú íomhánna dul chun cinn suntasach ar fhoghlaim ionadaíochta. Ar an drochuair, tá comhcheangail thrasmhódúla teoranta ag tacair sonraí: ní dhéantar íomhánna a phéireáil le híomhánna eile, ní dhéantar fortheidil a phéireáil ach le fotheidil eile den íomhá chéanna, níl aon chomhcheangail dhiúltacha ann agus tá comhlachais thrasmhódúla dearfacha in easnamh. Baineann sé seo an bonn ó thaighde ar an tionchar a bhíonn ag foghlaim idirmhódúlachta ar thascanna inmhódúlachta. Tugaimid aghaidh ar an mbearna seo le Fotheidil Crisscrossed (CxC), síneadh ar an tacar sonraí MS-COCO le breithiúnais chosúlachta shéimeantaigh dhaonna do 267,095 péire inmhódúlachta agus idirmhódúlachta. Tuairiscímid torthaí bonnlíne ar CxC do mhúnlaí láidre aonmhódacha agus ilmhódacha atá ann cheana féin. Déanaimid meastóireacht freisin ar ionchódóir déach il-thasc atá oilte ar phéirí íomhá-theideal agus ceannteideal a léiríonn go ríthábhachtach an luach atá ag CxC maidir le tionchar na foghlama laistigh agus idirmhódúlachta a thomhas.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ha_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Yi ƙarfafa da tsarin mai motsi na multi-modal, don ka sami tsarin zane da zane-zane masu motsi, sun nuna taƙaitacce mai girma a kan lõkaci na halartawa. Babu rabo, tsarin database sun ƙayyade associations masu tsohon-tsohon mutane: ba za'a haɗa zane da wasu zane ba, tsohon sunayen su sau da wasu sunayen sunayen zane da shi kawai, ba za'a sãmu da associations masu motsi kuma ba za'a sava associations masu son sura-modal. Wannan yana ƙaranci research a kan yadda learning a cikin-moda ke shagala taskõkin-moda. Tuna address wannan gap da aka yi Kriskrysset Titanin (CxC), an yalwato wa MA-CO dataset da mutane na semantic misãli wa 267,095 guda da-moda. Muna buga wani matsalar da ke ƙaranci a kan CxC wa misãlai masu ƙaranci da ke gaba da kwamfyuta masu ƙarfi. Tuna ƙaddara koden dubu wanda aka yi wa wa kodi na zane da sunan-zane da wasu mutane, da za'a nuna kimar CxC da muhimu a ƙayyade muhimmin sha'anin tsarin da za'a karanta cikin-da-kiyaye.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=he_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>על ידי תמיכה באימונים והעריכה של השיגה המולטימודלית, קבוצות נתונים של התמונות מעוררו התקדמות מדהימה על לימוד מייצג. למרבה הצער, קבוצות נתונים יש איגודות דרך מודליות מוגבלות: תמונות לא זוויות עם תמונות אחרות, כותרות זוויות רק עם כותרות אחרות של אותה תמונה, אין איגודות שליליות ואין איגודות דרך מודליות חיוביות חסרות. This undermines research into how inter-modality learning impacts intra-modality tasks. אנו מתמודדים עם הפער הזה עם "Crisscrossed Captions" (CxC), התאריך של קבוצת מידע MS-COCO עם שיפוטים סמנטיים של בני אדם של 267,095 זוגות בתוך ומודליות בין. אנחנו מדווחים על תוצאות בסיסית על CxC עבור דוגמנים חד-מודליים ויותרים מודליים קיימים חזקים. אנחנו גם מעריכים קודד כפול במשימות רבות מאומן על זוגות תמונה-כותר וגם כותר-כותר שמוכיחים באופן חשוב את ערכו של CxC למדוד השפעה של הלימוד בתוך ומודילי.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=hi_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>मल्टी-मोडल पुनर्प्राप्ति प्रशिक्षण और मूल्यांकन का समर्थन करके, छवि कैप्शनिंग डेटासेट ने प्रतिनिधित्व सीखने पर उल्लेखनीय प्रगति को प्रेरित किया है। दुर्भाग्य से, डेटासेट में सीमित क्रॉस-मोडल एसोसिएशन हैं: छवियों को अन्य छवियों के साथ जोड़ा नहीं जाता है, कैप्शन केवल एक ही छवि के अन्य कैप्शन के साथ जोड़े जाते हैं, कोई नकारात्मक संघ नहीं हैं और सकारात्मक क्रॉस-मोडल संघों की कमी है। यह अनुसंधान को कमजोर करता है कि अंतर-साधन सीखने इंट्रा-मोडलिटी कार्यों को कैसे प्रभावित करता है। हम इस अंतर को क्रिसक्रॉस्ड कैप्शन (सीएक्ससी) के साथ संबोधित करते हैं, जो 267,095 इंट्रा- और इंटर-मोडलिटी जोड़े के लिए मानव शब्दार्थ समानता निर्णय के साथ एमएस-कोको डेटासेट का एक विस्तार है। हम मजबूत मौजूदा यूनिमोडल और मल्टीमॉडल मॉडल के लिए सीएक्ससी पर बेसलाइन परिणामों की रिपोर्ट करते हैं। हम छवि-कैप्शन और कैप्शन-कैप्शन जोड़े दोनों पर प्रशिक्षित एक मल्टीटास्क दोहरे एनकोडर का भी मूल्यांकन करते हैं जो महत्वपूर्ण रूप से इंट्रा- और इंटर-मोडलिटी लर्निंग के प्रभाव को मापने के लिए सीएक्ससी के मूल्य को प्रदर्शित करता है।</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=hr_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Podržavajući višemodalnu obuku i procjenu povratka, podaci podataka za snimanje slika izazvali su izuzetan napredak na učenju predstavljanja. Nažalost, podaci imaju ograničene krsnomodalne asocijacije: slike se ne povezuju s drugim slikama, naslovi se povezuju samo sa drugim naslovima iste slike, nema negativnih asocijacija i nedostaju pozitivne krsnomodalne asocijacije. To potiče istraživanje o tome kako međumodalno učenje utječe na zadatke unutar modaliteta. Ovaj prasak se obrađujemo s kapcima Crisscrossed (CxC), produženjem kompleta podataka MS-COCO-a s osuđivanjima ljudske semantičke sličnosti za 267.095 unutar i međumodalnih parova. Prijavljujemo početne rezultate CxC za jake postojeće unimodalne i multimodalne modele. Također procjenjujemo dvostruki koder koji je obučen na parovima slike i kapcije, koji su ključno pokazivali vrijednost CxC-a za mjerenje utjecaja učenja unutar i međumodaliteta.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=hu_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>A multimodális visszakeresési képzés és értékelés támogatásával a képalkotó adatkészletek figyelemreméltó előrelépést eredményeztek a reprezentációs tanulás terén. Sajnos az adatkészleteknek korlátozott mértékben vannak transzmodális asszociációi: a képeket nem párosítják más képekkel, a feliratokat csak ugyanannak a képnek a többi feliratával párosítják, nincsenek negatív asszociációk és hiányoznak pozitív transzmodális asszociációk. Ez aláássa azt a kutatást, hogy az intermodális tanulás milyen hatással van az intermodális feladatokra. Ezt a hiányt a Crisscrossed Captions (CxC) segítségével kezeljük, amely az MS-COCO adatkészlet kiterjesztése 267 095 intra- és intermodalitási párra vonatkozó humán szemantikai hasonlósági ítéletekkel. Az erős unimodális és multimodális modellek esetében jelentjük a CxC alapvető eredményeit. Emellett értékelünk egy multifeladatos kettős kódolót, amely mind a kép-felirat, mind a felirat-felirat párokra képzett, és amely alapvetően bizonyítja a CxC értékét az intra- és intermodalitás tanulás hatásának mérésében.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=hy_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>By supporting multi-modal retrieval training and evaluation, image captioning datasets have spurred remarkable progress on representation learning. Դժբախտաբար, տվյալների համակարգերը սահմանափակ խաչմոդալ կապեր ունեն. պատկերները չեն զուգավորվում այլ պատկերների հետ, վերնագրերը միայն զուգավորվում են նույն պատկերի այլ վերնագրերի հետ, չկան բացասական կապեր և բացակայում են դրական խաչմոդալ կապեր: Սա նվազեցնում է հետազոտությունը, թե ինչպես է միջ-մոդեալ ուսումնասիրությունը ազդում միջ-մոդեալ խնդիրների վրա: Մենք լուծում ենք այս տարբերությունը Քրիսխրոսսփեսի (CxC) կառուցվածքների միջոցով, ՄՍ-ԿՕԿՕ տվյալների համակարգի ընդլայնումը մարդկային սեմանտիկ նմանության դատողություններով 267,085 ինտեր- և միջ-մոդեալ զույգերի համար: Մենք տեղեկացնում ենք CxC-ի հիմնական արդյունքները գոյություն ունեցող ուժեղ միամոդալ և բազմամոդալ մոդելների համար: Մենք նաև գնահատում ենք բազմախնդիրների երկու կոդեր, որը կրթություն է ստացվում և պատկերի-վերլուծության, և վերլուծության-վերլուծության զույգերի վրա, որոնք հիմնականում ցույց են տալիս CxC-ի արժեքը ինտեր- և միջ-մոդելային ուսումնասիրության ազդեցության չափման համար:</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=id_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Dengan mendukung pelatihan dan evaluasi retrieval multi modal, set data captioning gambar telah mendorong kemajuan yang luar biasa dalam pembelajaran representation. Sayangnya, dataset memiliki asosiasi transmodal terbatas: gambar tidak berpasangan dengan gambar lain, captions hanya berpasangan dengan captions lain dari gambar yang sama, tidak ada asosiasi negatif dan tidak ada asosiasi transmodal positif. Ini merusak penelitian bagaimana intermodalitas belajar mempengaruhi tugas intramodalitas. Kami mengatasi ruang ini dengan Crisscrossed Captions (CxC), sebuah ekstensi dari dataset MS-COCO dengan penghakiman semantis persamaan manusia untuk 267.095 pasangan intra- dan intermodalitas. Kami melaporkan hasil dasar pada CxC untuk model unimodal dan multimodal yang kuat. Kami juga mengevaluasi koder dual multitask yang dilatih pada pasangan image-caption dan caption-caption yang paling penting menunjukkan nilai CxC untuk mengukur pengaruh belajar intra- dan intermodalitas.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=it_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Sostenendo la formazione e la valutazione multimodali di recupero, i set di dati per la didascalia delle immagini hanno stimolato notevoli progressi nell'apprendimento della rappresentazione. Purtroppo, i dataset hanno associazioni cross-modali limitate: le immagini non sono accoppiate ad altre immagini, le didascalie sono accoppiate solo ad altre didascalie della stessa immagine, non ci sono associazioni negative e mancano associazioni cross-modali positive. Ciò compromette la ricerca sull'impatto dell'apprendimento intermodale sui compiti intramodali. Affrontiamo questo gap con Crisscrossed Captions (CxC), un'estensione del dataset MS-COCO con giudizi di somiglianza semantica umana per 267.095 coppie intra- e intermodalità. Riportiamo i risultati di base su CxC per modelli unimodali e multimodali forti esistenti. Valutiamo anche un encoder dual multitask addestrato sia sulle coppie immagine-didascalia che didascalia-didascalia che dimostra in modo cruciale il valore di CxC per misurare l'influenza dell'apprendimento intra-e intermodale.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ja_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>マルチモーダル検索トレーニングと評価をサポートすることで、画像キャプションデータセットは表現学習の顕著な進歩を促進しました。残念ながら、データセットのクロスモーダル関連付けは限られています。画像は他の画像とペアになっておらず、キャプションは同じ画像の他のキャプションとペアになっているだけで、負の関連付けはなく、正のクロスモーダル関連付けはありません。これは、モダリティ間学習がモダリティ内タスクにどのように影響するかに関する研究を損なう。我々は、267,095のモーダリティ内およびモーダリティ間のペアについてのヒトのセマンティック類似性判定を備えたMS - COCOデータセットの拡張であるCrisscrossed Captions (CxC)でこのギャップに対処する。強力な既存の単式およびマルチモーダルモデルのCxCのベースライン結果を報告します。また、画像キャプションとキャプションの両方のペアでトレーニングされたマルチタスクデュアルエンコーダーを評価します。これは、モーダリティ内およびモーダリティ間の学習の影響を測定するためのCxCの価値を重要に示します。</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=jv_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>politenessoffpolite"), and when there is a change ("assertivepoliteness item-set This undermines R&R&R&I;S.A.: Awak dhéwé jagat iki karo krisskros Captions Awak dhéwé éntuk sistem sing dadi CxC nggo gambar sistem Unimodal lan model multimodal. Awak dhéwé éntukno sistem multitask duplikasi nggawe lan sesilèh-caption lan caption-caption-caption kuwi nggawe nyimpen CxC nggawe barang nggawe gerakan kanggo mehurakno kanggo ngilanggar nggambar luwih dumateng.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ka_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>მრავალმედი მოდიალური გავიღოთ განსწავლების და განსაზღვრების მიხედვით, გამოსახულების შესაძლებელი მონაცემების შესაძლებელი პროგრესი განსწავლების შესაძლებელად გამოიყენებ მართლად, მონაცემების კონფიგურაციები აქვს გადარჩენებული კრისმოდიალური აზოციაციები: გამოსახულები სხვა გამოსახულებით არიან დაკავშირებული, შესახებ მხოლოდ სხვა გამოსახულებით დაკავშირებული, არიან დარ ეს მოდილიტების შესახებ, როგორ მოდილიტების შესახებ მოდილიტების შესახებ. ჩვენ ამ განსხვავებას კრისკროსური კატაციებით (CxC), MS-COCO მონაცემების მონაცემების განსხვავება, რომელიც ადამიანის სენმანტიკური განსხვავებებით 267 095 ინტერული და ინტერმოდილიტური კოსტაციების განსხვავ ჩვენ CxC-ის მუშაობელი უნიმოდეალური და მულტიმოდეალური მოდელებისთვის შეტყობინებთ. ჩვენ ასევე მრავალ დავამუშავებთ ეუალური კოდერი, რომელიც გამოსახულებული სახელსაწყოთან და სახელსაწყოთან სახელსაწყოთან მრავალური კოდერის მნიშვნელობა CxC-ის მნიშვნელობა, რომელიც ინტერე- და ინტე</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=kk_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Көптеген модельді алу және оқытуды қолдау арқылы кескін айдарындағы деректер жиындары көрсетілетін оқыту үшін белгілі жұмыс істейді. Кешіріңіз, деректер жиындарының көп модельді байланыстары шектелген: кескіндер басқа кескіндермен біріктірілмейді, айдары тек бір кескіндің басқа айдарының біріктірілмейді, негативті байланыстары жоқ және оң көп модель Бұл модельдік үйренудің ішкі ішкі тапсырмаларды қалай әсер ететін зерттеулерді бақылайды. Біз бұл кеңістікті Крискросс айдарлары (CxC) мен, көмегімен 267 095 ішкі және ішкі моделиттік қорлары үшін MS- COCO деректер қорларының кеңістігін қолданамыз. Біз CxC негізгі негізгі нәтижелерді бір модель және көп модель үлгілері үшін хабарлаймыз. Біз сондай-ақ кескін айдары мен айдарының екеуінде оқылған көптеген екі тапсырма кодерін бағалаймыз. Бұл CxC интернет- және медициналық оқыту үшін көпшілікті оқыту үшін мәнін көрсетеді.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ko_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>다중모드 검색 교육과 평가를 지원함으로써 이미지 자막 데이터 집합은 표징 학습에 현저한 진전을 거두었다.불행하게도 데이터 집합의 크로스모드 관련은 유한하다. 이미지는 다른 이미지와 어울리지 않고 자막은 같은 이미지의 다른 자막과 어울리지 않으며 부정적인 관련도 없고 긍정적인 크로스모드 관련도 부족하다.이것은 크로스모드 학습이 크로스모드 임무에 어떻게 영향을 미치는지에 대한 연구를 파괴했다.우리는 교차자막(CxC)으로 이 차이를 해결했다. 이것은 MS-COCO 데이터 집합의 확장이고 267095개의 모드 내와 모드 간에 대한 인류의 의미 유사성 판단을 가지고 있다.우리는 기존의 단봉과 다봉 모델의 CxC 기선 결과를 보고했다.우리는 또한 이미지 제목과 제목 제목이 훈련에 대한 다중 임무 듀얼 인코더를 평가했는데, 이것은 CxC가 모드와 모드 사이의 학습에 미치는 영향을 측정하는 데 있어서의 가치를 매우 중요하게 증명했다.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=lt_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Remiant daugiarūšio naudojimo mokymus ir vertinimą, vaizdo įrašų duomenų rinkiniai paskatino pastebimą pažangą atstovavimo mokymosi srityje. Deja, duomenų rinkiniai turi ribotas tarpmodalines asociacijas: vaizdai nesuderinami su kitais vaizdais, antraštės susiejamos tik su kitais to paties vaizdo antraštėmis, nėra neigiamų asociacijų ir trūksta teigiamų tarpmodalinių asociacijų. This undermines research into how inter-modality learning impacts intra-modality tasks. We address this gap with Crisscrossed Captions (CxC), an extension of the MS-COCO dataset with human semantic similarity judgments for 267,095 intra- and inter-modality pairs. We report baseline results on CxC for strong existing unimodal and multimodal models. Taip pat vertiname daugelį užduočių atliekantį dvigubą kodatorių, apmokytą tiek vaizdo, tiek antraštinės dalies ir antraštinės dalies poromis, kuris iš esmės parodo CxC vertę vertinant mokymosi būdų viduje ir tarpusavio mokymosi įtaką.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=mk_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Со поддршката на мултимодалната обука и евалуација за преземање на слики, наборите на податоци за наслови на сликите поттикнаа извонреден напредок во учењето на претставништвото. За жал, датотеките имаат ограничени крстомодални асоцијации: сликите не се парирани со други слики, насловите се парирани само со други наслови на истата слика, нема негативни асоцијации и недостасуваат позитивни крстомодални асоцијации. Ова го поткопува истражувањето за тоа како интермодијалното учење влијае на интермодијалните задачи. Ние ја решаваме оваа празнина со Crisscrossed Captions (CxC), продолжување на податоците на MS-COCO со човечки семантични пресуди за сличност за 267.095 интермодитални и меѓумодитални парови. Ние известуваме за основните резултати на CxC за силни постоечки унимодални и мултимодални модели. We also evaluate a multitask dual encoder trained on both image-caption and caption-caption pairs that crucially demonstrates CxC's value for measuring the influence of intra- and inter-modality learning.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ml_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>പല-മോഡാല്‍ പിന്തുണയ്ക്കുന്നതിനെയും പിന്തുണയ്ക്കുന്നതിനെയും പിന്തുണയ്ക്കുന്നതിനാല്‍, ഇമേജ് ക്യാപ്റ്റേഷന്‍ ഡ നിര്‍ഭാഗ്യവശാല്‍, ഡാറ്റാസറ്റുകള്‍ ക്രോസ്മോഡല്‍ സംഘടികള്‍ പരിധിയിലാണ്: ചിത്രങ്ങള്‍ മറ്റു ചിത്രങ്ങളോടൊപ്പം ഇണക്കുന്നില്ല, ശേഖരങ്ങള്‍ ഒരേ ചിത്രത്തിന്റ ഇത് വിദ്യാഭ്യാസ പഠിക്കുന്നത് എങ്ങനെയാണെന്ന് ശ്രദ്ധിക്കുന്നു ക്രിസ്ക്രോസ്സ് ക്രോസ്സ് ക്രോസ് ക്രോസ് ക്രോപ്സ് തപ്പുകളുമായി ഈ വ്യത്യാസം വിശദീകരിക്കുന്നു. MS-CO ഡാറ്റാസെറ്റിന്റെ വിശേഷവും, മനുഷ്യന്‍ We report baseline results on CxC for strong existing unimodal and multimodal models. ചിത്രത്തിന്റെ തലക്കെട്ടിന്റെയും തലക്കെട്ടിന്റെയും ജോട്ടില്‍ പഠിപ്പിക്കപ്പെട്ട ഒരു മുള്‍ട്ടിട്ടിക്കൂട്ടിക്കൊണ്ടിരിക്കുന്ന ഒരു രണ്ട് കോഡ</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=mn_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Ихэнх моделийн аврах сургалтыг дэмжиж, үнэлгээ дэмжиж, зурагт хэвлэх өгөгдлийн сангууд нь үзүүлэх сургалтын тухай гайхалтай хөгжлийг дэмжиж байна. Харамсалтай нь, өгөгдлийн сангууд өөр зурагтай холбогдож байдаг: зураг нь өөр зурагтай холбогдож байхгүй, зураг нь зөвхөн ижил зурагтай холбогдож байдаг, сөрөг холбогдол байхгүй, эерэг хэлбэртэй холбогдол байхгүй. Энэ нь зан чанарын суралцах үйл ажиллагаанд хэрхэн нөлөөлдөг талаар судалгааг бууруулдаг. Бид үүнийг Crisscrossed Captions (CxC), MS-COCO өгөгдлийн санг хүн төрөлхтний зэрэгцээ ижил хэмжээний 267,095 интро-болон интромодал хоёрын хувьд нэмэгдүүлэхэд тусалдаг. Бид CxC-ын үндсэн үр дүнг суурилсан нэг болон олон моделийн загваруудын тулд мэдээлэл өгдөг. Мөн бид олон ажлын хоёр давхар коддогчийг дүгнэж байгаа зураг, дуудлага хэвлэлийн хоёр давхар, CxC-ын үнэ цэнийг интро болон интровертийн суралцааны нөлөөлөлийг хэмжихэд үзүүлдэг.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ms_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Dengan menyokong latihan dan penilaian pemulihan berbilang-modal, set data captioning imej telah mendorong kemajuan yang luar biasa dalam pembelajaran perwakilan. Malangnya, set data mempunyai kumpulan melintasi-modal terbatas: imej tidak dipasang dengan imej lain, tajuk hanya dipasang dengan tajuk lain imej yang sama, tiada kumpulan negatif dan hilang kumpulan melintasi-modal positif. Ini merusak kajian bagaimana pembelajaran antarmodalitas mempengaruhi tugas intra-modaliti. Kami mengatasi ruang ini dengan Capsi Crisscrossed (CxC), sambungan set data MS-COCO dengan penilaian semantik persamaan manusia untuk 267,095 pasangan intra- dan intermodaliti. We report baseline results on CxC for strong existing unimodal and multimodal models. Kami juga menilai pengekod dua tugas berbilang yang dilatih pada pasangan imej-caption dan caption-caption yang paling penting menunjukkan nilai CxC untuk mengukur pengaruh pembelajaran intra- dan intermodaliti.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=mt_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>By supporting multi-modal retrieval training and evaluation, image captioning datasets have spurred remarkable progress on representation learning. Sfortunatament, is-settijiet tad-dejta għandhom assoċjazzjonijiet transmodali limitati: l-immaġni mhumiex imqabbla ma’ immaġni oħra, il-intestaturi huma mqabbla biss ma’ intestaturi oħra tal-istess immaġni, ma hemm l-ebda assoċjazzjonijiet negattivi u hemm nuqqas ta’ assoċjazzjonijiet transmodali pożittivi. Dan jimmina r-riċerka dwar kif it-tagħlim inter-modalità għandu impatt fuq il-kompiti intramodali. Aħna nindirizzaw din id-distakk mal-Kapitoli Crisscrossed (CxC), estensjoni tad-dataset MS-COCO b’sentenzi semantiċi ta’ similarità umana għal 267,095 par intra- u inter-modality. Aħna nirrappurtaw ir-riżultati tal-linja bażi dwar CxC għal mudelli unimodali u multimodali eżistenti b’saħħithom. Aħna jevalwaw ukoll kodifikatur doppju multikompiti mħarreġ kemm fuq par ta' immaġni-titolu kif ukoll titolu-titolu li juri b'mod kruċjali l-valur ta' CxC għall-kejl tal-influwenza tat-tagħlim intra-modali u intermodali.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=nl_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Door multimodale retrieval training en evaluatie te ondersteunen, hebben beeldbijschriftdatasets opmerkelijke vooruitgang op het gebied van representatie learning gestimuleerd. Helaas hebben datasets beperkte crossmodale associaties: afbeeldingen worden niet gekoppeld aan andere afbeeldingen, bijschriften worden alleen gekoppeld aan andere bijschriften van dezelfde afbeelding, er zijn geen negatieve associaties en er ontbreken positieve crossmodale associaties. Dit ondermijnt het onderzoek naar de invloed van intermodaal leren op intramodale taken. We verhelpen deze kloof met Crisscrossed Captions (CxC), een uitbreiding van de MS-COCO dataset met humane semantische vergelijkingsoordelen voor 267,095 intra- en intermodaliteitsparen. We rapporteren baseline resultaten op CxC voor sterke bestaande unimodale en multimodale modellen. We evalueren ook een multitask dual encoder die getraind is op zowel beeld-bijschrift als bijschrift-bijschrift paren die de waarde van CxC voor het meten van de invloed van intra- en intermodaliteit leren cruciaal aantoont.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=no_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Med å støtte fleire modal opplæring og evaluering, har biletetatassett ført til merkelige framgang ved å lære representasjonar. Dessverre har datasett begrenset krysmodale tilknytingar: bilete er ikkje saman med andre bilete, tittel er berre saman med andre tittel av det same biletet, det finst ingen negativ tilknytingar og det manglar positiv krysmodale tilknytingar. Dette understrekar forskning på korleis læring av intermodalitet påvirkar inni modalitetsoppgåver. Vi adresserer dette mellomrommet med Crisscrossed Captions (CxC), eit utviding av MS-COCO-dataset med menneskelsemantiske sprøytebruk for 267,095 intra- og mellommodalitetspar. Vi rapporterer baseline resultat på CxC for sterke unimodal og multimodal modeller som finst. Vi evaluerer også ein fleire oppgåver dual koder som treng på både biletet- tittel og tittel- par som viser CxC- verdien for å måle effekten av læring av intra- og mellommodalitet.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=pl_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Dzięki wspieraniu szkoleń i oceny multimodalnego odzyskiwania zbiorów danych dotyczących napisów zdjęć przyczyniły się do niezwykłego postępu w nauce reprezentacji. Niestety zbiory danych mają ograniczone skojarzenia crossmodalne: obrazy nie są sparowane z innymi obrazami, podpisy są sparowane tylko z innymi podpisami tego samego obrazu, nie ma negatywnych skojarzeń i brakuje pozytywnych skojarzeń crossmodalnych. Podważa to badania nad tym, w jaki sposób uczenie się intermodalne wpływa na zadania intermodalne. Rozwiązujemy tę lukę za pomocą Crisscross Captions (CxC), rozszerzenia zbioru danych MS-COCO o ludzkie oceny podobieństwa semantycznego dla par wewnątrz- i intermodalnych 267,095. Raportujemy wyniki bazowe dotyczące CxC dla silnych istniejących modeli unimodalnych i multimodalnych. Oceniamy również wielozadaniowy podwójny koder przeszkolony zarówno na parach obrazu-podpisu, jak i napisu-podpisu, który istotnie demonstruje wartość CxC w mierzeniu wpływu uczenia się wewnątrz- i intermodalnego.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=pt_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Ao oferecer suporte ao treinamento e avaliação de recuperação multimodal, os conjuntos de dados de legendagem de imagens estimularam um progresso notável no aprendizado de representação. Infelizmente, os conjuntos de dados têm associações multimodais limitadas: as imagens não são emparelhadas com outras imagens, as legendas são emparelhadas apenas com outras legendas da mesma imagem, não há associações negativas e estão ausentes associações modais cruzadas positivas. Isso prejudica a pesquisa sobre como a aprendizagem intermodal afeta as tarefas intramodal. Abordamos essa lacuna com legendas cruzadas (CxC), uma extensão do conjunto de dados MS-COCO com julgamentos de semelhança semântica humana para 267.095 pares intra e intermodalidade. Relatamos resultados de linha de base em CxC para modelos unimodais e multimodais fortes existentes. Também avaliamos um codificador duplo multitarefa treinado em pares de legenda e legenda de imagem que demonstra crucialmente o valor do CxC para medir a influência do aprendizado intra e intermodalidade.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ro_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Prin sprijinirea formării și evaluării multimodale de recuperare, seturile de date privind subtitrarea imaginilor au stimulat progrese remarcabile în învățarea reprezentării. Din păcate, seturile de date au asocieri cross-modale limitate: imaginile nu sunt asociate cu alte imagini, subtitrările sunt asociate doar cu alte subtitrări ale aceleiași imagini, nu există asocieri negative și lipsesc asocieri cross-modale pozitive. Acest lucru subminează cercetarea privind impactul învățării intermodale asupra sarcinilor intramodale. Abordăm acest decalaj cu Crisscrossed Captions (CxC), o extensie a setului de date MS-COCO cu judecăți de similitudine semantică umană pentru 267.095 perechi intra- și intermodalitate. Raportăm rezultatele referitoare la CxC pentru modele unimodale și multimodale puternice existente. Evaluăm, de asemenea, un encoder dual multitask instruit atât pe perechile imagine-captare, cât și pe perechile captare-captare, care demonstrează esențial valoarea CxC pentru măsurarea influenței învățării intra-și intermodale.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ru_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Поддерживая обучение и оценку мультимодального поиска, наборы данных для подписи к изображениям способствовали значительному прогрессу в обучении репрезентации. К сожалению, наборы данных имеют ограниченные кросс-модальные ассоциации: изображения не спариваются с другими изображениями, подписи спариваются только с другими подписями того же изображения, нет отрицательных связей и отсутствуют положительные кросс-модальные связи. Это подрывает исследования о том, как интермодальное обучение влияет на внутримодальные задачи. Мы устраняем этот пробел с помощью перекрестных заголовков (CxC), расширения набора данных MS-COCO с оценками семантического сходства человека для 267 095 внутримодальных и межмодальных пар. Мы сообщаем исходные результаты по CxC для надежных существующих моделей для отдельных видов транспорта и смешанных моделей. Мы также оцениваем многозадачный двойной кодировщик, обученный как парам захвата изображений, так и парам субтитров, который критически демонстрирует ценность CxC для измерения влияния внутримодального и интермодального обучения.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=si_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>ගොඩක් මොඩාල් ප්‍රධානය සහ විශ්ලේෂණය සඳහා පින්තූර ප්‍රධානය සඳහා පින්තූර ප්‍රධානය සඳහා පින්තූර ප්‍රධ අවාසනාවෙන්න, දත්ත සැට් සම්බන්ධ විශේෂයක් තියෙනවා: පින්තූරය අනිත් පින්තූරය සමග සම්බන්ධ වෙන්නේ නැහැ, පින්තූරය සම්බන්ධ වෙන්න මේක පරීක්ෂණය අඩුවෙනවා කොහොමද අන්තර්භාවිතාවක් ඉගෙන ගන්නේ කියලා, අන්තර්භාවිතාවක් වැඩේ අ අපි මේක ක්‍රිස්ක්‍රොස් කැප්ටන්ස් සමග මෙහෙයුම් කරනවා, MS-COCO දත්ත සැකසුම් සමග මිනිස්සුන්ගේ සැමැන්තික සමාන්‍ය විශ්වාස කරන්න 267,325 intra- සහ අ අපි CxC වලට ප්‍රතිචාර ප්‍රතිචාරයක් තියෙන්නේ බලපොරොත්තු නිමෝඩාල් සහ ගොඩක් මෝඩේල් වලට. අපි වගේම ගොඩක් වැඩි කාර්යයක් අවශ්‍ය කරනවා දුවන් කේප්ටර් එක්ක සහ පින්තූරණය සහ පින්තූර-කේප්ටර් එක්ක දෙන්නම් ප්‍රශ්නය කරලා තියෙන්න</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=sk_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>S podporo večmodalnemu usposabljanju in vrednotenju pridobivanja podatkov o napisovanju slik so nabori podatkov o napisovanju slik spodbudili izjemen napredek pri učenju reprezentacije. Na žalost imajo nabori podatkov omejene medmodalne povezave: slike niso združene z drugimi slikami, napisi so združeni samo z drugimi napisi iste slike, negativnih povezav ni in manjkajo pozitivne medmodalne povezave. To spodkopava raziskave o tem, kako intermodalno učenje vpliva na naloge znotraj modalnosti. To vrzel obravnavamo s Crisscrossed Captions (CxC), razširitvijo podatkovnega nabora MS-COCO s sodbami človeške semantične podobnosti za 267.095 intra- in intermodalnih parov. Poročamo osnovne rezultate CxC za močne obstoječe unimodalne in multimodalne modele. Ocenjujemo tudi večopravilni dvojni kodirnik, usposobljen za pare slike-napis in napis-napis, ki ključno dokazuje vrednost CxC za merjenje vpliva znotraj- in intermodalnega učenja.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=so_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Kaalmeynta waxbarashada dib u qaadashada iyo qiimeynta, sawirka la qabsashada macluumaadka waxaa soo baxay horumar la yaab leh oo ku saabsan waxbarashada la barto. Unfortunately, datasets have limited cross-modal associations: images are not paired with other images, captions are only paired with other captions of the same image, there are no negative associations and there are missing positive cross-modal associations. Waxbarashadan waxaa hoos looga baaraandegayaa sida waxbarashada hab-dhexe ay u saameyso shaqada qaabka ah. Waxaannu ka sheekeysanaynaa goobahan Crisscross Captions (CxC), extension of the MS-CO dataset with humane semantic similarity for 267,095 intra- and inter-modality pairs. Waxaan wargelinaynaa resultiyada aasaasiga ah ee CxC ku saabsan tusaalooyin aad u xoog badan oo ay ku jiraan noocyo badan. Sidoo kale waxaynu qiimeynaynaa qodobka labada qodob ee loo baray sawirka iyo labada nooc oo ay si muhiim ah u muujiyaan qiimaha CxC si loo qiyaaso saameynta barashada xilliga iyo barashada xilliga dhexe.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=sq_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Duke mbështetur trajnimin dhe vlerësimin e tërheqjes multimodale, grupet e të dhënave të titullimit të imazheve kanë nxitur përparim të shquar në mësimin e përfaqësimit. Fatkeqësisht, grupet e të dhënave kanë shoqata të kufizuara ndërmodali: imazhet nuk janë të barabarta me imazhe të tjera, titujt janë të barabarta vetëm me titujt e tjerë të të njëjtit imazh, nuk ka shoqata negative dhe mungon shoqata pozitive ndërmodali. Kjo dëmton kërkimin se si mësimi ndërmodalitet ndikon në detyrat brenda modalitetit. Ne e trajtojmë këtë boshllëk me Captions Crisscrossed (CxC), një zgjerim të dataset MS-COCO me gjykime njerëzore semantike të ngjashmërisë për 267,095 çifte brenda dhe ndërmodaliteti. Ne raportojmë rezultatet bazë në CxC për modele të forta ekzistuese unimodale dhe multimodale. We also evaluate a multitask dual encoder trained on both image-caption and caption-caption pairs that crucially demonstrates CxC's value for measuring the influence of intra- and inter-modality learning.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=sr_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Podržavajući multimodalnu obuku povratka i procjenu, kompleta podataka za snimanje slika izazvala je izuzetan napredak na učenje predstavljanja. Nažalost, podaci imaju ograničene krsnomodalne asocijacije: slike nisu povezani sa drugim slikama, naslovi su povezani samo sa drugim naslovima iste slike, nema negativnih asocijacija i nedostaju pozitivne krsnomodalne asocijacije. To potakne istraživanje o tome kako međumodalno učenje utiče na zadatak unutar modaliteta. Rešavamo taj praznik sa kapcima Crisscrossed (CxC), produženjem kompleta podataka MS-COCO sa osuđivanjima ljudske semantičke sličnosti za 267.095 unutrašnjih i međumodalnih parova. Prijavljamo početne rezultate CxC za jake postojeće unimodalne i multimodalne modele. Takođe procjenjujemo dvostruki koder koji je obučen i na parovima slike i kapcije, koji su ključno pokazivali vrijednost CxC-a za mjerenje utjecaja učenja unutar i međumodaliteta.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=sv_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Genom att stﾃｶdja multimodal hﾃ､mtning utbildning och utvﾃ､rdering har bildtextningsdatauppsﾃ､ttningar sporrat anmﾃ､rkningsvﾃ､rda framsteg nﾃ､r det gﾃ､ller representationsinlﾃ､rning. Tyvﾃ､rr har datauppsﾃ､ttningar begrﾃ､nsade tvﾃ､rmodala associationer: bilder paras inte ihop med andra bilder, bildtexter paras bara ihop med andra bildtexter av samma bild, det finns inga negativa associationer och det saknas positiva tvﾃ､rmodala associationer. Detta undergrﾃ､ver forskningen om hur intermodalt lﾃ､rande pﾃ･verkar intramodalitetsuppgifter. Vi tar itu med detta gap med Crisscrossed Captions (CxC), en fﾃｶrlﾃ､ngning av MS-COCO datauppsﾃ､ttningen med mﾃ､nskliga semantiska likheter bedﾃｶmningar fﾃｶr 267 095 intra- och intermodalitetspar. Vi rapporterar basresultat pﾃ･ CxC fﾃｶr starka befintliga unimodala och multimodala modeller. Vi utvﾃ､rderar ocksﾃ･ en multitask dual encoder utbildad pﾃ･ bﾃ･de bild-bildtext och bildtext-bildtext par som avgﾃｶrande visar CxC:s vﾃ､rde fﾃｶr att mﾃ､ta pﾃ･verkan av intra- och intermodalitetslﾃ､rande.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=sw_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Kwa kuunga mkono mafunzo na kutathmini mafunzo ya upatikanaji wa mitandao mbalimbali, seti za picha zinazoonyesha maendeleo mazuri kuhusu kujifunza kwa uwakilishi. Kwa bahati mbaya, seti za taarifa zimekuwa vizuizi vikubwa vya vyama vya upande: picha hazijaunganishwa na picha nyingine, vichwa vyao vinajungwa na vichwa vingine vya picha hiyo, hakuna vyama vya hasi na hakuna vyama vyenye vyama vya upande chanya. Hii inapunguza utafiti wa namna ya kujifunza kwa namna ya aina tofauti inavyoathiri kazi za utamaduni. Tunaongelea gaidi hili na vichwa vya habari vya Crisscross (CxC), kuongezeka kwa taarifa za MS-CO zenye maamuzi yanayofanana na wanadamu 267,095 kwa wanaume wa ndani na tofauti. Tunatoa taarifa za matokeo ya msingi kwenye CxC kwa mifano yenye nguvu ya kipekee na mifano mingi. Kadhalika tunatathmini kodi ya mara mbili inayofundishwa kwa picha pamoja na viwili vya vichwa vya habari ambavyo vinaonyesha thamani ya CxC kwa kupima ushawishi wa kujifunza kwa njia za ndani na kwa njia nyingine.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ta_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>பல- மாற்று மீட்டெடுப்பு பயிற்சி மற்றும் மதிப்பீட்டை ஆதரிக்கையால், பிம்பம் பிடிப்பு தகவல் பெறுதல் தகவல் அமைப்புகள் பிரத துரதிர்ஷ்டவசமாக, தகவல் அமைப்புகள் கிடைக்கப்பட்டுள்ளது: பிம்பங்கள் மற்ற படங்களுடன் இணைக்கப்படவில்லை, தலைப்புகள் மட்டும் ஒரே பிம்பத்தின் மற்ற தலைப்புகளுடன் ஜோ இந்த ஆராய்ச்சியை குறைவாக்குகிறது இடைமுறையில் கற்றுக்கொள்ளும் வகையில் உள்ள முறைமையான பணிகளை எவ்வா @ info நாம் அடிப்படைக்கோடு முடிவு பிம்பத்தின் தலைப்புகள் மற்றும் தலைப்புகள் ஜோடியிலும் பயிற்சி செய்யப்பட்ட பல்திருக்கும் இரட்டை குறியீட்டை மதிப்பிடுகிறோம். இது CxC விளைவு</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=tr_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Çoklu modal gaýd etmek we deňlenmek üçin resim küpşenleri taýýarlamak üçin aýratyn öwrenmek üçin aýratyn ösümligi bellendirdi. Gynansakda, veri düzümleri çykaryp modal baglaýyşlar bardyr: suratlar başga suratlar bilen baglanmaýar, käpşenler diňe ayn suratyň başga käpşenleri bilen baglanmaýar, hiç hili täsir baglaýyşlar ýok we ymyk modal baglaýyşlar ýok. Bu modalitet öwrenmeniň modalitet zadynyň içine nähili täsiri täsirini azaltýar. Biz bu gaplary Crisscrossed Captions (CxC) ile çözeriz, MS-COCO veri setiniň 267,095 intra- we modalitet çiftleri üçin bir döwletlere golaýlaşýarys. Biz bu ýerde bolan unimodal we multimodal nusgalar üçin CxC barada esasy netijeleri baglaýarys. Biz hem surat-başlığı hem de kelime-başlığı üzerinde eğitilen bir çoklu işaretli çizgini değerlendiriyoruz. Bu şekilde CxC'nin intra ve modalitet öğrenmesinin etkisini ölçüde değerlendirmek üçin önemli bir şekilde gösteriyor.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ur_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>بہت سی موڈال پھیرنے کی تعلیم اور ارزیابی کی مدد کے ذریعہ، تصویر کاپٹینگ ڈیٹ سٹ کے ذریعہ نمایش کی تعلیم کے بارے میں بہت اچھا پیشرفت ہے. بدبختی ہے، ڈاٹ سٹ کے لئے مقدار کرس موڈال اتصال ہیں: تصاویر دوسرے تصاویروں کے ساتھ جوڑ نہیں ہوتے، کپٹ صرف ایک تصویر کے دوسرے کپٹ کے ساتھ جوڑ کئے جاتے ہیں، کوئی منفی اتصال نہیں ہے اور کوئی مثبت کرس موڈال اتصال نہیں ہوتے۔ یہ تحقیقات کے ذریعہ تحقیقات کو کمزور کرتا ہے کہ درمیان موڈلی تعلیم کس طرح موڈلی کے کاموں پر اثر دیتا ہے۔ ہم اس فاصلہ کو کریسسکروس کی کاپیشن (CxC) کے ساتھ سمجھتے ہیں، ایک مئس-COCO ڈیٹ سٹ کے مطابق 267,095 داخل اور درمیان موڈلیٹی جوڑوں کے ساتھ انسان کی سیمانٹی برابری کا فیصلہ کرنا ہے. ہم CxC پر بنیاس لین نتائج راپورٹ کر رہے ہیں مضبوط موجود غیر موڈال اور متعدم موڈل موڈل کے لئے۔ ہم نے بھی ایک multitask دوئل کوڈر کا ارزش کیا ہے جو تصویر-کپشن اور کپشن-کپشن جوڑوں پر آموزش کی جاتی ہے جو CxC کے ارزش کو دکھاتا ہے کہ اس کے اندر اور درمیان موڈلیت کی تعلیم کے مطابق اندازے کے لئے۔</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=uz_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Koʻproq modulni olib tashlash va qiymatni qoʻllashda, rasm olish va maʼlumot etish tugmasini taʼminlashda ajoyib muvaffaqiyatsiz qoʻlladi. Uzunasiz, maʼlumotlar tarkibi boshqa modal aloqalarida chegara boʻlgan. Rasmlar boshqa rasmlar bilan bir xil boʻlmaydi, sarlavhaslar faqat bitta rasmning boshqa sarlavhasi bilan bir nechta boʻlmaydi, negativ bogʻliqlar mavjud emas va joylashtirish usuli yoʻq. Bu o'rganishni o'rganishning har xil usuli vazifalarini qanday qilishini yaratadi. @ info Biz muvaffaqiyatli unimodal va multimodal modellari uchun CxC asosiy natijalari haqida xabar beramiz. Biz sur'ning sarlavhasi va sarlavhasining ikki marta o'rganilgan multitask kodlash qoidasini qiymatimiz. Bu yerda CxC qiymatini ko'rsatadi.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=vi_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Bằng cách hỗ trợ đào tạo và đánh giá nhiều phương tiện khác nhau, chương trình đánh giá ảnh đã thúc đẩy một tiến bộ đáng chú ý về học quyền đại diện. Không may, các tập tin có giới hạn các cách mạng: hình ảnh không kết hợp với các ảnh khác, hình ảnh chỉ kết hợp với các chỉ dẫn khác của cùng một hình ảnh, không có liên quan tiêu cực nào và thiếu các liên quan tiêu biểu tích cực. Việc này ảnh hưởng đến cách học nội động. Chúng ta giải quyết vấn đề này bằng các Bắt tại giải mã, một phần mở rộng dữ liệu của xơ rải rác CO với tỉ lệ giống nhau cơ bản chung với giá 27,095 phân loại và liên cách. Chúng tôi báo cáo kết quả sơ bộ về CxC cho những mô- đun và đa phương. Chúng tôi cũng đánh giá một bộ mã hóa kép đa nhiệm được huấn luyện về cả hai cặp chụp ảnh và chụp ảnh mà cho thấy giá trị của C5C với việc đo lường ảnh hưởng của học tập nội giao và nội giao.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=zh_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>多模态检练评估,图像字幕数集于学。 不幸者,数集有限跨模态关联:图不与他配对,标题仅与同题配对,不负关联,且缺正跨模态联。 此坏模态间学模态内事也。 CxC)以纵横之标(CxC)决此差,CxCMS-COCO数集之广,有267,095模态内模态间人语义相似性决之。 以告 CxC 之强大单式与多式联运之基线也。 又评图像-标题-标对上练之多任务双编码器,当编码器至要而证之CxC量模内模态之间学之价。</span></div></div><dl><dt>Anthology ID:</dt><dd>2021.eacl-main.249</dd><dt>Volume:</dt><dd><a href=/volumes/2021.eacl-main/>Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</a></dd><dt>Month:</dt><dd>April</dd><dt>Year:</dt><dd>2021</dd><dt>Address:</dt><dd>Online</dd><dt>Venue:</dt><dd><a href=/venues/eacl/>EACL</a></dd><dt>SIG:</dt><dd></dd><dt>Publisher:</dt><dd>Association for Computational Linguistics</dd><dt>Note:</dt><dd></dd><dt>Pages:</dt><dd>2855–2870</dd><dt>Language:</dt><dd></dd><dt>URL:</dt><dd><a href=https://aclanthology.org/2021.eacl-main.249>https://aclanthology.org/2021.eacl-main.249</a></dd><dt>DOI:</dt><dd><a href=http://dx.doi.org/10.18653/v1/2021.eacl-main.249 title="To the current version of the paper by DOI">10.18653/v1/2021.eacl-main.249</a></dd><dt class=acl-button-row>Bibkey:</dt><dd class=acl-button-row><button type=button class="btn btn-clipboard-outside btn-secondary btn-sm d-none" data-clipboard-target=#citePaperBibkey><i class="far fa-clipboard"></i><span id=citePaperBibkey class="pl-2 text-monospace">parekh-etal-2021-crisscrossed</span></button></dd><dt>Cite (ACL):</dt><dd><span id=citeACL>Zarana Parekh, Jason Baldridge, Daniel Cer, Austin Waters, and Yinfei Yang. 2021. <a href=https://aclanthology.org/2021.eacl-main.249>Crisscrossed Captions : Extended Intramodal and Intermodal Semantic Similarity Judgments for MS-COCOMS-COCO</a>. In <i>Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</i>, pages 2855–2870, Online. Association for Computational Linguistics.</span><button type=button class="btn btn-clipboard btn-secondary btn-sm d-none ml-2" data-clipboard-target=#citeACL><i class="far fa-clipboard"></i></button></dd><dt>Cite (Informal):</dt><dd><span id=citeRichText><a href=https://aclanthology.org/2021.eacl-main.249>Crisscrossed Captions : Extended Intramodal and Intermodal Semantic Similarity Judgments for MS-COCOMS-COCO</a> (Parekh et al., EACL 2021)</span><button type=button class="btn btn-clipboard btn-secondary btn-sm d-none ml-2" data-clipboard-target=#citeRichText><i class="far fa-clipboard"></i></button></dd><dt class=acl-button-row>Copy Citation:</dt><dd class=acl-button-row><button type=button class="btn btn-clipboard-outside btn-secondary btn-sm d-none" data-clipboard-target=#citeMarkdownContent><i class="far fa-clipboard pr-2"></i>Markdown</button>
<button type=button class="btn btn-secondary btn-sm" data-toggle=modal data-target=#citeModal>More options…</button></dd><dt>PDF:</dt><dd><a href=https://aclanthology.org/2021.eacl-main.249.pdf>https://aclanthology.org/2021.eacl-main.249.pdf</a></dd><dt>Data</dt><dd><a href=https://paperswithcode.com/dataset/cxc>CxC</a>,&nbsp;<a href=https://paperswithcode.com/dataset/coco>COCO</a>,&nbsp;<a href=https://paperswithcode.com/dataset/conceptual-captions>Conceptual Captions</a>,&nbsp;<a href=https://paperswithcode.com/dataset/flickr30k>Flickr30k</a></dd><dt>Terminologies:</dt><dd id=terms></dd></dl></div><div class=acl-paper-link-block><a class="btn btn-primary" href=https://aclanthology.org/2021.eacl-main.249.pdf title="Open PDF of 'Crisscrossed Captions : Extended Intramodal and Intermodal Semantic Similarity Judgments for MS-COCOMS-COCO'"><i class="far fa-file-pdf"></i><span class=pl-2>PDF</span></a>
<a class="btn btn-secondary" href="https://www.semanticscholar.org/search?q=Crisscrossed+Captions+%3A+Extended+Intramodal+and+Intermodal+Semantic+Similarity+Judgments+for+MS-COCOMS-COCO" title="Search for 'Crisscrossed Captions : Extended Intramodal and Intermodal Semantic Similarity Judgments for MS-COCOMS-COCO' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class="pl-sm-2 d-none d-sm-inline">Search</span></a>
<a class="btn btn-dark" data-toggle=modal data-target=#translateModal title="Translate for 'Crisscrossed Captions : Extended Intramodal and Intermodal Semantic Similarity Judgments for MS-COCOMS-COCO'" style=color:#fff><i class="fas fa-language"></i><span class=pl-2>Translate</span></a></div></div><hr><div class="modal fade" id=citeModal tabindex=-1 role=dialog aria-labelledby=citeModalLabel aria-hidden=true><div class="modal-dialog modal-lg" role=document><div class=modal-content><div class=modal-header><h5 class=modal-title id=citeModalLabel>Export citation</h5><button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><ul class="nav nav-tabs mb-2" id=citeFormats role=tablist><li class=nav-item><a class="nav-link disabled" data-toggle=list href=#citeBibtex role=tab aria-controls=citeBibtex aria-selected=false>BibTeX</a></li><li class=nav-item><a class="nav-link disabled" data-toggle=list href=#citeMods role=tab aria-controls=citeMods aria-selected=false>MODS XML</a></li><li class=nav-item><a class="nav-link disabled" data-toggle=list href=#citeEndnote role=tab aria-controls=citeEndnote aria-selected=false>Endnote</a></li><li class=nav-item><a class="nav-link active" data-toggle=list href=#citeMarkdown role=tab aria-controls=citeMarkdown aria-selected=true>Preformatted</a></li></ul><div class=tab-content id=citeFormatsContent><div class="tab-pane active" id=citeBibtex role=tabpanel></div><div class=tab-pane id=citeMods role=tabpanel></div><div class=tab-pane id=citeEndnote role=tabpanel></div><div class=tab-pane id=citeMarkdown role=tabpanel><h5>Markdown (Informal)</h5><p id=citeMarkdownContent class="text-monospace small bg-light border p-2">[Crisscrossed Captions : Extended Intramodal and Intermodal Semantic Similarity Judgments for MS-COCOMS-COCO](https://aclanthology.org/2021.eacl-main.249) (Parekh et al., EACL 2021)</p><ul class=mt-2><li><a href=https://aclanthology.org/2021.eacl-main.249>Crisscrossed Captions : Extended Intramodal and Intermodal Semantic Similarity Judgments for MS-COCOMS-COCO</a> (Parekh et al., EACL 2021)</li></ul><h5>ACL</h5><ul class=mt-2><li id=citeACLstyleContent>Zarana Parekh, Jason Baldridge, Daniel Cer, Austin Waters, and Yinfei Yang. 2021. <a href=https://aclanthology.org/2021.eacl-main.249>Crisscrossed Captions : Extended Intramodal and Intermodal Semantic Similarity Judgments for MS-COCOMS-COCO</a>. In <i>Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</i>, pages 2855–2870, Online. Association for Computational Linguistics.</li></ul><div class="modal-footer pb-1"><button type=button class="btn btn-clipboard btn-primary d-none" data-clipboard-target=#citeMarkdownContent><i class="far fa-clipboard pr-2"></i>Copy Markdown to Clipboard</button>
<button type=button class="btn btn-clipboard btn-primary d-none" data-clipboard-target=#citeACLstyleContent><i class="far fa-clipboard pr-2"></i>Copy ACL to Clipboard</button></div></div></div></div></div></div></div><div class="modal fade" id=translateModal tabindex=-1 role=dialog aria-labelledby=translateModalLabel aria-hidden=true><div class="modal-dialog modal-lg" role=document><div class=modal-content><div class=modal-header><h5 class=modal-title id=citeModalLabel><i class="fas fa-language"></i> Translate</h5><button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body style=text-align:center><input id=lang_query type=text class="form-control mr-sm-2" style="width:50%;margin:0 auto!important" name=language placeholder=Search...><br><div id=buttons></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script><script src=/js/clipboard.min.js></script>
<script>let lang_codes=["af","sq","am","ar","hy","az","bn","bs","bg","ca","zh","hr","cs","da","nl","et","fl","fi","fr","ka","de","el","ha","he","hi","hu","is","id","ga","it","ja","jv","kk","ko","lt","mk","ms","ml","mt","mn","no","fa","pl","pt","ro","ru","sr","si","sk","so","es","sw","sv","ta","bo","tr","uk","ur","uz","vi","en"],languages=["Afrikaans","Albanian","Amharic","Arabic","Armenian","Azerbaijani","Bengali","Bosnian","Bulgarian","Catalan","Chinese","Croatian","Czech","Danish","Dutch","Estonian","Filipino","Finnish","French","Georgian","German","Greek","Hausa","Hebrew","Hindi","Hungarian","Icelandic","Indonesian","Irish","Italian","Japanese","Javanese","Kazakh","Korean","Lithuanian","Macedonian","Malay","Malayalam","Maltese","Mongolian","Norwegian","Persian","Polish","Portuguese","Romanian","Russian","Serbian","Sinhala","Slovak","Somali","Spanish","Swahili","Swedish","Tamil","Tibetan","Turkish","Ukranian","Urdu","Uzbek","Vietnamese","English"];$(document).ready(function(){if(create_buttons(),ClipboardJS.isSupported()){success_fn=function(t){var e=$(t.trigger);e.toggleClass("btn-success"),e.children("i").toggleClass("far fa-clipboard fas fa-clipboard-check"),t.clearSelection(),setTimeout(function(){e.toggleClass("btn-success"),e.children("i").toggleClass("far fa-clipboard fas fa-clipboard-check")},2e3)};var e,t=new ClipboardJS(".btn-clipboard");t.on("success",success_fn),$(".btn-clipboard").removeClass("d-none"),e=new ClipboardJS(".btn-clipboard-outside",{text:function(e){var t=e.getAttribute("data-clipboard-target");return $(t).text()}}),e.on("success",success_fn),$(".btn-clipboard-outside").removeClass("d-none")}}),$("#lang_query").on("input",function(){var e=$(this),t=e.val();let n=document.getElementById("buttons");if(n.innerHTML="",e.data("lastval")!=t){e.data("lastval",t);for(let e in languages){let s=languages[e],o=lang_codes[e];s.includes(t)&&(n.innerHTML+=`<button class='btn btn-secondary' onclick="show_lang('${o}')" data-dismiss='modal' style='margin:10px; width:120px; text-align: center;'><span class='pl-2'>${s}</span></button>`)}}});function create_buttons(){let e=document.getElementById("buttons");for(let t in languages){let n=languages[t],s=lang_codes[t];e.innerHTML+=`<button class='btn btn-secondary' onclick="show_lang('${s}')" data-dismiss='modal' style='margin:10px; width:120px; text-align: center;'><span class='pl-2'>${n}</span></button>`}}function show_lang(e){hide_all(),console.log(e),$("#"+e+"_abstract").show(),$("#"+e+"_title").show()}function hide_all(){for(let t in lang_codes){let e=lang_codes[t];$("#"+e+"_abstract").hide(),$("#"+e+"_title").hide()}}</script></body></html>