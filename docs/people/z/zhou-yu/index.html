<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Zhou Yu - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Zhou</span> <span class=font-weight-bold>Yu</span></h2><hr><div class=row><div class=col-lg-9><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.189.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--189 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.189 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.189/>Discovering Dialogue Slots with Weak Supervision</a></strong><br><a href=/people/v/vojtech-hudecek/>Vojtěch Hudeček</a>
|
<a href=/people/o/ondrej-dusek/>Ondřej Dušek</a>
|
<a href=/people/z/zhou-yu/>Zhou Yu</a><br><a href=/volumes/2021.acl-long/ class=text-muted>Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--189><div class="card-body p-3 small">Task-oriented dialogue systems typically require manual annotation of dialogue slots in training data, which is costly to obtain. We propose a method that eliminates this requirement : We use weak supervision from existing linguistic annotation models to identify potential slot candidates, then automatically identify domain-relevant slots by using clustering algorithms. Furthermore, we use the resulting slot annotation to train a neural-network-based tagger that is able to perform slot tagging with no human intervention. This <a href=https://en.wikipedia.org/wiki/Tag_(metadata)>tagger</a> is trained solely on the outputs of our <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>method</a> and thus does not rely on any labeled data. Our model demonstrates state-of-the-art performance in slot tagging without labeled training data on four different dialogue domains. Moreover, we find that slot annotations discovered by our <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> significantly improve the performance of an end-to-end dialogue response generation model, compared to using no slot annotation at all.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.283.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--283 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.283 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.283" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.283/>HERALD : An Annotation Efficient Method to Detect User Disengagement in Social Conversations<span class=acl-fixed-case>HERALD</span>: An Annotation Efficient Method to Detect User Disengagement in Social Conversations</a></strong><br><a href=/people/w/weixin-liang/>Weixin Liang</a>
|
<a href=/people/k/kai-hui-liang/>Kai-Hui Liang</a>
|
<a href=/people/z/zhou-yu/>Zhou Yu</a><br><a href=/volumes/2021.acl-long/ class=text-muted>Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--283><div class="card-body p-3 small">Open-domain dialog systems have a user-centric goal : to provide humans with an engaging conversation experience. User engagement is one of the most important metrics for evaluating open-domain dialog systems, and could also be used as real-time feedback to benefit dialog policy learning. Existing work on detecting user disengagement typically requires hand-labeling many dialog samples. We propose HERALD, an efficient annotation framework that reframes the training data annotation process as a denoising problem. Specifically, instead of manually labeling training samples, we first use a set of labeling heuristics to label training samples automatically. We then denoise the weakly labeled data using the Shapley algorithm. Finally, we use the <a href=https://en.wikipedia.org/wiki/Noise_(signal_processing)>denoised data</a> to train a user engagement detector. Our experiments show that HERALD improves annotation efficiency significantly and achieves 86 % user disengagement detection accuracy in two dialog corpora.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.544.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--544 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.544 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.544/>The R-U-A-Robot Dataset : Helping Avoid Chatbot Deception by Detecting User Questions About Human or Non-Human Identity<span class=acl-fixed-case>R</span>-<span class=acl-fixed-case>U</span>-A-Robot Dataset: Helping Avoid Chatbot Deception by Detecting User Questions About Human or Non-Human Identity</a></strong><br><a href=/people/d/david-gros/>David Gros</a>
|
<a href=/people/y/yu-li/>Yu Li</a>
|
<a href=/people/z/zhou-yu/>Zhou Yu</a><br><a href=/volumes/2021.acl-long/ class=text-muted>Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--544><div class="card-body p-3 small">Humans are increasingly interacting with machines through <a href=https://en.wikipedia.org/wiki/Language>language</a>, sometimes in contexts where the user may not know they are talking to a machine (like over the phone or a text chatbot). We aim to understand how system designers and researchers might allow their systems to confirm its <a href=https://en.wikipedia.org/wiki/Non-human>non-human identity</a>. We collect over 2,500 phrasings related to the intent of Are you a robot?. This is paired with over 2,500 adversarially selected utterances where only confirming the system is non-human would be insufficient or disfluent. We compare <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifiers</a> to recognize the intent and discuss the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>precision</a> / recall and model complexity tradeoffs. Such classifiers could be integrated into <a href=https://en.wikipedia.org/wiki/Dialog_(software)>dialog systems</a> to avoid undesired deception. We then explore how both a generative research model (Blender) as well as two deployed systems (Amazon Alexa, Google Assistant) handle this intent, finding that systems often fail to confirm their non-human identity. Finally, we try to understand what a good response to the intent would be, and conduct a user study to compare the important aspects when responding to this intent.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-short.40.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-short--40 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-short.40 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-short.40/>PRAL : A Tailored Pre-Training Model for Task-Oriented Dialog Generation<span class=acl-fixed-case>PRAL</span>: A Tailored Pre-Training Model for Task-Oriented Dialog Generation</a></strong><br><a href=/people/j/jing-gu/>Jing Gu</a>
|
<a href=/people/q/qingyang-wu/>Qingyang Wu</a>
|
<a href=/people/c/chongruo-wu/>Chongruo Wu</a>
|
<a href=/people/w/weiyan-shi/>Weiyan Shi</a>
|
<a href=/people/z/zhou-yu/>Zhou Yu</a><br><a href=/volumes/2021.acl-short/ class=text-muted>Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-short--40><div class="card-body p-3 small">Large pre-trained language generation models such as GPT-2 have demonstrated their effectiveness as language priors by reaching state-of-the-art results in various language generation tasks. However, the performance of pre-trained <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> on task-oriented dialog tasks is still under-explored. We propose a Pre-trainedRole Alternating Language model (PRAL), explicitly designed for task-oriented conversational systems. We design several techniques : start position randomization, knowledge distillation, and history discount to improve pre-training performance. In addition, we introduce a high-quality large-scale task-oriented dialog pre-training dataset by post-prossessing13 dialog datasets. We effectively adapt PRALon three downstream tasks. The results show that PRAL outperforms or is on par with state-of-the-art models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.94.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--94 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.94 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.94/>MIDAS : A Dialog Act Annotation Scheme for Open Domain HumanMachine Spoken Conversations<span class=acl-fixed-case>MIDAS</span>: A Dialog Act Annotation Scheme for Open Domain <span class=acl-fixed-case>H</span>uman<span class=acl-fixed-case>M</span>achine Spoken Conversations</a></strong><br><a href=/people/d/dian-yu/>Dian Yu</a>
|
<a href=/people/z/zhou-yu/>Zhou Yu</a><br><a href=/volumes/2021.eacl-main/ class=text-muted>Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--94><div class="card-body p-3 small">Dialog act prediction in open-domain conversations is an essential language comprehension task for both dialog system building and <a href=https://en.wikipedia.org/wiki/Discourse_analysis>discourse analysis</a>. Previous dialog act schemes, such as SWBD-DAMSL, are designed mainly for <a href=https://en.wikipedia.org/wiki/Discourse_analysis>discourse analysis</a> in human-human conversations. In this paper, we present a dialog act annotation scheme, MIDAS (Machine Interaction Dialog Act Scheme), targeted at open-domain human-machine conversations. MIDAS is designed to assist machines to improve their ability to understand human partners. MIDAS has a <a href=https://en.wikipedia.org/wiki/Hierarchical_organization>hierarchical structure</a> and supports multi-label annotations. We collected and annotated a large open-domain human-machine spoken conversation dataset (consisting of 24 K utterances). To validate our scheme, we leveraged <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning methods</a> to train a multi-label dialog act prediction model and reached an <a href=https://en.wikipedia.org/wiki/F-number>F1 score</a> of 0.79.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.177.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--177 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.177 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.eacl-main.177" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.177/>ChainCQG : Flow-Aware Conversational Question Generation<span class=acl-fixed-case>C</span>hain<span class=acl-fixed-case>CQG</span>: Flow-Aware Conversational Question Generation</a></strong><br><a href=/people/j/jing-gu/>Jing Gu</a>
|
<a href=/people/m/mostafa-mirshekari/>Mostafa Mirshekari</a>
|
<a href=/people/z/zhou-yu/>Zhou Yu</a>
|
<a href=/people/a/aaron-sisto/>Aaron Sisto</a><br><a href=/volumes/2021.eacl-main/ class=text-muted>Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--177><div class="card-body p-3 small">Conversational systems enable numerous valuable <a href=https://en.wikipedia.org/wiki/Application_software>applications</a>, and <a href=https://en.wikipedia.org/wiki/Question_answering>question-answering</a> is an important component underlying many of these. However, conversational question-answering remains challenging due to the lack of realistic, domain-specific training data. Inspired by this bottleneck, we focus on conversational question generation as a means to generate synthetic conversations for training and evaluation purposes. We present a number of novel <a href=https://en.wikipedia.org/wiki/Strategy>strategies</a> to improve conversational flow and accommodate varying question types and overall fluidity. Specifically, we design ChainCQG as a two-stage architecture that learns question-answer representations across multiple dialogue turns using a flow propagation training strategy. ChainCQG significantly outperforms both answer-aware and answer-unaware SOTA baselines (e.g., up to 48 % BLEU-1 improvement). Additionally, our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is able to generate different types of questions, with improved <a href=https://en.wikipedia.org/wiki/Fluid_dynamics>fluidity</a> and coreference alignment.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.sigdial-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.sigdial-1.0/>Proceedings of the 22nd Annual Meeting of the Special Interest Group on Discourse and Dialogue</a></strong><br><a href=/people/h/haizhou-li/>Haizhou Li</a>
|
<a href=/people/g/gina-anne-levow/>Gina-Anne Levow</a>
|
<a href=/people/z/zhou-yu/>Zhou Yu</a>
|
<a href=/people/c/chitralekha-gupta/>Chitralekha Gupta</a>
|
<a href=/people/b/berrak-sisman/>Berrak Sisman</a>
|
<a href=/people/s/siqi-cai/>Siqi Cai</a>
|
<a href=/people/d/david-vandyke/>David Vandyke</a>
|
<a href=/people/n/nina-dethlefs/>Nina Dethlefs</a>
|
<a href=/people/y/yan-wu/>Yan Wu</a>
|
<a href=/people/j/junyi-jessy-li/>Junyi Jessy Li</a><br><a href=/volumes/2021.sigdial-1/ class=text-muted>Proceedings of the 22nd Annual Meeting of the Special Interest Group on Discourse and Dialogue</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.sigdial-1.6.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--sigdial-1--6 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.sigdial-1.6 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href="https://www.youtube.com/watch?v=JIGvcylPvPI" data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2021.sigdial-1.6/>Improving Named Entity Recognition in Spoken Dialog Systems by Context and Speech Pattern Modeling</a></strong><br><a href=/people/m/minh-nguyen/>Minh Nguyen</a>
|
<a href=/people/z/zhou-yu/>Zhou Yu</a><br><a href=/volumes/2021.sigdial-1/ class=text-muted>Proceedings of the 22nd Annual Meeting of the Special Interest Group on Discourse and Dialogue</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--sigdial-1--6><div class="card-body p-3 small">While named entity recognition (NER) from <a href=https://en.wikipedia.org/wiki/Speech>speech</a> has been around as long as NER from written text has, the accuracy of NER from <a href=https://en.wikipedia.org/wiki/Speech>speech</a> has generally been much lower than that of NER from <a href=https://en.wikipedia.org/wiki/Written_language>text</a>. The rise in popularity of spoken dialog systems such as <a href=https://en.wikipedia.org/wiki/Siri>Siri</a> or <a href=https://en.wikipedia.org/wiki/Amazon_Alexa>Alexa</a> highlights the need for more accurate <a href=https://en.wikipedia.org/wiki/Natural_language_understanding>NER</a> from speech because <a href=https://en.wikipedia.org/wiki/Natural_language_understanding>NER</a> is a core component for understanding what users said in dialogs. Deployed spoken dialog systems receive user input in the form of automatic speech recognition (ASR) transcripts, and simply applying NER model trained on written text to ASR transcripts often leads to low accuracy because compared to written text, ASR transcripts lack important cues such as <a href=https://en.wikipedia.org/wiki/Punctuation>punctuation</a> and <a href=https://en.wikipedia.org/wiki/Capitalization>capitalization</a>. Besides, errors in ASR transcripts also make <a href=https://en.wikipedia.org/wiki/Near-infrared_spectroscopy>NER</a> from <a href=https://en.wikipedia.org/wiki/Speech>speech</a> challenging. We propose two models that exploit dialog context and speech pattern clues to extract named entities more accurately from open-domain dialogs in spoken dialog systems. Our results show the benefit of modeling dialog context and speech patterns in two settings : a standard setting with random partition of data and a more realistic but also more difficult setting where many named entities encountered during deployment are unseen during training.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.622.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--622 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.622 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.622" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.622/>Zero-Shot Dialogue State Tracking via Cross-Task Transfer</a></strong><br><a href=/people/z/zhaojiang-lin/>Zhaojiang Lin</a>
|
<a href=/people/b/bing-liu/>Bing Liu</a>
|
<a href=/people/a/andrea-madotto/>Andrea Madotto</a>
|
<a href=/people/s/seungwhan-moon/>Seungwhan Moon</a>
|
<a href=/people/z/zhenpeng-zhou/>Zhenpeng Zhou</a>
|
<a href=/people/p/paul-a-crook/>Paul Crook</a>
|
<a href=/people/z/zhiguang-wang/>Zhiguang Wang</a>
|
<a href=/people/z/zhou-yu/>Zhou Yu</a>
|
<a href=/people/e/eunjoon-cho/>Eunjoon Cho</a>
|
<a href=/people/r/rajen-subba/>Rajen Subba</a>
|
<a href=/people/p/pascale-fung/>Pascale Fung</a><br><a href=/volumes/2021.emnlp-main/ class=text-muted>Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--622><div class="card-body p-3 small">Zero-shot transfer learning for dialogue state tracking (DST) enables us to handle a variety of task-oriented dialogue domains without the expense of collecting in-domain data. In this work, we propose to transfer the cross-task knowledge from general question answering (QA) corpora for the zero-shot DST task. Specifically, we propose TransferQA, a transferable generative QA model that seamlessly combines extractive QA and multi-choice QA via a text-to-text transformer framework, and tracks both categorical slots and non-categorical slots in DST. In addition, we introduce two effective ways to construct unanswerable questions, namely, negative question sampling and context truncation, which enable our model to handle none value slots in the zero-shot DST setting. The extensive experiments show that our approaches substantially improve the existing zero-shot and few-shot results on MultiWoz. Moreover, compared to the fully trained <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline</a> on the Schema-Guided Dialogue dataset, our approach shows better generalization ability in unseen domains.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.239.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--239 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.239 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.naacl-main.239" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.239/>Action-Based Conversations Dataset : A Corpus for Building More In-Depth Task-Oriented Dialogue Systems</a></strong><br><a href=/people/d/derek-chen/>Derek Chen</a>
|
<a href=/people/h/howard-chen/>Howard Chen</a>
|
<a href=/people/y/yi-yang/>Yi Yang</a>
|
<a href=/people/a/alexander-lin/>Alexander Lin</a>
|
<a href=/people/z/zhou-yu/>Zhou Yu</a><br><a href=/volumes/2021.naacl-main/ class=text-muted>Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--239><div class="card-body p-3 small">Existing goal-oriented dialogue datasets focus mainly on identifying slots and values. However, customer support interactions in reality often involve agents following multi-step procedures derived from explicitly-defined company policies as well. To study customer service dialogue systems in more realistic settings, we introduce the Action-Based Conversations Dataset (ABCD), a fully-labeled dataset with over 10 K human-to-human dialogues containing 55 distinct user intents requiring unique sequences of actions constrained by policies to achieve task success. We propose two additional dialog tasks, Action State Tracking and Cascading Dialogue Success, and establish a series of baselines involving large-scale, pre-trained language models on this dataset. Empirical results demonstrate that while more sophisticated <a href=https://en.wikipedia.org/wiki/Computer_network>networks</a> outperform simpler <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>, a considerable gap (50.8 % absolute accuracy) still exists to reach human-level performance on ABCD.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.dialdoc-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.dialdoc-1.0/>Proceedings of the 1st Workshop on Document-grounded Dialogue and Conversational Question Answering (DialDoc 2021)</a></strong><br><a href=/people/s/song-feng/>Song Feng</a>
|
<a href=/people/s/siva-reddy/>Siva Reddy</a>
|
<a href=/people/m/malihe-alikhani/>Malihe Alikhani</a>
|
<a href=/people/h/he-he/>He He</a>
|
<a href=/people/y/yangfeng-ji/>Yangfeng Ji</a>
|
<a href=/people/m/mohit-iyyer/>Mohit Iyyer</a>
|
<a href=/people/z/zhou-yu/>Zhou Yu</a><br><a href=/volumes/2021.dialdoc-1/ class=text-muted>Proceedings of the 1st Workshop on Document-grounded Dialogue and Conversational Question Answering (DialDoc 2021)</a></span></p><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.355.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--355 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.355 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938646 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.355/>ALICE : <a href=https://en.wikipedia.org/wiki/Active_learning>Active Learning</a> with Contrastive Natural Language Explanations<span class=acl-fixed-case>ALICE</span>: Active Learning with Contrastive Natural Language Explanations</a></strong><br><a href=/people/w/weixin-liang/>Weixin Liang</a>
|
<a href=/people/j/james-zou/>James Zou</a>
|
<a href=/people/z/zhou-yu/>Zhou Yu</a><br><a href=/volumes/2020.emnlp-main/ class=text-muted>Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--355><div class="card-body p-3 small">Training a supervised neural network classifier typically requires many annotated training samples. Collecting and annotating a large number of data points are costly and sometimes even infeasible. Traditional annotation process uses a low-bandwidth human-machine communication interface : classification labels, each of which only provides a few bits of information. We propose Active Learning with Contrastive Explanations (ALICE), an expert-in-the-loop training framework that utilizes contrastive natural language explanations to improve data efficiency in learning. AL-ICE learns to first use active learning to select the most informative pairs of label classes to elicit contrastive natural language explanations from experts. Then <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> extracts knowledge from these <a href=https://en.wikipedia.org/wiki/Explanation>explanations</a> using a <a href=https://en.wikipedia.org/wiki/Semantic_parser>semantic parser</a>. Finally, <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> incorporates the extracted knowledge through dynamically changing the <a href=https://en.wikipedia.org/wiki/Machine_learning>learning model</a>&#8217;s structure. We applied ALICEin two visual recognition tasks, bird species classification and <a href=https://en.wikipedia.org/wiki/Social_relation>social relationship classification</a>. We found by incorporating contrastive explanations, our <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> outperform baseline models that are trained with 40-100 % more training data. We found that adding1expla-nation leads to similar performance gain as adding 13-30 labeled training data points.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.acl-main.567.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--acl-main--567 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.acl-main.567 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://slideslive.com/38929216 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.acl-main.567/>SAS : Dialogue State Tracking via Slot Attention and Slot Information Sharing<span class=acl-fixed-case>SAS</span>: Dialogue State Tracking via Slot Attention and Slot Information Sharing</a></strong><br><a href=/people/j/jiaying-hu/>Jiaying Hu</a>
|
<a href=/people/y/yan-yang/>Yan Yang</a>
|
<a href=/people/c/chencai-chen/>Chencai Chen</a>
|
<a href=/people/l/liang-he/>Liang He</a>
|
<a href=/people/z/zhou-yu/>Zhou Yu</a><br><a href=/volumes/2020.acl-main/ class=text-muted>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--acl-main--567><div class="card-body p-3 small">Dialogue state tracker is responsible for inferring user intentions through dialogue history. Previous methods have difficulties in handling <a href=https://en.wikipedia.org/wiki/Dialogue>dialogues</a> with <a href=https://en.wikipedia.org/wiki/Context_(language_use)>long interaction context</a>, due to the excessive information. We propose a Dialogue State Tracker with Slot Attention and Slot Information Sharing (SAS) to reduce redundant information&#8217;s interference and improve long dialogue context tracking. Specially, we first apply a Slot Attention to learn a set of slot-specific features from the original dialogue and then integrate them using a slot information sharing module. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> yields a significantly improved performance compared to previous state-of the-art models on the MultiWOZ dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.nlp4convai-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.nlp4convai-1.0/>Proceedings of the 2nd Workshop on Natural Language Processing for Conversational AI</a></strong><br><a href=/people/t/tsung-hsien-wen/>Tsung-Hsien Wen</a>
|
<a href=/people/a/asli-celikyilmaz/>Asli Celikyilmaz</a>
|
<a href=/people/z/zhou-yu/>Zhou Yu</a>
|
<a href=/people/a/alexandros-papangelis/>Alexandros Papangelis</a>
|
<a href=/people/m/mihail-eric/>Mihail Eric</a>
|
<a href=/people/a/anuj-kumar/>Anuj Kumar</a>
|
<a href=/people/i/inigo-casanueva/>Iñigo Casanueva</a>
|
<a href=/people/r/rushin-shah/>Rushin Shah</a><br><a href=/volumes/2020.nlp4convai-1/ class=text-muted>Proceedings of the 2nd Workshop on Natural Language Processing for Conversational AI</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.431.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--431 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.431 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.431/>Understanding User Resistance Strategies in Persuasive Conversations</a></strong><br><a href=/people/y/youzhi-tian/>Youzhi Tian</a>
|
<a href=/people/w/weiyan-shi/>Weiyan Shi</a>
|
<a href=/people/c/chen-li/>Chen Li</a>
|
<a href=/people/z/zhou-yu/>Zhou Yu</a><br><a href=/volumes/2020.findings-emnlp/ class=text-muted>Findings of the Association for Computational Linguistics: EMNLP 2020</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--431><div class="card-body p-3 small">Persuasive dialog systems have various usages, such as donation persuasion and physical exercise persuasion. Previous persuasive dialog systems research mostly focused on analyzing the persuader&#8217;s strategies and paid little attention to the persuadee (user). However, understanding and addressing users&#8217; resistance strategies is an essential job of a persuasive dialog system. So, we adopt a preliminary framework on persuasion resistance in <a href=https://en.wikipedia.org/wiki/Psychology>psychology</a> and design a fine-grained resistance strategy annotation scheme. We annotate the PersuasionForGood dataset with the <a href=https://en.wikipedia.org/wiki/Scheme_(mathematics)>scheme</a>. With the enriched annotations, we build a <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifier</a> to predict the resistance strategies. Furthermore, we analyze the relationships between <a href=https://en.wikipedia.org/wiki/Persuasion>persuasion strategies</a> and persuasion resistance strategies. Our work lays the ground for developing a persuasive dialogue system that can understand and address user resistance strategy appropriately. The code and data will be released.</div></div><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1206.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1206 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1206 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1206.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1206" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1206/>How to Build User Simulators to Train RL-based Dialog Systems<span class=acl-fixed-case>RL</span>-based Dialog Systems</a></strong><br><a href=/people/w/weiyan-shi/>Weiyan Shi</a>
|
<a href=/people/k/kun-qian/>Kun Qian</a>
|
<a href=/people/x/xuewei-wang/>Xuewei Wang</a>
|
<a href=/people/z/zhou-yu/>Zhou Yu</a><br><a href=/volumes/D19-1/ class=text-muted>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1206><div class="card-body p-3 small">User simulators are essential for training reinforcement learning (RL) based dialog models. The performance of the <a href=https://en.wikipedia.org/wiki/Simulation>simulator</a> directly impacts the <a href=https://en.wikipedia.org/wiki/Non-linear_gameplay>RL policy</a>. However, building a good user simulator that models real user behaviors is challenging. We propose a method of standardizing user simulator building that can be used by the community to compare dialog system quality using the same set of user simulators fairly. We present implementations of six user simulators trained with different dialog planning and generation methods. We then calculate a set of automatic metrics to evaluate the quality of these <a href=https://en.wikipedia.org/wiki/Simulation>simulators</a> both directly and indirectly. We also ask human users to assess the simulators directly and indirectly by rating the simulated dialogs and interacting with the trained systems. This paper presents a comprehensive evaluation framework for user simulator study and provides a better understanding of the pros and cons of different user simulators, as well as their impacts on the trained systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5935.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5935 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5935 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5935/>A Large-Scale User Study of an Alexa Prize Chatbot : Effect of TTS Dynamism on Perceived Quality of Social Dialog<span class=acl-fixed-case>A</span>lexa <span class=acl-fixed-case>P</span>rize Chatbot: Effect of <span class=acl-fixed-case>TTS</span> Dynamism on Perceived Quality of Social Dialog</a></strong><br><a href=/people/m/michelle-cohn/>Michelle Cohn</a>
|
<a href=/people/c/chun-yen-chen/>Chun-Yen Chen</a>
|
<a href=/people/z/zhou-yu/>Zhou Yu</a><br><a href=/volumes/W19-59/ class=text-muted>Proceedings of the 20th Annual SIGdial Meeting on Discourse and Dialogue</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5935><div class="card-body p-3 small">This study tests the effect of cognitive-emotional expression in an Alexa text-to-speech (TTS) voice on users&#8217; experience with a social dialog system. We systematically introduced emotionally expressive interjections (e.g., Wow !) and <a href=https://en.wikipedia.org/wiki/Filler_(media)>filler words</a> (e.g., um, mhmm) in an Amazon Alexa Prize socialbot, Gunrock. We tested whether these TTS manipulations improved users&#8217; ratings of their conversation across thousands of real user interactions (n=5,527). Results showed that <a href=https://en.wikipedia.org/wiki/Interjection>interjections</a> and <a href=https://en.wikipedia.org/wiki/Filler_(media)>fillers</a> each improved users&#8217; holistic ratings, an improvement that further increased if the <a href=https://en.wikipedia.org/wiki/System>system</a> used both manipulations. A separate perception experiment corroborated the findings from the user study, with improved social ratings for conversations including interjections ; however, no positive effect was observed for fillers, suggesting that the role of the rater in the conversationas active participant or external listeneris an important factor in assessing social dialogs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1178.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1178 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1178 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1178" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N19-1178/>Unsupervised Dialog Structure Learning</a></strong><br><a href=/people/w/weiyan-shi/>Weiyan Shi</a>
|
<a href=/people/t/tiancheng-zhao/>Tiancheng Zhao</a>
|
<a href=/people/z/zhou-yu/>Zhou Yu</a><br><a href=/volumes/N19-1/ class=text-muted>Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1178><div class="card-body p-3 small">Learning a shared dialog structure from a set of task-oriented dialogs is an important challenge in <a href=https://en.wikipedia.org/wiki/Computational_linguistics>computational linguistics</a>. The learned dialog structure can shed light on how to analyze human dialogs, and more importantly contribute to the design and evaluation of dialog systems. We propose to extract dialog structures using a modified VRNN model with discrete latent vectors. Different from existing HMM-based models, our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is based on variational-autoencoder (VAE). Such <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is able to capture more dynamics in <a href=https://en.wikipedia.org/wiki/Dialogue>dialogs</a> beyond the surface forms of the language. We find that qualitatively, our method extracts meaningful dialog structure, and quantitatively, outperforms previous models on the ability to predict unseen data. We further evaluate the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a>&#8217;s effectiveness in a downstream task, the dialog system building task. Experiments show that, by integrating the learned dialog structure into the reward function design, the model converges faster and to a better outcome in a reinforcement learning setting.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1566.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1566 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1566 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/385225678 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1566" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1566/>Persuasion for Good : Towards a Personalized Persuasive Dialogue System for Social Good</a></strong><br><a href=/people/x/xuewei-wang/>Xuewei Wang</a>
|
<a href=/people/w/weiyan-shi/>Weiyan Shi</a>
|
<a href=/people/r/richard-kim/>Richard Kim</a>
|
<a href=/people/y/yoojung-oh/>Yoojung Oh</a>
|
<a href=/people/s/sijia-yang/>Sijia Yang</a>
|
<a href=/people/j/jingwen-zhang/>Jingwen Zhang</a>
|
<a href=/people/z/zhou-yu/>Zhou Yu</a><br><a href=/volumes/P19-1/ class=text-muted>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1566><div class="card-body p-3 small">Developing intelligent persuasive conversational agents to change people&#8217;s opinions and actions for social good is the frontier in advancing the ethical development of automated dialogue systems. To do so, the first step is to understand the intricate organization of strategic disclosures and appeals employed in human persuasion conversations. We designed an online persuasion task where one participant was asked to persuade the other to donate to a specific charity. We collected a large dataset with 1,017 dialogues and annotated emerging <a href=https://en.wikipedia.org/wiki/Persuasion>persuasion strategies</a> from a subset. Based on the annotation, we built a baseline classifier with <a href=https://en.wikipedia.org/wiki/Context_(language_use)>context information</a> and sentence-level features to predict the 10 <a href=https://en.wikipedia.org/wiki/Persuasion>persuasion strategies</a> used in the <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a>. Furthermore, to develop an understanding of personalized persuasion processes, we analyzed the relationships between individuals&#8217; demographic and psychological backgrounds including <a href=https://en.wikipedia.org/wiki/Personality>personality</a>, <a href=https://en.wikipedia.org/wiki/Morality>morality</a>, <a href=https://en.wikipedia.org/wiki/Value_(ethics)>value systems</a>, and their willingness for donation. Then, we analyzed which types of <a href=https://en.wikipedia.org/wiki/Persuasion>persuasion strategies</a> led to a greater amount of donation depending on the individuals&#8217; personal backgrounds. This work lays the ground for developing a personalized persuasive dialogue system.</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1385.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1385 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1385 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1385" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D18-1385/>Cross-Lingual Cross-Platform Rumor Verification Pivoting on Multimedia Content</a></strong><br><a href=/people/w/weiming-wen/>Weiming Wen</a>
|
<a href=/people/s/songwen-su/>Songwen Su</a>
|
<a href=/people/z/zhou-yu/>Zhou Yu</a><br><a href=/volumes/D18-1/ class=text-muted>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1385><div class="card-body p-3 small">With the increasing popularity of <a href=https://en.wikipedia.org/wiki/Smart_device>smart devices</a>, <a href=https://en.wikipedia.org/wiki/Rumor>rumors</a> with <a href=https://en.wikipedia.org/wiki/Multimedia>multimedia content</a> become more and more common on <a href=https://en.wikipedia.org/wiki/List_of_social_networking_websites>social networks</a>. The <a href=https://en.wikipedia.org/wiki/Multimedia>multimedia information</a> usually makes <a href=https://en.wikipedia.org/wiki/Rumor>rumors</a> look more convincing. Therefore, finding an automatic approach to verify <a href=https://en.wikipedia.org/wiki/Rumor>rumors</a> with <a href=https://en.wikipedia.org/wiki/Multimedia>multimedia content</a> is a pressing task. Previous rumor verification research only utilizes <a href=https://en.wikipedia.org/wiki/Multimedia>multimedia</a> as input features. We propose not to use the <a href=https://en.wikipedia.org/wiki/Multimedia>multimedia content</a> but to find <a href=https://en.wikipedia.org/wiki/Infographic>external information</a> in other <a href=https://en.wikipedia.org/wiki/Online_newspaper>news platforms</a> pivoting on it. We introduce a new features set, cross-lingual cross-platform features that leverage the semantic similarity between the rumors and the external information. When implemented, <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning methods</a> utilizing such <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> achieved the state-of-the-art rumor verification results.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1400.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1400 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1400 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D18-1400.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/306149028 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1400/>A Visual Attention Grounding Neural Model for Multimodal Machine Translation</a></strong><br><a href=/people/m/mingyang-zhou/>Mingyang Zhou</a>
|
<a href=/people/r/runxiang-cheng/>Runxiang Cheng</a>
|
<a href=/people/y/yong-jae-lee/>Yong Jae Lee</a>
|
<a href=/people/z/zhou-yu/>Zhou Yu</a><br><a href=/volumes/D18-1/ class=text-muted>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1400><div class="card-body p-3 small">We introduce a novel multimodal machine translation model that utilizes parallel visual and textual information. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> jointly optimizes the learning of a shared visual-language embedding and a <a href=https://en.wikipedia.org/wiki/Translation>translator</a>. The <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> leverages a visual attention grounding mechanism that links the visual semantics with the corresponding textual semantics. Our approach achieves competitive state-of-the-art results on the Multi30 K and the Ambiguous COCO datasets. We also collected a new multilingual multimodal product description dataset to simulate a real-world international online shopping scenario. On this <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>, our visual attention grounding model outperforms other <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> by a large margin.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5015.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5015 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5015 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5015/>Multimodal Hierarchical Reinforcement Learning Policy for Task-Oriented Visual Dialog</a></strong><br><a href=/people/j/jiaping-zhang/>Jiaping Zhang</a>
|
<a href=/people/t/tiancheng-zhao/>Tiancheng Zhao</a>
|
<a href=/people/z/zhou-yu/>Zhou Yu</a><br><a href=/volumes/W18-50/ class=text-muted>Proceedings of the 19th Annual SIGdial Meeting on Discourse and Dialogue</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5015><div class="card-body p-3 small">Creating an intelligent conversational system that understands <a href=https://en.wikipedia.org/wiki/Visual_system>vision</a> and language is one of the ultimate goals in Artificial Intelligence (AI) (Winograd, 1972). Extensive research has focused on vision-to-language generation, however, limited research has touched on combining these two modalities in a goal-driven dialog context. We propose a multimodal hierarchical reinforcement learning framework that dynamically integrates vision and language for task-oriented visual dialog. The <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> jointly learns the multimodal dialog state representation and the hierarchical dialog policy to improve both dialog task success and efficiency. We also propose a new technique, state adaptation, to integrate <a href=https://en.wikipedia.org/wiki/Context_awareness>context awareness</a> in the dialog state representation. We evaluate the proposed <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> and the state adaptation technique in an image guessing game and achieve promising results.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1140.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1140 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1140 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1140.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1140.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-1140/>Sentiment Adaptive End-to-End Dialog Systems</a></strong><br><a href=/people/w/weiyan-shi/>Weiyan Shi</a>
|
<a href=/people/z/zhou-yu/>Zhou Yu</a><br><a href=/volumes/P18-1/ class=text-muted>Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1140><div class="card-body p-3 small">End-to-end learning framework is useful for building <a href=https://en.wikipedia.org/wiki/Dialogue_system>dialog systems</a> for its simplicity in training and efficiency in model updating. However, current <a href=https://en.wikipedia.org/wiki/End-to-end_principle>end-to-end approaches</a> only consider user semantic inputs in learning and under-utilize other <a href=https://en.wikipedia.org/wiki/User_information>user information</a>. Therefore, we propose to include user sentiment obtained through multimodal information (acoustic, dialogic and textual), in the end-to-end learning framework to make systems more user-adaptive and effective. We incorporated user sentiment information in both supervised and reinforcement learning settings. In both settings, adding <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment information</a> reduced the dialog length and improved the task success rate on a bus information search task. This work is the first attempt to incorporate multimodal user information in the adaptive end-to-end dialog system training framework and attained state-of-the-art performance.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Zhou+Yu" title="Search for 'Zhou Yu' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/w/weiyan-shi/ class=align-middle>Weiyan Shi</a>
<span class="badge badge-secondary align-middle ml-2">6</span></li><li class=list-group-item><a href=/people/w/weixin-liang/ class=align-middle>Weixin Liang</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/j/jing-gu/ class=align-middle>Jing Gu</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/x/xuewei-wang/ class=align-middle>Xuewei Wang</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/t/tiancheng-zhao/ class=align-middle>Tiancheng Zhao</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/v/vojtech-hudecek/ class=align-middle>Vojtěch Hudeček</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/o/ondrej-dusek/ class=align-middle>Ondřej Dušek</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/kai-hui-liang/ class=align-middle>Kai-Hui Liang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/david-gros/ class=align-middle>David Gros</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yu-li/ class=align-middle>Yu Li</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/q/qingyang-wu/ class=align-middle>Qingyang Wu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/chongruo-wu/ class=align-middle>Chongruo Wu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/james-zou/ class=align-middle>James Zou</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jiaying-hu/ class=align-middle>Jiaying Hu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yan-yang/ class=align-middle>Yan Yang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/chencai-chen/ class=align-middle>Chencai Chen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/liang-he/ class=align-middle>Liang He</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/dian-yu/ class=align-middle>Dian Yu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/mostafa-mirshekari/ class=align-middle>Mostafa Mirshekari</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/aaron-sisto/ class=align-middle>Aaron Sisto</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/haizhou-li/ class=align-middle>Haizhou Li</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/g/gina-anne-levow/ class=align-middle>Gina-Anne Levow</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/chitralekha-gupta/ class=align-middle>Chitralekha Gupta</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/b/berrak-sisman/ class=align-middle>Berrak Sisman</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/siqi-cai/ class=align-middle>Siqi Cai</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/david-vandyke/ class=align-middle>David Vandyke</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/n/nina-dethlefs/ class=align-middle>Nina Dethlefs</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yan-wu/ class=align-middle>Yan Wu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/junyi-jessy-li/ class=align-middle>Junyi Jessy Li</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/minh-nguyen/ class=align-middle>Minh Nguyen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/w/weiming-wen/ class=align-middle>Weiming Wen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/songwen-su/ class=align-middle>Songwen Su</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/mingyang-zhou/ class=align-middle>Mingyang Zhou</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/runxiang-cheng/ class=align-middle>Runxiang Cheng</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yong-jae-lee/ class=align-middle>Yong Jae Lee</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/tsung-hsien-wen/ class=align-middle>Tsung-Hsien Wen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/asli-celikyilmaz/ class=align-middle>Asli Celikyilmaz</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/alexandros-papangelis/ class=align-middle>Alexandros Papangelis</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/mihail-eric/ class=align-middle>Mihail Eric</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/anuj-kumar/ class=align-middle>Anuj Kumar</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/i/inigo-casanueva/ class=align-middle>Iñigo Casanueva</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/rushin-shah/ class=align-middle>Rushin Shah</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/zhaojiang-lin/ class=align-middle>Zhaojiang Lin</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/b/bing-liu/ class=align-middle>Bing Liu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/andrea-madotto/ class=align-middle>Andrea Madotto</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/seungwhan-moon/ class=align-middle>Seungwhan Moon</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/zhenpeng-zhou/ class=align-middle>Zhenpeng Zhou</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/p/paul-a-crook/ class=align-middle>Paul A. Crook</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/zhiguang-wang/ class=align-middle>Zhiguang Wang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/e/eunjoon-cho/ class=align-middle>Eunjoon Cho</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/rajen-subba/ class=align-middle>Rajen Subba</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/p/pascale-fung/ class=align-middle>Pascale Fung</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/kun-qian/ class=align-middle>Kun Qian</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/derek-chen/ class=align-middle>Derek Chen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/howard-chen/ class=align-middle>Howard Chen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yi-yang/ class=align-middle>Yi Yang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/alexander-lin/ class=align-middle>Alexander Lin</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/song-feng/ class=align-middle>Song Feng</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/siva-reddy/ class=align-middle>Siva Reddy</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/malihe-alikhani/ class=align-middle>Malihe Alikhani</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/he-he/ class=align-middle>He He</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yangfeng-ji/ class=align-middle>Yangfeng Ji</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/mohit-iyyer/ class=align-middle>Mohit Iyyer</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/youzhi-tian/ class=align-middle>Youzhi Tian</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/chen-li/ class=align-middle>Chen Li</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jiaping-zhang/ class=align-middle>Jiaping Zhang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/michelle-cohn/ class=align-middle>Michelle Cohn</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/chun-yen-chen/ class=align-middle>Chun-Yen Chen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/richard-kim/ class=align-middle>Richard Kim</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yoojung-oh/ class=align-middle>Yoojung Oh</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/sijia-yang/ class=align-middle>Sijia Yang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jingwen-zhang/ class=align-middle>Jingwen Zhang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/acl/ class=align-middle>ACL</a><span class="badge badge-secondary align-middle ml-2">7</span></li><li class=list-group-item><a href=/venues/emnlp/ class=align-middle>EMNLP</a><span class="badge badge-secondary align-middle ml-2">5</span></li><li class=list-group-item><a href=/venues/eacl/ class=align-middle>EACL</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/sigdial/ class=align-middle>SIGDIAL</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/naacl/ class=align-middle>NAACL</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-venues aria-expanded=false aria-controls=more-venues>show all...</li><div class="collapse border-top" id=more-venues><li class=list-group-item><a href=/venues/ws/ class=align-middle>WS</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/nlp4convai/ class=align-middle>NLP4ConvAI</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/dialdoc/ class=align-middle>dialdoc</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/findings/ class=align-middle>Findings</a><span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>