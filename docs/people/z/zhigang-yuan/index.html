<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Zhigang Yuan - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Zhigang</span> <span class=font-weight-bold>Yuan</span></h2><hr><div class=row><div class=col-lg-9><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.aacl-main.22.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--aacl-main--22 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.aacl-main.22 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.aacl-main.22/>Named Entity Recognition in Multi-level Contexts</a></strong><br><a href=/people/y/yubo-chen/>Yubo Chen</a>
|
<a href=/people/c/chuhan-wu/>Chuhan Wu</a>
|
<a href=/people/t/tao-qi/>Tao Qi</a>
|
<a href=/people/z/zhigang-yuan/>Zhigang Yuan</a>
|
<a href=/people/y/yongfeng-huang/>Yongfeng Huang</a><br><a href=/volumes/2020.aacl-main/ class=text-muted>Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--aacl-main--22><div class="card-body p-3 small">Named entity recognition is a critical task in the <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing field</a>. Most existing <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> for this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> can only exploit <a href=https://en.wikipedia.org/wiki/Context_(language_use)>contextual information</a> within a sentence. However, their performance on recognizing entities in limited or ambiguous sentence-level contexts is usually unsatisfactory. Fortunately, other sentences in the same document can provide supplementary document-level contexts to help recognize these entities. In addition, words themselves contain word-level contextual information since they usually have different preferences of entity type and relative position from named entities. In this paper, we propose a unified framework to incorporate <a href=https://en.wikipedia.org/wiki/Context_(computing)>multi-level contexts</a> for <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>named entity recognition</a>. We use TagLM as our basic <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> to capture sentence-level contexts. To incorporate document-level contexts, we propose to capture interactions between sentences via a multi-head self attention network. To mine word-level contexts, we propose an auxiliary task to predict the type of each word to capture its type preference. We jointly train our model in entity recognition and the auxiliary classification task via <a href=https://en.wikipedia.org/wiki/Multi-task_learning>multi-task learning</a>. The experimental results on several <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmark datasets</a> validate the effectiveness of our <a href=https://en.wikipedia.org/wiki/Methodology>method</a>.</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S18-1006.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S18-1006 data-toggle=collapse aria-expanded=false aria-controls=abstract-S18-1006 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/S18-1006/>THU_NGN at SemEval-2018 Task 3 : Tweet Irony Detection with Densely connected LSTM and Multi-task Learning<span class=acl-fixed-case>THU</span>_<span class=acl-fixed-case>NGN</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2018 Task 3: Tweet Irony Detection with Densely connected <span class=acl-fixed-case>LSTM</span> and Multi-task Learning</a></strong><br><a href=/people/c/chuhan-wu/>Chuhan Wu</a>
|
<a href=/people/f/fangzhao-wu/>Fangzhao Wu</a>
|
<a href=/people/s/sixing-wu/>Sixing Wu</a>
|
<a href=/people/j/junxin-liu/>Junxin Liu</a>
|
<a href=/people/z/zhigang-yuan/>Zhigang Yuan</a>
|
<a href=/people/y/yongfeng-huang/>Yongfeng Huang</a><br><a href=/volumes/S18-1/ class=text-muted>Proceedings of The 12th International Workshop on Semantic Evaluation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S18-1006><div class="card-body p-3 small">Detecting irony is an important task to mine fine-grained information from social web messages. Therefore, the Semeval-2018 task 3 is aimed to detect the ironic tweets (subtask A) and their ironic types (subtask B). In order to address this task, we propose a <a href=https://en.wikipedia.org/wiki/System>system</a> based on a densely connected LSTM network with multi-task learning strategy. In our dense LSTM model, each layer will take all outputs from previous layers as input. The last LSTM layer will output the hidden representations of texts, and they will be used in three classification task. In addition, we incorporate several types of features to improve the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> performance. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieved an <a href=https://en.wikipedia.org/wiki/F-score>F-score</a> of 70.54 (ranked 2/43) in the subtask A and 49.47 (ranked 3/29) in the subtask B. The experimental results validate the effectiveness of our <a href=https://en.wikipedia.org/wiki/System>system</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S18-1028.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S18-1028 data-toggle=collapse aria-expanded=false aria-controls=abstract-S18-1028 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/S18-1028/>THU_NGN at SemEval-2018 Task 1 : Fine-grained Tweet Sentiment Intensity Analysis with Attention CNN-LSTM<span class=acl-fixed-case>THU</span>_<span class=acl-fixed-case>NGN</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2018 Task 1: Fine-grained Tweet Sentiment Intensity Analysis with Attention <span class=acl-fixed-case>CNN</span>-<span class=acl-fixed-case>LSTM</span></a></strong><br><a href=/people/c/chuhan-wu/>Chuhan Wu</a>
|
<a href=/people/f/fangzhao-wu/>Fangzhao Wu</a>
|
<a href=/people/j/junxin-liu/>Junxin Liu</a>
|
<a href=/people/z/zhigang-yuan/>Zhigang Yuan</a>
|
<a href=/people/s/sixing-wu/>Sixing Wu</a>
|
<a href=/people/y/yongfeng-huang/>Yongfeng Huang</a><br><a href=/volumes/S18-1/ class=text-muted>Proceedings of The 12th International Workshop on Semantic Evaluation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S18-1028><div class="card-body p-3 small">Traditional sentiment analysis approaches mainly focus on classifying the sentiment polarities or emotion categories of texts. However, <a href=https://en.wikipedia.org/wiki/Information_technology>they</a> ca n&#8217;t exploit the <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment intensity information</a>. Therefore, the SemEval-2018 Task 1 is aimed to automatically determine the intensity of emotions or sentiment of tweets to mine fine-grained sentiment information. In order to address this task, we propose a <a href=https://en.wikipedia.org/wiki/System>system</a> based on an attention CNN-LSTM model. In our <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a>, LSTM is used to extract the long-term contextual information from texts. We apply <a href=https://en.wikipedia.org/wiki/Attentional_control>attention techniques</a> to selecting this <a href=https://en.wikipedia.org/wiki/Information>information</a>. A CNN layer with different size of kernels is used to extract local features. The dense layers take the pooled CNN feature maps and predict the intensity scores. Our system reaches average Pearson correlation score of 0.722 (ranked 12/48) in emotion intensity regression task, and 0.810 in valence regression task (ranked 15/38). It indicates that our <a href=https://en.wikipedia.org/wiki/System>system</a> can be further extended.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S18-1063.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S18-1063 data-toggle=collapse aria-expanded=false aria-controls=abstract-S18-1063 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/S18-1063/>THU_NGN at SemEval-2018 Task 2 : Residual CNN-LSTM Network with Attention for English Emoji Prediction<span class=acl-fixed-case>THU</span>_<span class=acl-fixed-case>NGN</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2018 Task 2: Residual <span class=acl-fixed-case>CNN</span>-<span class=acl-fixed-case>LSTM</span> Network with Attention for <span class=acl-fixed-case>E</span>nglish Emoji Prediction</a></strong><br><a href=/people/c/chuhan-wu/>Chuhan Wu</a>
|
<a href=/people/f/fangzhao-wu/>Fangzhao Wu</a>
|
<a href=/people/s/sixing-wu/>Sixing Wu</a>
|
<a href=/people/z/zhigang-yuan/>Zhigang Yuan</a>
|
<a href=/people/j/junxin-liu/>Junxin Liu</a>
|
<a href=/people/y/yongfeng-huang/>Yongfeng Huang</a><br><a href=/volumes/S18-1/ class=text-muted>Proceedings of The 12th International Workshop on Semantic Evaluation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S18-1063><div class="card-body p-3 small">Emojis are widely used by <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> and social network users when posting their messages. It is important to study the relationships between messages and <a href=https://en.wikipedia.org/wiki/Emoji>emojis</a>. Thus, in SemEval-2018 Task 2 an interesting and challenging <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> is proposed, i.e., predicting which <a href=https://en.wikipedia.org/wiki/Emoji>emojis</a> are evoked by <a href=https://en.wikipedia.org/wiki/Twitter>text-based tweets</a>. We propose a residual CNN-LSTM with attention (RCLA) model for this task. Our model combines CNN and LSTM layers to capture both local and long-range contextual information for tweet representation. In addition, attention mechanism is used to select important components. Besides, residual connection is applied to CNN layers to facilitate the training of <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a>. We also incorporated additional <a href=https://en.wikipedia.org/wiki/Software_feature>features</a> such as POS tags and sentiment features extracted from <a href=https://en.wikipedia.org/wiki/Lexicon>lexicons</a>. Our model achieved 30.25 % macro-averaged F-score in the first subtask (i.e., emoji prediction in English), ranking 7th out of 48 participants.<b>RCLA</b>) model\n for this task. Our model combines CNN and LSTM layers to capture\n both local and long-range contextual information for tweet\n representation. In addition, attention mechanism is used to\n select important components. Besides, residual connection is\n applied to CNN layers to facilitate the training of neural\n networks. We also incorporated additional features such as POS\n tags and sentiment features extracted from lexicons. Our model\n achieved 30.25% macro-averaged F-score in the first subtask\n (i.e., emoji prediction in English), ranking 7th out of 48\n participants.\n</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S18-1157.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S18-1157 data-toggle=collapse aria-expanded=false aria-controls=abstract-S18-1157 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/S18-1157/>THU_NGN at SemEval-2018 Task 10 : Capturing Discriminative Attributes with MLP-CNN model<span class=acl-fixed-case>THU</span>_<span class=acl-fixed-case>NGN</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2018 Task 10: Capturing Discriminative Attributes with <span class=acl-fixed-case>MLP</span>-<span class=acl-fixed-case>CNN</span> model</a></strong><br><a href=/people/c/chuhan-wu/>Chuhan Wu</a>
|
<a href=/people/f/fangzhao-wu/>Fangzhao Wu</a>
|
<a href=/people/s/sixing-wu/>Sixing Wu</a>
|
<a href=/people/z/zhigang-yuan/>Zhigang Yuan</a>
|
<a href=/people/y/yongfeng-huang/>Yongfeng Huang</a><br><a href=/volumes/S18-1/ class=text-muted>Proceedings of The 12th International Workshop on Semantic Evaluation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S18-1157><div class="card-body p-3 small">Existing semantic models are capable of identifying the semantic similarity of words. However, it&#8217;s hard for these <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> to discriminate between a word and another similar word. Thus, the aim of SemEval-2018 Task 10 is to predict whether a word is a discriminative attribute between two concepts. In this task, we apply a multilayer perceptron (MLP)-convolutional neural network (CNN) model to identify whether an attribute is discriminative. The CNNs are used to extract low-level features from the inputs. The MLP takes both the flatten CNN maps and inputs to predict the labels. The evaluation F-score of our <a href=https://en.wikipedia.org/wiki/System>system</a> on the test set is 0.629 (ranked 15th), which indicates that our <a href=https://en.wikipedia.org/wiki/System>system</a> still needs to be improved. However, the behaviours of our <a href=https://en.wikipedia.org/wiki/System>system</a> in our experiments provide useful information, which can help to improve the collective understanding of this novel <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0913.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0913 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0913 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0913/>Neural Metaphor Detecting with CNN-LSTM Model<span class=acl-fixed-case>CNN</span>-<span class=acl-fixed-case>LSTM</span> Model</a></strong><br><a href=/people/c/chuhan-wu/>Chuhan Wu</a>
|
<a href=/people/f/fangzhao-wu/>Fangzhao Wu</a>
|
<a href=/people/y/yubo-chen/>Yubo Chen</a>
|
<a href=/people/s/sixing-wu/>Sixing Wu</a>
|
<a href=/people/z/zhigang-yuan/>Zhigang Yuan</a>
|
<a href=/people/y/yongfeng-huang/>Yongfeng Huang</a><br><a href=/volumes/W18-09/ class=text-muted>Proceedings of the Workshop on Figurative Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0913><div class="card-body p-3 small">Metaphors are <a href=https://en.wikipedia.org/wiki/Literal_and_figurative_language>figurative languages</a> widely used in daily life and literatures. It&#8217;s an important task to detect the <a href=https://en.wikipedia.org/wiki/Metaphor>metaphors</a> evoked by texts. Thus, the metaphor shared task is aimed to extract <a href=https://en.wikipedia.org/wiki/Metaphor>metaphors</a> from plain texts at <a href=https://en.wikipedia.org/wiki/Literal_and_figurative_language>word level</a>. We propose to use a CNN-LSTM model for this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. Our model combines CNN and LSTM layers to utilize both local and long-range contextual information for identifying metaphorical information. In addition, we compare the performance of the softmax classifier and conditional random field (CRF) for sequential labeling in this task. We also incorporated some additional features such as part of speech (POS) tags and word cluster to improve the performance of model. Our best <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> achieved 65.06 % <a href=https://en.wikipedia.org/wiki/F-score>F-score</a> in the all POS testing subtask and 67.15 % in the verbs testing subtask.</div></div><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-4007.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-4007 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-4007 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-4007/>THU_NGN at IJCNLP-2017 Task 2 : Dimensional Sentiment Analysis for Chinese Phrases with Deep LSTM<span class=acl-fixed-case>THU</span>_<span class=acl-fixed-case>NGN</span> at <span class=acl-fixed-case>IJCNLP</span>-2017 Task 2: Dimensional Sentiment Analysis for <span class=acl-fixed-case>C</span>hinese Phrases with Deep <span class=acl-fixed-case>LSTM</span></a></strong><br><a href=/people/c/chuhan-wu/>Chuhan Wu</a>
|
<a href=/people/f/fangzhao-wu/>Fangzhao Wu</a>
|
<a href=/people/y/yongfeng-huang/>Yongfeng Huang</a>
|
<a href=/people/s/sixing-wu/>Sixing Wu</a>
|
<a href=/people/z/zhigang-yuan/>Zhigang Yuan</a><br><a href=/volumes/I17-4/ class=text-muted>Proceedings of the IJCNLP 2017, Shared Tasks</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-4007><div class="card-body p-3 small">Predicting valence-arousal ratings for words and phrases is very useful for constructing affective resources for dimensional sentiment analysis. Since the existing valence-arousal resources of <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a> are mainly in word-level and there is a lack of phrase-level ones, the Dimensional Sentiment Analysis for Chinese Phrases (DSAP) task aims to predict the valence-arousal ratings for <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese affective words and phrases</a> automatically. In this task, we propose an approach using a densely connected LSTM network and word features to identify dimensional sentiment on valence and arousal for words and phrases jointly. We use <a href=https://en.wikipedia.org/wiki/Word_embedding>word embedding</a> as major feature and choose part of speech (POS) and word clusters as additional features to train the dense LSTM network. The evaluation results of our submissions (1st and 2nd in average performance) validate the effectiveness of our system to predict valence and arousal dimensions for Chinese words and phrases.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Zhigang+Yuan" title="Search for 'Zhigang Yuan' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/c/chuhan-wu/ class=align-middle>Chuhan Wu</a>
<span class="badge badge-secondary align-middle ml-2">7</span></li><li class=list-group-item><a href=/people/y/yongfeng-huang/ class=align-middle>Yongfeng Huang</a>
<span class="badge badge-secondary align-middle ml-2">7</span></li><li class=list-group-item><a href=/people/f/fangzhao-wu/ class=align-middle>Fangzhao Wu</a>
<span class="badge badge-secondary align-middle ml-2">6</span></li><li class=list-group-item><a href=/people/s/sixing-wu/ class=align-middle>Sixing Wu</a>
<span class="badge badge-secondary align-middle ml-2">6</span></li><li class=list-group-item><a href=/people/j/junxin-liu/ class=align-middle>Junxin Liu</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/y/yubo-chen/ class=align-middle>Yubo Chen</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/t/tao-qi/ class=align-middle>Tao Qi</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/semeval/ class=align-middle>SemEval</a><span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/venues/ijcnlp/ class=align-middle>IJCNLP</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/aacl/ class=align-middle>AACL</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/ws/ class=align-middle>WS</a><span class="badge badge-secondary align-middle ml-2">1</span></li></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>