<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Zhiyuan Liu - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Zhiyuan</span> <span class=font-weight-bold>Liu</span></h2><hr><div class=row><div class=col-lg-9><h4>2022</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.acl-long.151.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--acl-long--151 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.acl-long.151 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.acl-long.151/>bert2<span class=acl-fixed-case>BERT</span>: Towards Reusable Pretrained Language Models</a></strong><br><a href=/people/c/cheng-chen/>Cheng Chen</a>
|
<a href=/people/y/yichun-yin/>Yichun Yin</a>
|
<a href=/people/l/lifeng-shang/>Lifeng Shang</a>
|
<a href=/people/x/xin-jiang/>Xin Jiang</a>
|
<a href=/people/y/yujia-qin/>Yujia Qin</a>
|
<a href=/people/f/fengyu-wang/>Fengyu Wang</a>
|
<a href=/people/z/zhi-wang/>Zhi Wang</a>
|
<a href=/people/x/xiao-chen/>Xiao Chen</a>
|
<a href=/people/z/zhiyuan-liu/>Zhiyuan Liu</a>
|
<a href=/people/q/qun-liu/>Qun Liu</a><br><a href=/volumes/2022.acl-long/ class=text-muted>Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--acl-long--151><div class="card-body p-3 small">In recent years, researchers tend to pre-train ever-larger language models to explore the upper limit of deep models. However, large language model pre-training costs intensive computational resources, and most of the models are trained from scratch without reusing the existing pre-trained models, which is wasteful. In this paper, we propose bert2BERT, which can effectively transfer the knowledge of an existing smaller pre-trained model to a large model through parameter initialization and significantly improve the pre-training efficiency of the large model. Specifically, we extend the previous function-preserving method proposed in computer vision on the Transformer-based language model, and further improve it by proposing a novel method, advanced knowledge for large model&#8217;s initialization. In addition, a two-stage learning method is proposed to further accelerate the pre-training. We conduct extensive experiments on representative PLMs (e.g., BERT and GPT) and demonstrate that (1) our method can save a significant amount of training cost compared with baselines including learning from scratch, StackBERT and MSLT; (2) our method is generic and applicable to different types of pre-trained models. In particular, bert2BERT saves about 45% and 47% computational cost of pre-training BERT<tex-math>_{\\rm BASE}</tex-math> and GPT<tex-math>_{\\rm BASE}</tex-math> by reusing the models of almost their half sizes.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.acl-long.347.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--acl-long--347 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.acl-long.347 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2022.acl-long.347.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.acl-long.347/>Pass off Fish Eyes for Pearls: Attacking Model Selection of Pre-trained Models</a></strong><br><a href=/people/b/biru-zhu/>Biru Zhu</a>
|
<a href=/people/y/yujia-qin/>Yujia Qin</a>
|
<a href=/people/f/fanchao-qi/>Fanchao Qi</a>
|
<a href=/people/y/yangdong-deng/>Yangdong Deng</a>
|
<a href=/people/z/zhiyuan-liu/>Zhiyuan Liu</a>
|
<a href=/people/m/maosong-sun/>Maosong Sun</a>
|
<a href=/people/m/ming-gu/>Ming Gu</a><br><a href=/volumes/2022.acl-long/ class=text-muted>Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--acl-long--347><div class="card-body p-3 small">Selecting an appropriate pre-trained model (PTM) for a specific downstream task typically requires significant efforts of fine-tuning. To accelerate this process, researchers propose feature-based model selection (FMS) methods, which assess PTMs&#8217; transferability to a specific task in a fast way without fine-tuning. In this work, we argue that current FMS methods are vulnerable, as the assessment mainly relies on the static features extracted from PTMs. However, such features are derived without training PTMs on downstream tasks, and are not necessarily reliable indicators for the PTM&#8217;s transferability. To validate our viewpoints, we design two methods to evaluate the robustness of FMS: (1) model disguise attack, which post-trains an inferior PTM with a contrastive objective, and (2) evaluation data selection, which selects a subset of the data points for FMS evaluation based on K-means clustering. Experimental results prove that both methods can successfully make FMS mistakenly judge the transferability of PTMs. Moreover, we find that these two methods can further be combined with the backdoor attack to misguide the FMS to select poisoned models. To the best of our knowledge, this is the first work to demonstrate the defects of current FMS algorithms and evaluate their potential security risks. By identifying previously unseen risks of FMS, our study indicates new directions for improving the robustness of FMS.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.acl-long.483.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--acl-long--483 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.acl-long.483 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.acl-long.483" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2022.acl-long.483/>Prototypical Verbalizer for Prompt-based Few-shot Tuning</a></strong><br><a href=/people/g/ganqu-cui/>Ganqu Cui</a>
|
<a href=/people/s/shengding-hu/>Shengding Hu</a>
|
<a href=/people/n/ning-ding/>Ning Ding</a>
|
<a href=/people/l/longtao-huang/>Longtao Huang</a>
|
<a href=/people/z/zhiyuan-liu/>Zhiyuan Liu</a><br><a href=/volumes/2022.acl-long/ class=text-muted>Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--acl-long--483><div class="card-body p-3 small">Prompt-based tuning for pre-trained language models (PLMs) has shown its effectiveness in few-shot learning. Typically, prompt-based tuning wraps the input text into a cloze question. To make predictions, the model maps the output words to labels via a verbalizer, which is either manually designed or automatically built. However, manual verbalizers heavily depend on domain-specific prior knowledge and human efforts, while finding appropriate label words automatically still remains challenging.In this work, we propose the prototypical verbalizer (ProtoVerb) which is built directly from training data. Specifically, ProtoVerb learns prototype vectors as verbalizers by contrastive learning. In this way, the prototypes summarize training instances and are able to enclose rich class-level semantics. We conduct experiments on both topic classification and entity typing tasks, and the results demonstrate that ProtoVerb significantly outperforms current automatic verbalizers, especially when training data is extremely scarce. More surprisingly, ProtoVerb consistently boosts prompt-based tuning even on untuned PLMs, indicating an elegant non-tuning way to utilize PLMs. Our codes are avaliable at https://github.com/thunlp/OpenPrompt.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.acl-long.559.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--acl-long--559 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.acl-long.559 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.acl-long.559" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2022.acl-long.559/>Program Transfer for Answering Complex Questions over Knowledge Bases</a></strong><br><a href=/people/s/shulin-cao/>Shulin Cao</a>
|
<a href=/people/j/jiaxin-shi/>Jiaxin Shi</a>
|
<a href=/people/z/zijun-yao/>Zijun Yao</a>
|
<a href=/people/x/xin-lv/>Xin Lv</a>
|
<a href=/people/j/jifan-yu/>Jifan Yu</a>
|
<a href=/people/l/lei-hou/>Lei Hou</a>
|
<a href=/people/j/juanzi-li/>Juanzi Li</a>
|
<a href=/people/z/zhiyuan-liu/>Zhiyuan Liu</a>
|
<a href=/people/j/jinghui-xiao/>Jinghui Xiao</a><br><a href=/volumes/2022.acl-long/ class=text-muted>Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--acl-long--559><div class="card-body p-3 small">Program induction for answering complex questions over knowledge bases KBs aims to decompose a question into a multi step program whose execution against the KB produces the final answer Learning to induce programs relies on a large number of parallel question program pairs for the given KB However for most KBs the gold program annotations are usually lacking making learning difficult In this paper we propose the approach of program transfer which aims to leverage the valuable program annotations on the rich resourced KBs as external supervision signals to aid program induction for the low resourced KBs that lack program annotations For program transfer we design a novel two stage parsing framework with an efficient ontology guided pruning strategy First a sketch parser translates the question into a high level program sketch which is the composition of functions Second given the question and sketch an argument parser searches the detailed arguments from the KB for functions During the searching we incorporate the KB ontology to prune the search space The experiments on ComplexWebQuestions and WebQuestionSP show that our method outperforms SOTA methods significantly demonstrating the effectiveness of program transfer and our framework Our codes and datasets can be obtained from https://github.com/THU-KEG/ProgramTransfer</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.acl-long.576.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--acl-long--576 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.acl-long.576 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2022.acl-long.576.software.tgz data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.acl-long.576" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2022.acl-long.576/><span class=acl-fixed-case>PPT</span>: Pre-trained Prompt Tuning for Few-shot Learning</a></strong><br><a href=/people/y/yuxian-gu/>Yuxian Gu</a>
|
<a href=/people/x/xu-han/>Xu Han</a>
|
<a href=/people/z/zhiyuan-liu/>Zhiyuan Liu</a>
|
<a href=/people/m/minlie-huang/>Minlie Huang</a><br><a href=/volumes/2022.acl-long/ class=text-muted>Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--acl-long--576><div class="card-body p-3 small">Prompts for pre-trained language models (PLMs) have shown remarkable performance by bridging the gap between pre-training tasks and various downstream tasks. Among these methods, prompt tuning, which freezes PLMs and only tunes soft prompts, provides an efficient and effective solution for adapting large-scale PLMs to downstream tasks. However, prompt tuning is yet to be fully explored. In our pilot experiments, we find that prompt tuning performs comparably with conventional full-model tuning when downstream data are sufficient, whereas it is much worse under few-shot learning settings, which may hinder the application of prompt tuning. We attribute this low performance to the manner of initializing soft prompts. Therefore, in this work, we propose to pre-train prompts by adding soft prompts into the pre-training stage to obtain a better initialization. We name this Pre-trained Prompt Tuning framework &#8220;PPT&#8221;. To ensure the generalization of PPT, we formulate similar classification tasks into a unified task form and pre-train soft prompts for this unified task. Extensive experiments show that tuning pre-trained prompts for downstream tasks can reach or even outperform full-model fine-tuning under both full-data and few-shot settings. Our approach is effective and efficient for using large-scale PLMs in practice.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-acl.17.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-acl--17 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-acl.17 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2022.findings-acl.17.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.findings-acl.17" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2022.findings-acl.17/><span class=acl-fixed-case>LEVEN</span>: A Large-Scale <span class=acl-fixed-case>C</span>hinese Legal Event Detection Dataset</a></strong><br><a href=/people/f/feng-yao/>Feng Yao</a>
|
<a href=/people/c/chaojun-xiao/>Chaojun Xiao</a>
|
<a href=/people/x/xiaozhi-wang/>Xiaozhi Wang</a>
|
<a href=/people/z/zhiyuan-liu/>Zhiyuan Liu</a>
|
<a href=/people/l/lei-hou/>Lei Hou</a>
|
<a href=/people/c/cunchao-tu/>Cunchao Tu</a>
|
<a href=/people/j/juanzi-li/>Juanzi Li</a>
|
<a href=/people/y/yun-liu/>Yun Liu</a>
|
<a href=/people/w/weixing-shen/>Weixing Shen</a>
|
<a href=/people/m/maosong-sun/>Maosong Sun</a><br><a href=/volumes/2022.findings-acl/ class=text-muted>Findings of the Association for Computational Linguistics: ACL 2022</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-acl--17><div class="card-body p-3 small">Recognizing facts is the most fundamental step in making judgments, hence detecting events in the legal documents is important to legal case analysis tasks. However, existing Legal Event Detection (LED) datasets only concern incomprehensive event types and have limited annotated data, which restricts the development of LED methods and their downstream applications. To alleviate these issues, we present LEVEN a large-scale Chinese LEgal eVENt detection dataset, with 8,116 legal documents and 150,977 human-annotated event mentions in 108 event types. Not only charge-related events, LEVEN also covers general events, which are critical for legal case understanding but neglected in existing LED datasets. To our knowledge, LEVEN is the largest LED dataset and has dozens of times the data scale of others, which shall significantly promote the training and evaluation of LED methods. The results of extensive experiments indicate that LED is challenging and needs further effort. Moreover, we simply utilize legal events as side information to promote downstream applications. The method achieves improvements of average 2.2 points precision in low-resource judgment prediction, and 1.5 points mean average precision in unsupervised case retrieval, which suggests the fundamentality of LED. The source code and dataset can be obtained from https://github.com/thunlp/LEVEN.</div></div><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-demo.43.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-demo--43 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-demo.43 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-demo.43" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-demo.43/>OpenAttack : An Open-source Textual Adversarial Attack Toolkit<span class=acl-fixed-case>O</span>pen<span class=acl-fixed-case>A</span>ttack: An Open-source Textual Adversarial Attack Toolkit</a></strong><br><a href=/people/g/guoyang-zeng/>Guoyang Zeng</a>
|
<a href=/people/f/fanchao-qi/>Fanchao Qi</a>
|
<a href=/people/q/qianrui-zhou/>Qianrui Zhou</a>
|
<a href=/people/t/tingji-zhang/>Tingji Zhang</a>
|
<a href=/people/z/zixian-ma/>Zixian Ma</a>
|
<a href=/people/b/bairu-hou/>Bairu Hou</a>
|
<a href=/people/y/yuan-zang/>Yuan Zang</a>
|
<a href=/people/z/zhiyuan-liu/>Zhiyuan Liu</a>
|
<a href=/people/m/maosong-sun/>Maosong Sun</a><br><a href=/volumes/2021.acl-demo/ class=text-muted>Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-demo--43><div class="card-body p-3 small">Textual adversarial attacking has received wide and increasing attention in recent years. Various <a href=https://en.wikipedia.org/wiki/Attack_model>attack models</a> have been proposed, which are enormously distinct and implemented with different programming frameworks and settings. These facts hinder quick utilization and fair comparison of attack models. In this paper, we present an open-source textual adversarial attack toolkit named OpenAttack to solve these issues. Compared with existing other textual adversarial attack toolkits, OpenAttack has its unique strengths in support for all attack types, <a href=https://en.wikipedia.org/wiki/Multilinguality>multilinguality</a>, and <a href=https://en.wikipedia.org/wiki/Parallel_computing>parallel processing</a>. Currently, OpenAttack includes 15 typical attack models that cover all attack types. Its highly inclusive modular design not only supports quick utilization of existing attack models, but also enables great flexibility and extensibility. OpenAttack has broad uses including comparing and evaluating <a href=https://en.wikipedia.org/wiki/Attack_model>attack models</a>, measuring robustness of a model, assisting in developing new <a href=https://en.wikipedia.org/wiki/Attack_model>attack models</a>, and <a href=https://en.wikipedia.org/wiki/Adversarial_system>adversarial training</a>. Source code and documentation can be obtained at https://github.com/thunlp/OpenAttack.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.366.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--366 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.366 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.366" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.366/>CodRED : A Cross-Document Relation Extraction Dataset for Acquiring Knowledge in the Wild<span class=acl-fixed-case>C</span>od<span class=acl-fixed-case>RED</span>: A Cross-Document Relation Extraction Dataset for Acquiring Knowledge in the Wild</a></strong><br><a href=/people/y/yuan-yao/>Yuan Yao</a>
|
<a href=/people/j/jiaju-du/>Jiaju Du</a>
|
<a href=/people/y/yankai-lin/>Yankai Lin</a>
|
<a href=/people/p/peng-li/>Peng Li</a>
|
<a href=/people/z/zhiyuan-liu/>Zhiyuan Liu</a>
|
<a href=/people/j/jie-zhou/>Jie Zhou</a>
|
<a href=/people/m/maosong-sun/>Maosong Sun</a><br><a href=/volumes/2021.emnlp-main/ class=text-muted>Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--366><div class="card-body p-3 small">Existing relation extraction (RE) methods typically focus on extracting relational facts between entity pairs within single sentences or documents. However, a large quantity of relational facts in <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge bases</a> can only be inferred across documents in practice. In this work, we present the problem of cross-document RE, making an initial step towards <a href=https://en.wikipedia.org/wiki/Knowledge_acquisition>knowledge acquisition</a> in the wild. To facilitate the research, we construct the first human-annotated cross-document RE dataset CodRED. Compared to existing RE datasets, CodRED presents two key challenges : Given two entities, (1) it requires finding the relevant documents that can provide clues for identifying their relations ; (2) it requires reasoning over multiple documents to extract the relational facts. We conduct comprehensive experiments to show that CodRED is challenging to existing RE methods including strong BERT-based models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.374.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--374 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.374 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.374" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.374/>Mind the Style of Text ! Adversarial and Backdoor Attacks Based on Text Style Transfer</a></strong><br><a href=/people/f/fanchao-qi/>Fanchao Qi</a>
|
<a href=/people/y/yangyi-chen/>Yangyi Chen</a>
|
<a href=/people/x/xurui-zhang/>Xurui Zhang</a>
|
<a href=/people/m/mukai-li/>Mukai Li</a>
|
<a href=/people/z/zhiyuan-liu/>Zhiyuan Liu</a>
|
<a href=/people/m/maosong-sun/>Maosong Sun</a><br><a href=/volumes/2021.emnlp-main/ class=text-muted>Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--374><div class="card-body p-3 small">Adversarial attacks and <a href=https://en.wikipedia.org/wiki/Backdoor_(computing)>backdoor attacks</a> are two common security threats that hang over <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning</a>. Both of them harness task-irrelevant features of data in their implementation. Text style is a feature that is naturally irrelevant to most NLP tasks, and thus suitable for adversarial and backdoor attacks. In this paper, we make the first attempt to conduct adversarial and backdoor attacks based on text style transfer, which is aimed at altering the style of a sentence while preserving its meaning. We design an adversarial attack method and a <a href=https://en.wikipedia.org/wiki/Backdoor_(computing)>backdoor attack method</a>, and conduct extensive experiments to evaluate them. Experimental results show that popular NLP models are vulnerable to both adversarial and backdoor attacks based on text style transferthe attack success rates can exceed 90 % without much effort. It reflects the limited ability of <a href=https://en.wikipedia.org/wiki/Neuro-linguistic_programming>NLP models</a> to handle the feature of text style that has not been widely realized. In addition, the style transfer-based adversarial and backdoor attack methods show superiority to <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a> in many aspects. All the code and data of this paper can be obtained at https://github.com/thunlp/StyleAttack.</div></div><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.459.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--459 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.459 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938756 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.459" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.459/>Dynamic Anticipation and Completion for Multi-Hop Reasoning over Sparse Knowledge Graph</a></strong><br><a href=/people/x/xin-lv/>Xin Lv</a>
|
<a href=/people/x/xu-han/>Xu Han</a>
|
<a href=/people/l/lei-hou/>Lei Hou</a>
|
<a href=/people/j/juanzi-li/>Juanzi Li</a>
|
<a href=/people/z/zhiyuan-liu/>Zhiyuan Liu</a>
|
<a href=/people/w/wei-zhang/>Wei Zhang</a>
|
<a href=/people/y/yichi-zhang/>Yichi Zhang</a>
|
<a href=/people/h/hao-kong/>Hao Kong</a>
|
<a href=/people/s/suhui-wu/>Suhui Wu</a><br><a href=/volumes/2020.emnlp-main/ class=text-muted>Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--459><div class="card-body p-3 small">Multi-hop reasoning has been widely studied in recent years to seek an effective and interpretable method for knowledge graph (KG) completion. Most previous <a href=https://en.wikipedia.org/wiki/Automated_reasoning>reasoning methods</a> are designed for dense KGs with enough paths between entities, but can not work well on those sparse KGs that only contain sparse paths for <a href=https://en.wikipedia.org/wiki/Automated_reasoning>reasoning</a>. On the one hand, sparse KGs contain less information, which makes it difficult for the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> to choose correct paths. On the other hand, the lack of evidential paths to target entities also makes the reasoning process difficult. To solve these problems, we propose a multi-hop reasoning model over sparse KGs, by applying novel dynamic anticipation and completion strategies : (1) The anticipation strategy utilizes the latent prediction of embedding-based models to make our model perform more potential path search over sparse KGs. (2) Based on the anticipation information, the completion strategy dynamically adds edges as additional actions during the path search, which further alleviates the sparseness problem of KGs. The experimental results on five datasets sampled from <a href=https://en.wikipedia.org/wiki/Freebase>Freebase</a>, <a href=https://en.wikipedia.org/wiki/NELL>NELL</a> and <a href=https://en.wikipedia.org/wiki/Wikidata>Wikidata</a> show that our method outperforms state-of-the-art baselines. Our codes and datasets can be obtained from https://github.com/THU-KEG/DacKGR.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.515.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--515 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.515 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938987 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.515" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.515/>Exploring and Evaluating Attributes, Values, and Structures for Entity Alignment</a></strong><br><a href=/people/z/zhiyuan-liu/>Zhiyuan Liu</a>
|
<a href=/people/y/yixin-cao/>Yixin Cao</a>
|
<a href=/people/l/liangming-pan/>Liangming Pan</a>
|
<a href=/people/j/juanzi-li/>Juanzi Li</a>
|
<a href=/people/z/zhiyuan-liu/>Zhiyuan Liu</a>
|
<a href=/people/t/tat-seng-chua/>Tat-Seng Chua</a><br><a href=/volumes/2020.emnlp-main/ class=text-muted>Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--515><div class="card-body p-3 small">Entity alignment (EA) aims at building a unified Knowledge Graph (KG) of rich content by linking the equivalent entities from various KGs. GNN-based EA methods present promising performance by modeling the KG structure defined by relation triples. However, attribute triples can also provide crucial alignment signal but have not been well explored yet. In this paper, we propose to utilize an attributed value encoder and partition the KG into subgraphs to model the various types of attribute triples efficiently. Besides, the performances of current EA methods are overestimated because of the name-bias of existing EA datasets. To make an objective evaluation, we propose a hard experimental setting where we select equivalent entity pairs with very different names as the test set. Under both the regular and hard settings, our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> achieves significant improvements (5.10 % on average Hits@1 in DBP15k) over 12 baselines in cross-lingual and monolingual datasets. Ablation studies on different subgraphs and a case study about attribute types further demonstrate the effectiveness of our method. Source code and data can be found at.<url>https://github.com/thunlp/explore-and-evaluate</url>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.515.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--515 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.515 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938987 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.515" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.515/>Exploring and Evaluating Attributes, Values, and Structures for Entity Alignment</a></strong><br><a href=/people/z/zhiyuan-liu/>Zhiyuan Liu</a>
|
<a href=/people/y/yixin-cao/>Yixin Cao</a>
|
<a href=/people/l/liangming-pan/>Liangming Pan</a>
|
<a href=/people/j/juanzi-li/>Juanzi Li</a>
|
<a href=/people/z/zhiyuan-liu/>Zhiyuan Liu</a>
|
<a href=/people/t/tat-seng-chua/>Tat-Seng Chua</a><br><a href=/volumes/2020.emnlp-main/ class=text-muted>Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--515><div class="card-body p-3 small">Entity alignment (EA) aims at building a unified Knowledge Graph (KG) of rich content by linking the equivalent entities from various KGs. GNN-based EA methods present promising performance by modeling the KG structure defined by relation triples. However, attribute triples can also provide crucial alignment signal but have not been well explored yet. In this paper, we propose to utilize an attributed value encoder and partition the KG into subgraphs to model the various types of attribute triples efficiently. Besides, the performances of current EA methods are overestimated because of the name-bias of existing EA datasets. To make an objective evaluation, we propose a hard experimental setting where we select equivalent entity pairs with very different names as the test set. Under both the regular and hard settings, our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> achieves significant improvements (5.10 % on average Hits@1 in DBP15k) over 12 baselines in cross-lingual and monolingual datasets. Ablation studies on different subgraphs and a case study about attribute types further demonstrate the effectiveness of our method. Source code and data can be found at.<url>https://github.com/thunlp/explore-and-evaluate</url>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.566.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--566 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.566 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938884 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.566" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.566/>Train No Evil : Selective Masking for Task-Guided Pre-Training</a></strong><br><a href=/people/y/yuxian-gu/>Yuxian Gu</a>
|
<a href=/people/z/zhengyan-zhang/>Zhengyan Zhang</a>
|
<a href=/people/x/xiaozhi-wang/>Xiaozhi Wang</a>
|
<a href=/people/z/zhiyuan-liu/>Zhiyuan Liu</a>
|
<a href=/people/m/maosong-sun/>Maosong Sun</a><br><a href=/volumes/2020.emnlp-main/ class=text-muted>Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--566><div class="card-body p-3 small">Recently, pre-trained language models mostly follow the pre-train-then-fine-tuning paradigm and have achieved great performance on various downstream tasks. However, since the pre-training stage is typically task-agnostic and the fine-tuning stage usually suffers from insufficient supervised data, the models can not always well capture the domain-specific and task-specific patterns. In this paper, we propose a three-stage framework by adding a task-guided pre-training stage with selective masking between general pre-training and fine-tuning. In this stage, the model is trained by masked language modeling on in-domain unsupervised data to learn domain-specific patterns and we propose a novel selective masking strategy to learn task-specific patterns. Specifically, we design a method to measure the importance of each token in sequences and selectively mask the important tokens. Experimental results on two sentiment analysis tasks show that our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> can achieve comparable or even better performance with less than 50 % of computation cost, which indicates our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> is both effective and efficient. The source code of this paper can be obtained from.<url>https://github.com/thunlp/SelectiveMasking</url>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.738.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--738 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.738 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939283 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.738" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.738/>Partially-Aligned Data-to-Text Generation with Distant Supervision</a></strong><br><a href=/people/z/zihao-fu/>Zihao Fu</a>
|
<a href=/people/b/bei-shi/>Bei Shi</a>
|
<a href=/people/w/wai-lam/>Wai Lam</a>
|
<a href=/people/l/lidong-bing/>Lidong Bing</a>
|
<a href=/people/z/zhiyuan-liu/>Zhiyuan Liu</a><br><a href=/volumes/2020.emnlp-main/ class=text-muted>Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--738><div class="card-body p-3 small">The Data-to-Text task aims to generate <a href=https://en.wikipedia.org/wiki/Human-readable_medium>human-readable text</a> for describing some given structured data enabling more interpretability. However, the typical generation task is confined to a few particular domains since it requires well-aligned data which is difficult and expensive to obtain. Using partially-aligned data is an alternative way of solving the dataset scarcity problem. This kind of <a href=https://en.wikipedia.org/wiki/Data>data</a> is much easier to obtain since <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> can be produced automatically. However, using this kind of <a href=https://en.wikipedia.org/wiki/Data>data</a> induces the over-generation problem posing difficulties for existing <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>, which tends to add unrelated excerpts during the generation procedure. In order to effectively utilize automatically annotated partially-aligned datasets, we extend the traditional generation task to a refined task called Partially-Aligned Data-to-Text Generation (PADTG) which is more practical since it utilizes automatically annotated data for training and thus considerably expands the application domains. To tackle this new <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>, we propose a novel distant supervision generation framework. It firstly estimates the input data&#8217;s supportiveness for each target word with an <a href=https://en.wikipedia.org/wiki/Estimator>estimator</a> and then applies a supportiveness adaptor and a rebalanced beam search to harness the over-generation problem in the training and generation phases respectively. We also contribute a partially-aligned dataset (The data and source code of this paper can be obtained from https://github.com/fuzihaofzh/distant_supervision_nlg) by sampling sentences from <a href=https://en.wikipedia.org/wiki/Wikipedia>Wikipedia</a> and automatically extracting corresponding KB triples for each sentence from <a href=https://en.wikipedia.org/wiki/Wikidata>Wikidata</a>. The experimental results show that our <a href=https://en.wikipedia.org/wiki/Conceptual_framework>framework</a> outperforms all baseline models as well as verify the feasibility of utilizing partially-aligned data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.acl-main.540.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--acl-main--540 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.acl-main.540 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://slideslive.com/38928950 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.acl-main.540" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.acl-main.540/>Word-level Textual Adversarial Attacking as <a href=https://en.wikipedia.org/wiki/Combinatorial_optimization>Combinatorial Optimization</a></a></strong><br><a href=/people/y/yuan-zang/>Yuan Zang</a>
|
<a href=/people/f/fanchao-qi/>Fanchao Qi</a>
|
<a href=/people/c/chenghao-yang/>Chenghao Yang</a>
|
<a href=/people/z/zhiyuan-liu/>Zhiyuan Liu</a>
|
<a href=/people/m/meng-zhang/>Meng Zhang</a>
|
<a href=/people/q/qun-liu/>Qun Liu</a>
|
<a href=/people/m/maosong-sun/>Maosong Sun</a><br><a href=/volumes/2020.acl-main/ class=text-muted>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--acl-main--540><div class="card-body p-3 small">Adversarial attacks are carried out to reveal the vulnerability of <a href=https://en.wikipedia.org/wiki/Deep_learning>deep neural networks</a>. Textual adversarial attacking is challenging because text is discrete and a small perturbation can bring significant change to the original input. Word-level attacking, which can be regarded as a <a href=https://en.wikipedia.org/wiki/Combinatorial_optimization>combinatorial optimization problem</a>, is a well-studied class of textual attack methods. However, existing word-level attack models are far from perfect, largely because unsuitable search space reduction methods and inefficient <a href=https://en.wikipedia.org/wiki/Mathematical_optimization>optimization algorithms</a> are employed. In this paper, we propose a novel attack model, which incorporates the sememe-based word substitution method and particle swarm optimization-based search algorithm to solve the two problems separately. We conduct exhaustive experiments to evaluate our attack model by attacking BiLSTM and <a href=https://en.wikipedia.org/wiki/BERT>BERT</a> on three benchmark datasets. Experimental results demonstrate that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> consistently achieves much higher attack success rates and crafts more high-quality adversarial examples as compared to baseline methods. Also, further experiments show our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> has higher transferability and can bring more robustness enhancement to victim models by adversarial training. All the code and data of this paper can be obtained on https://github.com/thunlp/SememePSO-Attack.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.aacl-main.77.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--aacl-main--77 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.aacl-main.77 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.aacl-main.77/>ExpanRL : Hierarchical Reinforcement Learning for Course Concept Expansion in MOOCs<span class=acl-fixed-case>E</span>xpan<span class=acl-fixed-case>RL</span>: Hierarchical Reinforcement Learning for Course Concept Expansion in <span class=acl-fixed-case>MOOC</span>s</a></strong><br><a href=/people/j/jifan-yu/>Jifan Yu</a>
|
<a href=/people/c/chenyu-wang/>Chenyu Wang</a>
|
<a href=/people/g/gan-luo/>Gan Luo</a>
|
<a href=/people/l/lei-hou/>Lei Hou</a>
|
<a href=/people/j/juanzi-li/>Juanzi Li</a>
|
<a href=/people/j/jie-tang/>Jie Tang</a>
|
<a href=/people/m/minlie-huang/>Minlie Huang</a>
|
<a href=/people/z/zhiyuan-liu/>Zhiyuan Liu</a><br><a href=/volumes/2020.aacl-main/ class=text-muted>Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--aacl-main--77><div class="card-body p-3 small">Within the prosperity of Massive Open Online Courses (MOOCs), the education applications that automatically provide extracurricular knowledge for MOOC users become rising research topics. However, MOOC courses&#8217; diversity and rapid updates make it more challenging to find suitable new knowledge for students. In this paper, we present ExpanRL, an end-to-end hierarchical reinforcement learning (HRL) model for concept expansion in <a href=https://en.wikipedia.org/wiki/Massive_open_online_course>MOOCs</a>. Employing a two-level HRL mechanism of seed selection and concept expansion, ExpanRL is more feasible to adjust the expansion strategy to find new concepts based on the students&#8217; feedback on expansion results. Our experiments on nine novel datasets from real MOOCs show that ExpanRL achieves significant improvements over existing methods and maintain competitive performance under different settings.</div></div><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1025.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1025 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1025 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1025" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1025/>Low-Resource Name Tagging Learned with Weakly Labeled Data</a></strong><br><a href=/people/y/yixin-cao/>Yixin Cao</a>
|
<a href=/people/z/zikun-hu/>Zikun Hu</a>
|
<a href=/people/t/tat-seng-chua/>Tat-seng Chua</a>
|
<a href=/people/z/zhiyuan-liu/>Zhiyuan Liu</a>
|
<a href=/people/h/heng-ji/>Heng Ji</a><br><a href=/volumes/D19-1/ class=text-muted>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1025><div class="card-body p-3 small">Name tagging in low-resource languages or domains suffers from inadequate training data. Existing work heavily relies on additional information, while leaving those noisy annotations unexplored that extensively exist on the web. In this paper, we propose a novel neural model for name tagging solely based on weakly labeled (WL) data, so that it can be applied in any low-resource settings. To take the best advantage of all WL sentences, we split them into high-quality and noisy portions for two modules, respectively : (1) a classification module focusing on the large portion of noisy data can efficiently and robustly pretrain the tag classifier by capturing textual context semantics ; and (2) a costly sequence labeling module focusing on high-quality data utilizes Partial-CRFs with non-entity sampling to achieve global optimum. Two <a href=https://en.wikipedia.org/wiki/Module_(computer_science)>modules</a> are combined via <a href=https://en.wikipedia.org/wiki/Parameter_(computer_programming)>shared parameters</a>. Extensive experiments involving five low-resource languages and fine-grained food domain demonstrate our superior performance (6 % and 7.8 % F1 gains on average) as well as <a href=https://en.wikipedia.org/wiki/Efficiency>efficiency</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1251.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1251 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1251 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1251.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1251" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1251/>NumNet : Machine Reading Comprehension with <a href=https://en.wikipedia.org/wiki/Numerical_analysis>Numerical Reasoning</a><span class=acl-fixed-case>N</span>um<span class=acl-fixed-case>N</span>et: Machine Reading Comprehension with Numerical Reasoning</a></strong><br><a href=/people/q/qiu-ran/>Qiu Ran</a>
|
<a href=/people/y/yankai-lin/>Yankai Lin</a>
|
<a href=/people/p/peng-li/>Peng Li</a>
|
<a href=/people/j/jie-zhou/>Jie Zhou</a>
|
<a href=/people/z/zhiyuan-liu/>Zhiyuan Liu</a><br><a href=/volumes/D19-1/ class=text-muted>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1251><div class="card-body p-3 small">Numerical reasoning, such as <a href=https://en.wikipedia.org/wiki/Addition>addition</a>, <a href=https://en.wikipedia.org/wiki/Subtraction>subtraction</a>, <a href=https://en.wikipedia.org/wiki/Sorting>sorting</a> and <a href=https://en.wikipedia.org/wiki/Counting>counting</a> is a critical skill in human&#8217;s reading comprehension, which has not been well considered in existing machine reading comprehension (MRC) systems. To address this issue, we propose a numerical MRC model named as NumNet, which utilizes a numerically-aware graph neural network to consider the comparing information and performs numerical reasoning over numbers in the question and passage. Our system achieves an EM-score of 64.56 % on the DROP dataset, outperforming all existing <a href=https://en.wikipedia.org/wiki/Machine_learning>machine reading comprehension models</a> by considering the <a href=https://en.wikipedia.org/wiki/Numerical_analysis>numerical relations</a> among numbers.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1584.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1584 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1584 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1584.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1584" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1584/>HMEAE : Hierarchical Modular Event Argument Extraction<span class=acl-fixed-case>HMEAE</span>: Hierarchical Modular Event Argument Extraction</a></strong><br><a href=/people/x/xiaozhi-wang/>Xiaozhi Wang</a>
|
<a href=/people/z/ziqi-wang/>Ziqi Wang</a>
|
<a href=/people/x/xu-han/>Xu Han</a>
|
<a href=/people/z/zhiyuan-liu/>Zhiyuan Liu</a>
|
<a href=/people/j/juanzi-li/>Juanzi Li</a>
|
<a href=/people/p/peng-li/>Peng Li</a>
|
<a href=/people/m/maosong-sun/>Maosong Sun</a>
|
<a href=/people/j/jie-zhou/>Jie Zhou</a>
|
<a href=/people/x/xiang-ren/>Xiang Ren</a><br><a href=/volumes/D19-1/ class=text-muted>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1584><div class="card-body p-3 small">Existing <a href=https://en.wikipedia.org/wiki/Event_(probability_theory)>event extraction methods</a> classify each argument role independently, ignoring the conceptual correlations between different argument roles. In this paper, we propose a Hierarchical Modular Event Argument Extraction (HMEAE) model, to provide effective <a href=https://en.wikipedia.org/wiki/Inductive_bias>inductive bias</a> from the concept hierarchy of event argument roles. Specifically, we design a neural module network for each basic unit of the concept hierarchy, and then hierarchically compose relevant unit modules with logical operations into a role-oriented modular network to classify a specific argument role. As many argument roles share the same high-level unit module, their correlation can be utilized to extract specific event arguments better. Experiments on real-world datasets show that HMEAE can effectively leverage useful knowledge from the concept hierarchy and significantly outperform the state-of-the-art baselines. The source code can be obtained from https://github.com/thunlp/HMEAE.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1137.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1137 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1137 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1137" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-1137/>DIAG-NRE : A Neural Pattern Diagnosis Framework for Distantly Supervised Neural Relation Extraction<span class=acl-fixed-case>DIAG</span>-<span class=acl-fixed-case>NRE</span>: A Neural Pattern Diagnosis Framework for Distantly Supervised Neural Relation Extraction</a></strong><br><a href=/people/s/shun-zheng/>Shun Zheng</a>
|
<a href=/people/x/xu-han/>Xu Han</a>
|
<a href=/people/y/yankai-lin/>Yankai Lin</a>
|
<a href=/people/p/peilin-yu/>Peilin Yu</a>
|
<a href=/people/l/lu-chen/>Lu Chen</a>
|
<a href=/people/l/ling-huang/>Ling Huang</a>
|
<a href=/people/z/zhiyuan-liu/>Zhiyuan Liu</a>
|
<a href=/people/w/wei-xu/>Wei Xu</a><br><a href=/volumes/P19-1/ class=text-muted>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1137><div class="card-body p-3 small">Pattern-based labeling methods have achieved promising results in alleviating the inevitable labeling noises of distantly supervised neural relation extraction. However, these methods require significant expert labor to write relation-specific patterns, which makes them too sophisticated to generalize quickly. To ease the labor-intensive workload of pattern writing and enable the quick generalization to new relation types, we propose a neural pattern diagnosis framework, DIAG-NRE, that can automatically summarize and refine high-quality relational patterns from noise data with human experts in the loop. To demonstrate the effectiveness of DIAG-NRE, we apply it to two real-world datasets and present both significant and interpretable improvements over state-of-the-art methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1139.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1139 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1139 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1139" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-1139/>ERNIE : Enhanced Language Representation with Informative Entities<span class=acl-fixed-case>ERNIE</span>: Enhanced Language Representation with Informative Entities</a></strong><br><a href=/people/z/zhengyan-zhang/>Zhengyan Zhang</a>
|
<a href=/people/x/xu-han/>Xu Han</a>
|
<a href=/people/z/zhiyuan-liu/>Zhiyuan Liu</a>
|
<a href=/people/x/xin-jiang/>Xin Jiang</a>
|
<a href=/people/m/maosong-sun/>Maosong Sun</a>
|
<a href=/people/q/qun-liu/>Qun Liu</a><br><a href=/volumes/P19-1/ class=text-muted>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1139><div class="card-body p-3 small">Neural language representation models such as BERT pre-trained on large-scale corpora can well capture rich semantic patterns from plain text, and be fine-tuned to consistently improve the performance of various NLP tasks. However, the existing pre-trained language models rarely consider incorporating knowledge graphs (KGs), which can provide rich structured knowledge facts for better <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>language understanding</a>. We argue that informative entities in KGs can enhance <a href=https://en.wikipedia.org/wiki/Representation_(mathematics)>language representation</a> with <a href=https://en.wikipedia.org/wiki/Knowledge>external knowledge</a>. In this paper, we utilize both large-scale textual corpora and KGs to train an enhanced language representation model (ERNIE), which can take full advantage of lexical, syntactic, and knowledge information simultaneously. The experimental results have demonstrated that ERNIE achieves significant improvements on various knowledge-driven tasks, and meanwhile is comparable with the state-of-the-art model BERT on other common NLP tasks. The code and datasets will be available in the future.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1140.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1140 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1140 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1140" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-1140/>Multi-Channel Graph Neural Network for Entity Alignment</a></strong><br><a href=/people/y/yixin-cao/>Yixin Cao</a>
|
<a href=/people/z/zhiyuan-liu/>Zhiyuan Liu</a>
|
<a href=/people/c/chengjiang-li/>Chengjiang Li</a>
|
<a href=/people/z/zhiyuan-liu/>Zhiyuan Liu</a>
|
<a href=/people/j/juanzi-li/>Juanzi Li</a>
|
<a href=/people/t/tat-seng-chua/>Tat-Seng Chua</a><br><a href=/volumes/P19-1/ class=text-muted>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1140><div class="card-body p-3 small">Entity alignment typically suffers from the issues of structural heterogeneity and limited seed alignments. In this paper, we propose a novel Multi-channel Graph Neural Network model (MuGNN) to learn alignment-oriented knowledge graph (KG) embeddings by robustly encoding two KGs via multiple channels. Each channel encodes KGs via different relation weighting schemes with respect to self-attention towards KG completion and cross-KG attention for pruning exclusive entities respectively, which are further combined via pooling techniques. Moreover, we also infer and transfer rule knowledge for completing two KGs consistently. MuGNN is expected to reconcile the structural differences of two KGs, and thus make better use of seed alignments. Extensive experiments on five publicly available datasets demonstrate our superior performance (5 % Hits@1 up on average). Source code and data used in the experiments can be accessed at https://github.com/thunlp/MuGNN.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1140.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1140 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1140 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1140" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-1140/>Multi-Channel Graph Neural Network for Entity Alignment</a></strong><br><a href=/people/y/yixin-cao/>Yixin Cao</a>
|
<a href=/people/z/zhiyuan-liu/>Zhiyuan Liu</a>
|
<a href=/people/c/chengjiang-li/>Chengjiang Li</a>
|
<a href=/people/z/zhiyuan-liu/>Zhiyuan Liu</a>
|
<a href=/people/j/juanzi-li/>Juanzi Li</a>
|
<a href=/people/t/tat-seng-chua/>Tat-Seng Chua</a><br><a href=/volumes/P19-1/ class=text-muted>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1140><div class="card-body p-3 small">Entity alignment typically suffers from the issues of structural heterogeneity and limited seed alignments. In this paper, we propose a novel Multi-channel Graph Neural Network model (MuGNN) to learn alignment-oriented knowledge graph (KG) embeddings by robustly encoding two KGs via multiple channels. Each channel encodes KGs via different relation weighting schemes with respect to self-attention towards KG completion and cross-KG attention for pruning exclusive entities respectively, which are further combined via pooling techniques. Moreover, we also infer and transfer rule knowledge for completing two KGs consistently. MuGNN is expected to reconcile the structural differences of two KGs, and thus make better use of seed alignments. Extensive experiments on five publicly available datasets demonstrate our superior performance (5 % Hits@1 up on average). Source code and data used in the experiments can be accessed at https://github.com/thunlp/MuGNN.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1571.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1571 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1571 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/385428643 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1571" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1571/>Modeling Semantic Compositionality with Sememe Knowledge</a></strong><br><a href=/people/f/fanchao-qi/>Fanchao Qi</a>
|
<a href=/people/j/junjie-huang/>Junjie Huang</a>
|
<a href=/people/c/chenghao-yang/>Chenghao Yang</a>
|
<a href=/people/z/zhiyuan-liu/>Zhiyuan Liu</a>
|
<a href=/people/x/xiao-chen/>Xiao Chen</a>
|
<a href=/people/q/qun-liu/>Qun Liu</a>
|
<a href=/people/m/maosong-sun/>Maosong Sun</a><br><a href=/volumes/P19-1/ class=text-muted>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1571><div class="card-body p-3 small">Semantic compositionality (SC) refers to the phenomenon that the meaning of a complex linguistic unit can be composed of the meanings of its constituents. Most related works focus on using complicated compositionality functions to model SC while few works consider external knowledge in <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>. In this paper, we verify the effectiveness of <a href=https://en.wikipedia.org/wiki/Sememe>sememes</a>, the minimum semantic units of human languages, in modeling <a href=https://en.wikipedia.org/wiki/Semantic_space>SC</a> by a confirmatory experiment. Furthermore, we make the first attempt to incorporate sememe knowledge into SC models, and employ the sememe-incorporated models in learning representations of multiword expressions, a typical task of SC. In experiments, we implement our models by incorporating knowledge from a famous sememe knowledge base HowNet and perform both intrinsic and extrinsic evaluations. Experimental results show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> achieve significant performance boost as compared to the baseline methods without considering sememe knowledge. We further conduct quantitative analysis and case studies to demonstrate the effectiveness of applying <a href=https://en.wikipedia.org/wiki/Sememe>sememe knowledge</a> in modeling SC.All the code and data of this paper can be obtained on https://github.com/thunlp/Sememe-SC.</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1041.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1041 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1041 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=C18-1041" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/C18-1041/>Few-Shot Charge Prediction with Discriminative Legal Attributes</a></strong><br><a href=/people/z/zikun-hu/>Zikun Hu</a>
|
<a href=/people/x/xiang-li/>Xiang Li</a>
|
<a href=/people/c/cunchao-tu/>Cunchao Tu</a>
|
<a href=/people/z/zhiyuan-liu/>Zhiyuan Liu</a>
|
<a href=/people/m/maosong-sun/>Maosong Sun</a><br><a href=/volumes/C18-1/ class=text-muted>Proceedings of the 27th International Conference on Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1041><div class="card-body p-3 small">Automatic charge prediction aims to predict the final charges according to the fact descriptions in criminal cases and plays a crucial role in legal assistant systems. Existing works on charge prediction perform adequately on those high-frequency charges but are not yet capable of predicting few-shot charges with limited cases. Moreover, these exist many confusing charge pairs, whose fact descriptions are fairly similar to each other. To address these issues, we introduce several discriminative attributes of charges as the internal mapping between fact descriptions and <a href=https://en.wikipedia.org/wiki/Charge_(physics)>charges</a>. These attributes provide additional information for few-shot charges, as well as effective signals for distinguishing confusing charges. More specifically, we propose an attribute-attentive charge prediction model to infer the attributes and charges simultaneously. Experimental results on real-work datasets demonstrate that our proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieves significant and consistent improvements than other state-of-the-art baselines. Specifically, our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms other baselines by more than 50 % in the few-shot scenario. Our codes and datasets can be obtained from https://github.com/thunlp/attribute_charge.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1057.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1057 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1057 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=C18-1057" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/C18-1057/>Neural Collective Entity Linking</a></strong><br><a href=/people/y/yixin-cao/>Yixin Cao</a>
|
<a href=/people/l/lei-hou/>Lei Hou</a>
|
<a href=/people/j/juanzi-li/>Juanzi Li</a>
|
<a href=/people/z/zhiyuan-liu/>Zhiyuan Liu</a><br><a href=/volumes/C18-1/ class=text-muted>Proceedings of the 27th International Conference on Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1057><div class="card-body p-3 small">Entity Linking aims to link entity mentions in texts to <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge bases</a>, and neural models have achieved recent success in this task. However, most existing methods rely on local contexts to resolve entities independently, which may usually fail due to the data sparsity of local information. To address this issue, we propose a novel neural model for collective entity linking, named as NCEL. NCEL apply Graph Convolutional Network to integrate both local contextual features and global coherence information for <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entity linking</a>. To improve the computation efficiency, we approximately perform <a href=https://en.wikipedia.org/wiki/Convolution>graph convolution</a> on a subgraph of adjacent entity mentions instead of those in the entire text. We further introduce an attention scheme to improve the robustness of NCEL to <a href=https://en.wikipedia.org/wiki/Noise_(signal_processing)>data noise</a> and train the model on Wikipedia hyperlinks to avoid <a href=https://en.wikipedia.org/wiki/Overfitting>overfitting</a> and domain bias. In experiments, we evaluate NCEL on five publicly available datasets to verify the <a href=https://en.wikipedia.org/wiki/Linker_(computing)>linking</a> performance as well as generalization ability. We also conduct an extensive analysis of <a href=https://en.wikipedia.org/wiki/Time_complexity>time complexity</a>, the impact of key modules, and qualitative results, which demonstrate the effectiveness and efficiency of our proposed method.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1099.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1099 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1099 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=C18-1099" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/C18-1099/>Adversarial Multi-lingual Neural Relation Extraction</a></strong><br><a href=/people/x/xiaozhi-wang/>Xiaozhi Wang</a>
|
<a href=/people/x/xu-han/>Xu Han</a>
|
<a href=/people/y/yankai-lin/>Yankai Lin</a>
|
<a href=/people/z/zhiyuan-liu/>Zhiyuan Liu</a>
|
<a href=/people/m/maosong-sun/>Maosong Sun</a><br><a href=/volumes/C18-1/ class=text-muted>Proceedings of the 27th International Conference on Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1099><div class="card-body p-3 small">Multi-lingual relation extraction aims to find unknown relational facts from text in various languages. Existing <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> can not well capture the consistency and diversity of relation patterns in different languages. To address these issues, we propose an adversarial multi-lingual neural relation extraction (AMNRE) model, which builds both consistent and individual representations for each sentence to consider the consistency and diversity among languages. Further, we adopt an adversarial training strategy to ensure those consistent sentence representations could effectively extract the language-consistent relation patterns. The experimental results on real-world datasets demonstrate that our AMNRE model significantly outperforms the state-of-the-art models. The source code of this paper can be obtained from https://github.com/thunlp/AMNRE.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1021.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1021 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1021 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-1021/>Joint Representation Learning of Cross-lingual Words and Entities via Attentive Distant Supervision</a></strong><br><a href=/people/y/yixin-cao/>Yixin Cao</a>
|
<a href=/people/l/lei-hou/>Lei Hou</a>
|
<a href=/people/j/juanzi-li/>Juanzi Li</a>
|
<a href=/people/z/zhiyuan-liu/>Zhiyuan Liu</a>
|
<a href=/people/c/chengjiang-li/>Chengjiang Li</a>
|
<a href=/people/x/xu-chen/>Xu Chen</a>
|
<a href=/people/t/tiansi-dong/>Tiansi Dong</a><br><a href=/volumes/D18-1/ class=text-muted>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1021><div class="card-body p-3 small">Jointly representation learning of words and entities benefits many NLP tasks, but has not been well explored in cross-lingual settings. In this paper, we propose a novel method for joint representation learning of cross-lingual words and entities. It captures mutually complementary knowledge, and enables cross-lingual inferences among <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge bases</a> and texts. Our method does not require <a href=https://en.wikipedia.org/wiki/Parallel_corpus>parallel corpus</a>, and automatically generates comparable data via distant supervision using <a href=https://en.wikipedia.org/wiki/Multilingualism>multi-lingual knowledge bases</a>. We utilize two types of regularizers to align cross-lingual words and entities, and design knowledge attention and cross-lingual attention to further reduce noises. We conducted a series of experiments on three tasks : <a href=https://en.wikipedia.org/wiki/Translation>word translation</a>, <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entity relatedness</a>, and cross-lingual entity linking. The results, both qualitative and quantitative, demonstrate the significance of our <a href=https://en.wikipedia.org/wiki/Methodology>method</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1033.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1033 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1033 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1033" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D18-1033/>Cross-lingual Lexical Sememe Prediction</a></strong><br><a href=/people/f/fanchao-qi/>Fanchao Qi</a>
|
<a href=/people/y/yankai-lin/>Yankai Lin</a>
|
<a href=/people/m/maosong-sun/>Maosong Sun</a>
|
<a href=/people/h/hao-zhu/>Hao Zhu</a>
|
<a href=/people/r/ruobing-xie/>Ruobing Xie</a>
|
<a href=/people/z/zhiyuan-liu/>Zhiyuan Liu</a><br><a href=/volumes/D18-1/ class=text-muted>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1033><div class="card-body p-3 small">Sememes are defined as the minimum semantic units of human languages. As important knowledge sources, sememe-based linguistic knowledge bases have been widely used in many <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP tasks</a>. However, most languages still do not have sememe-based linguistic knowledge bases. Thus we present a task of cross-lingual lexical sememe prediction, aiming to automatically predict <a href=https://en.wikipedia.org/wiki/Sememe>sememes</a> for words in other languages. We propose a novel framework to model correlations between <a href=https://en.wikipedia.org/wiki/Sememe>sememes</a> and multi-lingual words in low-dimensional semantic space for sememe prediction. Experimental results on real-world datasets show that our proposed model achieves consistent and significant improvements as compared to baseline methods in cross-lingual sememe prediction. The codes and data of this paper are available at.<url>https://github.com/thunlp/CL-SP</url>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1121.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1121 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1121 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1121" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D18-1121/>Put It Back : Entity Typing with Language Model Enhancement</a></strong><br><a href=/people/j/ji-xin/>Ji Xin</a>
|
<a href=/people/h/hao-zhu/>Hao Zhu</a>
|
<a href=/people/x/xu-han/>Xu Han</a>
|
<a href=/people/z/zhiyuan-liu/>Zhiyuan Liu</a>
|
<a href=/people/m/maosong-sun/>Maosong Sun</a><br><a href=/volumes/D18-1/ class=text-muted>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1121><div class="card-body p-3 small">Entity typing aims to classify semantic types of an entity mention in a specific context. Most existing <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> obtain training data using distant supervision, and inevitably suffer from the problem of noisy labels. To address this issue, we propose <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entity typing</a> with language model enhancement. It utilizes a <a href=https://en.wikipedia.org/wiki/Language_model>language model</a> to measure the compatibility between context sentences and labels, and thereby automatically focuses more on context-dependent labels. Experiments on benchmark datasets demonstrate that our method is capable of enhancing the entity typing model with information from the language model, and significantly outperforms the state-of-the-art baseline. Code and data for this paper can be found from.<url>https://github.com/thunlp/LME</url>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1390.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1390 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1390 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1390" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D18-1390/>Legal Judgment Prediction via Topological Learning</a></strong><br><a href=/people/h/haoxi-zhong/>Haoxi Zhong</a>
|
<a href=/people/z/zhipeng-guo/>Zhipeng Guo</a>
|
<a href=/people/c/cunchao-tu/>Cunchao Tu</a>
|
<a href=/people/c/chaojun-xiao/>Chaojun Xiao</a>
|
<a href=/people/z/zhiyuan-liu/>Zhiyuan Liu</a>
|
<a href=/people/m/maosong-sun/>Maosong Sun</a><br><a href=/volumes/D18-1/ class=text-muted>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1390><div class="card-body p-3 small">Legal Judgment Prediction (LJP) aims to predict the judgment result based on the facts of a case and becomes a promising application of artificial intelligence techniques in the <a href=https://en.wikipedia.org/wiki/Law>legal field</a>. In real-world scenarios, <a href=https://en.wikipedia.org/wiki/Legal_judgment>legal judgment</a> usually consists of multiple subtasks, such as the decisions of applicable law articles, <a href=https://en.wikipedia.org/wiki/Criminal_charge>charges</a>, <a href=https://en.wikipedia.org/wiki/Fine_(penalty)>fines</a>, and the term of penalty. Moreover, there exist topological dependencies among these subtasks. While most existing works only focus on a specific subtask of judgment prediction and ignore the dependencies among subtasks, we formalize the dependencies among subtasks as a Directed Acyclic Graph (DAG) and propose a topological multi-task learning framework, TopJudge, which incorporates multiple subtasks and DAG dependencies into judgment prediction. We conduct experiments on several real-world large-scale datasets of criminal cases in the <a href=https://en.wikipedia.org/wiki/Civil_law_(legal_system)>civil law system</a>. Experimental results show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieves consistent and significant improvements over baselines on all judgment prediction tasks. The source code can be obtained from.<url>https://github.com/thunlp/TopJudge</url>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-2024.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-2024 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-2024 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-2024" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D18-2024/>OpenKE : An Open Toolkit for Knowledge Embedding<span class=acl-fixed-case>O</span>pen<span class=acl-fixed-case>KE</span>: An Open Toolkit for Knowledge Embedding</a></strong><br><a href=/people/x/xu-han/>Xu Han</a>
|
<a href=/people/s/shulin-cao/>Shulin Cao</a>
|
<a href=/people/x/xin-lv/>Xin Lv</a>
|
<a href=/people/y/yankai-lin/>Yankai Lin</a>
|
<a href=/people/z/zhiyuan-liu/>Zhiyuan Liu</a>
|
<a href=/people/m/maosong-sun/>Maosong Sun</a>
|
<a href=/people/j/juanzi-li/>Juanzi Li</a><br><a href=/volumes/D18-2/ class=text-muted>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-2024><div class="card-body p-3 small">We release an open toolkit for knowledge embedding (OpenKE), which provides a unified framework and various fundamental models to embed knowledge graphs into a continuous low-dimensional space. OpenKE prioritizes <a href=https://en.wikipedia.org/wiki/Operational_efficiency>operational efficiency</a> to support quick model validation and large-scale knowledge representation learning. Meanwhile, OpenKE maintains sufficient modularity and extensibility to easily incorporate new <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> into the <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a>. Besides the toolkit, the embeddings of some existing large-scale knowledge graphs pre-trained by OpenKE are also available, which can be directly applied for many applications including <a href=https://en.wikipedia.org/wiki/Information_retrieval>information retrieval</a>, personalized recommendation and <a href=https://en.wikipedia.org/wiki/Question_answering>question answering</a>. The <a href=https://en.wikipedia.org/wiki/List_of_toolkits>toolkit</a>, documentation, and pre-trained embeddings are all released on.<url>http://openke.thunlp.org/</url>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1223.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1223 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1223 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1223.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-1223" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P18-1223/>Entity-Duet Neural Ranking : Understanding the Role of Knowledge Graph Semantics in Neural Information Retrieval</a></strong><br><a href=/people/z/zhenghao-liu/>Zhenghao Liu</a>
|
<a href=/people/c/chenyan-xiong/>Chenyan Xiong</a>
|
<a href=/people/m/maosong-sun/>Maosong Sun</a>
|
<a href=/people/z/zhiyuan-liu/>Zhiyuan Liu</a><br><a href=/volumes/P18-1/ class=text-muted>Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1223><div class="card-body p-3 small">This paper presents the Entity-Duet Neural Ranking Model (EDRM), which introduces <a href=https://en.wikipedia.org/wiki/Knowledge_graph>knowledge graphs</a> to neural search systems. EDRM represents queries and documents by their words and entity annotations. The <a href=https://en.wikipedia.org/wiki/Semantics>semantics</a> from <a href=https://en.wikipedia.org/wiki/Knowledge_graph>knowledge graphs</a> are integrated in the distributed representations of their entities, while the <a href=https://en.wikipedia.org/wiki/Ranking>ranking</a> is conducted by interaction-based neural ranking networks. The two <a href=https://en.wikipedia.org/wiki/Component-based_software_engineering>components</a> are learned end-to-end, making EDRM a natural combination of entity-oriented search and neural information retrieval. Our experiments on a commercial search log demonstrate the effectiveness of <a href=https://en.wikipedia.org/wiki/EDRM>EDRM</a>. Our analyses reveal that knowledge graph semantics significantly improve the <a href=https://en.wikipedia.org/wiki/Generalization>generalization ability</a> of neural ranking models.</div></div><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1004.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1004 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1004 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234953158 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-1004/>Neural Relation Extraction with Multi-lingual Attention</a></strong><br><a href=/people/y/yankai-lin/>Yankai Lin</a>
|
<a href=/people/z/zhiyuan-liu/>Zhiyuan Liu</a>
|
<a href=/people/m/maosong-sun/>Maosong Sun</a><br><a href=/volumes/P17-1/ class=text-muted>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1004><div class="card-body p-3 small">Relation extraction has been widely used for finding unknown relational facts from <a href=https://en.wikipedia.org/wiki/Plain_text>plain text</a>. Most existing methods focus on exploiting mono-lingual data for <a href=https://en.wikipedia.org/wiki/Relation_extraction>relation extraction</a>, ignoring massive information from the texts in various languages. To address this issue, we introduce a multi-lingual neural relation extraction framework, which employs mono-lingual attention to utilize the information within mono-lingual texts and further proposes cross-lingual attention to consider the information consistency and complementarity among cross-lingual texts. Experimental results on real-world datasets show that, our model can take advantage of multi-lingual texts and consistently achieve significant improvements on relation extraction as compared with baselines.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1158.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1158 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1158 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P17-1158.Software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P17-1158.Datasets.zip data-toggle=tooltip data-placement=top title=Dataset><i class="fas fa-file-archive"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P17-1158" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P17-1158/>CANE : Context-Aware Network Embedding for Relation Modeling<span class=acl-fixed-case>CANE</span>: Context-Aware Network Embedding for Relation Modeling</a></strong><br><a href=/people/c/cunchao-tu/>Cunchao Tu</a>
|
<a href=/people/h/han-liu/>Han Liu</a>
|
<a href=/people/z/zhiyuan-liu/>Zhiyuan Liu</a>
|
<a href=/people/m/maosong-sun/>Maosong Sun</a><br><a href=/volumes/P17-1/ class=text-muted>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1158><div class="card-body p-3 small">Network embedding (NE) is playing a critical role in <a href=https://en.wikipedia.org/wiki/Network_analysis_(electrical_circuits)>network analysis</a>, due to its ability to represent <a href=https://en.wikipedia.org/wiki/Vertex_(graph_theory)>vertices</a> with efficient low-dimensional embedding vectors. However, existing NE models aim to learn a fixed context-free embedding for each vertex and neglect the diverse roles when interacting with other vertices. In this paper, we assume that one vertex usually shows different aspects when interacting with different neighbor vertices, and should own different <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a> respectively. Therefore, we present Context-Aware Network Embedding (CANE), a novel NE model to address this issue. CANE learns context-aware embeddings for <a href=https://en.wikipedia.org/wiki/Vertex_(graph_theory)>vertices</a> with mutual attention mechanism and is expected to model the semantic relationships between vertices more precisely. In experiments, we compare our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> with existing NE models on three real-world datasets. Experimental results show that CANE achieves significant improvement than state-of-the-art methods on link prediction and comparable performance on vertex classification. The source code and datasets can be obtained from.<url>https://github.com/thunlp/CANE</url>.\n</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1187.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1187 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1187 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P17-1187.Software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P17-1187.Datasets.zip data-toggle=tooltip data-placement=top title=Dataset><i class="fas fa-file-archive"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P17-1187" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P17-1187/>Improved Word Representation Learning with <a href=https://en.wikipedia.org/wiki/Sememe>Sememes</a></a></strong><br><a href=/people/y/yilin-niu/>Yilin Niu</a>
|
<a href=/people/r/ruobing-xie/>Ruobing Xie</a>
|
<a href=/people/z/zhiyuan-liu/>Zhiyuan Liu</a>
|
<a href=/people/m/maosong-sun/>Maosong Sun</a><br><a href=/volumes/P17-1/ class=text-muted>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1187><div class="card-body p-3 small">Sememes are minimum semantic units of word meanings, and the meaning of each word sense is typically composed by several sememes. Since <a href=https://en.wikipedia.org/wiki/Sememe>sememes</a> are not explicit for each word, people manually annotate word sememes and form linguistic common-sense knowledge bases. In this paper, we present that, word sememe information can improve word representation learning (WRL), which maps words into a low-dimensional semantic space and serves as a fundamental step for many NLP tasks. The key idea is to utilize word sememes to capture exact meanings of a word within specific contexts accurately. More specifically, we follow the framework of <a href=https://en.wikipedia.org/wiki/Skip-gram>Skip-gram</a> and present three sememe-encoded models to learn representations of sememes, <a href=https://en.wikipedia.org/wiki/Word_sense>senses</a> and words, where we apply the attention scheme to detect word senses in various contexts. We conduct experiments on two tasks including word similarity and word analogy, and our <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> significantly outperform baselines. The results indicate that WRL can benefit from <a href=https://en.wikipedia.org/wiki/Sememe>sememes</a> via the attention scheme, and also confirm our models being capable of correctly modeling sememe information.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1186.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1186 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1186 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D17-1186" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D17-1186/>Incorporating Relation Paths in Neural Relation Extraction</a></strong><br><a href=/people/w/wenyuan-zeng/>Wenyuan Zeng</a>
|
<a href=/people/y/yankai-lin/>Yankai Lin</a>
|
<a href=/people/z/zhiyuan-liu/>Zhiyuan Liu</a>
|
<a href=/people/m/maosong-sun/>Maosong Sun</a><br><a href=/volumes/D17-1/ class=text-muted>Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1186><div class="card-body p-3 small">Distantly supervised relation extraction has been widely used to find novel relational facts from <a href=https://en.wikipedia.org/wiki/Plain_text>plain text</a>. To predict the relation between a pair of two target entities, existing methods solely rely on those direct sentences containing both entities. In fact, there are also many sentences containing only one of the target entities, which also provide rich useful information but not yet employed by <a href=https://en.wikipedia.org/wiki/Relation_extraction>relation extraction</a>. To address this issue, we build inference chains between two target entities via intermediate entities, and propose a path-based neural relation extraction model to encode the relational semantics from both direct sentences and inference chains. Experimental results on real-world datasets show that, our model can make full use of those sentences containing only one target entity, and achieves significant and consistent improvements on relation extraction as compared with strong baselines. The source code of this paper can be obtained from.<url>https://github.com/thunlp/PathNRE</url>.\n</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Zhiyuan+Liu" title="Search for 'Zhiyuan Liu' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/m/maosong-sun/ class=align-middle>Maosong Sun (孙茂松)</a>
<span class="badge badge-secondary align-middle ml-2">21</span></li><li class=list-group-item><a href=/people/j/juanzi-li/ class=align-middle>Juanzi Li</a>
<span class="badge badge-secondary align-middle ml-2">12</span></li><li class=list-group-item><a href=/people/x/xu-han/ class=align-middle>Xu Han</a>
<span class="badge badge-secondary align-middle ml-2">8</span></li><li class=list-group-item><a href=/people/y/yankai-lin/ class=align-middle>Yankai Lin</a>
<span class="badge badge-secondary align-middle ml-2">8</span></li><li class=list-group-item><a href=/people/y/yixin-cao/ class=align-middle>Yixin Cao</a>
<span class="badge badge-secondary align-middle ml-2">7</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/l/lei-hou/ class=align-middle>Lei Hou</a>
<span class="badge badge-secondary align-middle ml-2">6</span></li><li class=list-group-item><a href=/people/f/fanchao-qi/ class=align-middle>Fanchao Qi</a>
<span class="badge badge-secondary align-middle ml-2">6</span></li><li class=list-group-item><a href=/people/t/tat-seng-chua/ class=align-middle>Tat-Seng Chua</a>
<span class="badge badge-secondary align-middle ml-2">5</span></li><li class=list-group-item><a href=/people/c/cunchao-tu/ class=align-middle>Cunchao Tu</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/people/x/xiaozhi-wang/ class=align-middle>Xiaozhi Wang</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/people/q/qun-liu/ class=align-middle>Qun Liu</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/people/x/xin-lv/ class=align-middle>Xin Lv</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/c/chengjiang-li/ class=align-middle>Chengjiang Li</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/p/peng-li/ class=align-middle>Peng Li</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/j/jie-zhou/ class=align-middle>Jie Zhou</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/z/zikun-hu/ class=align-middle>Zikun Hu</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/y/yuan-zang/ class=align-middle>Yuan Zang</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/l/liangming-pan/ class=align-middle>Liangming Pan</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/y/yuxian-gu/ class=align-middle>Yuxian Gu</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/z/zhengyan-zhang/ class=align-middle>Zhengyan Zhang</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/c/chenghao-yang/ class=align-middle>Chenghao Yang</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/r/ruobing-xie/ class=align-middle>Ruobing Xie</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/x/xin-jiang/ class=align-middle>Xin Jiang</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/y/yujia-qin/ class=align-middle>Yujia Qin</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/x/xiao-chen/ class=align-middle>Xiao Chen</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/s/shulin-cao/ class=align-middle>Shulin Cao</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/j/jifan-yu/ class=align-middle>Jifan Yu</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/m/minlie-huang/ class=align-middle>Minlie Huang</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/h/hao-zhu/ class=align-middle>Hao Zhu</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/c/chaojun-xiao/ class=align-middle>Chaojun Xiao</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/x/xiang-li/ class=align-middle>Xiang Li</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/g/guoyang-zeng/ class=align-middle>Guoyang Zeng</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/q/qianrui-zhou/ class=align-middle>Qianrui Zhou</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/tingji-zhang/ class=align-middle>Tingji Zhang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/zixian-ma/ class=align-middle>Zixian Ma</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/b/bairu-hou/ class=align-middle>Bairu Hou</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/w/wei-zhang/ class=align-middle>Wei Zhang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yichi-zhang/ class=align-middle>Yichi Zhang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/hao-kong/ class=align-middle>Hao Kong</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/suhui-wu/ class=align-middle>Suhui Wu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/zihao-fu/ class=align-middle>Zihao Fu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/b/bei-shi/ class=align-middle>Bei Shi</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/w/wai-lam/ class=align-middle>Wai Lam</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/lidong-bing/ class=align-middle>Lidong Bing</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/meng-zhang/ class=align-middle>Meng Zhang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/han-liu/ class=align-middle>Han Liu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yilin-niu/ class=align-middle>Yilin Niu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/cheng-chen/ class=align-middle>Cheng Chen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yichun-yin/ class=align-middle>Yichun Yin</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/lifeng-shang/ class=align-middle>Lifeng Shang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/f/fengyu-wang/ class=align-middle>Fengyu Wang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/zhi-wang/ class=align-middle>Zhi Wang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/b/biru-zhu/ class=align-middle>Biru Zhu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yangdong-deng/ class=align-middle>Yangdong Deng</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/ming-gu/ class=align-middle>Ming Gu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/g/ganqu-cui/ class=align-middle>Ganqu Cui</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/shengding-hu/ class=align-middle>Shengding Hu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/n/ning-ding/ class=align-middle>Ning Ding</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/longtao-huang/ class=align-middle>Longtao Huang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jiaxin-shi/ class=align-middle>Jiaxin Shi</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/zijun-yao/ class=align-middle>Zijun Yao</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jinghui-xiao/ class=align-middle>Jinghui Xiao</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/chenyu-wang/ class=align-middle>Chenyu Wang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/g/gan-luo/ class=align-middle>Gan Luo</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jie-tang/ class=align-middle>Jie Tang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/x/xu-chen/ class=align-middle>Xu Chen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/tiansi-dong/ class=align-middle>Tiansi Dong</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/ji-xin/ class=align-middle>Ji Xin</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/haoxi-zhong/ class=align-middle>Haoxi Zhong</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/zhipeng-guo/ class=align-middle>Zhipeng Guo</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yuan-yao/ class=align-middle>Yuan Yao</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jiaju-du/ class=align-middle>Jiaju Du</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yangyi-chen/ class=align-middle>Yangyi Chen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/x/xurui-zhang/ class=align-middle>Xurui Zhang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/mukai-li/ class=align-middle>Mukai Li</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/heng-ji/ class=align-middle>Heng Ji</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/q/qiu-ran/ class=align-middle>Qiu Ran</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/ziqi-wang/ class=align-middle>Ziqi Wang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/x/xiang-ren/ class=align-middle>Xiang Ren</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/f/feng-yao/ class=align-middle>Feng Yao</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yun-liu/ class=align-middle>Yun Liu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/w/weixing-shen/ class=align-middle>Weixing Shen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/w/wenyuan-zeng/ class=align-middle>Wenyuan Zeng</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/zhenghao-liu/ class=align-middle>Zhenghao Liu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/chenyan-xiong/ class=align-middle>Chenyan Xiong</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/shun-zheng/ class=align-middle>Shun Zheng</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/p/peilin-yu/ class=align-middle>Peilin Yu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/lu-chen/ class=align-middle>Lu Chen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/ling-huang/ class=align-middle>Ling Huang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/w/wei-xu/ class=align-middle>Wei Xu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/junjie-huang/ class=align-middle>Junjie Huang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/acl/ class=align-middle>ACL</a><span class="badge badge-secondary align-middle ml-2">16</span></li><li class=list-group-item><a href=/venues/emnlp/ class=align-middle>EMNLP</a><span class="badge badge-secondary align-middle ml-2">16</span></li><li class=list-group-item><a href=/venues/coling/ class=align-middle>COLING</a><span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/venues/aacl/ class=align-middle>AACL</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/findings/ class=align-middle>Findings</a><span class="badge badge-secondary align-middle ml-2">1</span></li></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>