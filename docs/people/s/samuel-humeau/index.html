<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Samuel Humeau - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Samuel</span> <span class=font-weight-bold>Humeau</span></h2><hr><div class=row><div class=col-lg-9><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.acl-main.219.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--acl-main--219 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.acl-main.219 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://slideslive.com/38928905 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.acl-main.219/>Image-Chat : Engaging Grounded Conversations</a></strong><br><a href=/people/k/kurt-shuster/>Kurt Shuster</a>
|
<a href=/people/s/samuel-humeau/>Samuel Humeau</a>
|
<a href=/people/a/antoine-bordes/>Antoine Bordes</a>
|
<a href=/people/j/jason-weston/>Jason Weston</a><br><a href=/volumes/2020.acl-main/ class=text-muted>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--acl-main--219><div class="card-body p-3 small">To achieve the long-term goal of machines being able to engage humans in conversation, our <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> should captivate the interest of their speaking partners. Communication grounded in images, whereby a <a href=https://en.wikipedia.org/wiki/Dialogue>dialogue</a> is conducted based on a given photo, is a setup naturally appealing to humans (Hu et al., 2014). In this work we study <a href=https://en.wikipedia.org/wiki/Computer_architecture>large-scale architectures</a> and <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> for this goal. We test a set of neural architectures using state-of-the-art image and text representations, considering various ways to fuse the <a href=https://en.wikipedia.org/wiki/Component-based_software_engineering>components</a>. To test such models, we collect a dataset of grounded human-human conversations, where speakers are asked to play roles given a provided emotional mood or style, as the use of such traits is also a key factor in engagingness (Guo et al., 2019). Our dataset, Image-Chat, consists of 202k dialogues over 202k images using 215 possible style traits. Automatic metrics and human evaluations of engagingness show the efficacy of our approach ; in particular, we obtain state-of-the-art performance on the existing IGC task, and our best performing model is almost on par with humans on the Image-Chat test set (preferred 47.7 % of the time).</div></div><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1062.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1062 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1062 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1062.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D19-1062/>Learning to Speak and Act in a Fantasy Text Adventure Game</a></strong><br><a href=/people/j/jack-urbanek/>Jack Urbanek</a>
|
<a href=/people/a/angela-fan/>Angela Fan</a>
|
<a href=/people/s/siddharth-karamcheti/>Siddharth Karamcheti</a>
|
<a href=/people/s/saachi-jain/>Saachi Jain</a>
|
<a href=/people/s/samuel-humeau/>Samuel Humeau</a>
|
<a href=/people/e/emily-dinan/>Emily Dinan</a>
|
<a href=/people/t/tim-rocktaschel/>Tim Rockt√§schel</a>
|
<a href=/people/d/douwe-kiela/>Douwe Kiela</a>
|
<a href=/people/a/arthur-szlam/>Arthur Szlam</a>
|
<a href=/people/j/jason-weston/>Jason Weston</a><br><a href=/volumes/D19-1/ class=text-muted>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1062><div class="card-body p-3 small">We introduce a large-scale crowdsourced text adventure game as a research platform for studying grounded dialogue. In it, agents can perceive, emote, and act whilst conducting dialogue with other agents. Models and humans can both act as characters within the <a href=https://en.wikipedia.org/wiki/Game>game</a>. We describe the results of training state-of-the-art generative and retrieval models in this setting. We show that in addition to using past dialogue, these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> are able to effectively use the state of the underlying world to condition their predictions. In particular, we show that grounding on the details of the local environment, including location descriptions, and the objects (and their affordances) and characters (and their previous actions) present within it allows better predictions of agent behavior and <a href=https://en.wikipedia.org/wiki/Dialogue>dialogue</a>. We analyze the ingredients necessary for successful grounding in this <a href=https://en.wikipedia.org/wiki/Setting_(narrative)>setting</a>, and how each of these factors relate to agents that can talk and act successfully.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1461.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1461 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1461 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1461.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D19-1461/>Build it Break it Fix it for Dialogue Safety : Robustness from Adversarial Human Attack</a></strong><br><a href=/people/e/emily-dinan/>Emily Dinan</a>
|
<a href=/people/s/samuel-humeau/>Samuel Humeau</a>
|
<a href=/people/b/bharath-chintagunta/>Bharath Chintagunta</a>
|
<a href=/people/j/jason-weston/>Jason Weston</a><br><a href=/volumes/D19-1/ class=text-muted>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1461><div class="card-body p-3 small">The detection of offensive language in the context of a <a href=https://en.wikipedia.org/wiki/Dialogue>dialogue</a> has become an increasingly important application of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a>. The detection of trolls in public forums (Galn-Garca et al., 2016), and the deployment of <a href=https://en.wikipedia.org/wiki/Chatbot>chatbots</a> in the <a href=https://en.wikipedia.org/wiki/Public_domain>public domain</a> (Wolf et al., 2017) are two examples that show the necessity of guarding against adversarially offensive behavior on the part of humans. In this work, we develop a training scheme for a <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> to become robust to such human attacks by an iterative build it, break it, fix it scheme with humans and models in the loop. In detailed experiments we show this <a href=https://en.wikipedia.org/wiki/Scientific_method>approach</a> is considerably more robust than previous <a href=https://en.wikipedia.org/wiki/System>systems</a>. Further, we show that offensive language used within a conversation critically depends on the dialogue context, and can not be viewed as a single sentence offensive detection task as in most previous work. Our newly collected <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> and methods are all made open source and publicly available.</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1298.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1298 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1298 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/305943582 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1298/>Training Millions of Personalized Dialogue Agents</a></strong><br><a href=/people/p/pierre-emmanuel-mazare/>Pierre-Emmanuel Mazar√©</a>
|
<a href=/people/s/samuel-humeau/>Samuel Humeau</a>
|
<a href=/people/m/martin-raison/>Martin Raison</a>
|
<a href=/people/a/antoine-bordes/>Antoine Bordes</a><br><a href=/volumes/D18-1/ class=text-muted>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1298><div class="card-body p-3 small">Current dialogue systems fail at being engaging for users, especially when trained end-to-end without relying on proactive reengaging scripted strategies. Zhang et al. (2018) showed that the engagement level of end-to-end dialogue models increases when conditioning them on text personas providing some personalized back-story to the model. However, the <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> used in Zhang et al. (2018) is synthetic and only contains around 1k different <a href=https://en.wikipedia.org/wiki/Persona>personas</a>. In this paper we introduce a new <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> providing 5 million <a href=https://en.wikipedia.org/wiki/Persona>personas</a> and 700 million persona-based dialogues. Our experiments show that, at this scale, training using <a href=https://en.wikipedia.org/wiki/Persona>personas</a> still improves the performance of <a href=https://en.wikipedia.org/wiki/End-to-end_principle>end-to-end systems</a>. In addition, we show that other tasks benefit from the wide coverage of our <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> by fine-tuning our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> on the data from Zhang et al. (2018) and achieving state-of-the-art results.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Samuel+Humeau" title="Search for 'Samuel Humeau' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/j/jason-weston/ class=align-middle>Jason Weston</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/a/antoine-bordes/ class=align-middle>Antoine Bordes</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/e/emily-dinan/ class=align-middle>Emily Dinan</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/k/kurt-shuster/ class=align-middle>Kurt Shuster</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/p/pierre-emmanuel-mazare/ class=align-middle>Pierre-Emmanuel Mazare</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/m/martin-raison/ class=align-middle>Martin Raison</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jack-urbanek/ class=align-middle>Jack Urbanek</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/angela-fan/ class=align-middle>Angela Fan</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/siddharth-karamcheti/ class=align-middle>Siddharth Karamcheti</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/saachi-jain/ class=align-middle>Saachi Jain</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/tim-rocktaschel/ class=align-middle>Tim Rockt√§schel</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/douwe-kiela/ class=align-middle>Douwe Kiela</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/arthur-szlam/ class=align-middle>Arthur Szlam</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/b/bharath-chintagunta/ class=align-middle>Bharath Chintagunta</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/emnlp/ class=align-middle>EMNLP</a><span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/venues/acl/ class=align-middle>ACL</a><span class="badge badge-secondary align-middle ml-2">1</span></li></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ¬©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>