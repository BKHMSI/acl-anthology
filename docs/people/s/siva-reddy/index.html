<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Siva Reddy - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Siva</span> <span class=font-weight-bold>Reddy</span></h2><hr><div class=row><div class=col-lg-9><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.516.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--516 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.516 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.516/>Mind the Context : The Impact of <a href=https://en.wikipedia.org/wiki/Contextualization>Contextualization</a> in Neural Module Networks for Grounding Visual Referring Expressions</a></strong><br><a href=/people/a/arjun-akula/>Arjun Akula</a>
|
<a href=/people/s/spandana-gella/>Spandana Gella</a>
|
<a href=/people/k/keze-wang/>Keze Wang</a>
|
<a href=/people/s/song-chun-zhu/>Song-Chun Zhu</a>
|
<a href=/people/s/siva-reddy/>Siva Reddy</a><br><a href=/volumes/2021.emnlp-main/ class=text-muted>Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--516><div class="card-body p-3 small">Neural module networks (NMN) are a popular approach for grounding visual referring expressions. Prior implementations of NMN use pre-defined and fixed textual inputs in their module instantiation. This necessitates a large number of <a href=https://en.wikipedia.org/wiki/Modular_programming>modules</a> as they lack the ability to share weights and exploit associations between similar textual contexts (e.g. dark cube on the left vs. black cube on the left). In this work, we address these limitations and evaluate the impact of contextual clues in improving the performance of NMN models. First, we address the problem of fixed textual inputs by parameterizing the module arguments. This substantially reduce the number of <a href=https://en.wikipedia.org/wiki/Modular_programming>modules</a> in NMN by up to 75 % without any loss in performance. Next we propose a method to contextualize our parameterized model to enhance the module&#8217;s capacity in exploiting the visiolinguistic associations. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms the state-of-the-art NMN model on CLEVR-Ref+ dataset with +8.1 % improvement in <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> on the single-referent test set and +4.3 % on the full test set. Additionally, we demonstrate that contextualization provides +11.2 % and +1.7 % improvements in <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> over prior NMN models on CLOSURE and NLVR2. We further evaluate the impact of our <a href=https://en.wikipedia.org/wiki/Contextualization>contextualization</a> by constructing a <a href=https://en.wikipedia.org/wiki/Contrast_(vision)>contrast set</a> for CLEVR-Ref+, which we call CC-Ref+. We significantly outperform the <a href=https://en.wikipedia.org/wiki/Baseline_(medicine)>baselines</a> by as much as +10.4 % absolute <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> on CC-Ref+, illustrating the generalization skills of our approach.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.dialdoc-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.dialdoc-1.0/>Proceedings of the 1st Workshop on Document-grounded Dialogue and Conversational Question Answering (DialDoc 2021)</a></strong><br><a href=/people/s/song-feng/>Song Feng</a>
|
<a href=/people/s/siva-reddy/>Siva Reddy</a>
|
<a href=/people/m/malihe-alikhani/>Malihe Alikhani</a>
|
<a href=/people/h/he-he/>He He</a>
|
<a href=/people/y/yangfeng-ji/>Yangfeng Ji</a>
|
<a href=/people/m/mohit-iyyer/>Mohit Iyyer</a>
|
<a href=/people/z/zhou-yu/>Zhou Yu</a><br><a href=/volumes/2021.dialdoc-1/ class=text-muted>Proceedings of the 1st Workshop on Document-grounded Dialogue and Conversational Question Answering (DialDoc 2021)</a></span></p><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.acl-main.586.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--acl-main--586 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.acl-main.586 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://slideslive.com/38929224 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.acl-main.586" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.acl-main.586/>Words Are n’t Enough, Their Order Matters : On the Robustness of Grounding Visual Referring Expressions</a></strong><br><a href=/people/a/arjun-akula/>Arjun Akula</a>
|
<a href=/people/s/spandana-gella/>Spandana Gella</a>
|
<a href=/people/y/yaser-al-onaizan/>Yaser Al-Onaizan</a>
|
<a href=/people/s/song-chun-zhu/>Song-Chun Zhu</a>
|
<a href=/people/s/siva-reddy/>Siva Reddy</a><br><a href=/volumes/2020.acl-main/ class=text-muted>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--acl-main--586><div class="card-body p-3 small">Visual referring expression recognition is a challenging task that requires <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language understanding</a> in the context of an <a href=https://en.wikipedia.org/wiki/Image>image</a>. We critically examine RefCOCOg, a standard <a href=https://en.wikipedia.org/wiki/Benchmarking>benchmark</a> for this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>, using a human study and show that 83.7 % of test instances do not require reasoning on linguistic structure, i.e., words are enough to identify the target object, the <a href=https://en.wikipedia.org/wiki/Word_order>word order</a> does n&#8217;t matter. To measure the true progress of existing <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a>, we split the test set into two sets, one which requires reasoning on linguistic structure and the other which does n&#8217;t. Additionally, we create an out-of-distribution dataset Ref-Adv by asking crowdworkers to perturb in-domain examples such that the target object changes. Using these datasets, we empirically show that existing methods fail to exploit linguistic structure and are 12 % to 23 % lower in performance than the established progress for this task. We also propose two methods, one based on contrastive learning and the other based on <a href=https://en.wikipedia.org/wiki/Multi-task_learning>multi-task learning</a>, to increase the <a href=https://en.wikipedia.org/wiki/Robustness_(computer_science)>robustness</a> of ViLBERT, the current state-of-the-art model for this task. Our datasets are publicly available at https://github.com/aws/aws-refcocog-adv.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.clinicalnlp-1.15.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--clinicalnlp-1--15 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.clinicalnlp-1.15 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.clinicalnlp-1.15.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939819 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.clinicalnlp-1.15" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.clinicalnlp-1.15/>MeDAL : Medical Abbreviation Disambiguation Dataset for Natural Language Understanding Pretraining<span class=acl-fixed-case>M</span>e<span class=acl-fixed-case>DAL</span>: Medical Abbreviation Disambiguation Dataset for Natural Language Understanding Pretraining</a></strong><br><a href=/people/z/zhi-wen/>Zhi Wen</a>
|
<a href=/people/x/xing-han-lu/>Xing Han Lu</a>
|
<a href=/people/s/siva-reddy/>Siva Reddy</a><br><a href=/volumes/2020.clinicalnlp-1/ class=text-muted>Proceedings of the 3rd Clinical Natural Language Processing Workshop</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--clinicalnlp-1--15><div class="card-body p-3 small">One of the biggest challenges that prohibit the use of many current <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP methods</a> in clinical settings is the availability of <a href=https://en.wikipedia.org/wiki/Data_set>public datasets</a>. In this work, we present MeDAL, a large medical text dataset curated for abbreviation disambiguation, designed for <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language understanding</a> pre-training in the medical domain. We pre-trained several models of common architectures on this dataset and empirically showed that such pre-training leads to improved performance and <a href=https://en.wikipedia.org/wiki/Convergence_of_random_variables>convergence speed</a> when <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a> on downstream medical tasks.</div></div><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K17-3001.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K17-3001 data-toggle=collapse aria-expanded=false aria-controls=abstract-K17-3001 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/K17-3001/>CoNLL 2017 Shared Task : Multilingual Parsing from Raw Text to Universal Dependencies<span class=acl-fixed-case>C</span>o<span class=acl-fixed-case>NLL</span> 2017 Shared Task: Multilingual Parsing from Raw Text to <span class=acl-fixed-case>U</span>niversal <span class=acl-fixed-case>D</span>ependencies</a></strong><br><a href=/people/d/daniel-zeman/>Daniel Zeman</a>
|
<a href=/people/m/martin-popel/>Martin Popel</a>
|
<a href=/people/m/milan-straka/>Milan Straka</a>
|
<a href=/people/j/jan-hajic/>Jan Hajič</a>
|
<a href=/people/j/joakim-nivre/>Joakim Nivre</a>
|
<a href=/people/f/filip-ginter/>Filip Ginter</a>
|
<a href=/people/j/juhani-luotolahti/>Juhani Luotolahti</a>
|
<a href=/people/s/sampo-pyysalo/>Sampo Pyysalo</a>
|
<a href=/people/s/slav-petrov/>Slav Petrov</a>
|
<a href=/people/m/martin-potthast/>Martin Potthast</a>
|
<a href=/people/f/francis-tyers/>Francis Tyers</a>
|
<a href=/people/e/elena-badmaeva/>Elena Badmaeva</a>
|
<a href=/people/m/memduh-gokirmak/>Memduh Gokirmak</a>
|
<a href=/people/a/anna-nedoluzhko/>Anna Nedoluzhko</a>
|
<a href=/people/s/silvie-cinkova/>Silvie Cinková</a>
|
<a href=/people/j/jan-hajic-jr/>Jan Hajič jr.</a>
|
<a href=/people/j/jaroslava-hlavacova/>Jaroslava Hlaváčová</a>
|
<a href=/people/v/vaclava-kettnerova/>Václava Kettnerová</a>
|
<a href=/people/z/zdenka-uresova/>Zdeňka Urešová</a>
|
<a href=/people/j/jenna-kanerva/>Jenna Kanerva</a>
|
<a href=/people/s/stina-ojala/>Stina Ojala</a>
|
<a href=/people/a/anna-missila/>Anna Missilä</a>
|
<a href=/people/c/christopher-d-manning/>Christopher D. Manning</a>
|
<a href=/people/s/sebastian-schuster/>Sebastian Schuster</a>
|
<a href=/people/s/siva-reddy/>Siva Reddy</a>
|
<a href=/people/d/dima-taji/>Dima Taji</a>
|
<a href=/people/n/nizar-habash/>Nizar Habash</a>
|
<a href=/people/h/herman-leung/>Herman Leung</a>
|
<a href=/people/m/marie-catherine-de-marneffe/>Marie-Catherine de Marneffe</a>
|
<a href=/people/m/manuela-sanguinetti/>Manuela Sanguinetti</a>
|
<a href=/people/m/maria-simi/>Maria Simi</a>
|
<a href=/people/h/hiroshi-kanayama/>Hiroshi Kanayama</a>
|
<a href=/people/v/valeria-de-paiva/>Valeria de Paiva</a>
|
<a href=/people/k/kira-droganova/>Kira Droganova</a>
|
<a href=/people/h/hector-martinez-alonso/>Héctor Martínez Alonso</a>
|
<a href=/people/c/cagri-coltekin/>Çağrı Çöltekin</a>
|
<a href=/people/u/umut-sulubacak/>Umut Sulubacak</a>
|
<a href=/people/h/hans-uszkoreit/>Hans Uszkoreit</a>
|
<a href=/people/v/vivien-macketanz/>Vivien Macketanz</a>
|
<a href=/people/a/aljoscha-burchardt/>Aljoscha Burchardt</a>
|
<a href=/people/k/kim-harris/>Kim Harris</a>
|
<a href=/people/k/katrin-marheinecke/>Katrin Marheinecke</a>
|
<a href=/people/g/georg-rehm/>Georg Rehm</a>
|
<a href=/people/t/tolga-kayadelen/>Tolga Kayadelen</a>
|
<a href=/people/m/mohammed-attia/>Mohammed Attia</a>
|
<a href=/people/a/ali-elkahky/>Ali Elkahky</a>
|
<a href=/people/z/zhuoran-yu/>Zhuoran Yu</a>
|
<a href=/people/e/emily-pitler/>Emily Pitler</a>
|
<a href=/people/s/saran-lertpradit/>Saran Lertpradit</a>
|
<a href=/people/m/michael-mandel/>Michael Mandl</a>
|
<a href=/people/j/jesse-kirchner/>Jesse Kirchner</a>
|
<a href=/people/h/hector-fernandez-alcalde/>Hector Fernandez Alcalde</a>
|
<a href=/people/j/jana-strnadova/>Jana Strnadová</a>
|
<a href=/people/e/esha-banerjee/>Esha Banerjee</a>
|
<a href=/people/r/ruli-manurung/>Ruli Manurung</a>
|
<a href=/people/a/antonio-stella/>Antonio Stella</a>
|
<a href=/people/a/atsuko-shimada/>Atsuko Shimada</a>
|
<a href=/people/s/sookyoung-kwak/>Sookyoung Kwak</a>
|
<a href=/people/g/gustavo-mendonca/>Gustavo Mendonça</a>
|
<a href=/people/t/tatiana-lando/>Tatiana Lando</a>
|
<a href=/people/r/rattima-nitisaroj/>Rattima Nitisaroj</a>
|
<a href=/people/j/josie-li/>Josie Li</a><br><a href=/volumes/K17-3/ class=text-muted>Proceedings of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K17-3001><div class="card-body p-3 small">The Conference on Computational Natural Language Learning (CoNLL) features a shared task, in which participants train and test their <a href=https://en.wikipedia.org/wiki/Machine_learning>learning systems</a> on the same data sets. In 2017, the task was devoted to learning dependency parsers for a large number of languages, in a real-world setting without any gold-standard annotation on input. All <a href=https://en.wikipedia.org/wiki/Test_set>test sets</a> followed a unified annotation scheme, namely that of Universal Dependencies. In this paper, we define the task and evaluation methodology, describe how the data sets were prepared, report and analyze the main results, and provide a brief categorization of the different approaches of the participating systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1005.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1005 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1005 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234954083 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P17-1005" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-1005/>Learning Structured Natural Language Representations for Semantic Parsing</a></strong><br><a href=/people/j/jianpeng-cheng/>Jianpeng Cheng</a>
|
<a href=/people/s/siva-reddy/>Siva Reddy</a>
|
<a href=/people/v/vijay-saraswat/>Vijay Saraswat</a>
|
<a href=/people/m/mirella-lapata/>Mirella Lapata</a><br><a href=/volumes/P17-1/ class=text-muted>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1005><div class="card-body p-3 small">We introduce a neural semantic parser which is interpretable and scalable. Our model converts natural language utterances to intermediate, domain-general natural language representations in the form of predicate-argument structures, which are induced with a transition system and subsequently mapped to target domains. The <a href=https://en.wikipedia.org/wiki/Semantic_parser>semantic parser</a> is trained end-to-end using annotated logical forms or their denotations. We achieve the state of the art on SPADES and GRAPHQUESTIONS and obtain competitive results on GEOQUERY and WEBQUESTIONS. The induced predicate-argument structures shed light on the types of <a href=https://en.wikipedia.org/wiki/Representation_(mathematics)>representations</a> useful for <a href=https://en.wikipedia.org/wiki/Semantic_parsing>semantic parsing</a> and how these are different from linguistically motivated ones.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2057.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2057 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2057 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-2057/>Question Answering on Knowledge Bases and Text using Universal Schema and Memory Networks</a></strong><br><a href=/people/r/rajarshi-das/>Rajarshi Das</a>
|
<a href=/people/m/manzil-zaheer/>Manzil Zaheer</a>
|
<a href=/people/s/siva-reddy/>Siva Reddy</a>
|
<a href=/people/a/andrew-mccallum/>Andrew McCallum</a><br><a href=/volumes/P17-2/ class=text-muted>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2057><div class="card-body p-3 small">Existing <a href=https://en.wikipedia.org/wiki/Question_answering>question answering methods</a> infer answers either from a <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge base</a> or from <a href=https://en.wikipedia.org/wiki/Text_corpus>raw text</a>. While knowledge base (KB) methods are good at answering compositional questions, their performance is often affected by the incompleteness of the KB. Au contraire, <a href=https://en.wikipedia.org/wiki/Web_page>web text</a> contains millions of facts that are absent in the KB, however in an <a href=https://en.wikipedia.org/wiki/Unstructured_data>unstructured form</a>. Universal schema can support reasoning on the union of both structured KBs and <a href=https://en.wikipedia.org/wiki/Unstructured_data>unstructured text</a> by aligning them in a common embedded space. In this paper we extend universal schema to <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language question answering</a>, employing Memory networks to attend to the large body of facts in the combination of text and KB. Our <a href=https://en.wikipedia.org/wiki/Model_(person)>models</a> can be trained in an end-to-end fashion on question-answer pairs. Evaluation results on Spades fill-in-the-blank question answering dataset show that exploiting universal schema for <a href=https://en.wikipedia.org/wiki/Question_answering>question answering</a> is better than using either a KB or text alone. This <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> also outperforms the current <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a> by 8.5 F1 points.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1804.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1804 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1804 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1804/>Universal Dependencies to <a href=https://en.wikipedia.org/wiki/Logical_form>Logical Form</a> with Negation Scope<span class=acl-fixed-case>U</span>niversal <span class=acl-fixed-case>D</span>ependencies to Logical Form with Negation Scope</a></strong><br><a href=/people/f/federico-fancellu/>Federico Fancellu</a>
|
<a href=/people/s/siva-reddy/>Siva Reddy</a>
|
<a href=/people/a/adam-lopez/>Adam Lopez</a>
|
<a href=/people/b/bonnie-webber/>Bonnie Webber</a><br><a href=/volumes/W17-18/ class=text-muted>Proceedings of the Workshop Computational Semantics Beyond Events and Roles</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1804><div class="card-body p-3 small">Many language technology applications would benefit from the ability to represent <a href=https://en.wikipedia.org/wiki/Affirmation_and_negation>negation</a> and its scope on top of widely-used linguistic resources. In this paper, we investigate the possibility of obtaining a <a href=https://en.wikipedia.org/wiki/First-order_logic>first-order logic representation</a> with <a href=https://en.wikipedia.org/wiki/Scope_(computer_science)>negation scope</a> marked using Universal Dependencies. To do so, we enhance UDepLambda, a <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> that converts <a href=https://en.wikipedia.org/wiki/Dependency_graph>dependency graphs</a> to <a href=https://en.wikipedia.org/wiki/Logical_form>logical forms</a>. The resulting UDepLambda is able to handle phenomena related to scope by means of an higher-order type theory, relevant not only to <a href=https://en.wikipedia.org/wiki/Negation>negation</a> but also to <a href=https://en.wikipedia.org/wiki/Universal_quantification>universal quantification</a> and other complex semantic phenomena. The initial conversion we did for <a href=https://en.wikipedia.org/wiki/English_language>English</a> is promising, in that one can represent the scope of negation also in the presence of more complex phenomena such as <a href=https://en.wikipedia.org/wiki/Universal_quantification>universal quantifiers</a>.<i>Universal Dependencies</i>. To do so, we enhance <i>UDepLambda</i>, a framework that converts dependency graphs to logical forms. The resulting <i>UDepLambda<tex-math>\\lnot</tex-math>\n </i>\n\nis able to handle phenomena related to scope by means of an higher-order type theory, relevant not only to negation but also to universal quantification and other complex semantic phenomena. The initial conversion we did for English is promising, in that one can represent the scope of negation also in the presence of more complex phenomena such as universal quantifiers.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1009.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1009 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1009 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D17-1009.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/238236598 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D17-1009" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/D17-1009/>Universal Semantic Parsing</a></strong><br><a href=/people/s/siva-reddy/>Siva Reddy</a>
|
<a href=/people/o/oscar-tackstrom/>Oscar Täckström</a>
|
<a href=/people/s/slav-petrov/>Slav Petrov</a>
|
<a href=/people/m/mark-steedman/>Mark Steedman</a>
|
<a href=/people/m/mirella-lapata/>Mirella Lapata</a><br><a href=/volumes/D17-1/ class=text-muted>Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1009><div class="card-body p-3 small">Universal Dependencies (UD) offer a uniform cross-lingual syntactic representation, with the aim of advancing multilingual applications. Recent work shows that <a href=https://en.wikipedia.org/wiki/Semantic_parsing>semantic parsing</a> can be accomplished by transforming syntactic dependencies to logical forms. However, this work is limited to <a href=https://en.wikipedia.org/wiki/English_language>English</a>, and can not process <a href=https://en.wikipedia.org/wiki/Dependency_graph>dependency graphs</a>, which allow handling complex phenomena such as <a href=https://en.wikipedia.org/wiki/Control_flow>control</a>. In this work, we introduce UDepLambda, a semantic interface for UD, which maps <a href=https://en.wikipedia.org/wiki/Natural_language>natural language</a> to logical forms in an almost language-independent fashion and can process dependency graphs. We perform experiments on <a href=https://en.wikipedia.org/wiki/Question_answering>question answering</a> against <a href=https://en.wikipedia.org/wiki/Freebase>Freebase</a> and provide German and Spanish translations of the WebQuestions and GraphQuestions datasets to facilitate multilingual evaluation. Results show that UDepLambda outperforms strong baselines across languages and datasets. For <a href=https://en.wikipedia.org/wiki/English_language>English</a>, <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> achieves a 4.9 F1 point improvement over the <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a> on GraphQuestions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1091.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1091 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1091 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1091/>Learning to Paraphrase for Question Answering</a></strong><br><a href=/people/l/li-dong/>Li Dong</a>
|
<a href=/people/j/jonathan-mallinson/>Jonathan Mallinson</a>
|
<a href=/people/s/siva-reddy/>Siva Reddy</a>
|
<a href=/people/m/mirella-lapata/>Mirella Lapata</a><br><a href=/volumes/D17-1/ class=text-muted>Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1091><div class="card-body p-3 small">Question answering (QA) systems are sensitive to the many different ways <a href=https://en.wikipedia.org/wiki/Natural_language>natural language</a> expresses the same information need. In this paper we turn to <a href=https://en.wikipedia.org/wiki/Paraphrase>paraphrases</a> as a means of capturing this knowledge and present a general framework which learns felicitous paraphrases for various QA tasks. Our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> is trained end-to-end using question-answer pairs as a <a href=https://en.wikipedia.org/wiki/Signal_processing>supervision signal</a>. A question and its <a href=https://en.wikipedia.org/wiki/Paraphrase>paraphrases</a> serve as input to a neural scoring model which assigns higher weights to linguistic expressions most likely to yield correct answers. We evaluate our approach on <a href=https://en.wikipedia.org/wiki/Quality_assurance>QA</a> over <a href=https://en.wikipedia.org/wiki/Freebase>Freebase</a> and answer sentence selection. Experimental results on three <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> show that our <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> consistently improves performance, achieving competitive results despite the use of simple <a href=https://en.wikipedia.org/wiki/Quality_assurance>QA models</a>.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Siva+Reddy" title="Search for 'Siva Reddy' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/m/mirella-lapata/ class=align-middle>Mirella Lapata</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/a/arjun-akula/ class=align-middle>Arjun Akula</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/s/spandana-gella/ class=align-middle>Spandana Gella</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/s/song-chun-zhu/ class=align-middle>Song-chun Zhu</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/s/slav-petrov/ class=align-middle>Slav Petrov</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/y/yaser-al-onaizan/ class=align-middle>Yaser Al-Onaizan</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/daniel-zeman/ class=align-middle>Daniel Zeman</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/martin-popel/ class=align-middle>Martin Popel</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/milan-straka/ class=align-middle>Milan Straka</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jan-hajic/ class=align-middle>Jan Hajic</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/joakim-nivre/ class=align-middle>Joakim Nivre</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/f/filip-ginter/ class=align-middle>Filip Ginter</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/juhani-luotolahti/ class=align-middle>Juhani Luotolahti</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/sampo-pyysalo/ class=align-middle>Sampo Pyysalo</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/martin-potthast/ class=align-middle>Martin Potthast</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/f/francis-tyers/ class=align-middle>Francis Tyers</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/e/elena-badmaeva/ class=align-middle>Elena Badmaeva</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/memduh-gokirmak/ class=align-middle>Memduh Gökırmak</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/anna-nedoluzhko/ class=align-middle>Anna Nedoluzhko</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/silvie-cinkova/ class=align-middle>Silvie Cinková</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jan-hajic-jr/ class=align-middle>Jan Hajič jr.</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jaroslava-hlavacova/ class=align-middle>Jaroslava Hlaváčová</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/v/vaclava-kettnerova/ class=align-middle>Václava Kettnerová</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/zdenka-uresova/ class=align-middle>Zdenka Uresova</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jenna-kanerva/ class=align-middle>Jenna Kanerva</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/stina-ojala/ class=align-middle>Stina Ojala</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/anna-missila/ class=align-middle>Anna Missilä</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/christopher-d-manning/ class=align-middle>Christopher D. Manning</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/sebastian-schuster/ class=align-middle>Sebastian Schuster</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/dima-taji/ class=align-middle>Dima Taji</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/n/nizar-habash/ class=align-middle>Nizar Habash</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/herman-leung/ class=align-middle>Herman Leung</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/marie-catherine-de-marneffe/ class=align-middle>Marie-Catherine de Marneffe</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/manuela-sanguinetti/ class=align-middle>Manuela Sanguinetti</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/maria-simi/ class=align-middle>Maria Simi</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/hiroshi-kanayama/ class=align-middle>Hiroshi Kanayama</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/v/valeria-de-paiva/ class=align-middle>Valeria de Paiva</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/kira-droganova/ class=align-middle>Kira Droganova</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/hector-martinez-alonso/ class=align-middle>Héctor Martínez Alonso</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/cagri-coltekin/ class=align-middle>Çağrı Çöltekin</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/u/umut-sulubacak/ class=align-middle>Umut Sulubacak</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/hans-uszkoreit/ class=align-middle>Hans Uszkoreit</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/v/vivien-macketanz/ class=align-middle>Vivien Macketanz</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/aljoscha-burchardt/ class=align-middle>Aljoscha Burchardt</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/kim-harris/ class=align-middle>Kim Harris</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/katrin-marheinecke/ class=align-middle>Katrin Marheinecke</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/g/georg-rehm/ class=align-middle>Georg Rehm</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/tolga-kayadelen/ class=align-middle>Tolga Kayadelen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/mohammed-attia/ class=align-middle>Mohammed Attia</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/ali-elkahky/ class=align-middle>Ali Elkahky</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/zhuoran-yu/ class=align-middle>Zhuoran Yu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/e/emily-pitler/ class=align-middle>Emily Pitler</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/saran-lertpradit/ class=align-middle>Saran Lertpradit</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/michael-mandel/ class=align-middle>Michael Mandel</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jesse-kirchner/ class=align-middle>Jesse Kirchner</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/hector-fernandez-alcalde/ class=align-middle>Hector Fernandez Alcalde</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jana-strnadova/ class=align-middle>Jana Strnadová</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/e/esha-banerjee/ class=align-middle>Esha Banerjee</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/ruli-manurung/ class=align-middle>Ruli Manurung</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/antonio-stella/ class=align-middle>Antonio Stella</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/atsuko-shimada/ class=align-middle>Atsuko Shimada</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/sookyoung-kwak/ class=align-middle>Sookyoung Kwak</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/g/gustavo-mendonca/ class=align-middle>Gustavo Mendonca</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/tatiana-lando/ class=align-middle>Tatiana Lando</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/rattima-nitisaroj/ class=align-middle>Rattima Nitisaroj</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/josie-li/ class=align-middle>Josie Li</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jianpeng-cheng/ class=align-middle>Jianpeng Cheng</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/v/vijay-saraswat/ class=align-middle>Vijay Saraswat</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/rajarshi-das/ class=align-middle>Rajarshi Das</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/manzil-zaheer/ class=align-middle>Manzil Zaheer</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/andrew-mccallum/ class=align-middle>Andrew McCallum</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/f/federico-fancellu/ class=align-middle>Federico Fancellu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/adam-lopez/ class=align-middle>Adam Lopez</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/b/bonnie-webber/ class=align-middle>Bonnie Webber</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/keze-wang/ class=align-middle>Keze Wang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/o/oscar-tackstrom/ class=align-middle>Oscar Täckström</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/mark-steedman/ class=align-middle>Mark Steedman</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/li-dong/ class=align-middle>Li Dong</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jonathan-mallinson/ class=align-middle>Jonathan Mallinson</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/song-feng/ class=align-middle>Song Feng</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/malihe-alikhani/ class=align-middle>Malihe Alikhani</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/he-he/ class=align-middle>He He</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yangfeng-ji/ class=align-middle>Yangfeng Ji</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/mohit-iyyer/ class=align-middle>Mohit Iyyer</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/zhou-yu/ class=align-middle>Zhou Yu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/zhi-wen/ class=align-middle>Zhi Wen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/x/xing-han-lu/ class=align-middle>Xing Han Lu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/acl/ class=align-middle>ACL</a><span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/venues/emnlp/ class=align-middle>EMNLP</a><span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/venues/conll/ class=align-middle>CoNLL</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/ws/ class=align-middle>WS</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/dialdoc/ class=align-middle>dialdoc</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-venues aria-expanded=false aria-controls=more-venues>show all...</li><div class="collapse border-top" id=more-venues><li class=list-group-item><a href=/venues/clinicalnlp/ class=align-middle>ClinicalNLP</a><span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>