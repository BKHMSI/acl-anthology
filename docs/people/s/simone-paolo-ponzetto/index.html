<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Simone Paolo Ponzetto - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Simone Paolo</span> <span class=font-weight-bold>Ponzetto</span></h2><hr><div class=row><div class=col-lg-9><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.615.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--615 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.615 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.615/>Come hither or go away? Recognising pre-electoral coalition signals in the news</a></strong><br><a href=/people/i/ines-rehbein/>Ines Rehbein</a>
|
<a href=/people/s/simone-paolo-ponzetto/>Simone Paolo Ponzetto</a>
|
<a href=/people/a/anna-adendorf/>Anna Adendorf</a>
|
<a href=/people/o/oke-bahnsen/>Oke Bahnsen</a>
|
<a href=/people/l/lukas-stoetzer/>Lukas Stoetzer</a>
|
<a href=/people/h/heiner-stuckenschmidt/>Heiner Stuckenschmidt</a><br><a href=/volumes/2021.emnlp-main/ class=text-muted>Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--615><div class="card-body p-3 small">In this paper, we introduce the task of political coalition signal prediction from text, that is, the task of recognizing from the <a href=https://en.wikipedia.org/wiki/News_media>news coverage</a> leading up to an election the (un)willingness of political parties to form a <a href=https://en.wikipedia.org/wiki/Coalition_government>government coalition</a>. We decompose our problem into two related, but distinct tasks : (i) predicting whether a reported statement from a politician or a journalist refers to a potential <a href=https://en.wikipedia.org/wiki/Coalition>coalition</a> and (ii) predicting the polarity of the signal namely, whether the speaker is in favour of or against the coalition. For this, we explore the benefits of <a href=https://en.wikipedia.org/wiki/Multi-task_learning>multi-task learning</a> and investigate which setup and task formulation is best suited for each <a href=https://en.wikipedia.org/wiki/Task_(project_management)>sub-task</a>. We evaluate our approach, based on hand-coded newspaper articles, covering elections in three countries (Ireland, <a href=https://en.wikipedia.org/wiki/Germany>Germany</a>, Austria) and two languages (English, German). Our results show that the multi-task learning approach can further improve results over a strong monolingual transfer learning baseline.</div></div><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wanlp-1.17.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wanlp-1--17 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wanlp-1.17 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.wanlp-1.17/>AraWEAT : Multidimensional Analysis of Biases in Arabic Word Embeddings<span class=acl-fixed-case>A</span>ra<span class=acl-fixed-case>WEAT</span>: Multidimensional Analysis of Biases in <span class=acl-fixed-case>A</span>rabic Word Embeddings</a></strong><br><a href=/people/a/anne-lauscher/>Anne Lauscher</a>
|
<a href=/people/r/rafik-takieddin/>Rafik Takieddin</a>
|
<a href=/people/s/simone-paolo-ponzetto/>Simone Paolo Ponzetto</a>
|
<a href=/people/g/goran-glavas/>Goran Glavaš</a><br><a href=/volumes/2020.wanlp-1/ class=text-muted>Proceedings of the Fifth Arabic Natural Language Processing Workshop</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wanlp-1--17><div class="card-body p-3 small">Recent work has shown that distributional word vector spaces often encode <a href=https://en.wikipedia.org/wiki/Bias>human biases</a> like <a href=https://en.wikipedia.org/wiki/Sexism>sexism</a> or <a href=https://en.wikipedia.org/wiki/Racism>racism</a>. In this work, we conduct an extensive analysis of biases in Arabic word embeddings by applying a range of recently introduced bias tests on a variety of embedding spaces induced from corpora in Arabic. We measure the presence of biases across several dimensions, namely : embedding models (Skip-Gram, CBOW, and FastText) and vector sizes, types of text (encyclopedic text, and news vs. user-generated content), dialects (Egyptian Arabic vs. Modern Standard Arabic), and time (diachronic analyses over corpora from different time periods). Our analysis yields several interesting findings, e.g., that implicit gender bias in <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a> trained on Arabic news corpora steadily increases over time (between 2007 and 2017). We make the Arabic bias specifications (AraWEAT) publicly available.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.728.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--728 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.728 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.728/>Word Sense Disambiguation for 158 Languages using Word Embeddings Only</a></strong><br><a href=/people/v/varvara-logacheva/>Varvara Logacheva</a>
|
<a href=/people/d/denis-teslenko/>Denis Teslenko</a>
|
<a href=/people/a/artem-shelmanov/>Artem Shelmanov</a>
|
<a href=/people/s/steffen-remus/>Steffen Remus</a>
|
<a href=/people/d/dmitry-ustalov/>Dmitry Ustalov</a>
|
<a href=/people/a/andrey-kutuzov/>Andrey Kutuzov</a>
|
<a href=/people/e/ekaterina-artemova/>Ekaterina Artemova</a>
|
<a href=/people/c/chris-biemann/>Chris Biemann</a>
|
<a href=/people/s/simone-paolo-ponzetto/>Simone Paolo Ponzetto</a>
|
<a href=/people/a/alexander-panchenko/>Alexander Panchenko</a><br><a href=/volumes/2020.lrec-1/ class=text-muted>Proceedings of the 12th Language Resources and Evaluation Conference</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--728><div class="card-body p-3 small">Disambiguation of word senses in context is easy for humans, but is a major challenge for automatic approaches. Sophisticated supervised and knowledge-based models were developed to solve this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. However, (i) the inherent <a href=https://en.wikipedia.org/wiki/Zipfian_distribution>Zipfian distribution</a> of supervised training instances for a given word and/or (ii) the quality of linguistic knowledge representations motivate the development of completely unsupervised and knowledge-free approaches to word sense disambiguation (WSD). They are particularly useful for under-resourced languages which do not have any resources for building either supervised and/or knowledge-based models. In this paper, we present a method that takes as input a standard pre-trained word embedding model and induces a fully-fledged word sense inventory, which can be used for disambiguation in context. We use this method to induce a collection of sense inventories for 158 languages on the basis of the original pre-trained fastText word embeddings by Grave et al., (2018), enabling WSD in these <a href=https://en.wikipedia.org/wiki/Language>languages</a>. Models and system are available online.</div></div><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-3034.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-3034 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-3034 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-3034/>SEAGLE : A Platform for Comparative Evaluation of Semantic Encoders for Information Retrieval<span class=acl-fixed-case>SEAGLE</span>: A Platform for Comparative Evaluation of Semantic Encoders for Information Retrieval</a></strong><br><a href=/people/f/fabian-david-schmidt/>Fabian David Schmidt</a>
|
<a href=/people/m/markus-dietsche/>Markus Dietsche</a>
|
<a href=/people/s/simone-paolo-ponzetto/>Simone Paolo Ponzetto</a>
|
<a href=/people/g/goran-glavas/>Goran Glavaš</a><br><a href=/volumes/D19-3/ class=text-muted>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP): System Demonstrations</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-3034><div class="card-body p-3 small">We introduce Seagle, a platform for comparative evaluation of semantic text encoding models on information retrieval (IR) tasks. Seagle implements (1) word embedding aggregators, which represent texts as algebraic aggregations of pretrained word embeddings and (2) pretrained semantic encoders, and allows for their comparative evaluation on arbitrary (monolingual and cross-lingual) IR collections. We benchmark Seagle&#8217;s models on monolingual document retrieval and cross-lingual sentence retrieval. Seagle functionality can be exploited via an easy-to-use web interface and its modular backend (micro-service architecture) can easily be extended with additional semantic search models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1490.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1490 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1490 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P19-1490.Supplementary.zip data-toggle=tooltip data-placement=top title=Supplementary><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/385216016 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1490/>Multilingual and Cross-Lingual Graded Lexical Entailment</a></strong><br><a href=/people/i/ivan-vulic/>Ivan Vulić</a>
|
<a href=/people/s/simone-paolo-ponzetto/>Simone Paolo Ponzetto</a>
|
<a href=/people/g/goran-glavas/>Goran Glavaš</a><br><a href=/volumes/P19-1/ class=text-muted>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1490><div class="card-body p-3 small">Grounded in <a href=https://en.wikipedia.org/wiki/Cognitive_linguistics>cognitive linguistics</a>, graded lexical entailment (GR-LE) is concerned with fine-grained assertions regarding the directional hierarchical relationships between concepts on a continuous scale. In this paper, we present the first work on cross-lingual generalisation of GR-LE relation. Starting from HyperLex, the only available GR-LE dataset in <a href=https://en.wikipedia.org/wiki/English_language>English</a>, we construct new monolingual GR-LE datasets for three other languages, and combine those to create a set of six cross-lingual GR-LE datasets termed CL-HYPERLEX. We next present a novel method dubbed CLEAR (Cross-Lingual Lexical Entailment Attract-Repel) for effectively capturing graded (and binary) LE, both monolingually in different languages as well as across languages (i.e., on CL-HYPERLEX). Coupled with a <a href=https://en.wikipedia.org/wiki/Bilingual_dictionary>bilingual dictionary</a>, CLEAR leverages <a href=https://en.wikipedia.org/wiki/Taxonomy_(biology)>taxonomic LE knowledge</a> in a resource-rich language (e.g., English) and propagates it to other languages. Supported by cross-lingual LE transfer, CLEAR sets competitive baseline performance on three new monolingual GR-LE datasets and six cross-lingual GR-LE datasets. In addition, we show that CLEAR outperforms current <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a> on binary cross-lingual LE detection by a wide margin for diverse language pairs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-4004.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-4004 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-4004 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-4004/>Computational Analysis of Political Texts : Bridging Research Efforts Across Communities</a></strong><br><a href=/people/g/goran-glavas/>Goran Glavaš</a>
|
<a href=/people/f/federico-nanni/>Federico Nanni</a>
|
<a href=/people/s/simone-paolo-ponzetto/>Simone Paolo Ponzetto</a><br><a href=/volumes/P19-4/ class=text-muted>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Tutorial Abstracts</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-4004><div class="card-body p-3 small">In the last twenty years, political scientists started adopting and developing natural language processing (NLP) methods more actively in order to exploit <a href=https://en.wikipedia.org/wiki/Text_(literary_theory)>text</a> as an additional source of data in their analyses. Over the last decade the usage of computational methods for analysis of political texts has drastically expanded in scope, allowing for a sustained growth of the text-as-data community in <a href=https://en.wikipedia.org/wiki/Political_science>political science</a>. In <a href=https://en.wikipedia.org/wiki/Political_science>political science</a>, NLP methods have been extensively used for a number of analyses types and tasks, including inferring policy position of actors from textual evidence, detecting topics in <a href=https://en.wikipedia.org/wiki/Political_philosophy>political texts</a>, and analyzing stylistic aspects of <a href=https://en.wikipedia.org/wiki/Political_philosophy>political texts</a> (e.g., assessing the role of <a href=https://en.wikipedia.org/wiki/Ambiguity>language ambiguity</a> in framing the political agenda). Just like in numerous other domains, much of the work on computational analysis of political texts has been enabled and facilitated by the development of resources such as, the topically coded electoral programmes (e.g., the Manifesto Corpus) or topically coded legislative texts (e.g., the Comparative Agenda Project). Political scientists created resources and used available NLP methods to process textual data largely in isolation from the NLP community. At the same time, NLP researchers addressed closely related tasks such as election prediction, ideology classification, and stance detection. In other words, these two communities have been largely agnostic of one another, with NLP researchers mostly unaware of interesting applications in <a href=https://en.wikipedia.org/wiki/Political_science>political science</a> and political scientists not applying cutting-edge NLP methodology to their problems. The main goal of this tutorial is to systematize and analyze the body of research work on political texts from both communities. We aim to provide a gentle, all-round introduction to methods and tasks related to computational analysis of political texts. Our vision is to bring the two research communities closer to each other and contribute to faster and more significant developments in this interdisciplinary research area.</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1370.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1370 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1370 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D18-1370.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1370/>Investigating the Role of <a href=https://en.wikipedia.org/wiki/Argumentation_theory>Argumentation</a> in the <a href=https://en.wikipedia.org/wiki/Rhetorical_analysis>Rhetorical Analysis</a> of <a href=https://en.wikipedia.org/wiki/Scientific_literature>Scientific Publications</a> with Neural Multi-Task Learning Models</a></strong><br><a href=/people/a/anne-lauscher/>Anne Lauscher</a>
|
<a href=/people/g/goran-glavas/>Goran Glavaš</a>
|
<a href=/people/s/simone-paolo-ponzetto/>Simone Paolo Ponzetto</a>
|
<a href=/people/k/kai-eckert/>Kai Eckert</a><br><a href=/volumes/D18-1/ class=text-muted>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1370><div class="card-body p-3 small">Exponential growth in the number of <a href=https://en.wikipedia.org/wiki/Scientific_literature>scientific publications</a> yields the need for effective automatic analysis of rhetorical aspects of scientific writing. Acknowledging the argumentative nature of <a href=https://en.wikipedia.org/wiki/Scientific_literature>scientific text</a>, in this work we investigate the link between the argumentative structure of scientific publications and rhetorical aspects such as discourse categories or <a href=https://en.wikipedia.org/wiki/Citation>citation contexts</a>. To this end, we (1) augment a corpus of scientific publications annotated with four layers of rhetoric annotations with argumentation annotations and (2) investigate neural multi-task learning architectures combining argument extraction with a set of rhetorical classification tasks. By coupling rhetorical classifiers with the extraction of argumentative components in a joint multi-task learning setting, we obtain significant performance gains for different rhetorical analysis tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S18-1132.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S18-1132 data-toggle=collapse aria-expanded=false aria-controls=abstract-S18-1132 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/S18-1132/>UniMa at SemEval-2018 Task 7 : Semantic Relation Extraction and Classification from Scientific Publications<span class=acl-fixed-case>U</span>ni<span class=acl-fixed-case>M</span>a at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2018 Task 7: Semantic Relation Extraction and Classification from Scientific Publications</a></strong><br><a href=/people/t/thorsten-keiper/>Thorsten Keiper</a>
|
<a href=/people/z/zhonghao-lyu/>Zhonghao Lyu</a>
|
<a href=/people/s/sara-pooladzadeh/>Sara Pooladzadeh</a>
|
<a href=/people/y/yuan-xu/>Yuan Xu</a>
|
<a href=/people/j/jingyi-zhang/>Jingyi Zhang</a>
|
<a href=/people/a/anne-lauscher/>Anne Lauscher</a>
|
<a href=/people/s/simone-paolo-ponzetto/>Simone Paolo Ponzetto</a><br><a href=/volumes/S18-1/ class=text-muted>Proceedings of The 12th International Workshop on Semantic Evaluation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S18-1132><div class="card-body p-3 small">Large repositories of scientific literature call for the development of robust methods to extract information from <a href=https://en.wikipedia.org/wiki/Academic_publishing>scholarly papers</a>. This <a href=https://en.wikipedia.org/wiki/Problem_solving>problem</a> is addressed by the SemEval 2018 Task 7 on extracting and classifying relations found within <a href=https://en.wikipedia.org/wiki/Scientific_literature>scientific publications</a>. In this paper, we present a feature-based and a deep learning-based approach to the task and discuss the results of the system runs that we submitted for evaluation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5206.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5206 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5206 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5206/>An Argument-Annotated Corpus of Scientific Publications</a></strong><br><a href=/people/a/anne-lauscher/>Anne Lauscher</a>
|
<a href=/people/g/goran-glavas/>Goran Glavaš</a>
|
<a href=/people/s/simone-paolo-ponzetto/>Simone Paolo Ponzetto</a><br><a href=/volumes/W18-52/ class=text-muted>Proceedings of the 5th Workshop on Argument Mining</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5206><div class="card-body p-3 small">Argumentation is an essential feature of scientific language. We present an annotation study resulting in a <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus of scientific publications</a> annotated with argumentative components and relations. The argumentative annotations have been added to the existing Dr. Inventor Corpus, already annotated for four other rhetorical aspects. We analyze the annotated argumentative structures and investigate the relations between <a href=https://en.wikipedia.org/wiki/Argumentation_theory>argumentation</a> and other rhetorical aspects of <a href=https://en.wikipedia.org/wiki/Scientific_literature>scientific writing</a>, such as discourse roles and citation contexts.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2010.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2010 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2010 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2010.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2010.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-2010" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P18-2010/>Unsupervised Semantic Frame Induction using Triclustering</a></strong><br><a href=/people/d/dmitry-ustalov/>Dmitry Ustalov</a>
|
<a href=/people/a/alexander-panchenko/>Alexander Panchenko</a>
|
<a href=/people/a/andrey-kutuzov/>Andrey Kutuzov</a>
|
<a href=/people/c/chris-biemann/>Chris Biemann</a>
|
<a href=/people/s/simone-paolo-ponzetto/>Simone Paolo Ponzetto</a><br><a href=/volumes/P18-2/ class=text-muted>Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2010><div class="card-body p-3 small">We use dependency triples automatically extracted from a Web-scale corpus to perform unsupervised semantic frame induction. We cast the frame induction problem as a triclustering problem that is a generalization of <a href=https://en.wikipedia.org/wiki/Cluster_analysis>clustering</a> for triadic data. Our replicable benchmarks demonstrate that the proposed graph-based approach, Triframes, shows state-of-the art results on this task on a FrameNet-derived dataset and performing on par with competitive methods on a verb class clustering task.</div></div><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2014.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2014 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2014 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234956698 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-2014/>Exploring Neural Text Simplification Models</a></strong><br><a href=/people/s/sergiu-nisioi/>Sergiu Nisioi</a>
|
<a href=/people/s/sanja-stajner/>Sanja Štajner</a>
|
<a href=/people/s/simone-paolo-ponzetto/>Simone Paolo Ponzetto</a>
|
<a href=/people/l/liviu-p-dinu/>Liviu P. Dinu</a><br><a href=/volumes/P17-2/ class=text-muted>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2014><div class="card-body p-3 small">We present the first attempt at using sequence to sequence neural networks to model text simplification (TS). Unlike the previously proposed automated TS systems, our neural text simplification (NTS) systems are able to simultaneously perform lexical simplification and content reduction. An extensive human evaluation of the output has shown that NTS systems achieve almost perfect <a href=https://en.wikipedia.org/wiki/Grammaticality>grammaticality</a> and meaning preservation of output sentences and higher level of simplification than the state-of-the-art automated TS systems</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2906.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2906 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2906 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-2906/>Cross-Lingual Classification of Topics in Political Texts</a></strong><br><a href=/people/g/goran-glavas/>Goran Glavaš</a>
|
<a href=/people/f/federico-nanni/>Federico Nanni</a>
|
<a href=/people/s/simone-paolo-ponzetto/>Simone Paolo Ponzetto</a><br><a href=/volumes/W17-29/ class=text-muted>Proceedings of the Second Workshop on NLP and Computational Social Science</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2906><div class="card-body p-3 small">In this paper, we propose an approach for cross-lingual topical coding of sentences from electoral manifestos of political parties in different languages. To this end, we exploit continuous semantic text representations and induce a joint multilingual semantic vector spaces to enable <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised learning</a> using manually-coded sentences across different languages. Our experimental results show that <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifiers</a> trained on multilingual data yield performance boosts over monolingual topic classification.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5030.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5030 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5030 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5030/>Effects of Lexical Properties on Viewing Time per Word in Autistic and Neurotypical Readers</a></strong><br><a href=/people/s/sanja-stajner/>Sanja Štajner</a>
|
<a href=/people/v/victoria-yaneva/>Victoria Yaneva</a>
|
<a href=/people/r/ruslan-mitkov/>Ruslan Mitkov</a>
|
<a href=/people/s/simone-paolo-ponzetto/>Simone Paolo Ponzetto</a><br><a href=/volumes/W17-50/ class=text-muted>Proceedings of the 12th Workshop on Innovative Use of NLP for Building Educational Applications</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5030><div class="card-body p-3 small">Eye tracking studies from the past few decades have shaped the way we think of word complexity and <a href=https://en.wikipedia.org/wiki/Cognitive_load>cognitive load</a> : words that are long, rare and ambiguous are more difficult to read. However, online processing techniques have been scarcely applied to investigating the reading difficulties of people with autism and what vocabulary is challenging for them. We present parallel gaze data obtained from adult readers with autism and a control group of neurotypical readers and show that the former required higher cognitive effort to comprehend the texts as evidenced by three gaze-based measures. We divide all words into four classes based on their viewing times for both groups and investigate the relationship between longer viewing times and word length, word frequency, and four cognitively-based measures (word concreteness, familiarity, age of acquisition and imagability).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1185.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1185 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1185 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D17-1185.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D17-1185/>Dual Tensor Model for Detecting Asymmetric Lexico-Semantic Relations</a></strong><br><a href=/people/g/goran-glavas/>Goran Glavaš</a>
|
<a href=/people/s/simone-paolo-ponzetto/>Simone Paolo Ponzetto</a><br><a href=/volumes/D17-1/ class=text-muted>Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1185><div class="card-body p-3 small">Detection of lexico-semantic relations is one of the central tasks of <a href=https://en.wikipedia.org/wiki/Computational_semantics>computational semantics</a>. Although some fundamental relations (e.g., hypernymy) are asymmetric, most existing models account for asymmetry only implicitly and use the same concept representations to support detection of symmetric and asymmetric relations alike. In this work, we propose the Dual Tensor model, a neural architecture with which we explicitly model the asymmetry and capture the translation between unspecialized and specialized word embeddings via a pair of <a href=https://en.wikipedia.org/wiki/Tensor>tensors</a>. Although our Dual Tensor model needs only unspecialized embeddings as input, our experiments on <a href=https://en.wikipedia.org/wiki/Hypernymy>hypernymy</a> and <a href=https://en.wikipedia.org/wiki/Meronymy>meronymy detection</a> suggest that it can outperform more complex and resource-intensive models. We further demonstrate that the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> can account for <a href=https://en.wikipedia.org/wiki/Polysemy>polysemy</a> and that it exhibits stable performance across languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1318.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1318 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1318 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/238236263 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D17-1318/>Topic-Based Agreement and Disagreement in <a href=https://en.wikipedia.org/wiki/United_States_Electoral_College>US Electoral Manifestos</a><span class=acl-fixed-case>US</span> Electoral Manifestos</a></strong><br><a href=/people/s/stefano-menini/>Stefano Menini</a>
|
<a href=/people/f/federico-nanni/>Federico Nanni</a>
|
<a href=/people/s/simone-paolo-ponzetto/>Simone Paolo Ponzetto</a>
|
<a href=/people/s/sara-tonelli/>Sara Tonelli</a><br><a href=/volumes/D17-1/ class=text-muted>Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1318><div class="card-body p-3 small">We present a topic-based analysis of agreement and disagreement in <a href=https://en.wikipedia.org/wiki/Manifesto>political manifestos</a>, which relies on a new method for topic detection based on key concept clustering. Our approach outperforms both standard techniques like LDA and a state-of-the-art graph-based method, and provides promising initial results for this new task in <a href=https://en.wikipedia.org/wiki/Computational_social_science>computational social science</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-2016.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-2016 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-2016 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D17-2016" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D17-2016/>Unsupervised, Knowledge-Free, and Interpretable Word Sense Disambiguation</a></strong><br><a href=/people/a/alexander-panchenko/>Alexander Panchenko</a>
|
<a href=/people/f/fide-marten/>Fide Marten</a>
|
<a href=/people/e/eugen-ruppert/>Eugen Ruppert</a>
|
<a href=/people/s/stefano-faralli/>Stefano Faralli</a>
|
<a href=/people/d/dmitry-ustalov/>Dmitry Ustalov</a>
|
<a href=/people/s/simone-paolo-ponzetto/>Simone Paolo Ponzetto</a>
|
<a href=/people/c/chris-biemann/>Chris Biemann</a><br><a href=/volumes/D17-2/ class=text-muted>Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-2016><div class="card-body p-3 small">Interpretability of a <a href=https://en.wikipedia.org/wiki/Predictive_modelling>predictive model</a> is a powerful feature that gains the trust of users in the correctness of the predictions. In word sense disambiguation (WSD), knowledge-based systems tend to be much more interpretable than knowledge-free counterparts as they rely on the wealth of manually-encoded elements representing word senses, such as hypernyms, usage examples, and images. We present a WSD system that bridges the gap between these two so far disconnected groups of methods. Namely, our <a href=https://en.wikipedia.org/wiki/System>system</a>, providing access to several state-of-the-art WSD models, aims to be interpretable as a knowledge-based system while it remains completely unsupervised and knowledge-free. The presented tool features a Web interface for all-word disambiguation of texts that makes the sense predictions human readable by providing interpretable word sense inventories, sense representations, and disambiguation results. We provide a public API, enabling seamless integration.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1009.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1009 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1009 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1009/>Unsupervised Does Not Mean Uninterpretable : The Case for Word Sense Induction and Disambiguation</a></strong><br><a href=/people/a/alexander-panchenko/>Alexander Panchenko</a>
|
<a href=/people/e/eugen-ruppert/>Eugen Ruppert</a>
|
<a href=/people/s/stefano-faralli/>Stefano Faralli</a>
|
<a href=/people/s/simone-paolo-ponzetto/>Simone Paolo Ponzetto</a>
|
<a href=/people/c/chris-biemann/>Chris Biemann</a><br><a href=/volumes/E17-1/ class=text-muted>Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1009><div class="card-body p-3 small">The current trend in <a href=https://en.wikipedia.org/wiki/Neuro-linguistic_programming>NLP</a> is the use of highly opaque models, e.g. neural networks and <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a>. While these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> yield state-of-the-art results on a range of <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a>, their drawback is poor interpretability. On the example of word sense induction and disambiguation (WSID), we show that it is possible to develop an interpretable <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> that matches the state-of-the-art models in <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>. Namely, we present an unsupervised, knowledge-free WSID approach, which is interpretable at three levels : word sense inventory, sense feature representations, and disambiguation procedure. Experiments show that our model performs on par with state-of-the-art word sense embeddings and other <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised systems</a> while offering the possibility to justify its decisions in <a href=https://en.wikipedia.org/wiki/Human-readable_medium>human-readable form</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1056.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1056 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1056 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1056/>The ContrastMedium Algorithm : Taxonomy Induction From Noisy Knowledge Graphs With Just A Few Links<span class=acl-fixed-case>C</span>ontrast<span class=acl-fixed-case>M</span>edium Algorithm: Taxonomy Induction From Noisy Knowledge Graphs With Just A Few Links</a></strong><br><a href=/people/s/stefano-faralli/>Stefano Faralli</a>
|
<a href=/people/a/alexander-panchenko/>Alexander Panchenko</a>
|
<a href=/people/c/chris-biemann/>Chris Biemann</a>
|
<a href=/people/s/simone-paolo-ponzetto/>Simone Paolo Ponzetto</a><br><a href=/volumes/E17-1/ class=text-muted>Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1056><div class="card-body p-3 small">In this paper, we present ContrastMedium, an <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> that transforms noisy semantic networks into full-fledged, clean taxonomies. ContrastMedium is able to identify the embedded taxonomy structure from a noisy knowledge graph without explicit human supervision such as, for instance, a set of manually selected input root and leaf concepts. This is achieved by leveraging structural information from a companion reference taxonomy, to which the input <a href=https://en.wikipedia.org/wiki/Knowledge_graph>knowledge graph</a> is linked (either automatically or manually). When used in conjunction with methods for hypernym acquisition and knowledge base linking, our methodology provides a complete solution for end-to-end taxonomy induction. We conduct experiments using automatically acquired knowledge graphs, as well as a SemEval benchmark, and show that our method is able to achieve high performance on the task of taxonomy induction.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2083.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2083 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2083 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2083/>Improving Neural Knowledge Base Completion with Cross-Lingual Projections</a></strong><br><a href=/people/p/patrick-klein/>Patrick Klein</a>
|
<a href=/people/s/simone-paolo-ponzetto/>Simone Paolo Ponzetto</a>
|
<a href=/people/g/goran-glavas/>Goran Glavaš</a><br><a href=/volumes/E17-2/ class=text-muted>Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2083><div class="card-body p-3 small">In this paper we present a cross-lingual extension of a neural tensor network model for knowledge base completion. We exploit multilingual synsets from <a href=https://en.wikipedia.org/wiki/BabelNet>BabelNet</a> to translate English triples to other languages and then augment the reference knowledge base with cross-lingual triples. We project monolingual embeddings of different languages to a shared multilingual space and use them for <a href=https://en.wikipedia.org/wiki/Network_topology>network initialization</a> (i.e., as initial concept embeddings). We then train the <a href=https://en.wikipedia.org/wiki/Computer_network>network</a> with triples from the cross-lingually augmented knowledge base. Results on WordNet link prediction show that leveraging cross-lingual information yields significant gains over exploiting only monolingual triples.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2109.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2109 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2109 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=E17-2109" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/E17-2109/>Unsupervised Cross-Lingual Scaling of Political Texts</a></strong><br><a href=/people/g/goran-glavas/>Goran Glavaš</a>
|
<a href=/people/f/federico-nanni/>Federico Nanni</a>
|
<a href=/people/s/simone-paolo-ponzetto/>Simone Paolo Ponzetto</a><br><a href=/volumes/E17-2/ class=text-muted>Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2109><div class="card-body p-3 small">Political text scaling aims to linearly order parties and politicians across <a href=https://en.wikipedia.org/wiki/Political_dimension>political dimensions</a> (e.g., <a href=https://en.wikipedia.org/wiki/Left&#8211;right_political_spectrum>left-to-right ideology</a>) based on <a href=https://en.wikipedia.org/wiki/Content_(media)>textual content</a> (e.g., <a href=https://en.wikipedia.org/wiki/Public_speaking>politician speeches</a> or party manifestos). Existing <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> scale texts based on relative word usage and can not be used for cross-lingual analyses. Additionally, there is little quantitative evidence that the output of these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> correlates with common political dimensions like left-to-right orientation. Experimental results show that the semantically-informed scaling models better predict the party positions than the existing word-based models in two different political dimensions. Furthermore, the proposed <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> exhibit no drop in performance in the cross-lingual compared to monolingual setting.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Simone+Paolo+Ponzetto" title="Search for 'Simone Paolo Ponzetto' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/g/goran-glavas/ class=align-middle>Goran Glavaš</a>
<span class="badge badge-secondary align-middle ml-2">10</span></li><li class=list-group-item><a href=/people/a/alexander-panchenko/ class=align-middle>Alexander Panchenko</a>
<span class="badge badge-secondary align-middle ml-2">5</span></li><li class=list-group-item><a href=/people/c/chris-biemann/ class=align-middle>Chris Biemann</a>
<span class="badge badge-secondary align-middle ml-2">5</span></li><li class=list-group-item><a href=/people/f/federico-nanni/ class=align-middle>Federico Nanni</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/people/a/anne-lauscher/ class=align-middle>Anne Lauscher</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/s/stefano-faralli/ class=align-middle>Stefano Faralli</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/d/dmitry-ustalov/ class=align-middle>Dmitry Ustalov</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/s/sanja-stajner/ class=align-middle>Sanja Štajner</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/e/eugen-ruppert/ class=align-middle>Eugen Ruppert</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/a/andrey-kutuzov/ class=align-middle>Andrey Kutuzov</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/s/sergiu-nisioi/ class=align-middle>Sergiu Nisioi</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/liviu-p-dinu/ class=align-middle>Liviu P. Dinu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/v/victoria-yaneva/ class=align-middle>Victoria Yaneva</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/ruslan-mitkov/ class=align-middle>Ruslan Mitkov</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/kai-eckert/ class=align-middle>Kai Eckert</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/i/ines-rehbein/ class=align-middle>Ines Rehbein</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/anna-adendorf/ class=align-middle>Anna Adendorf</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/o/oke-bahnsen/ class=align-middle>Oke Bahnsen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/lukas-stoetzer/ class=align-middle>Lukas Stoetzer</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/heiner-stuckenschmidt/ class=align-middle>Heiner Stuckenschmidt</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/f/fabian-david-schmidt/ class=align-middle>Fabian David Schmidt</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/markus-dietsche/ class=align-middle>Markus Dietsche</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/stefano-menini/ class=align-middle>Stefano Menini</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/sara-tonelli/ class=align-middle>Sara Tonelli</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/f/fide-marten/ class=align-middle>Fide Marten</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/thorsten-keiper/ class=align-middle>Thorsten Keiper</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/zhonghao-lyu/ class=align-middle>Zhonghao Lyu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/sara-pooladzadeh/ class=align-middle>Sara Pooladzadeh</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yuan-xu/ class=align-middle>Yuan Xu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jingyi-zhang/ class=align-middle>Jingyi Zhang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/rafik-takieddin/ class=align-middle>Rafik Takieddin</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/v/varvara-logacheva/ class=align-middle>Varvara Logacheva</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/denis-teslenko/ class=align-middle>Denis Teslenko</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/artem-shelmanov/ class=align-middle>Artem Shelmanov</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/steffen-remus/ class=align-middle>Steffen Remus</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/e/ekaterina-artemova/ class=align-middle>Ekaterina Artemova</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/p/patrick-klein/ class=align-middle>Patrick Klein</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/i/ivan-vulic/ class=align-middle>Ivan Vulić</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/emnlp/ class=align-middle>EMNLP</a><span class="badge badge-secondary align-middle ml-2">6</span></li><li class=list-group-item><a href=/venues/acl/ class=align-middle>ACL</a><span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/venues/eacl/ class=align-middle>EACL</a><span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/venues/ws/ class=align-middle>WS</a><span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/venues/semeval/ class=align-middle>SemEval</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-venues aria-expanded=false aria-controls=more-venues>show all...</li><div class="collapse border-top" id=more-venues><li class=list-group-item><a href=/venues/wanlp/ class=align-middle>WANLP</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/lrec/ class=align-middle>LREC</a><span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>