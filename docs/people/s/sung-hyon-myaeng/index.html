<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Sung-Hyon Myaeng - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Sung-Hyon</span> <span class=font-weight-bold>Myaeng</span></h2><p class="font-weight-light text-muted"><span class=font-italic>Also published as:</span>
Sung-hyon <span class=font-weight-normal>Myaeng</span></p><hr><div class=row><div class=col-lg-9><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.563.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--563 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.563 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.563/>Have You Seen That Number? Investigating Extrapolation in Question Answering Models</a></strong><br><a href=/people/j/jeonghwan-kim/>Jeonghwan Kim</a>
|
<a href=/people/g/giwon-hong/>Giwon Hong</a>
|
<a href=/people/k/kyung-min-kim/>Kyung-min Kim</a>
|
<a href=/people/j/junmo-kang/>Junmo Kang</a>
|
<a href=/people/s/sung-hyon-myaeng/>Sung-Hyon Myaeng</a><br><a href=/volumes/2021.emnlp-main/ class=text-muted>Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--563><div class="card-body p-3 small">Numerical reasoning in machine reading comprehension (MRC) has shown drastic improvements over the past few years. While the previous models for numerical MRC are able to interpolate the learned numerical reasoning capabilities, it is not clear whether they can perform just as well on numbers unseen in the training dataset. Our work rigorously tests state-of-the-art models on DROP, a numerical MRC dataset, to see if they can handle passages that contain out-of-range numbers. One of the key findings is that the <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> fail to extrapolate to unseen numbers. Presenting <a href=https://en.wikipedia.org/wiki/Number>numbers</a> as digit-by-digit input to the model, we also propose the E-digit number form that alleviates the lack of extrapolation in models and reveals the need to treat <a href=https://en.wikipedia.org/wiki/Number>numbers</a> differently from regular words in the text. Our work provides a valuable insight into the numerical MRC models and the way to represent number forms in MRC.<i>E-digit</i> number form that alleviates the lack of extrapolation in models and reveals the need to treat numbers differently from regular words in the text. Our work provides a valuable insight into the numerical MRC models and the way to represent number forms in MRC.</div></div><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.acl-main.311.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--acl-main--311 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.acl-main.311 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://slideslive.com/38928695 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.acl-main.311/>Roles and Utilization of Attention Heads in Transformer-based Neural Language Models</a></strong><br><a href=/people/j/jae-young-jo/>Jae-young Jo</a>
|
<a href=/people/s/sung-hyon-myaeng/>Sung-Hyon Myaeng</a><br><a href=/volumes/2020.acl-main/ class=text-muted>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--acl-main--311><div class="card-body p-3 small">Sentence encoders based on the transformer architecture have shown promising results on various natural language tasks. The main impetus lies in the pre-trained neural language models that capture long-range dependencies among words, owing to multi-head attention that is unique in the architecture. However, little is known for how linguistic properties are processed, represented, and utilized for downstream tasks among hundreds of <a href=https://en.wikipedia.org/wiki/Attentional_control>attention heads</a> inside the pre-trained transformer-based model. For the initial goal of examining the roles of attention heads in handling a set of linguistic features, we conducted a set of experiments with ten probing tasks and three downstream tasks on four pre-trained transformer families (GPT, GPT2, BERT, and ELECTRA). Meaningful insights are shown through the lens of heat map visualization and utilized to propose a relatively simple sentence representation method that takes advantage of most influential attention heads, resulting in additional performance improvements on the downstream tasks.</div></div><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-0412.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-0412 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-0412 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-0412/>Aligning Open IE Relations and KB Relations using a Siamese Network Based on Word Embedding<span class=acl-fixed-case>IE</span> Relations and <span class=acl-fixed-case>KB</span> Relations using a <span class=acl-fixed-case>S</span>iamese Network Based on Word Embedding</a></strong><br><a href=/people/r/rifki-afina-putri/>Rifki Afina Putri</a>
|
<a href=/people/g/giwon-hong/>Giwon Hong</a>
|
<a href=/people/s/sung-hyon-myaeng/>Sung-Hyon Myaeng</a><br><a href=/volumes/W19-04/ class=text-muted>Proceedings of the 13th International Conference on Computational Semantics - Long Papers</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-0412><div class="card-body p-3 small">Open Information Extraction (Open IE) aims at generating entity-relation-entity triples from a large amount of text, aiming at capturing key semantics of the text. Given a triple, the <a href=https://en.wikipedia.org/wiki/Binary_relation>relation</a> expresses the type of semantic relation between the entities. Although relations from an Open IE system are more extensible than those used in a traditional Information Extraction system and a Knowledge Base (KB) such as Knowledge Graphs, the former lacks in semantics ; an Open IE relation is simply a sequence of words, whereas a KB relation has a predefined meaning. As a way to provide a meaning to an Open IE relation, we attempt to align it with one of the predefined set of relations used in a KB. Our approach is to use a <a href=https://en.wikipedia.org/wiki/Siamese_network>Siamese network</a> that compares two sequences of word embeddings representing an Open IE relation and a predefined KB relation. In order to make the approach practical, we automatically generate a <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training dataset</a> using a distant supervision approach instead of relying on a <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>hand-labeled dataset</a>. Our experiment shows that the proposed <a href=https://en.wikipedia.org/wiki/Methodology>method</a> can capture the <a href=https://en.wikipedia.org/wiki/Relational_semantics>relational semantics</a> better than the recent <a href=https://en.wikipedia.org/wiki/Methodology>approaches</a>.</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5442.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5442 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5442 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5442/>Interpretable Word Embedding Contextualization</a></strong><br><a href=/people/k/kyoung-rok-jang/>Kyoung-Rok Jang</a>
|
<a href=/people/s/sung-hyon-myaeng/>Sung-Hyon Myaeng</a>
|
<a href=/people/s/sang-bum-kim/>Sang-Bum Kim</a><br><a href=/volumes/W18-54/ class=text-muted>Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5442><div class="card-body p-3 small">In this paper, we propose a method of calibrating a <a href=https://en.wikipedia.org/wiki/Word_embedding>word embedding</a>, so that the semantic it conveys becomes more relevant to the context. Our method is novel because the output shows clearly which senses that were originally presented in a target <a href=https://en.wikipedia.org/wiki/Word_embedding>word embedding</a> become stronger or weaker. This is possible by utilizing the technique of using <a href=https://en.wikipedia.org/wiki/Sparse_coding>sparse coding</a> to recover senses that comprises a <a href=https://en.wikipedia.org/wiki/Word_embedding>word embedding</a>.</div></div><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1022.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1022 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1022 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-1022/>A Computational Study on Word Meanings and Their Distributed Representations via Polymodal Embedding</a></strong><br><a href=/people/j/joohee-park/>Joohee Park</a>
|
<a href=/people/s/sung-hyon-myaeng/>Sung-hyon Myaeng</a><br><a href=/volumes/I17-1/ class=text-muted>Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1022><div class="card-body p-3 small">A <a href=https://en.wikipedia.org/wiki/Distributed_representation>distributed representation</a> has become a popular approach to capturing a <a href=https://en.wikipedia.org/wiki/Meaning_(linguistics)>word meaning</a>. Besides its success and practical value, however, questions arise about the relationships between a true word meaning and its <a href=https://en.wikipedia.org/wiki/Distributed_representation>distributed representation</a>. In this paper, we examine such a relationship via polymodal embedding approach inspired by the theory that humans tend to use diverse sources in developing a word meaning. The result suggests that the existing <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a> lack in capturing certain aspects of <a href=https://en.wikipedia.org/wiki/Semantics>word meanings</a> which can be significantly improved by the polymodal approach. Also, we show distinct characteristics of different types of words (e.g. concreteness) via <a href=https://en.wikipedia.org/wiki/Computational_neuroscience>computational studies</a>. Finally, we show our proposed embedding method outperforms the baselines in the word similarity measure tasks and the hypernym prediction tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1911.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1911 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1911 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1911/>Elucidating Conceptual Properties from Word Embeddings</a></strong><br><a href=/people/k/kyoung-rok-jang/>Kyoung-Rok Jang</a>
|
<a href=/people/s/sung-hyon-myaeng/>Sung-Hyon Myaeng</a><br><a href=/volumes/W17-19/ class=text-muted>Proceedings of the 1st Workshop on Sense, Concept and Entity Representations and their Applications</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1911><div class="card-body p-3 small">In this paper, we introduce a <a href=https://en.wikipedia.org/wiki/Scientific_method>method</a> of identifying the components (i.e. dimensions) of word embeddings that strongly signifies properties of a word. By elucidating such properties hidden in <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a>, we could make <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> more interpretable, and also could perform property-based meaning comparison. With the capability, we can answer questions like To what degree a given word has the property cuteness? or In what perspective two words are similar?. We verify our method by examining how the strength of property-signifying components correlates with the degree of prototypicality of a target word.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Sung-Hyon+Myaeng" title="Search for 'Sung-Hyon Myaeng' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/k/kyoung-rok-jang/ class=align-middle>Kyoung-Rok Jang</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/g/giwon-hong/ class=align-middle>Giwon Hong</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/j/joohee-park/ class=align-middle>Joohee Park</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jae-young-jo/ class=align-middle>Jae-young Jo</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jeonghwan-kim/ class=align-middle>Jeonghwan Kim</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/k/kyung-min-kim/ class=align-middle>Kyung-min Kim</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/junmo-kang/ class=align-middle>Junmo Kang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/sang-bum-kim/ class=align-middle>Sang-Bum Kim</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/rifki-afina-putri/ class=align-middle>Rifki Afina Putri</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/ws/ class=align-middle>WS</a><span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/venues/ijcnlp/ class=align-middle>IJCNLP</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/acl/ class=align-middle>ACL</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/emnlp/ class=align-middle>EMNLP</a><span class="badge badge-secondary align-middle ml-2">1</span></li></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>