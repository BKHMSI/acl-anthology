<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Santanu Pal - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Santanu</span> <span class=font-weight-bold>Pal</span></h2><hr><div class=row><div class=col-lg-9><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.acl-demos.37.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--acl-demos--37 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.acl-demos.37 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://slideslive.com/38928587 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.acl-demos.37/>MMPE : A Multi-Modal Interface using Handwriting, Touch Reordering, and Speech Commands for Post-Editing Machine Translation<span class=acl-fixed-case>MMPE</span>: <span class=acl-fixed-case>A</span> <span class=acl-fixed-case>M</span>ulti-<span class=acl-fixed-case>M</span>odal <span class=acl-fixed-case>I</span>nterface using <span class=acl-fixed-case>H</span>andwriting, <span class=acl-fixed-case>T</span>ouch <span class=acl-fixed-case>R</span>eordering, and <span class=acl-fixed-case>S</span>peech <span class=acl-fixed-case>C</span>ommands for <span class=acl-fixed-case>P</span>ost-<span class=acl-fixed-case>E</span>diting <span class=acl-fixed-case>M</span>achine <span class=acl-fixed-case>T</span>ranslation</a></strong><br><a href=/people/n/nico-herbig/>Nico Herbig</a>
|
<a href=/people/s/santanu-pal/>Santanu Pal</a>
|
<a href=/people/t/tim-duwel/>Tim Düwel</a>
|
<a href=/people/k/kalliopi-meladaki/>Kalliopi Meladaki</a>
|
<a href=/people/m/mahsa-monshizadeh/>Mahsa Monshizadeh</a>
|
<a href=/people/v/vladislav-hnatovskiy/>Vladislav Hnatovskiy</a>
|
<a href=/people/a/antonio-kruger/>Antonio Krüger</a>
|
<a href=/people/j/josef-van-genabith/>Josef van Genabith</a><br><a href=/volumes/2020.acl-demos/ class=text-muted>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--acl-demos--37><div class="card-body p-3 small">The shift from traditional <a href=https://en.wikipedia.org/wiki/Translation>translation</a> to post-editing (PE) of machine-translated (MT) text can save time and reduce errors, but it also affects the design of <a href=https://en.wikipedia.org/wiki/Translation>translation interfaces</a>, as the task changes from mainly generating text to correcting errors within otherwise helpful translation proposals. Since this paradigm shift offers potential for modalities other than <a href=https://en.wikipedia.org/wiki/Computer_mouse>mouse</a> and <a href=https://en.wikipedia.org/wiki/Computer_keyboard>keyboard</a>, we present MMPE, the first prototype to combine traditional input modes with pen, touch, and speech modalities for PE of MT. Users can directly cross out or hand-write new text, drag and drop words for reordering, or use spoken commands to update the text in place. All text manipulations are logged in an easily interpretable format to simplify subsequent translation process research. The results of an evaluation with professional translators suggest that pen and touch interaction are suitable for deletion and reordering tasks, while speech and multi-modal combinations of select & speech are considered suitable for replacements and insertions. Overall, experiment participants were enthusiastic about the new modalities and saw them as useful extensions to mouse & keyboard, but not as a complete substitute.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.524.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--524 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.524 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.524/>The Transference Architecture for Automatic Post-Editing</a></strong><br><a href=/people/s/santanu-pal/>Santanu Pal</a>
|
<a href=/people/h/hongfei-xu/>Hongfei Xu</a>
|
<a href=/people/n/nico-herbig/>Nico Herbig</a>
|
<a href=/people/s/sudip-kumar-naskar/>Sudip Kumar Naskar</a>
|
<a href=/people/a/antonio-kruger/>Antonio Krüger</a>
|
<a href=/people/j/josef-van-genabith/>Josef van Genabith</a><br><a href=/volumes/2020.coling-main/ class=text-muted>Proceedings of the 28th International Conference on Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--524><div class="card-body p-3 small">In automatic post-editing (APE) it makes sense to condition post-editing (pe) decisions on both the source (src) and the machine translated text (mt) as input. This has led to multi-encoder based neural APE approaches. A research challenge now is the search for architectures that best support the capture, preparation and provision of src and mt information and its integration with pe decisions. In this paper we present an efficient multi-encoder based APE model, called <a href=https://en.wikipedia.org/wiki/Transference>transference</a>. Unlike previous approaches, it (i) uses a transformer encoder block for src, (ii) followed by a decoder block, but without masking for self-attention on mt, which effectively acts as second encoder combining src mt, and (iii) feeds this representation into a final decoder block generating pe. Our model outperforms the best performing systems by 1 BLEU point on the WMT 2016, 2017, and 2018 EnglishGerman APE shared tasks (PBSMT and NMT). Furthermore, the results of our model on the WMT 2019 APE task using NMT data shows a comparable performance to the state-of-the-art system. The <a href=https://en.wikipedia.org/wiki/Time_complexity>inference time</a> of our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is similar to the vanilla transformer-based NMT system although our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> deals with two separate encoders. We further investigate the importance of our newly introduced second encoder and find that a too small amount of <a href=https://en.wikipedia.org/wiki/Abstraction_layer>layers</a> does hurt the performance, while reducing the number of layers of the <a href=https://en.wikipedia.org/wiki/Code>decoder</a> does not matter much.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wat-1.14.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wat-1--14 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wat-1.14 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.wat-1.14/>WT : Wipro AI Submissions to the WAT 2020<span class=acl-fixed-case>WT</span>: Wipro <span class=acl-fixed-case>AI</span> Submissions to the <span class=acl-fixed-case>WAT</span> 2020</a></strong><br><a href=/people/s/santanu-pal/>Santanu Pal</a><br><a href=/volumes/2020.wat-1/ class=text-muted>Proceedings of the 7th Workshop on Asian Translation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wat-1--14><div class="card-body p-3 small">In this paper we present an EnglishHindi and HindiEnglish neural machine translation (NMT) system, submitted to the Translation shared Task organized at WAT 2020. We trained a multilingual NMT system based on transformer architecture. In this paper we show : (i) how effective pre-processing helps to improve performance, (ii) how synthetic data through <a href=https://en.wikipedia.org/wiki/Back-translation>back-translation</a> from available monolingual data can help in overall translation performance, (iii) how language similarity can aid more onto it. Our submissions ranked 1st in both English to Hindi and Hindi to English translation achieving <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a> 20.80 and 29.59 respectively.</div></div><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5430.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5430 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5430 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5430/>UDSDFKI Submission to the WMT2019 CzechPolish Similar Language Translation Shared Task<span class=acl-fixed-case>UDS</span>–<span class=acl-fixed-case>DFKI</span> Submission to the <span class=acl-fixed-case>WMT</span>2019 <span class=acl-fixed-case>C</span>zech–<span class=acl-fixed-case>P</span>olish Similar Language Translation Shared Task</a></strong><br><a href=/people/s/santanu-pal/>Santanu Pal</a>
|
<a href=/people/m/marcos-zampieri/>Marcos Zampieri</a>
|
<a href=/people/j/josef-van-genabith/>Josef van Genabith</a><br><a href=/volumes/W19-54/ class=text-muted>Proceedings of the Fourth Conference on Machine Translation (Volume 3: Shared Task Papers, Day 2)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5430><div class="card-body p-3 small">In this paper we present the UDS-DFKI system submitted to the Similar Language Translation shared task at WMT 2019. The first edition of this shared task featured data from three pairs of similar languages : <a href=https://en.wikipedia.org/wiki/Czech_language>Czech</a> and <a href=https://en.wikipedia.org/wiki/Polish_language>Polish</a>, <a href=https://en.wikipedia.org/wiki/Hindi>Hindi</a> and Nepali, and <a href=https://en.wikipedia.org/wiki/Portuguese_language>Portuguese</a> and <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a>. Participants could choose to participate in any of these three tracks and submit system outputs in any translation direction. We report the results obtained by our <a href=https://en.wikipedia.org/wiki/System>system</a> in translating from <a href=https://en.wikipedia.org/wiki/Czech_language>Czech</a> to <a href=https://en.wikipedia.org/wiki/Polish_language>Polish</a> and comment on the impact of out-of-domain test data in the performance of our <a href=https://en.wikipedia.org/wiki/System>system</a>. UDS-DFKI achieved competitive performance ranking second among ten teams in <a href=https://en.wikipedia.org/wiki/Czech_language>Czech</a> to Polish translation.</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-2411.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-2411 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-2411 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-2411/>A Deep Learning Based Approach to <a href=https://en.wikipedia.org/wiki/Transliteration>Transliteration</a></a></strong><br><a href=/people/s/soumyadeep-kundu/>Soumyadeep Kundu</a>
|
<a href=/people/s/sayantan-paul/>Sayantan Paul</a>
|
<a href=/people/s/santanu-pal/>Santanu Pal</a><br><a href=/volumes/W18-24/ class=text-muted>Proceedings of the Seventh Named Entities Workshop</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-2411><div class="card-body p-3 small">In this paper, we propose different <a href=https://en.wikipedia.org/wiki/Computer_architecture>architectures</a> for language independent machine transliteration which is extremely important for <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing (NLP) applications</a>. Though a number of statistical models for <a href=https://en.wikipedia.org/wiki/Transliteration>transliteration</a> have already been proposed in the past few decades, we proposed some neural network based deep learning architectures for the <a href=https://en.wikipedia.org/wiki/Transliteration>transliteration of named entities</a>. Our transliteration systems adapt two different neural machine translation (NMT) frameworks : <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent neural network</a> and convolutional sequence to sequence based NMT. It is shown that our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> provides quite satisfactory results when it comes to multi lingual machine transliteration. Our submitted runs are an ensemble of different <a href=https://en.wikipedia.org/wiki/Transliteration>transliteration systems</a> for all the language pairs. In the NEWS 2018 Shared Task on <a href=https://en.wikipedia.org/wiki/Transliteration>Transliteration</a>, our method achieves top performance for the EnPe and PeEn language pairs and comparable results for other cases.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3931.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3931 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3931 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3931/>A Neural Approach to Language Variety Translation</a></strong><br><a href=/people/m/marta-r-costa-jussa/>Marta R. Costa-jussà</a>
|
<a href=/people/m/marcos-zampieri/>Marcos Zampieri</a>
|
<a href=/people/s/santanu-pal/>Santanu Pal</a><br><a href=/volumes/W18-39/ class=text-muted>Proceedings of the Fifth Workshop on NLP for Similar Languages, Varieties and Dialects (VarDial 2018)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3931><div class="card-body p-3 small">In this paper we present the first neural-based machine translation system trained to translate between standard national varieties of the same language. We take the pair <a href=https://en.wikipedia.org/wiki/Brazilian_Portuguese>Brazilian-European Portuguese</a> as an example and compare the performance of this method to a phrase-based statistical machine translation system. We report a performance improvement of 0.9 BLEU points in translating from European to Brazilian Portuguese and 0.2 BLEU points when translating in the opposite direction. We also carried out a human evaluation experiment with native speakers of <a href=https://en.wikipedia.org/wiki/Brazilian_Portuguese>Brazilian Portuguese</a> which indicates that humans prefer the output produced by the neural-based system in comparison to the statistical system.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6468.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6468 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6468 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6468/>A Transformer-Based Multi-Source Automatic Post-Editing System</a></strong><br><a href=/people/s/santanu-pal/>Santanu Pal</a>
|
<a href=/people/n/nico-herbig/>Nico Herbig</a>
|
<a href=/people/a/antonio-kruger/>Antonio Krüger</a>
|
<a href=/people/j/josef-van-genabith/>Josef van Genabith</a><br><a href=/volumes/W18-64/ class=text-muted>Proceedings of the Third Conference on Machine Translation: Shared Task Papers</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6468><div class="card-body p-3 small">This paper presents our EnglishGerman Automatic Post-Editing (APE) system submitted to the APE Task organized at WMT 2018 (Chatterjee et al., 2018). The proposed model is an extension of the transformer architecture : two separate self-attention-based encoders encode the machine translation output (mt) and the source (src), followed by a joint encoder that attends over a combination of these two encoded sequences (encsrc and encmt) for generating the post-edited sentence. We compare this multi-source architecture (i.e, <a href=https://en.wikipedia.org/wiki/Source_code>src</a>, mt pe) to a monolingual transformer (i.e., mt pe) model and an ensemble combining the multi-source src, mt pe and single-source mt pe models. For both the PBSMT and the NMT task, the <a href=https://en.wikipedia.org/wiki/Statistical_ensemble_(mathematical_physics)>ensemble</a> yields the best results, followed by the multi-source model and last the single-source approach. Our best <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>, the <a href=https://en.wikipedia.org/wiki/Ensemble_cast>ensemble</a>, achieves a BLEU score of 66.16 and 74.22 for the PBSMT and NMT task, respectively.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Santanu+Pal" title="Search for 'Santanu Pal' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/j/josef-van-genabith/ class=align-middle>Josef van Genabith</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/people/n/nico-herbig/ class=align-middle>Nico Herbig</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/a/antonio-kruger/ class=align-middle>Antonio Krüger</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/m/marcos-zampieri/ class=align-middle>Marcos Zampieri</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/t/tim-duwel/ class=align-middle>Tim Düwel</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/k/kalliopi-meladaki/ class=align-middle>Kalliopi Meladaki</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/mahsa-monshizadeh/ class=align-middle>Mahsa Monshizadeh</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/v/vladislav-hnatovskiy/ class=align-middle>Vladislav Hnatovskiy</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/soumyadeep-kundu/ class=align-middle>Soumyadeep Kundu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/sayantan-paul/ class=align-middle>Sayantan Paul</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/marta-r-costa-jussa/ class=align-middle>Marta R. Costa-jussà</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/hongfei-xu/ class=align-middle>Hongfei Xu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/sudip-kumar-naskar/ class=align-middle>Sudip Kumar Naskar</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/ws/ class=align-middle>WS</a><span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/venues/acl/ class=align-middle>ACL</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/coling/ class=align-middle>COLING</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/wat/ class=align-middle>WAT</a><span class="badge badge-secondary align-middle ml-2">1</span></li></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>