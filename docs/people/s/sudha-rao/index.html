<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Sudha Rao - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Sudha</span> <span class=font-weight-bold>Rao</span></h2><hr><div class=row><div class=col-lg-9><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.340.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--340 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.340 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.naacl-main.340" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.340/>Ask what’s missing and what’s useful : Improving Clarification Question Generation using Global Knowledge</a></strong><br><a href=/people/b/bodhisattwa-prasad-majumder/>Bodhisattwa Prasad Majumder</a>
|
<a href=/people/s/sudha-rao/>Sudha Rao</a>
|
<a href=/people/m/michel-galley/>Michel Galley</a>
|
<a href=/people/j/julian-mcauley/>Julian McAuley</a><br><a href=/volumes/2021.naacl-main/ class=text-muted>Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--340><div class="card-body p-3 small">The ability to generate clarification questions i.e., questions that identify useful missing information in a given context, is important in reducing <a href=https://en.wikipedia.org/wiki/Ambiguity>ambiguity</a>. Humans use previous experience with similar contexts to form a global view and compare it to the given context to ascertain what is missing and what is useful in the context. Inspired by this, we propose a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> for clarification question generation where we first identify what is missing by taking a difference between the global and the local view and then train a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> to identify what is useful and generate a question about it. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms several <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a> as judged by both <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>automatic metrics</a> and <a href=https://en.wikipedia.org/wiki/Human>humans</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.381.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--381 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.381 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.naacl-main.381" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.381/>Enriching Transformers with Structured Tensor-Product Representations for Abstractive Summarization</a></strong><br><a href=/people/y/yichen-jiang/>Yichen Jiang</a>
|
<a href=/people/a/asli-celikyilmaz/>Asli Celikyilmaz</a>
|
<a href=/people/p/paul-smolensky/>Paul Smolensky</a>
|
<a href=/people/p/paul-soulos/>Paul Soulos</a>
|
<a href=/people/s/sudha-rao/>Sudha Rao</a>
|
<a href=/people/h/hamid-palangi/>Hamid Palangi</a>
|
<a href=/people/r/roland-fernandez/>Roland Fernandez</a>
|
<a href=/people/c/caitlin-smith/>Caitlin Smith</a>
|
<a href=/people/m/mohit-bansal/>Mohit Bansal</a>
|
<a href=/people/j/jianfeng-gao/>Jianfeng Gao</a><br><a href=/volumes/2021.naacl-main/ class=text-muted>Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--381><div class="card-body p-3 small">Abstractive summarization, the task of generating a concise summary of input documents, requires : (1) reasoning over the source document to determine the salient pieces of information scattered across the long document, and (2) composing a cohesive text by reconstructing these salient facts into a shorter summary that faithfully reflects the complex relations connecting these facts. In this paper, we adapt TP-Transformer (Schlag et al., 2019), an architecture that enriches the original Transformer (Vaswani et al., 2017) with the explicitly compositional Tensor Product Representation (TPR), for the task of abstractive summarization. The key feature of our model is a structural bias that we introduce by encoding two separate representations for each token to represent the syntactic structure (with role vectors) and semantic content (with filler vectors) separately. The <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> then binds the role and filler vectors into the TPR as the layer output. We argue that the structured intermediate representations enable the model to take better control of the contents (salient facts) and structures (the syntax that connects the facts) when generating the summary. Empirically, we show that our TP-Transformer outperforms the Transformer and the original TP-Transformer significantly on several abstractive summarization datasets based on both automatic and human evaluations. On several syntactic and semantic probing tasks, we demonstrate the emergent structural information in the role vectors and the performance gain by information specificity of the role vectors and improved syntactic interpretability in the TPR layer outputs. (Code and models are available at https://github.com/jiangycTarheel/TPT-Summ)</div></div><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1489.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1489 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1489 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1489/>Comparing and Developing Tools to Measure the Readability of Domain-Specific Texts</a></strong><br><a href=/people/e/elissa-redmiles/>Elissa Redmiles</a>
|
<a href=/people/l/lisa-maszkiewicz/>Lisa Maszkiewicz</a>
|
<a href=/people/e/emily-hwang/>Emily Hwang</a>
|
<a href=/people/d/dhruv-kuchhal/>Dhruv Kuchhal</a>
|
<a href=/people/e/everest-liu/>Everest Liu</a>
|
<a href=/people/m/miraida-morales/>Miraida Morales</a>
|
<a href=/people/d/denis-peskov/>Denis Peskov</a>
|
<a href=/people/s/sudha-rao/>Sudha Rao</a>
|
<a href=/people/r/rock-stevens/>Rock Stevens</a>
|
<a href=/people/k/kristina-gligoric/>Kristina Gligorić</a>
|
<a href=/people/s/sean-kross/>Sean Kross</a>
|
<a href=/people/m/michelle-mazurek/>Michelle Mazurek</a>
|
<a href=/people/h/hal-daume-iii/>Hal Daumé III</a><br><a href=/volumes/D19-1/ class=text-muted>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1489><div class="card-body p-3 small">The <a href=https://en.wikipedia.org/wiki/Readability>readability</a> of a digital text can influence people&#8217;s ability to learn new things about a range topics from <a href=https://en.wikipedia.org/wiki/Web_resource>digital resources</a> (e.g., <a href=https://en.wikipedia.org/wiki/Wikipedia>Wikipedia</a>, <a href=https://en.wikipedia.org/wiki/WebMD>WebMD</a>). Readability also impacts <a href=https://en.wikipedia.org/wiki/Search_engine_results_page>search rankings</a>, and is used to evaluate the performance of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP systems</a>. Despite this, we lack a thorough understanding of how to validly measure <a href=https://en.wikipedia.org/wiki/Readability>readability</a> at scale, especially for domain-specific texts. In this work, we present a comparison of the <a href=https://en.wikipedia.org/wiki/Validity_(statistics)>validity</a> of well-known readability measures and introduce a novel approach, Smart Cloze, which is designed to address shortcomings of existing measures. We compare these approaches across four different corpora : crowdworker-generated stories, Wikipedia articles, security and privacy advice, and health information. On these corpora, we evaluate the convergent and content validity of each measure, and detail tradeoffs in score precision, domain-specificity, and participant burden. These results provide a foundation for more accurate readability measurements and better evaluation of new <a href=https://en.wikipedia.org/wiki/Natural-language_processing>natural-language-processing systems</a> and tools.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5604.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5604 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5604 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5604/>Generating a Common Question from Multiple Documents using Multi-source Encoder-Decoder Models</a></strong><br><a href=/people/w/woon-sang-cho/>Woon Sang Cho</a>
|
<a href=/people/y/yizhe-zhang/>Yizhe Zhang</a>
|
<a href=/people/s/sudha-rao/>Sudha Rao</a>
|
<a href=/people/c/chris-brockett/>Chris Brockett</a>
|
<a href=/people/s/sungjin-lee/>Sungjin Lee</a><br><a href=/volumes/D19-56/ class=text-muted>Proceedings of the 3rd Workshop on Neural Generation and Translation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5604><div class="card-body p-3 small">Ambiguous user queries in <a href=https://en.wikipedia.org/wiki/Web_search_engine>search engines</a> result in the retrieval of documents that often span multiple topics. One potential solution is for the <a href=https://en.wikipedia.org/wiki/Web_search_engine>search engine</a> to generate multiple refined queries, each of which relates to a subset of the documents spanning the same topic. A preliminary step towards this goal is to generate a question that captures common concepts of multiple documents. We propose a new task of generating common question from multiple documents and present simple variant of an existing multi-source encoder-decoder framework, called the Multi-Source Question Generator (MSQG). We first train an RNN-based single encoder-decoder generator from (single document, question) pairs. At test time, given multiple documents, the Distribute step of our MSQG model predicts target word distributions for each document using the trained <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a>. The Aggregate step aggregates these <a href=https://en.wikipedia.org/wiki/Probability_distribution>distributions</a> to generate a common question. This simple yet effective strategy significantly outperforms several existing baseline models applied to the new task when evaluated using automated metrics and human judgments on the MS-MARCO-QA dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3619 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3619 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3619/>Controlling the Specificity of Clarification Question Generation</a></strong><br><a href=/people/y/yang-trista-cao/>Yang Trista Cao</a>
|
<a href=/people/s/sudha-rao/>Sudha Rao</a>
|
<a href=/people/h/hal-daume-iii/>Hal Daumé III</a><br><a href=/volumes/W19-36/ class=text-muted>Proceedings of the 2019 Workshop on Widening NLP</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3619><div class="card-body p-3 small">Unlike comprehension-style questions, clarification questions look for some missing information in a given context. However, without guidance, neural models for question generation, similar to dialog generation models, lead to generic and bland questions that cannot elicit useful information. We argue that controlling the level of specificity of the generated questions can have useful applications and propose a neural clarification question generation model for the same. We first train a classifier that annotates a clarification question with its level of specificity (generic or specific) to the given context. Our results on the Amazon questions dataset demonstrate that training a clarification question generation model on specificity annotated data can generate questions with varied levels of specificity to the given context.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1013.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1013 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1013 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N19-1013.Supplementary.pdf data-toggle=tooltip data-placement=top title=Supplementary><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/353418933 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1013" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1013/>Answer-based Adversarial Training for Generating Clarification Questions<span class=acl-fixed-case>A</span>nswer-based <span class=acl-fixed-case>A</span>dversarial <span class=acl-fixed-case>T</span>raining for <span class=acl-fixed-case>G</span>enerating <span class=acl-fixed-case>C</span>larification <span class=acl-fixed-case>Q</span>uestions</a></strong><br><a href=/people/s/sudha-rao/>Sudha Rao</a>
|
<a href=/people/h/hal-daume-iii/>Hal Daumé III</a><br><a href=/volumes/N19-1/ class=text-muted>Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1013><div class="card-body p-3 small">We present an approach for generating clarification questions with the goal of eliciting new information that would make the given textual context more complete. We propose that modeling hypothetical answers (to clarification questions) as <a href=https://en.wikipedia.org/wiki/Latent_variable>latent variables</a> can guide our approach into generating more useful clarification questions. We develop a Generative Adversarial Network (GAN) where the generator is a sequence-to-sequence model and the discriminator is a utility function that models the value of updating the context with the answer to the clarification question. We evaluate on two datasets, using both automatic metrics and human judgments of usefulness, specificity and relevance, showing that our approach outperforms both a retrieval-based model and ablations that exclude the utility model and the adversarial training.</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1012.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1012 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1012 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N18-1012.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N18-1012" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N18-1012/>Dear Sir or Madam, May I Introduce the GYAFC Dataset : Corpus, Benchmarks and Metrics for Formality Style Transfer<span class=acl-fixed-case>I</span> Introduce the <span class=acl-fixed-case>GYAFC</span> Dataset: Corpus, Benchmarks and Metrics for Formality Style Transfer</a></strong><br><a href=/people/s/sudha-rao/>Sudha Rao</a>
|
<a href=/people/j/joel-tetreault/>Joel Tetreault</a><br><a href=/volumes/N18-1/ class=text-muted>Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1012><div class="card-body p-3 small">Style transfer is the task of automatically transforming a piece of text in one particular style into another. A major barrier to progress in this field has been a lack of <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training and evaluation datasets</a>, as well as benchmarks and <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>automatic metrics</a>. In this work, we create the largest <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> for a particular stylistic transfer (formality) and show that techniques from the machine translation community can serve as strong baselines for future work. We also discuss challenges of using <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>automatic metrics</a>.</div></div><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2315.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2315 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2315 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-2315/>Biomedical Event Extraction using Abstract Meaning Representation<span class=acl-fixed-case>A</span>bstract <span class=acl-fixed-case>M</span>eaning <span class=acl-fixed-case>R</span>epresentation</a></strong><br><a href=/people/s/sudha-rao/>Sudha Rao</a>
|
<a href=/people/d/daniel-marcu/>Daniel Marcu</a>
|
<a href=/people/k/kevin-knight/>Kevin Knight</a>
|
<a href=/people/h/hal-daume-iii/>Hal Daumé III</a><br><a href=/volumes/W17-23/ class=text-muted>BioNLP 2017</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2315><div class="card-body p-3 small">We propose a novel, Abstract Meaning Representation (AMR) based approach to identifying molecular events / interactions in biomedical text. Our key contributions are : (1) an empirical validation of our hypothesis that an event is a subgraph of the AMR graph, (2) a neural network-based model that identifies such an event subgraph given an AMR, and (3) a distant supervision based approach to gather additional training data. We evaluate our approach on the 2013 Genia Event Extraction dataset and show promising results.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5400.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5400/>Proceedings of the First Workshop on Building Linguistically Generalizable <span class=acl-fixed-case>NLP</span> Systems</a></strong><br><a href=/people/e/emily-m-bender/>Emily Bender</a>
|
<a href=/people/h/hal-daume-iii/>Hal Daumé III</a>
|
<a href=/people/a/allyson-ettinger/>Allyson Ettinger</a>
|
<a href=/people/s/sudha-rao/>Sudha Rao</a><br><a href=/volumes/W17-54/ class=text-muted>Proceedings of the First Workshop on Building Linguistically Generalizable NLP Systems</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5401.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5401 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5401 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5401/>Towards Linguistically Generalizable NLP Systems : A Workshop and Shared Task<span class=acl-fixed-case>NLP</span> Systems: A Workshop and Shared Task</a></strong><br><a href=/people/a/allyson-ettinger/>Allyson Ettinger</a>
|
<a href=/people/s/sudha-rao/>Sudha Rao</a>
|
<a href=/people/h/hal-daume-iii/>Hal Daumé III</a>
|
<a href=/people/e/emily-m-bender/>Emily M. Bender</a><br><a href=/volumes/W17-54/ class=text-muted>Proceedings of the First Workshop on Building Linguistically Generalizable NLP Systems</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5401><div class="card-body p-3 small">This paper presents a summary of the first Workshop on Building Linguistically Generalizable Natural Language Processing Systems, and the associated Build It Break It, The Language Edition shared task. The goal of this workshop was to bring together researchers in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> and <a href=https://en.wikipedia.org/wiki/Linguistics>linguistics</a> with a carefully designed shared task aimed at testing the generalizability of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP systems</a> beyond the distributions of their training data. We describe the motivation, setup, and participation of the shared task, provide discussion of some highlighted results, and discuss lessons learned.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Sudha+Rao" title="Search for 'Sudha Rao' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/h/hal-daume-iii/ class=align-middle>Hal Daumé III</a>
<span class="badge badge-secondary align-middle ml-2">6</span></li><li class=list-group-item><a href=/people/e/emily-m-bender/ class=align-middle>Emily M. Bender</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/a/allyson-ettinger/ class=align-middle>Allyson Ettinger</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/d/daniel-marcu/ class=align-middle>Daniel Marcu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/kevin-knight/ class=align-middle>Kevin Knight</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/e/elissa-redmiles/ class=align-middle>Elissa Redmiles</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/lisa-maszkiewicz/ class=align-middle>Lisa Maszkiewicz</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/e/emily-hwang/ class=align-middle>Emily Hwang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/dhruv-kuchhal/ class=align-middle>Dhruv Kuchhal</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/e/everest-liu/ class=align-middle>Everest Liu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/miraida-morales/ class=align-middle>Miraida Morales</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/denis-peskov/ class=align-middle>Denis Peskov</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/rock-stevens/ class=align-middle>Rock Stevens</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/kristina-gligoric/ class=align-middle>Kristina Gligorić</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/sean-kross/ class=align-middle>Sean Kross</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/michelle-mazurek/ class=align-middle>Michelle Mazurek</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/w/woon-sang-cho/ class=align-middle>Woon Sang Cho</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yizhe-zhang/ class=align-middle>Yizhe Zhang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/chris-brockett/ class=align-middle>Chris Brockett</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/sungjin-lee/ class=align-middle>Sungjin Lee</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/b/bodhisattwa-prasad-majumder/ class=align-middle>Bodhisattwa Prasad Majumder</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/michel-galley/ class=align-middle>Michel Galley</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/julian-mcauley/ class=align-middle>Julian McAuley</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yichen-jiang/ class=align-middle>Yichen Jiang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/asli-celikyilmaz/ class=align-middle>Asli Celikyilmaz</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/p/paul-smolensky/ class=align-middle>Paul Smolensky</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/p/paul-soulos/ class=align-middle>Paul Soulos</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/hamid-palangi/ class=align-middle>Hamid Palangi</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/roland-fernandez/ class=align-middle>Roland Fernandez</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/caitlin-smith/ class=align-middle>Caitlin Smith</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/mohit-bansal/ class=align-middle>Mohit Bansal</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jianfeng-gao/ class=align-middle>Jianfeng Gao</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yang-trista-cao/ class=align-middle>Yang Trista Cao</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/joel-tetreault/ class=align-middle>Joel Tetreault</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/ws/ class=align-middle>WS</a><span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/venues/naacl/ class=align-middle>NAACL</a><span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/venues/emnlp/ class=align-middle>EMNLP</a><span class="badge badge-secondary align-middle ml-2">2</span></li></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>