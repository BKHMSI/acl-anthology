<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Stephen Clark - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Stephen</span> <span class=font-weight-bold>Clark</span></h2><hr><div class=row><div class=col-lg-9><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1233.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1233 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1233 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1233/>Neural Generative Rhetorical Structure Parsing</a></strong><br><a href=/people/a/amandla-mabona/>Amandla Mabona</a>
|
<a href=/people/l/laura-rimell/>Laura Rimell</a>
|
<a href=/people/s/stephen-clark/>Stephen Clark</a>
|
<a href=/people/a/andreas-vlachos/>Andreas Vlachos</a><br><a href=/volumes/D19-1/ class=text-muted>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1233><div class="card-body p-3 small">Rhetorical structure trees have been shown to be useful for several document-level tasks including <a href=https://en.wikipedia.org/wiki/Automatic_summarization>summarization</a> and <a href=https://en.wikipedia.org/wiki/Document_classification>document classification</a>. Previous approaches to RST parsing have used <a href=https://en.wikipedia.org/wiki/Discriminative_model>discriminative models</a> ; however, these are less sample efficient than <a href=https://en.wikipedia.org/wiki/Generative_model>generative models</a>, and RST parsing datasets are typically small. In this paper, we present the first <a href=https://en.wikipedia.org/wiki/Generative_model>generative model</a> for RST parsing. Our <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> is a document-level RNN grammar (RNNG) with a <a href=https://en.wikipedia.org/wiki/Top-down_and_bottom-up_design>bottom-up traversal order</a>. We show that, for our parser&#8217;s traversal order, previous beam search algorithms for RNNGs have a left-branching bias which is ill-suited for RST parsing. We develop a novel beam search algorithm that keeps track of both structure-and word-generating actions without exhibit-ing this branching bias and results in absolute improvements of 6.8 and 2.9 on unlabelled and labelled F1 over previous <a href=https://en.wikipedia.org/wiki/Algorithm>algorithms</a>. Overall, our <a href=https://en.wikipedia.org/wiki/Generative_model>generative model</a> outperforms a <a href=https://en.wikipedia.org/wiki/Discriminative_model>discriminative model</a> with the same <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> by 2.6 <a href=https://en.wikipedia.org/wiki/F-number>F1points</a> and achieves performance comparable to the state-of-the-art, outperforming all published parsers from a recent replication study that do not use additional training data</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1223.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1223 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1223 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/364735719 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1223/>Factorising AMR generation through syntax<span class=acl-fixed-case>AMR</span> generation through syntax</a></strong><br><a href=/people/k/kris-cao/>Kris Cao</a>
|
<a href=/people/s/stephen-clark/>Stephen Clark</a><br><a href=/volumes/N19-1/ class=text-muted>Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1223><div class="card-body p-3 small">Generating from Abstract Meaning Representation (AMR) is an underspecified problem, as many syntactic decisions are not specified by the semantic graph. To explicitly account for this variation, we break down generating from AMR into two steps : first generate a <a href=https://en.wikipedia.org/wiki/Syntax>syntactic structure</a>, and then generate the surface form. We show that decomposing the generation process this way leads to state-of-the-art single model performance generating from AMR without additional unlabelled data. We also demonstrate that we can generate meaning-preserving syntactic paraphrases of the same AMR graph, as judged by humans.</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1132.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1132 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1132 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1132.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285803729 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-1132/>LSTMs Can Learn Syntax-Sensitive Dependencies Well, But <a href=https://en.wikipedia.org/wiki/Structure_(mathematical_logic)>Modeling Structure</a> Makes Them Better<span class=acl-fixed-case>LSTM</span>s Can Learn Syntax-Sensitive Dependencies Well, But Modeling Structure Makes Them Better</a></strong><br><a href=/people/a/adhiguna-kuncoro/>Adhiguna Kuncoro</a>
|
<a href=/people/c/chris-dyer/>Chris Dyer</a>
|
<a href=/people/j/john-hale/>John Hale</a>
|
<a href=/people/d/dani-yogatama/>Dani Yogatama</a>
|
<a href=/people/s/stephen-clark/>Stephen Clark</a>
|
<a href=/people/p/phil-blunsom/>Phil Blunsom</a><br><a href=/volumes/P18-1/ class=text-muted>Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1132><div class="card-body p-3 small">Language exhibits hierarchical structure, but recent work using a subject-verb agreement diagnostic argued that state-of-the-art language models, LSTMs, fail to learn long-range syntax sensitive dependencies. Using the same diagnostic, we show that, in fact, LSTMs do succeed in learning such dependenciesprovided they have enough capacity. We then explore whether <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> that have access to explicit syntactic information learn <a href=https://en.wikipedia.org/wiki/Agreement_(linguistics)>agreement</a> more effectively, and how the way in which this structural information is incorporated into the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> impacts performance. We find that the mere presence of syntactic information does not improve <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>, but when model architecture is determined by <a href=https://en.wikipedia.org/wiki/Syntax>syntax</a>, <a href=https://en.wikipedia.org/wiki/Agreement_(linguistics)>number agreement</a> is improved. Further, we find that the choice of how syntactic structure is built affects how well <a href=https://en.wikipedia.org/wiki/Agreement_(linguistics)>number agreement</a> is learned : <a href=https://en.wikipedia.org/wiki/Top-down_and_bottom-up_design>top-down construction</a> outperforms <a href=https://en.wikipedia.org/wiki/Top-down_and_bottom-up_design>left-corner and bottom-up variants</a> in capturing non-local structural dependencies.</div></div><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/Q17-1002.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-Q17-1002 data-toggle=collapse aria-expanded=false aria-controls=abstract-Q17-1002 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234954554 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/Q17-1002/>Visually Grounded and Textual Semantic Models Differentially Decode Brain Activity Associated with Concrete and Abstract Nouns</a></strong><br><a href=/people/a/andrew-j-anderson/>Andrew J. Anderson</a>
|
<a href=/people/d/douwe-kiela/>Douwe Kiela</a>
|
<a href=/people/s/stephen-clark/>Stephen Clark</a>
|
<a href=/people/m/massimo-poesio/>Massimo Poesio</a><br><a href=/volumes/Q17-1/ class=text-muted>Transactions of the Association for Computational Linguistics, Volume 5</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-Q17-1002><div class="card-body p-3 small">Important advances have recently been made using computational semantic models to decode brain activity patterns associated with <a href=https://en.wikipedia.org/wiki/Concept>concepts</a> ; however, this work has almost exclusively focused on concrete nouns. How well these <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> extend to decoding <a href=https://en.wikipedia.org/wiki/Noun>abstract nouns</a> is largely unknown. We address this question by applying state-of-the-art computational models to decode functional Magnetic Resonance Imaging (fMRI) activity patterns, elicited by participants reading and imagining a diverse set of both concrete and abstract nouns. One of the <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> we use is linguistic, exploiting the recent word2vec skipgram approach trained on <a href=https://en.wikipedia.org/wiki/Wikipedia>Wikipedia</a>. The <a href=https://en.wikipedia.org/wiki/Second>second</a> is visually grounded, using <a href=https://en.wikipedia.org/wiki/Deep_learning>deep convolutional neural networks</a> trained on <a href=https://en.wikipedia.org/wiki/Google_Images>Google Images</a>. Dual coding theory considers concrete concepts to be encoded in the brain both linguistically and visually, and abstract concepts only linguistically. Splitting the fMRI data according to human concreteness ratings, we indeed observe that both models significantly decode the most concrete nouns ; however, accuracy is significantly greater using the text-based models for the most abstract nouns. More generally this confirms that current <a href=https://en.wikipedia.org/wiki/Computational_model>computational models</a> are sufficiently advanced to assist in investigating the representational structure of <a href=https://en.wikipedia.org/wiki/Concept>abstract concepts</a> in the brain.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1113.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1113 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1113 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/238235824 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D17-1113/>Speaking, Seeing, Understanding : Correlating semantic models with conceptual representation in the brain</a></strong><br><a href=/people/l/luana-bulat/>Luana Bulat</a>
|
<a href=/people/s/stephen-clark/>Stephen Clark</a>
|
<a href=/people/e/ekaterina-shutova/>Ekaterina Shutova</a><br><a href=/volumes/D17-1/ class=text-muted>Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1113><div class="card-body p-3 small">Research in <a href=https://en.wikipedia.org/wiki/Computational_semantics>computational semantics</a> is increasingly guided by our understanding of human semantic processing. However, semantic models are typically studied in the context of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing system</a> performance. In this paper, we present a systematic evaluation and comparison of a range of widely-used, state-of-the-art semantic models in their ability to predict patterns of conceptual representation in the human brain. Our results provide new insights both for the design of computational semantic models and for further research in <a href=https://en.wikipedia.org/wiki/Cognitive_neuroscience>cognitive neuroscience</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2029.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2029 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2029 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2029/>Latent Variable Dialogue Models and their Diversity</a></strong><br><a href=/people/k/kris-cao/>Kris Cao</a>
|
<a href=/people/s/stephen-clark/>Stephen Clark</a><br><a href=/volumes/E17-2/ class=text-muted>Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2029><div class="card-body p-3 small">We present a dialogue generation model that directly captures the variability in possible responses to a given input, which reduces the &#8216;boring output&#8217; issue of deterministic dialogue models. Experiments show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> generates more diverse outputs than baseline models, and also generates more consistently acceptable output than sampling from a deterministic encoder-decoder model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2084.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2084 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2084 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2084/>Modelling metaphor with attribute-based semantics</a></strong><br><a href=/people/l/luana-bulat/>Luana Bulat</a>
|
<a href=/people/s/stephen-clark/>Stephen Clark</a>
|
<a href=/people/e/ekaterina-shutova/>Ekaterina Shutova</a><br><a href=/volumes/E17-2/ class=text-muted>Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2084><div class="card-body p-3 small">One of the key problems in computational metaphor modelling is finding the optimal level of abstraction of semantic representations, such that these are able to capture and generalise metaphorical mechanisms. In this paper we present the first metaphor identification method that uses <a href=https://en.wikipedia.org/wiki/Representation_(mathematics)>representations</a> constructed from <a href=https://en.wikipedia.org/wiki/Norm_(philosophy)>property norms</a>. Such <a href=https://en.wikipedia.org/wiki/Social_norm>norms</a> have been previously shown to provide a cognitively plausible representation of concepts in terms of <a href=https://en.wikipedia.org/wiki/Semantics>semantic properties</a>. Our results demonstrate that such property-based semantic representations provide a suitable model of cross-domain knowledge projection in metaphors, outperforming standard distributional models on a metaphor identification task.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Stephen+Clark" title="Search for 'Stephen Clark' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/l/luana-bulat/ class=align-middle>Luana Bulat</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/e/ekaterina-shutova/ class=align-middle>Ekaterina Shutova</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/k/kris-cao/ class=align-middle>Kris Cao</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/a/andrew-j-anderson/ class=align-middle>Andrew J. Anderson</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/douwe-kiela/ class=align-middle>Douwe Kiela</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/m/massimo-poesio/ class=align-middle>Massimo Poesio</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/amandla-mabona/ class=align-middle>Amandla Mabona</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/laura-rimell/ class=align-middle>Laura Rimell</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/andreas-vlachos/ class=align-middle>Andreas Vlachos</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/adhiguna-kuncoro/ class=align-middle>Adhiguna Kuncoro</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/chris-dyer/ class=align-middle>Chris Dyer</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/john-hale/ class=align-middle>John Hale</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/dani-yogatama/ class=align-middle>Dani Yogatama</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/p/phil-blunsom/ class=align-middle>Phil Blunsom</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/emnlp/ class=align-middle>EMNLP</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/eacl/ class=align-middle>EACL</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/tacl/ class=align-middle>TACL</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/naacl/ class=align-middle>NAACL</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/acl/ class=align-middle>ACL</a><span class="badge badge-secondary align-middle ml-2">1</span></li></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>