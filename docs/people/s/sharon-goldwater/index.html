<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Sharon Goldwater - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Sharon</span> <span class=font-weight-bold>Goldwater</span></h2><hr><div class=row><div class=col-lg-9><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.127.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--127 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.127 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><span class="align-middle mr-1" data-toggle=tooltip data-placement=bottom title="Honorable Mention for Best Long Paper"><i class="fas fa-award"></i></span></span>
<span class=d-block><strong><a class=align-middle href=/2021.eacl-main.127/>A phonetic model of non-native spoken word processing</a></strong><br><a href=/people/y/yevgen-matusevych/>Yevgen Matusevych</a>
|
<a href=/people/h/herman-kamper/>Herman Kamper</a>
|
<a href=/people/t/thomas-schatz/>Thomas Schatz</a>
|
<a href=/people/n/naomi-feldman/>Naomi Feldman</a>
|
<a href=/people/s/sharon-goldwater/>Sharon Goldwater</a><br><a href=/volumes/2021.eacl-main/ class=text-muted>Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--127><div class="card-body p-3 small">Non-native speakers show difficulties with spoken word processing. Many studies attribute these difficulties to imprecise phonological encoding of words in the lexical memory. We test an alternative hypothesis : that some of these difficulties can arise from the non-native speakers&#8217; phonetic perception. We train a computational model of phonetic learning, which has no access to <a href=https://en.wikipedia.org/wiki/Phonology>phonology</a>, on either one or two languages. We first show that the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> exhibits predictable behaviors on phone-level and word-level discrimination tasks. We then test the model on a spoken word processing task, showing that <a href=https://en.wikipedia.org/wiki/Phonology>phonology</a> may not be necessary to explain some of the word processing effects observed in non-native speakers. We run an additional analysis of the model&#8217;s lexical representation space, showing that the two training languages are not fully separated in that space, similarly to the languages of a bilingual human speaker.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.sigmorphon-1.9.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--sigmorphon-1--9 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.sigmorphon-1.9 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.sigmorphon-1.9/>Adaptor Grammars for Unsupervised Paradigm Clustering<span class=acl-fixed-case>A</span>daptor <span class=acl-fixed-case>G</span>rammars for Unsupervised Paradigm Clustering</a></strong><br><a href=/people/k/kate-mccurdy/>Kate McCurdy</a>
|
<a href=/people/s/sharon-goldwater/>Sharon Goldwater</a>
|
<a href=/people/a/adam-lopez/>Adam Lopez</a><br><a href=/volumes/2021.sigmorphon-1/ class=text-muted>Proceedings of the 18th SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--sigmorphon-1--9><div class="card-body p-3 small">This work describes the Edinburgh submission to the SIGMORPHON 2021 Shared Task 2 on unsupervised morphological paradigm clustering. Given raw text input, the task was to assign each token to a cluster with other tokens from the same paradigm. We use Adaptor Grammar segmentations combined with frequency-based heuristics to predict paradigm clusters. Our system achieved the highest average F1 score across 9 test languages, placing first out of 15 submissions.</div></div><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.acl-main.159.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--acl-main--159 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.acl-main.159 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.acl-main.159.Dataset.zip data-toggle=tooltip data-placement=top title=Dataset><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=http://slideslive.com/38929385 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.acl-main.159/>Inflecting When Thereâ€™s No Majority : Limitations of Encoder-Decoder Neural Networks as Cognitive Models for German Plurals<span class=acl-fixed-case>G</span>erman Plurals</a></strong><br><a href=/people/k/kate-mccurdy/>Kate McCurdy</a>
|
<a href=/people/s/sharon-goldwater/>Sharon Goldwater</a>
|
<a href=/people/a/adam-lopez/>Adam Lopez</a><br><a href=/volumes/2020.acl-main/ class=text-muted>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--acl-main--159><div class="card-body p-3 small">Can <a href=https://en.wikipedia.org/wiki/Artificial_neural_network>artificial neural networks</a> learn to represent <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>inflectional morphology</a> and generalize to new words as <a href=https://en.wikipedia.org/wiki/Speech>human speakers</a> do? Kirov and Cotterell (2018) argue that the answer is yes : modern Encoder-Decoder (ED) architectures learn human-like behavior when inflecting English verbs, such as extending the regular past tense form /-(e)d/ to novel words. However, their work does not address the criticism raised by Marcus et al. (1995): that neural models may learn to extend not the regular, but the most frequent class and thus fail on tasks like German number inflection, where infrequent suffixes like /-s/ can still be productively generalized. To investigate this question, we first collect a new <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> from <a href=https://en.wikipedia.org/wiki/German_language>German speakers</a> (production and ratings of plural forms for novel nouns) that is designed to avoid sources of information unavailable to the ED model. The speaker data show high variability, and two <a href=https://en.wikipedia.org/wiki/Suffix>suffixes</a> evince &#8216;regular&#8217; behavior, appearing more often with phonologically atypical inputs. Encoder-decoder models do generalize the most frequently produced plural class, but do not show human-like variability or &#8216;regular&#8217; extension of these other plural markers. We conclude that modern neural models may still struggle with minority-class generalization.</div></div><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1418.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1418 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1418 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N19-1418.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/361822826 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1418" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1418/>Data Augmentation for Context-Sensitive Neural Lemmatization Using Inflection Tables and Raw Text</a></strong><br><a href=/people/t/toms-bergmanis/>Toms Bergmanis</a>
|
<a href=/people/s/sharon-goldwater/>Sharon Goldwater</a><br><a href=/volumes/N19-1/ class=text-muted>Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1418><div class="card-body p-3 small">Lemmatization aims to reduce the sparse data problem by relating the <a href=https://en.wikipedia.org/wiki/Inflection>inflected forms</a> of a word to its <a href=https://en.wikipedia.org/wiki/Dictionary>dictionary form</a>. Using <a href=https://en.wikipedia.org/wiki/Context_(language_use)>context</a> can help, both for unseen and ambiguous words. Yet most context-sensitive approaches require full lemma-annotated sentences for training, which may be scarce or unavailable in low-resource languages. In addition (as shown here), in a low-resource setting, a <a href=https://en.wikipedia.org/wiki/Lemmatizer>lemmatizer</a> can learn more from n labeled examples of distinct words (types) than from n (contiguous) labeled tokens, since the latter contain far fewer distinct types. To combine the efficiency of type-based learning with the benefits of context, we propose a way to train a context-sensitive lemmatizer with little or no labeled corpus data, using inflection tables from the UniMorph project and raw text examples from <a href=https://en.wikipedia.org/wiki/Wikipedia>Wikipedia</a> that provide sentence contexts for the unambiguous UniMorph examples. Despite these being unambiguous examples, the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> successfully generalizes from them, leading to improved results (both overall, and especially on unseen words) in comparison to a baseline that does not use <a href=https://en.wikipedia.org/wiki/Context_(language_use)>context</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1376.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1376 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1376 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-1376/>Are we there yet? Encoder-decoder neural networks as cognitive models of English past tense inflection<span class=acl-fixed-case>E</span>nglish past tense inflection</a></strong><br><a href=/people/m/maria-corkery/>Maria Corkery</a>
|
<a href=/people/y/yevgen-matusevych/>Yevgen Matusevych</a>
|
<a href=/people/s/sharon-goldwater/>Sharon Goldwater</a><br><a href=/volumes/P19-1/ class=text-muted>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1376><div class="card-body p-3 small">The <a href=https://en.wikipedia.org/wiki/Cognition>cognitive mechanisms</a> needed to account for the English past tense have long been a subject of debate in <a href=https://en.wikipedia.org/wiki/Linguistics>linguistics</a> and <a href=https://en.wikipedia.org/wiki/Cognitive_science>cognitive science</a>. Neural network models were proposed early on, but were shown to have clear flaws. Recently, however, Kirov and Cotterell (2018) showed that modern encoder-decoder (ED) models overcome many of these flaws. They also presented evidence that ED models demonstrate humanlike performance in a nonce-word task. Here, we look more closely at the behaviour of their <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> in this task. We find that (1) the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> exhibits instability across multiple simulations in terms of its correlation with human data, and (2) even when results are aggregated across simulations (treating each simulation as an individual human participant), the fit to the human data is not strongworse than an older rule-based model. These findings hold up through several alternative training regimes and <a href=https://en.wikipedia.org/wiki/Evaluation>evaluation measures</a>. Although other neural architectures might do better, we conclude that there is still insufficient evidence to claim that <a href=https://en.wikipedia.org/wiki/Artificial_neural_network>neural nets</a> are a good <a href=https://en.wikipedia.org/wiki/Cognitive_model>cognitive model</a> for this task.</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6101.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6101 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6101 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-6101" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-6101/>Inducing a lexicon of sociolinguistic variables from code-mixed text</a></strong><br><a href=/people/p/philippa-shoemark/>Philippa Shoemark</a>
|
<a href=/people/j/james-kirby/>James Kirby</a>
|
<a href=/people/s/sharon-goldwater/>Sharon Goldwater</a><br><a href=/volumes/W18-61/ class=text-muted>Proceedings of the 2018 EMNLP Workshop W-NUT: The 4th Workshop on Noisy User-generated Text</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6101><div class="card-body p-3 small">Sociolinguistics is often concerned with how variants of a linguistic item (e.g., nothing vs. nothin&#8217;) are used by different groups or in different situations. We introduce the task of inducing lexical variables from code-mixed text : that is, identifying equivalence pairs such as (football, fitba) along with their linguistic code (footballBritish, fitbaScottish). We adapt a framework for identifying gender-biased word pairs to this new task, and present results on three different pairs of <a href=https://en.wikipedia.org/wiki/List_of_dialects_of_English>English dialects</a>, using <a href=https://en.wikipedia.org/wiki/Twitter>tweets</a> as the code-mixed text. Our system achieves <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>precision</a> of over 70 % for two of these three <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>, and produces useful results even without extensive parameter tuning. Our success in adapting this <a href=https://en.wikipedia.org/wiki/Conceptual_framework>framework</a> from <a href=https://en.wikipedia.org/wiki/Gender>gender</a> to <a href=https://en.wikipedia.org/wiki/Variety_(linguistics)>language variety</a> suggests that it could be used to discover other types of analogous pairs as well.<i>nothing</i> vs. <i>nothin&#8217;</i>) are used by different groups or in different situations. We introduce the task of inducing lexical variables from code-mixed text: that is, identifying equivalence pairs such as (<i>football</i>, <i>fitba</i>) along with their linguistic code (<i>football</i>&#8594;British, <i>fitba</i>&#8594;Scottish). We adapt a framework for identifying gender-biased word pairs to this new task, and present results on three different pairs of English dialects, using tweets as the code-mixed text. Our system achieves precision of over 70% for two of these three datasets, and produces useful results even without extensive parameter tuning. Our success in adapting this framework from gender to language variety suggests that it could be used to discover other types of analogous pairs as well.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1126.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1126 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1126 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-1126/>Context Sensitive Neural Lemmatization with Lematus<span class=acl-fixed-case>L</span>ematus</a></strong><br><a href=/people/t/toms-bergmanis/>Toms Bergmanis</a>
|
<a href=/people/s/sharon-goldwater/>Sharon Goldwater</a><br><a href=/volumes/N18-1/ class=text-muted>Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1126><div class="card-body p-3 small">The main motivation for developing contextsensitive lemmatizers is to improve performance on unseen and ambiguous words. Yet previous systems have not carefully evaluated whether the use of <a href=https://en.wikipedia.org/wiki/Context_(language_use)>context</a> actually helps in these cases. We introduce Lematus, a <a href=https://en.wikipedia.org/wiki/Lemmatizer>lemmatizer</a> based on a standard encoder-decoder architecture, which incorporates character-level sentence context. We evaluate its lemmatization accuracy across 20 languages in both a full data setting and a lower-resource setting with 10k training examples in each language. In both settings, we show that including <a href=https://en.wikipedia.org/wiki/Context_(language_use)>context</a> significantly improves results against a context-free version of the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>. Context helps more for ambiguous words than for unseen words, though the latter has a greater effect on overall performance differences between languages. We also compare to three previous context-sensitive lemmatization systems, which all use pre-extracted edit trees as well as hand-selected features and/or additional sources of information such as tagged training data. Without using any of these, our context-sensitive model outperforms the best competitor system (Lemming) in the fulldata setting, and performs on par in the lowerresource setting.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2113.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2113 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2113 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-2113/>Evaluating Historical Text Normalization Systems : How Well Do They Generalize?</a></strong><br><a href=/people/a/alexander-robertson/>Alexander Robertson</a>
|
<a href=/people/s/sharon-goldwater/>Sharon Goldwater</a><br><a href=/volumes/N18-2/ class=text-muted>Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2113><div class="card-body p-3 small">We highlight several issues in the evaluation of historical text normalization systems that make it hard to tell how well these <a href=https://en.wikipedia.org/wiki/System>systems</a> would actually work in practicei.e., for new datasets or languages ; in comparison to more nave systems ; or as a preprocessing step for downstream NLP tools. We illustrate these issues and exemplify our proposed evaluation practices by comparing two neural models against a nave baseline system. We show that the neural models generalize well to unseen words in tests on five languages ; nevertheless, they provide no clear benefit over the nave baseline for downstream POS tagging of an English historical collection. We conclude that future work should include more rigorous <a href=https://en.wikipedia.org/wiki/Evaluation>evaluation</a>, including both intrinsic and extrinsic measures where possible.</div></div><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4607.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4607 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4607 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4607/>Spoken Term Discovery for <a href=https://en.wikipedia.org/wiki/Language_documentation>Language Documentation</a> using Translations</a></strong><br><a href=/people/a/antonios-anastasopoulos/>Antonios Anastasopoulos</a>
|
<a href=/people/s/sameer-bansal/>Sameer Bansal</a>
|
<a href=/people/d/david-chiang/>David Chiang</a>
|
<a href=/people/s/sharon-goldwater/>Sharon Goldwater</a>
|
<a href=/people/a/adam-lopez/>Adam Lopez</a><br><a href=/volumes/W17-46/ class=text-muted>Proceedings of the Workshop on Speech-Centric Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4607><div class="card-body p-3 small">Vast amounts of speech data collected for <a href=https://en.wikipedia.org/wiki/Language_documentation>language documentation</a> and research remain untranscribed and unsearchable, but often a small amount of speech may have text translations available. We present a <a href=https://en.wikipedia.org/wiki/Methodology>method</a> for partially labeling additional speech with translations in this <a href=https://en.wikipedia.org/wiki/Scenario>scenario</a>. We modify an unsupervised speech-to-translation alignment model and obtain prototype speech segments that match the translation words, which are in turn used to discover terms in the unlabelled data. We evaluate our method on a Spanish-English speech translation corpus and on two corpora of endangered languages, <a href=https://en.wikipedia.org/wiki/Arapaho_language>Arapaho</a> and Ainu, demonstrating its appropriateness and applicability in an actual very-low-resource scenario.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4908.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4908 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4908 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4908/>Topic and audience effects on distinctively Scottish vocabulary usage in Twitter data<span class=acl-fixed-case>S</span>cottish vocabulary usage in <span class=acl-fixed-case>T</span>witter data</a></strong><br><a href=/people/p/philippa-shoemark/>Philippa Shoemark</a>
|
<a href=/people/j/james-kirby/>James Kirby</a>
|
<a href=/people/s/sharon-goldwater/>Sharon Goldwater</a><br><a href=/volumes/W17-49/ class=text-muted>Proceedings of the Workshop on Stylistic Variation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4908><div class="card-body p-3 small">Sociolinguistic research suggests that speakers modulate their language style in response to their audience. Similar effects have recently been claimed to occur in the informal written context of <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a>, with users choosing less region-specific and non-standard vocabulary when addressing larger audiences. However, these studies have not carefully controlled for the possible confound of topic : that is, tweets addressed to a broad audience might also tend towards topics that engender a more <a href=https://en.wikipedia.org/wiki/Style_(manner_of_address)>formal style</a>. In addition, it is not clear to what extent previous results generalize to different samples of users. Using mixed-effects models, we show that audience and topic have independent effects on the rate of distinctively Scottish usage in two demographically distinct Twitter user samples. However, not all effects are consistent between the two groups, underscoring the importance of replicating studies on distinct user samples before drawing strong conclusions from social media data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1032.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1032 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1032 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1032/>From Segmentation to Analyses : a Probabilistic Model for Unsupervised Morphology Induction</a></strong><br><a href=/people/t/toms-bergmanis/>Toms Bergmanis</a>
|
<a href=/people/s/sharon-goldwater/>Sharon Goldwater</a><br><a href=/volumes/E17-1/ class=text-muted>Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1032><div class="card-body p-3 small">A major motivation for unsupervised morphological analysis is to reduce the sparse data problem in under-resourced languages. Most previous work focus on segmenting surface forms into their constituent morphs (taking : tak + ing), but surface form segmentation does not solve the sparse data problem as the analyses of take and taking are not connected to each other. We present a system that adapts the MorphoChains system (Narasimhan et al., 2015) to provide morphological analyses that aim to abstract over spelling differences in functionally similar morphs. This results in analyses that are not compelled to use all the orthographic material of a word (stopping : stop + ing) or limited to only that material (acidified : acid + ify + ed). On average across six typologically varied languages our <a href=https://en.wikipedia.org/wiki/System>system</a> has a similar or better F-score on EMMA (a measure of underlying morpheme accuracy) than three strong baselines ; moreover, the total number of distinct morphemes identified by our <a href=https://en.wikipedia.org/wiki/System>system</a> is on average 12.8 % lower than for Morfessor (Virpioja et al., 2013), a state-of-the-art surface segmentation system.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1116.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1116 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1116 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1116/>Aye or naw, whit dae ye hink? <a href=https://en.wikipedia.org/wiki/Scottish_independence>Scottish independence</a> and linguistic identity on <a href=https://en.wikipedia.org/wiki/Social_media>social media</a><span class=acl-fixed-case>S</span>cottish independence and linguistic identity on social media</a></strong><br><a href=/people/p/philippa-shoemark/>Philippa Shoemark</a>
|
<a href=/people/d/debnil-sur/>Debnil Sur</a>
|
<a href=/people/l/luke-shrimpton/>Luke Shrimpton</a>
|
<a href=/people/i/iain-murray/>Iain Murray</a>
|
<a href=/people/s/sharon-goldwater/>Sharon Goldwater</a><br><a href=/volumes/E17-1/ class=text-muted>Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1116><div class="card-body p-3 small">Political surveys have indicated a relationship between a sense of Scottish identity and voting decisions in the 2014 <a href=https://en.wikipedia.org/wiki/2014_Scottish_independence_referendum>Scottish Independence Referendum</a>. Identity is often reflected in language use, suggesting the intuitive hypothesis that individuals who support <a href=https://en.wikipedia.org/wiki/Scottish_independence>Scottish independence</a> are more likely to use distinctively Scottish words than those who oppose it. In the first large-scale study of sociolinguistic variation on <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> in the UK, we identify distinctively Scottish terms in a data-driven way, and find that these <a href=https://en.wikipedia.org/wiki/Terminology>terms</a> are indeed used at a higher rate by users of pro-independence hashtags than by users of anti-independence hashtags. However, we also find that in general people are less likely to use distinctively Scottish words in tweets with referendum-related hashtags than in their general Twitter activity. We attribute this difference to style shifting relative to audience, aligning with previous work showing that Twitter users tend to use fewer local variants when addressing a broader audience.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Sharon+Goldwater" title="Search for 'Sharon Goldwater' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/a/adam-lopez/ class=align-middle>Adam Lopez</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/p/philippa-shoemark/ class=align-middle>Philippa Shoemark</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/t/toms-bergmanis/ class=align-middle>Toms Bergmanis</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/k/kate-mccurdy/ class=align-middle>Kate McCurdy</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/y/yevgen-matusevych/ class=align-middle>Yevgen Matusevych</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/j/james-kirby/ class=align-middle>James Kirby</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/e/elizabeth-nielsen/ class=align-middle>Elizabeth Nielsen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/mark-steedman/ class=align-middle>Mark Steedman</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/herman-kamper/ class=align-middle>Herman Kamper</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/thomas-schatz/ class=align-middle>Thomas Schatz</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/n/naomi-feldman/ class=align-middle>Naomi Feldman</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/antonios-anastasopoulos/ class=align-middle>Antonios Anastasopoulos</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/sameer-bansal/ class=align-middle>Sameer Bansal</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/david-chiang/ class=align-middle>David Chiang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/alexander-robertson/ class=align-middle>Alexander Robertson</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/debnil-sur/ class=align-middle>Debnil Sur</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/luke-shrimpton/ class=align-middle>Luke Shrimpton</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/i/iain-murray/ class=align-middle>Iain Murray</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/maria-corkery/ class=align-middle>Maria Corkery</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/acl/ class=align-middle>ACL</a><span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/venues/eacl/ class=align-middle>EACL</a><span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/venues/ws/ class=align-middle>WS</a><span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/venues/naacl/ class=align-middle>NAACL</a><span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/venues/sigmorphon/ class=align-middle>SIGMORPHON</a><span class="badge badge-secondary align-middle ml-2">1</span></li></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright Â©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>