<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Sara Stymne - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Sara</span> <span class=font-weight-bold>Stymne</span></h2><hr><div class=row><div class=col-lg-9><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.vardial-1.5.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--vardial-1--5 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.vardial-1.5 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.vardial-1.5/>Whit’s the Richt Pairt o Speech : PoS tagging for Scots<span class=acl-fixed-case>P</span>o<span class=acl-fixed-case>S</span> tagging for <span class=acl-fixed-case>S</span>cots</a></strong><br><a href=/people/h/harm-lameris/>Harm Lameris</a>
|
<a href=/people/s/sara-stymne/>Sara Stymne</a><br><a href=/volumes/2021.vardial-1/ class=text-muted>Proceedings of the Eighth Workshop on NLP for Similar Languages, Varieties and Dialects</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--vardial-1--5><div class="card-body p-3 small">In this paper we explore PoS tagging for the <a href=https://en.wikipedia.org/wiki/Scots_language>Scots language</a>. Scots is spoken in <a href=https://en.wikipedia.org/wiki/Scotland>Scotland</a> and Northern Ireland, and is closely related to <a href=https://en.wikipedia.org/wiki/English_language>English</a>. As no linguistically annotated Scots data were available, we manually PoS tagged a small set that is used for evaluation and training. We use <a href=https://en.wikipedia.org/wiki/English_language>English</a> as a transfer language to examine zero-shot transfer and transfer learning methods. We find that training on a very small amount of Scots data was superior to zero-shot transfer from <a href=https://en.wikipedia.org/wiki/English_language>English</a>. Combining the Scots and English data led to further improvements, with a <a href=https://en.wikipedia.org/wiki/Concatenation>concatenation method</a> giving the best results. We also compared the use of two different English treebanks and found that a treebank containing web data was superior in the zero-shot setting, while it was outperformed by a <a href=https://en.wikipedia.org/wiki/Treebank>treebank</a> containing a mix of genres when combined with Scots data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.semeval-1.15.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--semeval-1--15 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.semeval-1.15 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.semeval-1.15/>Uppsala NLP at SemEval-2021 Task 2 : Multilingual Language Models for Fine-tuning and Feature Extraction in Word-in-Context Disambiguation<span class=acl-fixed-case>U</span>ppsala <span class=acl-fixed-case>NLP</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2021 Task 2: Multilingual Language Models for Fine-tuning and Feature Extraction in Word-in-Context Disambiguation</a></strong><br><a href=/people/h/huiling-you/>Huiling You</a>
|
<a href=/people/x/xingran-zhu/>Xingran Zhu</a>
|
<a href=/people/s/sara-stymne/>Sara Stymne</a><br><a href=/volumes/2021.semeval-1/ class=text-muted>Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--semeval-1--15><div class="card-body p-3 small">We describe the Uppsala NLP submission to SemEval-2021 Task 2 on multilingual and cross-lingual word-in-context disambiguation. We explore the usefulness of three pre-trained multilingual language models, XLM-RoBERTa (XLMR), Multilingual BERT (mBERT) and multilingual distilled BERT (mDistilBERT). We compare these three <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> in two setups, <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a> and as <a href=https://en.wikipedia.org/wiki/Software_feature>feature extractors</a>. In the second case we also experiment with using dependency-based information. We find that <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a> is better than <a href=https://en.wikipedia.org/wiki/Feature_extraction>feature extraction</a>. XLMR performs better than mBERT in the cross-lingual setting both with <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a> and <a href=https://en.wikipedia.org/wiki/Feature_extraction>feature extraction</a>, whereas these two models give a similar performance in the multilingual setting. mDistilBERT performs poorly with <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a> but gives similar results to the other <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> when used as a feature extractor. We submitted our two best <a href=https://en.wikipedia.org/wiki/System>systems</a>, fine-tuned with XLMR and mBERT.</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6305.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6305 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6305 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6305/>Discourse-Related Language Contrasts in English-Croatian Human and Machine Translation<span class=acl-fixed-case>E</span>nglish-<span class=acl-fixed-case>C</span>roatian Human and Machine Translation</a></strong><br><a href=/people/m/margita-sostaric/>Margita Šoštarić</a>
|
<a href=/people/c/christian-hardmeier/>Christian Hardmeier</a>
|
<a href=/people/s/sara-stymne/>Sara Stymne</a><br><a href=/volumes/W18-63/ class=text-muted>Proceedings of the Third Conference on Machine Translation: Research Papers</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6305><div class="card-body p-3 small">We present an analysis of a number of <a href=https://en.wikipedia.org/wiki/Coreference>coreference phenomena</a> in English-Croatian human and machine translations. The aim is to shed light on the differences in the way these structurally different languages make use of discourse information and provide insights for discourse-aware machine translation system development. The phenomena are automatically identified in parallel data using <a href=https://en.wikipedia.org/wiki/Annotation>annotation</a> produced by <a href=https://en.wikipedia.org/wiki/Parsing>parsers</a> and word alignment tools, enabling us to pinpoint patterns of interest in both languages. We make the <a href=https://en.wikipedia.org/wiki/Analysis>analysis</a> more fine-grained by including three corpora pertaining to three different registers. In a second step, we create a <a href=https://en.wikipedia.org/wiki/Test_set>test set</a> with the challenging linguistic constructions and use it to evaluate the performance of three MT systems. We show that both SMT and NMT systems struggle with handling these discourse phenomena, even though NMT tends to perform somewhat better than SMT. By providing an overview of patterns frequently occurring in actual language use, as well as by pointing out the weaknesses of current MT systems that commonly mistranslate them, we hope to contribute to the effort of resolving the issue of discourse phenomena in MT applications.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K18-2011.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K18-2011 data-toggle=collapse aria-expanded=false aria-controls=abstract-K18-2011 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/K18-2011/>82 Treebanks, 34 Models : Universal Dependency Parsing with Multi-Treebank Models<span class=acl-fixed-case>U</span>niversal <span class=acl-fixed-case>D</span>ependency Parsing with Multi-Treebank Models</a></strong><br><a href=/people/a/aaron-smith/>Aaron Smith</a>
|
<a href=/people/b/bernd-bohnet/>Bernd Bohnet</a>
|
<a href=/people/m/miryam-de-lhoneux/>Miryam de Lhoneux</a>
|
<a href=/people/j/joakim-nivre/>Joakim Nivre</a>
|
<a href=/people/y/yan-shao/>Yan Shao</a>
|
<a href=/people/s/sara-stymne/>Sara Stymne</a><br><a href=/volumes/K18-2/ class=text-muted>Proceedings of the CoNLL 2018 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K18-2011><div class="card-body p-3 small">We present the Uppsala system for the CoNLL 2018 Shared Task on universal dependency parsing. Our system is a pipeline consisting of three components : the first performs joint word and sentence segmentation ; the second predicts <a href=https://en.wikipedia.org/wiki/Part-of-speech_tagging>part-of-speech tags</a> and <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphological features</a> ; the third predicts dependency trees from words and tags. Instead of training a single parsing model for each <a href=https://en.wikipedia.org/wiki/Treebank>treebank</a>, we trained <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> with multiple treebanks for one language or closely related languages, greatly reducing the number of <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a>. On the official test run, we ranked 7th of 27 teams for the LAS and MLAS metrics. Our system obtained the best scores overall for <a href=https://en.wikipedia.org/wiki/Word_segmentation>word segmentation</a>, universal POS tagging, and <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphological features</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2098.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2098 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2098 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2098.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-2098" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P18-2098/>Parser Training with Heterogeneous Treebanks</a></strong><br><a href=/people/s/sara-stymne/>Sara Stymne</a>
|
<a href=/people/m/miryam-de-lhoneux/>Miryam de Lhoneux</a>
|
<a href=/people/a/aaron-smith/>Aaron Smith</a>
|
<a href=/people/j/joakim-nivre/>Joakim Nivre</a><br><a href=/volumes/P18-2/ class=text-muted>Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2098><div class="card-body p-3 small">How to make the most of multiple heterogeneous treebanks when training a monolingual dependency parser is an open question. We start by investigating previously suggested, but little evaluated, strategies for exploiting multiple treebanks based on concatenating training sets, with or without <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a>. We go on to propose a new <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>method</a> based on treebank embeddings. We perform experiments for several languages and show that in many cases <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a> and treebank embeddings lead to substantial improvements over single treebanks or <a href=https://en.wikipedia.org/wiki/Concatenation>concatenation</a>, with average gains of 2.03.5 LAS points. We argue that treebank embeddings should be preferred due to their conceptual simplicity, flexibility and extensibility.</div></div><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4801.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4801 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4801 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W17-4801.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/W17-4801/>Findings of the 2017 DiscoMT Shared Task on Cross-lingual Pronoun Prediction<span class=acl-fixed-case>D</span>isco<span class=acl-fixed-case>MT</span> Shared Task on Cross-lingual Pronoun Prediction</a></strong><br><a href=/people/s/sharid-loaiciga/>Sharid Loáiciga</a>
|
<a href=/people/s/sara-stymne/>Sara Stymne</a>
|
<a href=/people/p/preslav-nakov/>Preslav Nakov</a>
|
<a href=/people/c/christian-hardmeier/>Christian Hardmeier</a>
|
<a href=/people/j/jorg-tiedemann/>Jörg Tiedemann</a>
|
<a href=/people/m/mauro-cettolo/>Mauro Cettolo</a>
|
<a href=/people/y/yannick-versley/>Yannick Versley</a><br><a href=/volumes/W17-48/ class=text-muted>Proceedings of the Third Workshop on Discourse in Machine Translation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4801><div class="card-body p-3 small">We describe the design, the setup, and the evaluation results of the DiscoMT 2017 shared task on cross-lingual pronoun prediction. The task asked participants to predict a <a href=https://en.wikipedia.org/wiki/Pronoun>target-language pronoun</a> given a <a href=https://en.wikipedia.org/wiki/Pronoun>source-language pronoun</a> in the context of a sentence. We further provided a lemmatized target-language human-authored translation of the source sentence, and automatic word alignments between the source sentence words and the target-language lemmata. The aim of the task was to predict, for each target-language pronoun placeholder, the word that should replace it from a small, closed set of classes, using any type of information that can be extracted from the entire document. We offered four subtasks, each for a different language pair and translation direction : <a href=https://en.wikipedia.org/wiki/English_as_a_second_or_foreign_language>English-to-French</a>, <a href=https://en.wikipedia.org/wiki/English_as_a_second_or_foreign_language>English-to-German</a>, German-to-English, and <a href=https://en.wikipedia.org/wiki/Spanish_as_a_second_or_foreign_language>Spanish-to-English</a>. Five teams participated in the shared task, making submissions for all language pairs. The evaluation results show that most participating teams outperformed two strong n-gram-based language model-based baseline systems by a sizable margin.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4805.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4805 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4805 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4805/>A BiLSTM-based System for Cross-lingual Pronoun Prediction<span class=acl-fixed-case>B</span>i<span class=acl-fixed-case>LSTM</span>-based System for Cross-lingual Pronoun Prediction</a></strong><br><a href=/people/s/sara-stymne/>Sara Stymne</a>
|
<a href=/people/s/sharid-loaiciga/>Sharid Loáiciga</a>
|
<a href=/people/f/fabienne-cap/>Fabienne Cap</a><br><a href=/volumes/W17-48/ class=text-muted>Proceedings of the Third Workshop on Discourse in Machine Translation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4805><div class="card-body p-3 small">We describe the Uppsala system for the 2017 DiscoMT shared task on cross-lingual pronoun prediction. The <a href=https://en.wikipedia.org/wiki/System>system</a> is based on a lower layer of BiLSTMs reading the source and target sentences respectively. Classification is based on the BiLSTM representation of the source and target positions for the <a href=https://en.wikipedia.org/wiki/Pronoun>pronouns</a>. In addition we enrich our system with dependency representations from an <a href=https://en.wikipedia.org/wiki/Parsing>external parser</a> and <a href=https://en.wikipedia.org/wiki/Character_encoding>character representations</a> of the source sentence. We show that these additions perform well for <a href=https://en.wikipedia.org/wiki/German_language>German</a> and <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a> as source languages. Our <a href=https://en.wikipedia.org/wiki/System>system</a> is competitive and is in first or second place for all language pairs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-6314.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-6314 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-6314 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W17-6314.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W17-6314" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W17-6314/>Arc-Hybrid Non-Projective Dependency Parsing with a Static-Dynamic Oracle</a></strong><br><a href=/people/m/miryam-de-lhoneux/>Miryam de Lhoneux</a>
|
<a href=/people/s/sara-stymne/>Sara Stymne</a>
|
<a href=/people/j/joakim-nivre/>Joakim Nivre</a><br><a href=/volumes/W17-63/ class=text-muted>Proceedings of the 15th International Conference on Parsing Technologies</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-6314><div class="card-body p-3 small">In this paper, we extend the arc-hybrid system for transition-based parsing with a swap transition that enables reordering of the words and construction of non-projective trees. Although this extension breaks the arc-decomposability of the transition system, we show how the existing dynamic oracle for this <a href=https://en.wikipedia.org/wiki/System>system</a> can be modified and combined with a static oracle only for the swap transition. Experiments on 5 languages show that the new <a href=https://en.wikipedia.org/wiki/System>system</a> gives competitive accuracy and is significantly better than a <a href=https://en.wikipedia.org/wiki/System>system</a> trained with a purely static oracle.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Sara+Stymne" title="Search for 'Sara Stymne' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/m/miryam-de-lhoneux/ class=align-middle>Miryam de Lhoneux</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/j/joakim-nivre/ class=align-middle>Joakim Nivre</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/s/sharid-loaiciga/ class=align-middle>Sharid Loáiciga</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/c/christian-hardmeier/ class=align-middle>Christian Hardmeier</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/a/aaron-smith/ class=align-middle>Aaron Smith</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/h/harm-lameris/ class=align-middle>Harm Lameris</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/p/preslav-nakov/ class=align-middle>Preslav Nakov</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jorg-tiedemann/ class=align-middle>Jörg Tiedemann</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/mauro-cettolo/ class=align-middle>Mauro Cettolo</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yannick-versley/ class=align-middle>Yannick Versley</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/f/fabienne-cap/ class=align-middle>Fabienne Cap</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/margita-sostaric/ class=align-middle>Margita Šoštarić</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/huiling-you/ class=align-middle>Huiling You</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/x/xingran-zhu/ class=align-middle>Xingran Zhu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/b/bernd-bohnet/ class=align-middle>Bernd Bohnet</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yan-shao/ class=align-middle>Yan Shao</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/ws/ class=align-middle>WS</a><span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/venues/vardial/ class=align-middle>VarDial</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/semeval/ class=align-middle>SemEval</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/conll/ class=align-middle>CoNLL</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/acl/ class=align-middle>ACL</a><span class="badge badge-secondary align-middle ml-2">1</span></li></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>