<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Sabine Schulte im Walde - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Sabine</span> <span class=font-weight-bold>Schulte im Walde</span></h2><hr><div class=row><div class=col-lg-9><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.543.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--543 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.543 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.543" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.543/>Lexical Semantic Change Discovery</a></strong><br><a href=/people/s/sinan-kurtyigit/>Sinan Kurtyigit</a>
|
<a href=/people/m/maike-park/>Maike Park</a>
|
<a href=/people/d/dominik-schlechtweg/>Dominik Schlechtweg</a>
|
<a href=/people/j/jonas-kuhn/>Jonas Kuhn</a>
|
<a href=/people/s/sabine-schulte-im-walde/>Sabine Schulte im Walde</a><br><a href=/volumes/2021.acl-long/ class=text-muted>Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--543><div class="card-body p-3 small">While there is a large amount of research in the field of Lexical Semantic Change Detection, only few approaches go beyond a standard benchmark evaluation of existing <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a>. In this paper, we propose a shift of focus from change detection to change discovery, i.e., discovering novel word senses over time from the full corpus vocabulary. By heavily fine-tuning a type-based and a token-based approach on recently published German data, we demonstrate that both models can successfully be applied to discover new words undergoing meaning change. Furthermore, we provide an almost fully automated <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> for both <a href=https://en.wikipedia.org/wiki/Evaluation>evaluation</a> and <a href=https://en.wikipedia.org/wiki/Discovery_(observation)>discovery</a>.</div></div><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.acl-main.258.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--acl-main--258 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.acl-main.258 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://slideslive.com/38928698 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.acl-main.258/>Predicting Degrees of Technicality in Automatic Terminology Extraction</a></strong><br><a href=/people/a/anna-hatty/>Anna Hätty</a>
|
<a href=/people/d/dominik-schlechtweg/>Dominik Schlechtweg</a>
|
<a href=/people/m/michael-dorna/>Michael Dorna</a>
|
<a href=/people/s/sabine-schulte-im-walde/>Sabine Schulte im Walde</a><br><a href=/volumes/2020.acl-main/ class=text-muted>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--acl-main--258><div class="card-body p-3 small">While automatic term extraction is a well-researched area, computational approaches to distinguish between degrees of technicality are still understudied. We semi-automatically create a German gold standard of technicality across four domains, and illustrate the impact of a web-crawled general-language corpus on technicality prediction. When defining a classification approach that combines general-language and domain-specific word embeddings, we go beyond previous work and align vector spaces to gain comparative embeddings. We suggest two novel models to exploit general- vs. domain-specific comparisons : a simple <a href=https://en.wikipedia.org/wiki/Neural_network>neural network model</a> with pre-computed comparative-embedding information as input, and a multi-channel model computing the comparison internally. Both <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> outperform previous approaches, with the multi-channel model performing best.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.537.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--537 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.537 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.537/>A Domain-Specific Dataset of Difficulty Ratings for German Noun Compounds in the Domains DIY, Cooking and Automotive<span class=acl-fixed-case>G</span>erman Noun Compounds in the Domains <span class=acl-fixed-case>DIY</span>, Cooking and Automotive</a></strong><br><a href=/people/j/julia-bettinger/>Julia Bettinger</a>
|
<a href=/people/a/anna-hatty/>Anna Hätty</a>
|
<a href=/people/m/michael-dorna/>Michael Dorna</a>
|
<a href=/people/s/sabine-schulte-im-walde/>Sabine Schulte im Walde</a><br><a href=/volumes/2020.lrec-1/ class=text-muted>Proceedings of the 12th Language Resources and Evaluation Conference</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--537><div class="card-body p-3 small">We present a dataset with difficulty ratings for 1,030 German closed noun compounds extracted from domain-specific texts for do-it-ourself (DIY), cooking and automotive. The dataset includes two-part compounds for cooking and DIY, and two- to four-part compounds for automotive. The <a href=https://en.wikipedia.org/wiki/Chemical_compound>compounds</a> were identified in text using the Simple Compound Splitter (Weller-Di Marco, 2017) ; a subset was filtered and balanced for frequency and productivity criteria as basis for manual annotation and fine-grained interpretation. This study presents the creation, the final <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> with ratings from 20 annotators and statistics over the <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>, to provide insight into the perception of domain-specific term difficulty. It is particularly striking that annotators agree on a coarse, binary distinction between easy vs. difficult domain-specific compounds but that a more fine grained distinction of difficulty is not meaningful. We finally discuss the challenges of an <a href=https://en.wikipedia.org/wiki/Annotation>annotation</a> for difficulty, which includes both the task description as well as the selection of the data basis.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.859.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--859 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.859 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.859/>CCOHA : Clean Corpus of Historical American English<span class=acl-fixed-case>CCOHA</span>: Clean Corpus of Historical <span class=acl-fixed-case>A</span>merican <span class=acl-fixed-case>E</span>nglish</a></strong><br><a href=/people/r/reem-alatrash/>Reem Alatrash</a>
|
<a href=/people/d/dominik-schlechtweg/>Dominik Schlechtweg</a>
|
<a href=/people/j/jonas-kuhn/>Jonas Kuhn</a>
|
<a href=/people/s/sabine-schulte-im-walde/>Sabine Schulte im Walde</a><br><a href=/volumes/2020.lrec-1/ class=text-muted>Proceedings of the 12th Language Resources and Evaluation Conference</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--859><div class="card-body p-3 small">Modelling language change is an increasingly important area of interest within the fields of <a href=https://en.wikipedia.org/wiki/Sociolinguistics>sociolinguistics</a> and <a href=https://en.wikipedia.org/wiki/Historical_linguistics>historical linguistics</a>. In recent years, there has been a growing number of publications whose main concern is studying changes that have occurred within the past centuries. The Corpus of Historical American English (COHA) is one of the most commonly used large corpora in diachronic studies in <a href=https://en.wikipedia.org/wiki/English_language>English</a>. This paper describes methods applied to the downloadable version of the COHA corpus in order to overcome its main limitations, such as inconsistent lemmas and malformed tokens, without compromising its qualitative and distributional properties. The resulting <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> CCOHA contains a larger number of cleaned word tokens which can offer better insights into <a href=https://en.wikipedia.org/wiki/Language_change>language change</a> and allow for a larger variety of tasks to be performed.</div></div><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1477.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1477 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1477 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1477.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D19-1477/>You Shall Know a User by the Company It Keeps : Dynamic Representations for Social Media Users in NLP<span class=acl-fixed-case>NLP</span></a></strong><br><a href=/people/m/marco-del-tredici/>Marco Del Tredici</a>
|
<a href=/people/d/diego-marcheggiani/>Diego Marcheggiani</a>
|
<a href=/people/s/sabine-schulte-im-walde/>Sabine Schulte im Walde</a>
|
<a href=/people/r/raquel-fernandez/>Raquel Fernández</a><br><a href=/volumes/D19-1/ class=text-muted>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1477><div class="card-body p-3 small">Information about individuals can help to better understand what they say, particularly in <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> where texts are short. Current approaches to modelling social media users pay attention to their social connections, but exploit this information in a static way, treating all connections uniformly. This ignores the fact, well known in <a href=https://en.wikipedia.org/wiki/Sociolinguistics>sociolinguistics</a>, that an individual may be part of several communities which are not equally relevant in all communicative situations. We present a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> based on Graph Attention Networks that captures this observation. It dynamically explores the <a href=https://en.wikipedia.org/wiki/Social_graph>social graph</a> of a user, computes a user representation given the most relevant connections for a target task, and combines it with linguistic information to make a prediction. We apply our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> to three different tasks, evaluate it against alternative <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>, and analyse the results extensively, showing that it significantly outperforms other current methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S19-1001.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S19-1001 data-toggle=collapse aria-expanded=false aria-controls=abstract-S19-1001 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/S19-1001/>SURel : A Gold Standard for Incorporating Meaning Shifts into Term Extraction<span class=acl-fixed-case>SUR</span>el: A Gold Standard for Incorporating Meaning Shifts into Term Extraction</a></strong><br><a href=/people/a/anna-hatty/>Anna Hätty</a>
|
<a href=/people/d/dominik-schlechtweg/>Dominik Schlechtweg</a>
|
<a href=/people/s/sabine-schulte-im-walde/>Sabine Schulte im Walde</a><br><a href=/volumes/S19-1/ class=text-muted>Proceedings of the Eighth Joint Conference on Lexical and Computational Semantics (*SEM 2019)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S19-1001><div class="card-body p-3 small">We introduce SURel, a novel <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> with human-annotated meaning shifts between general-language and domain-specific contexts. We show that meaning shifts of term candidates cause errors in <a href=https://en.wikipedia.org/wiki/Term_extraction>term extraction</a>, and demonstrate that the SURel annotation reflects these errors. Furthermore, we illustrate that SURel enables us to assess optimisations of term extraction techniques when incorporating meaning shifts.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-0506.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-0506 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-0506 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-0506/>Distributional Interaction of Concreteness and Abstractness in VerbNoun Subcategorisation</a></strong><br><a href=/people/d/diego-frassinelli/>Diego Frassinelli</a>
|
<a href=/people/s/sabine-schulte-im-walde/>Sabine Schulte im Walde</a><br><a href=/volumes/W19-05/ class=text-muted>Proceedings of the 13th International Conference on Computational Semantics - Short Papers</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-0506><div class="card-body p-3 small">In recent years, both cognitive and computational research has provided empirical analyses of contextual co-occurrence of concrete and abstract words, partially resulting in inconsistent pictures. In this work we provide a more fine-grained description of the distributional nature in the corpus-based interaction of verbs and nouns within <a href=https://en.wikipedia.org/wiki/Subcategorization>subcategorisation</a>, by investigating the concreteness of verbs and nouns that are in a specific syntactic relationship with each other, i.e., subject, direct object, and <a href=https://en.wikipedia.org/wiki/Preposition_and_postposition>prepositional object</a>. Overall, our experiments show consistent patterns in the distributional representation of subcategorising and subcategorised concrete and abstract words. At the same time, the studies reveal empirical evidence why contextual abstractness represents a valuable indicator for automatic non-literal language identification.</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1070.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1070 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1070 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=C18-1070" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/C18-1070/>Projecting Embeddings for Domain Adaption : Joint Modeling of Sentiment Analysis in Diverse Domains</a></strong><br><a href=/people/j/jeremy-barnes/>Jeremy Barnes</a>
|
<a href=/people/r/roman-klinger/>Roman Klinger</a>
|
<a href=/people/s/sabine-schulte-im-walde/>Sabine Schulte im Walde</a><br><a href=/volumes/C18-1/ class=text-muted>Proceedings of the 27th International Conference on Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1070><div class="card-body p-3 small">Domain adaptation for <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> is challenging due to the fact that <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised classifiers</a> are very sensitive to changes in domain. The two most prominent approaches to this <a href=https://en.wikipedia.org/wiki/Problem_solving>problem</a> are structural correspondence learning and <a href=https://en.wikipedia.org/wiki/Autoencoder>autoencoders</a>. However, they either require long training times or suffer greatly on highly divergent domains. Inspired by recent advances in cross-lingual sentiment analysis, we provide a novel perspective and cast the domain adaptation problem as an embedding projection task. Our model takes as input two mono-domain embedding spaces and learns to project them to a bi-domain space, which is jointly optimized to (1) project across domains and to (2) predict sentiment. We perform <a href=https://en.wikipedia.org/wiki/Domain_adaptation>domain adaptation</a> experiments on 20 source-target domain pairs for sentiment classification and report novel state-of-the-art results on 11 domain pairs, including the Amazon domain adaptation datasets and SemEval 2013 and 2016 datasets. Our analysis shows that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> performs comparably to state-of-the-art approaches on domains that are similar, while performing significantly better on highly divergent domains. Our code is available at https://github.com/jbarnesspain/domain_blse</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S18-2003.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S18-2003 data-toggle=collapse aria-expanded=false aria-controls=abstract-S18-2003 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/S18-2003/>Assessing Meaning Components in German Complex Verbs : A Collection of Source-Target Domains and Directionality<span class=acl-fixed-case>G</span>erman Complex Verbs: A Collection of Source-Target Domains and Directionality</a></strong><br><a href=/people/s/sabine-schulte-im-walde/>Sabine Schulte im Walde</a>
|
<a href=/people/m/maximilian-koper/>Maximilian Köper</a>
|
<a href=/people/s/sylvia-springorum/>Sylvia Springorum</a><br><a href=/volumes/S18-2/ class=text-muted>Proceedings of the Seventh Joint Conference on Lexical and Computational Semantics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S18-2003><div class="card-body p-3 small">This paper presents a collection to assess <a href=https://en.wikipedia.org/wiki/Meaning_(linguistics)>meaning components</a> in German complex verbs, which frequently undergo <a href=https://en.wikipedia.org/wiki/Semantic_change>meaning shifts</a>. We use a novel strategy to obtain source and target domain characterisations via sentence generation rather than sentence annotation. A selection of <a href=https://en.wikipedia.org/wiki/Arrow_(symbol)>arrows</a> adds spatial directional information to the generated contexts. We provide a broad qualitative description of the <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>, and a series of standard <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a> experiments verifies the quantitative reliability of the presented resource. The setup for collecting the meaning components is applicable also to other languages, regarding complex verbs as well as other language-specific targets that involve meaning shifts.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S18-2008.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S18-2008 data-toggle=collapse aria-expanded=false aria-controls=abstract-S18-2008 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/S18-2008/>Quantitative Semantic Variation in the Contexts of Concrete and Abstract Words</a></strong><br><a href=/people/d/daniela-naumann/>Daniela Naumann</a>
|
<a href=/people/d/diego-frassinelli/>Diego Frassinelli</a>
|
<a href=/people/s/sabine-schulte-im-walde/>Sabine Schulte im Walde</a><br><a href=/volumes/S18-2/ class=text-muted>Proceedings of the Seventh Joint Conference on Lexical and Computational Semantics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S18-2008><div class="card-body p-3 small">Across disciplines, researchers are eager to gain insight into empirical features of <a href=https://en.wikipedia.org/wiki/Abstract_and_concrete>abstract vs. concrete concepts</a>. In this work, we provide a detailed characterisation of the distributional nature of <a href=https://en.wikipedia.org/wiki/Abstract_and_concrete>abstract and concrete words</a> across 16,620 <a href=https://en.wikipedia.org/wiki/English_nouns>English nouns</a>, verbs and adjectives. Specifically, we investigate the following questions : (1) What is the distribution of concreteness in the contexts of concrete and abstract target words? (2) What are the differences between concrete and abstract words in terms of contextual semantic diversity? (3) How does the <a href=https://en.wikipedia.org/wiki/Entropy_(information_theory)>entropy</a> of concrete and abstract word contexts differ? Overall, our studies show consistent differences in the distributional representation of concrete and abstract words, thus challenging existing theories of cognition and providing a more fine-grained description of their nature.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4909.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4909 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4909 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4909/>Fine-Grained Termhood Prediction for German Compound Terms Using <a href=https://en.wikipedia.org/wiki/Neural_network>Neural Networks</a><span class=acl-fixed-case>G</span>erman Compound Terms Using Neural Networks</a></strong><br><a href=/people/a/anna-hatty/>Anna Hätty</a>
|
<a href=/people/s/sabine-schulte-im-walde/>Sabine Schulte im Walde</a><br><a href=/volumes/W18-49/ class=text-muted>Proceedings of the Joint Workshop on Linguistic Annotation, Multiword Expressions and Constructions (LAW-MWE-CxG-2018)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4909><div class="card-body p-3 small">Automatic term identification and investigating the understandability of terms in a specialized domain are often treated as two separate lines of research. We propose a combined approach for this matter, by defining fine-grained classes of termhood and framing a classification task. The <a href=https://en.wikipedia.org/wiki/Class_(computer_programming)>classes</a> reflect tiers of a term&#8217;s association to a domain. The new setup is applied to German closed compounds as term candidates in the domain of cooking. For the prediction of the classes, we compare several neural network architectures and also take salient information about the compounds&#8217; components into account. We show that applying a similar class distinction to the compounds&#8217; components and propagating this information within the <a href=https://en.wikipedia.org/wiki/Flow_network>network</a> improves the compound class prediction results.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2024.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2024 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2024 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-2024/>Analogies in Complex Verb Meaning Shifts : the Effect of Affect in Semantic Similarity Models</a></strong><br><a href=/people/m/maximilian-koper/>Maximilian Köper</a>
|
<a href=/people/s/sabine-schulte-im-walde/>Sabine Schulte im Walde</a><br><a href=/volumes/N18-2/ class=text-muted>Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2024><div class="card-body p-3 small">We present a <a href=https://en.wikipedia.org/wiki/Computational_model>computational model</a> to detect and distinguish analogies in meaning shifts between German base and complex verbs. In contrast to <a href=https://en.wikipedia.org/wiki/Corpus_linguistics>corpus-based studies</a>, a novel <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> demonstrates that regular shifts represent the smallest class. Classification experiments relying on a standard similarity model successfully distinguish between four types of shifts, with verb classes boosting the performance, and affective features for abstractness, <a href=https://en.wikipedia.org/wiki/Emotion>emotion</a> and <a href=https://en.wikipedia.org/wiki/Sentimentality>sentiment</a> representing the most salient indicators.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2027.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2027 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2027 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N18-2027.Datasets.zip data-toggle=tooltip data-placement=top title=Dataset><i class="fas fa-file-archive"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N18-2027/>Diachronic Usage Relatedness (DURel): A Framework for the Annotation of Lexical Semantic Change<span class=acl-fixed-case>DUR</span>el): A Framework for the Annotation of Lexical Semantic Change</a></strong><br><a href=/people/d/dominik-schlechtweg/>Dominik Schlechtweg</a>
|
<a href=/people/s/sabine-schulte-im-walde/>Sabine Schulte im Walde</a>
|
<a href=/people/s/stefanie-eckmann/>Stefanie Eckmann</a><br><a href=/volumes/N18-2/ class=text-muted>Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2027><div class="card-body p-3 small">We propose a framework that extends synchronic polysemy annotation to diachronic changes in lexical meaning, to counteract the lack of resources for evaluating computational models of lexical semantic change. Our framework exploits an intuitive notion of semantic relatedness, and distinguishes between innovative and reductive meaning changes with high inter-annotator agreement. The resulting <a href=https://en.wikipedia.org/wiki/Test_(assessment)>test set</a> for <a href=https://en.wikipedia.org/wiki/German_language>German</a> comprises ratings from five annotators for the relatedness of 1,320 use pairs across 22 target words.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2032.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2032 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2032 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-2032/>Introducing Two Vietnamese Datasets for Evaluating Semantic Models of (Dis-)Similarity and Relatedness<span class=acl-fixed-case>V</span>ietnamese Datasets for Evaluating Semantic Models of (Dis-)Similarity and Relatedness</a></strong><br><a href=/people/k/kim-anh-nguyen/>Kim Anh Nguyen</a>
|
<a href=/people/s/sabine-schulte-im-walde/>Sabine Schulte im Walde</a>
|
<a href=/people/n/ngoc-thang-vu/>Ngoc Thang Vu</a><br><a href=/volumes/N18-2/ class=text-muted>Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2032><div class="card-body p-3 small">We present two novel datasets for the low-resource language Vietnamese to assess models of semantic similarity : ViCon comprises pairs of <a href=https://en.wikipedia.org/wiki/Synonym>synonyms</a> and <a href=https://en.wikipedia.org/wiki/Opposite_(semantics)>antonyms</a> across word classes, thus offering data to distinguish between similarity and dissimilarity. ViSim-400 provides degrees of similarity across five semantic relations, as rated by human judges. The two <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> are verified through standard co-occurrence and neural network models, showing results comparable to the respective English datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2052.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2052 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2052 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-2052/>A Laypeople Study on Terminology Identification across Domains and Task Definitions</a></strong><br><a href=/people/a/anna-hatty/>Anna Hätty</a>
|
<a href=/people/s/sabine-schulte-im-walde/>Sabine Schulte im Walde</a><br><a href=/volumes/N18-2/ class=text-muted>Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2052><div class="card-body p-3 small">This paper introduces a new <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> of term annotation. Given that even experts vary significantly in their understanding of termhood, and that term identification is mostly performed as a binary task, we offer a novel perspective to explore the common, natural understanding of what constitutes a term : Laypeople annotate single-word and multi-word terms, across four domains and across four task definitions. Analyses based on <a href=https://en.wikipedia.org/wiki/Inter-annotator_agreement>inter-annotator agreement</a> offer insights into differences in term specificity, <a href=https://en.wikipedia.org/wiki/Granularity>term granularity</a> and subtermhood.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-4002.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-4002 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-4002 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-4002/>Combining Abstractness and Language-specific Theoretical Indicators for Detecting Non-Literal Usage of Estonian Particle Verbs<span class=acl-fixed-case>E</span>stonian Particle Verbs</a></strong><br><a href=/people/e/eleri-aedmaa/>Eleri Aedmaa</a>
|
<a href=/people/m/maximilian-koper/>Maximilian Köper</a>
|
<a href=/people/s/sabine-schulte-im-walde/>Sabine Schulte im Walde</a><br><a href=/volumes/N18-4/ class=text-muted>Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Student Research Workshop</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-4002><div class="card-body p-3 small">This paper presents two novel <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> and a random-forest classifier to automatically predict <a href=https://en.wikipedia.org/wiki/Literal_and_figurative_language>literal vs. non-literal language usage</a> for a highly frequent type of <a href=https://en.wikipedia.org/wiki/Interlingue>multi-word expression</a> in a low-resource language, i.e., <a href=https://en.wikipedia.org/wiki/Estonian_language>Estonian</a>. We demonstrate the value of language-specific indicators induced from theoretical linguistic research, which outperform a high majority baseline when combined with language-independent features of non-literal language (such as abstractness).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1231.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1231 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1231 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1231.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-1231" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P18-1231/>Bilingual Sentiment Embeddings : Joint Projection of Sentiment Across Languages</a></strong><br><a href=/people/j/jeremy-barnes/>Jeremy Barnes</a>
|
<a href=/people/r/roman-klinger/>Roman Klinger</a>
|
<a href=/people/s/sabine-schulte-im-walde/>Sabine Schulte im Walde</a><br><a href=/volumes/P18-1/ class=text-muted>Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1231><div class="card-body p-3 small">Sentiment analysis in low-resource languages suffers from a lack of annotated corpora to estimate high-performing models. Machine translation and bilingual word embeddings provide some relief through cross-lingual sentiment approaches. However, they either require large amounts of parallel data or do not sufficiently capture <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment information</a>. We introduce Bilingual Sentiment Embeddings (BLSE), which jointly represent <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment information</a> in a source and target language. This model only requires a small <a href=https://en.wikipedia.org/wiki/Bilingual_lexicon>bilingual lexicon</a>, a source-language corpus annotated for <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment</a>, and monolingual word embeddings for each language. We perform experiments on three language combinations (Spanish, Catalan, Basque) for sentence-level cross-lingual sentiment classification and find that our model significantly outperforms state-of-the-art methods on four out of six experimental setups, as well as capturing complementary information to <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>. Our analysis of the resulting <a href=https://en.wikipedia.org/wiki/Embedding>embedding space</a> provides evidence that it represents <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment information</a> in the resource-poor target language without any annotated data in that language.</div></div><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K17-1036.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K17-1036 data-toggle=collapse aria-expanded=false aria-controls=abstract-K17-1036 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/K17-1036.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=K17-1036" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/K17-1036/>German in Flux : Detecting Metaphoric Change via Word Entropy<span class=acl-fixed-case>G</span>erman in Flux: Detecting Metaphoric Change via Word Entropy</a></strong><br><a href=/people/d/dominik-schlechtweg/>Dominik Schlechtweg</a>
|
<a href=/people/s/stefanie-eckmann/>Stefanie Eckmann</a>
|
<a href=/people/e/enrico-santus/>Enrico Santus</a>
|
<a href=/people/s/sabine-schulte-im-walde/>Sabine Schulte im Walde</a>
|
<a href=/people/d/daniel-hole/>Daniel Hole</a><br><a href=/volumes/K17-1/ class=text-muted>Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K17-1036><div class="card-body p-3 small">This paper explores the information-theoretic measure entropy to detect metaphoric change, transferring ideas from hypernym detection to research on <a href=https://en.wikipedia.org/wiki/Language_change>language change</a>. We build the first diachronic test set for <a href=https://en.wikipedia.org/wiki/German_language>German</a> as a standard for metaphoric change annotation. Our <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> is unsupervised, language-independent and generalizable to other processes of <a href=https://en.wikipedia.org/wiki/Semantic_change>semantic change</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1708.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1708 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1708 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1708/>Factoring Ambiguity out of the Prediction of Compositionality for German Multi-Word Expressions<span class=acl-fixed-case>G</span>erman Multi-Word Expressions</a></strong><br><a href=/people/s/stefan-bott/>Stefan Bott</a>
|
<a href=/people/s/sabine-schulte-im-walde/>Sabine Schulte im Walde</a><br><a href=/volumes/W17-17/ class=text-muted>Proceedings of the 13th Workshop on Multiword Expressions (MWE 2017)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1708><div class="card-body p-3 small">Ambiguity represents an obstacle for distributional semantic models(DSMs), which typically subsume the contexts of all word senses within one vector. While individual vector space approaches have been concerned with sense discrimination (e.g., Schtze 1998, Erk 2009, Erk and Pado 2010), such <a href=https://en.wikipedia.org/wiki/Discrimination>discrimination</a> has rarely been integrated into DSMs across <a href=https://en.wikipedia.org/wiki/Semantic_analysis_(linguistics)>semantic tasks</a>. This paper presents a soft-clustering approach to sense discrimination that filters sense-irrelevant features when predicting the degrees of compositionality for German noun-noun compounds and German particle verbs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1728.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1728 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1728 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1728/>Complex Verbs are Different : Exploring the Visual Modality in Multi-Modal Models to Predict Compositionality</a></strong><br><a href=/people/m/maximilian-koper/>Maximilian Köper</a>
|
<a href=/people/s/sabine-schulte-im-walde/>Sabine Schulte im Walde</a><br><a href=/volumes/W17-17/ class=text-muted>Proceedings of the 13th Workshop on Multiword Expressions (MWE 2017)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1728><div class="card-body p-3 small">This paper compares a neural network DSM relying on textual co-occurrences with a multi-modal model integrating visual information. We focus on <a href=https://en.wikipedia.org/wiki/Compound_(linguistics)>nominal vs. verbal compounds</a>, and zoom into lexical, empirical and perceptual target properties to explore the contribution of the <a href=https://en.wikipedia.org/wiki/Visual_system>visual modality</a>. Our experiments show that (i) visual features contribute differently for verbs than for nouns, and (ii) <a href=https://en.wikipedia.org/wiki/Image>images</a> complement textual information, if (a) the textual modality by itself is poor and appropriate image subsets are used, or (b) the textual modality by itself is rich and large (potentially noisy) images are added.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1903.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1903 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1903 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1903/>Improving Verb Metaphor Detection by Propagating Abstractness to Words, Phrases and Individual Senses</a></strong><br><a href=/people/m/maximilian-koper/>Maximilian Köper</a>
|
<a href=/people/s/sabine-schulte-im-walde/>Sabine Schulte im Walde</a><br><a href=/volumes/W17-19/ class=text-muted>Proceedings of the 1st Workshop on Sense, Concept and Entity Representations and their Applications</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1903><div class="card-body p-3 small">Abstract words refer to things that can not be seen, heard, felt, smelled, or tasted as opposed to concrete words. Among other <a href=https://en.wikipedia.org/wiki/Application_software>applications</a>, the degree of <a href=https://en.wikipedia.org/wiki/Abstraction>abstractness</a> has been shown to be a useful information for metaphor detection. Our contribution to this topic are as follows : i) we compare supervised techniques to learn and extend <a href=https://en.wikipedia.org/wiki/Abstraction>abstractness ratings</a> for huge vocabularies ii) we learn and investigate norms for larger units by propagating <a href=https://en.wikipedia.org/wiki/Abstraction>abstractness</a> to verb-noun pairs which lead to better metaphor detection iii) we overcome the limitation of learning a single rating per word and show that multi-sense abstractness ratings are potentially useful for metaphor detection. Finally, with this paper we publish automatically created abstractness norms for 3million English words and multi-words as well as automatically created sense specific abstractness ratings</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1022.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1022 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1022 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1022/>Hierarchical Embeddings for Hypernymy Detection and Directionality</a></strong><br><a href=/people/k/kim-anh-nguyen/>Kim Anh Nguyen</a>
|
<a href=/people/m/maximilian-koper/>Maximilian Köper</a>
|
<a href=/people/s/sabine-schulte-im-walde/>Sabine Schulte im Walde</a>
|
<a href=/people/n/ngoc-thang-vu/>Ngoc Thang Vu</a><br><a href=/volumes/D17-1/ class=text-muted>Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1022><div class="card-body p-3 small">We present a novel neural model HyperVec to learn hierarchical embeddings for hypernymy detection and directionality. While previous <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a> have shown limitations on prototypical hypernyms, HyperVec represents an <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised measure</a> where <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a> are learned in a specific order and capture the hypernymhyponym distributional hierarchy. Moreover, our model is able to generalize over unseen hypernymy pairs, when using only small sets of training data, and by mapping to other languages. Results on benchmark datasets show that HyperVec outperforms both state-of-the-art unsupervised measures and embedding models on hypernymy detection and directionality, and on predicting graded lexical entailment.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1008.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1008 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1008 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=E17-1008" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/E17-1008/>Distinguishing Antonyms and Synonyms in a Pattern-based Neural Network</a></strong><br><a href=/people/k/kim-anh-nguyen/>Kim Anh Nguyen</a>
|
<a href=/people/s/sabine-schulte-im-walde/>Sabine Schulte im Walde</a>
|
<a href=/people/n/ngoc-thang-vu/>Ngoc Thang Vu</a><br><a href=/volumes/E17-1/ class=text-muted>Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1008><div class="card-body p-3 small">Distinguishing between <a href=https://en.wikipedia.org/wiki/Opposite_(semantics)>antonyms</a> and <a href=https://en.wikipedia.org/wiki/Synonym>synonyms</a> is a key task to achieve high performance in NLP systems. While they are notoriously difficult to distinguish by distributional co-occurrence models, pattern-based methods have proven effective to differentiate between the relations. In this paper, we present a novel neural network model AntSynNET that exploits lexico-syntactic patterns from syntactic parse trees. In addition to the lexical and syntactic information, we successfully integrate the distance between the related words along the syntactic path as a new pattern feature. The results from <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a> experiments show that AntSynNET improves the performance over prior <a href=https://en.wikipedia.org/wiki/Pattern_recognition>pattern-based methods</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2086.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2086 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2086 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2086/>Applying Multi-Sense Embeddings for German Verbs to Determine Semantic Relatedness and to Detect Non-Literal Language<span class=acl-fixed-case>G</span>erman Verbs to Determine Semantic Relatedness and to Detect Non-Literal Language</a></strong><br><a href=/people/m/maximilian-koper/>Maximilian Köper</a>
|
<a href=/people/s/sabine-schulte-im-walde/>Sabine Schulte im Walde</a><br><a href=/volumes/E17-2/ class=text-muted>Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2086><div class="card-body p-3 small">Up to date, the majority of <a href=https://en.wikipedia.org/wiki/Computational_linguistics>computational models</a> still determines the <a href=https://en.wikipedia.org/wiki/Semantic_similarity>semantic relatedness</a> between words (or larger linguistic units) on the type level. In this paper, we compare and extend multi-sense embeddings, in order to model and utilise <a href=https://en.wikipedia.org/wiki/Word_sense>word senses</a> on the token level. We focus on the challenging class of complex verbs, and evaluate the model variants on various semantic tasks : semantic classification ; predicting compositionality ; and detecting non-literal language usage. While there is no overall best model, all models significantly outperform a word2vec single-sense skip baseline, thus demonstrating the need to distinguish between <a href=https://en.wikipedia.org/wiki/Word_sense>word senses</a> in a distributional semantic model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2099.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2099 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2099 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2099/>Addressing Problems across Linguistic Levels in SMT : Combining Approaches to Model <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>Morphology</a>, <a href=https://en.wikipedia.org/wiki/Syntax>Syntax</a> and Lexical Choice<span class=acl-fixed-case>SMT</span>: Combining Approaches to Model Morphology, Syntax and Lexical Choice</a></strong><br><a href=/people/m/marion-weller-di-marco/>Marion Weller-Di Marco</a>
|
<a href=/people/a/alexander-fraser/>Alexander Fraser</a>
|
<a href=/people/s/sabine-schulte-im-walde/>Sabine Schulte im Walde</a><br><a href=/volumes/E17-2/ class=text-muted>Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2099><div class="card-body p-3 small">Many errors in phrase-based SMT can be attributed to problems on three linguistic levels : <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphological complexity</a> in the target language, structural differences and <a href=https://en.wikipedia.org/wiki/Lexical_choice>lexical choice</a>. We explore combinations of linguistically motivated approaches to address these problems in English-to-German SMT and show that they are complementary to one another, but also that the popular verbal pre-ordering can cause problems on the morphological and lexical level. A discriminative classifier can overcome these problems, in particular when enriching standard lexical features with <a href=https://en.wikipedia.org/wiki/Feature_(linguistics)>features</a> geared towards verbal inflection.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-4012.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-4012 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-4012 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-4012/>Evaluating the Reliability and Interaction of Recursively Used Feature Classes for Terminology Extraction</a></strong><br><a href=/people/a/anna-hatty/>Anna Hätty</a>
|
<a href=/people/m/michael-dorna/>Michael Dorna</a>
|
<a href=/people/s/sabine-schulte-im-walde/>Sabine Schulte im Walde</a><br><a href=/volumes/E17-4/ class=text-muted>Proceedings of the Student Research Workshop at the 15th Conference of the European Chapter of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-4012><div class="card-body p-3 small">Feature design and selection is a crucial aspect when treating <a href=https://en.wikipedia.org/wiki/Terminology_extraction>terminology extraction</a> as a machine learning classification problem. We designed feature classes which characterize different properties of terms based on <a href=https://en.wikipedia.org/wiki/Probability_distribution>distributions</a>, and propose a new feature class for components of term candidates. By using <a href=https://en.wikipedia.org/wiki/Random_forest>random forests</a>, we infer optimal features which are later used to build <a href=https://en.wikipedia.org/wiki/Decision_tree_learning>decision tree classifiers</a>. We evaluate our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> using the ACL RD-TEC dataset. We demonstrate the importance of the novel feature class for downgrading termhood which exploits properties of term components. Furthermore, our <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a> suggests that the identification of reliable term candidates should be performed successively, rather than just once.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Sabine+Schulte+im+Walde" title="Search for 'Sabine Schulte im Walde' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/m/maximilian-koper/ class=align-middle>Maximilian Köper</a>
<span class="badge badge-secondary align-middle ml-2">7</span></li><li class=list-group-item><a href=/people/d/dominik-schlechtweg/ class=align-middle>Dominik Schlechtweg</a>
<span class="badge badge-secondary align-middle ml-2">6</span></li><li class=list-group-item><a href=/people/a/anna-hatty/ class=align-middle>Anna Hätty</a>
<span class="badge badge-secondary align-middle ml-2">6</span></li><li class=list-group-item><a href=/people/m/michael-dorna/ class=align-middle>Michael Dorna</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/k/kim-anh-nguyen/ class=align-middle>Kim Anh Nguyen</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/n/ngoc-thang-vu/ class=align-middle>Ngoc Thang Vu</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/j/jeremy-barnes/ class=align-middle>Jeremy Barnes</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/r/roman-klinger/ class=align-middle>Roman Klinger</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/j/jonas-kuhn/ class=align-middle>Jonas Kuhn</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/s/stefanie-eckmann/ class=align-middle>Stefanie Eckmann</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/d/diego-frassinelli/ class=align-middle>Diego Frassinelli</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/s/sinan-kurtyigit/ class=align-middle>Sinan Kurtyigit</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/maike-park/ class=align-middle>Maike Park</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/e/enrico-santus/ class=align-middle>Enrico Santus</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/daniel-hole/ class=align-middle>Daniel Hole</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/stefan-bott/ class=align-middle>Stefan Bott</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/marco-del-tredici/ class=align-middle>Marco Del Tredici</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/diego-marcheggiani/ class=align-middle>Diego Marcheggiani</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/raquel-fernandez/ class=align-middle>Raquel Fernández</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/sylvia-springorum/ class=align-middle>Sylvia Springorum</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/daniela-naumann/ class=align-middle>Daniela Naumann</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/e/eleri-aedmaa/ class=align-middle>Eleri Aedmaa</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/julia-bettinger/ class=align-middle>Julia Bettinger</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/reem-alatrash/ class=align-middle>Reem Alatrash</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/marion-weller-di-marco/ class=align-middle>Marion Weller-Di Marco</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/alexander-fraser/ class=align-middle>Alexander Fraser</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/ws/ class=align-middle>WS</a><span class="badge badge-secondary align-middle ml-2">5</span></li><li class=list-group-item><a href=/venues/naacl/ class=align-middle>NAACL</a><span class="badge badge-secondary align-middle ml-2">5</span></li><li class=list-group-item><a href=/venues/eacl/ class=align-middle>EACL</a><span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/venues/acl/ class=align-middle>ACL</a><span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/venues/semeval/ class=align-middle>SemEval</a><span class="badge badge-secondary align-middle ml-2">3</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-venues aria-expanded=false aria-controls=more-venues>show all...</li><div class="collapse border-top" id=more-venues><li class=list-group-item><a href=/venues/emnlp/ class=align-middle>EMNLP</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/lrec/ class=align-middle>LREC</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/coling/ class=align-middle>COLING</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/conll/ class=align-middle>CoNLL</a><span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>