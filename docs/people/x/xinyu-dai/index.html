<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Xinyu Dai - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Xinyu</span> <span class=font-weight-bold>Dai</span></h2><p class="font-weight-light text-muted"><span class=font-italic>Also published as:</span>
Xin-Yu <span class=font-weight-normal>Dai</span>,
Xin-yu <span class=font-weight-normal>Dai</span></p><hr><div class=row><div class=col-lg-9><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.nlptea-1.7.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--nlptea-1--7 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.nlptea-1.7 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.nlptea-1.7/>Integrating BERT and Score-based Feature Gates for Chinese Grammatical Error Diagnosis<span class=acl-fixed-case>BERT</span> and Score-based Feature Gates for <span class=acl-fixed-case>C</span>hinese Grammatical Error Diagnosis</a></strong><br><a href=/people/y/yongchang-cao/>Yongchang Cao</a>
|
<a href=/people/l/liang-he/>Liang He</a>
|
<a href=/people/r/robert-ridley/>Robert Ridley</a>
|
<a href=/people/x/xinyu-dai/>Xinyu Dai</a><br><a href=/volumes/2020.nlptea-1/ class=text-muted>Proceedings of the 6th Workshop on Natural Language Processing Techniques for Educational Applications</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--nlptea-1--7><div class="card-body p-3 small">This paper describes our proposed <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> for the Chinese Grammatical Error Diagnosis (CGED) task in NLPTEA2020. The goal of CGED is to use <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing techniques</a> to automatically diagnose <a href=https://en.wikipedia.org/wiki/Chinese_grammar>Chinese grammatical errors</a> in sentences. To this end, we design and implement a CGED model named BERT with Score-feature Gates Error Diagnoser (BSGED), which is based on the BERT model, Bidirectional Long Short-Term Memory (BiLSTM) and conditional random field (CRF). In order to address the problem of losing partial-order relationships when embedding continuous feature items as with previous works, we propose a gating mechanism for integrating continuous feature items, which effectively retains the <a href=https://en.wikipedia.org/wiki/Partially_ordered_set>partial-order relationships</a> between feature items. We perform LSTM processing on the encoding result of the BERT model, and further extract the sequence features. In the final test-set evaluation, we obtained the highest F1 score at the detection level and are among the top 3 F1 scores at the identification level.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.234.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--234 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.234 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.234" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.234/>Grid Tagging Scheme for Aspect-oriented Fine-grained Opinion Extraction</a></strong><br><a href=/people/z/zhen-wu/>Zhen Wu</a>
|
<a href=/people/c/chengcan-ying/>Chengcan Ying</a>
|
<a href=/people/f/fei-zhao/>Fei Zhao</a>
|
<a href=/people/z/zhifang-fan/>Zhifang Fan</a>
|
<a href=/people/x/xinyu-dai/>Xinyu Dai</a>
|
<a href=/people/r/rui-xia/>Rui Xia</a><br><a href=/volumes/2020.findings-emnlp/ class=text-muted>Findings of the Association for Computational Linguistics: EMNLP 2020</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--234><div class="card-body p-3 small">Aspect-oriented Fine-grained Opinion Extraction (AFOE) aims at extracting aspect terms and opinion terms from review in the form of opinion pairs or additionally extracting sentiment polarity of aspect term to form opinion triplet. Because of containing several opinion factors, the complete AFOE task is usually divided into multiple subtasks and achieved in the <a href=https://en.wikipedia.org/wiki/Pipeline_(software)>pipeline</a>. However, pipeline approaches easily suffer from <a href=https://en.wikipedia.org/wiki/Error_propagation>error propagation</a> and inconvenience in real-world scenarios. To this end, we propose a novel tagging scheme, Grid Tagging Scheme (GTS), to address the AFOE task in an end-to-end fashion only with one unified grid tagging task. Additionally, we design an effective inference strategy on GTS to exploit mutual indication between different opinion factors for more accurate extractions. To validate the feasibility and compatibility of GTS, we implement three different GTS models respectively based on CNN, BiLSTM, and BERT, and conduct experiments on the aspect-oriented opinion pair extraction and opinion triplet extraction datasets. Extensive experimental results indicate that GTS models outperform strong baselines significantly and achieve state-of-the-art performance.</div></div><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1086.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1086 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1086 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1086.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1086" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1086/>Dynamic Past and Future for <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a></a></strong><br><a href=/people/z/zaixiang-zheng/>Zaixiang Zheng</a>
|
<a href=/people/s/shujian-huang/>Shujian Huang</a>
|
<a href=/people/z/zhaopeng-tu/>Zhaopeng Tu</a>
|
<a href=/people/x/xinyu-dai/>Xin-Yu Dai</a>
|
<a href=/people/j/jiajun-chen/>Jiajun Chen</a><br><a href=/volumes/D19-1/ class=text-muted>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1086><div class="card-body p-3 small">Previous studies have shown that neural machine translation (NMT) models can benefit from explicitly modeling translated () and untranslated () source contents as recurrent states (CITATION). However, this less interpretable recurrent process hinders its power to model the dynamic updating of and contents during decoding. In this paper, we propose to model the dynamic principles by explicitly separating source words into groups of translated and untranslated contents through parts-to-wholes assignment. The assignment is learned through a novel variant of routing-by-agreement mechanism (CITATION), namely Guided Dynamic Routing, where the translating status at each decoding step guides the routing process to assign each source word to its associated group (i.e., translated or untranslated content) represented by a capsule, enabling translation to be made from holistic context. Experiments show that our approach achieves substantial improvements over both <a href=https://en.wikipedia.org/wiki/Reverse_Polish_notation>Rnmt</a> and <a href=https://en.wikipedia.org/wiki/Reverse_Polish_notation>Transformer</a> by producing more adequate translations. Extensive analysis demonstrates that our method is highly interpretable, which is able to recognize the translated and untranslated contents as expected.<i>dynamic principles</i> by explicitly separating source words into groups of translated and untranslated contents through parts-to-wholes assignment. The assignment is learned through a novel variant of routing-by-agreement mechanism (CITATION), namely <i>Guided Dynamic Routing</i>, where the translating status at each decoding step <i>guides</i> the routing process to assign each source word to its associated group (i.e., translated or untranslated content) represented by a capsule, enabling translation to be made from holistic context. Experiments show that our approach achieves substantial improvements over both Rnmt and Transformer by producing more adequate translations. Extensive analysis demonstrates that our method is highly interpretable, which is able to recognize the translated and untranslated contents as expected.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1192.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1192 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1192 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-1192/>Online Distilling from Checkpoints for Neural Machine Translation</a></strong><br><a href=/people/h/hao-ran-wei/>Hao-Ran Wei</a>
|
<a href=/people/s/shujian-huang/>Shujian Huang</a>
|
<a href=/people/r/ran-wang/>Ran Wang</a>
|
<a href=/people/x/xinyu-dai/>Xin-yu Dai</a>
|
<a href=/people/j/jiajun-chen/>Jiajun Chen</a><br><a href=/volumes/N19-1/ class=text-muted>Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1192><div class="card-body p-3 small">Current predominant neural machine translation (NMT) models often have a deep structure with large amounts of parameters, making these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> hard to train and easily suffering from <a href=https://en.wikipedia.org/wiki/Overfitting>over-fitting</a>. A common practice is to utilize a validation set to evaluate the training process and select the best checkpoint. Average and ensemble techniques on checkpoints can lead to further performance improvement. However, as these methods do not affect the <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training process</a>, the <a href=https://en.wikipedia.org/wiki/System>system</a> performance is restricted to the checkpoints generated in original <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training procedure</a>. In contrast, we propose an online knowledge distillation method. Our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> on-the-fly generates a teacher model from <a href=https://en.wikipedia.org/wiki/Checkpoint>checkpoints</a>, guiding the <a href=https://en.wikipedia.org/wiki/Training>training process</a> to obtain better performance. Experiments on several datasets and language pairs show steady improvement over a strong self-attention-based baseline system. We also provide analysis on data-limited setting against <a href=https://en.wikipedia.org/wiki/Overfitting>over-fitting</a>. Furthermore, our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> leads to an improvement in a machine reading experiment as well.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1325.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1325 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1325 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-1325/>Exploiting Noisy Data in Distant Supervision Relation Classification</a></strong><br><a href=/people/k/kaijia-yang/>Kaijia Yang</a>
|
<a href=/people/l/liang-he/>Liang He</a>
|
<a href=/people/x/xinyu-dai/>Xin-yu Dai</a>
|
<a href=/people/s/shujian-huang/>Shujian Huang</a>
|
<a href=/people/j/jiajun-chen/>Jiajun Chen</a><br><a href=/volumes/N19-1/ class=text-muted>Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1325><div class="card-body p-3 small">Distant supervision has obtained great progress on relation classification task. However, <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> still suffers from noisy labeling problem. Different from previous works that underutilize <a href=https://en.wikipedia.org/wiki/Noisy_data>noisy data</a> which inherently characterize the property of classification, in this paper, we propose RCEND, a novel framework to enhance Relation Classification by Exploiting <a href=https://en.wikipedia.org/wiki/Noisy_data>Noisy Data</a>. First, an instance discriminator with <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning</a> is designed to split the noisy data into correctly labeled data and incorrectly labeled data. Second, we learn a robust relation classifier in semi-supervised learning way, whereby the correctly and incorrectly labeled data are treated as labeled and unlabeled data respectively. The experimental results show that our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> outperforms the state-of-the-art models.</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/Q18-1011.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-Q18-1011 data-toggle=collapse aria-expanded=false aria-controls=abstract-Q18-1011 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=Q18-1011" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/Q18-1011/>Modeling Past and Future for Neural Machine Translation</a></strong><br><a href=/people/z/zaixiang-zheng/>Zaixiang Zheng</a>
|
<a href=/people/h/hao-zhou/>Hao Zhou</a>
|
<a href=/people/s/shujian-huang/>Shujian Huang</a>
|
<a href=/people/l/lili-mou/>Lili Mou</a>
|
<a href=/people/x/xinyu-dai/>Xinyu Dai</a>
|
<a href=/people/j/jiajun-chen/>Jiajun Chen</a>
|
<a href=/people/z/zhaopeng-tu/>Zhaopeng Tu</a><br><a href=/volumes/Q18-1/ class=text-muted>Transactions of the Association for Computational Linguistics, Volume 6</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-Q18-1011><div class="card-body p-3 small">Existing neural machine translation systems do not explicitly model what has been translated and what has not during the decoding phase. To address this problem, we propose a novel mechanism that separates the source information into two parts : translated Past contents and untranslated Future contents, which are modeled by two additional recurrent layers. The Past and Future contents are fed to both the attention model and the decoder states, which provides Neural Machine Translation (NMT) systems with the knowledge of translated and untranslated contents. Experimental results show that the proposed approach significantly improves the performance in Chinese-English, German-English, and English-German translation tasks. Specifically, the proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms the conventional coverage model in terms of both the translation quality and the alignment error rate.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1116.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1116 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1116 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-1116/>Combining Character and Word Information in <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a> Using a Multi-Level Attention</a></strong><br><a href=/people/h/huadong-chen/>Huadong Chen</a>
|
<a href=/people/s/shujian-huang/>Shujian Huang</a>
|
<a href=/people/d/david-chiang/>David Chiang</a>
|
<a href=/people/x/xinyu-dai/>Xinyu Dai</a>
|
<a href=/people/j/jiajun-chen/>Jiajun Chen</a><br><a href=/volumes/N18-1/ class=text-muted>Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1116><div class="card-body p-3 small">Natural language sentences, being hierarchical, can be represented at different levels of <a href=https://en.wikipedia.org/wiki/Granularity>granularity</a>, like <a href=https://en.wikipedia.org/wiki/Word>words</a>, <a href=https://en.wikipedia.org/wiki/Subword>subwords</a>, or <a href=https://en.wikipedia.org/wiki/Character_(symbol)>characters</a>. But most neural machine translation systems require the sentence to be represented as a sequence at a single level of granularity. It can be difficult to determine which <a href=https://en.wikipedia.org/wiki/Granularity>granularity</a> is better for a particular translation task. In this paper, we improve the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> by incorporating multiple levels of granularity. Specifically, we propose (1) an encoder with character attention which augments the (sub)word-level representation with character-level information ; (2) a decoder with multiple attentions that enable the representations from different levels of granularity to control the translation cooperatively. Experiments on three translation tasks demonstrate that our proposed models outperform the standard word-based model, the subword-based model, and a strong character-based model.</div></div><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K17-1011.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K17-1011 data-toggle=collapse aria-expanded=false aria-controls=abstract-K17-1011 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/K17-1011/>Top-Rank Enhanced Listwise Optimization for <a href=https://en.wikipedia.org/wiki/Statistical_machine_translation>Statistical Machine Translation</a></a></strong><br><a href=/people/h/huadong-chen/>Huadong Chen</a>
|
<a href=/people/s/shujian-huang/>Shujian Huang</a>
|
<a href=/people/d/david-chiang/>David Chiang</a>
|
<a href=/people/x/xinyu-dai/>Xinyu Dai</a>
|
<a href=/people/j/jiajun-chen/>Jiajun Chen</a><br><a href=/volumes/K17-1/ class=text-muted>Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K17-1011><div class="card-body p-3 small">Pairwise ranking methods are the most widely used discriminative training approaches for structure prediction problems in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing (NLP)</a>. Decomposing the problem of ranking hypotheses into <a href=https://en.wikipedia.org/wiki/Pairwise_comparisons>pairwise comparisons</a> enables simple and efficient solutions. However, neglecting the global ordering of the hypothesis list may hinder <a href=https://en.wikipedia.org/wiki/Learning>learning</a>. We propose a listwise learning framework for structure prediction problems such as <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>. Our framework directly models the entire translation list&#8217;s ordering to learn parameters which may better fit the given listwise samples. Furthermore, we propose top-rank enhanced loss functions, which are more sensitive to ranking errors at higher positions. Experiments on a large-scale Chinese-English translation task show that both our listwise learning framework and top-rank enhanced listwise losses lead to significant improvements in translation quality.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1013.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1013 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1013 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1013/>Neural Machine Translation with Word Predictions</a></strong><br><a href=/people/r/rongxiang-weng/>Rongxiang Weng</a>
|
<a href=/people/s/shujian-huang/>Shujian Huang</a>
|
<a href=/people/z/zaixiang-zheng/>Zaixiang Zheng</a>
|
<a href=/people/x/xinyu-dai/>Xinyu Dai</a>
|
<a href=/people/j/jiajun-chen/>Jiajun Chen</a><br><a href=/volumes/D17-1/ class=text-muted>Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1013><div class="card-body p-3 small">In the encoder-decoder architecture for neural machine translation (NMT), the hidden states of the recurrent structures in the <a href=https://en.wikipedia.org/wiki/Encoder>encoder</a> and <a href=https://en.wikipedia.org/wiki/Decoder>decoder</a> carry the crucial information about the sentence. These <a href=https://en.wikipedia.org/wiki/Euclidean_vector>vectors</a> are generated by parameters which are updated by back-propagation of translation errors through time. We argue that propagating errors through the end-to-end recurrent structures are not a direct way of control the hidden vectors. In this paper, we propose to use <a href=https://en.wikipedia.org/wiki/Word_prediction>word predictions</a> as a mechanism for direct supervision. More specifically, we require these <a href=https://en.wikipedia.org/wiki/Vector_(mathematics_and_physics)>vectors</a> to be able to predict the vocabulary in target sentence. Our simple mechanism ensures better representations in the <a href=https://en.wikipedia.org/wiki/Encoder>encoder</a> and <a href=https://en.wikipedia.org/wiki/Code>decoder</a> without using any extra data or annotation. It is also helpful in reducing the target side vocabulary and improving the decoding efficiency. Experiments on Chinese-English machine translation task show an average BLEU improvement by 4.53, respectively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1079.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1079 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1079 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1079/>Word-Context Character Embeddings for Chinese Word Segmentation<span class=acl-fixed-case>C</span>hinese Word Segmentation</a></strong><br><a href=/people/h/hao-zhou/>Hao Zhou</a>
|
<a href=/people/z/zhenting-yu/>Zhenting Yu</a>
|
<a href=/people/y/yue-zhang/>Yue Zhang</a>
|
<a href=/people/s/shujian-huang/>Shujian Huang</a>
|
<a href=/people/x/xinyu-dai/>Xinyu Dai</a>
|
<a href=/people/j/jiajun-chen/>Jiajun Chen</a><br><a href=/volumes/D17-1/ class=text-muted>Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1079><div class="card-body p-3 small">Neural parsers have benefited from automatically labeled data via dependency-context word embeddings. We investigate training character embeddings on a word-based context in a similar way, showing that the simple method improves state-of-the-art neural word segmentation models significantly, beating tri-training baselines for leveraging auto-segmented data.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Xinyu+Dai" title="Search for 'Xinyu Dai' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/s/shujian-huang/ class=align-middle>Shujian Huang</a>
<span class="badge badge-secondary align-middle ml-2">8</span></li><li class=list-group-item><a href=/people/j/jiajun-chen/ class=align-middle>Jiajun Chen</a>
<span class="badge badge-secondary align-middle ml-2">8</span></li><li class=list-group-item><a href=/people/z/zaixiang-zheng/ class=align-middle>Zaixiang Zheng</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/h/huadong-chen/ class=align-middle>Huadong Chen</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/d/david-chiang/ class=align-middle>David Chiang</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/z/zhaopeng-tu/ class=align-middle>Zhaopeng Tu</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/h/hao-zhou/ class=align-middle>Hao Zhou</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/l/liang-he/ class=align-middle>Liang He</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/l/lili-mou/ class=align-middle>Lili Mou</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/rongxiang-weng/ class=align-middle>Rongxiang Weng</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/zhenting-yu/ class=align-middle>Zhenting Yu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yue-zhang/ class=align-middle>Yue Zhang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yongchang-cao/ class=align-middle>Yongchang Cao</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/robert-ridley/ class=align-middle>Robert Ridley</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/zhen-wu/ class=align-middle>Zhen Wu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/chengcan-ying/ class=align-middle>Chengcan Ying</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/f/fei-zhao/ class=align-middle>Fei Zhao</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/zhifang-fan/ class=align-middle>Zhifang Fan</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/rui-xia/ class=align-middle>Rui Xia</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/hao-ran-wei/ class=align-middle>Hao-Ran Wei</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/ran-wang/ class=align-middle>Ran Wang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/kaijia-yang/ class=align-middle>Kaijia Yang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/emnlp/ class=align-middle>EMNLP</a><span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/venues/naacl/ class=align-middle>NAACL</a><span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/venues/conll/ class=align-middle>CoNLL</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/tacl/ class=align-middle>TACL</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/nlptea/ class=align-middle>NLP-TEA</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-venues aria-expanded=false aria-controls=more-venues>show all...</li><div class="collapse border-top" id=more-venues><li class=list-group-item><a href=/venues/findings/ class=align-middle>Findings</a><span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright Â©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>