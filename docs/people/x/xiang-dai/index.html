<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Xiang Dai - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Xiang</span> <span class=font-weight-bold>Dai</span></h2><p class="font-weight-light text-muted"><span class=font-italic>Also published as:</span>
Xiangying <span class=font-weight-normal>Dai</span></p><hr><div class=row><div class=col-lg-9><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.acl-main.520.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--acl-main--520 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.acl-main.520 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://slideslive.com/38928958 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.acl-main.520" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.acl-main.520/>An Effective Transition-based Model for Discontinuous NER<span class=acl-fixed-case>NER</span></a></strong><br><a href=/people/x/xiang-dai/>Xiang Dai</a>
|
<a href=/people/s/sarvnaz-karimi/>Sarvnaz Karimi</a>
|
<a href=/people/b/ben-hachey/>Ben Hachey</a>
|
<a href=/people/c/cecile-paris/>Cecile Paris</a><br><a href=/volumes/2020.acl-main/ class=text-muted>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--acl-main--520><div class="card-body p-3 small">Unlike widely used Named Entity Recognition (NER) data sets in generic domains, biomedical NER data sets often contain mentions consisting of discontinuous spans. Conventional sequence tagging techniques encode <a href=https://en.wikipedia.org/wiki/Markov_assumption>Markov assumptions</a> that are efficient but preclude recovery of these mentions. We propose a simple, effective transition-based model with generic neural encoding for discontinuous NER. Through extensive experiments on three biomedical data sets, we show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> can effectively recognize discontinuous mentions without sacrificing the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> on continuous mentions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.151.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--151 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.151 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.151/>Cost-effective Selection of Pretraining Data : A Case Study of Pretraining BERT on <a href=https://en.wikipedia.org/wiki/Social_media>Social Media</a><span class=acl-fixed-case>BERT</span> on Social Media</a></strong><br><a href=/people/x/xiang-dai/>Xiang Dai</a>
|
<a href=/people/s/sarvnaz-karimi/>Sarvnaz Karimi</a>
|
<a href=/people/b/ben-hachey/>Ben Hachey</a>
|
<a href=/people/c/cecile-paris/>Cecile Paris</a><br><a href=/volumes/2020.findings-emnlp/ class=text-muted>Findings of the Association for Computational Linguistics: EMNLP 2020</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--151><div class="card-body p-3 small">Recent studies on domain-specific BERT models show that effectiveness on downstream tasks can be improved when <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> are pretrained on in-domain data. Often, the pretraining data used in these <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> are selected based on their subject matter, e.g., <a href=https://en.wikipedia.org/wiki/Biology>biology</a> or <a href=https://en.wikipedia.org/wiki/Computer_science>computer science</a>. Given the range of applications using social media text, and its unique language variety, we pretrain two models on <a href=https://en.wikipedia.org/wiki/Twitter>tweets</a> and <a href=https://en.wikipedia.org/wiki/Internet_forum>forum text</a> respectively, and empirically demonstrate the effectiveness of these two resources. In addition, we investigate how <a href=https://en.wikipedia.org/wiki/Similarity_measure>similarity measures</a> can be used to nominate in-domain pretraining data. We publicly release our pretrained models at https://bit.ly/35RpTf0.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.343.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--343 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.343 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.343/>An Analysis of Simple Data Augmentation for Named Entity Recognition</a></strong><br><a href=/people/x/xiang-dai/>Xiang Dai</a>
|
<a href=/people/h/heike-adel/>Heike Adel</a><br><a href=/volumes/2020.coling-main/ class=text-muted>Proceedings of the 28th International Conference on Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--343><div class="card-body p-3 small">Simple yet effective data augmentation techniques have been proposed for sentence-level and sentence-pair natural language processing tasks. Inspired by these efforts, we design and compare <a href=https://en.wikipedia.org/wiki/Data_augmentation>data augmentation</a> for <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>named entity recognition</a>, which is usually modeled as a token-level sequence labeling problem. Through experiments on two data sets from the biomedical and materials science domains (i2b2-2010 and MaSciP), we show that simple augmentation can boost performance for both recurrent and transformer-based models, especially for small training sets.</div></div><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1510.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1510 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1510 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-1510/>NNE : A Dataset for Nested Named Entity Recognition in English Newswire<span class=acl-fixed-case>NNE</span>: A Dataset for Nested Named Entity Recognition in <span class=acl-fixed-case>E</span>nglish Newswire</a></strong><br><a href=/people/n/nicky-ringland/>Nicky Ringland</a>
|
<a href=/people/x/xiang-dai/>Xiang Dai</a>
|
<a href=/people/b/ben-hachey/>Ben Hachey</a>
|
<a href=/people/s/sarvnaz-karimi/>Sarvnaz Karimi</a>
|
<a href=/people/c/cecile-paris/>Cecile Paris</a>
|
<a href=/people/j/james-r-curran/>James R. Curran</a><br><a href=/volumes/P19-1/ class=text-muted>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1510><div class="card-body p-3 small">Named entity recognition (NER) is widely used in <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language processing applications</a> and downstream tasks. However, most NER tools target flat annotation from popular <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>, eschewing the semantic information available in nested entity mentions. We describe NNEa fine-grained, nested named entity dataset over the full Wall Street Journal portion of the Penn Treebank (PTB). Our <a href=https://en.wikipedia.org/wiki/Annotation>annotation</a> comprises 279,795 mentions of 114 entity types with up to 6 layers of <a href=https://en.wikipedia.org/wiki/Nesting_(computing)>nesting</a>. We hope the public release of this large <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> for English newswire will encourage development of new techniques for nested NER.</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5911.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5911 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5911 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5911/>Shot Or Not : Comparison of NLP Approaches for Vaccination Behaviour Detection<span class=acl-fixed-case>NLP</span> Approaches for Vaccination Behaviour Detection</a></strong><br><a href=/people/a/aditya-joshi/>Aditya Joshi</a>
|
<a href=/people/x/xiang-dai/>Xiang Dai</a>
|
<a href=/people/s/sarvnaz-karimi/>Sarvnaz Karimi</a>
|
<a href=/people/r/ross-sparks/>Ross Sparks</a>
|
<a href=/people/c/cecile-paris/>Cécile Paris</a>
|
<a href=/people/c/c-raina-macintyre/>C Raina MacIntyre</a><br><a href=/volumes/W18-59/ class=text-muted>Proceedings of the 2018 EMNLP Workshop SMM4H: The 3rd Social Media Mining for Health Applications Workshop & Shared Task</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5911><div class="card-body p-3 small">Vaccination behaviour detection deals with predicting whether or not a person received / was about to receive a vaccine. We present our submission for vaccination behaviour detection shared task at the SMM4H workshop. Our findings are based on three prevalent text classification approaches : rule-based, statistical and deep learning-based. Our final submissions are : (1) an ensemble of statistical classifiers with task-specific features derived using lexicons, language processing tools and word embeddings ; and, (2) a LSTM classifier with pre-trained language models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2033.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2033 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2033 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2033.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-2033/>Task-oriented Dialogue System for Automatic Diagnosis</a></strong><br><a href=/people/z/zhongyu-wei/>Zhongyu Wei</a>
|
<a href=/people/q/qianlong-liu/>Qianlong Liu</a>
|
<a href=/people/b/baolin-peng/>Baolin Peng</a>
|
<a href=/people/h/huaixiao-tou/>Huaixiao Tou</a>
|
<a href=/people/t/ting-chen/>Ting Chen</a>
|
<a href=/people/x/xuan-jing-huang/>Xuanjing Huang</a>
|
<a href=/people/k/kam-fai-wong/>Kam-fai Wong</a>
|
<a href=/people/x/xiang-dai/>Xiangying Dai</a><br><a href=/volumes/P18-2/ class=text-muted>Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2033><div class="card-body p-3 small">In this paper, we make a move to build a <a href=https://en.wikipedia.org/wiki/Dialogue_system>dialogue system</a> for automatic diagnosis. We first build a <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> collected from an online medical forum by extracting symptoms from both patients&#8217; self-reports and conversational data between patients and doctors. Then we propose a task-oriented dialogue system framework to make <a href=https://en.wikipedia.org/wiki/Diagnosis>diagnosis</a> for patients automatically, which can converse with patients to collect additional <a href=https://en.wikipedia.org/wiki/Symptom>symptoms</a> beyond their self-reports. Experimental results on our dataset show that additional <a href=https://en.wikipedia.org/wiki/Symptom>symptoms</a> extracted from conversation can greatly improve the accuracy for disease identification and our dialogue system is able to collect these <a href=https://en.wikipedia.org/wiki/Symptom>symptoms</a> automatically and make a better <a href=https://en.wikipedia.org/wiki/Diagnosis>diagnosis</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-3006.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-3006 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-3006 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P18-3006/>Recognizing Complex Entity Mentions : A Review and Future Directions</a></strong><br><a href=/people/x/xiang-dai/>Xiang Dai</a><br><a href=/volumes/P18-3/ class=text-muted>Proceedings of ACL 2018, Student Research Workshop</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-3006><div class="card-body p-3 small">Standard named entity recognizers can effectively recognize entity mentions that consist of contiguous tokens and do not overlap with each other. However, in practice, there are many domains, such as the biomedical domain, in which there are nested, overlapping, and discontinuous entity mentions. These complex mentions can not be directly recognized by conventional sequence tagging models because they may break the assumptions based on which sequence tagging techniques are built. We review the existing methods which are revised to tackle complex entity mentions and categorize them as tokenlevel and sentence-level approaches. We then identify the research gap, and discuss some directions that we are exploring.</div></div><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2342.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2342 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2342 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-2342/>Automatic Diagnosis Coding of Radiology Reports : A Comparison of <a href=https://en.wikipedia.org/wiki/Deep_learning>Deep Learning</a> and Conventional Classification Methods</a></strong><br><a href=/people/s/sarvnaz-karimi/>Sarvnaz Karimi</a>
|
<a href=/people/x/xiang-dai/>Xiang Dai</a>
|
<a href=/people/h/hamed-hassanzadeh/>Hamed Hassanzadeh</a>
|
<a href=/people/a/anthony-nguyen/>Anthony Nguyen</a><br><a href=/volumes/W17-23/ class=text-muted>BioNLP 2017</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2342><div class="card-body p-3 small">Diagnosis autocoding services and research intend to both improve the productivity of clinical coders and the accuracy of the coding. It is an important step in <a href=https://en.wikipedia.org/wiki/Data_analysis>data analysis</a> for funding and reimbursement, as well as health services planning and <a href=https://en.wikipedia.org/wiki/Resource_allocation>resource allocation</a>. We investigate the applicability of <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning</a> at autocoding of radiology reports using International Classification of Diseases (ICD). Deep learning methods are known to require <a href=https://en.wikipedia.org/wiki/Big_data>large training data</a>. Our goal is to explore how to use these <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> when the training data is sparse, skewed and relatively small, and how their effectiveness compares to conventional methods. We identify optimal parameters that could be used in setting up a <a href=https://en.wikipedia.org/wiki/Convolutional_neural_network>convolutional neural network</a> for <a href=https://en.wikipedia.org/wiki/Autocoding>autocoding</a> with comparable results to that of conventional methods.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Xiang+Dai" title="Search for 'Xiang Dai' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/s/sarvnaz-karimi/ class=align-middle>Sarvnaz Karimi</a>
<span class="badge badge-secondary align-middle ml-2">5</span></li><li class=list-group-item><a href=/people/c/cecile-paris/ class=align-middle>Cecile Paris</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/people/b/ben-hachey/ class=align-middle>Ben Hachey</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/h/hamed-hassanzadeh/ class=align-middle>Hamed Hassanzadeh</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/anthony-nguyen/ class=align-middle>Anthony Nguyen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/a/aditya-joshi/ class=align-middle>Aditya Joshi</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/ross-sparks/ class=align-middle>Ross Sparks</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/c-raina-macintyre/ class=align-middle>C Raina MacIntyre</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/heike-adel/ class=align-middle>Heike Adel</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/zhongyu-wei/ class=align-middle>Zhongyu Wei</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/q/qianlong-liu/ class=align-middle>Qianlong Liu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/b/baolin-peng/ class=align-middle>Baolin Peng</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/huaixiao-tou/ class=align-middle>Huaixiao Tou</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/ting-chen/ class=align-middle>Ting Chen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/x/xuan-jing-huang/ class=align-middle>Xuan-Jing Huang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/kam-fai-wong/ class=align-middle>Kam-Fai Wong</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/n/nicky-ringland/ class=align-middle>Nicky Ringland</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/james-r-curran/ class=align-middle>James R. Curran</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/acl/ class=align-middle>ACL</a><span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/venues/ws/ class=align-middle>WS</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/findings/ class=align-middle>Findings</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/coling/ class=align-middle>COLING</a><span class="badge badge-secondary align-middle ml-2">1</span></li></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>