<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Xiang Li - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Xiang</span> <span class=font-weight-bold>Li</span></h2><hr><div class=row><div class=col-lg-9><h4>2022</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.acl-long.353.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--acl-long--353 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.acl-long.353 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2022.acl-long.353.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.acl-long.353" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2022.acl-long.353/>A Neural Network Architecture for <a href=https://en.wikipedia.org/wiki/Program_understanding>Program Understanding</a> Inspired by Human Behaviors</a></strong><br><a href=/people/r/renyu-zhu/>Renyu Zhu</a>
|
<a href=/people/l/lei-yuan/>Lei Yuan</a>
|
<a href=/people/x/xiang-li/>Xiang Li</a>
|
<a href=/people/m/ming-gao/>Ming Gao</a>
|
<a href=/people/w/wenyuan-cai/>Wenyuan Cai</a><br><a href=/volumes/2022.acl-long/ class=text-muted>Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--acl-long--353><div class="card-body p-3 small">Program understanding is a fundamental task in program language processing Despite the success existing works fail to take <a href=https://en.wikipedia.org/wiki/Human_behavior>human behaviors</a> as reference in understanding programs In this paper we consider <a href=https://en.wikipedia.org/wiki/Human_behavior>human behaviors</a> and propose the PGNN EK model that consists of two main components On the one hand inspired by the divide and conquer reading behaviors of humans we present a partitioning based graph neural network model PGNN on the upgraded AST of codes On the other hand to characterize human behaviors of resorting to other resources to help code comprehension we transform raw codes with external knowledge and apply pre training techniques for <a href=https://en.wikipedia.org/wiki/Information_extraction>information extraction</a> Finally we combine the two <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a> generated from the two components to output code embeddings We conduct extensive experiments to show the superior performance of PGNN EK on the <a href=https://en.wikipedia.org/wiki/Automatic_summarization>code summarization</a> and code clone detection tasks In particular to show the generalization ability of our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> we release a new <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> that is more challenging for code clone detection and could advance the development of the community Our codes and data are publicly available at https://github.com/RecklessRonan/PGNN-EK</div></div><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.480.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--480 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.480 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.480/>Good for Misconceived Reasons : An Empirical Revisiting on the Need for Visual Context in Multimodal Machine Translation</a></strong><br><a href=/people/z/zhiyong-wu/>Zhiyong Wu</a>
|
<a href=/people/l/lingpeng-kong/>Lingpeng Kong</a>
|
<a href=/people/w/wei-bi/>Wei Bi</a>
|
<a href=/people/x/xiang-li/>Xiang Li</a>
|
<a href=/people/b/ben-kao/>Ben Kao</a><br><a href=/volumes/2021.acl-long/ class=text-muted>Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--480><div class="card-body p-3 small">A neural multimodal machine translation (MMT) system is one that aims to perform better <a href=https://en.wikipedia.org/wiki/Translation>translation</a> by extending conventional text-only translation models with multimodal information. Many recent studies report improvements when equipping their <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> with the multimodal module, despite the controversy of whether such improvements indeed come from the multimodal part. We revisit the contribution of <a href=https://en.wikipedia.org/wiki/Multimodal_interaction>multimodal information</a> in <a href=https://en.wikipedia.org/wiki/Multimodal_interaction>MMT</a> by devising two interpretable MMT models. To our surprise, although our <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> replicate similar gains as recently developed multimodal-integrated systems achieved, our <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> learn to ignore the multimodal information. Upon further investigation, we discover that the improvements achieved by the multimodal models over text-only counterparts are in fact results of the regularization effect. We report empirical findings that highlight the importance of MMT models&#8217; interpretability, and discuss how our findings will benefit future research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.semeval-1.63.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--semeval-1--63 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.semeval-1.63 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.semeval-1.63/>HITSZ-HLT at SemEval-2021 Task 5 : Ensemble Sequence Labeling and Span Boundary Detection for Toxic Span Detection<span class=acl-fixed-case>HITSZ</span>-<span class=acl-fixed-case>HLT</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2021 Task 5: Ensemble Sequence Labeling and Span Boundary Detection for Toxic Span Detection</a></strong><br><a href=/people/q/qinglin-zhu/>Qinglin Zhu</a>
|
<a href=/people/z/zijie-lin/>Zijie Lin</a>
|
<a href=/people/y/yice-zhang/>Yice Zhang</a>
|
<a href=/people/j/jingyi-sun/>Jingyi Sun</a>
|
<a href=/people/x/xiang-li/>Xiang Li</a>
|
<a href=/people/q/qihui-lin/>Qihui Lin</a>
|
<a href=/people/y/yixue-dang/>Yixue Dang</a>
|
<a href=/people/r/ruifeng-xu/>Ruifeng Xu</a><br><a href=/volumes/2021.semeval-1/ class=text-muted>Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--semeval-1--63><div class="card-body p-3 small">This paper presents the winning <a href=https://en.wikipedia.org/wiki/System>system</a> that participated in SemEval-2021 Task 5 : Toxic Spans Detection. This task aims to locate those spans that attribute to the text&#8217;s toxicity within a text, which is crucial for semi-automated moderation in online discussions. We formalize this task as the Sequence Labeling (SL) problem and the Span Boundary Detection (SBD) problem separately and employ three state-of-the-art models. Next, we integrate predictions of these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> to produce a more credible and complement result. Our <a href=https://en.wikipedia.org/wiki/System>system</a> achieves a char-level score of 70.83 %, ranking 1/91. In addition, we also explore the lexicon-based method, which is strongly interpretable and flexible in practice.</div></div><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.acl-demos.19.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--acl-demos--19 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.acl-demos.19 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://slideslive.com/38928597 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.acl-demos.19" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.acl-demos.19/>ConvLab-2 : An Open-Source Toolkit for Building, Evaluating, and Diagnosing Dialogue Systems<span class=acl-fixed-case>C</span>onv<span class=acl-fixed-case>L</span>ab-2: An Open-Source Toolkit for Building, Evaluating, and Diagnosing Dialogue Systems</a></strong><br><a href=/people/q/qi-zhu/>Qi Zhu</a>
|
<a href=/people/z/zheng-zhang/>Zheng Zhang</a>
|
<a href=/people/y/yan-fang/>Yan Fang</a>
|
<a href=/people/x/xiang-li/>Xiang Li</a>
|
<a href=/people/r/ryuichi-takanobu/>Ryuichi Takanobu</a>
|
<a href=/people/j/jinchao-li/>Jinchao Li</a>
|
<a href=/people/b/baolin-peng/>Baolin Peng</a>
|
<a href=/people/j/jianfeng-gao/>Jianfeng Gao</a>
|
<a href=/people/x/xiaoyan-zhu/>Xiaoyan Zhu</a>
|
<a href=/people/m/minlie-huang/>Minlie Huang</a><br><a href=/volumes/2020.acl-demos/ class=text-muted>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--acl-demos--19><div class="card-body p-3 small">We present ConvLab-2, an open-source toolkit that enables researchers to build task-oriented dialogue systems with state-of-the-art models, perform an end-to-end evaluation, and diagnose the weakness of systems. As the successor of ConvLab, ConvLab-2 inherits ConvLab&#8217;s framework but integrates more powerful dialogue models and supports more datasets. Besides, we have developed an <a href=https://en.wikipedia.org/wiki/Analysis>analysis tool</a> and an <a href=https://en.wikipedia.org/wiki/Interactive_computing>interactive tool</a> to assist researchers in diagnosing <a href=https://en.wikipedia.org/wiki/Dialogue_system>dialogue systems</a>. The analysis tool presents rich statistics and summarizes common mistakes from simulated dialogues, which facilitates error analysis and <a href=https://en.wikipedia.org/wiki/Systems_engineering>system improvement</a>. The interactive tool provides an <a href=https://en.wikipedia.org/wiki/User_interface>user interface</a> that allows developers to diagnose an assembled dialogue system by interacting with the <a href=https://en.wikipedia.org/wiki/System>system</a> and modifying the output of each system component.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.autosimtrans-1.5.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--autosimtrans-1--5 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.autosimtrans-1.5 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://slideslive.com/38929921 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.autosimtrans-1.5/>Modeling Discourse Structure for Document-level Neural Machine Translation</a></strong><br><a href=/people/j/junxuan-chen/>Junxuan Chen</a>
|
<a href=/people/x/xiang-li/>Xiang Li</a>
|
<a href=/people/j/jiarui-zhang/>Jiarui Zhang</a>
|
<a href=/people/c/chulun-zhou/>Chulun Zhou</a>
|
<a href=/people/j/jianwei-cui/>Jianwei Cui</a>
|
<a href=/people/b/bin-wang/>Bin Wang</a>
|
<a href=/people/j/jinsong-su/>Jinsong Su</a><br><a href=/volumes/2020.autosimtrans-1/ class=text-muted>Proceedings of the First Workshop on Automatic Simultaneous Translation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--autosimtrans-1--5><div class="card-body p-3 small">Recently, document-level neural machine translation (NMT) has become a hot topic in the community of <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>. Despite its success, most of existing studies ignored the discourse structure information of the input document to be translated, which has shown effective in other tasks. In this paper, we propose to improve document-level NMT with the aid of discourse structure information. Our <a href=https://en.wikipedia.org/wiki/Encoder>encoder</a> is based on a hierarchical attention network (HAN) (Miculicich et al., 2018). Specifically, we first parse the input document to obtain its discourse structure. Then, we introduce a Transformer-based path encoder to embed the discourse structure information of each word. Finally, we combine the discourse structure information with the <a href=https://en.wikipedia.org/wiki/Word_embedding>word embedding</a> before it is fed into the <a href=https://en.wikipedia.org/wiki/Encoder>encoder</a>. Experimental results on the English-to-German dataset show that our model can significantly outperform both Transformer and Transformer+HAN.</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1041.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1041 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1041 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=C18-1041" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/C18-1041/>Few-Shot Charge Prediction with Discriminative Legal Attributes</a></strong><br><a href=/people/z/zikun-hu/>Zikun Hu</a>
|
<a href=/people/x/xiang-li/>Xiang Li</a>
|
<a href=/people/c/cunchao-tu/>Cunchao Tu</a>
|
<a href=/people/z/zhiyuan-liu/>Zhiyuan Liu</a>
|
<a href=/people/m/maosong-sun/>Maosong Sun</a><br><a href=/volumes/C18-1/ class=text-muted>Proceedings of the 27th International Conference on Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1041><div class="card-body p-3 small">Automatic charge prediction aims to predict the final charges according to the fact descriptions in criminal cases and plays a crucial role in legal assistant systems. Existing works on charge prediction perform adequately on those high-frequency charges but are not yet capable of predicting few-shot charges with limited cases. Moreover, these exist many confusing charge pairs, whose fact descriptions are fairly similar to each other. To address these issues, we introduce several discriminative attributes of charges as the internal mapping between fact descriptions and <a href=https://en.wikipedia.org/wiki/Charge_(physics)>charges</a>. These attributes provide additional information for few-shot charges, as well as effective signals for distinguishing confusing charges. More specifically, we propose an attribute-attentive charge prediction model to infer the attributes and charges simultaneously. Experimental results on real-work datasets demonstrate that our proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieves significant and consistent improvements than other state-of-the-art baselines. Specifically, our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms other baselines by more than 50 % in the few-shot scenario. Our codes and datasets can be obtained from https://github.com/thunlp/attribute_charge.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1025.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1025 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1025 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1025.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-1025/>Probabilistic Embedding of Knowledge Graphs with Box Lattice Measures</a></strong><br><a href=/people/l/luke-vilnis/>Luke Vilnis</a>
|
<a href=/people/x/xiang-li/>Xiang Li</a>
|
<a href=/people/s/shikhar-murty/>Shikhar Murty</a>
|
<a href=/people/a/andrew-mccallum/>Andrew McCallum</a><br><a href=/volumes/P18-1/ class=text-muted>Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1025><div class="card-body p-3 small">Embedding methods which enforce a partial order or lattice structure over the concept space, such as Order Embeddings (OE), are a natural way to model transitive relational data (e.g. entailment graphs). However, OE learns a deterministic knowledge base, limiting expressiveness of queries and the ability to use <a href=https://en.wikipedia.org/wiki/Uncertainty>uncertainty</a> for both <a href=https://en.wikipedia.org/wiki/Prediction>prediction</a> and <a href=https://en.wikipedia.org/wiki/Machine_learning>learning</a> (e.g. learning from expectations). Probabilistic extensions of OE have provided the ability to somewhat calibrate these denotational probabilities while retaining the consistency and inductive bias of ordered models, but lack the ability to model the negative correlations found in real-world knowledge. In this work we show that a broad class of models that assign probability measures to OE can never capture negative correlation, which motivates our construction of a novel box lattice and accompanying <a href=https://en.wikipedia.org/wiki/Probability_measure>probability measure</a> to capture anti-correlation and even disjoint concepts, while still providing the benefits of probabilistic modeling, such as the ability to perform rich joint and conditional queries over arbitrary sets of concepts, and both learning from and predicting calibrated uncertainty. We show improvements over previous approaches in modeling the Flickr and WordNet entailment graphs, and investigate the power of the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Xiang+Li" title="Search for 'Xiang Li' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/z/zikun-hu/ class=align-middle>Zikun Hu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/cunchao-tu/ class=align-middle>Cunchao Tu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/zhiyuan-liu/ class=align-middle>Zhiyuan Liu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/maosong-sun/ class=align-middle>Maosong Sun (孙茂松)</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/zhiyong-wu/ class=align-middle>Zhiyong Wu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/l/lingpeng-kong/ class=align-middle>Lingpeng Kong</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/w/wei-bi/ class=align-middle>Wei Bi</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/b/ben-kao/ class=align-middle>Ben Kao</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/q/qi-zhu/ class=align-middle>Qi Zhu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/zheng-zhang/ class=align-middle>Zheng Zhang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yan-fang/ class=align-middle>Yan Fang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/ryuichi-takanobu/ class=align-middle>Ryuichi Takanobu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jinchao-li/ class=align-middle>Jinchao Li</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/b/baolin-peng/ class=align-middle>Baolin Peng</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jianfeng-gao/ class=align-middle>Jianfeng Gao</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/x/xiaoyan-zhu/ class=align-middle>Xiaoyan Zhu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/minlie-huang/ class=align-middle>Minlie Huang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/renyu-zhu/ class=align-middle>Renyu Zhu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/lei-yuan/ class=align-middle>Lei Yuan</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/ming-gao/ class=align-middle>Ming Gao</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/w/wenyuan-cai/ class=align-middle>Wenyuan Cai</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/junxuan-chen/ class=align-middle>Junxuan Chen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jiarui-zhang/ class=align-middle>Jiarui Zhang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/chulun-zhou/ class=align-middle>Chulun Zhou</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jianwei-cui/ class=align-middle>Jianwei Cui</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/b/bin-wang/ class=align-middle>Bin Wang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jinsong-su/ class=align-middle>Jinsong Su</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/q/qinglin-zhu/ class=align-middle>Qinglin Zhu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/zijie-lin/ class=align-middle>Zijie Lin</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yice-zhang/ class=align-middle>Yice Zhang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jingyi-sun/ class=align-middle>Jingyi Sun</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/q/qihui-lin/ class=align-middle>Qihui Lin</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yixue-dang/ class=align-middle>Yixue Dang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/ruifeng-xu/ class=align-middle>Ruifeng Xu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/luke-vilnis/ class=align-middle>Luke Vilnis</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/shikhar-murty/ class=align-middle>Shikhar Murty</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/andrew-mccallum/ class=align-middle>Andrew McCallum</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/acl/ class=align-middle>ACL</a><span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/venues/coling/ class=align-middle>COLING</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/autosimtrans/ class=align-middle>AutoSimTrans</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/semeval/ class=align-middle>SemEval</a><span class="badge badge-secondary align-middle ml-2">1</span></li></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>