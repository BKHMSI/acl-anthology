<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Xiaoyu Shen - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Xiaoyu</span> <span class=font-weight-bold>Shen</span></h2><hr><div class=row><div class=col-lg-9><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.64.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--64 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.64 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.64/>Neural Data-to-Text Generation with LM-based Text Augmentation<span class=acl-fixed-case>LM</span>-based Text Augmentation</a></strong><br><a href=/people/e/ernie-chang/>Ernie Chang</a>
|
<a href=/people/x/xiaoyu-shen/>Xiaoyu Shen</a>
|
<a href=/people/d/dawei-zhu/>Dawei Zhu</a>
|
<a href=/people/v/vera-demberg/>Vera Demberg</a>
|
<a href=/people/h/hui-su/>Hui Su</a><br><a href=/volumes/2021.eacl-main/ class=text-muted>Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--64><div class="card-body p-3 small">For many new application domains for data-to-text generation, the main obstacle in training neural models consists of a lack of training data. While usually large numbers of instances are available on the data side, often only very few text samples are available. To address this problem, we here propose a novel few-shot approach for this setting. Our approach automatically augments the <a href=https://en.wikipedia.org/wiki/Data>data</a> available for training by (i) generating new text samples based on replacing specific values by alternative ones from the same category, (ii) generating new text samples based on GPT-2, and (iii) proposing an automatic method for pairing the new text samples with <a href=https://en.wikipedia.org/wiki/Data>data samples</a>. As the text augmentation can introduce noise to the training data, we use cycle consistency as an objective, in order to make sure that a given data sample can be correctly reconstructed after having been formulated as text (and that text samples can be reconstructed from data). On both the E2E and WebNLG benchmarks, we show that this weakly supervised training paradigm is able to outperform fully supervised sequence-to-sequence models with less than 10 % of the training set. By utilizing all annotated data, our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> can boost the performance of a standard sequence-to-sequence model by over 5 BLEU points, establishing a new state-of-the-art on both datasets.</div></div><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.acl-main.641.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--acl-main--641 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.acl-main.641 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://slideslive.com/38929190 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.acl-main.641/>Neural Data-to-Text Generation via Jointly Learning the Segmentation and Correspondence</a></strong><br><a href=/people/x/xiaoyu-shen/>Xiaoyu Shen</a>
|
<a href=/people/e/ernie-chang/>Ernie Chang</a>
|
<a href=/people/h/hui-su/>Hui Su</a>
|
<a href=/people/c/cheng-niu/>Cheng Niu</a>
|
<a href=/people/d/dietrich-klakow/>Dietrich Klakow</a><br><a href=/volumes/2020.acl-main/ class=text-muted>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--acl-main--641><div class="card-body p-3 small">The neural attention model has achieved great success in data-to-text generation tasks. Though usually excelling at producing fluent text, it suffers from the problem of information missing, <a href=https://en.wikipedia.org/wiki/Repetition_(rhetorical_device)>repetition</a> and <a href=https://en.wikipedia.org/wiki/Hallucination>hallucination</a>. Due to the black-box nature of the neural attention architecture, avoiding these problems in a systematic way is non-trivial. To address this concern, we propose to explicitly segment target text into fragment units and align them with their data correspondences. The segmentation and correspondence are jointly learned as <a href=https://en.wikipedia.org/wiki/Latent_variable>latent variables</a> without any human annotations. We further impose a soft statistical constraint to regularize the segmental granularity. The resulting architecture maintains the same <a href=https://en.wikipedia.org/wiki/Expressive_power_(computer_science)>expressive power</a> as neural attention models, while being able to generate fully interpretable outputs with several times less <a href=https://en.wikipedia.org/wiki/Computational_cost>computational cost</a>. On both E2E and WebNLG benchmarks, we show the proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> consistently outperforms its neural attention counterparts.</div></div><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1390.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1390 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1390 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1390.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D19-1390/>Improving Latent Alignment in Text Summarization by Generalizing the Pointer Generator</a></strong><br><a href=/people/x/xiaoyu-shen/>Xiaoyu Shen</a>
|
<a href=/people/y/yang-zhao/>Yang Zhao</a>
|
<a href=/people/h/hui-su/>Hui Su</a>
|
<a href=/people/d/dietrich-klakow/>Dietrich Klakow</a><br><a href=/volumes/D19-1/ class=text-muted>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1390><div class="card-body p-3 small">Pointer Generators have been the de facto standard for modern summarization systems. However, this architecture faces two major drawbacks : Firstly, the <a href=https://en.wikipedia.org/wiki/Pointer_(computer_programming)>pointer</a> is limited to copying the exact words while ignoring possible <a href=https://en.wikipedia.org/wiki/Inflection_point>inflections</a> or <a href=https://en.wikipedia.org/wiki/Abstraction_(computer_science)>abstractions</a>, which restricts its power of capturing richer latent alignment. Secondly, the copy mechanism results in a strong bias towards extractive generations, where most sentences are produced by simply copying from the source text. In this paper, we address these problems by allowing the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> to edit pointed tokens instead of always hard copying them. The editing is performed by transforming the pointed word vector into a target space with a learned <a href=https://en.wikipedia.org/wiki/Glossary_of_graph_theory_terms>relation embedding</a>. On three large-scale summarization dataset, we show the model is able to (1) capture more latent alignment relations than exact word matches, (2) improve word alignment accuracy, allowing for better model interpretation and controlling, (3) generate higher-quality summaries validated by both qualitative and quantitative evaluations and (4) bring more abstraction to the generated summaries.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1003.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1003 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1003 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/383951307 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1003/>Improving Multi-turn Dialogue Modelling with Utterance ReWriter<span class=acl-fixed-case>R</span>e<span class=acl-fixed-case>W</span>riter</a></strong><br><a href=/people/h/hui-su/>Hui Su</a>
|
<a href=/people/x/xiaoyu-shen/>Xiaoyu Shen</a>
|
<a href=/people/r/rongzhi-zhang/>Rongzhi Zhang</a>
|
<a href=/people/f/fei-sun/>Fei Sun</a>
|
<a href=/people/p/pengwei-hu/>Pengwei Hu</a>
|
<a href=/people/c/cheng-niu/>Cheng Niu</a>
|
<a href=/people/j/jie-zhou/>Jie Zhou</a><br><a href=/volumes/P19-1/ class=text-muted>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1003><div class="card-body p-3 small">Recent research has achieved impressive results in single-turn dialogue modelling. In the multi-turn setting, however, current <a href=https://en.wikipedia.org/wiki/Computer_simulation>models</a> are still far from satisfactory. One major challenge is the frequently occurred coreference and information omission in our daily conversation, making it hard for machines to understand the real intention. In this paper, we propose rewriting the human utterance as a <a href=https://en.wikipedia.org/wiki/Pre-process>pre-process</a> to help multi-turn dialgoue modelling. Each utterance is first rewritten to recover all coreferred and omitted information. The next processing steps are then performed based on the rewritten utterance. To properly train the utterance rewriter, we collect a new <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> with human annotations and introduce a Transformer-based utterance rewriting architecture using the pointer network. We show the proposed <a href=https://en.wikipedia.org/wiki/Software_architecture>architecture</a> achieves remarkably good performance on the utterance rewriting task. The trained utterance rewriter can be easily integrated into <a href=https://en.wikipedia.org/wiki/Chatbot>online chatbots</a> and brings general improvement over different domains.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1216.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1216 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1216 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P19-1216.Supplementary.pdf data-toggle=tooltip data-placement=top title=Supplementary><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1216/>Unsupervised Rewriter for Multi-Sentence Compression</a></strong><br><a href=/people/y/yang-zhao/>Yang Zhao</a>
|
<a href=/people/x/xiaoyu-shen/>Xiaoyu Shen</a>
|
<a href=/people/w/wei-bi/>Wei Bi</a>
|
<a href=/people/a/akiko-aizawa/>Akiko Aizawa</a><br><a href=/volumes/P19-1/ class=text-muted>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1216><div class="card-body p-3 small">Multi-sentence compression (MSC) aims to generate a grammatical but reduced compression from multiple input sentences while retaining their key information. Previous dominating approach for <a href=https://en.wikipedia.org/wiki/Microsoft_SQL_Server>MSC</a> is the extraction-based word graph approach. A few variants further leveraged <a href=https://en.wikipedia.org/wiki/Lexical_substitution>lexical substitution</a> to yield more abstractive compression. However, two limitations exist. First, the word graph approach that simply concatenates fragments from multiple sentences may yield non-fluent or ungrammatical compression. Second, <a href=https://en.wikipedia.org/wiki/Lexical_substitution>lexical substitution</a> is often inappropriate without the consideration of <a href=https://en.wikipedia.org/wiki/Context_(language_use)>context information</a>. To tackle the above-mentioned issues, we present a neural rewriter for multi-sentence compression that does not need any <a href=https://en.wikipedia.org/wiki/Parallel_text>parallel corpus</a>. Empirical studies have shown that our approach achieves comparable results upon automatic evaluation and improves the <a href=https://en.wikipedia.org/wiki/Grammaticality>grammaticality</a> of compression based on human evaluation. A <a href=https://en.wikipedia.org/wiki/Parallel_corpus>parallel corpus</a> with more than 140,000 (sentence group, compression) pairs is also constructed as a by-product for future research.</div></div><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1099.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1099 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1099 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/I17-1099.Datasets.zip data-toggle=tooltip data-placement=top title=Dataset><i class="fas fa-file-archive"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=I17-1099" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/I17-1099/>DailyDialog : A Manually Labelled Multi-turn Dialogue Dataset<span class=acl-fixed-case>D</span>aily<span class=acl-fixed-case>D</span>ialog: A Manually Labelled Multi-turn Dialogue Dataset</a></strong><br><a href=/people/y/yanran-li/>Yanran Li</a>
|
<a href=/people/h/hui-su/>Hui Su</a>
|
<a href=/people/x/xiaoyu-shen/>Xiaoyu Shen</a>
|
<a href=/people/w/wenjie-li/>Wenjie Li</a>
|
<a href=/people/z/ziqiang-cao/>Ziqiang Cao</a>
|
<a href=/people/s/shuzi-niu/>Shuzi Niu</a><br><a href=/volumes/I17-1/ class=text-muted>Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1099><div class="card-body p-3 small">We develop a high-quality multi-turn dialog dataset, DailyDialog, which is intriguing in several aspects. The <a href=https://en.wikipedia.org/wiki/Language>language</a> is human-written and less noisy. The dialogues in the <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> reflect our daily communication way and cover various topics about our daily life. We also manually label the developed <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> with <a href=https://en.wikipedia.org/wiki/Intentionality>communication intention</a> and <a href=https://en.wikipedia.org/wiki/Emotion>emotion information</a>. Then, we evaluate existing approaches on DailyDialog dataset and hope it benefit the research field of dialog systems. The <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> is available on<b>DailyDialog</b>, which is intriguing in several aspects. The language is human-written and less noisy. The dialogues in the dataset reflect our daily communication way and cover various topics about our daily life. We also manually label the developed dataset with communication intention and emotion information. Then, we evaluate existing approaches on DailyDialog dataset and hope it benefit the research field of dialog systems. The dataset is available on <url>http://yanran.li/dailydialog</url>\n</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2080.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2080 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2080 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-2080/>A Conditional Variational Framework for Dialog Generation</a></strong><br><a href=/people/x/xiaoyu-shen/>Xiaoyu Shen</a>
|
<a href=/people/h/hui-su/>Hui Su</a>
|
<a href=/people/y/yanran-li/>Yanran Li</a>
|
<a href=/people/w/wenjie-li/>Wenjie Li</a>
|
<a href=/people/s/shuzi-niu/>Shuzi Niu</a>
|
<a href=/people/y/yang-zhao/>Yang Zhao</a>
|
<a href=/people/a/akiko-aizawa/>Akiko Aizawa</a>
|
<a href=/people/g/guoping-long/>Guoping Long</a><br><a href=/volumes/P17-2/ class=text-muted>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2080><div class="card-body p-3 small">Deep latent variable models have been shown to facilitate the response generation for open-domain dialog systems. However, these latent variables are highly randomized, leading to uncontrollable generated responses. In this paper, we propose a <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> allowing conditional response generation based on specific attributes. These <a href=https://en.wikipedia.org/wiki/Attribute_(computing)>attributes</a> can be either manually assigned or automatically detected. Moreover, the dialog states for both speakers are modeled separately in order to reflect personal features. We validate this <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> on two different scenarios, where the attribute refers to <a href=https://en.wikipedia.org/wiki/Generic_property>genericness</a> and <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment states</a> respectively. The experiment result testified the potential of our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>, where meaningful responses can be generated in accordance with the specified attributes.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Xiaoyu+Shen" title="Search for 'Xiaoyu Shen' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/h/hui-su/ class=align-middle>Hui Su</a>
<span class="badge badge-secondary align-middle ml-2">6</span></li><li class=list-group-item><a href=/people/y/yang-zhao/ class=align-middle>Yang Zhao</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/y/yanran-li/ class=align-middle>Yanran Li</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/w/wenjie-li/ class=align-middle>Wenjie Li</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/s/shuzi-niu/ class=align-middle>Shuzi Niu</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/e/ernie-chang/ class=align-middle>Ernie Chang</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/c/cheng-niu/ class=align-middle>Cheng Niu</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/d/dietrich-klakow/ class=align-middle>Dietrich Klakow</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/a/akiko-aizawa/ class=align-middle>Akiko Aizawa</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/z/ziqiang-cao/ class=align-middle>Ziqiang Cao</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/g/guoping-long/ class=align-middle>Guoping Long</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/dawei-zhu/ class=align-middle>Dawei Zhu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/v/vera-demberg/ class=align-middle>Vera Demberg</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/rongzhi-zhang/ class=align-middle>Rongzhi Zhang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/f/fei-sun/ class=align-middle>Fei Sun</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/p/pengwei-hu/ class=align-middle>Pengwei Hu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jie-zhou/ class=align-middle>Jie Zhou</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/w/wei-bi/ class=align-middle>Wei Bi</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/acl/ class=align-middle>ACL</a><span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/venues/ijcnlp/ class=align-middle>IJCNLP</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/eacl/ class=align-middle>EACL</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/emnlp/ class=align-middle>EMNLP</a><span class="badge badge-secondary align-middle ml-2">1</span></li></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>