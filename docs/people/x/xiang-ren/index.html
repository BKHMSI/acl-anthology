<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Xiang Ren - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Xiang</span> <span class=font-weight-bold>Ren</span></h2><hr><div class=row><div class=col-lg-9><h4>2022</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.acl-long.192.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--acl-long--192 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.acl-long.192 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2022.acl-long.192.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.acl-long.192" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2022.acl-long.192/>Good Examples Make A Faster Learner: Simple Demonstration-based Learning for Low-resource <span class=acl-fixed-case>NER</span></a></strong><br><a href=/people/d/dong-ho-lee/>Dong-Ho Lee</a>
|
<a href=/people/a/akshen-kadakia/>Akshen Kadakia</a>
|
<a href=/people/k/kangmin-tan/>Kangmin Tan</a>
|
<a href=/people/m/mahak-agarwal/>Mahak Agarwal</a>
|
<a href=/people/x/xinyu-feng/>Xinyu Feng</a>
|
<a href=/people/t/takashi-shibuya/>Takashi Shibuya</a>
|
<a href=/people/r/ryosuke-mitani/>Ryosuke Mitani</a>
|
<a href=/people/t/toshiyuki-sekiya/>Toshiyuki Sekiya</a>
|
<a href=/people/j/jay-pujara/>Jay Pujara</a>
|
<a href=/people/x/xiang-ren/>Xiang Ren</a><br><a href=/volumes/2022.acl-long/ class=text-muted>Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--acl-long--192><div class="card-body p-3 small">Recent advances in prompt-based learning have shown strong results on few-shot text classification by using cloze-style templates.Similar attempts have been made on named entity recognition (NER) which manually design templates to predict entity types for every text span in a sentence. However, such methods may suffer from error propagation induced by entity span detection, high cost due to enumeration of all possible text spans, and omission of inter-dependencies among token labels in a sentence. Here we present a simple demonstration-based learning method for NER, which lets the input be prefaced by task demonstrations for in-context learning. We perform a systematic study on demonstration strategy regarding what to include (entity examples, with or without surrounding context), how to select the examples, and what templates to use. Results on in-domain learning and domain adaptation show that the model&#8217;s performance in low-resource settings can be largely improved with a suitable demonstration strategy (e.g., a 4-17% improvement on 25 train instances). We also find that good demonstration can save many labeled examples and consistency in demonstration contributes to better performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.acl-long.340.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--acl-long--340 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.acl-long.340 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.acl-long.340/><span class=acl-fixed-case>KG</span>-<span class=acl-fixed-case>F</span>i<span class=acl-fixed-case>D</span>: Infusing Knowledge Graph in Fusion-in-Decoder for Open-Domain Question Answering</a></strong><br><a href=/people/d/donghan-yu/>Donghan Yu</a>
|
<a href=/people/c/chenguang-zhu/>Chenguang Zhu</a>
|
<a href=/people/y/yuwei-fang/>Yuwei Fang</a>
|
<a href=/people/w/wenhao-yu/>Wenhao Yu</a>
|
<a href=/people/s/shuohang-wang/>Shuohang Wang</a>
|
<a href=/people/y/yichong-xu/>Yichong Xu</a>
|
<a href=/people/x/xiang-ren/>Xiang Ren</a>
|
<a href=/people/y/yiming-yang/>Yiming Yang</a>
|
<a href=/people/m/michael-zeng/>Michael Zeng</a><br><a href=/volumes/2022.acl-long/ class=text-muted>Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--acl-long--340><div class="card-body p-3 small">Current Open-Domain Question Answering (ODQA) models typically include a retrieving module and a reading module, where the retriever selects potentially relevant passages from open-source documents for a given question, and the reader produces an answer based on the retrieved passages. The recently proposed Fusion-in-Decoder (FiD) framework is a representative example, which is built on top of a dense passage retriever and a generative reader, achieving the state-of-the-art performance. In this paper we further improve the FiD approach by introducing a knowledge-enhanced version, namely KG-FiD. Our new model uses a knowledge graph to establish the structural relationship among the retrieved passages, and a graph neural network (GNN) to re-rank the passages and select only a top few for further processing. Our experiments on common ODQA benchmark datasets (Natural Questions and TriviaQA) demonstrate that KG-FiD can achieve comparable or better performance in answer prediction than FiD, with less than 40% of the computation cost.</div></div><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.357.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--357 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.357 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.357/>ForecastQA : A Question Answering Challenge for Event Forecasting with Temporal Text Data<span class=acl-fixed-case>F</span>orecast<span class=acl-fixed-case>QA</span>: A Question Answering Challenge for Event Forecasting with Temporal Text Data</a></strong><br><a href=/people/w/woojeong-jin/>Woojeong Jin</a>
|
<a href=/people/r/rahul-khanna/>Rahul Khanna</a>
|
<a href=/people/s/suji-kim/>Suji Kim</a>
|
<a href=/people/d/dong-ho-lee/>Dong-Ho Lee</a>
|
<a href=/people/f/fred-morstatter/>Fred Morstatter</a>
|
<a href=/people/a/aram-galstyan/>Aram Galstyan</a>
|
<a href=/people/x/xiang-ren/>Xiang Ren</a><br><a href=/volumes/2021.acl-long/ class=text-muted>Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--357><div class="card-body p-3 small">Event forecasting is a challenging, yet important task, as humans seek to constantly plan for the future. Existing automated forecasting studies rely mostly on structured data, such as time-series or event-based knowledge graphs, to help predict future events. In this work, we aim to formulate a <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>, construct a <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>, and provide benchmarks for developing methods for event forecasting with large volumes of <a href=https://en.wikipedia.org/wiki/Unstructured_data>unstructured text data</a>. To simulate the forecasting scenario on temporal news documents, we formulate the problem as a restricted-domain, multiple-choice, question-answering (QA) task. Unlike existing QA tasks, our <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> limits accessible information, and thus a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> has to make a forecasting judgement. To showcase the usefulness of this task formulation, we introduce ForecastQA, a <a href=https://en.wikipedia.org/wiki/Question_answering>question-answering dataset</a> consisting of 10,392 event forecasting questions, which have been collected and verified via crowdsourcing efforts. We present our experiments on ForecastQA using BERTbased models and find that our best <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieves 61.0 % <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> on the <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>, which still lags behind human performance by about 19 %. We hope ForecastQA will support future research efforts in bridging this gap.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.362.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--362 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.362 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.362/>AdaTag : Multi-Attribute Value Extraction from Product Profiles with Adaptive Decoding<span class=acl-fixed-case>A</span>da<span class=acl-fixed-case>T</span>ag: Multi-Attribute Value Extraction from Product Profiles with Adaptive Decoding</a></strong><br><a href=/people/j/jun-yan/>Jun Yan</a>
|
<a href=/people/n/nasser-zalmout/>Nasser Zalmout</a>
|
<a href=/people/y/yan-liang/>Yan Liang</a>
|
<a href=/people/c/christan-grant/>Christan Grant</a>
|
<a href=/people/x/xiang-ren/>Xiang Ren</a>
|
<a href=/people/x/xin-luna-dong/>Xin Luna Dong</a><br><a href=/volumes/2021.acl-long/ class=text-muted>Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--362><div class="card-body p-3 small">Automatic extraction of product attribute values is an important enabling technology in <a href=https://en.wikipedia.org/wiki/E-commerce>e-Commerce platforms</a>. This <a href=https://en.wikipedia.org/wiki/Task_(computing)>task</a> is usually modeled using sequence labeling architectures, with several extensions to handle multi-attribute extraction. One line of previous work constructs attribute-specific models, through separate <a href=https://en.wikipedia.org/wiki/Code>decoders</a> or entirely separate models. However, this approach constrains <a href=https://en.wikipedia.org/wiki/Knowledge_sharing>knowledge sharing</a> across different attributes. Other contributions use a single multi-attribute model, with different techniques to embed <a href=https://en.wikipedia.org/wiki/Attribute_(computing)>attribute information</a>. But sharing the entire network parameters across all attributes can limit the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>&#8217;s capacity to capture attribute-specific characteristics. In this paper we present AdaTag, which uses adaptive decoding to handle <a href=https://en.wikipedia.org/wiki/Data_extraction>extraction</a>. We parameterize the decoder with pretrained attribute embeddings, through a hypernetwork and a Mixture-of-Experts (MoE) module. This allows for separate, but semantically correlated, <a href=https://en.wikipedia.org/wiki/Code>decoders</a> to be generated on the fly for different attributes. This approach facilitates <a href=https://en.wikipedia.org/wiki/Knowledge_sharing>knowledge sharing</a>, while maintaining the specificity of each attribute. Our experiments on a real-world e-Commerce dataset show marked improvements over previous methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.302.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--302 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.302 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.302" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.302/>RockNER : A Simple Method to Create Adversarial Examples for Evaluating the Robustness of Named Entity Recognition Models<span class=acl-fixed-case>R</span>ock<span class=acl-fixed-case>NER</span>: A Simple Method to Create Adversarial Examples for Evaluating the Robustness of Named Entity Recognition Models</a></strong><br><a href=/people/b/bill-yuchen-lin/>Bill Yuchen Lin</a>
|
<a href=/people/w/wenyang-gao/>Wenyang Gao</a>
|
<a href=/people/j/jun-yan/>Jun Yan</a>
|
<a href=/people/r/ryan-moreno/>Ryan Moreno</a>
|
<a href=/people/x/xiang-ren/>Xiang Ren</a><br><a href=/volumes/2021.emnlp-main/ class=text-muted>Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--302><div class="card-body p-3 small">To audit the <a href=https://en.wikipedia.org/wiki/Robustness_(computer_science)>robustness</a> of named entity recognition (NER) models, we propose RockNER, a simple yet effective method to create natural adversarial examples. Specifically, at the entity level, we replace target entities with other entities of the same semantic class in <a href=https://en.wikipedia.org/wiki/Wikidata>Wikidata</a> ; at the <a href=https://en.wikipedia.org/wiki/Context_(language_use)>context level</a>, we use pre-trained <a href=https://en.wikipedia.org/wiki/Language_model>language models</a> (e.g., BERT) to generate word substitutions. Together, the two levels of at- tack produce natural adversarial examples that result in a shifted distribution from the training data on which our target models have been trained. We apply the proposed method to the OntoNotes dataset and create a new benchmark named OntoRock for evaluating the robustness of existing NER models via a systematic evaluation protocol. Our experiments and analysis reveal that even the best <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> has a significant performance drop, and these models seem to memorize in-domain entity patterns instead of reasoning from the context. Our work also studies the effects of a few simple data augmentation methods to improve the <a href=https://en.wikipedia.org/wiki/Robust_statistics>robustness</a> of NER models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.436.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--436 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.436 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.436" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.436/>ECONET : Effective Continual Pretraining of <a href=https://en.wikipedia.org/wiki/Language_model>Language Models</a> for Event Temporal Reasoning<span class=acl-fixed-case>ECONET</span>: Effective Continual Pretraining of Language Models for Event Temporal Reasoning</a></strong><br><a href=/people/r/rujun-han/>Rujun Han</a>
|
<a href=/people/x/xiang-ren/>Xiang Ren</a>
|
<a href=/people/n/nanyun-peng/>Nanyun Peng</a><br><a href=/volumes/2021.emnlp-main/ class=text-muted>Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--436><div class="card-body p-3 small">While pre-trained language models (PTLMs) have achieved noticeable success on many NLP tasks, they still struggle for tasks that require event temporal reasoning, which is essential for event-centric applications. We present a continual pre-training approach that equips PTLMs with targeted knowledge about event temporal relations. We design self-supervised learning objectives to recover masked-out event and temporal indicators and to discriminate sentences from their corrupted counterparts (where event or temporal indicators got replaced). By further pre-training a PTLM with these objectives jointly, we reinforce its attention to event and temporal information, yielding enhanced capability on event temporal reasoning. This * * E**ffective * * CON**tinual pre-training framework for * * E**vent * * T**emporal reasoning (ECONET) improves the PTLMs&#8217; fine-tuning performances across five relation extraction and question answering tasks and achieves new or on-par state-of-the-art performances in most of our downstream tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.598.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--598 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.598 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.598/>RICA : Evaluating Robust Inference Capabilities Based on Commonsense Axioms<span class=acl-fixed-case>RICA</span>: Evaluating Robust Inference Capabilities Based on Commonsense Axioms</a></strong><br><a href=/people/p/pei-zhou/>Pei Zhou</a>
|
<a href=/people/r/rahul-khanna/>Rahul Khanna</a>
|
<a href=/people/s/seyeon-lee/>Seyeon Lee</a>
|
<a href=/people/b/bill-yuchen-lin/>Bill Yuchen Lin</a>
|
<a href=/people/d/daniel-ho/>Daniel Ho</a>
|
<a href=/people/j/jay-pujara/>Jay Pujara</a>
|
<a href=/people/x/xiang-ren/>Xiang Ren</a><br><a href=/volumes/2021.emnlp-main/ class=text-muted>Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--598><div class="card-body p-3 small">Pre-trained language models (PTLMs) have achieved impressive performance on commonsense inference benchmarks, but their ability to employ commonsense to make robust inferences, which is crucial for effective communications with humans, is debated. In the pursuit of advancing fluid human-AI communication, we propose a new challenge, RICA : Robust Inference using Commonsense Axioms, that evaluates robust commonsense inference despite textual perturbations. To generate data for this challenge, we develop a systematic and scalable procedure using commonsense knowledge bases and probe PTLMs across two different evaluation settings. Extensive experiments on our generated probe sets with more than 10k statements show that PTLMs perform no better than random guessing on the zero-shot setting, are heavily impacted by statistical biases, and are not robust to perturbation attacks. We also find that <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a> on similar statements offer limited gains, as PTLMs still fail to generalize to unseen inferences. Our new large-scale benchmark exposes a significant gap between PTLMs and human-level language understanding and offers a new challenge for PTLMs to demonstrate commonsense.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.805.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--805 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.805 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.805" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.805/>Discretized Integrated Gradients for Explaining Language Models</a></strong><br><a href=/people/s/soumya-sanyal/>Soumya Sanyal</a>
|
<a href=/people/x/xiang-ren/>Xiang Ren</a><br><a href=/volumes/2021.emnlp-main/ class=text-muted>Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--805><div class="card-body p-3 small">As a prominent attribution-based explanation algorithm, Integrated Gradients (IG) is widely adopted due to its desirable explanation axioms and the ease of gradient computation. It measures feature importance by averaging the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>&#8217;s output gradient interpolated along a straight-line path in the input data space. However, such straight-line interpolated points are not representative of text data due to the inherent discreteness of the word embedding space. This questions the faithfulness of the <a href=https://en.wikipedia.org/wiki/Gradient>gradients</a> computed at the interpolated points and consequently, the quality of the generated explanations. Here we propose Discretized Integrated Gradients (DIG), which allows effective attribution along non-linear interpolation paths. We develop two interpolation strategies for the discrete word embedding space that generates interpolation points that lie close to actual words in the <a href=https://en.wikipedia.org/wiki/Glossary_of_graph_theory_terms>embedding space</a>, yielding more faithful gradient computation. We demonstrate the effectiveness of DIG over IG through experimental and human evaluations on multiple sentiment classification datasets. We provide the source code of DIG to encourage reproducible research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.woah-1.10.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--woah-1--10 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.woah-1.10 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.woah-1.10/>Improving Counterfactual Generation for Fair Hate Speech Detection</a></strong><br><a href=/people/a/aida-mostafazadeh-davani/>Aida Mostafazadeh Davani</a>
|
<a href=/people/a/ali-omrani/>Ali Omrani</a>
|
<a href=/people/b/brendan-kennedy/>Brendan Kennedy</a>
|
<a href=/people/m/mohammad-atari/>Mohammad Atari</a>
|
<a href=/people/x/xiang-ren/>Xiang Ren</a>
|
<a href=/people/m/morteza-dehghani/>Morteza Dehghani</a><br><a href=/volumes/2021.woah-1/ class=text-muted>Proceedings of the 5th Workshop on Online Abuse and Harms (WOAH 2021)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--woah-1--10><div class="card-body p-3 small">Bias mitigation approaches reduce models&#8217; dependence on sensitive features of data, such as social group tokens (SGTs), resulting in equal predictions across the sensitive features. In <a href=https://en.wikipedia.org/wiki/Hate_speech>hate speech detection</a>, however, equalizing model predictions may ignore important differences among targeted social groups, as <a href=https://en.wikipedia.org/wiki/Hate_speech>hate speech</a> can contain stereotypical language specific to each SGT. Here, to take the specific language about each SGT into account, we rely on counterfactual fairness and equalize predictions among counterfactuals, generated by changing the SGTs. Our method evaluates the similarity in sentence likelihoods (via pre-trained language models) among <a href=https://en.wikipedia.org/wiki/Counterfactual_conditional>counterfactuals</a>, to treat SGTs equally only within interchangeable contexts. By applying logit pairing to equalize outcomes on the restricted set of counterfactuals for each instance, we improve fairness metrics while preserving model performance on hate speech detection.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.283.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--283 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.283 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.naacl-main.283" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.283/>X-METRA-ADA : Cross-lingual Meta-Transfer learning Adaptation to <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>Natural Language Understanding</a> and Question Answering<span class=acl-fixed-case>X</span>-<span class=acl-fixed-case>METRA</span>-<span class=acl-fixed-case>ADA</span>: Cross-lingual Meta-Transfer learning Adaptation to Natural Language Understanding and Question Answering</a></strong><br><a href=/people/m/meryem-mhamdi/>Meryem M’hamdi</a>
|
<a href=/people/d/doo-soon-kim/>Doo Soon Kim</a>
|
<a href=/people/f/franck-dernoncourt/>Franck Dernoncourt</a>
|
<a href=/people/t/trung-bui/>Trung Bui</a>
|
<a href=/people/x/xiang-ren/>Xiang Ren</a>
|
<a href=/people/j/jonathan-may/>Jonathan May</a><br><a href=/volumes/2021.naacl-main/ class=text-muted>Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--283><div class="card-body p-3 small">Multilingual models, such as M-BERT and XLM-R, have gained increasing popularity, due to their zero-shot cross-lingual transfer learning capabilities. However, their <a href=https://en.wikipedia.org/wiki/Generalization>generalization ability</a> is still inconsistent for <a href=https://en.wikipedia.org/wiki/Linguistic_typology>typologically diverse languages</a> and across different benchmarks. Recently, <a href=https://en.wikipedia.org/wiki/Meta-learning>meta-learning</a> has garnered attention as a promising technique for enhancing <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a> under low-resource scenarios : particularly for cross-lingual transfer in Natural Language Understanding (NLU). In this work, we propose X-METRA-ADA, a cross-lingual MEta-TRAnsfer learning ADAptation approach for NLU. Our approach adapts MAML, an optimization-based meta-learning approach, to learn to adapt to new languages. We extensively evaluate our framework on two challenging cross-lingual NLU tasks : multilingual task-oriented dialog and typologically diverse question answering. We show that our approach outperforms naive fine-tuning, reaching competitive performance on both tasks for most languages. Our analysis reveals that X-METRA-ADA can leverage limited data for faster adaptation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.335.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--335 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.335 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.335/>TaxoClass : Hierarchical Multi-Label Text Classification Using Only Class Names<span class=acl-fixed-case>T</span>axo<span class=acl-fixed-case>C</span>lass: Hierarchical Multi-Label Text Classification Using Only Class Names</a></strong><br><a href=/people/j/jiaming-shen/>Jiaming Shen</a>
|
<a href=/people/w/wenda-qiu/>Wenda Qiu</a>
|
<a href=/people/y/yu-meng/>Yu Meng</a>
|
<a href=/people/j/jingbo-shang/>Jingbo Shang</a>
|
<a href=/people/x/xiang-ren/>Xiang Ren</a>
|
<a href=/people/j/jiawei-han/>Jiawei Han</a><br><a href=/volumes/2021.naacl-main/ class=text-muted>Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--335><div class="card-body p-3 small">Hierarchical multi-label text classification (HMTC) aims to tag each document with a set of classes from a <a href=https://en.wikipedia.org/wiki/Taxonomic_rank>taxonomic class hierarchy</a>. Most existing HMTC methods train <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifiers</a> using massive human-labeled documents, which are often too costly to obtain in real-world applications. In this paper, we explore to conduct HMTC based on only class surface names as supervision signals. We observe that to perform HMTC, human experts typically first pinpoint a few most essential classes for the document as its core classes, and then check core classes&#8217; ancestor classes to ensure the coverage. To mimic human experts, we propose a novel HMTC framework, named TaxoClass. Specifically, TaxoClass (1) calculates document-class similarities using a textual entailment model, (2) identifies a document&#8217;s core classes and utilizes confident core classes to train a taxonomy-enhanced classifier, and (3) generalizes the <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifier</a> via multi-label self-training. Our experiments on two challenging datasets show TaxoClass can achieve around 0.71 Example-F1 using only <a href=https://en.wikipedia.org/wiki/Class_(computer_programming)>class names</a>, outperforming the best previous <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>method</a> by 25 %.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.trustnlp-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.trustnlp-1.0/>Proceedings of the First Workshop on Trustworthy Natural Language Processing</a></strong><br><a href=/people/y/yada-pruksachatkun/>Yada Pruksachatkun</a>
|
<a href=/people/a/anil-ramakrishna/>Anil Ramakrishna</a>
|
<a href=/people/k/kai-wei-chang/>Kai-Wei Chang</a>
|
<a href=/people/s/satyapriya-krishna/>Satyapriya Krishna</a>
|
<a href=/people/j/jwala-dhamala/>Jwala Dhamala</a>
|
<a href=/people/t/tanaya-guha/>Tanaya Guha</a>
|
<a href=/people/x/xiang-ren/>Xiang Ren</a><br><a href=/volumes/2021.trustnlp-1/ class=text-muted>Proceedings of the First Workshop on Trustworthy Natural Language Processing</a></span></p><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.136.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--136 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.136 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938676 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.136" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.136/>Multi-document Summarization with Maximal Marginal Relevance-guided Reinforcement Learning</a></strong><br><a href=/people/y/yuning-mao/>Yuning Mao</a>
|
<a href=/people/y/yanru-qu/>Yanru Qu</a>
|
<a href=/people/y/yiqing-xie/>Yiqing Xie</a>
|
<a href=/people/x/xiang-ren/>Xiang Ren</a>
|
<a href=/people/j/jiawei-han/>Jiawei Han</a><br><a href=/volumes/2020.emnlp-main/ class=text-muted>Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--136><div class="card-body p-3 small">While neural sequence learning methods have made significant progress in single-document summarization (SDS), they produce unsatisfactory results on <a href=https://en.wikipedia.org/wiki/Multi-document_summarization>multi-document summarization (MDS)</a>. We observe two major challenges when adapting SDS advances to MDS : (1) MDS involves larger search space and yet more limited training data, setting obstacles for neural methods to learn adequate representations ; (2) MDS needs to resolve higher information redundancy among the source documents, which SDS methods are less effective to handle. To close the gap, we present RL-MMR, Maximal Margin Relevance-guided Reinforcement Learning for MDS, which unifies advanced neural SDS methods and statistical measures used in classical MDS. RL-MMR casts MMR guidance on fewer promising candidates, which restrains the search space and thus leads to better <a href=https://en.wikipedia.org/wiki/Representation_learning>representation learning</a>. Additionally, the explicit <a href=https://en.wikipedia.org/wiki/Redundancy_(engineering)>redundancy measure</a> in MMR helps the neural representation of the summary to better capture redundancy. Extensive experiments demonstrate that RL-MMR achieves state-of-the-art performance on benchmark MDS datasets. In particular, we show the benefits of incorporating MMR into end-to-end learning when adapting SDS to MDS in terms of both learning effectiveness and <a href=https://en.wikipedia.org/wiki/Efficiency>efficiency</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.541.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--541 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.541 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938911 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.541/>Recurrent Event Network : Autoregressive Structure Inferenceover Temporal Knowledge Graphs</a></strong><br><a href=/people/w/woojeong-jin/>Woojeong Jin</a>
|
<a href=/people/m/meng-qu/>Meng Qu</a>
|
<a href=/people/x/xisen-jin/>Xisen Jin</a>
|
<a href=/people/x/xiang-ren/>Xiang Ren</a><br><a href=/volumes/2020.emnlp-main/ class=text-muted>Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--541><div class="card-body p-3 small">Knowledge graph reasoning is a critical task in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a>. The <a href=https://en.wikipedia.org/wiki/Task_(computing)>task</a> becomes more challenging on temporal knowledge graphs, where each fact is associated with a timestamp. Most existing methods focus on reasoning at past timestamps and they are not able to predict facts happening in the future. This paper proposes Recurrent Event Network (RE-Net), a novel autoregressive architecture for predicting future interactions. The occurrence of a fact (event) is modeled as a <a href=https://en.wikipedia.org/wiki/Probability_distribution>probability distribution</a> conditioned on temporal sequences of past knowledge graphs. Specifically, our RE-Net employs a recurrent event encoder to encode past facts, and uses a neighborhood aggregator to model the connection of facts at the same timestamp. Future facts can then be inferred in a sequential manner based on the two <a href=https://en.wikipedia.org/wiki/Module_(mathematics)>modules</a>. We evaluate our proposed <a href=https://en.wikipedia.org/wiki/Methodology>method</a> via link prediction at future times on five public datasets. Through extensive experiments, we demonstrate the strength of RE-Net, especially on multi-step inference over future timestamps, and achieve state-of-the-art performance on all five datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.acl-main.193.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--acl-main--193 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.acl-main.193 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://slideslive.com/38928969 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.acl-main.193" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.acl-main.193/>Learning to Contextually Aggregate Multi-Source Supervision for Sequence Labeling</a></strong><br><a href=/people/o/ouyu-lan/>Ouyu Lan</a>
|
<a href=/people/x/xiao-huang/>Xiao Huang</a>
|
<a href=/people/b/bill-yuchen-lin/>Bill Yuchen Lin</a>
|
<a href=/people/h/he-jiang/>He Jiang</a>
|
<a href=/people/l/liyuan-liu/>Liyuan Liu</a>
|
<a href=/people/x/xiang-ren/>Xiang Ren</a><br><a href=/volumes/2020.acl-main/ class=text-muted>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--acl-main--193><div class="card-body p-3 small">Sequence labeling is a fundamental task for a range of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing problems</a>. When used in practice, its performance is largely influenced by the annotation quality and quantity, and meanwhile, obtaining ground truth labels is often costly. In many cases, ground truth labels do not exist, but noisy annotations or <a href=https://en.wikipedia.org/wiki/Annotation>annotations</a> from different domains are accessible. In this paper, we propose a novel framework Consensus Network (ConNet) that can be trained on annotations from multiple sources (e.g., crowd annotation, cross-domain data). It learns individual representation for every source and dynamically aggregates source-specific knowledge by a context-aware attention module. Finally, it leads to a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> reflecting the agreement (consensus) among multiple sources. We evaluate the proposed framework in two practical settings of multi-source learning : learning with crowd annotations and unsupervised cross-domain model adaptation. Extensive experimental results show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieves significant improvements over existing <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> in both settings. We also demonstrate that the <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>method</a> can apply to various <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> and cope with different <a href=https://en.wikipedia.org/wiki/Code>encoders</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.145.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--145 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.145 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.145" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.145/>Teaching Machine Comprehension with Compositional Explanations</a></strong><br><a href=/people/q/qinyuan-ye/>Qinyuan Ye</a>
|
<a href=/people/x/xiao-huang/>Xiao Huang</a>
|
<a href=/people/e/elizabeth-boschee/>Elizabeth Boschee</a>
|
<a href=/people/x/xiang-ren/>Xiang Ren</a><br><a href=/volumes/2020.findings-emnlp/ class=text-muted>Findings of the Association for Computational Linguistics: EMNLP 2020</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--145><div class="card-body p-3 small">Advances in machine reading comprehension (MRC) rely heavily on the collection of large scale human-annotated examples in the form of (question, paragraph, answer) triples. In contrast, humans are typically able to generalize with only a few examples, relying on deeper underlying <a href=https://en.wikipedia.org/wiki/World_knowledge>world knowledge</a>, linguistic sophistication, and/or simply superior <a href=https://en.wikipedia.org/wiki/Deductive_reasoning>deductive powers</a>. In this paper, we focus on teaching machines reading comprehension, using a small number of semi-structured explanations that explicitly inform machines why answer spans are correct. We extract structured variables and rules from explanations and compose neural module teachers that annotate instances for training downstream MRC models. We use learnable neural modules and soft logic to handle linguistic variation and overcome sparse coverage ; the modules are jointly optimized with the MRC model to improve final performance. On the SQuAD dataset, our proposed method achieves 70.14 % <a href=https://en.wikipedia.org/wiki/F-number>F1 score</a> with supervision from 26 explanations, comparable to plain <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised learning</a> using 1,100 labeled instances, yielding a 12x speed up.</div></div><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1026.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1026 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1026 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1026.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1026" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1026/>Learning Dynamic Context Augmentation for Global Entity Linking</a></strong><br><a href=/people/x/xiyuan-yang/>Xiyuan Yang</a>
|
<a href=/people/x/xiaotao-gu/>Xiaotao Gu</a>
|
<a href=/people/s/sheng-lin/>Sheng Lin</a>
|
<a href=/people/s/siliang-tang/>Siliang Tang</a>
|
<a href=/people/y/yueting-zhuang/>Yueting Zhuang</a>
|
<a href=/people/f/fei-wu/>Fei Wu</a>
|
<a href=/people/z/zhigang-chen/>Zhigang Chen</a>
|
<a href=/people/g/guoping-hu/>Guoping Hu</a>
|
<a href=/people/x/xiang-ren/>Xiang Ren</a><br><a href=/volumes/D19-1/ class=text-muted>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1026><div class="card-body p-3 small">Despite of the recent success of collective entity linking (EL) methods, these global inference methods may yield sub-optimal results when the all-mention coherence assumption breaks, and often suffer from high computational cost at the inference stage, due to the complex search space. In this paper, we propose a simple yet effective solution, called Dynamic Context Augmentation (DCA), for collective EL, which requires only one pass through the mentions in a document. DCA sequentially accumulates context information to make efficient, collective inference, and can cope with different local EL models as a plug-and-enhance module. We explore both supervised and reinforcement learning strategies for learning the DCA model. Extensive experiments show the effectiveness of our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> with different learning settings, base models, <a href=https://en.wikipedia.org/wiki/Decision-making>decision orders</a> and <a href=https://en.wikipedia.org/wiki/Attentional_control>attention mechanisms</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1269.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1269 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1269 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1269.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1269" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1269/>Collaborative Policy Learning for Open Knowledge Graph Reasoning</a></strong><br><a href=/people/c/cong-fu/>Cong Fu</a>
|
<a href=/people/t/tong-chen/>Tong Chen</a>
|
<a href=/people/m/meng-qu/>Meng Qu</a>
|
<a href=/people/w/woojeong-jin/>Woojeong Jin</a>
|
<a href=/people/x/xiang-ren/>Xiang Ren</a><br><a href=/volumes/D19-1/ class=text-muted>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1269><div class="card-body p-3 small">In recent years, there has been a surge of interests in interpretable graph reasoning methods. However, these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> often suffer from limited performance when working on sparse and incomplete graphs, due to the lack of evidential paths that can reach target entities. Here we study open knowledge graph reasoninga task that aims to reason for missing facts over a <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph</a> augmented by a <a href=https://en.wikipedia.org/wiki/Text_corpus>background text corpus</a>. A key challenge of the task is to filter out irrelevant facts extracted from corpus, in order to maintain an effective <a href=https://en.wikipedia.org/wiki/Feasible_region>search space</a> during path inference. We propose a novel reinforcement learning framework to train two <a href=https://en.wikipedia.org/wiki/Collaborative_learning>collaborative agents</a> jointly, i.e., a multi-hop graph reasoner and a fact extractor. The fact extraction agent generates fact triples from corpora to enrich the graph on the fly ; while the reasoning agent provides feedback to the fact extractor and guides it towards promoting facts that are helpful for the interpretable reasoning. Experiments on two public datasets demonstrate the effectiveness of the proposed approach.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1397.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1397 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1397 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1397.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1397" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1397/>Looking Beyond Label Noise : Shifted Label Distribution Matters in Distantly Supervised Relation Extraction</a></strong><br><a href=/people/q/qinyuan-ye/>Qinyuan Ye</a>
|
<a href=/people/l/liyuan-liu/>Liyuan Liu</a>
|
<a href=/people/m/maosen-zhang/>Maosen Zhang</a>
|
<a href=/people/x/xiang-ren/>Xiang Ren</a><br><a href=/volumes/D19-1/ class=text-muted>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1397><div class="card-body p-3 small">In recent years there is a surge of interest in applying distant supervision (DS) to automatically generate training data for relation extraction (RE). In this paper, we study the problem what limits the performance of DS-trained neural models, conduct thorough analyses, and identify a factor that can influence the performance greatly, shifted label distribution. Specifically, we found this problem commonly exists in real-world DS datasets, and without special handing, typical DS-RE models can not automatically adapt to this shift, thus achieving deteriorated performance. To further validate our intuition, we develop a simple yet effective adaptation method for DS-trained models, bias adjustment, which updates <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> learned over the source domain (i.e., DS training set) with a label distribution estimated on the target domain (i.e., test set). Experiments demonstrate that bias adjustment achieves consistent performance gains on DS-trained models, especially on neural models, with an up to 23 % relative F1 improvement, which verifies our assumptions. Our code and data can be found at https://github.com/INK-USC/shifted-label-distribution.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1584.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1584 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1584 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1584.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1584" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1584/>HMEAE : Hierarchical Modular Event Argument Extraction<span class=acl-fixed-case>HMEAE</span>: Hierarchical Modular Event Argument Extraction</a></strong><br><a href=/people/x/xiaozhi-wang/>Xiaozhi Wang</a>
|
<a href=/people/z/ziqi-wang/>Ziqi Wang</a>
|
<a href=/people/x/xu-han/>Xu Han</a>
|
<a href=/people/z/zhiyuan-liu/>Zhiyuan Liu</a>
|
<a href=/people/j/juanzi-li/>Juanzi Li</a>
|
<a href=/people/p/peng-li/>Peng Li</a>
|
<a href=/people/m/maosong-sun/>Maosong Sun</a>
|
<a href=/people/j/jie-zhou/>Jie Zhou</a>
|
<a href=/people/x/xiang-ren/>Xiang Ren</a><br><a href=/volumes/D19-1/ class=text-muted>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1584><div class="card-body p-3 small">Existing <a href=https://en.wikipedia.org/wiki/Event_(probability_theory)>event extraction methods</a> classify each argument role independently, ignoring the conceptual correlations between different argument roles. In this paper, we propose a Hierarchical Modular Event Argument Extraction (HMEAE) model, to provide effective <a href=https://en.wikipedia.org/wiki/Inductive_bias>inductive bias</a> from the concept hierarchy of event argument roles. Specifically, we design a neural module network for each basic unit of the concept hierarchy, and then hierarchically compose relevant unit modules with logical operations into a role-oriented modular network to classify a specific argument role. As many argument roles share the same high-level unit module, their correlation can be utilized to extract specific event arguments better. Experiments on real-world datasets show that HMEAE can effectively leverage useful knowledge from the concept hierarchy and significantly outperform the state-of-the-art baselines. The source code can be obtained from https://github.com/thunlp/HMEAE.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-6100.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-6100/>Proceedings of the 2nd Workshop on Deep Learning Approaches for Low-Resource NLP (DeepLo 2019)</a></strong><br><a href=/people/c/colin-cherry/>Colin Cherry</a>
|
<a href=/people/g/greg-durrett/>Greg Durrett</a>
|
<a href=/people/g/george-foster/>George Foster</a>
|
<a href=/people/g/gholamreza-haffari/>Reza Haffari</a>
|
<a href=/people/s/shahram-khadivi/>Shahram Khadivi</a>
|
<a href=/people/n/nanyun-peng/>Nanyun Peng</a>
|
<a href=/people/x/xiang-ren/>Xiang Ren</a>
|
<a href=/people/s/swabha-swayamdipta/>Swabha Swayamdipta</a><br><a href=/volumes/D19-61/ class=text-muted>Proceedings of the 2nd Workshop on Deep Learning Approaches for Low-Resource NLP (DeepLo 2019)</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4300.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4300/>Proceedings of the 4th Workshop on Representation Learning for NLP (RepL4NLP-2019)</a></strong><br><a href=/people/i/isabelle-augenstein/>Isabelle Augenstein</a>
|
<a href=/people/s/spandana-gella/>Spandana Gella</a>
|
<a href=/people/s/sebastian-ruder/>Sebastian Ruder</a>
|
<a href=/people/k/katharina-kann/>Katharina Kann</a>
|
<a href=/people/b/burcu-can/>Burcu Can</a>
|
<a href=/people/j/johannes-welbl/>Johannes Welbl</a>
|
<a href=/people/a/alexis-conneau/>Alexis Conneau</a>
|
<a href=/people/x/xiang-ren/>Xiang Ren</a>
|
<a href=/people/m/marek-rei/>Marek Rei</a><br><a href=/volumes/W19-43/ class=text-muted>Proceedings of the 4th Workshop on Representation Learning for NLP (RepL4NLP-2019)</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1290.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1290 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1290 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-1290/>Posterior-regularized REINFORCE for Instance Selection in Distant Supervision<span class=acl-fixed-case>REINFORCE</span> for Instance Selection in Distant Supervision</a></strong><br><a href=/people/q/qi-zhang/>Qi Zhang</a>
|
<a href=/people/s/siliang-tang/>Siliang Tang</a>
|
<a href=/people/x/xiang-ren/>Xiang Ren</a>
|
<a href=/people/f/fei-wu/>Fei Wu</a>
|
<a href=/people/s/shiliang-pu/>Shiliang Pu</a>
|
<a href=/people/y/yueting-zhuang/>Yueting Zhuang</a><br><a href=/volumes/N19-1/ class=text-muted>Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1290><div class="card-body p-3 small">This paper provides a new way to improve the efficiency of the REINFORCE training process. We apply <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> to the task of <a href=https://en.wikipedia.org/wiki/Instance_selection>instance selection</a> in distant supervision. Modeling the instance selection in one bag as a sequential decision process, a reinforcement learning agent is trained to determine whether an instance is valuable or not and construct a new bag with less noisy instances. However <a href=https://en.wikipedia.org/wiki/Bias_(statistics)>unbiased methods</a>, such as REINFORCE, could usually take much time to train. This paper adopts posterior regularization (PR) to integrate some domain-specific rules in instance selection using REINFORCE. As the experiment results show, this method remarkably improves the performance of the relation classifier trained on cleaned distant supervision dataset as well as the efficiency of the REINFORCE training.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-3017.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-3017 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-3017 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-3017" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-3017/>KCAT : A Knowledge-Constraint Typing Annotation Tool<span class=acl-fixed-case>KCAT</span>: A Knowledge-Constraint Typing Annotation Tool</a></strong><br><a href=/people/s/sheng-lin/>Sheng Lin</a>
|
<a href=/people/l/luye-zheng/>Luye Zheng</a>
|
<a href=/people/b/bo-chen/>Bo Chen</a>
|
<a href=/people/s/siliang-tang/>Siliang Tang</a>
|
<a href=/people/z/zhigang-chen/>Zhigang Chen</a>
|
<a href=/people/g/guoping-hu/>Guoping Hu</a>
|
<a href=/people/y/yueting-zhuang/>Yueting Zhuang</a>
|
<a href=/people/f/fei-wu/>Fei Wu</a>
|
<a href=/people/x/xiang-ren/>Xiang Ren</a><br><a href=/volumes/P19-3/ class=text-muted>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: System Demonstrations</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-3017><div class="card-body p-3 small">In this paper, we propose an efficient Knowledge Constraint Fine-grained Entity Typing Annotation Tool, which further improves the entity typing process through entity linking together with some practical functions.</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1153.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1153 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1153 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/305209444 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1153" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1153/>Efficient Contextualized Representation : Language Model Pruning for Sequence Labeling</a></strong><br><a href=/people/l/liyuan-liu/>Liyuan Liu</a>
|
<a href=/people/x/xiang-ren/>Xiang Ren</a>
|
<a href=/people/j/jingbo-shang/>Jingbo Shang</a>
|
<a href=/people/x/xiaotao-gu/>Xiaotao Gu</a>
|
<a href=/people/j/jian-peng/>Jian Peng</a>
|
<a href=/people/j/jiawei-han/>Jiawei Han</a><br><a href=/volumes/D18-1/ class=text-muted>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1153><div class="card-body p-3 small">Many efforts have been made to facilitate natural language processing tasks with pre-trained language models (LMs), and brought significant improvements to various applications. To fully leverage the nearly unlimited corpora and capture linguistic information of multifarious levels, large-size LMs are required ; but for a specific task, only parts of these information are useful. Such large-sized LMs, even in the inference stage, may cause heavy computation workloads, making them too time-consuming for large-scale applications. Here we propose to compress bulky LMs while preserving useful information with regard to a specific <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. As different layers of the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> keep different information, we develop a layer selection method for <a href=https://en.wikipedia.org/wiki/Mathematical_model>model pruning</a> using sparsity-inducing regularization. By introducing the dense connectivity, we can detach any layer without affecting others, and stretch shallow and wide LMs to be deep and narrow. In <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>model training</a>, LMs are learned with layer-wise dropouts for better robustness. Experiments on two benchmark datasets demonstrate the effectiveness of our <a href=https://en.wikipedia.org/wiki/Methodology>method</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1230.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1230 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1230 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1230" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D18-1230/>Learning Named Entity Tagger using Domain-Specific Dictionary</a></strong><br><a href=/people/j/jingbo-shang/>Jingbo Shang</a>
|
<a href=/people/l/liyuan-liu/>Liyuan Liu</a>
|
<a href=/people/x/xiaotao-gu/>Xiaotao Gu</a>
|
<a href=/people/x/xiang-ren/>Xiang Ren</a>
|
<a href=/people/t/teng-ren/>Teng Ren</a>
|
<a href=/people/j/jiawei-han/>Jiawei Han</a><br><a href=/volumes/D18-1/ class=text-muted>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1230><div class="card-body p-3 small">Recent advances in <a href=https://en.wikipedia.org/wiki/Deep_learning>deep neural models</a> allow us to build reliable named entity recognition (NER) systems without handcrafting features. However, such <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> require large amounts of manually-labeled training data. There have been efforts on replacing human annotations with distant supervision (in conjunction with external dictionaries), but the generated noisy labels pose significant challenges on learning effective neural models. Here we propose two neural models to suit noisy distant supervision from the dictionary. First, under the traditional sequence labeling framework, we propose a revised fuzzy CRF layer to handle tokens with multiple possible labels. After identifying the nature of noisy labels in distant supervision, we go beyond the traditional framework and propose a novel, more effective neural model AutoNER with a new Tie or Break scheme. In addition, we discuss how to refine distant supervision for better NER performance. Extensive experiments on three benchmark datasets demonstrate that AutoNER achieves the best performance when only using dictionaries with no additional human effort, and delivers competitive results with state-of-the-art supervised benchmarks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-6003.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-6003 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-6003 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/279154243 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N18-6003/>Scalable Construction and Reasoning of Massive Knowledge Bases</a></strong><br><a href=/people/x/xiang-ren/>Xiang Ren</a>
|
<a href=/people/n/nanyun-peng/>Nanyun Peng</a>
|
<a href=/people/w/william-yang-wang/>William Yang Wang</a><br><a href=/volumes/N18-6/ class=text-muted>Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Tutorial Abstracts</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-6003><div class="card-body p-3 small">In today&#8217;s information-based society, there is abundant knowledge out there carried in the form of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language texts</a> (e.g., <a href=https://en.wikipedia.org/wiki/Article_(publishing)>news articles</a>, <a href=https://en.wikipedia.org/wiki/Social_media>social media posts</a>, scientific publications), which spans across various domains (e.g., corporate documents, <a href=https://en.wikipedia.org/wiki/Advertising>advertisements</a>, <a href=https://en.wikipedia.org/wiki/Act_of_Parliament>legal acts</a>, medical reports), which grows at an astonishing rate. Yet this knowledge is mostly inaccessible to computers and overwhelming for human experts to absorb. How to turn such massive and unstructured text data into structured, actionable knowledge, and furthermore, how to teach machines learn to reason and complete the extracted knowledge is a grand challenge to the research community. Traditional IE systems assume abundant human annotations for training high quality machine learning models, which is impractical when trying to deploy IE systems to a broad range of domains, settings and languages. In the first part of the tutorial, we introduce how to extract structured facts (i.e., entities and their relations for types of interest) from <a href=https://en.wikipedia.org/wiki/Text_corpus>text corpora</a> to construct <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge bases</a>, with a focus on methods that are weakly-supervised and domain-independent for timely knowledge base construction across various application domains. In the second part, we introduce how to leverage other knowledge, such as the distributional statistics of characters and words, the annotations for other tasks and other domains, and the linguistics and problem structures, to combat the problem of inadequate supervision, and conduct low-resource information extraction. In the third part, we describe recent advances in knowledge base reasoning.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1229.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1229 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1229 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1229.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-1229" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P18-1229/>End-to-End Reinforcement Learning for Automatic Taxonomy Induction</a></strong><br><a href=/people/y/yuning-mao/>Yuning Mao</a>
|
<a href=/people/x/xiang-ren/>Xiang Ren</a>
|
<a href=/people/j/jiaming-shen/>Jiaming Shen</a>
|
<a href=/people/x/xiaotao-gu/>Xiaotao Gu</a>
|
<a href=/people/j/jiawei-han/>Jiawei Han</a><br><a href=/volumes/P18-1/ class=text-muted>Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1229><div class="card-body p-3 small">We present a novel end-to-end reinforcement learning approach to <a href=https://en.wikipedia.org/wiki/Automatic_taxonomy_induction>automatic taxonomy induction</a> from a set of terms. While prior methods treat the <a href=https://en.wikipedia.org/wiki/Problem_solving>problem</a> as a two-phase task (i.e.,, detecting hypernymy pairs followed by organizing these pairs into a tree-structured hierarchy), we argue that such two-phase methods may suffer from <a href=https://en.wikipedia.org/wiki/Error_propagation>error propagation</a>, and can not effectively optimize <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> that capture the holistic structure of a taxonomy. In our approach, the representations of term pairs are learned using multiple sources of information and used to determine which term to select and where to place it on the <a href=https://en.wikipedia.org/wiki/Taxonomy_(biology)>taxonomy</a> via a policy network. All components are trained in an end-to-end manner with cumulative rewards, measured by a holistic tree metric over the training <a href=https://en.wikipedia.org/wiki/Taxonomy_(biology)>taxonomies</a>. Experiments on two public datasets of different domains show that our approach outperforms prior state-of-the-art taxonomy induction methods up to 19.6 % on ancestor F1.<i>i.e.</i>,, detecting hypernymy pairs followed by organizing these pairs into a tree-structured hierarchy), we argue that such two-phase methods may suffer from error propagation, and cannot effectively optimize metrics that capture the holistic structure of a taxonomy. In our approach, the representations of term pairs are learned using multiple sources of information and used to determine <i>which</i> term to select and <i>where</i> to place it on the taxonomy via a policy network. All components are trained in an end-to-end manner with cumulative rewards, measured by a holistic tree metric over the training taxonomies. Experiments on two public datasets of different domains show that our approach outperforms prior state-of-the-art taxonomy induction methods up to 19.6% on ancestor F1.</div></div><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1005.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1005 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1005 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/238228823 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D17-1005" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/D17-1005/>Heterogeneous Supervision for Relation Extraction : A Representation Learning Approach</a></strong><br><a href=/people/l/liyuan-liu/>Liyuan Liu</a>
|
<a href=/people/x/xiang-ren/>Xiang Ren</a>
|
<a href=/people/q/qi-zhu/>Qi Zhu</a>
|
<a href=/people/s/shi-zhi/>Shi Zhi</a>
|
<a href=/people/h/huan-gui/>Huan Gui</a>
|
<a href=/people/h/heng-ji/>Heng Ji</a>
|
<a href=/people/j/jiawei-han/>Jiawei Han</a><br><a href=/volumes/D17-1/ class=text-muted>Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1005><div class="card-body p-3 small">Relation extraction is a fundamental task in <a href=https://en.wikipedia.org/wiki/Information_extraction>information extraction</a>. Most existing methods have heavy reliance on <a href=https://en.wikipedia.org/wiki/Annotation>annotations</a> labeled by human experts, which are costly and time-consuming. To overcome this drawback, we propose a novel framework, REHession, to conduct relation extractor learning using annotations from heterogeneous information source, e.g., knowledge base and domain heuristics. These annotations, referred as heterogeneous supervision, often conflict with each other, which brings a new challenge to the original relation extraction task : how to infer the true label from noisy labels for a given instance. Identifying context information as the backbone of both relation extraction and true label discovery, we adopt embedding techniques to learn the distributed representations of context, which bridges all components with mutual enhancement in an iterative fashion. Extensive experimental results demonstrate the superiority of REHession over the <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a>.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Xiang+Ren" title="Search for 'Xiang Ren' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/j/jiawei-han/ class=align-middle>Jiawei Han</a>
<span class="badge badge-secondary align-middle ml-2">6</span></li><li class=list-group-item><a href=/people/l/liyuan-liu/ class=align-middle>Liyuan Liu</a>
<span class="badge badge-secondary align-middle ml-2">5</span></li><li class=list-group-item><a href=/people/x/xiaotao-gu/ class=align-middle>Xiaotao Gu</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/people/w/woojeong-jin/ class=align-middle>Woojeong Jin</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/b/bill-yuchen-lin/ class=align-middle>Bill Yuchen Lin</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/j/jingbo-shang/ class=align-middle>Jingbo Shang</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/n/nanyun-peng/ class=align-middle>Nanyun Peng</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/s/siliang-tang/ class=align-middle>Siliang Tang</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/y/yueting-zhuang/ class=align-middle>Yueting Zhuang</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/f/fei-wu/ class=align-middle>Fei Wu</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/r/rahul-khanna/ class=align-middle>Rahul Khanna</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/d/dong-ho-lee/ class=align-middle>Dong-Ho Lee</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/j/jun-yan/ class=align-middle>Jun Yan</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/y/yuning-mao/ class=align-middle>Yuning Mao</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/m/meng-qu/ class=align-middle>Meng Qu</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/x/xiao-huang/ class=align-middle>Xiao Huang</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/j/jay-pujara/ class=align-middle>Jay Pujara</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/s/sheng-lin/ class=align-middle>Sheng Lin</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/z/zhigang-chen/ class=align-middle>Zhigang Chen</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/g/guoping-hu/ class=align-middle>Guoping Hu</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/q/qinyuan-ye/ class=align-middle>Qinyuan Ye</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/j/jiaming-shen/ class=align-middle>Jiaming Shen</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/s/suji-kim/ class=align-middle>Suji Kim</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/f/fred-morstatter/ class=align-middle>Fred Morstatter</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/aram-galstyan/ class=align-middle>Aram Galstyan</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/n/nasser-zalmout/ class=align-middle>Nasser Zalmout</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yan-liang/ class=align-middle>Yan Liang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/christan-grant/ class=align-middle>Christan Grant</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/x/xin-luna-dong/ class=align-middle>Xin Luna Dong</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yanru-qu/ class=align-middle>Yanru Qu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yiqing-xie/ class=align-middle>Yiqing Xie</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/x/xisen-jin/ class=align-middle>Xisen Jin</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/o/ouyu-lan/ class=align-middle>Ouyu Lan</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/he-jiang/ class=align-middle>He Jiang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/akshen-kadakia/ class=align-middle>Akshen Kadakia</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/kangmin-tan/ class=align-middle>Kangmin Tan</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/mahak-agarwal/ class=align-middle>Mahak Agarwal</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/x/xinyu-feng/ class=align-middle>Xinyu Feng</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/takashi-shibuya/ class=align-middle>Takashi Shibuya</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/ryosuke-mitani/ class=align-middle>Ryosuke Mitani</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/toshiyuki-sekiya/ class=align-middle>Toshiyuki Sekiya</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/donghan-yu/ class=align-middle>Donghan Yu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/chenguang-zhu/ class=align-middle>Chenguang Zhu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yuwei-fang/ class=align-middle>Yuwei Fang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/w/wenhao-yu/ class=align-middle>Wenhao Yu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/shuohang-wang/ class=align-middle>Shuohang Wang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yichong-xu/ class=align-middle>Yichong Xu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yiming-yang/ class=align-middle>Yiming Yang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/michael-zeng/ class=align-middle>Michael Zeng</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jian-peng/ class=align-middle>Jian Peng</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/teng-ren/ class=align-middle>Teng Ren</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/w/wenyang-gao/ class=align-middle>Wenyang Gao</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/ryan-moreno/ class=align-middle>Ryan Moreno</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/rujun-han/ class=align-middle>Rujun Han</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/p/pei-zhou/ class=align-middle>Pei Zhou</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/seyeon-lee/ class=align-middle>Seyeon Lee</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/daniel-ho/ class=align-middle>Daniel Ho</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/soumya-sanyal/ class=align-middle>Soumya Sanyal</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/x/xiyuan-yang/ class=align-middle>Xiyuan Yang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/cong-fu/ class=align-middle>Cong Fu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/tong-chen/ class=align-middle>Tong Chen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/maosen-zhang/ class=align-middle>Maosen Zhang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/x/xiaozhi-wang/ class=align-middle>Xiaozhi Wang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/ziqi-wang/ class=align-middle>Ziqi Wang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/x/xu-han/ class=align-middle>Xu Han</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/zhiyuan-liu/ class=align-middle>Zhiyuan Liu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/juanzi-li/ class=align-middle>Juanzi Li</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/p/peng-li/ class=align-middle>Peng Li</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/maosong-sun/ class=align-middle>Maosong Sun (孙茂松)</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jie-zhou/ class=align-middle>Jie Zhou</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/colin-cherry/ class=align-middle>Colin Cherry</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/g/greg-durrett/ class=align-middle>Greg Durrett</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/g/george-foster/ class=align-middle>George Foster</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/g/gholamreza-haffari/ class=align-middle>Gholamreza Haffari</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/shahram-khadivi/ class=align-middle>Shahram Khadivi</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/swabha-swayamdipta/ class=align-middle>Swabha Swayamdipta</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/aida-mostafazadeh-davani/ class=align-middle>Aida Mostafazadeh Davani</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/ali-omrani/ class=align-middle>Ali Omrani</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/b/brendan-kennedy/ class=align-middle>Brendan Kennedy</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/mohammad-atari/ class=align-middle>Mohammad Atari</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/morteza-dehghani/ class=align-middle>Morteza Dehghani</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/q/qi-zhu/ class=align-middle>Qi Zhu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/shi-zhi/ class=align-middle>Shi Zhi</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/huan-gui/ class=align-middle>Huan Gui</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/heng-ji/ class=align-middle>Heng Ji</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/meryem-mhamdi/ class=align-middle>Meryem M’hamdi</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/doo-soon-kim/ class=align-middle>Doo Soon Kim</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/f/franck-dernoncourt/ class=align-middle>Franck Dernoncourt</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/trung-bui/ class=align-middle>Trung Bui</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jonathan-may/ class=align-middle>Jonathan May</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/w/wenda-qiu/ class=align-middle>Wenda Qiu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yu-meng/ class=align-middle>Yu Meng</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/e/elizabeth-boschee/ class=align-middle>Elizabeth Boschee</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/i/isabelle-augenstein/ class=align-middle>Isabelle Augenstein</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/spandana-gella/ class=align-middle>Spandana Gella</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/sebastian-ruder/ class=align-middle>Sebastian Ruder</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/katharina-kann/ class=align-middle>Katharina Kann</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/b/burcu-can/ class=align-middle>Burcu Can</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/johannes-welbl/ class=align-middle>Johannes Welbl</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/alexis-conneau/ class=align-middle>Alexis Conneau</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/marek-rei/ class=align-middle>Marek Rei</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/q/qi-zhang/ class=align-middle>Qi Zhang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/shiliang-pu/ class=align-middle>Shiliang Pu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/w/william-yang-wang/ class=align-middle>William Yang Wang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yada-pruksachatkun/ class=align-middle>Yada Pruksachatkun</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/anil-ramakrishna/ class=align-middle>Anil Ramakrishna</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/kai-wei-chang/ class=align-middle>Kai-Wei Chang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/satyapriya-krishna/ class=align-middle>Satyapriya Krishna</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jwala-dhamala/ class=align-middle>Jwala Dhamala</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/tanaya-guha/ class=align-middle>Tanaya Guha</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/luye-zheng/ class=align-middle>Luye Zheng</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/b/bo-chen/ class=align-middle>Bo Chen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/emnlp/ class=align-middle>EMNLP</a><span class="badge badge-secondary align-middle ml-2">14</span></li><li class=list-group-item><a href=/venues/acl/ class=align-middle>ACL</a><span class="badge badge-secondary align-middle ml-2">7</span></li><li class=list-group-item><a href=/venues/naacl/ class=align-middle>NAACL</a><span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/venues/woah/ class=align-middle>WOAH</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/findings/ class=align-middle>Findings</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-venues aria-expanded=false aria-controls=more-venues>show all...</li><div class="collapse border-top" id=more-venues><li class=list-group-item><a href=/venues/ws/ class=align-middle>WS</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/trustnlp/ class=align-middle>TrustNLP</a><span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>