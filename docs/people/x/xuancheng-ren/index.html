<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Xuancheng Ren - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Xuancheng</span> <span class=font-weight-bold>Ren</span></h2><hr><div class=row><div class=col-lg-9><h4>2022</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-acl.57.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-acl--57 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-acl.57 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.findings-acl.57/>Hierarchical Inductive Transfer for Continual Dialogue Learning</a></strong><br><a href=/people/s/shaoxiong-feng/>Shaoxiong Feng</a>
|
<a href=/people/x/xuancheng-ren/>Xuancheng Ren</a>
|
<a href=/people/k/kan-li/>Kan Li</a>
|
<a href=/people/x/xu-sun/>Xu Sun</a><br><a href=/volumes/2022.findings-acl/ class=text-muted>Findings of the Association for Computational Linguistics: ACL 2022</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-acl--57><div class="card-body p-3 small">Pre trained models have achieved excellent performance on the <a href=https://en.wikipedia.org/wiki/Dialogue>dialogue task</a> However for the continual increase of online chit chat scenarios directly fine tuning these <a href=https://en.wikipedia.org/wiki/Computer_simulation>models</a> for each of the new tasks not only explodes the capacity of the dialogue system on the embedded devices but also causes knowledge forgetting on pre trained models and knowledge interference among diverse dialogue tasks In this work we propose a hierarchical inductive transfer framework to learn and deploy the <a href=https://en.wikipedia.org/wiki/Dialogue>dialogue skills</a> continually and efficiently First we introduce the adapter module into <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>pre trained models</a> for learning new dialogue tasks As the only trainable module it is beneficial for the <a href=https://en.wikipedia.org/wiki/Dialogue_system>dialogue system</a> on the embedded devices to acquire new <a href=https://en.wikipedia.org/wiki/Dialogue>dialogue skills</a> with negligible additional parameters Then for alleviating knowledge interference between tasks yet benefiting the regularization between them we further design hierarchical inductive transfer that enables new tasks to use general knowledge in the base adapter without being misled by diverse knowledge in task specific adapters Empirical evaluation and analysis indicate that our <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> obtains comparable performance under deployment friendly model capacity</div></div><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5105.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5105 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5105 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-5105.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D19-5105/>Incorporating Fine-grained Events in Stock Movement Prediction</a></strong><br><a href=/people/d/deli-chen/>Deli Chen</a>
|
<a href=/people/y/yanyan-zou/>Yanyan Zou</a>
|
<a href=/people/k/keiko-harimoto/>Keiko Harimoto</a>
|
<a href=/people/r/ruihan-bao/>Ruihan Bao</a>
|
<a href=/people/x/xuancheng-ren/>Xuancheng Ren</a>
|
<a href=/people/x/xu-sun/>Xu Sun</a><br><a href=/volumes/D19-51/ class=text-muted>Proceedings of the Second Workshop on Economics and Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5105><div class="card-body p-3 small">Considering event structure information has proven helpful in text-based stock movement prediction. However, existing works mainly adopt the coarse-grained events, which loses the specific semantic information of diverse event types. In this work, we propose to incorporate the fine-grained events in stock movement prediction. Firstly, we propose a professional finance event dictionary built by domain experts and use it to extract fine-grained events automatically from finance news. Then we design a neural model to combine finance news with fine-grained event structure and stock trade data to predict the stock movement. Besides, in order to improve the generalizability of the proposed method, we design an advanced model that uses the extracted fine-grained events as the distant supervised label to train a multi-task framework of <a href=https://en.wikipedia.org/wiki/Event_(probability_theory)>event extraction</a> and stock prediction. The experimental results show that our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> outperforms all the <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a> and has good generalizability.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1482.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1482 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1482 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/385265051 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1482/>A Hierarchical Reinforced Sequence Operation Method for Unsupervised Text Style Transfer</a></strong><br><a href=/people/c/chen-wu/>Chen Wu</a>
|
<a href=/people/x/xuancheng-ren/>Xuancheng Ren</a>
|
<a href=/people/f/fuli-luo/>Fuli Luo</a>
|
<a href=/people/x/xu-sun/>Xu Sun</a><br><a href=/volumes/P19-1/ class=text-muted>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1482><div class="card-body p-3 small">Unsupervised text style transfer aims to alter <a href=https://en.wikipedia.org/wiki/Style_guide>text styles</a> while preserving the content, without aligned data for <a href=https://en.wikipedia.org/wiki/Supervisor>supervision</a>. Existing seq2seq methods face three challenges : 1) the transfer is weakly interpretable, 2) generated outputs struggle in content preservation, and 3) the trade-off between content and style is intractable. To address these challenges, we propose a hierarchical reinforced sequence operation method, named Point-Then-Operate (PTO), which consists of a high-level agent that proposes operation positions and a low-level agent that alters the sentence. We provide comprehensive training objectives to control the fluency, style, and content of the outputs and a mask-based inference algorithm that allows for multi-step revision based on the single-step trained agents. Experimental results on two text style transfer datasets show that our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> significantly outperforms recent methods and effectively addresses the aforementioned challenges.</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1276.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1276 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1276 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=C18-1276" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/C18-1276/>Deconvolution-Based Global Decoding for <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a></a></strong><br><a href=/people/j/junyang-lin/>Junyang Lin</a>
|
<a href=/people/x/xu-sun/>Xu Sun</a>
|
<a href=/people/x/xuancheng-ren/>Xuancheng Ren</a>
|
<a href=/people/s/shuming-ma/>Shuming Ma</a>
|
<a href=/people/j/jinsong-su/>Jinsong Su</a>
|
<a href=/people/q/qi-su/>Qi Su</a><br><a href=/volumes/C18-1/ class=text-muted>Proceedings of the 27th International Conference on Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1276><div class="card-body p-3 small">A great proportion of sequence-to-sequence (Seq2Seq) models for Neural Machine Translation (NMT) adopt Recurrent Neural Network (RNN) to generate translation word by word following a sequential order. As the studies of <a href=https://en.wikipedia.org/wiki/Linguistics>linguistics</a> have proved that language is not linear word sequence but sequence of complex structure, <a href=https://en.wikipedia.org/wiki/Translation>translation</a> at each step should be conditioned on the whole target-side context. To tackle the problem, we propose a new NMT model that decodes the sequence with the guidance of its structural prediction of the context of the target sequence. Our model generates <a href=https://en.wikipedia.org/wiki/Translation_(biology)>translation</a> based on the structural prediction of the target-side context so that the <a href=https://en.wikipedia.org/wiki/Translation_(biology)>translation</a> can be freed from the bind of sequential order. Experimental results demonstrate that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is more competitive compared with the state-of-the-art methods, and the analysis reflects that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is also robust to translating sentences of different lengths and it also reduces <a href=https://en.wikipedia.org/wiki/Repetition_(rhetorical_device)>repetition</a> with the instruction from the target-side context for decoding.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2059.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2059 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2059 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N18-2059.Datasets.zip data-toggle=tooltip data-placement=top title=Dataset><i class="fas fa-file-archive"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N18-2059/>Structure Regularized Neural Network for Entity Relation Classification for Chinese Literature Text<span class=acl-fixed-case>C</span>hinese Literature Text</a></strong><br><a href=/people/j/ji-wen/>Ji Wen</a>
|
<a href=/people/x/xu-sun/>Xu Sun</a>
|
<a href=/people/x/xuancheng-ren/>Xuancheng Ren</a>
|
<a href=/people/q/qi-su/>Qi Su</a><br><a href=/volumes/N18-2/ class=text-muted>Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2059><div class="card-body p-3 small">Relation classification is an important semantic processing task in the field of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a>. In this paper, we propose the task of <a href=https://en.wikipedia.org/wiki/Relation_(database)>relation classification</a> for Chinese literature text. A new <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> of <a href=https://en.wikipedia.org/wiki/Chinese_literature>Chinese literature text</a> is constructed to facilitate the study in this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. We present a novel model, named Structure Regularized Bidirectional Recurrent Convolutional Neural Network (SR-BRCNN), to identify the relation between entities. The proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> learns relation representations along the shortest dependency path (SDP) extracted from the structure regularized dependency tree, which has the benefits of reducing the complexity of the whole <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>. Experimental results show that the proposed <a href=https://en.wikipedia.org/wiki/Methodology>method</a> significantly improves the <a href=https://en.wikipedia.org/wiki/F-number>F1 score</a> by 10.3, and outperforms the state-of-the-art approaches on <a href=https://en.wikipedia.org/wiki/Chinese_literature>Chinese literature text</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1090.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1090 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1090 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1090.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285801380 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-1090" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-1090/>Unpaired Sentiment-to-Sentiment Translation : A Cycled Reinforcement Learning Approach</a></strong><br><a href=/people/j/jingjing-xu/>Jingjing Xu</a>
|
<a href=/people/x/xu-sun/>Xu Sun</a>
|
<a href=/people/q/qi-zeng/>Qi Zeng</a>
|
<a href=/people/x/xiaodong-zhang/>Xiaodong Zhang</a>
|
<a href=/people/x/xuancheng-ren/>Xuancheng Ren</a>
|
<a href=/people/h/houfeng-wang/>Houfeng Wang</a>
|
<a href=/people/w/wenjie-li/>Wenjie Li</a><br><a href=/volumes/P18-1/ class=text-muted>Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1090><div class="card-body p-3 small">The goal of sentiment-to-sentiment translation is to change the underlying sentiment of a sentence while keeping its content. The main challenge is the lack of <a href=https://en.wikipedia.org/wiki/Parallel_computing>parallel data</a>. To solve this problem, we propose a cycled reinforcement learning method that enables training on unpaired data by collaboration between a neutralization module and an emotionalization module. We evaluate our approach on two review datasets, <a href=https://en.wikipedia.org/wiki/Yelp>Yelp</a> and <a href=https://en.wikipedia.org/wiki/Amazon_(company)>Amazon</a>. Experimental results show that our approach significantly outperforms the <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art systems</a>. Especially, the proposed <a href=https://en.wikipedia.org/wiki/Methodology>method</a> substantially improves the <a href=https://en.wikipedia.org/wiki/Digital_preservation>content preservation</a> performance. The BLEU score is improved from 1.64 to 22.46 and from 0.56 to 14.06 on the two datasets, respectively.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Xuancheng+Ren" title="Search for 'Xuancheng Ren' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/x/xu-sun/ class=align-middle>Xu Sun</a>
<span class="badge badge-secondary align-middle ml-2">6</span></li><li class=list-group-item><a href=/people/q/qi-su/ class=align-middle>Qi Su</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/j/junyang-lin/ class=align-middle>Junyang Lin</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/shuming-ma/ class=align-middle>Shuming Ma</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jinsong-su/ class=align-middle>Jinsong Su</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/d/deli-chen/ class=align-middle>Deli Chen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yanyan-zou/ class=align-middle>Yanyan Zou</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/keiko-harimoto/ class=align-middle>Keiko Harimoto</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/ruihan-bao/ class=align-middle>Ruihan Bao</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/shaoxiong-feng/ class=align-middle>Shaoxiong Feng</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/kan-li/ class=align-middle>Kan Li</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/ji-wen/ class=align-middle>Ji Wen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jingjing-xu/ class=align-middle>Jingjing Xu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/q/qi-zeng/ class=align-middle>Qi Zeng</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/x/xiaodong-zhang/ class=align-middle>Xiaodong Zhang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/houfeng-wang/ class=align-middle>Houfeng Wang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/w/wenjie-li/ class=align-middle>Wenjie Li</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/chen-wu/ class=align-middle>Chen Wu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/f/fuli-luo/ class=align-middle>Fuli Luo</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/acl/ class=align-middle>ACL</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/coling/ class=align-middle>COLING</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/emnlp/ class=align-middle>EMNLP</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/findings/ class=align-middle>Findings</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/naacl/ class=align-middle>NAACL</a><span class="badge badge-secondary align-middle ml-2">1</span></li></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright Â©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>