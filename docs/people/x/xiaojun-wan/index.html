<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Xiaojun Wan - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Xiaojun</span> <span class=font-weight-bold>Wan</span></h2><hr><div class=row><div class=col-lg-9><h4>2022</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.acl-long.531.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--acl-long--531 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.acl-long.531 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.acl-long.531" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2022.acl-long.531/>How Do <span class=acl-fixed-case>S</span>eq2<span class=acl-fixed-case>S</span>eq Models Perform on End-to-End Data-to-Text Generation?</a></strong><br><a href=/people/x/xunjian-yin/>Xunjian Yin</a>
|
<a href=/people/x/xiaojun-wan/>Xiaojun Wan</a><br><a href=/volumes/2022.acl-long/ class=text-muted>Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--acl-long--531><div class="card-body p-3 small">With the rapid development of deep learning, Seq2Seq paradigm has become prevalent for end-to-end data-to-text generation, and the BLEU scores have been increasing in recent years. However, it is widely recognized that there is still a gap between the quality of the texts generated by models and the texts written by human. In order to better understand the ability of Seq2Seq models, evaluate their performance and analyze the results, we choose to use Multidimensional Quality Metric(MQM) to evaluate several representative Seq2Seq models on end-to-end data-to-text generation. We annotate the outputs of five models on four datasets with eight error types and find that 1) copy mechanism is helpful for the improvement in Omission and Inaccuracy Extrinsic errors but it increases other types of errors such as Addition; 2) pre-training techniques are highly effective, and pre-training strategy and model size are very significant; 3) the structure of the dataset also influences the model&#8217;s performance greatly; 4) some specific types of errors are generally challenging for seq2seq models.</div></div><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.350.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--350 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.350 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.350/>Revisiting Pivot-Based Paraphrase Generation : Language Is Not the Only Optional Pivot</a></strong><br><a href=/people/y/yitao-cai/>Yitao Cai</a>
|
<a href=/people/y/yue-cao/>Yue Cao</a>
|
<a href=/people/x/xiaojun-wan/>Xiaojun Wan</a><br><a href=/volumes/2021.emnlp-main/ class=text-muted>Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--350><div class="card-body p-3 small">Paraphrases refer to texts that convey the same meaning with different expression forms. Pivot-based methods, also known as the <a href=https://en.wikipedia.org/wiki/Round-trip_translation>round-trip translation</a>, have shown promising results in generating high-quality paraphrases. However, existing pivot-based methods all rely on language as the pivot, where large-scale, high-quality parallel bilingual texts are required. In this paper, we explore the feasibility of using semantic and syntactic representations as the pivot for <a href=https://en.wikipedia.org/wiki/Paraphrase_generation>paraphrase generation</a>. Concretely, we transform a sentence into a variety of different semantic or syntactic representations (including AMR, UD, and latent semantic representation), and then decode the sentence back from the semantic representations. We further explore a pretraining-based approach to compress the pipeline process into an end-to-end framework. We conduct experiments comparing different approaches with different kinds of pivots. Experimental results show that taking AMR as pivot can obtain <a href=https://en.wikipedia.org/wiki/Paraphrase>paraphrases</a> with better quality than taking <a href=https://en.wikipedia.org/wiki/Language>language</a> as the pivot. The end-to-end framework can reduce <a href=https://en.wikipedia.org/wiki/Semantic_shift>semantic shift</a> when language is used as the pivot. Besides, several unsupervised pivot-based methods can generate <a href=https://en.wikipedia.org/wiki/Paraphrase>paraphrases</a> with similar quality as the supervised sequence-to-sequence model, which indicates that parallel data of paraphrases may not be necessary for <a href=https://en.wikipedia.org/wiki/Paraphrase_generation>paraphrase generation</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.630.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--630 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.630 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.630" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.630/>Document-Level Text Simplification : Dataset, Criteria and Baseline</a></strong><br><a href=/people/r/renliang-sun/>Renliang Sun</a>
|
<a href=/people/h/hanqi-jin/>Hanqi Jin</a>
|
<a href=/people/x/xiaojun-wan/>Xiaojun Wan</a><br><a href=/volumes/2021.emnlp-main/ class=text-muted>Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--630><div class="card-body p-3 small">Text simplification is a valuable technique. However, current research is limited to <a href=https://en.wikipedia.org/wiki/Sentence_simplification>sentence simplification</a>. In this paper, we define and investigate a new task of document-level text simplification, which aims to simplify a document consisting of multiple sentences. Based on <a href=https://en.wikipedia.org/wiki/Wikipedia>Wikipedia dumps</a>, we first construct a large-scale dataset named D-Wikipedia and perform analysis and human evaluation on it to show that the <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> is reliable. Then, we propose a new automatic evaluation metric called D-SARI that is more suitable for the document-level simplification task. Finally, we select several representative models as baseline models for this task and perform automatic evaluation and human evaluation. We analyze the results and point out the shortcomings of the <a href=https://en.wikipedia.org/wiki/Baseline_(medicine)>baseline models</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.310.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--310 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.310 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.310/>Continual Learning for <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a></a></strong><br><a href=/people/y/yue-cao/>Yue Cao</a>
|
<a href=/people/h/hao-ran-wei/>Hao-Ran Wei</a>
|
<a href=/people/b/boxing-chen/>Boxing Chen</a>
|
<a href=/people/x/xiaojun-wan/>Xiaojun Wan</a><br><a href=/volumes/2021.naacl-main/ class=text-muted>Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--310><div class="card-body p-3 small">Neural machine translation (NMT) models are data-driven and require large-scale training corpus. In practical applications, NMT models are usually trained on a general domain corpus and then fine-tuned by continuing training on the in-domain corpus. However, this bears the risk of catastrophic forgetting that the performance on the <a href=https://en.wikipedia.org/wiki/Domain_(software_engineering)>general domain</a> is decreased drastically. In this work, we propose a new continual learning framework for NMT models. We consider a scenario where the training is comprised of multiple stages and propose a dynamic knowledge distillation technique to alleviate the problem of catastrophic forgetting systematically. We also find that the bias exists in the output linear projection when fine-tuning on the in-domain corpus, and propose a bias-correction module to eliminate the bias. We conduct experiments on three representative settings of NMT application. Experimental results show that the proposed method achieves superior performance compared to baseline models in all settings.</div></div><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.311.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--311 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.311 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939193 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.311/>Routing Enforced <a href=https://en.wikipedia.org/wiki/Generative_model>Generative Model</a> for Recipe Generation</a></strong><br><a href=/people/z/zhiwei-yu/>Zhiwei Yu</a>
|
<a href=/people/h/hongyu-zang/>Hongyu Zang</a>
|
<a href=/people/x/xiaojun-wan/>Xiaojun Wan</a><br><a href=/volumes/2020.emnlp-main/ class=text-muted>Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--311><div class="card-body p-3 small">One of the most challenging part of recipe generation is to deal with the complex restrictions among the input ingredients. Previous researches simplify the problem by treating the inputs independently and generating recipes containing as much information as possible. In this work, we propose a routing method to dive into the content selection under the internal restrictions. The routing enforced generative model (RGM) can generate appropriate <a href=https://en.wikipedia.org/wiki/Recipe>recipes</a> according to the given ingredients and user preferences. Our model yields new state-of-the-art results on the recipe generation task with significant improvements on BLEU, <a href=https://en.wikipedia.org/wiki/F-number>F1</a> and human evaluation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.acl-main.400.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--acl-main--400 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.acl-main.400 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://slideslive.com/38929440 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.acl-main.400/>Multimodal Transformer for Multimodal Machine Translation</a></strong><br><a href=/people/s/shaowei-yao/>Shaowei Yao</a>
|
<a href=/people/x/xiaojun-wan/>Xiaojun Wan</a><br><a href=/volumes/2020.acl-main/ class=text-muted>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--acl-main--400><div class="card-body p-3 small">Multimodal Machine Translation (MMT) aims to introduce information from other modality, generally static images, to improve the translation quality. Previous works propose various incorporation methods, but most of them do not consider the relative importance of multiple modalities. Equally treating all modalities may encode too much useless information from less important modalities. In this paper, we introduce the multimodal self-attention in Transformer to solve the issues above in MMT. The proposed method learns the representation of images based on the text, which avoids encoding irrelevant information in <a href=https://en.wikipedia.org/wiki/Image>images</a>. Experiments and visualization analysis demonstrate that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> benefits from <a href=https://en.wikipedia.org/wiki/Visual_system>visual information</a> and substantially outperforms previous works and competitive baselines in terms of various <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.acl-main.556.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--acl-main--556 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.acl-main.556 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://slideslive.com/38929014 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.acl-main.556/>Multi-Granularity Interaction Network for Extractive and Abstractive Multi-Document Summarization</a></strong><br><a href=/people/h/hanqi-jin/>Hanqi Jin</a>
|
<a href=/people/t/tianming-wang/>Tianming Wang</a>
|
<a href=/people/x/xiaojun-wan/>Xiaojun Wan</a><br><a href=/volumes/2020.acl-main/ class=text-muted>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--acl-main--556><div class="card-body p-3 small">In this paper, we propose a multi-granularity interaction network for extractive and abstractive multi-document summarization, which jointly learn semantic representations for words, sentences, and documents. The word representations are used to generate an abstractive summary while the sentence representations are used to produce an extractive summary. We employ attention mechanisms to interact between different granularity of semantic representations, which helps to capture multi-granularity key information and improves the performance of both abstractive and extractive summarization. Experiment results show that our proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> substantially outperforms all strong baseline methods and achieves the best results on the Multi-News dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.acl-main.606.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--acl-main--606 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.acl-main.606 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://slideslive.com/38929300 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.acl-main.606/>Semantic Parsing for <a href=https://en.wikipedia.org/wiki/English_language>English</a> as a Second Language<span class=acl-fixed-case>E</span>nglish as a Second Language</a></strong><br><a href=/people/y/yuanyuan-zhao/>Yuanyuan Zhao</a>
|
<a href=/people/w/weiwei-sun/>Weiwei Sun</a>
|
<a href=/people/j/junjie-cao/>Junjie Cao</a>
|
<a href=/people/x/xiaojun-wan/>Xiaojun Wan</a><br><a href=/volumes/2020.acl-main/ class=text-muted>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--acl-main--606><div class="card-body p-3 small">This paper is concerned with <a href=https://en.wikipedia.org/wiki/Semantic_parsing>semantic parsing</a> for <a href=https://en.wikipedia.org/wiki/English_language>English</a> as a second language (ESL). Motivated by the theoretical emphasis on the learning challenges that occur at the syntax-semantics interface during second language acquisition, we formulate the task based on the divergence between literal and intended meanings. We combine the complementary strengths of English Resource Grammar, a linguistically-precise hand-crafted deep grammar, and TLE, an existing manually annotated ESL UD-TreeBank with a novel reranking model. Experiments demonstrate that in comparison to human annotations, our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> can obtain a very promising SemBanking quality. By means of the newly created <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a>, we evaluate state-of-the-art semantic parsing as well as grammatical error correction models. The evaluation profiles the performance of neural NLP techniques for handling <a href=https://en.wikipedia.org/wiki/English_as_a_second_or_foreign_language>ESL data</a> and suggests some research directions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.218.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--218 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.218 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.218/>DivGAN : Towards Diverse Paraphrase Generation via Diversified Generative Adversarial Network<span class=acl-fixed-case>D</span>iv<span class=acl-fixed-case>GAN</span>: Towards Diverse Paraphrase Generation via Diversified Generative Adversarial Network</a></strong><br><a href=/people/y/yue-cao/>Yue Cao</a>
|
<a href=/people/x/xiaojun-wan/>Xiaojun Wan</a><br><a href=/volumes/2020.findings-emnlp/ class=text-muted>Findings of the Association for Computational Linguistics: EMNLP 2020</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--218><div class="card-body p-3 small">Paraphrases refer to texts that convey the same meaning with different expression forms. Traditional seq2seq-based models on <a href=https://en.wikipedia.org/wiki/Paraphrase_generation>paraphrase generation</a> mainly focus on the <a href=https://en.wikipedia.org/wiki/Fidelity>fidelity</a> while ignoring the diversity of outputs. In this paper, we propose a deep generative model to generate diverse paraphrases. We build our model based on the conditional generative adversarial network, and propose to incorporate a simple yet effective diversity loss term into the model in order to improve the diversity of outputs. The proposed diversity loss maximizes the ratio of pairwise distance between the generated texts and their corresponding <a href=https://en.wikipedia.org/wiki/Latent_variable>latent codes</a>, forcing the generator to focus more on the <a href=https://en.wikipedia.org/wiki/Latent_variable>latent codes</a> and produce diverse samples. Experimental results on benchmarks of <a href=https://en.wikipedia.org/wiki/Paraphrase_generation>paraphrase generation</a> show that our proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> can generate more diverse paraphrases compared with baselines.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.231.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--231 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.231 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.231" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.231/>Abstractive Multi-Document Summarization via Joint Learning with Single-Document Summarization</a></strong><br><a href=/people/h/hanqi-jin/>Hanqi Jin</a>
|
<a href=/people/x/xiaojun-wan/>Xiaojun Wan</a><br><a href=/volumes/2020.findings-emnlp/ class=text-muted>Findings of the Association for Computational Linguistics: EMNLP 2020</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--231><div class="card-body p-3 small">Single-document and multi-document summarizations are very closely related in both task definition and solution method. In this work, we propose to improve neural abstractive multi-document summarization by jointly learning an abstractive single-document summarizer. We build a unified model for single-document and multi-document summarizations by fully sharing the <a href=https://en.wikipedia.org/wiki/Encoder>encoder</a> and decoder and utilizing a decoding controller to aggregate the <a href=https://en.wikipedia.org/wiki/Code>decoder</a>&#8217;s outputs for multiple input documents. We evaluate our model on two multi-document summarization datasets : Multi-News and DUC-04. Experimental results show the efficacy of our approach, and <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> can substantially outperform several strong <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a>. We also verify the helpfulness of single-document summarization to abstractive multi-document summarization task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.tacl-1.2.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--tacl-1--2 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.tacl-1.2 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.tacl-1.2/>AMR-To-Text Generation with Graph Transformer<span class=acl-fixed-case>AMR</span>-To-Text Generation with Graph Transformer</a></strong><br><a href=/people/t/tianming-wang/>Tianming Wang</a>
|
<a href=/people/x/xiaojun-wan/>Xiaojun Wan</a>
|
<a href=/people/h/hanqi-jin/>Hanqi Jin</a><br><a href=/volumes/2020.tacl-1/ class=text-muted>Transactions of the Association for Computational Linguistics, Volume 8</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--tacl-1--2><div class="card-body p-3 small">Abstract meaning representation (AMR)-to-text generation is the challenging task of generating natural language texts from AMR graphs, where <a href=https://en.wikipedia.org/wiki/Vertex_(graph_theory)>nodes</a> represent concepts and <a href=https://en.wikipedia.org/wiki/Glossary_of_graph_theory_terms>edges</a> denote relations. The current state-of-the-art methods use graph-to-sequence models ; however, they still can not significantly outperform the previous sequence-to-sequence models or statistical approaches. In this paper, we propose a novel graph-to-sequence model (Graph Transformer) to address this task. The <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> directly encodes the AMR graphs and learns the <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>node representations</a>. A pairwise interaction function is used for computing the semantic relations between the concepts. Moreover, <a href=https://en.wikipedia.org/wiki/Attentional_control>attention mechanisms</a> are used for aggregating the information from the incoming and outgoing neighbors, which help the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> to capture the semantic information effectively. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms the state-of-the-art <a href=https://en.wikipedia.org/wiki/Artificial_neural_network>neural approach</a> by 1.5 BLEU points on LDC2015E86 and 4.8 BLEU points on LDC2017T10 and achieves new state-of-the-art performances.</div></div><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1000.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1000/>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</a></strong><br><a href=/people/k/kentaro-inui/>Kentaro Inui</a>
|
<a href=/people/j/jing-jiang/>Jing Jiang</a>
|
<a href=/people/v/vincent-ng/>Vincent Ng</a>
|
<a href=/people/x/xiaojun-wan/>Xiaojun Wan</a><br><a href=/volumes/D19-1/ class=text-muted>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/J19-1003.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-J19-1003 data-toggle=collapse aria-expanded=false aria-controls=abstract-J19-1003 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/J19-1003/>Parsing Chinese Sentences with Grammatical Relations<span class=acl-fixed-case>C</span>hinese Sentences with Grammatical Relations</a></strong><br><a href=/people/w/weiwei-sun/>Weiwei Sun</a>
|
<a href=/people/y/yufei-chen/>Yufei Chen</a>
|
<a href=/people/x/xiaojun-wan/>Xiaojun Wan</a>
|
<a href=/people/m/meichun-liu/>Meichun Liu</a><br><a href=/volumes/J19-1/ class=text-muted>Computational Linguistics, Volume 45, Issue 1 - March 2019</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-J19-1003><div class="card-body p-3 small">We report our work on building linguistic resources and data-driven parsers in the grammatical relation (GR) analysis for <a href=https://en.wikipedia.org/wiki/Mandarin_Chinese>Mandarin Chinese</a>. Chinese, as an analytic language, encodes grammatical information in a highly configurational rather than morphological way. Accordingly, it is possible and reasonable to represent almost all <a href=https://en.wikipedia.org/wiki/Grammatical_relation>grammatical relations</a> as bilexical dependencies. In this work, we propose to represent <a href=https://en.wikipedia.org/wiki/Grammatical_relation>grammatical information</a> using general directed dependency graphs. Both only-local and rich long-distance dependencies are explicitly represented. To create high-quality annotations, we take advantage of an existing <a href=https://en.wikipedia.org/wiki/Treebank>TreeBank</a>, namely, Chinese TreeBank (CTB), which is grounded on the <a href=https://en.wikipedia.org/wiki/Government_and_binding_theory>Government and Binding theory</a>. We define a set of linguistic rules to explore CTB&#8217;s implicit phrase structural information and build deep dependency graphs. The reliability of this linguistically motivated GR extraction procedure is highlighted by manual evaluation. Based on the converted corpus, data-driven, including graph- and transition-based, models are explored for Chinese GR parsing. For graph-based parsing, a new perspective, graph merging, is proposed for building flexible dependency graphs : constructing complex graphs via constructing simple subgraphs. Two key problems are discussed in this perspective : (1) how to decompose a <a href=https://en.wikipedia.org/wiki/Complex_graph>complex graph</a> into simple subgraphs, and (2) how to combine <a href=https://en.wikipedia.org/wiki/Glossary_of_graph_theory_terms>subgraphs</a> into a coherent complex graph. For transition-based parsing, we introduce a neural parser based on a list-based transition system. We also discuss several other key problems, including dynamic oracle and <a href=https://en.wikipedia.org/wiki/Beam_search>beam search</a> for neural transition-based parsing.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-4004.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-4004 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-4004 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-4004/>INS : An Interactive Chinese News Synthesis System<span class=acl-fixed-case>INS</span>: An Interactive <span class=acl-fixed-case>C</span>hinese News Synthesis System</a></strong><br><a href=/people/h/hui-liu/>Hui Liu</a>
|
<a href=/people/w/wentao-qin/>Wentao Qin</a>
|
<a href=/people/x/xiaojun-wan/>Xiaojun Wan</a><br><a href=/volumes/N19-4/ class=text-muted>Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-4004><div class="card-body p-3 small">Nowadays, we are surrounded by more and more <a href=https://en.wikipedia.org/wiki/Online_newspaper>online news articles</a>. Tens or hundreds of <a href=https://en.wikipedia.org/wiki/Article_(publishing)>news articles</a> need to be read if we wish to explore a hot news event or topic. So it is of vital importance to automatically synthesize a batch of <a href=https://en.wikipedia.org/wiki/Article_(publishing)>news articles</a> related to the event or topic into a new synthesis article (or overview article) for reader&#8217;s convenience. It is so challenging to make news synthesis fully automatic that there is no successful solution by now. In this paper, we put forward a novel Interactive News Synthesis system (i.e. INS), which can help generate news overview articles automatically or by interacting with users. More importantly, <a href=https://en.wikipedia.org/wiki/Immigration_and_Naturalization_Service>INS</a> can serve as a tool for editors to help them finish their jobs. In our experiments, INS performs well on both <a href=https://en.wikipedia.org/wiki/Topic_and_comment>topic representation</a> and synthesis article generation. A <a href=https://en.wikipedia.org/wiki/User_study>user study</a> also demonstrates the usefulness and users&#8217; satisfaction with the INS tool. A demo video is available at.<url>https://youtu.be/7ItteKW3GEk</url>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K19-1097.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K19-1097 data-toggle=collapse aria-expanded=false aria-controls=abstract-K19-1097 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/K19-1097/>Towards a Unified End-to-End Approach for Fully Unsupervised Cross-Lingual Sentiment Analysis</a></strong><br><a href=/people/y/yanlin-feng/>Yanlin Feng</a>
|
<a href=/people/x/xiaojun-wan/>Xiaojun Wan</a><br><a href=/volumes/K19-1/ class=text-muted>Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K19-1097><div class="card-body p-3 small">Sentiment analysis in low-resource languages suffers from the lack of training data. Cross-lingual sentiment analysis (CLSA) aims to improve the performance on these <a href=https://en.wikipedia.org/wiki/Language>languages</a> by leveraging annotated data from other languages. Recent studies have shown that CLSA can be performed in a fully unsupervised manner, without exploiting either target language supervision or cross-lingual supervision. However, these methods rely heavily on unsupervised cross-lingual word embeddings (CLWE), which has been shown to have serious drawbacks on distant language pairs (e.g. English-Japanese). In this paper, we propose an end-to-end CLSA model by leveraging unlabeled data in multiple languages and multiple domains and eliminate the need for unsupervised CLWE. Our model applies to two CLSA settings : the traditional cross-lingual in-domain setting and the more challenging cross-lingual cross-domain setting. We empirically evaluate our approach on the multilingual multi-domain Amazon review dataset. Experimental results show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms the baselines by a large margin despite its minimal resource requirement.</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1089.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1089 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1089 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1089/>Point Precisely : Towards Ensuring the Precision of Data in Generated Texts Using Delayed Copy Mechanism</a></strong><br><a href=/people/l/liunian-li/>Liunian Li</a>
|
<a href=/people/x/xiaojun-wan/>Xiaojun Wan</a><br><a href=/volumes/C18-1/ class=text-muted>Proceedings of the 27th International Conference on Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1089><div class="card-body p-3 small">The task of data-to-text generation aims to generate descriptive texts conditioned on a number of database records, and recent neural models have shown significant progress on this task. The attention based encoder-decoder models with copy mechanism have achieved state-of-the-art results on a few data-to-text datasets. However, such <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> still face the problem of putting incorrect data records in the generated texts, especially on some more challenging datasets like <a href=https://en.wikipedia.org/wiki/RotoWire>RotoWire</a>. In this paper, we propose a two-stage approach with a delayed copy mechanism to improve the <a href=https://en.wikipedia.org/wiki/Precision_(computer_science)>precision</a> of data records in the generated texts. Our approach first adopts an encoder-decoder model to generate a template text with data slots to be filled and then leverages a proposed delayed copy mechanism to fill in the slots with proper data records. Our delayed copy mechanism can take into account all the information of the input data records and the full generated template text by using double attention, position-aware attention and a pairwise ranking loss. The two <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> in the two stages are trained separately. Evaluation results on the RotoWire dataset verify the efficacy of our proposed approach to generate better templates and copy data records more precisely.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1414.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1414 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1414 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/306119942 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1414" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1414/>Semantic Role Labeling for Learner Chinese : the Importance of Syntactic Parsing and L2-L1 Parallel Data<span class=acl-fixed-case>C</span>hinese: the Importance of Syntactic Parsing and <span class=acl-fixed-case>L</span>2-<span class=acl-fixed-case>L</span>1 Parallel Data</a></strong><br><a href=/people/z/zi-lin/>Zi Lin</a>
|
<a href=/people/y/yuguang-duan/>Yuguang Duan</a>
|
<a href=/people/y/yuanyuan-zhao/>Yuanyuan Zhao</a>
|
<a href=/people/w/weiwei-sun/>Weiwei Sun</a>
|
<a href=/people/x/xiaojun-wan/>Xiaojun Wan</a><br><a href=/volumes/D18-1/ class=text-muted>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1414><div class="card-body p-3 small">This paper studies <a href=https://en.wikipedia.org/wiki/Semantic_parsing>semantic parsing</a> for interlanguage (L2), taking semantic role labeling (SRL) as a case task and learner Chinese as a case language. We first manually annotate the semantic roles for a set of learner texts to derive a gold standard for automatic SRL. Based on the new data, we then evaluate three off-the-shelf SRL systems, i.e., the PCFGLA-parser-based, neural-parser-based and neural-syntax-agnostic systems, to gauge how successful SRL for learner Chinese can be. We find two non-obvious facts : 1) the L1-sentence-trained systems performs rather badly on the L2 data ; 2) the performance drop from the L1 data to the L2 data of the two parser-based systems is much smaller, indicating the importance of syntactic parsing in SRL for interlanguages. Finally, the paper introduces a new agreement-based model to explore the semantic coherency information in the large-scale L2-L1 parallel data. We then show such information is very effective to enhance SRL for learner texts. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieves an <a href=https://en.wikipedia.org/wiki/F-score>F-score</a> of 72.06, which is a 2.02 point improvement over the best <a href=https://en.wikipedia.org/wiki/Baseline_(medicine)>baseline</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6545.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6545 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6545 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6545/>Adapting Neural Single-Document Summarization Model for Abstractive Multi-Document Summarization : A Pilot Study</a></strong><br><a href=/people/j/jianmin-zhang/>Jianmin Zhang</a>
|
<a href=/people/j/jiwei-tan/>Jiwei Tan</a>
|
<a href=/people/x/xiaojun-wan/>Xiaojun Wan</a><br><a href=/volumes/W18-65/ class=text-muted>Proceedings of the 11th International Conference on Natural Language Generation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6545><div class="card-body p-3 small">Till now, neural abstractive summarization methods have achieved great success for single document summarization (SDS). However, due to the lack of large scale multi-document summaries, such methods can be hardly applied to multi-document summarization (MDS). In this paper, we investigate neural abstractive methods for MDS by adapting a state-of-the-art neural abstractive summarization model for SDS. We propose an approach to extend the neural abstractive model trained on large scale SDS data to the MDS task. Our approach only makes use of a small number of multi-document summaries for <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine tuning</a>. Experimental results on two benchmark DUC datasets demonstrate that our approach can outperform a variety of baseline neural models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K18-1054.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K18-1054 data-toggle=collapse aria-expanded=false aria-controls=abstract-K18-1054 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=K18-1054" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/K18-1054/>Neural Maximum Subgraph Parsing for Cross-Domain Semantic Dependency Analysis</a></strong><br><a href=/people/y/yufei-chen/>Yufei Chen</a>
|
<a href=/people/s/sheng-huang/>Sheng Huang</a>
|
<a href=/people/f/fang-wang/>Fang Wang</a>
|
<a href=/people/j/junjie-cao/>Junjie Cao</a>
|
<a href=/people/w/weiwei-sun/>Weiwei Sun</a>
|
<a href=/people/x/xiaojun-wan/>Xiaojun Wan</a><br><a href=/volumes/K18-1/ class=text-muted>Proceedings of the 22nd Conference on Computational Natural Language Learning</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K18-1054><div class="card-body p-3 small">We present experiments for cross-domain semantic dependency analysis with a neural Maximum Subgraph parser. Our <a href=https://en.wikipedia.org/wiki/Parsing>parser</a> targets 1-endpoint-crossing, pagenumber-2 graphs which are a good fit to semantic dependency graphs, and utilizes an efficient dynamic programming algorithm for decoding. For disambiguation, the <a href=https://en.wikipedia.org/wiki/Parsing>parser</a> associates words with BiLSTM vectors and utilizes these <a href=https://en.wikipedia.org/wiki/Euclidean_vector>vectors</a> to assign scores to candidate dependencies. We conduct experiments on the <a href=https://en.wikipedia.org/wiki/Data_set>data sets</a> from SemEval 2015 as well as Chinese CCGBank. Our <a href=https://en.wikipedia.org/wiki/Parsing>parser</a> achieves very competitive results for both <a href=https://en.wikipedia.org/wiki/English_language>English</a> and <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a>. To improve the parsing performance on cross-domain texts, we propose a data-oriented method to explore the linguistic generality encoded in English Resource Grammar, which is a precisionoriented, hand-crafted HPSG grammar, in an implicit way. Experiments demonstrate the effectiveness of our data-oriented method across a wide range of conditions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1153.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1153 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1153 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1153.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-1153/>A Neural Approach to Pun Generation</a></strong><br><a href=/people/z/zhiwei-yu/>Zhiwei Yu</a>
|
<a href=/people/j/jiwei-tan/>Jiwei Tan</a>
|
<a href=/people/x/xiaojun-wan/>Xiaojun Wan</a><br><a href=/volumes/P18-1/ class=text-muted>Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1153><div class="card-body p-3 small">Automatic pun generation is an interesting and challenging text generation task. Previous efforts rely on <a href=https://en.wikipedia.org/wiki/Template_(word_processing)>templates</a> or laboriously manually annotated pun datasets, which heavily constrains the quality and diversity of generated puns. Since sequence-to-sequence models provide an effective technique for text generation, it is promising to investigate these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> on the pun generation task. In this paper, we propose neural network models for homographic pun generation, and they can generate <a href=https://en.wikipedia.org/wiki/Pun>puns</a> without requiring any pun data for training. We first train a conditional neural language model from a general text corpus, and then generate puns from the <a href=https://en.wikipedia.org/wiki/Language_model>language model</a> with an elaborately designed decoding algorithm. Automatic and human evaluations show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> are able to generate homographic puns of good readability and quality.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1179.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1179 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1179 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1179.Software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1179.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/288152765 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-1179/>Language Generation via DAG Transduction<span class=acl-fixed-case>DAG</span> Transduction</a></strong><br><a href=/people/y/yajie-ye/>Yajie Ye</a>
|
<a href=/people/w/weiwei-sun/>Weiwei Sun</a>
|
<a href=/people/x/xiaojun-wan/>Xiaojun Wan</a><br><a href=/volumes/P18-1/ class=text-muted>Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1179><div class="card-body p-3 small">A DAG automaton is a formal device for manipulating <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graphs</a>. By augmenting a DAG automaton with transduction rules, a DAG transducer has potential applications in fundamental NLP tasks. In this paper, we propose a novel DAG transducer to perform graph-to-program transformation. The target structure of our <a href=https://en.wikipedia.org/wiki/Transducer>transducer</a> is a <a href=https://en.wikipedia.org/wiki/Computer_program>program</a> licensed by a <a href=https://en.wikipedia.org/wiki/Declarative_programming>declarative programming language</a> rather than linguistic structures. By executing such a <a href=https://en.wikipedia.org/wiki/Computer_program>program</a>, we can easily get a surface string. Our <a href=https://en.wikipedia.org/wiki/Transducer>transducer</a> is designed especially for natural language generation (NLG) from type-logical semantic graphs. Taking Elementary Dependency Structures, a format of English Resource Semantics, as input, our NLG system achieves a BLEU-4 score of 68.07. This remarkable result demonstrates the feasibility of applying a DAG transducer to resolve NLG, as well as the effectiveness of our design.</div></div><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-2060.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-2060 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-2060 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/I17-2060.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/I17-2060.Datasets.zip data-toggle=tooltip data-placement=top title=Dataset><i class="fas fa-file-archive"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/I17-2060/>Leveraging Diverse Lexical Chains to Construct Essays for Chinese College Entrance Examination<span class=acl-fixed-case>C</span>hinese College Entrance Examination</a></strong><br><a href=/people/l/liunian-li/>Liunian Li</a>
|
<a href=/people/x/xiaojun-wan/>Xiaojun Wan</a>
|
<a href=/people/j/jin-ge-yao/>Jin-ge Yao</a>
|
<a href=/people/s/siming-yan/>Siming Yan</a><br><a href=/volumes/I17-2/ class=text-muted>Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-2060><div class="card-body p-3 small">In this work we study the challenging task of automatically constructing essays for Chinese college entrance examination where the topic is specified in advance. We explore a sentence extraction framework based on diversified lexical chains to capture <a href=https://en.wikipedia.org/wiki/Coherence_(linguistics)>coherence</a> and richness. Experimental analysis shows the effectiveness of our approach and reveals the importance of information richness in essay writing.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K17-1005.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K17-1005 data-toggle=collapse aria-expanded=false aria-controls=abstract-K17-1005 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/K17-1005/>Parsing for <a href=https://en.wikipedia.org/wiki/Grammatical_relation>Grammatical Relations</a> via Graph Merging</a></strong><br><a href=/people/w/weiwei-sun/>Weiwei Sun</a>
|
<a href=/people/y/yantao-du/>Yantao Du</a>
|
<a href=/people/x/xiaojun-wan/>Xiaojun Wan</a><br><a href=/volumes/K17-1/ class=text-muted>Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K17-1005><div class="card-body p-3 small">This paper is concerned with building deep grammatical relation (GR) analysis using data-driven approach. To deal with this problem, we propose graph merging, a new perspective, for building flexible dependency graphs : Constructing complex graphs via constructing simple subgraphs. We discuss two key problems in this perspective : (1) how to decompose a <a href=https://en.wikipedia.org/wiki/Complex_graph>complex graph</a> into simple subgraphs, and (2) how to combine <a href=https://en.wikipedia.org/wiki/Glossary_of_graph_theory_terms>subgraphs</a> into a coherent complex graph. Experiments demonstrate the effectiveness of graph merging. Our <a href=https://en.wikipedia.org/wiki/Parsing>parser</a> reaches state-of-the-art performance and is significantly better than two transition-based parsers.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K17-1035.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K17-1035 data-toggle=collapse aria-expanded=false aria-controls=abstract-K17-1035 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/K17-1035/>The Covert Helps Parse the Overt</a></strong><br><a href=/people/x/xun-zhang/>Xun Zhang</a>
|
<a href=/people/w/weiwei-sun/>Weiwei Sun</a>
|
<a href=/people/x/xiaojun-wan/>Xiaojun Wan</a><br><a href=/volumes/K17-1/ class=text-muted>Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K17-1035><div class="card-body p-3 small">This paper is concerned with whether deep syntactic information can help surface parsing, with a particular focus on <a href=https://en.wikipedia.org/wiki/Empty_category>empty categories</a>. We design new algorithms to produce dependency trees in which empty elements are allowed, and evaluate the impact of information about <a href=https://en.wikipedia.org/wiki/Empty_category>empty category</a> on parsing overt elements. Such information is helpful to reduce the <a href=https://en.wikipedia.org/wiki/Approximation_error>approximation error</a> in a structured parsing model, but increases the <a href=https://en.wikipedia.org/wiki/Feasible_region>search space</a> for <a href=https://en.wikipedia.org/wiki/Statistical_inference>inference</a> and accordingly the estimation error. To deal with structure-based overfitting, we propose to integrate disambiguation models with and without empty elements, and perform structure regularization via joint decoding. Experiments on English and Chinese TreeBanks with different parsing models indicate that incorporating empty elements consistently improves surface parsing.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1077.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1077 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1077 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P17-1077.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P17-1077.Software.tgz data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234957523 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-1077/>Semantic Dependency Parsing via Book Embedding</a></strong><br><a href=/people/w/weiwei-sun/>Weiwei Sun</a>
|
<a href=/people/j/junjie-cao/>Junjie Cao</a>
|
<a href=/people/x/xiaojun-wan/>Xiaojun Wan</a><br><a href=/volumes/P17-1/ class=text-muted>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1077><div class="card-body p-3 small">We model a <a href=https://en.wikipedia.org/wiki/Dependency_graph>dependency graph</a> as a book, a particular kind of <a href=https://en.wikipedia.org/wiki/Topological_space>topological space</a>, for semantic dependency parsing. The spine of the book is made up of a sequence of words, and each page contains a subset of noncrossing arcs. To build a semantic graph for a given sentence, we design new Maximum Subgraph algorithms to generate noncrossing graphs on each page, and a Lagrangian Relaxation-based algorithm tocombine pages into a book. Experiments demonstrate the effectiveness of the bookembedding framework across a wide range of conditions. Our <a href=https://en.wikipedia.org/wiki/Parsing>parser</a> obtains comparable results with a state-of-the-art transition-based parser.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1108.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1108 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1108 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234959124 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-1108/>Abstractive Document Summarization with a Graph-Based Attentional Neural Model</a></strong><br><a href=/people/j/jiwei-tan/>Jiwei Tan</a>
|
<a href=/people/x/xiaojun-wan/>Xiaojun Wan</a>
|
<a href=/people/j/jianguo-xiao/>Jianguo Xiao</a><br><a href=/volumes/P17-1/ class=text-muted>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1108><div class="card-body p-3 small">Abstractive summarization is the ultimate goal of <a href=https://en.wikipedia.org/wiki/Document_summarization>document summarization</a> research, but previously it is less investigated due to the immaturity of text generation techniques. Recently impressive progress has been made to abstractive sentence summarization using neural models. Unfortunately, attempts on abstractive document summarization are still in a primitive stage, and the evaluation results are worse than extractive methods on benchmark datasets. In this paper, we review the difficulties of neural abstractive document summarization, and propose a novel graph-based attention mechanism in the sequence-to-sequence framework. The intuition is to address the <a href=https://en.wikipedia.org/wiki/Salience_(neuroscience)>saliency factor</a> of summarization, which has been overlooked by prior works. Experimental results demonstrate our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is able to achieve considerable improvement over previous neural abstractive models. The data-driven neural abstractive method is also competitive with state-of-the-art extractive methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1193.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1193 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1193 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P17-1193.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P17-1193.Software.tgz data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-1193/>Parsing to 1-Endpoint-Crossing, Pagenumber-2 Graphs</a></strong><br><a href=/people/j/junjie-cao/>Junjie Cao</a>
|
<a href=/people/s/sheng-huang/>Sheng Huang</a>
|
<a href=/people/w/weiwei-sun/>Weiwei Sun</a>
|
<a href=/people/x/xiaojun-wan/>Xiaojun Wan</a><br><a href=/volumes/P17-1/ class=text-muted>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1193><div class="card-body p-3 small">We study the Maximum Subgraph problem in deep dependency parsing. We consider two restrictions to deep dependency graphs : (a) 1-endpoint-crossing and (b) pagenumber-2. Our main contribution is an exact <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> that obtains maximum subgraphs satisfying both restrictions simultaneously in time O(n5). Moreover, ignoring one linguistically-rare structure descreases the <a href=https://en.wikipedia.org/wiki/Computational_complexity_theory>complexity</a> to O(n4). We also extend our quartic-time algorithm into a practical <a href=https://en.wikipedia.org/wiki/Parsing>parser</a> with a discriminative disambiguation model and evaluate its performance on four linguistic data sets used in semantic dependency parsing.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-3504.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-3504 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-3504 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-3504/>Content Selection for Real-time Sports News Construction from Commentary Texts</a></strong><br><a href=/people/j/jin-ge-yao/>Jin-ge Yao</a>
|
<a href=/people/j/jianmin-zhang/>Jianmin Zhang</a>
|
<a href=/people/x/xiaojun-wan/>Xiaojun Wan</a>
|
<a href=/people/j/jianguo-xiao/>Jianguo Xiao</a><br><a href=/volumes/W17-35/ class=text-muted>Proceedings of the 10th International Conference on Natural Language Generation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-3504><div class="card-body p-3 small">We study the task of constructing sports news report automatically from <a href=https://en.wikipedia.org/wiki/Sports_commentator>live commentary</a> and focus on content selection. Rather than receiving every piece of text of a sports match before news construction, as in previous related work, we novelly verify the feasibility of a more challenging but more useful setting to generate news report on the fly by treating live text input as a stream. Specifically, we design various scoring functions to address different requirements of the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. The near submodularity of scoring functions makes it possible to adapt efficient <a href=https://en.wikipedia.org/wiki/Greedy_algorithm>greedy algorithms</a> even in stream data settings. Experiments suggest that our proposed framework can already produce comparable results compared with previous work that relies on a supervised learning-to-rank model with heavy feature engineering.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-3526.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-3526 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-3526 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-3526/>Towards Automatic Generation of Product Reviews from Aspect-Sentiment Scores</a></strong><br><a href=/people/h/hongyu-zang/>Hongyu Zang</a>
|
<a href=/people/x/xiaojun-wan/>Xiaojun Wan</a><br><a href=/volumes/W17-35/ class=text-muted>Proceedings of the 10th International Conference on Natural Language Generation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-3526><div class="card-body p-3 small">Data-to-text generation is very essential and important in machine writing applications. The recent <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning models</a>, like <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>Recurrent Neural Networks (RNNs)</a>, have shown a bright future for relevant text generation tasks. However, rare work has been done for automatic generation of long reviews from user opinions. In this paper, we introduce a <a href=https://en.wikipedia.org/wiki/Deep_learning>deep neural network model</a> to generate long Chinese reviews from aspect-sentiment scores representing users&#8217; opinions. We conduct our study within the framework of encoder-decoder networks, and we propose a hierarchical structure with aligned attention in the Long-Short Term Memory (LSTM) decoder. Experiments show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms retrieval based baseline methods, and also beats the sequential generation models in qualitative evaluations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1053.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1053 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1053 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1053/>Towards a Universal Sentiment Classifier in Multiple languages</a></strong><br><a href=/people/k/kui-xu/>Kui Xu</a>
|
<a href=/people/x/xiaojun-wan/>Xiaojun Wan</a><br><a href=/volumes/D17-1/ class=text-muted>Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1053><div class="card-body p-3 small">Existing sentiment classifiers usually work for only one specific language, and different classification models are used in different languages. In this paper we aim to build a universal sentiment classifier with a single <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification model</a> in multiple different languages. In order to achieve this goal, we propose to learn multilingual sentiment-aware word embeddings simultaneously based only on the labeled reviews in <a href=https://en.wikipedia.org/wiki/English_language>English</a> and unlabeled parallel data available in a few language pairs. It is not required that the parallel data exist between <a href=https://en.wikipedia.org/wiki/English_language>English</a> and any other language, because the <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment information</a> can be transferred into any language via pivot languages. We present the evaluation results of our universal sentiment classifier in five languages, and the results are very promising even when the parallel data between <a href=https://en.wikipedia.org/wiki/English_language>English</a> and the target languages are not used. Furthermore, the universal single classifier is compared with a few cross-language sentiment classifiers relying on direct parallel data between the source and target languages, and the results show that the performance of our universal sentiment classifier is very promising compared to that of different cross-language classifiers in multiple target languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1224.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1224 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1224 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1224/>Towards Automatic Construction of News Overview Articles by News Synthesis</a></strong><br><a href=/people/j/jianmin-zhang/>Jianmin Zhang</a>
|
<a href=/people/x/xiaojun-wan/>Xiaojun Wan</a><br><a href=/volumes/D17-1/ class=text-muted>Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1224><div class="card-body p-3 small">In this paper we investigate a new task of automatically constructing an overview article from a given set of <a href=https://en.wikipedia.org/wiki/Article_(publishing)>news articles</a> about a news event. We propose a news synthesis approach to address this task based on passage segmentation, <a href=https://en.wikipedia.org/wiki/Ranking>ranking</a>, selection and merging. Our proposed approach is compared with several typical multi-document summarization methods on the Wikinews dataset, and achieves the best performance on both automatic evaluation and manual evaluation.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Xiaojun+Wan" title="Search for 'Xiaojun Wan' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/w/weiwei-sun/ class=align-middle>Weiwei Sun</a>
<span class="badge badge-secondary align-middle ml-2">9</span></li><li class=list-group-item><a href=/people/h/hanqi-jin/ class=align-middle>Hanqi Jin</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/people/j/junjie-cao/ class=align-middle>Junjie Cao</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/people/j/jiwei-tan/ class=align-middle>Jiwei Tan</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/j/jianmin-zhang/ class=align-middle>Jianmin Zhang</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/y/yue-cao/ class=align-middle>Yue Cao</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/l/liunian-li/ class=align-middle>Liunian Li</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/j/jin-ge-yao/ class=align-middle>Jin-ge Yao</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/z/zhiwei-yu/ class=align-middle>Zhiwei Yu</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/h/hongyu-zang/ class=align-middle>Hongyu Zang</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/t/tianming-wang/ class=align-middle>Tianming Wang</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/y/yuanyuan-zhao/ class=align-middle>Yuanyuan Zhao</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/j/jianguo-xiao/ class=align-middle>Jianguo Xiao</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/s/sheng-huang/ class=align-middle>Sheng Huang</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/y/yufei-chen/ class=align-middle>Yufei Chen</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/s/siming-yan/ class=align-middle>Siming Yan</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/shaowei-yao/ class=align-middle>Shaowei Yao</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yantao-du/ class=align-middle>Yantao Du</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/x/xun-zhang/ class=align-middle>Xun Zhang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/x/xunjian-yin/ class=align-middle>Xunjian Yin</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/zi-lin/ class=align-middle>Zi Lin</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yuguang-duan/ class=align-middle>Yuguang Duan</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yitao-cai/ class=align-middle>Yitao Cai</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/renliang-sun/ class=align-middle>Renliang Sun</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/kentaro-inui/ class=align-middle>Kentaro Inui</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jing-jiang/ class=align-middle>Jing Jiang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/v/vincent-ng/ class=align-middle>Vincent Ng</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/kui-xu/ class=align-middle>Kui Xu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/meichun-liu/ class=align-middle>Meichun Liu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/hao-ran-wei/ class=align-middle>Hao-Ran Wei</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/b/boxing-chen/ class=align-middle>Boxing Chen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/hui-liu/ class=align-middle>Hui Liu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/w/wentao-qin/ class=align-middle>Wentao Qin</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/f/fang-wang/ class=align-middle>Fang Wang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yajie-ye/ class=align-middle>Yajie Ye</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yanlin-feng/ class=align-middle>Yanlin Feng</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/acl/ class=align-middle>ACL</a><span class="badge badge-secondary align-middle ml-2">9</span></li><li class=list-group-item><a href=/venues/emnlp/ class=align-middle>EMNLP</a><span class="badge badge-secondary align-middle ml-2">7</span></li><li class=list-group-item><a href=/venues/conll/ class=align-middle>CoNLL</a><span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/venues/ws/ class=align-middle>WS</a><span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/venues/naacl/ class=align-middle>NAACL</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-venues aria-expanded=false aria-controls=more-venues>show all...</li><div class="collapse border-top" id=more-venues><li class=list-group-item><a href=/venues/findings/ class=align-middle>Findings</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/coling/ class=align-middle>COLING</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/ijcnlp/ class=align-middle>IJCNLP</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/cl/ class=align-middle>CL</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/tacl/ class=align-middle>TACL</a><span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>