<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Xuejie Zhang - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Xuejie</span> <span class=font-weight-bold>Zhang</span></h2><hr><div class=row><div class=col-lg-9><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.semeval-1.58.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--semeval-1--58 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.semeval-1.58 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.semeval-1.58/>YNU-HPCC at SemEval-2021 Task 11 : Using a BERT Model to Extract Contributions from NLP Scholarly Articles<span class=acl-fixed-case>YNU</span>-<span class=acl-fixed-case>HPCC</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2021 Task 11: Using a <span class=acl-fixed-case>BERT</span> Model to Extract Contributions from <span class=acl-fixed-case>NLP</span> Scholarly Articles</a></strong><br><a href=/people/x/xinge-ma/>Xinge Ma</a>
|
<a href=/people/j/jin-wang/>Jin Wang</a>
|
<a href=/people/x/xuejie-zhang/>Xuejie Zhang</a><br><a href=/volumes/2021.semeval-1/ class=text-muted>Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--semeval-1--58><div class="card-body p-3 small">This paper describes the system we built as the YNU-HPCC team in the SemEval-2021 Task 11 : NLPContributionGraph. This task involves first identifying sentences in the given natural language processing (NLP) scholarly articles that reflect research contributions through binary classification ; then identifying the core scientific terms and their relation phrases from these contribution sentences by sequence labeling ; and finally, these scientific terms and relation phrases are categorized, identified, and organized into subject-predicate-object triples to form a <a href=https://en.wikipedia.org/wiki/Knowledge_graph>knowledge graph</a> with the help of <a href=https://en.wikipedia.org/wiki/Multiclass_classification>multiclass classification</a> and <a href=https://en.wikipedia.org/wiki/Multi-label_classification>multi-label classification</a>. We developed a system for this task using a pre-trained language representation model called BERT that stands for Bidirectional Encoder Representations from Transformers, and achieved good results. The average F1-score for Evaluation Phase 2, Part 1 was 0.4562 and ranked 7th, and the average F1-score for Evaluation Phase 2, Part 2 was 0.6541, and also ranked 7th.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.semeval-1.144.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--semeval-1--144 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.semeval-1.144 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.semeval-1.144/>YNU-HPCC at SemEval-2021 Task 6 : Combining ALBERT and Text-CNN for Persuasion Detection in Texts and Images<span class=acl-fixed-case>YNU</span>-<span class=acl-fixed-case>HPCC</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2021 Task 6: Combining <span class=acl-fixed-case>ALBERT</span> and Text-<span class=acl-fixed-case>CNN</span> for Persuasion Detection in Texts and Images</a></strong><br><a href=/people/x/xingyu-zhu/>Xingyu Zhu</a>
|
<a href=/people/j/jin-wang/>Jin Wang</a>
|
<a href=/people/x/xuejie-zhang/>Xuejie Zhang</a><br><a href=/volumes/2021.semeval-1/ class=text-muted>Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--semeval-1--144><div class="card-body p-3 small">In recent years, <a href=https://en.wikipedia.org/wiki/Meme>memes</a> combining image and text have been widely used in <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>, and <a href=https://en.wikipedia.org/wiki/Meme>memes</a> are one of the most popular types of content used in online disinformation campaigns. In this paper, our study on the detection of persuasion techniques in <a href=https://en.wikipedia.org/wiki/Text_(literary_theory)>texts</a> and <a href=https://en.wikipedia.org/wiki/Image>images</a> in SemEval-2021 Task 6 is summarized. For propaganda technology detection in text, we propose a combination model of both ALBERT and Text CNN for text classification, as well as a BERT-based multi-task sequence labeling model for propaganda technology coverage span detection. For the meme classification task involved in text understanding and visual feature extraction, we designed a parallel channel model divided into text and image channels. Our <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>method</a> achieved a good performance on subtasks 1 and 3. The micro F1-scores of 0.492, 0.091, and 0.446 achieved on the test sets of the three subtasks ranked 12th, 7th, and 11th, respectively, and all are higher than the baseline model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.semeval-1.184.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--semeval-1--184 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.semeval-1.184 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.semeval-1.184/>YNU-HPCC at SemEval-2021 Task 10 : Using a Transformer-based Source-Free Domain Adaptation Model for Semantic Processing<span class=acl-fixed-case>YNU</span>-<span class=acl-fixed-case>HPCC</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2021 Task 10: Using a Transformer-based Source-Free Domain Adaptation Model for Semantic Processing</a></strong><br><a href=/people/z/zhewen-yu/>Zhewen Yu</a>
|
<a href=/people/j/jin-wang/>Jin Wang</a>
|
<a href=/people/x/xuejie-zhang/>Xuejie Zhang</a><br><a href=/volumes/2021.semeval-1/ class=text-muted>Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--semeval-1--184><div class="card-body p-3 small">Data sharing restrictions are common in NLP datasets. The purpose of this task is to develop a <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> trained in a source domain to make predictions for a target domain with related domain data. To address the issue, the organizers provided the <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> that fine-tuned a large number of source domain data on pre-trained models and the dev data for participants. But the source domain data was not distributed. This paper describes the provided <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> to the NER (Name entity recognition) task and the ways to develop the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>. As a little data provided, pre-trained models are suitable to solve the cross-domain tasks. The models fine-tuned by large number of another domain could be effective in new domain because the task had no change.</div></div><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.110.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--110 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.110 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.110/>YNU-HPCC at SemEval-2020 Task 7 : Using an Ensemble BiGRU Model to Evaluate the Humor of Edited News Titles<span class=acl-fixed-case>YNU</span>-<span class=acl-fixed-case>HPCC</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 7: Using an Ensemble <span class=acl-fixed-case>B</span>i<span class=acl-fixed-case>GRU</span> Model to Evaluate the Humor of Edited News Titles</a></strong><br><a href=/people/j/joseph-tomasulo/>Joseph Tomasulo</a>
|
<a href=/people/j/jin-wang/>Jin Wang</a>
|
<a href=/people/x/xuejie-zhang/>Xuejie Zhang</a><br><a href=/volumes/2020.semeval-1/ class=text-muted>Proceedings of the Fourteenth Workshop on Semantic Evaluation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--110><div class="card-body p-3 small">This paper describes an ensemble model designed for Semeval-2020 Task 7. The task is based on the Humicroedit dataset that is comprised of news titles and one-word substitutions designed to make them humorous. We use <a href=https://en.wikipedia.org/wiki/BERT>BERT</a>, <a href=https://en.wikipedia.org/wiki/FastText>FastText</a>, Elmo, and Word2Vec to encode these titles then pass them to a bidirectional gated recurrent unit (BiGRU) with attention. Finally, we used <a href=https://en.wikipedia.org/wiki/XGBoost>XGBoost</a> on the concatenation of the results of the different <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> to make predictions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.224.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--224 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.224 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.224/>YNU-HPCC at SemEval-2020 Task 10 : Using a Multi-granularity Ordinal Classification of the BiLSTM Model for Emphasis Selection<span class=acl-fixed-case>YNU</span>-<span class=acl-fixed-case>HPCC</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 10: Using a Multi-granularity Ordinal Classification of the <span class=acl-fixed-case>B</span>i<span class=acl-fixed-case>LSTM</span> Model for Emphasis Selection</a></strong><br><a href=/people/d/dawei-liao/>Dawei Liao</a>
|
<a href=/people/j/jin-wang/>Jin Wang</a>
|
<a href=/people/x/xuejie-zhang/>Xuejie Zhang</a><br><a href=/volumes/2020.semeval-1/ class=text-muted>Proceedings of the Fourteenth Workshop on Semantic Evaluation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--224><div class="card-body p-3 small">In this study, we propose a multi-granularity ordinal classification method to address the problem of emphasis selection. In detail, the <a href=https://en.wikipedia.org/wiki/Word_embedding>word embedding</a> is learned from Embeddings from Language Model (ELMO) to extract feature vector representation. Then, the ordinal classifica-tions are implemented on four different multi-granularities to approximate the continuous em-phasize values. Comparative experiments were conducted to compare the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> with baseline in which the <a href=https://en.wikipedia.org/wiki/Problem_solving>problem</a> is transformed to label distribution problem.</div></div><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1343.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1343 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1343 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1343/>Investigating Dynamic Routing in Tree-Structured LSTM for <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>Sentiment Analysis</a><span class=acl-fixed-case>LSTM</span> for Sentiment Analysis</a></strong><br><a href=/people/j/jin-wang/>Jin Wang</a>
|
<a href=/people/l/liang-chih-yu/>Liang-Chih Yu</a>
|
<a href=/people/k/k-robert-lai/>K. Robert Lai</a>
|
<a href=/people/x/xuejie-zhang/>Xuejie Zhang</a><br><a href=/volumes/D19-1/ class=text-muted>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1343><div class="card-body p-3 small">Deep neural network models such as <a href=https://en.wikipedia.org/wiki/Long_short-term_memory>long short-term memory (LSTM)</a> and tree-LSTM have been proven to be effective for <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a>. However, sequential LSTM is a bias model wherein the words in the tail of a sentence are more heavily emphasized than those in the header for building sentence representations. Even tree-LSTM, with useful structural information, could not avoid the bias problem because the root node will be dominant and the nodes in the bottom of the <a href=https://en.wikipedia.org/wiki/Parse_tree>parse tree</a> will be less emphasized even though they may contain salient information. To overcome the bias problem, this study proposes a capsule tree-LSTM model, introducing a dynamic routing algorithm as an aggregation layer to build sentence representation by assigning different weights to nodes according to their contributions to prediction. Experiments on Stanford Sentiment Treebank (SST) for sentiment classification and EmoBank for <a href=https://en.wikipedia.org/wiki/Regression_analysis>regression</a> show that the proposed method improved the performance of tree-LSTM and other neural network models. In addition, the deeper the <a href=https://en.wikipedia.org/wiki/Tree_structure>tree structure</a>, the bigger the improvement.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S19-2143.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S19-2143 data-toggle=collapse aria-expanded=false aria-controls=abstract-S19-2143 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/S19-2143/>YNUWB at SemEval-2019 Task 6 : K-max pooling CNN with average meta-embedding for identifying offensive language<span class=acl-fixed-case>YNUWB</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2019 Task 6: K-max pooling <span class=acl-fixed-case>CNN</span> with average meta-embedding for identifying offensive language</a></strong><br><a href=/people/b/bin-wang/>Bin Wang</a>
|
<a href=/people/x/xiaobing-zhou/>Xiaobing Zhou</a>
|
<a href=/people/x/xuejie-zhang/>Xuejie Zhang</a><br><a href=/volumes/S19-2/ class=text-muted>Proceedings of the 13th International Workshop on Semantic Evaluation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S19-2143><div class="card-body p-3 small">This paper describes the <a href=https://en.wikipedia.org/wiki/System>system</a> submitted to SemEval 2019 Task 6 : OffensEval 2019. The task aims to identify and categorize <a href=https://en.wikipedia.org/wiki/Profanity>offensive language</a> in <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>, we only participate in Sub-task A, which aims to identify <a href=https://en.wikipedia.org/wiki/Profanity>offensive language</a>. In order to address this task, we propose a system based on a K-max pooling convolutional neural network model, and use an argument for averaging as a valid meta-embedding technique to get a metaembedding. Finally, we also use a cyclic learning rate policy to improve <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> performance. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieves a Macro F1-score of 0.802 (ranked 9/103) in the Sub-task A.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S19-2207.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S19-2207 data-toggle=collapse aria-expanded=false aria-controls=abstract-S19-2207 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/S19-2207/>YNU-HPCC at SemEval-2019 Task 8 : Using A LSTM-Attention Model for Fact-Checking in Community Forums<span class=acl-fixed-case>YNU</span>-<span class=acl-fixed-case>HPCC</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2019 Task 8: Using A <span class=acl-fixed-case>LSTM</span>-Attention Model for Fact-Checking in Community Forums</a></strong><br><a href=/people/p/peng-liu/>Peng Liu</a>
|
<a href=/people/j/jin-wang/>Jin Wang</a>
|
<a href=/people/x/xuejie-zhang/>Xuejie Zhang</a><br><a href=/volumes/S19-2/ class=text-muted>Proceedings of the 13th International Workshop on Semantic Evaluation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S19-2207><div class="card-body p-3 small">We propose a system that uses a long short-term memory with attention mechanism (LSTM-Attention) model to complete the task. The LSTM-Attention model uses two LSTM to extract the <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> of the question and answer pair. Then, each of the <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> is sequentially composed using the <a href=https://en.wikipedia.org/wiki/Attentional_control>attention mechanism</a>, concatenating the two vectors into one. Finally, the concatenated vector is used as input for the <a href=https://en.wikipedia.org/wiki/Machine_learning>MLP</a> and the <a href=https://en.wikipedia.org/wiki/Machine_learning>MLP&#8217;s output layer</a> uses the softmax function to classify the provided answers into three categories. This <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is capable of extracting the <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> of the question and answer pair well. The results show that the proposed <a href=https://en.wikipedia.org/wiki/System>system</a> outperforms the <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline algorithm</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S19-2223.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S19-2223 data-toggle=collapse aria-expanded=false aria-controls=abstract-S19-2223 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/S19-2223/>YNU_DYX at SemEval-2019 Task 9 : A Stacked BiLSTM for Suggestion Mining Classification<span class=acl-fixed-case>YNU</span>_<span class=acl-fixed-case>DYX</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2019 Task 9: A Stacked <span class=acl-fixed-case>B</span>i<span class=acl-fixed-case>LSTM</span> for Suggestion Mining Classification</a></strong><br><a href=/people/y/yunxia-ding/>Yunxia Ding</a>
|
<a href=/people/x/xiaobing-zhou/>Xiaobing Zhou</a>
|
<a href=/people/x/xuejie-zhang/>Xuejie Zhang</a><br><a href=/volumes/S19-2/ class=text-muted>Proceedings of the 13th International Workshop on Semantic Evaluation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S19-2223><div class="card-body p-3 small">In this paper we describe a deep-learning system that competed as SemEval 2019 Task 9-SubTask A : Suggestion Mining from Online Reviews and Forums. We use Word2Vec to learn the distributed representations from sentences. This system is composed of a Stacked Bidirectional Long-Short Memory Network (SBiLSTM) for enriching word representations before and after the sequence relationship with context. We perform an <a href=https://en.wikipedia.org/wiki/Statistical_ensemble_(mathematical_physics)>ensemble</a> to improve the effectiveness of our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>. Our official submission results achieve an F1-score 0.5659.</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S18-1040.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S18-1040 data-toggle=collapse aria-expanded=false aria-controls=abstract-S18-1040 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/S18-1040/>YNU-HPCC at SemEval-2018 Task 1 : BiLSTM with Attention based Sentiment Analysis for Affect in Tweets<span class=acl-fixed-case>YNU</span>-<span class=acl-fixed-case>HPCC</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2018 Task 1: <span class=acl-fixed-case>B</span>i<span class=acl-fixed-case>LSTM</span> with Attention based Sentiment Analysis for Affect in Tweets</a></strong><br><a href=/people/y/you-zhang/>You Zhang</a>
|
<a href=/people/j/jin-wang/>Jin Wang</a>
|
<a href=/people/x/xuejie-zhang/>Xuejie Zhang</a><br><a href=/volumes/S18-1/ class=text-muted>Proceedings of The 12th International Workshop on Semantic Evaluation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S18-1040><div class="card-body p-3 small">We implemented the <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment system</a> in all five subtasks for <a href=https://en.wikipedia.org/wiki/English_language>English</a> and <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a>. All subtasks involve emotion or sentiment intensity prediction (regression and ordinal classification) and emotions determining (multi-labels classification). The useful BiLSTM (Bidirectional Long-Short Term Memory) model with attention mechanism was mainly applied for our <a href=https://en.wikipedia.org/wiki/System>system</a>. We use BiLSTM in order to get word information extracted from both directions. The attention mechanism was used to find the contribution of each word for improving the scores. Furthermore, based on BiLSTMATT (BiLSTM with attention mechanism) a few deep-learning algorithms were employed for different subtasks. For regression and ordinal classification tasks we used <a href=https://en.wikipedia.org/wiki/Domain_adaptation>domain adaptation</a> and ensemble learning methods to leverage base model. While a single base model was used for <a href=https://en.wikipedia.org/wiki/Task_(computing)>multi-labels task</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S18-1073.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S18-1073 data-toggle=collapse aria-expanded=false aria-controls=abstract-S18-1073 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/S18-1073/>YNU-HPCC at SemEval-2018 Task 2 : Multi-ensemble Bi-GRU Model with Attention Mechanism for Multilingual Emoji Prediction<span class=acl-fixed-case>YNU</span>-<span class=acl-fixed-case>HPCC</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2018 Task 2: Multi-ensemble <span class=acl-fixed-case>B</span>i-<span class=acl-fixed-case>GRU</span> Model with Attention Mechanism for Multilingual Emoji Prediction</a></strong><br><a href=/people/n/nan-wang/>Nan Wang</a>
|
<a href=/people/j/jin-wang/>Jin Wang</a>
|
<a href=/people/x/xuejie-zhang/>Xuejie Zhang</a><br><a href=/volumes/S18-1/ class=text-muted>Proceedings of The 12th International Workshop on Semantic Evaluation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S18-1073><div class="card-body p-3 small">This paper describes our approach to SemEval-2018 Task 2, which aims to predict the most likely associated <a href=https://en.wikipedia.org/wiki/Emoji>emoji</a>, given a tweet in <a href=https://en.wikipedia.org/wiki/English_language>English</a> or <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a>. We normalized text-based tweets during pre-processing, following which we utilized a bi-directional gated recurrent unit with an attention mechanism to build our base model. Multi-models with or without class weights were trained for the <a href=https://en.wikipedia.org/wiki/Ensemble_learning>ensemble methods</a>. We boosted models without <a href=https://en.wikipedia.org/wiki/Weight_function>class weights</a>, and only strong boost classifiers were identified. In our <a href=https://en.wikipedia.org/wiki/System>system</a>, not only was a boosting method used, but we also took advantage of the voting ensemble method to enhance our final <a href=https://en.wikipedia.org/wiki/System>system</a> result. Our method demonstrated an obvious improvement of approximately 3 % of the macro F1 score in <a href=https://en.wikipedia.org/wiki/English_language>English</a> and 2 % in <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S18-1101.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S18-1101 data-toggle=collapse aria-expanded=false aria-controls=abstract-S18-1101 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/S18-1101/>YNU-HPCC at SemEval-2018 Task 3 : Ensemble Neural Network Models for Irony Detection on Twitter<span class=acl-fixed-case>YNU</span>-<span class=acl-fixed-case>HPCC</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2018 Task 3: Ensemble Neural Network Models for Irony Detection on <span class=acl-fixed-case>T</span>witter</a></strong><br><a href=/people/b/bo-peng/>Bo Peng</a>
|
<a href=/people/j/jin-wang/>Jin Wang</a>
|
<a href=/people/x/xuejie-zhang/>Xuejie Zhang</a><br><a href=/volumes/S18-1/ class=text-muted>Proceedings of The 12th International Workshop on Semantic Evaluation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S18-1101><div class="card-body p-3 small">This paper describe the <a href=https://en.wikipedia.org/wiki/System>system</a> we proposed to participate the first year of Irony detection in English tweets competition. Previous works demonstrate that LSTMs models have achieved remarkable performance in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a> ; besides, combining multiple classification from various individual classifiers in general is more powerful than a single classification. In order to obtain more precision classification of irony detection, our system trained several individual neural network classifiers and combined their results according to the ensemble-learning algorithm.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S18-1177.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S18-1177 data-toggle=collapse aria-expanded=false aria-controls=abstract-S18-1177 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/S18-1177/>YNU-HPCC at Semeval-2018 Task 11 : Using an Attention-based CNN-LSTM for Machine Comprehension using Commonsense Knowledge<span class=acl-fixed-case>YNU</span>-<span class=acl-fixed-case>HPCC</span> at <span class=acl-fixed-case>S</span>emeval-2018 Task 11: Using an Attention-based <span class=acl-fixed-case>CNN</span>-<span class=acl-fixed-case>LSTM</span> for Machine Comprehension using Commonsense Knowledge</a></strong><br><a href=/people/h/hang-yuan/>Hang Yuan</a>
|
<a href=/people/j/jin-wang/>Jin Wang</a>
|
<a href=/people/x/xuejie-zhang/>Xuejie Zhang</a><br><a href=/volumes/S18-1/ class=text-muted>Proceedings of The 12th International Workshop on Semantic Evaluation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S18-1177><div class="card-body p-3 small">This shared task is a typical <a href=https://en.wikipedia.org/wiki/Question_answering>question answering task</a>. Compared with the normal question and answer system, it needs to give the answer to the question based on the text provided. The essence of the problem is actually <a href=https://en.wikipedia.org/wiki/Reading_comprehension>reading comprehension</a>. Typically, there are several questions for each text that correspond to it. And for each question, there are two candidate answers (and only one of them is correct). To solve this problem, the usual approach is to use <a href=https://en.wikipedia.org/wiki/Convolutional_neural_network>convolutional neural networks (CNN)</a> and <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent neural network (RNN)</a> or their improved models (such as long short-term memory (LSTM)). In this paper, an attention-based CNN-LSTM model is proposed for this task. By adding an <a href=https://en.wikipedia.org/wiki/Attentional_control>attention mechanism</a> and combining the two <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>, this experimental result has been significantly improved.</div></div><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-4035.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-4035 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-4035 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-4035/>YNU-HPCC at IJCNLP-2017 Task 5 : Multi-choice Question Answering in Exams Using an Attention-based LSTM Model<span class=acl-fixed-case>YNU</span>-<span class=acl-fixed-case>HPCC</span> at <span class=acl-fixed-case>IJCNLP</span>-2017 Task 5: Multi-choice Question Answering in Exams Using an Attention-based <span class=acl-fixed-case>LSTM</span> Model</a></strong><br><a href=/people/h/hang-yuan/>Hang Yuan</a>
|
<a href=/people/y/you-zhang/>You Zhang</a>
|
<a href=/people/j/jin-wang/>Jin Wang</a>
|
<a href=/people/x/xuejie-zhang/>Xuejie Zhang</a><br><a href=/volumes/I17-4/ class=text-muted>Proceedings of the IJCNLP 2017, Shared Tasks</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-4035><div class="card-body p-3 small">A shared task is a typical question answering task that aims to test how accurately the participants can answer the questions in exams. Typically, for each question, there are four candidate answers, and only one of the answers is correct. The existing methods for such a task usually implement a <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent neural network (RNN)</a> or <a href=https://en.wikipedia.org/wiki/Long_short-term_memory>long short-term memory (LSTM)</a>. However, both RNN and LSTM are biased models in which the words in the tail of a sentence are more dominant than the words in the header. In this paper, we propose the use of an attention-based LSTM (AT-LSTM) model for these tasks. By adding an attention mechanism to the standard LSTM, this <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> can more easily capture long contextual information.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5227.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5227 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5227 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5227/>YNU-HPCC at EmoInt-2017 : Using a CNN-LSTM Model for Sentiment Intensity Prediction<span class=acl-fixed-case>YNU</span>-<span class=acl-fixed-case>HPCC</span> at <span class=acl-fixed-case>E</span>mo<span class=acl-fixed-case>I</span>nt-2017: Using a <span class=acl-fixed-case>CNN</span>-<span class=acl-fixed-case>LSTM</span> Model for Sentiment Intensity Prediction</a></strong><br><a href=/people/y/you-zhang/>You Zhang</a>
|
<a href=/people/h/hang-yuan/>Hang Yuan</a>
|
<a href=/people/j/jin-wang/>Jin Wang</a>
|
<a href=/people/x/xuejie-zhang/>Xuejie Zhang</a><br><a href=/volumes/W17-52/ class=text-muted>Proceedings of the 8th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5227><div class="card-body p-3 small">In this paper, we present a system that uses a convolutional neural network with long short-term memory (CNN-LSTM) model to complete the task. The CNN-LSTM model has two combined parts : CNN extracts local n-gram features within tweets and LSTM composes the <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> to capture long-distance dependency across tweets. Additionally, we used other three models (CNN, LSTM, BiLSTM) as baseline algorithms. Our introduced <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> showed good performance in the experimental results.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1056.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1056 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1056 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1056/>Refining Word Embeddings for <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>Sentiment Analysis</a></a></strong><br><a href=/people/l/liang-chih-yu/>Liang-Chih Yu</a>
|
<a href=/people/j/jin-wang/>Jin Wang</a>
|
<a href=/people/k/k-robert-lai/>K. Robert Lai</a>
|
<a href=/people/x/xuejie-zhang/>Xuejie Zhang</a><br><a href=/volumes/D17-1/ class=text-muted>Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1056><div class="card-body p-3 small">Word embeddings that can capture semantic and syntactic information from contexts have been extensively used for various natural language processing tasks. However, existing methods for learning context-based word embeddings typically fail to capture sufficient sentiment information. This may result in words with similar <a href=https://en.wikipedia.org/wiki/Representation_(mathematics)>vector representations</a> having an opposite sentiment polarity (e.g., good and bad), thus degrading <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> performance. Therefore, this study proposes a word vector refinement model that can be applied to any pre-trained word vectors (e.g., <a href=https://en.wikipedia.org/wiki/Word2vec>Word2vec</a> and GloVe). The refinement model is based on adjusting the vector representations of words such that they can be closer to both semantically and sentimentally similar words and further away from sentimentally dissimilar words. Experimental results show that the proposed method can improve conventional <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> and outperform previously proposed sentiment embeddings for both binary and fine-grained classification on Stanford Sentiment Treebank (SST).</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Xuejie+Zhang" title="Search for 'Xuejie Zhang' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/j/jin-wang/ class=align-middle>Jin Wang</a>
<span class="badge badge-secondary align-middle ml-2">14</span></li><li class=list-group-item><a href=/people/h/hang-yuan/ class=align-middle>Hang Yuan</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/y/you-zhang/ class=align-middle>You Zhang</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/l/liang-chih-yu/ class=align-middle>Liang-Chih Yu</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/k/k-robert-lai/ class=align-middle>K. Robert Lai</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/x/xiaobing-zhou/ class=align-middle>Xiaobing Zhou</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/j/joseph-tomasulo/ class=align-middle>Joseph Tomasulo</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/dawei-liao/ class=align-middle>Dawei Liao</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/b/bin-wang/ class=align-middle>Bin Wang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/p/peng-liu/ class=align-middle>Peng Liu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yunxia-ding/ class=align-middle>Yunxia Ding</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/n/nan-wang/ class=align-middle>Nan Wang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/b/bo-peng/ class=align-middle>Bo Peng</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/x/xinge-ma/ class=align-middle>Xinge Ma</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/x/xingyu-zhu/ class=align-middle>Xingyu Zhu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/zhewen-yu/ class=align-middle>Zhewen Yu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/semeval/ class=align-middle>SemEval</a><span class="badge badge-secondary align-middle ml-2">12</span></li><li class=list-group-item><a href=/venues/emnlp/ class=align-middle>EMNLP</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/ijcnlp/ class=align-middle>IJCNLP</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/ws/ class=align-middle>WS</a><span class="badge badge-secondary align-middle ml-2">1</span></li></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>