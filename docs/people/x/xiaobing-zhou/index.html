<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Xiaobing Zhou - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Xiaobing</span> <span class=font-weight-bold>Zhou</span></h2><hr><div class=row><div class=col-lg-9><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.semeval-1.34.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--semeval-1--34 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.semeval-1.34 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.semeval-1.34/>Grenzlinie at SemEval-2021 Task 7 : Detecting and Rating Humor and Offense<span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2021 Task 7: Detecting and Rating Humor and Offense</a></strong><br><a href=/people/r/renyuan-liu/>Renyuan Liu</a>
|
<a href=/people/x/xiaobing-zhou/>Xiaobing Zhou</a><br><a href=/volumes/2021.semeval-1/ class=text-muted>Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--semeval-1--34><div class="card-body p-3 small">This paper introduces the result of Team Grenzlinie&#8217;s experiment in SemEval-2021 task 7 : HaHackathon : Detecting and Rating Humor and Offense. This <a href=https://en.wikipedia.org/wiki/Task_(computing)>task</a> has two <a href=https://en.wikipedia.org/wiki/Task_(computing)>subtasks</a>. Subtask1 includes the humor detection task, the humor rating prediction task, and the humor controversy detection task. Subtask2 is an offensive rating prediction task. Detection task is a binary classification task, and the rating prediction task is a regression task between 0 to 5. 0 means the task is not humorous or not offensive, 5 means the task is very humorous or very offensive. For all the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a>, this paper chooses RoBERTa as the pre-trained model. In classification tasks, Bi-LSTM and <a href=https://en.wikipedia.org/wiki/Adversarial_learning>adversarial training</a> are adopted. In the <a href=https://en.wikipedia.org/wiki/Regression_analysis>regression task</a>, the Bi-LSTM is also adopted. And then we propose a new <a href=https://en.wikipedia.org/wiki/Methodology>approach</a> named compare method. Finally, our system achieves an <a href=https://en.wikipedia.org/wiki/F-number>F1-score</a> of 95.05 % in the humor detection task, <a href=https://en.wikipedia.org/wiki/F-number>F1-score</a> of 61.74 % in the humor controversy detection task, 0.6143 RMSE in humor rating task, 0.4761 RMSE in the offensive rating task on the test datasets.</div></div><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.183.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--183 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.183 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.183/>Zyy1510 Team at SemEval-2020 Task 9 : Sentiment Analysis for Code-Mixed Social Media Text with Sub-word Level Representations<span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 9: Sentiment Analysis for Code-Mixed Social Media Text with Sub-word Level Representations</a></strong><br><a href=/people/y/yueying-zhu/>Yueying Zhu</a>
|
<a href=/people/x/xiaobing-zhou/>Xiaobing Zhou</a>
|
<a href=/people/h/hongling-li/>Hongling Li</a>
|
<a href=/people/k/kunjie-dong/>Kunjie Dong</a><br><a href=/volumes/2020.semeval-1/ class=text-muted>Proceedings of the Fourteenth Workshop on Semantic Evaluation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--183><div class="card-body p-3 small">This paper reports the zyy1510 team&#8217;s work in the International Workshop on Semantic Evaluation (SemEval-2020) shared task on <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>Sentiment analysis</a> for Code-Mixed (Hindi-English, English-Spanish) Social Media Text. The purpose of this task is to determine the polarity of the text, dividing it into one of the three labels positive, negative and neutral. To achieve this goal, we propose an ensemble model of word n-grams-based Multinomial Naive Bayes (MNB) and sub-word level representations in LSTM (Sub-word LSTM) to identify the sentiments of code-mixed data of Hindi-English and English-Spanish. This <a href=https://en.wikipedia.org/wiki/Ensemble_learning>ensemble model</a> combines the advantage of rich sequential patterns and the intermediate features after convolution from the LSTM model, and the polarity of keywords from the MNB model to obtain the final sentiment score. We have tested our system on Hindi-English and English-Spanish code-mixed social media data sets released for the task. Our model achieves the F1 score of 0.647 in the Hindi-English task and 0.682 in the English-Spanish task, respectively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.273.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--273 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.273 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.273/>Lee at SemEval-2020 Task 12 : A BERT Model Based on the Maximum Self-ensemble Strategy for Identifying Offensive Language<span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 12: A <span class=acl-fixed-case>BERT</span> Model Based on the Maximum Self-ensemble Strategy for Identifying Offensive Language</a></strong><br><a href=/people/j/junyi-li/>Junyi Li</a>
|
<a href=/people/x/xiaobing-zhou/>Xiaobing Zhou</a>
|
<a href=/people/z/zichen-zhang/>Zichen Zhang</a><br><a href=/volumes/2020.semeval-1/ class=text-muted>Proceedings of the Fourteenth Workshop on Semantic Evaluation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--273><div class="card-body p-3 small">This article describes the <a href=https://en.wikipedia.org/wiki/System>system</a> submitted to SemEval 2020 Task 12 : OffensEval 2020. This task aims to identify and classify offensive languages in different languages on <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>. We only participate in the English part of subtask A, which aims to identify offensive languages in English. To solve this task, we propose a BERT model system based on the transform mechanism, and use the maximum self-ensemble to improve <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> performance. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieved a macro F1 score of 0.913(ranked 13/82) in subtask A.</div></div><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S19-2143.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S19-2143 data-toggle=collapse aria-expanded=false aria-controls=abstract-S19-2143 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/S19-2143/>YNUWB at SemEval-2019 Task 6 : K-max pooling CNN with average meta-embedding for identifying offensive language<span class=acl-fixed-case>YNUWB</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2019 Task 6: K-max pooling <span class=acl-fixed-case>CNN</span> with average meta-embedding for identifying offensive language</a></strong><br><a href=/people/b/bin-wang/>Bin Wang</a>
|
<a href=/people/x/xiaobing-zhou/>Xiaobing Zhou</a>
|
<a href=/people/x/xuejie-zhang/>Xuejie Zhang</a><br><a href=/volumes/S19-2/ class=text-muted>Proceedings of the 13th International Workshop on Semantic Evaluation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S19-2143><div class="card-body p-3 small">This paper describes the <a href=https://en.wikipedia.org/wiki/System>system</a> submitted to SemEval 2019 Task 6 : OffensEval 2019. The task aims to identify and categorize <a href=https://en.wikipedia.org/wiki/Profanity>offensive language</a> in <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>, we only participate in Sub-task A, which aims to identify <a href=https://en.wikipedia.org/wiki/Profanity>offensive language</a>. In order to address this task, we propose a system based on a K-max pooling convolutional neural network model, and use an argument for averaging as a valid meta-embedding technique to get a metaembedding. Finally, we also use a cyclic learning rate policy to improve <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> performance. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieves a Macro F1-score of 0.802 (ranked 9/103) in the Sub-task A.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S19-2223.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S19-2223 data-toggle=collapse aria-expanded=false aria-controls=abstract-S19-2223 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/S19-2223/>YNU_DYX at SemEval-2019 Task 9 : A Stacked BiLSTM for Suggestion Mining Classification<span class=acl-fixed-case>YNU</span>_<span class=acl-fixed-case>DYX</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2019 Task 9: A Stacked <span class=acl-fixed-case>B</span>i<span class=acl-fixed-case>LSTM</span> for Suggestion Mining Classification</a></strong><br><a href=/people/y/yunxia-ding/>Yunxia Ding</a>
|
<a href=/people/x/xiaobing-zhou/>Xiaobing Zhou</a>
|
<a href=/people/x/xuejie-zhang/>Xuejie Zhang</a><br><a href=/volumes/S19-2/ class=text-muted>Proceedings of the 13th International Workshop on Semantic Evaluation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S19-2223><div class="card-body p-3 small">In this paper we describe a deep-learning system that competed as SemEval 2019 Task 9-SubTask A : Suggestion Mining from Online Reviews and Forums. We use Word2Vec to learn the distributed representations from sentences. This system is composed of a Stacked Bidirectional Long-Short Memory Network (SBiLSTM) for enriching word representations before and after the sequence relationship with context. We perform an <a href=https://en.wikipedia.org/wiki/Statistical_ensemble_(mathematical_physics)>ensemble</a> to improve the effectiveness of our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>. Our official submission results achieve an F1-score 0.5659.</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S18-1031.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S18-1031 data-toggle=collapse aria-expanded=false aria-controls=abstract-S18-1031 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/S18-1031/>Yuan at SemEval-2018 Task 1 : Tweets Emotion Intensity Prediction using Ensemble Recurrent Neural Network<span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2018 Task 1: Tweets Emotion Intensity Prediction using Ensemble Recurrent Neural Network</a></strong><br><a href=/people/m/min-wang/>Min Wang</a>
|
<a href=/people/x/xiaobing-zhou/>Xiaobing Zhou</a><br><a href=/volumes/S18-1/ class=text-muted>Proceedings of The 12th International Workshop on Semantic Evaluation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S18-1031><div class="card-body p-3 small">We perform the LSTM and BiLSTM model for the emotion intensity prediction. We only join the third subtask in Task 1 : Affect in Tweets. Our system rank 6th among all the teams.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S18-1174.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S18-1174 data-toggle=collapse aria-expanded=false aria-controls=abstract-S18-1174 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/S18-1174/>YNU_Deep at SemEval-2018 Task 11 : An Ensemble of Attention-based BiLSTM Models for Machine Comprehension<span class=acl-fixed-case>YNU</span>_<span class=acl-fixed-case>D</span>eep at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2018 Task 11: An Ensemble of Attention-based <span class=acl-fixed-case>B</span>i<span class=acl-fixed-case>LSTM</span> Models for Machine Comprehension</a></strong><br><a href=/people/p/peng-ding/>Peng Ding</a>
|
<a href=/people/x/xiaobing-zhou/>Xiaobing Zhou</a><br><a href=/volumes/S18-1/ class=text-muted>Proceedings of The 12th International Workshop on Semantic Evaluation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S18-1174><div class="card-body p-3 small">We firstly use GloVe to learn the distributed representations automatically from the instance, question and answer triples. Then an attentionbased Bidirectional LSTM (BiLSTM) model is used to encode the triples. We also perform a simple <a href=https://en.wikipedia.org/wiki/Ensemble_learning>ensemble method</a> to improve the effectiveness of our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>. The <a href=https://en.wikipedia.org/wiki/System>system</a> we developed obtains an encouraging result on this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. It achieves the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> 0.7472 on the test set. We rank 5th according to the official ranking.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S18-1180.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S18-1180 data-toggle=collapse aria-expanded=false aria-controls=abstract-S18-1180 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/S18-1180/>Lyb3b at SemEval-2018 Task 11 : Machine Comprehension Task using Deep Learning Models<span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2018 Task 11: Machine Comprehension Task using Deep Learning Models</a></strong><br><a href=/people/y/yongbin-li/>Yongbin Li</a>
|
<a href=/people/x/xiaobing-zhou/>Xiaobing Zhou</a><br><a href=/volumes/S18-1/ class=text-muted>Proceedings of The 12th International Workshop on Semantic Evaluation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S18-1180><div class="card-body p-3 small">Machine Comprehension of text is a typical Natural Language Processing task which remains an elusive challenge. This paper is to solve the task 11 of SemEval-2018, <a href=https://en.wikipedia.org/wiki/Machine_learning>Machine Comprehension</a> using Commonsense Knowledge task. We use deep learning model to solve the problem. We build distributed word embedding of text, question and answering respectively instead of manually extracting features by linguistic tools. Meanwhile, we use a series of frameworks such as CNN model, LSTM model, LSTM with attention model and biLSTM with attention model for processing word vector. Experiments demonstrate the superior performance of biLSTM with attention framework compared to other <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>. We also delete high frequency words and combine <a href=https://en.wikipedia.org/wiki/Word_vector>word vector and data augmentation methods</a>, achieved a certain effect. The approach we proposed rank 6th in official results, with <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy rate</a> of 0.7437 in test dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S18-1189.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S18-1189 data-toggle=collapse aria-expanded=false aria-controls=abstract-S18-1189 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/S18-1189/>YNU Deep at SemEval-2018 Task 12 : A BiLSTM Model with Neural Attention for Argument Reasoning Comprehension<span class=acl-fixed-case>YNU</span> Deep at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2018 Task 12: A <span class=acl-fixed-case>B</span>i<span class=acl-fixed-case>LSTM</span> Model with Neural Attention for Argument Reasoning Comprehension</a></strong><br><a href=/people/p/peng-ding/>Peng Ding</a>
|
<a href=/people/x/xiaobing-zhou/>Xiaobing Zhou</a><br><a href=/volumes/S18-1/ class=text-muted>Proceedings of The 12th International Workshop on Semantic Evaluation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S18-1189><div class="card-body p-3 small">This paper describes the system submitted to SemEval-2018 Task 12 (The Argument Reasoning Comprehension Task). Enabling a computer to understand a text so that it can answer comprehension questions is still a challenging goal of <a href=https://en.wikipedia.org/wiki/Neuro-linguistic_programming>NLP</a>. We propose a Bidirectional LSTM (BiLSTM) model that reads two sentences separated by a delimiter to determine which warrant is correct. We extend this <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> with a neural attention mechanism that encourages the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> to make reasoning over the given claims and reasons. Officially released results show that our <a href=https://en.wikipedia.org/wiki/System>system</a> ranks 6th among 22 submissions to this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>.</div></div><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-4032.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-4032 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-4032 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-4032/>YNUDLG at IJCNLP-2017 Task 5 : A CNN-LSTM Model with Attention for Multi-choice Question Answering in Examinations<span class=acl-fixed-case>YNUDLG</span> at <span class=acl-fixed-case>IJCNLP</span>-2017 Task 5: A <span class=acl-fixed-case>CNN</span>-<span class=acl-fixed-case>LSTM</span> Model with Attention for Multi-choice Question Answering in Examinations</a></strong><br><a href=/people/m/min-wang/>Min Wang</a>
|
<a href=/people/q/qingxun-liu/>Qingxun Liu</a>
|
<a href=/people/p/peng-ding/>Peng Ding</a>
|
<a href=/people/y/yongbin-li/>Yongbin Li</a>
|
<a href=/people/x/xiaobing-zhou/>Xiaobing Zhou</a><br><a href=/volumes/I17-4/ class=text-muted>Proceedings of the IJCNLP 2017, Shared Tasks</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-4032><div class="card-body p-3 small">In this paper, we perform convolutional neural networks (CNN) to learn the joint representations of question-answer pairs first, then use the joint representations as the inputs of the long short-term memory (LSTM) with attention to learn the answer sequence of a question for labeling the matching quality of each answer. We also incorporating external knowledge by training Word2Vec on Flashcards data, thus we get more compact embedding. Experimental results show that our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> achieves better or comparable performance compared with the <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline system</a>. The proposed approach achieves the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of 0.39, 0.42 in English valid set, test set, respectively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S17-2119.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S17-2119 data-toggle=collapse aria-expanded=false aria-controls=abstract-S17-2119 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/S17-2119/>YNUDLG at SemEval-2017 Task 4 : A GRU-SVM Model for Sentiment Classification and Quantification in Twitter<span class=acl-fixed-case>YNUDLG</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2017 Task 4: A <span class=acl-fixed-case>GRU</span>-<span class=acl-fixed-case>SVM</span> Model for Sentiment Classification and Quantification in <span class=acl-fixed-case>T</span>witter</a></strong><br><a href=/people/m/ming-wang/>Ming Wang</a>
|
<a href=/people/b/biao-chu/>Biao Chu</a>
|
<a href=/people/q/qingxun-liu/>Qingxun Liu</a>
|
<a href=/people/x/xiaobing-zhou/>Xiaobing Zhou</a><br><a href=/volumes/S17-2/ class=text-muted>Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S17-2119><div class="card-body p-3 small">Sentiment analysis is one of the central issues in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>Natural Language Processing</a> and has become more and more important in many fields. Typical sentiment analysis classifies the sentiment of sentences into several discrete classes (e.g.,positive or negative). In this paper we describe our deep learning system(combining GRU and SVM) to solve both two-, three- and five-tweet polarity classifications. We first trained a gated recurrent neural network using pre-trained word embeddings, then we extracted <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> from GRU layer and input these <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> into support vector machine to fulfill both the classification and quantification subtasks. The proposed approach achieved 37th, 19th, and 14rd places in subtasks A, B and C, respectively.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Xiaobing+Zhou" title="Search for 'Xiaobing Zhou' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/p/peng-ding/ class=align-middle>Peng Ding</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/m/min-wang/ class=align-middle>Min Wang</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/q/qingxun-liu/ class=align-middle>Qingxun Liu</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/y/yongbin-li/ class=align-middle>Yongbin Li</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/x/xuejie-zhang/ class=align-middle>Xuejie Zhang</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/y/yueying-zhu/ class=align-middle>Yueying Zhu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/hongling-li/ class=align-middle>Hongling Li</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/kunjie-dong/ class=align-middle>Kunjie Dong</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/junyi-li/ class=align-middle>Junyi Li</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/zichen-zhang/ class=align-middle>Zichen Zhang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/ming-wang/ class=align-middle>Ming Wang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/b/biao-chu/ class=align-middle>Biao Chu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/b/bin-wang/ class=align-middle>Bin Wang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yunxia-ding/ class=align-middle>Yunxia Ding</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/renyuan-liu/ class=align-middle>Renyuan Liu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/semeval/ class=align-middle>SemEval</a><span class="badge badge-secondary align-middle ml-2">10</span></li><li class=list-group-item><a href=/venues/ijcnlp/ class=align-middle>IJCNLP</a><span class="badge badge-secondary align-middle ml-2">1</span></li></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>