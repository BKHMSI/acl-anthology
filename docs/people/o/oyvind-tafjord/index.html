<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Oyvind Tafjord - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Oyvind</span> <span class=font-weight-bold>Tafjord</span></h2><hr><div class=row><div class=col-lg-9><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.585.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--585 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.585 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.585/>Explaining Answers with Entailment Trees</a></strong><br><a href=/people/b/bhavana-dalvi/>Bhavana Dalvi</a>
|
<a href=/people/p/peter-jansen/>Peter Jansen</a>
|
<a href=/people/o/oyvind-tafjord/>Oyvind Tafjord</a>
|
<a href=/people/z/zhengnan-xie/>Zhengnan Xie</a>
|
<a href=/people/h/hannah-smith/>Hannah Smith</a>
|
<a href=/people/l/leighanna-pipatanangkura/>Leighanna Pipatanangkura</a>
|
<a href=/people/p/peter-clark/>Peter Clark</a><br><a href=/volumes/2021.emnlp-main/ class=text-muted>Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--585><div class="card-body p-3 small">Our goal, in the context of open-domain textual question-answering (QA), is to explain answers by showing the line of reasoning from what is known to the answer, rather than simply showing a fragment of textual evidence (a rationale). If this could be done, new opportunities for understanding and debugging the <a href=https://en.wikipedia.org/wiki/System>system</a>&#8217;s reasoning become possible. Our approach is to generate explanations in the form of entailment trees, namely a tree of multipremise entailment steps from facts that are known, through intermediate conclusions, to the hypothesis of interest (namely the question + answer). To train a model with this skill, we created ENTAILMENTBANK, the first dataset to contain multistep entailment trees. Given a hypothesis (question + answer), we define three increasingly difficult explanation tasks : generate a valid <a href=https://en.wikipedia.org/wiki/Logical_consequence>entailment tree</a> given (a) all relevant sentences (b) all relevant and some irrelevant sentences, or (c) a <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a>. We show that a strong <a href=https://en.wikipedia.org/wiki/Language_model>language model</a> can partially solve these <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a>, in particular when the relevant sentences are included in the input (e.g., 35 % of trees for (a) are perfect), and with indications of generalization to other domains. This work is significant as it provides a new type of <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> (multistep entailments) and baselines, offering a new avenue for the community to generate richer, more systematic explanations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.697.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--697 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.697 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.697/>BeliefBank : Adding Memory to a Pre-Trained Language Model for a Systematic Notion of Belief<span class=acl-fixed-case>B</span>elief<span class=acl-fixed-case>B</span>ank: Adding Memory to a Pre-Trained Language Model for a Systematic Notion of Belief</a></strong><br><a href=/people/n/nora-kassner/>Nora Kassner</a>
|
<a href=/people/o/oyvind-tafjord/>Oyvind Tafjord</a>
|
<a href=/people/h/hinrich-schutze/>Hinrich Schütze</a>
|
<a href=/people/p/peter-clark/>Peter Clark</a><br><a href=/volumes/2021.emnlp-main/ class=text-muted>Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--697><div class="card-body p-3 small">Although pretrained language models (PTLMs) contain significant amounts of <a href=https://en.wikipedia.org/wiki/World_knowledge>world knowledge</a>, they can still produce inconsistent answers to questions when probed, even after specialized training. As a result, it can be hard to identify what the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> actually believes about the world, making it susceptible to <a href=https://en.wikipedia.org/wiki/Consistency>inconsistent behavior</a> and simple errors. Our goal is to reduce these problems. Our approach is to embed a PTLM in a broader system that also includes an evolving, symbolic memory of beliefs a BeliefBank that records but then may modify the raw PTLM answers. We describe two <a href=https://en.wikipedia.org/wiki/Mechanism_(sociology)>mechanisms</a> to improve belief consistency in the overall system. First, a reasoning component a weighted MaxSAT solver revises beliefs that significantly clash with others. Second, a feedback component issues future queries to the PTLM using known beliefs as context. We show that, in a controlled experimental setting, these two mechanisms result in more consistent beliefs in the overall system, improving both the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> and consistency of its answers over time. This is significant as it is a first step towards PTLM-based architectures with a systematic notion of belief, enabling them to construct a more coherent picture of the world, and improve over time without model retraining.</div></div><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.556.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--556 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.556 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938640 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.556/>You are grounded ! : Latent Name Artifacts in Pre-trained Language Models</a></strong><br><a href=/people/v/vered-shwartz/>Vered Shwartz</a>
|
<a href=/people/r/rachel-rudinger/>Rachel Rudinger</a>
|
<a href=/people/o/oyvind-tafjord/>Oyvind Tafjord</a><br><a href=/volumes/2020.emnlp-main/ class=text-muted>Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--556><div class="card-body p-3 small">Pre-trained language models (LMs) may perpetuate biases originating in their training corpus to downstream models. We focus on artifacts associated with the representation of given names (e.g., Donald), which, depending on the corpus, may be associated with specific <a href=https://en.wikipedia.org/wiki/Non-physical_entity>entities</a>, as indicated by next token prediction (e.g., Trump). While helpful in some contexts, grounding happens also in under-specified or inappropriate contexts. For example, endings generated for &#8216;Donald is a&#8217; substantially differ from those of other names, and often have more-than-average negative sentiment. We demonstrate the potential effect on downstream tasks with reading comprehension probes where name perturbation changes the model answers. As a silver lining, our experiments suggest that additional <a href=https://en.wikipedia.org/wiki/Training>pre-training</a> on different corpora may mitigate this bias.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.acl-demos.41.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--acl-demos--41 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.acl-demos.41 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://slideslive.com/38928607 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.acl-demos.41" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.acl-demos.41/>SUPP.AI : finding evidence for supplement-drug interactions<span class=acl-fixed-case>SUPP</span>.<span class=acl-fixed-case>AI</span>: finding evidence for supplement-drug interactions</a></strong><br><a href=/people/l/lucy-wang/>Lucy Wang</a>
|
<a href=/people/o/oyvind-tafjord/>Oyvind Tafjord</a>
|
<a href=/people/a/arman-cohan/>Arman Cohan</a>
|
<a href=/people/s/sarthak-jain/>Sarthak Jain</a>
|
<a href=/people/s/sam-skjonsberg/>Sam Skjonsberg</a>
|
<a href=/people/c/carissa-schoenick/>Carissa Schoenick</a>
|
<a href=/people/n/nick-botner/>Nick Botner</a>
|
<a href=/people/w/waleed-ammar/>Waleed Ammar</a><br><a href=/volumes/2020.acl-demos/ class=text-muted>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--acl-demos--41><div class="card-body p-3 small">Dietary supplements are used by a large portion of the population, but information on their <a href=https://en.wikipedia.org/wiki/Drug_interaction>pharmacologic interactions</a> is incomplete. To address this challenge, we present SUPP.AI, an <a href=https://en.wikipedia.org/wiki/Application_software>application</a> for browsing evidence of supplement-drug interactions (SDIs) extracted from the <a href=https://en.wikipedia.org/wiki/Medical_literature>biomedical literature</a>. We train a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> to automatically extract <a href=https://en.wikipedia.org/wiki/Supplement_(publishing)>supplement information</a> and identify such <a href=https://en.wikipedia.org/wiki/Interaction_(statistics)>interactions</a> from the <a href=https://en.wikipedia.org/wiki/Scientific_literature>scientific literature</a>. To address the lack of labeled data for SDI identification, we use labels of the closely related task of identifying drug-drug interactions (DDIs) for supervision. We fine-tune the contextualized word representations of the RoBERTa language model using labeled DDI data, and apply the fine-tuned <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> to identify supplement interactions. We extract 195k evidence sentences from 22 M articles (P=0.82, R=0.58, F1=0.68) for 60k interactions. We create the SUPP.AI application for users to search evidence sentences extracted by our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>. SUPP.AI is an attempt to close the information gap on <a href=https://en.wikipedia.org/wiki/Dietary_supplement>dietary supplements</a> by making up-to-date evidence on SDIs more discoverable for researchers, clinicians, and consumers. An informational video on how to use SUPP.AI is available at : https://youtu.be/dR0ucKdORwc</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.171.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--171 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.171 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.findings-emnlp.171.OptionalSupplementaryMaterial.pdf data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.171" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.171/>UNIFIEDQA : Crossing Format Boundaries with a Single QA System<span class=acl-fixed-case>UNIFIEDQA</span>: Crossing Format Boundaries with a Single <span class=acl-fixed-case>QA</span> System</a></strong><br><a href=/people/d/daniel-khashabi/>Daniel Khashabi</a>
|
<a href=/people/s/sewon-min/>Sewon Min</a>
|
<a href=/people/t/tushar-khot/>Tushar Khot</a>
|
<a href=/people/a/ashish-sabharwal/>Ashish Sabharwal</a>
|
<a href=/people/o/oyvind-tafjord/>Oyvind Tafjord</a>
|
<a href=/people/p/peter-clark/>Peter Clark</a>
|
<a href=/people/h/hannaneh-hajishirzi/>Hannaneh Hajishirzi</a><br><a href=/volumes/2020.findings-emnlp/ class=text-muted>Findings of the Association for Computational Linguistics: EMNLP 2020</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--171><div class="card-body p-3 small">Question answering (QA) tasks have been posed using a variety of <a href=https://en.wikipedia.org/wiki/File_format>formats</a>, such as extractive span selection, <a href=https://en.wikipedia.org/wiki/Multiple_choice>multiple choice</a>, etc. This has led to format-specialized models, and even to an implicit division in the QA community. We argue that such boundaries are artificial and perhaps unnecessary, given the reasoning abilities we seek to teach are not governed by the format. As evidence, we use the latest advances in <a href=https://en.wikipedia.org/wiki/Language_model>language modeling</a> to build a single pre-trained QA model, UNIFIEDQA, that performs well across 19 QA datasets spanning 4 diverse formats. UNIFIEDQA performs on par with 8 different <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> that were trained on individual datasets themselves. Even when faced with 12 unseen datasets of observed formats, UNIFIEDQA performs surprisingly well, showing strong generalization from its outof-format training data. Finally, simply finetuning this pre trained QA model into specialized models results in a new state of the art on 10 factoid and commonsense question answering datasets, establishing UNIFIEDQA as a strong starting point for building QA systems.</div></div><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5808.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5808 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5808 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-5808.Attachment.tgz data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D19-5808/>Reasoning Over Paragraph Effects in Situations</a></strong><br><a href=/people/k/kevin-lin/>Kevin Lin</a>
|
<a href=/people/o/oyvind-tafjord/>Oyvind Tafjord</a>
|
<a href=/people/p/peter-clark/>Peter Clark</a>
|
<a href=/people/m/matt-gardner/>Matt Gardner</a><br><a href=/volumes/D19-58/ class=text-muted>Proceedings of the 2nd Workshop on Machine Reading for Question Answering</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5808><div class="card-body p-3 small">A key component of successfully reading a passage of text is the ability to apply knowledge gained from the passage to a new situation. In order to facilitate progress on this kind of reading, we present <a href=https://en.wikipedia.org/wiki/ROPES>ROPES</a>, a challenging <a href=https://en.wikipedia.org/wiki/Benchmarking>benchmark</a> for <a href=https://en.wikipedia.org/wiki/Reading_comprehension>reading comprehension</a> targeting Reasoning Over Paragraph Effects in Situations. We target expository language describing causes and effects (e.g., animal pollinators increase efficiency of fertilization in flowers), as they have clear implications for new situations. A <a href=https://en.wikipedia.org/wiki/System>system</a> is presented a background passage containing at least one of these relations, a novel situation that uses this <a href=https://en.wikipedia.org/wiki/Context_(language_use)>background</a>, and questions that require reasoning about effects of the relationships in the background passage in the context of the situation. We collect background passages from science textbooks and <a href=https://en.wikipedia.org/wiki/Wikipedia>Wikipedia</a> that contain such phenomena, and ask crowd workers to author situations, questions, and answers, resulting in a 14,322 question dataset. We analyze the challenges of this task and evaluate the performance of state-of-the-art reading comprehension models. The best <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> performs only slightly better than randomly guessing an answer of the correct type, at 61.6 % <a href=https://en.wikipedia.org/wiki/F-number>F1</a>, well below the human performance of 89.0 %.</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-2501.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-2501 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-2501 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-2501" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-2501/>AllenNLP : A Deep Semantic Natural Language Processing Platform<span class=acl-fixed-case>A</span>llen<span class=acl-fixed-case>NLP</span>: A Deep Semantic Natural Language Processing Platform</a></strong><br><a href=/people/m/matt-gardner/>Matt Gardner</a>
|
<a href=/people/j/joel-grus/>Joel Grus</a>
|
<a href=/people/m/mark-neumann/>Mark Neumann</a>
|
<a href=/people/o/oyvind-tafjord/>Oyvind Tafjord</a>
|
<a href=/people/p/pradeep-dasigi/>Pradeep Dasigi</a>
|
<a href=/people/n/nelson-f-liu/>Nelson F. Liu</a>
|
<a href=/people/m/matthew-e-peters/>Matthew Peters</a>
|
<a href=/people/m/michael-schmitz/>Michael Schmitz</a>
|
<a href=/people/l/luke-zettlemoyer/>Luke Zettlemoyer</a><br><a href=/volumes/W18-25/ class=text-muted>Proceedings of Workshop for NLP Open Source Software (NLP-OSS)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-2501><div class="card-body p-3 small">Modern natural language processing (NLP) research requires writing code. Ideally this <a href=https://en.wikipedia.org/wiki/Source_code>code</a> would provide a precise definition of the approach, easy repeatability of results, and a basis for extending the <a href=https://en.wikipedia.org/wiki/Research>research</a>. However, many research codebases bury high-level parameters under implementation details, are challenging to run and debug, and are difficult enough to extend that they are more likely to be rewritten. This paper describes AllenNLP, a library for applying deep learning methods to NLP research that addresses these issues with easy-to-use command-line tools, declarative configuration-driven experiments, and modular NLP abstractions. AllenNLP has already increased the rate of research experimentation and the sharing of NLP components at the Allen Institute for Artificial Intelligence, and we are working to have the same impact across the field.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Oyvind+Tafjord" title="Search for 'Oyvind Tafjord' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/p/peter-clark/ class=align-middle>Peter Clark</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/people/m/matt-gardner/ class=align-middle>Matt Gardner</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/v/vered-shwartz/ class=align-middle>Vered Shwartz</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/rachel-rudinger/ class=align-middle>Rachel Rudinger</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/lucy-wang/ class=align-middle>Lucy Wang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/a/arman-cohan/ class=align-middle>Arman Cohan</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/sarthak-jain/ class=align-middle>Sarthak Jain</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/sam-skjonsberg/ class=align-middle>Sam Skjonsberg</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/carissa-schoenick/ class=align-middle>Carissa Schoenick</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/n/nick-botner/ class=align-middle>Nick Botner</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/w/waleed-ammar/ class=align-middle>Waleed Ammar</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/b/bhavana-dalvi/ class=align-middle>Bhavana Dalvi</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/p/peter-jansen/ class=align-middle>Peter Jansen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/zhengnan-xie/ class=align-middle>Zhengnan Xie</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/hannah-smith/ class=align-middle>Hannah Smith</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/leighanna-pipatanangkura/ class=align-middle>Leighanna Pipatanangkura</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/n/nora-kassner/ class=align-middle>Nora Kassner</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/hinrich-schutze/ class=align-middle>Hinrich Schütze</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/kevin-lin/ class=align-middle>Kevin Lin</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/daniel-khashabi/ class=align-middle>Daniel Khashabi</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/sewon-min/ class=align-middle>Sewon Min</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/tushar-khot/ class=align-middle>Tushar Khot</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/ashish-sabharwal/ class=align-middle>Ashish Sabharwal</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/hannaneh-hajishirzi/ class=align-middle>Hannaneh Hajishirzi</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/joel-grus/ class=align-middle>Joel Grus</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/mark-neumann/ class=align-middle>Mark Neumann</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/p/pradeep-dasigi/ class=align-middle>Pradeep Dasigi</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/n/nelson-f-liu/ class=align-middle>Nelson F. Liu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/matthew-e-peters/ class=align-middle>Matthew E. Peters</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/michael-schmitz/ class=align-middle>Michael Schmitz</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/luke-zettlemoyer/ class=align-middle>Luke Zettlemoyer</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/emnlp/ class=align-middle>EMNLP</a><span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/venues/acl/ class=align-middle>ACL</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/findings/ class=align-middle>Findings</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/ws/ class=align-middle>WS</a><span class="badge badge-secondary align-middle ml-2">1</span></li></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>