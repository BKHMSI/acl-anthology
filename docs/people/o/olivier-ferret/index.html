<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Olivier Ferret - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Olivier</span> <span class=font-weight-bold>Ferret</span></h2><hr><div class=row><div class=col-lg-9><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.609.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--609 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.609 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.coling-main.609" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.609/>CharacterBERT : Reconciling ELMo and BERT for Word-Level Open-Vocabulary Representations From Characters<span class=acl-fixed-case>C</span>haracter<span class=acl-fixed-case>BERT</span>: Reconciling <span class=acl-fixed-case>ELM</span>o and <span class=acl-fixed-case>BERT</span> for Word-Level Open-Vocabulary Representations From Characters</a></strong><br><a href=/people/h/hicham-el-boukkouri/>Hicham El Boukkouri</a>
|
<a href=/people/o/olivier-ferret/>Olivier Ferret</a>
|
<a href=/people/t/thomas-lavergne/>Thomas Lavergne</a>
|
<a href=/people/h/hiroshi-noji/>Hiroshi Noji</a>
|
<a href=/people/p/pierre-zweigenbaum/>Pierre Zweigenbaum</a>
|
<a href=/people/j/junichi-tsujii/>Jun’ichi Tsujii</a><br><a href=/volumes/2020.coling-main/ class=text-muted>Proceedings of the 28th International Conference on Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--609><div class="card-body p-3 small">Due to the compelling improvements brought by BERT, many recent representation models adopted the Transformer architecture as their main building block, consequently inheriting the wordpiece tokenization system despite it not being intrinsically linked to the notion of <a href=https://en.wikipedia.org/wiki/Transformers_(toy_line)>Transformers</a>. While this system is thought to achieve a good balance between the flexibility of characters and the efficiency of full words, using predefined wordpiece vocabularies from the <a href=https://en.wikipedia.org/wiki/Domain_(software_engineering)>general domain</a> is not always suitable, especially when building <a href=https://en.wikipedia.org/wiki/Conceptual_model_(computer_science)>models</a> for <a href=https://en.wikipedia.org/wiki/Domain_(software_engineering)>specialized domains</a> (e.g., the medical domain). Moreover, adopting a wordpiece tokenization shifts the focus from the word level to the subword level, making the <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> conceptually more complex and arguably less convenient in practice. For these reasons, we propose CharacterBERT, a new variant of BERT that drops the wordpiece system altogether and uses a Character-CNN module instead to represent entire words by consulting their characters. We show that this new model improves the performance of BERT on a variety of medical domain tasks while at the same time producing robust, word-level, and open-vocabulary representations.</div></div><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-2041.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-2041 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-2041 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-2041/>Embedding Strategies for Specialized Domains : Application to Clinical Entity Recognition</a></strong><br><a href=/people/h/hicham-el-boukkouri/>Hicham El Boukkouri</a>
|
<a href=/people/o/olivier-ferret/>Olivier Ferret</a>
|
<a href=/people/t/thomas-lavergne/>Thomas Lavergne</a>
|
<a href=/people/p/pierre-zweigenbaum/>Pierre Zweigenbaum</a><br><a href=/volumes/P19-2/ class=text-muted>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-2041><div class="card-body p-3 small">Using pre-trained word embeddings in conjunction with <a href=https://en.wikipedia.org/wiki/Deep_learning>Deep Learning models</a> has become the de facto approach in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>Natural Language Processing (NLP)</a>. While this usually yields satisfactory results, off-the-shelf <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> tend to perform poorly on texts from specialized domains such as clinical reports. Moreover, training specialized word representations from scratch is often either impossible or ineffective due to the lack of large enough in-domain data. In this work, we focus on the clinical domain for which we study embedding strategies that rely on general-domain resources only. We show that by combining off-the-shelf contextual embeddings (ELMo) with static word2vec embeddings trained on a small in-domain corpus built from the task data, we manage to reach and sometimes outperform representations learned from a large corpus in the medical domain.</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5622.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5622 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5622 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-5622" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-5622/>Evaluation of a Sequence Tagging Tool for Biomedical Texts</a></strong><br><a href=/people/j/julien-tourille/>Julien Tourille</a>
|
<a href=/people/m/matthieu-doutreligne/>Matthieu Doutreligne</a>
|
<a href=/people/o/olivier-ferret/>Olivier Ferret</a>
|
<a href=/people/a/aurelie-neveol/>Aurélie Névéol</a>
|
<a href=/people/n/nicolas-paris/>Nicolas Paris</a>
|
<a href=/people/x/xavier-tannier/>Xavier Tannier</a><br><a href=/volumes/W18-56/ class=text-muted>Proceedings of the Ninth International Workshop on Health Text Mining and Information Analysis</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5622><div class="card-body p-3 small">Many applications in biomedical natural language processing rely on sequence tagging as an initial step to perform more complex analysis. To support text analysis in the biomedical domain, we introduce Yet Another SEquence Tagger (YASET), an open-source multi purpose sequence tagger that implements state-of-the-art deep learning algorithms for sequence tagging. Herein, we evaluate YASET on <a href=https://en.wikipedia.org/wiki/Part-of-speech_tagging>part-of-speech tagging</a> and <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>named entity recognition</a> in a variety of text genres including articles from the biomedical literature in <a href=https://en.wikipedia.org/wiki/English_language>English</a> and clinical narratives in <a href=https://en.wikipedia.org/wiki/French_language>French</a>. To further characterize performance, we report distributions over 30 runs and different sizes of training datasets. YASET provides state-of-the-art performance on the CoNLL 2003 NER dataset (F1=0.87), MEDPOST corpus (F1=0.97), MERLoT corpus (F1=0.99) and NCBI disease corpus (F1=0.81). We believe that YASET is a versatile and efficient tool that can be used for sequence tagging in biomedical and clinical texts.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2056.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2056 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2056 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2056.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285803915 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-2056/>Using pseudo-senses for improving the extraction of synonyms from <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a></a></strong><br><a href=/people/o/olivier-ferret/>Olivier Ferret</a><br><a href=/volumes/P18-2/ class=text-muted>Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2056><div class="card-body p-3 small">The <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> proposed recently for specializing word embeddings according to a particular perspective generally rely on <a href=https://en.wikipedia.org/wiki/Knowledge>external knowledge</a>. In this article, we propose Pseudofit, a new method for specializing word embeddings according to <a href=https://en.wikipedia.org/wiki/Semantic_similarity>semantic similarity</a> without any external knowledge. Pseudofit exploits the notion of pseudo-sense for building several <a href=https://en.wikipedia.org/wiki/Representation_(mathematics)>representations</a> for each word and uses these <a href=https://en.wikipedia.org/wiki/Representation_(mathematics)>representations</a> for making the initial embeddings more generic. We illustrate the interest of Pseudofit for acquiring synonyms and study several variants of Pseudofit according to this perspective.</div></div><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1028.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1028 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1028 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-1028/>Turning Distributional Thesauri into Word Vectors for Synonym Extraction and Expansion</a></strong><br><a href=/people/o/olivier-ferret/>Olivier Ferret</a><br><a href=/volumes/I17-1/ class=text-muted>Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1028><div class="card-body p-3 small">In this article, we propose to investigate a new <a href=https://en.wikipedia.org/wiki/Problem_solving>problem</a> consisting in turning a distributional thesaurus into dense word vectors. We propose more precisely a method for performing such task by associating <a href=https://en.wikipedia.org/wiki/Graph_embedding>graph embedding</a> and distributed representation adaptation. We have applied and evaluated it for <a href=https://en.wikipedia.org/wiki/English_nouns>English nouns</a> at a large scale about its ability to retrieve <a href=https://en.wikipedia.org/wiki/Synonym>synonyms</a>. In this context, we have also illustrated the interest of the developed method for three different tasks : the improvement of already existing <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a>, the fusion of heterogeneous representations and the expansion of synsets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-2035.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-2035 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-2035 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-2035/>Taking into account Inter-sentence Similarity for Update Summarization</a></strong><br><a href=/people/m/maali-mnasri/>Maâli Mnasri</a>
|
<a href=/people/g/gael-de-chalendar/>Gaël de Chalendar</a>
|
<a href=/people/o/olivier-ferret/>Olivier Ferret</a><br><a href=/volumes/I17-2/ class=text-muted>Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-2035><div class="card-body p-3 small">Following Gillick and Favre (2009), a lot of work about extractive summarization has modeled this task by associating two contrary constraints : one aims at maximizing the coverage of the summary with respect to its information content while the other represents its size limit. In this context, the notion of <a href=https://en.wikipedia.org/wiki/Redundancy_(linguistics)>redundancy</a> is only implicitly taken into account. In this article, we extend the framework defined by Gillick and Favre (2009) by examining how and to what extent integrating semantic sentence similarity into an update summarization system can improve its results. We show more precisely the impact of this <a href=https://en.wikipedia.org/wiki/Strategy>strategy</a> through evaluations performed on DUC 2007 and TAC 2008 and 2009 datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2035.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2035 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2035 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-2035/>Neural Architecture for Temporal Relation Extraction : A Bi-LSTM Approach for Detecting Narrative Containers<span class=acl-fixed-case>B</span>i-<span class=acl-fixed-case>LSTM</span> Approach for Detecting Narrative Containers</a></strong><br><a href=/people/j/julien-tourille/>Julien Tourille</a>
|
<a href=/people/o/olivier-ferret/>Olivier Ferret</a>
|
<a href=/people/a/aurelie-neveol/>Aurélie Névéol</a>
|
<a href=/people/x/xavier-tannier/>Xavier Tannier</a><br><a href=/volumes/P17-2/ class=text-muted>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2035><div class="card-body p-3 small">We present a neural architecture for containment relation identification between medical events and/or temporal expressions. We experiment on a corpus of de-identified clinical notes in English from the Mayo Clinic, namely the THYME corpus. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieves an <a href=https://en.wikipedia.org/wiki/F-measure>F-measure</a> of 0.613 and outperforms the best result reported on this <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> to date.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4211.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4211 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4211 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4211/>Unsupervised Event Clustering and Aggregation from Newswire and Web Articles</a></strong><br><a href=/people/s/swen-ribeiro/>Swen Ribeiro</a>
|
<a href=/people/o/olivier-ferret/>Olivier Ferret</a>
|
<a href=/people/x/xavier-tannier/>Xavier Tannier</a><br><a href=/volumes/W17-42/ class=text-muted>Proceedings of the 2017 EMNLP Workshop: Natural Language Processing meets Journalism</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4211><div class="card-body p-3 small">In this paper, we present an unsupervised pipeline approach for clustering <a href=https://en.wikipedia.org/wiki/Article_(publishing)>news articles</a> based on identified event instances in their content. We leverage press agency newswire and monolingual word alignment techniques to build meaningful and linguistically varied clusters of articles from the web in the perspective of a broader event type detection task. We validate our approach on a manually annotated corpus of Web articles.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S17-2098.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S17-2098 data-toggle=collapse aria-expanded=false aria-controls=abstract-S17-2098 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/S17-2098/>LIMSI-COT at SemEval-2017 Task 12 : Neural Architecture for Temporal Information Extraction from Clinical Narratives<span class=acl-fixed-case>LIMSI</span>-<span class=acl-fixed-case>COT</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2017 Task 12: Neural Architecture for Temporal Information Extraction from Clinical Narratives</a></strong><br><a href=/people/j/julien-tourille/>Julien Tourille</a>
|
<a href=/people/o/olivier-ferret/>Olivier Ferret</a>
|
<a href=/people/x/xavier-tannier/>Xavier Tannier</a>
|
<a href=/people/a/aurelie-neveol/>Aurélie Névéol</a><br><a href=/volumes/S17-2/ class=text-muted>Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S17-2098><div class="card-body p-3 small">In this paper we present our participation to SemEval 2017 Task 12. We used a neural network based approach for entity and temporal relation extraction, and experimented with two domain adaptation strategies. We achieved competitive performance for both <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2117.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2117 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2117 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2117/>Temporal information extraction from clinical text</a></strong><br><a href=/people/j/julien-tourille/>Julien Tourille</a>
|
<a href=/people/o/olivier-ferret/>Olivier Ferret</a>
|
<a href=/people/x/xavier-tannier/>Xavier Tannier</a>
|
<a href=/people/a/aurelie-neveol/>Aurélie Névéol</a><br><a href=/volumes/E17-2/ class=text-muted>Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2117><div class="card-body p-3 small">In this paper, we present a method for temporal relation extraction from clinical narratives in <a href=https://en.wikipedia.org/wiki/French_language>French</a> and in <a href=https://en.wikipedia.org/wiki/English_language>English</a>. We experiment on two comparable corpora, the MERLOT corpus and the THYME corpus, and show that a common approach can be used for both languages.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Olivier+Ferret" title="Search for 'Olivier Ferret' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/x/xavier-tannier/ class=align-middle>Xavier Tannier</a>
<span class="badge badge-secondary align-middle ml-2">5</span></li><li class=list-group-item><a href=/people/j/julien-tourille/ class=align-middle>Julien Tourille</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/people/a/aurelie-neveol/ class=align-middle>Aurelie Neveol</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/people/h/hicham-el-boukkouri/ class=align-middle>Hicham El Boukkouri</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/t/thomas-lavergne/ class=align-middle>Thomas Lavergne</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/p/pierre-zweigenbaum/ class=align-middle>Pierre Zweigenbaum</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/m/maali-mnasri/ class=align-middle>Maâli Mnasri</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/g/gael-de-chalendar/ class=align-middle>Gaël de Chalendar</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/swen-ribeiro/ class=align-middle>Swen Ribeiro</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/matthieu-doutreligne/ class=align-middle>Matthieu Doutreligne</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/n/nicolas-paris/ class=align-middle>Nicolas Paris</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/hiroshi-noji/ class=align-middle>Hiroshi Noji</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/junichi-tsujii/ class=align-middle>Jun’ichi Tsujii</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/acl/ class=align-middle>ACL</a><span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/venues/ijcnlp/ class=align-middle>IJCNLP</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/ws/ class=align-middle>WS</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/semeval/ class=align-middle>SemEval</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/coling/ class=align-middle>COLING</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-venues aria-expanded=false aria-controls=more-venues>show all...</li><div class="collapse border-top" id=more-venues><li class=list-group-item><a href=/venues/eacl/ class=align-middle>EACL</a><span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>