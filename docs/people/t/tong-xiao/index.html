<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Tong Xiao - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Tong</span> <span class=font-weight-bold>Xiao</span></h2><hr><div class=row><div class=col-lg-9><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.162.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--162 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.162 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.162/>Weight Distillation : Transferring the Knowledge in Neural Network Parameters</a></strong><br><a href=/people/y/ye-lin/>Ye Lin</a>
|
<a href=/people/y/yanyang-li/>Yanyang Li</a>
|
<a href=/people/z/ziyang-wang/>Ziyang Wang</a>
|
<a href=/people/b/bei-li/>Bei Li</a>
|
<a href=/people/q/quan-du/>Quan Du</a>
|
<a href=/people/t/tong-xiao/>Tong Xiao</a>
|
<a href=/people/j/jingbo-zhu/>Jingbo Zhu</a><br><a href=/volumes/2021.acl-long/ class=text-muted>Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--162><div class="card-body p-3 small">Knowledge distillation has been proven to be effective in model acceleration and compression. It transfers knowledge from a large neural network to a small one by using the large neural network predictions as targets of the small neural network. But this way ignores the knowledge inside the <a href=https://en.wikipedia.org/wiki/Neural_network>large neural networks</a>, e.g., <a href=https://en.wikipedia.org/wiki/Parameter_(computer_programming)>parameters</a>. Our preliminary study as well as the recent success in <a href=https://en.wikipedia.org/wiki/Training>pre-training</a> suggests that transferring parameters are more effective in distilling knowledge. In this paper, we propose Weight Distillation to transfer the knowledge in parameters of a large neural network to a small neural network through a parameter generator. On the WMT16 En-Ro, NIST12 Zh-En, and WMT14 En-De machine translation tasks, our experiments show that weight distillation learns a small network that is 1.88 2.94x faster than the large network but with competitive BLEU performance. When fixing the size of small networks, weight distillation outperforms knowledge distillation by 0.51 1.82 BLEU points.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.204.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--204 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.204 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.204/>Stacked Acoustic-and-Textual Encoding : Integrating the Pre-trained Models into Speech Translation Encoders</a></strong><br><a href=/people/c/chen-xu/>Chen Xu</a>
|
<a href=/people/b/bojie-hu/>Bojie Hu</a>
|
<a href=/people/y/yanyang-li/>Yanyang Li</a>
|
<a href=/people/y/yuhao-zhang/>Yuhao Zhang</a>
|
<a href=/people/s/shen-huang/>Shen Huang</a>
|
<a href=/people/q/qi-ju/>Qi Ju</a>
|
<a href=/people/t/tong-xiao/>Tong Xiao</a>
|
<a href=/people/j/jingbo-zhu/>Jingbo Zhu</a><br><a href=/volumes/2021.acl-long/ class=text-muted>Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--204><div class="card-body p-3 small">Encoder pre-training is promising in end-to-end Speech Translation (ST), given the fact that speech-to-translation data is scarce. But ST encoders are not simple instances of Automatic Speech Recognition (ASR) or Machine Translation (MT) encoders. For example, we find that ASR encoders lack the global context representation, which is necessary for translation, whereas MT encoders are not designed to deal with long but locally attentive acoustic sequences. In this work, we propose a Stacked Acoustic-and-Textual Encoding (SATE) method for <a href=https://en.wikipedia.org/wiki/Speech_translation>speech translation</a>. Our <a href=https://en.wikipedia.org/wiki/Encoder>encoder</a> begins with processing the acoustic sequence as usual, but later behaves more like an MT encoder for a global representation of the input sequence. In this way, it is straightforward to incorporate the pre-trained models into the <a href=https://en.wikipedia.org/wiki/System>system</a>. Also, we develop an adaptor module to alleviate the representation inconsistency between the pre-trained ASR encoder and MT encoder, and develop a multi-teacher knowledge distillation method to preserve the pre-training knowledge. Experimental results on the LibriSpeech En-Fr and MuST-C En-De ST tasks show that our <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>method</a> achieves state-of-the-art <a href=https://en.wikipedia.org/wiki/Basic_Linear_Algebra_Subprograms>BLEU scores</a> of 18.3 and 25.2. To our knowledge, we are the first to develop an end-to-end ST system that achieves comparable or even better BLEU performance than the cascaded ST counterpart when large-scale ASR and MT data is available.</div></div><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.385.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--385 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.385 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.findings-emnlp.385.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.385/>Training Flexible Depth Model by Multi-Task Learning for Neural Machine Translation</a></strong><br><a href=/people/q/qiang-wang/>Qiang Wang</a>
|
<a href=/people/t/tong-xiao/>Tong Xiao</a>
|
<a href=/people/j/jingbo-zhu/>Jingbo Zhu</a><br><a href=/volumes/2020.findings-emnlp/ class=text-muted>Findings of the Association for Computational Linguistics: EMNLP 2020</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--385><div class="card-body p-3 small">The standard neural machine translation model can only decode with the same depth configuration as training. Restricted by this feature, we have to deploy models of various sizes to maintain the same translation latency, because the hardware conditions on different terminal devices (e.g., mobile phones) may vary greatly. Such individual training leads to increased model maintenance costs and slower model iterations, especially for the industry. In this work, we propose to use <a href=https://en.wikipedia.org/wiki/Multi-task_learning>multi-task learning</a> to train a flexible depth model that can adapt to different depth configurations during <a href=https://en.wikipedia.org/wiki/Inference>inference</a>. Experimental results show that our approach can simultaneously support decoding in 24 depth configurations and is superior to the individual training and another flexible depth model training methodLayerDrop.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.352.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--352 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.352 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.352/>Dynamic Curriculum Learning for Low-Resource Neural Machine Translation</a></strong><br><a href=/people/c/chen-xu/>Chen Xu</a>
|
<a href=/people/b/bojie-hu/>Bojie Hu</a>
|
<a href=/people/y/yufan-jiang/>Yufan Jiang</a>
|
<a href=/people/k/kai-feng/>Kai Feng</a>
|
<a href=/people/z/zeyang-wang/>Zeyang Wang</a>
|
<a href=/people/s/shen-huang/>Shen Huang</a>
|
<a href=/people/q/qi-ju/>Qi Ju</a>
|
<a href=/people/t/tong-xiao/>Tong Xiao</a>
|
<a href=/people/j/jingbo-zhu/>Jingbo Zhu</a><br><a href=/volumes/2020.coling-main/ class=text-muted>Proceedings of the 28th International Conference on Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--352><div class="card-body p-3 small">Large amounts of data has made neural machine translation (NMT) a big success in recent years. But it is still a challenge if we train these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> on small-scale corpora. In this case, the way of using data appears to be more important. Here, we investigate the effective use of <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training data</a> for low-resource NMT. In particular, we propose a dynamic curriculum learning (DCL) method to reorder training samples in training. Unlike previous work, we do not use a static scoring function for <a href=https://en.wikipedia.org/wiki/Order_of_operations>reordering</a>. Instead, the order of training samples is dynamically determined in two ways-loss decline and model competence. This eases training by highlighting easy samples that the current <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> has enough competence to learn. We test our DCL method in a Transformer-based system. Experimental results show that DCL outperforms several strong baselines on three low-resource machine translation benchmarks and different sized data of WMT&#8217;16 En-De.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.526.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--526 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.526 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.526/>A Simple and Effective Approach to Robust Unsupervised Bilingual Dictionary Induction</a></strong><br><a href=/people/y/yanyang-li/>Yanyang Li</a>
|
<a href=/people/y/yingfeng-luo/>Yingfeng Luo</a>
|
<a href=/people/y/ye-lin/>Ye Lin</a>
|
<a href=/people/q/quan-du/>Quan Du</a>
|
<a href=/people/h/huizhen-wang/>Huizhen Wang</a>
|
<a href=/people/s/shujian-huang/>Shujian Huang</a>
|
<a href=/people/t/tong-xiao/>Tong Xiao</a>
|
<a href=/people/j/jingbo-zhu/>Jingbo Zhu</a><br><a href=/volumes/2020.coling-main/ class=text-muted>Proceedings of the 28th International Conference on Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--526><div class="card-body p-3 small">Unsupervised Bilingual Dictionary Induction methods based on the initialization and the <a href=https://en.wikipedia.org/wiki/Autodidacticism>self-learning</a> have achieved great success in similar language pairs, e.g., <a href=https://en.wikipedia.org/wiki/English_as_a_second_or_foreign_language>English-Spanish</a>. But they still fail and have an <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of 0 % in many distant language pairs, e.g., English-Japanese. In this work, we show that this failure results from the gap between the actual initialization performance and the minimum initialization performance for the self-learning to succeed. We propose Iterative Dimension Reduction to bridge this gap. Our experiments show that this simple method does not hamper the performance of <a href=https://en.wikipedia.org/wiki/Similarity_measure>similar language pairs</a> and achieves an <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of 13.64 55.53 % between <a href=https://en.wikipedia.org/wiki/English_language>English</a> and four distant languages, i.e., <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a>, <a href=https://en.wikipedia.org/wiki/Japanese_language>Japanese</a>, <a href=https://en.wikipedia.org/wiki/Vietnamese_language>Vietnamese</a> and <a href=https://en.wikipedia.org/wiki/Thai_language>Thai</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wmt-1.37.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wmt-1--37 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wmt-1.37 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939572 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.wmt-1.37/>The NiuTrans Machine Translation Systems for WMT20<span class=acl-fixed-case>N</span>iu<span class=acl-fixed-case>T</span>rans Machine Translation Systems for <span class=acl-fixed-case>WMT</span>20</a></strong><br><a href=/people/y/yuhao-zhang/>Yuhao Zhang</a>
|
<a href=/people/z/ziyang-wang/>Ziyang Wang</a>
|
<a href=/people/r/runzhe-cao/>Runzhe Cao</a>
|
<a href=/people/b/binghao-wei/>Binghao Wei</a>
|
<a href=/people/w/weiqiao-shan/>Weiqiao Shan</a>
|
<a href=/people/s/shuhan-zhou/>Shuhan Zhou</a>
|
<a href=/people/a/abudurexiti-reheman/>Abudurexiti Reheman</a>
|
<a href=/people/t/tao-zhou/>Tao Zhou</a>
|
<a href=/people/x/xin-zeng/>Xin Zeng</a>
|
<a href=/people/l/laohu-wang/>Laohu Wang</a>
|
<a href=/people/y/yongyu-mu/>Yongyu Mu</a>
|
<a href=/people/j/jingnan-zhang/>Jingnan Zhang</a>
|
<a href=/people/x/xiaoqian-liu/>Xiaoqian Liu</a>
|
<a href=/people/x/xuanjun-zhou/>Xuanjun Zhou</a>
|
<a href=/people/y/yinqiao-li/>Yinqiao Li</a>
|
<a href=/people/b/bei-li/>Bei Li</a>
|
<a href=/people/t/tong-xiao/>Tong Xiao</a>
|
<a href=/people/j/jingbo-zhu/>Jingbo Zhu</a><br><a href=/volumes/2020.wmt-1/ class=text-muted>Proceedings of the Fifth Conference on Machine Translation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wmt-1--37><div class="card-body p-3 small">This paper describes NiuTrans neural machine translation systems of the WMT20 news translation tasks. We participated in Japanese-English, English-Chinese, Inuktitut-English and Tamil-English total five tasks and rank first in Japanese-English both sides. We mainly utilized iterative back-translation, different depth and widen model architectures, iterative knowledge distillation and iterative fine-tuning. And we find that adequately widened and deepened the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> simultaneously, the performance will significantly improve. Also, iterative fine-tuning strategy we implemented is effective during adapting domain. For Inuktitut-English and Tamil-English tasks, we built multilingual models separately and employed pretraining word embedding to obtain better performance.</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1255.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1255 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1255 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1255/>Multi-layer Representation Fusion for Neural Machine Translation</a></strong><br><a href=/people/q/qiang-wang/>Qiang Wang</a>
|
<a href=/people/f/fuxue-li/>Fuxue Li</a>
|
<a href=/people/t/tong-xiao/>Tong Xiao</a>
|
<a href=/people/y/yanyang-li/>Yanyang Li</a>
|
<a href=/people/y/yinqiao-li/>Yinqiao Li</a>
|
<a href=/people/j/jingbo-zhu/>Jingbo Zhu</a><br><a href=/volumes/C18-1/ class=text-muted>Proceedings of the 27th International Conference on Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1255><div class="card-body p-3 small">Neural machine translation systems require a number of stacked layers for <a href=https://en.wikipedia.org/wiki/Deep_learning>deep models</a>. But the prediction depends on the sentence representation of the top-most layer with no access to low-level representations. This makes it more difficult to train the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> and poses a risk of <a href=https://en.wikipedia.org/wiki/Information_loss>information loss</a> to prediction. In this paper, we propose a multi-layer representation fusion (MLRF) approach to fusing stacked layers. In particular, we design three <a href=https://en.wikipedia.org/wiki/Function_(mathematics)>fusion functions</a> to learn a better representation from the <a href=https://en.wikipedia.org/wiki/Call_stack>stack</a>. Experimental results show that our approach yields improvements of 0.92 and 0.56 BLEU points over the strong Transformer baseline on IWSLT German-English and NIST Chinese-English MT tasks respectively. The result is new state-of-the-art in German-English translation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2047.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2047 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2047 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2047.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2047.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-2047/>A Simple and Effective Approach to Coverage-Aware Neural Machine Translation</a></strong><br><a href=/people/y/yanyang-li/>Yanyang Li</a>
|
<a href=/people/t/tong-xiao/>Tong Xiao</a>
|
<a href=/people/y/yinqiao-li/>Yinqiao Li</a>
|
<a href=/people/q/qiang-wang/>Qiang Wang</a>
|
<a href=/people/c/changming-xu/>Changming Xu</a>
|
<a href=/people/j/jingbo-zhu/>Jingbo Zhu</a><br><a href=/volumes/P18-2/ class=text-muted>Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2047><div class="card-body p-3 small">We offer a simple and effective method to seek a better balance between model confidence and length preference for Neural Machine Translation (NMT). Unlike the popular length normalization and coverage models, our <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> does not require training nor reranking the limited n-best outputs. Moreover, it is robust to large beam sizes, which is not well studied in previous work. On the Chinese-English and English-German translation tasks, our <a href=https://en.wikipedia.org/wiki/Numerical_methods_for_ordinary_differential_equations>approach</a> yields +0.4 1.5 <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a> improvements over the state-of-the-art baselines.</div></div><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1052.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1052 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1052 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-1052/>Implicit Syntactic Features for Target-dependent Sentiment Analysis</a></strong><br><a href=/people/y/yuze-gao/>Yuze Gao</a>
|
<a href=/people/y/yue-zhang/>Yue Zhang</a>
|
<a href=/people/t/tong-xiao/>Tong Xiao</a><br><a href=/volumes/I17-1/ class=text-muted>Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1052><div class="card-body p-3 small">Targeted sentiment analysis investigates the sentiment polarities on given target mentions from input texts. Different from <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>sentence level sentiment</a>, it offers more fine-grained knowledge on each entity mention. While early work leveraged syntactic information, recent research has used neural representation learning to induce features automatically, thereby avoiding error propagation of syntactic parsers, which are particularly severe on social media texts. We study a method to leverage syntactic information without explicitly building the parser outputs, by training an encoder-decoder structure parser model on standard syntactic treebanks, and then leveraging its hidden encoder layers when analysing tweets. Such hidden vectors do not contain explicit syntactic outputs, yet encode rich syntactic features. We use them to augment the inputs to a baseline state-of-the-art targeted sentiment classifier, observing significant improvements on various <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmark datasets</a>. We obtain the best accuracies on all test sets.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Tong+Xiao" title="Search for 'Tong Xiao' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/j/jingbo-zhu/ class=align-middle>Jingbo Zhu</a>
<span class="badge badge-secondary align-middle ml-2">8</span></li><li class=list-group-item><a href=/people/y/yanyang-li/ class=align-middle>Yanyang Li</a>
<span class="badge badge-secondary align-middle ml-2">5</span></li><li class=list-group-item><a href=/people/q/qiang-wang/ class=align-middle>Qiang Wang</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/y/yinqiao-li/ class=align-middle>Yinqiao Li</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/y/ye-lin/ class=align-middle>Ye Lin</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/z/ziyang-wang/ class=align-middle>Ziyang Wang</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/b/bei-li/ class=align-middle>Bei Li</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/q/quan-du/ class=align-middle>Quan Du</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/c/chen-xu/ class=align-middle>Chen Xu</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/b/bojie-hu/ class=align-middle>Bojie Hu</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/y/yuhao-zhang/ class=align-middle>Yuhao Zhang</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/s/shen-huang/ class=align-middle>Shen Huang</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/q/qi-ju/ class=align-middle>Qi Ju</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/f/fuxue-li/ class=align-middle>Fuxue Li</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yuze-gao/ class=align-middle>Yuze Gao</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yue-zhang/ class=align-middle>Yue Zhang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yufan-jiang/ class=align-middle>Yufan Jiang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/kai-feng/ class=align-middle>Kai Feng</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/zeyang-wang/ class=align-middle>Zeyang Wang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yingfeng-luo/ class=align-middle>Yingfeng Luo</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/huizhen-wang/ class=align-middle>Huizhen Wang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/shujian-huang/ class=align-middle>Shujian Huang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/runzhe-cao/ class=align-middle>Runzhe Cao</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/b/binghao-wei/ class=align-middle>Binghao Wei</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/w/weiqiao-shan/ class=align-middle>Weiqiao Shan</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/shuhan-zhou/ class=align-middle>Shuhan Zhou</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/abudurexiti-reheman/ class=align-middle>Abudurexiti Reheman</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/tao-zhou/ class=align-middle>Tao Zhou</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/x/xin-zeng/ class=align-middle>Xin Zeng</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/laohu-wang/ class=align-middle>Laohu Wang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yongyu-mu/ class=align-middle>Yongyu Mu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jingnan-zhang/ class=align-middle>Jingnan Zhang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/x/xiaoqian-liu/ class=align-middle>Xiaoqian Liu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/x/xuanjun-zhou/ class=align-middle>Xuanjun Zhou</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/changming-xu/ class=align-middle>Changming Xu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/coling/ class=align-middle>COLING</a><span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/venues/acl/ class=align-middle>ACL</a><span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/venues/ijcnlp/ class=align-middle>IJCNLP</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/findings/ class=align-middle>Findings</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/wmt/ class=align-middle>WMT</a><span class="badge badge-secondary align-middle ml-2">1</span></li></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>