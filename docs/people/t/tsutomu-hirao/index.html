<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Tsutomu Hirao - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Tsutomu</span> <span class=font-weight-bold>Hirao</span></h2><hr><div class=row><div class=col-lg-9><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.77.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--77 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.77 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.findings-emnlp.77.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.77/>Sequential Span Classification with Neural Semi-Markov CRFs for Biomedical Abstracts<span class=acl-fixed-case>M</span>arkov <span class=acl-fixed-case>CRF</span>s for Biomedical Abstracts</a></strong><br><a href=/people/k/kosuke-yamada/>Kosuke Yamada</a>
|
<a href=/people/t/tsutomu-hirao/>Tsutomu Hirao</a>
|
<a href=/people/r/ryohei-sasano/>Ryohei Sasano</a>
|
<a href=/people/k/koichi-takeda/>Koichi Takeda</a>
|
<a href=/people/m/masaaki-nagata/>Masaaki Nagata</a><br><a href=/volumes/2020.findings-emnlp/ class=text-muted>Findings of the Association for Computational Linguistics: EMNLP 2020</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--77><div class="card-body p-3 small">Dividing biomedical abstracts into several segments with rhetorical roles is essential for supporting researchers&#8217; information access in the biomedical domain. Conventional methods have regarded the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> as a sequence labeling task based on sequential sentence classification, i.e., they assign a rhetorical label to each sentence by considering the context in the abstract. However, these methods have a critical problem : they are prone to mislabel longer continuous sentences with the same rhetorical label. To tackle the problem, we propose sequential span classification that assigns a rhetorical label, not to a single sentence but to a span that consists of continuous sentences. Accordingly, we introduce Neural Semi-Markov Conditional Random Fields to assign the labels to such spans by considering all possible spans of various lengths. Experimental results obtained from PubMed 20k RCT and NICTA-PIBOSO datasets demonstrate that our proposed method achieved the best micro sentence-F1 score as well as the best micro span-F1 score.</div></div><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1587.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1587 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1587 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1587/>Split or Merge : Which is Better for Unsupervised RST Parsing?<span class=acl-fixed-case>RST</span> Parsing?</a></strong><br><a href=/people/n/naoki-kobayashi/>Naoki Kobayashi</a>
|
<a href=/people/t/tsutomu-hirao/>Tsutomu Hirao</a>
|
<a href=/people/k/kengo-nakamura/>Kengo Nakamura</a>
|
<a href=/people/h/hidetaka-kamigaito/>Hidetaka Kamigaito</a>
|
<a href=/people/m/manabu-okumura/>Manabu Okumura</a>
|
<a href=/people/m/masaaki-nagata/>Masaaki Nagata</a><br><a href=/volumes/D19-1/ class=text-muted>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1587><div class="card-body p-3 small">Rhetorical Structure Theory (RST) parsing is crucial for many downstream NLP tasks that require a discourse structure for a text. Most of the previous RST parsers have been based on supervised learning approaches. That is, they require an annotated corpus of sufficient size and quality, and heavily rely on the language and domain dependent corpus. In this paper, we present two language-independent unsupervised RST parsing methods based on <a href=https://en.wikipedia.org/wiki/Dynamic_programming>dynamic programming</a>. The first one builds the optimal tree in terms of a dissimilarity score function that is defined for splitting a text span into smaller ones. The second builds the optimal tree in terms of a similarity score function that is defined for merging two adjacent spans into a large one. Experimental results on English and German RST treebanks showed that our <a href=https://en.wikipedia.org/wiki/Parsing>parser</a> based on span merging achieved the best score, around 0.8 F_1 score, which is close to the scores of the previous supervised parsers.<tex-math>_1</tex-math> score, which is close to the scores of the previous supervised parsers.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1674.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1674 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1674 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1674/>Generating Natural Anagrams : Towards <a href=https://en.wikipedia.org/wiki/Language_generation>Language Generation</a> Under Hard Combinatorial Constraints</a></strong><br><a href=/people/m/masaaki-nishino/>Masaaki Nishino</a>
|
<a href=/people/s/sho-takase/>Sho Takase</a>
|
<a href=/people/t/tsutomu-hirao/>Tsutomu Hirao</a>
|
<a href=/people/m/masaaki-nagata/>Masaaki Nagata</a><br><a href=/volumes/D19-1/ class=text-muted>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1674><div class="card-body p-3 small">An <a href=https://en.wikipedia.org/wiki/Anagram>anagram</a> is a sentence or a phrase that is made by permutating the characters of an input sentence or a phrase. For example, Trims cash is an anagram of Christmas. Existing automatic anagram generation methods can find possible combinations of words form an <a href=https://en.wikipedia.org/wiki/Anagram>anagram</a>. However, they do not pay much attention to the naturalness of the generated <a href=https://en.wikipedia.org/wiki/Anagram>anagrams</a>. In this paper, we show that simple <a href=https://en.wikipedia.org/wiki/Depth-first_search>depth-first search</a> can yield natural anagrams when it is combined with modern neural language models. Human evaluation results show that the proposed <a href=https://en.wikipedia.org/wiki/Methodology>method</a> can generate significantly more natural anagrams than baseline methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5365.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5365 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5365 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5365/>NTT’s Machine Translation Systems for WMT19 Robustness Task<span class=acl-fixed-case>NTT</span>’s Machine Translation Systems for <span class=acl-fixed-case>WMT</span>19 Robustness Task</a></strong><br><a href=/people/s/soichiro-murakami/>Soichiro Murakami</a>
|
<a href=/people/m/makoto-morishita/>Makoto Morishita</a>
|
<a href=/people/t/tsutomu-hirao/>Tsutomu Hirao</a>
|
<a href=/people/m/masaaki-nagata/>Masaaki Nagata</a><br><a href=/volumes/W19-53/ class=text-muted>Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5365><div class="card-body p-3 small">This paper describes NTT&#8217;s submission to the WMT19 robustness task. This <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> mainly focuses on translating <a href=https://en.wikipedia.org/wiki/Noise_(signal_processing)>noisy text</a> (e.g., posts on Twitter), which presents different difficulties from typical translation tasks such as <a href=https://en.wikipedia.org/wiki/News>news</a>. Our submission combined techniques including utilization of a synthetic corpus, <a href=https://en.wikipedia.org/wiki/Domain_adaptation>domain adaptation</a>, and a <a href=https://en.wikipedia.org/wiki/Placeholder_name>placeholder mechanism</a>, which significantly improved over the previous <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline</a>. Experimental results revealed the placeholder mechanism, which temporarily replaces the non-standard tokens including <a href=https://en.wikipedia.org/wiki/Emoji>emojis</a> and <a href=https://en.wikipedia.org/wiki/Emoticon>emoticons</a> with special placeholder tokens during <a href=https://en.wikipedia.org/wiki/Translation>translation</a>, improves <a href=https://en.wikipedia.org/wiki/Translation>translation accuracy</a> even with noisy texts.</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1450.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1450 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1450 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-1450/>Automatic Pyramid Evaluation Exploiting EDU-based Extractive Reference Summaries<span class=acl-fixed-case>EDU</span>-based Extractive Reference Summaries</a></strong><br><a href=/people/t/tsutomu-hirao/>Tsutomu Hirao</a>
|
<a href=/people/h/hidetaka-kamigaito/>Hidetaka Kamigaito</a>
|
<a href=/people/m/masaaki-nagata/>Masaaki Nagata</a><br><a href=/volumes/D18-1/ class=text-muted>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1450><div class="card-body p-3 small">This paper tackles automation of the pyramid method, a reliable manual evaluation framework. To construct a pyramid, we transform human-made reference summaries into extractive reference summaries that consist of Elementary Discourse Units (EDUs) obtained from source documents and then weight every EDU by counting the number of extractive reference summaries that contain the EDU. A summary is scored by the correspondences between EDUs in the summary and those in the pyramid. Experiments on DUC and TAC data sets show that our methods strongly correlate with various manual evaluations.</div></div><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-2002.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-2002 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-2002 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-2002/>Supervised Attention for Sequence-to-Sequence Constituency Parsing</a></strong><br><a href=/people/h/hidetaka-kamigaito/>Hidetaka Kamigaito</a>
|
<a href=/people/k/katsuhiko-hayashi/>Katsuhiko Hayashi</a>
|
<a href=/people/t/tsutomu-hirao/>Tsutomu Hirao</a>
|
<a href=/people/h/hiroya-takamura/>Hiroya Takamura</a>
|
<a href=/people/m/manabu-okumura/>Manabu Okumura</a>
|
<a href=/people/m/masaaki-nagata/>Masaaki Nagata</a><br><a href=/volumes/I17-2/ class=text-muted>Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-2002><div class="card-body p-3 small">The sequence-to-sequence (Seq2Seq) model has been successfully applied to <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation (MT)</a>. Recently, MT performances were improved by incorporating supervised attention into the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>. In this paper, we introduce supervised attention to constituency parsing that can be regarded as another translation task. Evaluation results on the PTB corpus showed that the bracketing F-measure was improved by supervised attention.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2043.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2043 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2043 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-2043/>Oracle Summaries of Compressive Summarization</a></strong><br><a href=/people/t/tsutomu-hirao/>Tsutomu Hirao</a>
|
<a href=/people/m/masaaki-nishino/>Masaaki Nishino</a>
|
<a href=/people/m/masaaki-nagata/>Masaaki Nagata</a><br><a href=/volumes/P17-2/ class=text-muted>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2043><div class="card-body p-3 small">This paper derives an Integer Linear Programming (ILP) formulation to obtain an oracle summary of the compressive summarization paradigm in terms of ROUGE. The oracle summary is essential to reveal the upper bound performance of the <a href=https://en.wikipedia.org/wiki/Paradigm>paradigm</a>. Experimental results on the DUC dataset showed that ROUGE scores of compressive oracles are significantly higher than those of extractive oracles and state-of-the-art summarization systems. These results reveal that compressive summarization is a promising paradigm and encourage us to continue with the research to produce informative summaries.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1037.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1037 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1037 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1037/>Enumeration of Extractive Oracle Summaries</a></strong><br><a href=/people/t/tsutomu-hirao/>Tsutomu Hirao</a>
|
<a href=/people/m/masaaki-nishino/>Masaaki Nishino</a>
|
<a href=/people/j/jun-suzuki/>Jun Suzuki</a>
|
<a href=/people/m/masaaki-nagata/>Masaaki Nagata</a><br><a href=/volumes/E17-1/ class=text-muted>Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1037><div class="card-body p-3 small">To analyze the limitations and the future directions of the extractive summarization paradigm, this paper proposes an Integer Linear Programming (ILP) formulation to obtain extractive oracle summaries in terms of ROUGE-N. We also propose an algorithm that enumerates all of the <a href=https://en.wikipedia.org/wiki/Oracle_machine>oracle summaries</a> for a set of reference summaries to exploit F-measures that evaluate which system summaries contain how many sentences that are extracted as an oracle summary. Our experimental results obtained from Document Understanding Conference (DUC) corpora demonstrated the following : (1) room still exists to improve the performance of extractive summarization ; (2) the F-measures derived from the enumerated oracle summaries have significantly stronger correlations with human judgment than those derived from single oracle summaries.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Tsutomu+Hirao" title="Search for 'Tsutomu Hirao' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/m/masaaki-nagata/ class=align-middle>Masaaki Nagata</a>
<span class="badge badge-secondary align-middle ml-2">8</span></li><li class=list-group-item><a href=/people/h/hidetaka-kamigaito/ class=align-middle>Hidetaka Kamigaito</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/m/masaaki-nishino/ class=align-middle>Masaaki Nishino</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/m/manabu-okumura/ class=align-middle>Manabu Okumura</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/k/katsuhiko-hayashi/ class=align-middle>Katsuhiko Hayashi</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/h/hiroya-takamura/ class=align-middle>Hiroya Takamura</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/n/naoki-kobayashi/ class=align-middle>Naoki Kobayashi</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/kengo-nakamura/ class=align-middle>Kengo Nakamura</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/sho-takase/ class=align-middle>Sho Takase</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/kosuke-yamada/ class=align-middle>Kosuke Yamada</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/ryohei-sasano/ class=align-middle>Ryohei Sasano</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/koichi-takeda/ class=align-middle>Koichi Takeda</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/soichiro-murakami/ class=align-middle>Soichiro Murakami</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/makoto-morishita/ class=align-middle>Makoto Morishita</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jun-suzuki/ class=align-middle>Jun Suzuki</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/emnlp/ class=align-middle>EMNLP</a><span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/venues/ijcnlp/ class=align-middle>IJCNLP</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/acl/ class=align-middle>ACL</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/findings/ class=align-middle>Findings</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/ws/ class=align-middle>WS</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-venues aria-expanded=false aria-controls=more-venues>show all...</li><div class="collapse border-top" id=more-venues><li class=list-group-item><a href=/venues/eacl/ class=align-middle>EACL</a><span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>