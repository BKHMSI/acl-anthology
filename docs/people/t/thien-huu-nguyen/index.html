<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Thien Huu Nguyen - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Thien Huu</span> <span class=font-weight-bold>Nguyen</span></h2><hr><div class=row><div class=col-lg-9><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.427.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--427 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.427 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.427" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.427/>Learning Prototype Representations Across Few-Shot Tasks for Event Detection</a></strong><br><a href=/people/v/viet-lai/>Viet Lai</a>
|
<a href=/people/f/franck-dernoncourt/>Franck Dernoncourt</a>
|
<a href=/people/t/thien-huu-nguyen/>Thien Huu Nguyen</a><br><a href=/volumes/2021.emnlp-main/ class=text-muted>Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--427><div class="card-body p-3 small">We address the <a href=https://en.wikipedia.org/wiki/Sampling_bias>sampling bias</a> and outlier issues in few-shot learning for event detection, a subtask of <a href=https://en.wikipedia.org/wiki/Information_extraction>information extraction</a>. We propose to model the relations between training tasks in episodic few-shot learning by introducing cross-task prototypes. We further propose to enforce prediction consistency among <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifiers</a> across tasks to make the <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> more robust to <a href=https://en.wikipedia.org/wiki/Outlier>outliers</a>. Our extensive experiment shows a consistent improvement on three few-shot learning datasets. The findings suggest that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is more robust when labeled data of novel event types is limited. The source code is available at http://github.com/laiviet/fsl-proact.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.3.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--3 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.3 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.3/>Cross-Task Instance Representation Interactions and Label Dependencies for Joint Information Extraction with Graph Convolutional Networks</a></strong><br><a href=/people/m/minh-van-nguyen/>Minh Van Nguyen</a>
|
<a href=/people/v/viet-lai/>Viet Lai</a>
|
<a href=/people/t/thien-huu-nguyen/>Thien Huu Nguyen</a><br><a href=/volumes/2021.naacl-main/ class=text-muted>Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--3><div class="card-body p-3 small">Existing works on information extraction (IE) have mainly solved the four main tasks separately (entity mention recognition, relation extraction, event trigger detection, and argument extraction), thus failing to benefit from inter-dependencies between tasks. This paper presents a novel deep learning model to simultaneously solve the four tasks of IE in a single <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> (called FourIE). Compared to few prior work on jointly performing four IE tasks, FourIE features two novel contributions to capture inter-dependencies between tasks. First, at the representation level, we introduce an interaction graph between instances of the four <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> that is used to enrich the prediction representation for one instance with those from related instances of other tasks. Second, at the label level, we propose a <a href=https://en.wikipedia.org/wiki/Dependency_graph>dependency graph</a> for the information types in the four IE tasks that captures the connections between the types expressed in an input sentence. A new <a href=https://en.wikipedia.org/wiki/Regularization_(mathematics)>regularization mechanism</a> is introduced to enforce the consistency between the golden and predicted type dependency graphs to improve <a href=https://en.wikipedia.org/wiki/Representation_learning>representation learning</a>. We show that the proposed model achieves the state-of-the-art performance for joint IE on both monolingual and multilingual learning settings with three different languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wnut-1.5.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wnut-1--5 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wnut-1.5 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wnut-1.5/>Fine-grained Temporal Relation Extraction with Ordered-Neuron LSTM and Graph Convolutional Networks<span class=acl-fixed-case>LSTM</span> and Graph Convolutional Networks</a></strong><br><a href=/people/m/minh-tran-phu/>Minh Tran Phu</a>
|
<a href=/people/m/minh-van-nguyen/>Minh Van Nguyen</a>
|
<a href=/people/t/thien-huu-nguyen/>Thien Huu Nguyen</a><br><a href=/volumes/2021.wnut-1/ class=text-muted>Proceedings of the Seventh Workshop on Noisy User-generated Text (W-NUT 2021)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wnut-1--5><div class="card-body p-3 small">Fine-grained temporal relation extraction (FineTempRel) aims to recognize the durations and timeline of event mentions in text. A missing part in the current deep learning models for FineTempRel is their failure to exploit the <a href=https://en.wikipedia.org/wiki/Syntax>syntactic structures</a> of the input sentences to enrich the <a href=https://en.wikipedia.org/wiki/Representation_(mathematics)>representation vectors</a>. In this work, we propose to fill this gap by introducing novel methods to integrate the <a href=https://en.wikipedia.org/wiki/Syntax>syntactic structures</a> into the <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning models</a> for FineTempRel. The proposed <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> focuses on two types of syntactic information from the <a href=https://en.wikipedia.org/wiki/Tree_(data_structure)>dependency trees</a>, i.e., the syntax-based importance scores for representation learning of the words and the syntactic connections to identify important <a href=https://en.wikipedia.org/wiki/Context_(language_use)>context words</a> for the <a href=https://en.wikipedia.org/wiki/Context_(language_use)>event mentions</a>. We also present two novel techniques to facilitate the knowledge transfer between the subtasks of FineTempRel, leading to a novel model with the state-of-the-art performance for this task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wanlp-1.27.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wanlp-1--27 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wanlp-1.27 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wanlp-1.27/>Improving Cross-Lingual Transfer for Event Argument Extraction with Language-Universal Sentence Structures</a></strong><br><a href=/people/m/minh-van-nguyen/>Minh Van Nguyen</a>
|
<a href=/people/t/thien-huu-nguyen/>Thien Huu Nguyen</a><br><a href=/volumes/2021.wanlp-1/ class=text-muted>Proceedings of the Sixth Arabic Natural Language Processing Workshop</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wanlp-1--27><div class="card-body p-3 small">We study the problem of Cross-lingual Event Argument Extraction (CEAE). The task aims to predict argument roles of entity mentions for events in text, whose language is different from the language that a <a href=https://en.wikipedia.org/wiki/Predictive_modelling>predictive model</a> has been trained on. Previous work on CEAE has shown the cross-lingual benefits of universal dependency trees in capturing shared syntactic structures of sentences across languages. In particular, this work exploits the existence of the syntactic connections between the words in the dependency trees as the anchor knowledge to transfer the representation learning across languages for CEAE models (i.e., via graph convolutional neural networks GCNs). In this paper, we introduce two novel sources of language-independent information for CEAE models based on the <a href=https://en.wikipedia.org/wiki/Semantic_similarity>semantic similarity</a> and the universal dependency relations of the word pairs in different languages. We propose to use the two sources of information to produce shared sentence structures to bridge the gap between languages and improve the cross-lingual performance of the CEAE models. Extensive experiments are conducted with <a href=https://en.wikipedia.org/wiki/Arabic>Arabic</a>, <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a>, and <a href=https://en.wikipedia.org/wiki/English_language>English</a> to demonstrate the effectiveness of the proposed method for CEAE.</div></div><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.nuse-1.5.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--nuse-1--5 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.nuse-1.5 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://slideslive.com/38929744 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.nuse-1.5/>Extensively Matching for Few-shot Learning Event Detection</a></strong><br><a href=/people/v/viet-dac-lai/>Viet Dac Lai</a>
|
<a href=/people/t/thien-huu-nguyen/>Thien Huu Nguyen</a>
|
<a href=/people/f/franck-dernoncourt/>Franck Dernoncourt</a><br><a href=/volumes/2020.nuse-1/ class=text-muted>Proceedings of the First Joint Workshop on Narrative Understanding, Storylines, and Events</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--nuse-1--5><div class="card-body p-3 small">Current event detection models under supervised learning settings fail to transfer to new event types. Few-shot learning has not been explored in event detection even though it allows a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> to perform well with high <a href=https://en.wikipedia.org/wiki/Generalization>generalization</a> on new event types. In this work, we formulate event detection as a few-shot learning problem to enable to extend event detection to new event types. We propose two novel <a href=https://en.wikipedia.org/wiki/Loss_factor>loss factors</a> that matching examples in the support set to provide more <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training signals</a> to the <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a>. Moreover, these <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training signals</a> can be applied in many metric-based few-shot learning models. Our extensive experiments on the ACE-2005 dataset (under a few-shot learning setting) show that the proposed method can improve the performance of few-shot learning.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.409.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--409 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.409 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.409/>The Dots Have Their Values : Exploiting the Node-Edge Connections in Graph-based Neural Models for Document-level Relation Extraction</a></strong><br><a href=/people/h/hieu-minh-tran/>Hieu Minh Tran</a>
|
<a href=/people/m/minh-trung-nguyen/>Minh Trung Nguyen</a>
|
<a href=/people/t/thien-huu-nguyen/>Thien Huu Nguyen</a><br><a href=/volumes/2020.findings-emnlp/ class=text-muted>Findings of the Association for Computational Linguistics: EMNLP 2020</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--409><div class="card-body p-3 small">The goal of Document-level Relation Extraction (DRE) is to recognize the relations between entity mentions that can span beyond sentence boundary. The current state-of-the-art method for this problem has involved the graph-based edge-oriented model where the entity mentions, entities, and sentences in the documents are used as the nodes of the document graphs for <a href=https://en.wikipedia.org/wiki/Representation_learning>representation learning</a>. However, this <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> does not capture the representations for the <a href=https://en.wikipedia.org/wiki/Vertex_(graph_theory)>nodes</a> in the <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graphs</a>, thus preventing it from effectively encoding the specific and relevant information of the <a href=https://en.wikipedia.org/wiki/Vertex_(graph_theory)>nodes</a> for DRE. To address this issue, we propose to explicitly compute the <a href=https://en.wikipedia.org/wiki/Representation_(mathematics)>representations</a> for the <a href=https://en.wikipedia.org/wiki/Vertex_(graph_theory)>nodes</a> in the graph-based edge-oriented model for <a href=https://en.wikipedia.org/wiki/Directed_acyclic_graph>DRE</a>. These node representations allow us to introduce two novel representation regularization mechanisms to improve the <a href=https://en.wikipedia.org/wiki/Representation_theory>representation vectors</a> for <a href=https://en.wikipedia.org/wiki/Directed_acyclic_graph>DRE</a>. The experiments show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieves state-of-the-art performance on two <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmark datasets</a>.</div></div><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-6203.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-6203 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-6203 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-6203/>On the Effectiveness of the Pooling Methods for Biomedical Relation Extraction with <a href=https://en.wikipedia.org/wiki/Deep_learning>Deep Learning</a></a></strong><br><a href=/people/t/tuan-ngo-nguyen/>Tuan Ngo Nguyen</a>
|
<a href=/people/f/franck-dernoncourt/>Franck Dernoncourt</a>
|
<a href=/people/t/thien-huu-nguyen/>Thien Huu Nguyen</a><br><a href=/volumes/D19-62/ class=text-muted>Proceedings of the Tenth International Workshop on Health Text Mining and Information Analysis (LOUHI 2019)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-6203><div class="card-body p-3 small">Deep learning models have achieved state-of-the-art performances on many relation extraction datasets. A common element in these deep learning models involves the pooling mechanisms where a sequence of hidden vectors is aggregated to generate a single <a href=https://en.wikipedia.org/wiki/Representation_(mathematics)>representation vector</a>, serving as the <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> to perform prediction for RE. Unfortunately, the <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> in the literature tend to employ different strategies to perform pooling for RE, leading to the challenge to determine the best pooling mechanism for this problem, especially in the <a href=https://en.wikipedia.org/wiki/Biomedicine>biomedical domain</a>. In order to answer this question, in this work, we conduct a comprehensive study to evaluate the effectiveness of different pooling mechanisms for the deep learning models in biomedical RE. The experimental results suggest that dependency-based pooling is the best pooling strategy for RE in the biomedical domain, yielding the state-of-the-art performance on two benchmark datasets for this problem.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1411.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1411 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1411 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/385254810 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1411/>Employing the Correspondence of Relations and Connectives to Identify Implicit Discourse Relations via Label Embeddings</a></strong><br><a href=/people/l/linh-the-nguyen/>Linh The Nguyen</a>
|
<a href=/people/l/linh-van-ngo/>Linh Van Ngo</a>
|
<a href=/people/k/khoat-than/>Khoat Than</a>
|
<a href=/people/t/thien-huu-nguyen/>Thien Huu Nguyen</a><br><a href=/volumes/P19-1/ class=text-muted>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1411><div class="card-body p-3 small">It has been shown that implicit connectives can be exploited to improve the performance of the <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> for implicit discourse relation recognition (IDRR). An important property of the implicit connectives is that they can be accurately mapped into the <a href=https://en.wikipedia.org/wiki/Discourse_relation>discourse relations</a> conveying their functions. In this work, we explore this property in a multi-task learning framework for IDRR in which the <a href=https://en.wikipedia.org/wiki/Binary_relation>relations</a> and the <a href=https://en.wikipedia.org/wiki/Binary_relation>connectives</a> are simultaneously predicted, and the mapping is leveraged to transfer knowledge between the two prediction tasks via the embeddings of relations and connectives. We propose several techniques to enable such knowledge transfer that yield the state-of-the-art performance for IDRR on several settings of the benchmark dataset (i.e., the Penn Discourse Treebank dataset).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1432.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1432 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1432 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/385264738 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1432/>Graph based Neural Networks for Event Factuality Prediction using Syntactic and Semantic Structures</a></strong><br><a href=/people/a/amir-pouran-ben-veyseh/>Amir Pouran Ben Veyseh</a>
|
<a href=/people/t/thien-huu-nguyen/>Thien Huu Nguyen</a>
|
<a href=/people/d/dejing-dou/>Dejing Dou</a><br><a href=/volumes/P19-1/ class=text-muted>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1432><div class="card-body p-3 small">Event factuality prediction (EFP) is the task of assessing the degree to which an event mentioned in a sentence has happened. For this task, both syntactic and semantic information are crucial to identify the important <a href=https://en.wikipedia.org/wiki/Context_(language_use)>context words</a>. The previous work for EFP has only combined these information in a simple way that can not fully exploit their coordination. In this work, we introduce a novel graph-based neural network for EFP that can integrate the semantic and syntactic information more effectively. Our experiments demonstrate the advantage of the proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> for EFP.</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1193.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1193 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1193 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1193/>Who is Killed by Police : Introducing Supervised Attention for Hierarchical LSTMs<span class=acl-fixed-case>LSTM</span>s</a></strong><br><a href=/people/m/minh-nguyen/>Minh Nguyen</a>
|
<a href=/people/t/thien-huu-nguyen/>Thien Huu Nguyen</a><br><a href=/volumes/C18-1/ class=text-muted>Proceedings of the 27th International Conference on Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1193><div class="card-body p-3 small">Finding names of people killed by police has become increasingly important as police shootings get more and more public attention (police killing detection). Unfortunately, there has been not much work in the literature addressing this problem. The early work in this field (Keith etal., 2017) proposed a distant supervision framework based on Expectation Maximization (EM) to deal with the multiple appearances of the names in documents. However, such EM-based framework can not take full advantages of deep learning models, necessitating the use of handdesigned features to improve the detection performance. In this work, we present a novel deep learning method to solve the problem of police killing recognition. The proposed method relies on hierarchical LSTMs to model the multiple sentences that contain the person names of interests, and introduce supervised attention mechanisms based on semantical word lists and dependency trees to upweight the important contextual words. Our experiments demonstrate the benefits of the proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> and yield the state-of-the-art performance for police killing detection.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6126.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6126 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6126 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6126/>A Case Study on Learning a Unified Encoder of Relations</a></strong><br><a href=/people/l/lisheng-fu/>Lisheng Fu</a>
|
<a href=/people/b/bonan-min/>Bonan Min</a>
|
<a href=/people/t/thien-huu-nguyen/>Thien Huu Nguyen</a>
|
<a href=/people/r/ralph-grishman/>Ralph Grishman</a><br><a href=/volumes/W18-61/ class=text-muted>Proceedings of the 2018 EMNLP Workshop W-NUT: The 4th Workshop on Noisy User-generated Text</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6126><div class="card-body p-3 small">Typical relation extraction models are trained on a single corpus annotated with a pre-defined relation schema. An individual corpus is often small, and the <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> may often be biased or overfitted to the corpus. We hypothesize that we can learn a better <a href=https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning>representation</a> by combining multiple relation datasets. We attempt to use a shared encoder to learn the unified feature representation and to augment it with <a href=https://en.wikipedia.org/wiki/Regularization_(mathematics)>regularization</a> by <a href=https://en.wikipedia.org/wiki/Adversarial_system>adversarial training</a>. The additional corpora feeding the <a href=https://en.wikipedia.org/wiki/Encoder>encoder</a> can help to learn a better feature representation layer even though the relation schemas are different. We use ACE05 and ERE datasets as our case study for experiments. The multi-task model obtains significant improvement on both <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>.</div></div><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-2072.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-2072 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-2072 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-2072/>Domain Adaptation for <a href=https://en.wikipedia.org/wiki/Relation_extraction>Relation Extraction</a> with Domain Adversarial Neural Network</a></strong><br><a href=/people/l/lisheng-fu/>Lisheng Fu</a>
|
<a href=/people/t/thien-huu-nguyen/>Thien Huu Nguyen</a>
|
<a href=/people/b/bonan-min/>Bonan Min</a>
|
<a href=/people/r/ralph-grishman/>Ralph Grishman</a><br><a href=/volumes/I17-2/ class=text-muted>Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-2072><div class="card-body p-3 small">Relations are expressed in many domains such as <a href=https://en.wikipedia.org/wiki/News_agency>newswire</a>, <a href=https://en.wikipedia.org/wiki/Blog>weblogs</a> and phone conversations. Trained on a source domain, a relation extractor&#8217;s performance degrades when applied to target domains other than the source. A common yet labor-intensive method for <a href=https://en.wikipedia.org/wiki/Domain_adaptation>domain adaptation</a> is to construct a target-domain-specific labeled dataset for adapting the extractor. In response, we present an unsupervised domain adaptation method which only requires labels from the source domain. Our method is a joint model consisting of a CNN-based relation classifier and a domain-adversarial classifier. The two <a href=https://en.wikipedia.org/wiki/Component-based_software_engineering>components</a> are optimized jointly to learn a domain-independent representation for <a href=https://en.wikipedia.org/wiki/Prediction>prediction</a> on the target domain. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms the <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a> on all three test domains of ACE 2005.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Thien+Huu+Nguyen" title="Search for 'Thien Huu Nguyen' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/f/franck-dernoncourt/ class=align-middle>Franck Dernoncourt</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/m/minh-van-nguyen/ class=align-middle>Minh Van Nguyen</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/l/lisheng-fu/ class=align-middle>Lisheng Fu</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/b/bonan-min/ class=align-middle>Bonan Min</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/r/ralph-grishman/ class=align-middle>Ralph Grishman</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/v/viet-lai/ class=align-middle>Viet Lai</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/m/minh-nguyen/ class=align-middle>Minh Nguyen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/v/viet-dac-lai/ class=align-middle>Viet Dac Lai</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/tuan-ngo-nguyen/ class=align-middle>Tuan Ngo Nguyen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/hieu-minh-tran/ class=align-middle>Hieu Minh Tran</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/minh-trung-nguyen/ class=align-middle>Minh Trung Nguyen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/minh-tran-phu/ class=align-middle>Minh Tran Phu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/linh-the-nguyen/ class=align-middle>Linh The Nguyen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/linh-van-ngo/ class=align-middle>Linh Van Ngo</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/khoat-than/ class=align-middle>Khoat Than</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/amir-pouran-ben-veyseh/ class=align-middle>Amir Pouran Ben Veyseh</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/dejing-dou/ class=align-middle>Dejing Dou</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/emnlp/ class=align-middle>EMNLP</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/acl/ class=align-middle>ACL</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/coling/ class=align-middle>COLING</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/nuse/ class=align-middle>NUSE</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/ijcnlp/ class=align-middle>IJCNLP</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-venues aria-expanded=false aria-controls=more-venues>show all...</li><div class="collapse border-top" id=more-venues><li class=list-group-item><a href=/venues/naacl/ class=align-middle>NAACL</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/findings/ class=align-middle>Findings</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/ws/ class=align-middle>WS</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/wnut/ class=align-middle>WNUT</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/wanlp/ class=align-middle>WANLP</a><span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>