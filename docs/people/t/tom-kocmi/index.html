<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Tom Kocmi - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Tom</span> <span class=font-weight-bold>Kocmi</span></h2><hr><div class=row><div class=col-lg-9><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wmt-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wmt-1.0/>Proceedings of the Sixth Conference on Machine Translation</a></strong><br><a href=/people/l/loic-barrault/>Loic Barrault</a>
|
<a href=/people/o/ondrej-bojar/>Ondrej Bojar</a>
|
<a href=/people/f/fethi-bougares/>Fethi Bougares</a>
|
<a href=/people/r/rajen-chatterjee/>Rajen Chatterjee</a>
|
<a href=/people/m/marta-r-costa-jussa/>Marta R. Costa-jussa</a>
|
<a href=/people/c/christian-federmann/>Christian Federmann</a>
|
<a href=/people/m/mark-fishel/>Mark Fishel</a>
|
<a href=/people/a/alexander-fraser/>Alexander Fraser</a>
|
<a href=/people/m/markus-freitag/>Markus Freitag</a>
|
<a href=/people/y/yvette-graham/>Yvette Graham</a>
|
<a href=/people/r/roman-grundkiewicz/>Roman Grundkiewicz</a>
|
<a href=/people/p/paco-guzman/>Paco Guzman</a>
|
<a href=/people/b/barry-haddow/>Barry Haddow</a>
|
<a href=/people/m/matthias-huck/>Matthias Huck</a>
|
<a href=/people/a/antonio-jimeno-yepes/>Antonio Jimeno Yepes</a>
|
<a href=/people/p/philipp-koehn/>Philipp Koehn</a>
|
<a href=/people/t/tom-kocmi/>Tom Kocmi</a>
|
<a href=/people/a/andre-f-t-martins/>Andre Martins</a>
|
<a href=/people/m/makoto-morishita/>Makoto Morishita</a>
|
<a href=/people/c/christof-monz/>Christof Monz</a><br><a href=/volumes/2021.wmt-1/ class=text-muted>Proceedings of the Sixth Conference on Machine Translation</a></span></p><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.eamt-1.3.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--eamt-1--3 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.eamt-1.3 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.eamt-1.3/>Efficiently Reusing Old Models Across Languages via Transfer Learning</a></strong><br><a href=/people/t/tom-kocmi/>Tom Kocmi</a>
|
<a href=/people/o/ondrej-bojar/>Ond≈ôej Bojar</a><br><a href=/volumes/2020.eamt-1/ class=text-muted>Proceedings of the 22nd Annual Conference of the European Association for Machine Translation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--eamt-1--3><div class="card-body p-3 small">Recent progress in neural machine translation (NMT) is directed towards larger <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a> trained on an increasing amount of hardware resources. As a result, NMT models are costly to train, both financially, due to the electricity and hardware cost, and environmentally, due to the <a href=https://en.wikipedia.org/wiki/Carbon_footprint>carbon footprint</a>. It is especially true in <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a> for its additional cost of training the parent model before transferring knowledge and training the desired <a href=https://en.wikipedia.org/wiki/Child_model>child model</a>. In this paper, we propose a simple method of re-using an already trained <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> for different language pairs where there is no need for modifications in <a href=https://en.wikipedia.org/wiki/Statistical_model>model architecture</a>. Our approach does not need a separate parent model for each investigated language pair, as it is typical in NMT transfer learning. To show the applicability of our method, we recycle a Transformer model trained by different researchers and use it to seed models for different language pairs. We achieve better translation quality and shorter <a href=https://en.wikipedia.org/wiki/Convergence_of_random_variables>convergence times</a> than when training from random initialization.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wmt-1.14.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wmt-1--14 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wmt-1.14 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939666 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.wmt-1.14/>CUNI Submission for the Inuktitut Language in WMT News 2020<span class=acl-fixed-case>CUNI</span> Submission for the <span class=acl-fixed-case>I</span>nuktitut Language in <span class=acl-fixed-case>WMT</span> News 2020</a></strong><br><a href=/people/t/tom-kocmi/>Tom Kocmi</a><br><a href=/volumes/2020.wmt-1/ class=text-muted>Proceedings of the Fifth Conference on Machine Translation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wmt-1--14><div class="card-body p-3 small">This paper describes CUNI submission to the WMT 2020 News Translation Shared Task for the low-resource scenario InuktitutEnglish in both translation directions. Our system combines <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a> from a CzechEnglish high-resource language pair and backtranslation. We notice surprising behaviour when using synthetic data, which can be possibly attributed to a narrow domain of training and test data. We are using the Transformer model in a constrained submission.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wmt-1.39.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wmt-1--39 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wmt-1.39 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939659 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.wmt-1.39/>Gender Coreference and Bias Evaluation at WMT 2020<span class=acl-fixed-case>WMT</span> 2020</a></strong><br><a href=/people/t/tom-kocmi/>Tom Kocmi</a>
|
<a href=/people/t/tomasz-limisiewicz/>Tomasz Limisiewicz</a>
|
<a href=/people/g/gabriel-stanovsky/>Gabriel Stanovsky</a><br><a href=/volumes/2020.wmt-1/ class=text-muted>Proceedings of the Fifth Conference on Machine Translation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wmt-1--39><div class="card-body p-3 small">Gender bias in <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> can manifest when choosing gender inflections based on spurious gender correlations. For example, always translating doctors as men and nurses as women. This can be particularly harmful as <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> become more popular and deployed within commercial systems. Our work presents the largest evidence for the phenomenon in more than 19 systems submitted to the WMT over four diverse target languages : <a href=https://en.wikipedia.org/wiki/Czech_language>Czech</a>, <a href=https://en.wikipedia.org/wiki/German_language>German</a>, <a href=https://en.wikipedia.org/wiki/Polish_language>Polish</a>, and <a href=https://en.wikipedia.org/wiki/Russian_language>Russian</a>. To achieve this, we use WinoMT, a recent automatic test suite which examines gender coreference and bias when translating from <a href=https://en.wikipedia.org/wiki/English_language>English</a> to languages with <a href=https://en.wikipedia.org/wiki/Grammatical_gender>grammatical gender</a>. We extend WinoMT to handle two new <a href=https://en.wikipedia.org/wiki/Language>languages</a> tested in WMT : <a href=https://en.wikipedia.org/wiki/Polish_language>Polish</a> and <a href=https://en.wikipedia.org/wiki/Czech_language>Czech</a>. We find that all <a href=https://en.wikipedia.org/wiki/System>systems</a> consistently use spurious correlations in the data rather than meaningful <a href=https://en.wikipedia.org/wiki/Context_(language_use)>contextual information</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wmt-1.133.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wmt-1--133 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wmt-1.133 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939641 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.wmt-1.133/>CUNI Systems for the Unsupervised and Very Low Resource Translation Task in WMT20<span class=acl-fixed-case>CUNI</span> Systems for the Unsupervised and Very Low Resource Translation Task in <span class=acl-fixed-case>WMT</span>20</a></strong><br><a href=/people/i/ivana-kvapilikova/>Ivana Kvapil√≠kov√°</a>
|
<a href=/people/t/tom-kocmi/>Tom Kocmi</a>
|
<a href=/people/o/ondrej-bojar/>Ond≈ôej Bojar</a><br><a href=/volumes/2020.wmt-1/ class=text-muted>Proceedings of the Fifth Conference on Machine Translation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wmt-1--133><div class="card-body p-3 small">This paper presents a description of CUNI systems submitted to the WMT20 task on unsupervised and very low-resource supervised machine translation between <a href=https://en.wikipedia.org/wiki/German_language>German</a> and Upper Sorbian. We experimented with training on <a href=https://en.wikipedia.org/wiki/Synthetic_data>synthetic data</a> and pre-training on a related language pair. In the fully unsupervised scenario, we achieved 25.5 and 23.7 BLEU translating from and into Upper Sorbian, respectively. Our low-resource systems relied on <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a> from German-Czech parallel data and achieved 57.4 BLEU and 56.1 BLEU, which is an improvement of 10 BLEU points over the baseline trained only on the available small German-Upper Sorbian parallel corpus.</div></div><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5322.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5322 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5322 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W19-5322.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/W19-5322/>CUNI Submission for Low-Resource Languages in WMT News 2019<span class=acl-fixed-case>CUNI</span> Submission for Low-Resource Languages in <span class=acl-fixed-case>WMT</span> News 2019</a></strong><br><a href=/people/t/tom-kocmi/>Tom Kocmi</a>
|
<a href=/people/o/ondrej-bojar/>Ond≈ôej Bojar</a><br><a href=/volumes/W19-53/ class=text-muted>Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5322><div class="card-body p-3 small">This paper describes the CUNI submission to the WMT 2019 News Translation Shared Task for the low-resource languages : Gujarati-English and Kazakh-English. We participated in both language pairs in both translation directions. Our system combines <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a> from a different high-resource language pair followed by training on backtranslated monolingual data. Thanks to the simultaneous training in both directions, we can iterate the backtranslation process. We are using the Transformer model in a constrained submission.</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6325.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6325 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6325 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6325/>Trivial Transfer Learning for Low-Resource Neural Machine Translation</a></strong><br><a href=/people/t/tom-kocmi/>Tom Kocmi</a>
|
<a href=/people/o/ondrej-bojar/>Ond≈ôej Bojar</a><br><a href=/volumes/W18-63/ class=text-muted>Proceedings of the Third Conference on Machine Translation: Research Papers</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6325><div class="card-body p-3 small">Transfer learning has been proven as an effective technique for <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a> under low-resource conditions. Existing methods require a common target language, <a href=https://en.wikipedia.org/wiki/Language_family>language relatedness</a>, or specific training tricks and regimes. We present a simple transfer learning method, where we first train a parent model for a high-resource language pair and then continue the <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training</a> on a low-resource pair only by replacing the training corpus. This <a href=https://en.wikipedia.org/wiki/Child_model>child model</a> performs significantly better than the <a href=https://en.wikipedia.org/wiki/Baseline_(medicine)>baseline</a> trained for low-resource pair only. We are the first to show this for targeting different languages, and we observe the improvements even for unrelated languages with different alphabets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6416.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6416 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6416 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6416/>CUNI Submissions in WMT18<span class=acl-fixed-case>CUNI</span> Submissions in <span class=acl-fixed-case>WMT</span>18</a></strong><br><a href=/people/t/tom-kocmi/>Tom Kocmi</a>
|
<a href=/people/r/roman-sudarikov/>Roman Sudarikov</a>
|
<a href=/people/o/ondrej-bojar/>Ond≈ôej Bojar</a><br><a href=/volumes/W18-64/ class=text-muted>Proceedings of the Third Conference on Machine Translation: Shared Task Papers</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6416><div class="card-body p-3 small">We participated in the WMT 2018 shared news translation task in three language pairs : <a href=https://en.wikipedia.org/wiki/Estonian_language>English-Estonian</a>, <a href=https://en.wikipedia.org/wiki/Finnish_language>English-Finnish</a>, and <a href=https://en.wikipedia.org/wiki/Czech_language>English-Czech</a>. Our main focus was the low-resource language pair of Estonian and English for which we utilized Finnish parallel data in a simple method. We first train a parent model for the high-resource language pair followed by <a href=https://en.wikipedia.org/wiki/Adaptation>adaptation</a> on the related low-resource language pair. This approach brings a substantial performance boost over the <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline system</a> trained only on Estonian-English parallel data. Our <a href=https://en.wikipedia.org/wiki/System>systems</a> are based on the Transformer architecture. For the English to Czech translation, we have evaluated our last year models of hybrid phrase-based approach and <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a> mainly for comparison purposes.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2018.iwslt-1.21.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2018--iwslt-1--21 data-toggle=collapse aria-expanded=false aria-controls=abstract-2018.iwslt-1.21 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2018.iwslt-1.21/>CUNI Basque-to-English Submission in IWSLT18<span class=acl-fixed-case>CUNI</span> <span class=acl-fixed-case>B</span>asque-to-<span class=acl-fixed-case>E</span>nglish Submission in <span class=acl-fixed-case>IWSLT</span>18</a></strong><br><a href=/people/t/tom-kocmi/>Tom Kocmi</a>
|
<a href=/people/d/dusan-varis/>Du≈°an Vari≈°</a>
|
<a href=/people/o/ondrej-bojar/>Ond≈ôej Bojar</a><br><a href=/volumes/2018.iwslt-1/ class=text-muted>Proceedings of the 15th International Conference on Spoken Language Translation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2018--iwslt-1--21><div class="card-body p-3 small">We present our submission to the IWSLT18 Low Resource task focused on the translation from Basque-to-English. Our submission is based on the current state-of-the-art self-attentive neural network architecture, Transformer. We further improve this strong <a href=https://en.wikipedia.org/wiki/Baseline_(medicine)>baseline</a> by exploiting available <a href=https://en.wikipedia.org/wiki/Monolingualism>monolingual data</a> using the back-translation technique. We also present further improvements gained by a <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a>, a technique that trains a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> using a high-resource language pair (Czech-English) and then fine-tunes the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> using the target low-resource language pair (Basque-English).</div></div><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5715.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5715 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5715 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5715/>CUNI NMT System for WAT 2017 Translation Tasks<span class=acl-fixed-case>CUNI</span> <span class=acl-fixed-case>NMT</span> System for <span class=acl-fixed-case>WAT</span> 2017 Translation Tasks</a></strong><br><a href=/people/t/tom-kocmi/>Tom Kocmi</a>
|
<a href=/people/d/dusan-varis/>Du≈°an Vari≈°</a>
|
<a href=/people/o/ondrej-bojar/>Ond≈ôej Bojar</a><br><a href=/volumes/W17-57/ class=text-muted>Proceedings of the 4th Workshop on Asian Translation (WAT2017)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5715><div class="card-body p-3 small">The paper presents this year&#8217;s CUNI submissions to the WAT 2017 Translation Task focusing on the Japanese-English translation, namely Scientific papers subtask, Patents subtask and Newswire subtask. We compare two neural network architectures, the standard sequence-to-sequence with attention (Seq2Seq) and an architecture using convolutional sentence encoder (FBConv2Seq), both implemented in the NMT framework Neural Monkey that we currently participate in developing. We also compare various types of preprocessing of the source <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>Japanese sentences</a> and their impact on the overall results. Furthermore, we include the results of our experiments with out-of-domain data obtained by combining the corpora provided for each subtask.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1087.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1087 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1087 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=E17-1087" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/E17-1087/>LanideNN : Multilingual Language Identification on Character Window<span class=acl-fixed-case>L</span>anide<span class=acl-fixed-case>NN</span>: Multilingual Language Identification on Character Window</a></strong><br><a href=/people/t/tom-kocmi/>Tom Kocmi</a>
|
<a href=/people/o/ondrej-bojar/>Ond≈ôej Bojar</a><br><a href=/volumes/E17-1/ class=text-muted>Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1087><div class="card-body p-3 small">In <a href=https://en.wikipedia.org/wiki/Language_identification>language identification</a>, a common first step in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a>, we want to automatically determine the language of some input text. Monolingual language identification assumes that the given document is written in one language. In multilingual language identification, the document is usually in two or three languages and we just want their names. We aim one step further and propose a method for textual language identification where languages can change arbitrarily and the goal is to identify the spans of each of the languages. Our method is based on Bidirectional Recurrent Neural Networks and it performs well in monolingual and multilingual language identification tasks on six datasets covering 131 languages. The method keeps the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> also for short documents and across domains, so it is ideal for off-the-shelf use without preparation of training data.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Tom+Kocmi" title="Search for 'Tom Kocmi' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/o/ondrej-bojar/ class=align-middle>Ond≈ôej Bojar</a>
<span class="badge badge-secondary align-middle ml-2">9</span></li><li class=list-group-item><a href=/people/d/dusan-varis/ class=align-middle>Dusan Varis</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/l/loic-barrault/ class=align-middle>Lo√Øc Barrault</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/f/fethi-bougares/ class=align-middle>Fethi Bougares</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/rajen-chatterjee/ class=align-middle>Rajen Chatterjee</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/m/marta-r-costa-jussa/ class=align-middle>Marta R. Costa-juss√†</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/christian-federmann/ class=align-middle>Christian Federmann</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/mark-fishel/ class=align-middle>Mark Fishel</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/alexander-fraser/ class=align-middle>Alexander Fraser</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/markus-freitag/ class=align-middle>Markus Freitag</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yvette-graham/ class=align-middle>Yvette Graham</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/roman-grundkiewicz/ class=align-middle>Roman Grundkiewicz</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/p/paco-guzman/ class=align-middle>Paco Guzman</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/b/barry-haddow/ class=align-middle>Barry Haddow</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/matthias-huck/ class=align-middle>Matthias Huck</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/antonio-jimeno-yepes/ class=align-middle>Antonio Jimeno Yepes</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/p/philipp-koehn/ class=align-middle>Philipp Koehn</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/andre-f-t-martins/ class=align-middle>Andr√© F. T. Martins</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/makoto-morishita/ class=align-middle>Makoto Morishita</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/christof-monz/ class=align-middle>Christof Monz</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/roman-sudarikov/ class=align-middle>Roman Sudarikov</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/tomasz-limisiewicz/ class=align-middle>Tomasz Limisiewicz</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/g/gabriel-stanovsky/ class=align-middle>Gabriel Stanovsky</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/i/ivana-kvapilikova/ class=align-middle>Ivana Kvapil√≠kov√°</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/ws/ class=align-middle>WS</a><span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/venues/wmt/ class=align-middle>WMT</a><span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/venues/eamt/ class=align-middle>EAMT</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/eacl/ class=align-middle>EACL</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/iwslt/ class=align-middle>IWSLT</a><span class="badge badge-secondary align-middle ml-2">1</span></li></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ¬©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>