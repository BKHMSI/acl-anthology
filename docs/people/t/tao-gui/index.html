<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Tao Gui - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Tao</span> <span class=font-weight-bold>Gui</span></h2><hr><div class=row><div class=col-lg-9><h4>2022</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.acl-long.157.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--acl-long--157 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.acl-long.157 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.acl-long.157" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2022.acl-long.157/>Robust Lottery Tickets for Pre-trained Language Models</a></strong><br><a href=/people/r/rui-zheng/>Rui Zheng</a>
|
<a href=/people/b/bao-rong/>Bao Rong</a>
|
<a href=/people/y/yuhao-zhou/>Yuhao Zhou</a>
|
<a href=/people/d/di-liang/>Di Liang</a>
|
<a href=/people/s/sirui-wang/>Sirui Wang</a>
|
<a href=/people/w/wei-wu/>Wei Wu</a>
|
<a href=/people/t/tao-gui/>Tao Gui</a>
|
<a href=/people/q/qi-zhang/>Qi Zhang</a>
|
<a href=/people/x/xuan-jing-huang/>Xuanjing Huang</a><br><a href=/volumes/2022.acl-long/ class=text-muted>Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--acl-long--157><div class="card-body p-3 small">Recent works on Lottery Ticket Hypothesis have shown that pre-trained language models (PLMs) contain smaller matching subnetworks(winning tickets) which are capable of reaching accuracy comparable to the original models. However, these tickets are proved to be notrobust to adversarial examples, and even worse than their PLM counterparts. To address this problem, we propose a novel method based on learning binary weight masks to identify robust tickets hidden in the original PLMs. Since the loss is not differentiable for the binary mask, we assign the hard concrete distribution to the masks and encourage their sparsity using a smoothing approximation of L0 regularization.Furthermore, we design an adversarial loss objective to guide the search for robust tickets and ensure that the tickets perform well bothin accuracy and robustness. Experimental results show the significant improvement of the proposed method over previous work on adversarial robustness evaluation.</div></div><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.451.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--451 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.451 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.451" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.451/>A Unified Generative Framework for Various NER Subtasks<span class=acl-fixed-case>NER</span> Subtasks</a></strong><br><a href=/people/h/hang-yan/>Hang Yan</a>
|
<a href=/people/t/tao-gui/>Tao Gui</a>
|
<a href=/people/j/junqi-dai/>Junqi Dai</a>
|
<a href=/people/q/qipeng-guo/>Qipeng Guo</a>
|
<a href=/people/z/zheng-zhang/>Zheng Zhang</a>
|
<a href=/people/x/xipeng-qiu/>Xipeng Qiu</a><br><a href=/volumes/2021.acl-long/ class=text-muted>Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--451><div class="card-body p-3 small">Named Entity Recognition (NER) is the task of identifying spans that represent entities in sentences. Whether the entity spans are nested or discontinuous, the NER task can be categorized into the flat NER, nested NER, and discontinuous NER subtasks. These subtasks have been mainly solved by the token-level sequence labelling or span-level classification. However, these <a href=https://en.wikipedia.org/wiki/Solution>solutions</a> can hardly tackle the three kinds of NER subtasks concurrently. To that end, we propose to formulate the NER subtasks as an entity span sequence generation task, which can be solved by a unified sequence-to-sequence (Seq2Seq) framework. Based on our unified framework, we can leverage the pre-trained Seq2Seq model to solve all three kinds of NER subtasks without the special design of the tagging schema or ways to enumerate spans. We exploit three types of <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entity representations</a> to linearize entities into a sequence. Our proposed framework is easy-to-implement and achieves state-of-the-art (SoTA) or near SoTA performance on eight English NER datasets, including two flat NER datasets, three nested NER datasets, and three discontinuous NER datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-demo.41.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-demo--41 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-demo.41 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-demo.41/>TextFlint : Unified Multilingual Robustness Evaluation Toolkit for <a href=https://en.wikipedia.org/wiki/Natural_language_processing>Natural Language Processing</a><span class=acl-fixed-case>T</span>ext<span class=acl-fixed-case>F</span>lint: Unified Multilingual Robustness Evaluation Toolkit for Natural Language Processing</a></strong><br><a href=/people/x/xiao-wang/>Xiao Wang</a>
|
<a href=/people/q/qin-liu/>Qin Liu</a>
|
<a href=/people/t/tao-gui/>Tao Gui</a>
|
<a href=/people/q/qi-zhang/>Qi Zhang</a>
|
<a href=/people/y/yicheng-zou/>Yicheng Zou</a>
|
<a href=/people/x/xin-zhou/>Xin Zhou</a>
|
<a href=/people/j/jiacheng-ye/>Jiacheng Ye</a>
|
<a href=/people/y/yongxin-zhang/>Yongxin Zhang</a>
|
<a href=/people/r/rui-zheng/>Rui Zheng</a>
|
<a href=/people/z/zexiong-pang/>Zexiong Pang</a>
|
<a href=/people/q/qinzhuo-wu/>Qinzhuo Wu</a>
|
<a href=/people/z/zhengyan-li/>Zhengyan Li</a>
|
<a href=/people/c/chong-zhang/>Chong Zhang</a>
|
<a href=/people/r/ruotian-ma/>Ruotian Ma</a>
|
<a href=/people/z/zichu-fei/>Zichu Fei</a>
|
<a href=/people/r/ruijian-cai/>Ruijian Cai</a>
|
<a href=/people/j/jun-zhao/>Jun Zhao</a>
|
<a href=/people/x/xingwu-hu/>Xingwu Hu</a>
|
<a href=/people/z/zhiheng-yan/>Zhiheng Yan</a>
|
<a href=/people/y/yiding-tan/>Yiding Tan</a>
|
<a href=/people/y/yuan-hu/>Yuan Hu</a>
|
<a href=/people/q/qiyuan-bian/>Qiyuan Bian</a>
|
<a href=/people/z/zhihua-liu/>Zhihua Liu</a>
|
<a href=/people/s/shan-qin/>Shan Qin</a>
|
<a href=/people/b/bolin-zhu/>Bolin Zhu</a>
|
<a href=/people/x/xiaoyu-xing/>Xiaoyu Xing</a>
|
<a href=/people/j/jinlan-fu/>Jinlan Fu</a>
|
<a href=/people/y/yue-zhang/>Yue Zhang</a>
|
<a href=/people/m/minlong-peng/>Minlong Peng</a>
|
<a href=/people/x/xiaoqing-zheng/>Xiaoqing Zheng</a>
|
<a href=/people/y/yaqian-zhou/>Yaqian Zhou</a>
|
<a href=/people/z/zhongyu-wei/>Zhongyu Wei</a>
|
<a href=/people/x/xipeng-qiu/>Xipeng Qiu</a>
|
<a href=/people/x/xuan-jing-huang/>Xuanjing Huang</a><br><a href=/volumes/2021.acl-demo/ class=text-muted>Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-demo--41><div class="card-body p-3 small">TextFlint is a multilingual robustness evaluation toolkit for NLP tasks that incorporates universal text transformation, task-specific transformation, adversarial attack, subpopulation, and their combinations to provide comprehensive robustness analyses. This enables practitioners to automatically evaluate their <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> from various aspects or to customize their evaluations as desired with just a few lines of code. TextFlint also generates complete analytical reports as well as targeted augmented data to address the shortcomings of the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> in terms of its <a href=https://en.wikipedia.org/wiki/Robustness_(computer_science)>robustness</a>. To guarantee acceptability, all the text transformations are linguistically based and all the transformed data selected (up to 100,000 texts) scored highly under human evaluation. To validate the utility, we performed large-scale empirical evaluations (over 67,000) on state-of-the-art <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning models</a>, classic <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised methods</a>, and real-world systems. The toolkit is already available at https://github.com/textflint with all the evaluation results demonstrated at textflint.io.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.765.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--765 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.765 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.765" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.765/>A Relation-Oriented Clustering Method for Open Relation Extraction</a></strong><br><a href=/people/j/jun-zhao/>Jun Zhao</a>
|
<a href=/people/t/tao-gui/>Tao Gui</a>
|
<a href=/people/q/qi-zhang/>Qi Zhang</a>
|
<a href=/people/y/yaqian-zhou/>Yaqian Zhou</a><br><a href=/volumes/2021.emnlp-main/ class=text-muted>Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--765><div class="card-body p-3 small">The clustering-based unsupervised relation discovery method has gradually become one of the important methods of open relation extraction (OpenRE). However, high-dimensional vectors can encode complex linguistic information which leads to the problem that the derived clusters can not explicitly align with the relational semantic classes. In this work, we propose a relation-oriented clustering model and use it to identify the novel relations in the unlabeled data. Specifically, to enable the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> to learn to cluster <a href=https://en.wikipedia.org/wiki/Relational_model>relational data</a>, our method leverages the readily available labeled data of pre-defined relations to learn a relation-oriented representation. We minimize distance between the instance with same relation by gathering the instances towards their corresponding relation centroids to form a cluster structure, so that the learned representation is cluster-friendly. To reduce the clustering bias on predefined classes, we optimize the model by minimizing a joint objective on both labeled and unlabeled data. Experimental results show that our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> reduces the <a href=https://en.wikipedia.org/wiki/Error_rate>error rate</a> by 29.2 % and 15.7 %, on two datasets respectively, compared with current SOTA methods.</div></div><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1096.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1096 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1096 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1096/>A Lexicon-Based Graph Neural Network for Chinese NER<span class=acl-fixed-case>C</span>hinese <span class=acl-fixed-case>NER</span></a></strong><br><a href=/people/t/tao-gui/>Tao Gui</a>
|
<a href=/people/y/yicheng-zou/>Yicheng Zou</a>
|
<a href=/people/q/qi-zhang/>Qi Zhang</a>
|
<a href=/people/m/minlong-peng/>Minlong Peng</a>
|
<a href=/people/j/jinlan-fu/>Jinlan Fu</a>
|
<a href=/people/z/zhongyu-wei/>Zhongyu Wei</a>
|
<a href=/people/x/xuan-jing-huang/>Xuanjing Huang</a><br><a href=/volumes/D19-1/ class=text-muted>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1096><div class="card-body p-3 small">Recurrent neural networks (RNN) used for Chinese named entity recognition (NER) that sequentially track character and word information have achieved great success. However, the characteristic of chain structure and the lack of global semantics determine that RNN-based models are vulnerable to <a href=https://en.wikipedia.org/wiki/Ambiguity>word ambiguities</a>. In this work, we try to alleviate this problem by introducing a lexicon-based graph neural network with global semantics, in which lexicon knowledge is used to connect characters to capture the local composition, while a global relay node can capture global sentence semantics and long-range dependency. Based on the multiple graph-based interactions among characters, potential words, and the whole-sentence semantics, word ambiguities can be effectively tackled. Experiments on four NER datasets show that the proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieves significant improvements against other baseline models.</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1275.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1275 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1275 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-1275/>Transferring from Formal Newswire Domain with Hypernet for Twitter POS Tagging<span class=acl-fixed-case>T</span>witter <span class=acl-fixed-case>POS</span> Tagging</a></strong><br><a href=/people/t/tao-gui/>Tao Gui</a>
|
<a href=/people/q/qi-zhang/>Qi Zhang</a>
|
<a href=/people/j/jingjing-gong/>Jingjing Gong</a>
|
<a href=/people/m/minlong-peng/>Minlong Peng</a>
|
<a href=/people/d/di-liang/>Di Liang</a>
|
<a href=/people/k/keyu-ding/>Keyu Ding</a>
|
<a href=/people/x/xuan-jing-huang/>Xuanjing Huang</a><br><a href=/volumes/D18-1/ class=text-muted>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1275><div class="card-body p-3 small">Part-of-Speech (POS) tagging for <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a> has received considerable attention in recent years. Because most POS tagging methods are based on <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised models</a>, they usually require a large amount of labeled data for training. However, the existing labeled datasets for <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a> are much smaller than those for newswire text. Hence, to help POS tagging for <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a>, most domain adaptation methods try to leverage newswire datasets by learning the shared features between the two domains. However, from a linguistic perspective, Twitter users not only tend to mimic the formal expressions of traditional media, like <a href=https://en.wikipedia.org/wiki/News>news</a>, but they also appear to be developing linguistically informal styles. Therefore, <a href=https://en.wikipedia.org/wiki/POS_tagging>POS tagging</a> for the formal Twitter context can be learned together with the newswire dataset, while <a href=https://en.wikipedia.org/wiki/POS_tagging>POS tagging</a> for the informal Twitter context should be learned separately. To achieve this task, in this work, we propose a hypernetwork-based method to generate different parameters to separately model contexts with different expression styles. Experimental results on three different datasets show that our approach achieves better performance than state-of-the-art methods in most cases.</div></div><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1256.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1256 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1256 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/238235246 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D17-1256/>Part-of-Speech Tagging for <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a> with Adversarial Neural Networks<span class=acl-fixed-case>T</span>witter with Adversarial Neural Networks</a></strong><br><a href=/people/t/tao-gui/>Tao Gui</a>
|
<a href=/people/q/qi-zhang/>Qi Zhang</a>
|
<a href=/people/h/haoran-huang/>Haoran Huang</a>
|
<a href=/people/m/minlong-peng/>Minlong Peng</a>
|
<a href=/people/x/xuan-jing-huang/>Xuanjing Huang</a><br><a href=/volumes/D17-1/ class=text-muted>Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1256><div class="card-body p-3 small">In this work, we study the problem of <a href=https://en.wikipedia.org/wiki/Part-of-speech_tagging>part-of-speech tagging</a> for <a href=https://en.wikipedia.org/wiki/Twitter>Tweets</a>. In contrast to newswire articles, <a href=https://en.wikipedia.org/wiki/Twitter>Tweets</a> are usually informal and contain numerous out-of-vocabulary words. Moreover, there is a lack of large scale labeled datasets for this <a href=https://en.wikipedia.org/wiki/Domain_(mathematical_analysis)>domain</a>. To tackle these challenges, we propose a novel <a href=https://en.wikipedia.org/wiki/Neural_network>neural network</a> to make use of out-of-domain labeled data, unlabeled in-domain data, and labeled in-domain data. Inspired by adversarial neural networks, the proposed method tries to learn common features through adversarial discriminator. In addition, we hypothesize that domain-specific features of target domain should be preserved in some degree. Hence, the proposed <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>method</a> adopts a sequence-to-sequence autoencoder to perform this <a href=https://en.wikipedia.org/wiki/Task_(computing)>task</a>. Experimental results on three different datasets show that our <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>method</a> achieves better performance than state-of-the-art methods.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Tao+Gui" title="Search for 'Tao Gui' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/q/qi-zhang/ class=align-middle>Qi Zhang</a>
<span class="badge badge-secondary align-middle ml-2">6</span></li><li class=list-group-item><a href=/people/x/xuan-jing-huang/ class=align-middle>Xuan-Jing Huang</a>
<span class="badge badge-secondary align-middle ml-2">5</span></li><li class=list-group-item><a href=/people/m/minlong-peng/ class=align-middle>Minlong Peng</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/people/x/xipeng-qiu/ class=align-middle>Xipeng Qiu</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/y/yicheng-zou/ class=align-middle>Yicheng Zou</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/r/rui-zheng/ class=align-middle>Rui Zheng</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/j/jun-zhao/ class=align-middle>Jun Zhao</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/j/jinlan-fu/ class=align-middle>Jinlan Fu</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/y/yaqian-zhou/ class=align-middle>Yaqian Zhou</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/z/zhongyu-wei/ class=align-middle>Zhongyu Wei</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/d/di-liang/ class=align-middle>Di Liang</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/h/hang-yan/ class=align-middle>Hang Yan</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/junqi-dai/ class=align-middle>Junqi Dai</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/q/qipeng-guo/ class=align-middle>Qipeng Guo</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/zheng-zhang/ class=align-middle>Zheng Zhang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/x/xiao-wang/ class=align-middle>Xiao Wang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/q/qin-liu/ class=align-middle>Qin Liu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/x/xin-zhou/ class=align-middle>Xin Zhou</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jiacheng-ye/ class=align-middle>Jiacheng Ye</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yongxin-zhang/ class=align-middle>Yongxin Zhang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/zexiong-pang/ class=align-middle>Zexiong Pang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/q/qinzhuo-wu/ class=align-middle>Qinzhuo Wu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/zhengyan-li/ class=align-middle>Zhengyan Li</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/chong-zhang/ class=align-middle>Chong Zhang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/ruotian-ma/ class=align-middle>Ruotian Ma</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/zichu-fei/ class=align-middle>Zichu Fei</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/ruijian-cai/ class=align-middle>Ruijian Cai</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/x/xingwu-hu/ class=align-middle>Xingwu Hu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/zhiheng-yan/ class=align-middle>Zhiheng Yan</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yiding-tan/ class=align-middle>Yiding Tan</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yuan-hu/ class=align-middle>Yuan Hu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/q/qiyuan-bian/ class=align-middle>Qiyuan Bian</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/zhihua-liu/ class=align-middle>Zhihua Liu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/shan-qin/ class=align-middle>Shan Qin</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/b/bolin-zhu/ class=align-middle>Bolin Zhu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/x/xiaoyu-xing/ class=align-middle>Xiaoyu Xing</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yue-zhang/ class=align-middle>Yue Zhang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/x/xiaoqing-zheng/ class=align-middle>Xiaoqing Zheng</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/b/bao-rong/ class=align-middle>Bao Rong</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yuhao-zhou/ class=align-middle>Yuhao Zhou</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/sirui-wang/ class=align-middle>Sirui Wang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/w/wei-wu/ class=align-middle>Wei Wu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jingjing-gong/ class=align-middle>Jingjing Gong</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/keyu-ding/ class=align-middle>Keyu Ding</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/haoran-huang/ class=align-middle>Haoran Huang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/emnlp/ class=align-middle>EMNLP</a><span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/venues/acl/ class=align-middle>ACL</a><span class="badge badge-secondary align-middle ml-2">3</span></li></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright Â©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>