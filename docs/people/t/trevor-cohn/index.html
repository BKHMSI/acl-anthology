<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Trevor Cohn - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Trevor</span> <span class=font-weight-bold>Cohn</span></h2><p class="font-weight-light text-muted">University of Melbourne</p><p class="font-weight-light text-muted"><span class=font-italic>Other people with similar names:</span>
<a href=/people/t/trevor-cohen/>Trevor Cohen</a>
(University of Washington)</p><hr><div class=row><div class=col-lg-9><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.conll-1.38.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--conll-1--38 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.conll-1.38 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.conll-1.38/>Commonsense Knowledge in <a href=https://en.wikipedia.org/wiki/Word_association>Word Associations</a> and ConceptNet<span class=acl-fixed-case>C</span>oncept<span class=acl-fixed-case>N</span>et</a></strong><br><a href=/people/c/chunhua-liu/>Chunhua Liu</a>
|
<a href=/people/t/trevor-cohn/>Trevor Cohn</a>
|
<a href=/people/l/lea-frermann/>Lea Frermann</a><br><a href=/volumes/2021.conll-1/ class=text-muted>Proceedings of the 25th Conference on Computational Natural Language Learning</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--conll-1--38><div class="card-body p-3 small">Humans use countless basic, shared facts about the world to efficiently navigate in their environment. This <a href=https://en.wikipedia.org/wiki/Commonsense_knowledge>commonsense knowledge</a> is rarely communicated explicitly, however, understanding how <a href=https://en.wikipedia.org/wiki/Commonsense_knowledge>commonsense knowledge</a> is represented in different paradigms is important for (a) a deeper understanding of human cognition and (b) augmenting automatic reasoning systems. This paper presents an in-depth comparison of two large-scale resources of general knowledge : <a href=https://en.wikipedia.org/wiki/ConceptNet>ConceptNet</a>, an engineered relational database, and SWOW, a <a href=https://en.wikipedia.org/wiki/Knowledge_graph>knowledge graph</a> derived from crowd-sourced word associations. We examine the structure, overlap and differences between the two <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graphs</a>, as well as the extent of situational commonsense knowledge present in the two <a href=https://en.wikipedia.org/wiki/Factors_of_production>resources</a>. We finally show empirically that both resources improve downstream task performance on commonsense reasoning benchmarks over text-only baselines, suggesting that large-scale word association data, which have been obtained for several languages through crowd-sourcing, can be a valuable complement to curated knowledge graphs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.239.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--239 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.239 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.eacl-main.239" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.239/>Diverse Adversaries for Mitigating Bias in Training</a></strong><br><a href=/people/x/xudong-han/>Xudong Han</a>
|
<a href=/people/t/timothy-baldwin/>Timothy Baldwin</a>
|
<a href=/people/t/trevor-cohn/>Trevor Cohn</a><br><a href=/volumes/2021.eacl-main/ class=text-muted>Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--239><div class="card-body p-3 small">Adversarial learning can learn fairer and less biased models of language processing than standard training. However, current adversarial techniques only partially mitigate the problem of model bias, added to which their training procedures are often unstable. In this paper, we propose a novel approach to <a href=https://en.wikipedia.org/wiki/Adversarial_learning>adversarial learning</a> based on the use of multiple diverse discriminators, whereby <a href=https://en.wikipedia.org/wiki/Discriminator>discriminators</a> are encouraged to learn orthogonal hidden representations from one another. Experimental results show that our method substantially improves over standard adversarial removal methods, in terms of reducing bias and stability of training.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.155.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--155 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.155 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.155/>Fairness-aware Class Imbalanced Learning</a></strong><br><a href=/people/s/shivashankar-subramanian/>Shivashankar Subramanian</a>
|
<a href=/people/a/afshin-rahimi/>Afshin Rahimi</a>
|
<a href=/people/t/timothy-baldwin/>Timothy Baldwin</a>
|
<a href=/people/t/trevor-cohn/>Trevor Cohn</a>
|
<a href=/people/l/lea-frermann/>Lea Frermann</a><br><a href=/volumes/2021.emnlp-main/ class=text-muted>Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--155><div class="card-body p-3 small">Class imbalance is a common challenge in many NLP tasks, and has clear connections to <a href=https://en.wikipedia.org/wiki/Bias>bias</a>, in that bias in training data often leads to higher <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> for majority groups at the expense of minority groups. However there has traditionally been a disconnect between research on class-imbalanced learning and mitigating bias, and only recently have the two been looked at through a common lens. In this work we evaluate long-tail learning methods for tweet sentiment and occupation classification, and extend a margin-loss based approach with methods to enforce fairness. We empirically show through controlled experiments that the proposed approaches help mitigate both class imbalance and demographic biases.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.alta-1.13.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--alta-1--13 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.alta-1.13 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.alta-1.13/>Document Level Hierarchical Transformer</a></strong><br><a href=/people/n/najam-zaidi/>Najam Zaidi</a>
|
<a href=/people/t/trevor-cohn/>Trevor Cohn</a>
|
<a href=/people/g/gholamreza-haffari/>Gholamreza Haffari</a><br><a href=/volumes/2021.alta-1/ class=text-muted>Proceedings of the The 19th Annual Workshop of the Australasian Language Technology Association</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--alta-1--13><div class="card-body p-3 small">Generating long and coherent text is an important and challenging task encompassing many application areas such as <a href=https://en.wikipedia.org/wiki/Automatic_summarization>summarization</a>, document level machine translation and <a href=https://en.wikipedia.org/wiki/Storytelling>story generation</a>. Despite the success in modeling intra-sentence coherence, existing long text generation models (e.g., BART and GPT-3) still struggle to maintain a coherent event sequence throughout the generated text. We conjecture that this is because of the difficulty for the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> to revise, replace, revoke or delete any part that has been generated by the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>. In this paper, we present a novel semi-autoregressive document generation model capable of revising and editing the generated text. Building on recent models by (Gu et al., 2019 ; Xu and Carpuat, 2020) we propose document generation as a hierarchical Markov decision process with a two level hierarchy, where the high and low level editing programs. We train our model using imitation learning (Hussein et al., 2017) and introduce roll-in policy such that each <a href=https://en.wikipedia.org/wiki/Policy>policy</a> learns on the output of applying the previous action. Experiments applying the proposed approach sheds various insights on the problems of long text generation using our <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a>. We suggest various remedies such as using distilled dataset, designing better attention mechanisms and using <a href=https://en.wikipedia.org/wiki/Autoregressive_model>autoregressive models</a> as a <a href=https://en.wikipedia.org/wiki/Low-level_programming_language>low level program</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.semeval-1.54.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--semeval-1--54 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.semeval-1.54 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.semeval-1.54/>PTST-UoM at SemEval-2021 Task 10 : Parsimonious Transfer for Sequence Tagging<span class=acl-fixed-case>PTST</span>-<span class=acl-fixed-case>U</span>o<span class=acl-fixed-case>M</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2021 Task 10: Parsimonious Transfer for Sequence Tagging</a></strong><br><a href=/people/k/kemal-kurniawan/>Kemal Kurniawan</a>
|
<a href=/people/l/lea-frermann/>Lea Frermann</a>
|
<a href=/people/p/philip-schulz/>Philip Schulz</a>
|
<a href=/people/t/trevor-cohn/>Trevor Cohn</a><br><a href=/volumes/2021.semeval-1/ class=text-muted>Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--semeval-1--54><div class="card-body p-3 small">This paper describes PTST, a source-free unsupervised domain adaptation technique for sequence tagging, and its application to the SemEval-2021 Task 10 on time expression recognition. PTST is an extension of the cross-lingual parsimonious parser transfer framework, which uses high-probability predictions of the source model as a supervision signal in self-training. We extend the <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> to a sequence prediction setting, and demonstrate its applicability to unsupervised domain adaptation. PTST achieves F1 score of 79.6 % on the official test set, with the <a href=https://en.wikipedia.org/wiki/Precision_(statistics)>precision</a> of 90.1 %, the highest out of 14 submissions.</div></div><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.0/>Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</a></strong><br><a href=/people/b/bonnie-webber/>Bonnie Webber</a>
|
<a href=/people/t/trevor-cohn/>Trevor Cohn</a>
|
<a href=/people/y/yulan-he/>Yulan He</a>
|
<a href=/people/y/yang-liu-icsi/>Yang Liu</a><br><a href=/volumes/2020.emnlp-main/ class=text-muted>Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.0/>Findings of the Association for Computational Linguistics: EMNLP 2020</a></strong><br><a href=/people/t/trevor-cohn/>Trevor Cohn</a>
|
<a href=/people/y/yulan-he/>Yulan He</a>
|
<a href=/people/y/yang-liu-icsi/>Yang Liu</a><br><a href=/volumes/2020.findings-emnlp/ class=text-muted>Findings of the Association for Computational Linguistics: EMNLP 2020</a></span></p><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1182.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1182 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1182 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1182" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1182/>Deep Ordinal Regression for Pledge Specificity Prediction</a></strong><br><a href=/people/s/shivashankar-subramanian/>Shivashankar Subramanian</a>
|
<a href=/people/t/trevor-cohn/>Trevor Cohn</a>
|
<a href=/people/t/timothy-baldwin/>Timothy Baldwin</a><br><a href=/volumes/D19-1/ class=text-muted>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1182><div class="card-body p-3 small">Many pledges are made in the course of an <a href=https://en.wikipedia.org/wiki/Political_campaign>election campaign</a>, forming important corpora for political analysis of campaign strategy and governmental accountability. At present, there are no publicly available annotated datasets of pledges, and most political analyses rely on manual annotations. In this paper we collate a novel <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> of manifestos from eleven <a href=https://en.wikipedia.org/wiki/Elections_in_Australia>Australian federal election cycles</a>, with over 12,000 sentences annotated with <a href=https://en.wikipedia.org/wiki/Sensitivity_and_specificity>specificity</a> (e.g., rhetorical vs detailed pledge) on a <a href=https://en.wikipedia.org/wiki/Scale_(social_sciences)>fine-grained scale</a>. We propose deep ordinal regression approaches for specificity prediction, under both supervised and semi-supervised settings, and provide empirical results demonstrating the effectiveness of the proposed techniques over several baseline approaches. We analyze the utility of pledge specificity modeling across a spectrum of policy issues in performing ideology prediction, and further provide qualitative analysis in terms of capturing party-specific issue salience across election cycles.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5304.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5304 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5304 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-5304.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D19-5304/>Neural Speech Translation using <a href=https://en.wikipedia.org/wiki/Lattice_model_(physics)>Lattice Transformations</a> and Graph Networks</a></strong><br><a href=/people/d/daniel-beck/>Daniel Beck</a>
|
<a href=/people/t/trevor-cohn/>Trevor Cohn</a>
|
<a href=/people/g/gholamreza-haffari/>Gholamreza Haffari</a><br><a href=/volumes/D19-53/ class=text-muted>Proceedings of the Thirteenth Workshop on Graph-Based Methods for Natural Language Processing (TextGraphs-13)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5304><div class="card-body p-3 small">Speech translation systems usually follow a pipeline approach, using word lattices as an <a href=https://en.wikipedia.org/wiki/Intermediate_representation>intermediate representation</a>. However, previous work assume access to the original <a href=https://en.wikipedia.org/wiki/Transcription_(linguistics)>transcriptions</a> used to train the ASR system, which can limit applicability in real scenarios. In this work we propose an approach for <a href=https://en.wikipedia.org/wiki/Speech_translation>speech translation</a> through lattice transformations and neural models based on <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph networks</a>. Experimental results show that our approach reaches competitive performance without relying on <a href=https://en.wikipedia.org/wiki/Transcription_(biology)>transcriptions</a>, while also being orders of magnitude faster than previous work.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-6405.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-6405 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-6405 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-6405/>On the Role of Scene Graphs in Image Captioning</a></strong><br><a href=/people/d/dalin-wang/>Dalin Wang</a>
|
<a href=/people/d/daniel-beck/>Daniel Beck</a>
|
<a href=/people/t/trevor-cohn/>Trevor Cohn</a><br><a href=/volumes/D19-64/ class=text-muted>Proceedings of the Beyond Vision and LANguage: inTEgrating Real-world kNowledge (LANTERN)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-6405><div class="card-body p-3 small">Scene graphs represent semantic information in images, which can help <a href=https://en.wikipedia.org/wiki/Image>image captioning system</a> to produce more descriptive outputs versus using only the <a href=https://en.wikipedia.org/wiki/Image>image</a> as context. Recent captioning approaches rely on ad-hoc approaches to obtain <a href=https://en.wikipedia.org/wiki/Graph_of_a_function>graphs</a> for <a href=https://en.wikipedia.org/wiki/Image>images</a>. However, those <a href=https://en.wikipedia.org/wiki/Graph_of_a_function>graphs</a> introduce <a href=https://en.wikipedia.org/wiki/Noise_(electronics)>noise</a> and it is unclear the effect of <a href=https://en.wikipedia.org/wiki/Parsing>parser errors</a> on captioning accuracy. In this work, we investigate to what extent <a href=https://en.wikipedia.org/wiki/Scene_graph>scene graphs</a> can help image captioning. Our results show that a state-of-the-art scene graph parser can boost performance almost as much as the ground truth graphs, showing that the bottleneck currently resides more on the captioning models than on the performance of the scene graph parser.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1203.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1203 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1203 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/354264026 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1203/>Contextualization of Morphological Inflection</a></strong><br><a href=/people/e/ekaterina-vylomova/>Ekaterina Vylomova</a>
|
<a href=/people/r/ryan-cotterell/>Ryan Cotterell</a>
|
<a href=/people/t/trevor-cohn/>Trevor Cohn</a>
|
<a href=/people/t/timothy-baldwin/>Timothy Baldwin</a>
|
<a href=/people/j/jason-eisner/>Jason Eisner</a><br><a href=/volumes/N19-1/ class=text-muted>Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1203><div class="card-body p-3 small">Critical to <a href=https://en.wikipedia.org/wiki/Natural-language_generation>natural language generation</a> is the production of correctly inflected text. In this paper, we isolate the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> of predicting a fully inflected sentence from its partially lemmatized version. Unlike traditional <a href=https://en.wikipedia.org/wiki/Inflection>morphological inflection</a> or surface realization, our task input does not provide gold tags that specify what morphological features to realize on each lemmatized word ; rather, such features must be inferred from sentential context. We develop a neural hybrid graphical model that explicitly reconstructs <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphological features</a> before predicting the <a href=https://en.wikipedia.org/wiki/Inflection>inflected forms</a>, and compare this to a system that directly predicts the <a href=https://en.wikipedia.org/wiki/Inflection>inflected forms</a> without relying on any morphological annotation. We experiment on several typologically diverse languages from the Universal Dependencies treebanks, showing the utility of incorporating linguistically-motivated latent variables into NLP models.</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1310.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1310 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1310 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/306112516 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1310" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1310/>Evaluating the Utility of <a href=https://en.wikipedia.org/wiki/Handicraft>Hand-crafted Features</a> in Sequence Labelling</a></strong><br><a href=/people/m/minghao-wu/>Minghao Wu</a>
|
<a href=/people/f/fei-liu-unimelb/>Fei Liu</a>
|
<a href=/people/t/trevor-cohn/>Trevor Cohn</a><br><a href=/volumes/D18-1/ class=text-muted>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1310><div class="card-body p-3 small">Conventional wisdom is that hand-crafted features are redundant for deep learning models, as they already learn adequate representations of text automatically from corpora. In this work, we test this claim by proposing a new method for exploiting handcrafted features as part of a novel hybrid learning approach, incorporating a feature auto-encoder loss component. We evaluate on the task of named entity recognition (NER), where we show that including manual features for part-of-speech, word shapes and gazetteers can improve the performance of a neural CRF model. We obtain a F 1 of 91.89 for the CoNLL-2003 English shared task, which significantly outperforms a collection of highly competitive baseline models. We also present an ablation study showing the importance of auto-encoding, over using features as either inputs or outputs alone, and moreover, show including the autoencoder components reduces training requirements to 60 %, while retaining the same predictive accuracy.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6102.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6102 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6102 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6102/>Twitter Geolocation using Knowledge-Based Methods<span class=acl-fixed-case>T</span>witter Geolocation using Knowledge-Based Methods</a></strong><br><a href=/people/t/taro-miyazaki/>Taro Miyazaki</a>
|
<a href=/people/a/afshin-rahimi/>Afshin Rahimi</a>
|
<a href=/people/t/trevor-cohn/>Trevor Cohn</a>
|
<a href=/people/t/timothy-baldwin/>Timothy Baldwin</a><br><a href=/volumes/W18-61/ class=text-muted>Proceedings of the 2018 EMNLP Workshop W-NUT: The 4th Workshop on Noisy User-generated Text</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6102><div class="card-body p-3 small">Automatic geolocation of microblog posts from their text content is particularly difficult because many location-indicative terms are rare terms, notably entity names such as locations, people or local organisations. Their low frequency means that key terms observed in testing are often unseen in training, such that standard <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifiers</a> are unable to learn weights for them. We propose a method for reasoning over such terms using a <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge base</a>, through exploiting their relations with other entities. Our technique uses a <a href=https://en.wikipedia.org/wiki/Graph_embedding>graph embedding</a> over the <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge base</a>, which we couple with a text representation to learn a geolocation classifier, trained end-to-end. We show that our method improves over purely text-based methods, which we ascribe to more robust treatment of low-count and out-of-vocabulary entities.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1178.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1178 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1178 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://vimeo.com/277672945 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N18-1178/>Hierarchical Structured Model for Fine-to-Coarse Manifesto Text Analysis</a></strong><br><a href=/people/s/shivashankar-subramanian/>Shivashankar Subramanian</a>
|
<a href=/people/t/trevor-cohn/>Trevor Cohn</a>
|
<a href=/people/t/timothy-baldwin/>Timothy Baldwin</a><br><a href=/volumes/N18-1/ class=text-muted>Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1178><div class="card-body p-3 small">Election manifestos document the intentions, motives, and views of political parties. They are often used for analysing a party&#8217;s fine-grained position on a particular issue, as well as for coarse-grained positioning of a party on the leftright spectrum. In this paper we propose a two-stage model for automatically performing both levels of analysis over <a href=https://en.wikipedia.org/wiki/Manifesto>manifestos</a>. In the first step we employ a hierarchical multi-task structured deep model to predict fine- and coarse-grained positions, and in the second step we perform post-hoc calibration of coarse-grained positions using probabilistic soft logic. We empirically show that the proposed model outperforms state-of-art approaches at both granularities using manifestos from twelve countries, written in ten different languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2045.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2045 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2045 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N18-2045" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N18-2045/>Recurrent Entity Networks with Delayed Memory Update for Targeted Aspect-Based Sentiment Analysis</a></strong><br><a href=/people/f/fei-liu-unimelb/>Fei Liu</a>
|
<a href=/people/t/trevor-cohn/>Trevor Cohn</a>
|
<a href=/people/t/timothy-baldwin/>Timothy Baldwin</a><br><a href=/volumes/N18-2/ class=text-muted>Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2045><div class="card-body p-3 small">While <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a> have been shown to achieve impressive results for sentence-level sentiment analysis, targeted aspect-based sentiment analysis (TABSA) extraction of fine-grained opinion polarity w.r.t. a pre-defined set of aspects remains a difficult task. Motivated by recent advances in memory-augmented models for machine reading, we propose a novel architecture, utilising external memory chains with a delayed memory update mechanism to track entities. On a TABSA task, the proposed model demonstrates substantial improvements over state-of-the-art approaches, including those using external knowledge bases.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/U18-1001.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-U18-1001 data-toggle=collapse aria-expanded=false aria-controls=abstract-U18-1001 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/U18-1001/>Improved Neural Machine Translation using Side Information</a></strong><br><a href=/people/c/cong-duy-vu-hoang/>Cong Duy Vu Hoang</a>
|
<a href=/people/g/gholamreza-haffari/>Gholamreza Haffari</a>
|
<a href=/people/t/trevor-cohn/>Trevor Cohn</a><br><a href=/volumes/U18-1/ class=text-muted>Proceedings of the Australasian Language Technology Association Workshop 2018</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-U18-1001><div class="card-body p-3 small">In this work, we investigate whether side information is helpful in neural machine translation (NMT). We study various kinds of side information, including topical information, <a href=https://en.wikipedia.org/wiki/Trait_theory>personal trait</a>, then propose different ways of incorporating them into the existing NMT models. Our experimental results show the benefits of side information in improving the NMT models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1187.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1187 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1187 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1187.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1187.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285805016 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-1187/>Semi-supervised User Geolocation via Graph Convolutional Networks</a></strong><br><a href=/people/a/afshin-rahimi/>Afshin Rahimi</a>
|
<a href=/people/t/trevor-cohn/>Trevor Cohn</a>
|
<a href=/people/t/timothy-baldwin/>Timothy Baldwin</a><br><a href=/volumes/P18-1/ class=text-muted>Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1187><div class="card-body p-3 small">Social media user geolocation is vital to many <a href=https://en.wikipedia.org/wiki/Application_software>applications</a> such as <a href=https://en.wikipedia.org/wiki/Geolocation>event detection</a>. In this paper, we propose GCN, a multiview geolocation model based on Graph Convolutional Networks, that uses both text and network context. We compare GCN to the <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a>, and to two baselines we propose, and show that our model achieves or is competitive with the <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a> over three benchmark geolocation datasets when sufficient supervision is available. We also evaluate GCN under a minimal supervision scenario, and show it outperforms <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a>. We find that highway network gates are essential for controlling the amount of useful neighbourhood expansion in GCN.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2045.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2045 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2045 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-2045" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P18-2045/>Narrative Modeling with Memory Chains and Semantic Supervision</a></strong><br><a href=/people/f/fei-liu-unimelb/>Fei Liu</a>
|
<a href=/people/t/trevor-cohn/>Trevor Cohn</a>
|
<a href=/people/t/timothy-baldwin/>Timothy Baldwin</a><br><a href=/volumes/P18-2/ class=text-muted>Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2045><div class="card-body p-3 small">Story comprehension requires a deep semantic understanding of the narrative, making it a challenging task. Inspired by previous studies on ROC Story Cloze Test, we propose a novel method, tracking various semantic aspects with external neural memory chains while encouraging each to focus on a particular semantic aspect. Evaluated on the task of story ending prediction, our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> demonstrates superior performance to a collection of competitive baselines, setting a new state of the art.</div></div><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1056.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1056 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1056 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=I17-1056" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/I17-1056/>Capturing Long-range Contextual Dependencies with Memory-enhanced Conditional Random Fields</a></strong><br><a href=/people/f/fei-liu-utdallas/>Fei Liu</a>
|
<a href=/people/t/timothy-baldwin/>Timothy Baldwin</a>
|
<a href=/people/t/trevor-cohn/>Trevor Cohn</a><br><a href=/volumes/I17-1/ class=text-muted>Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1056><div class="card-body p-3 small">Despite successful applications across a broad range of NLP tasks, conditional random fields (CRFs), in particular the linear-chain variant, are only able to model local features. While this has important benefits in terms of inference tractability, it limits the ability of the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> to capture long-range dependencies between items. Attempts to extend CRFs to capture long-range dependencies have largely come at the cost of <a href=https://en.wikipedia.org/wiki/Computational_complexity_theory>computational complexity</a> and <a href=https://en.wikipedia.org/wiki/Approximate_inference>approximate inference</a>. In this work, we propose an extension to CRFs by integrating <a href=https://en.wikipedia.org/wiki/External_memory>external memory</a>, taking inspiration from memory networks, thereby allowing CRFs to incorporate information far beyond neighbouring steps. Experiments across two <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> show substantial improvements over strong CRF and LSTM baselines.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-2012.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-2012 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-2012 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/I17-2012.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/I17-2012/>Learning Kernels over Strings using <a href=https://en.wikipedia.org/wiki/Gaussian_function>Gaussian Processes</a><span class=acl-fixed-case>G</span>aussian Processes</a></strong><br><a href=/people/d/daniel-beck/>Daniel Beck</a>
|
<a href=/people/t/trevor-cohn/>Trevor Cohn</a><br><a href=/volumes/I17-2/ class=text-muted>Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-2012><div class="card-body p-3 small">Non-contiguous word sequences are widely known to be important in <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>modelling natural language</a>. However they not explicitly encoded in <a href=https://en.wikipedia.org/wiki/Universal_Coded_Character_Set>common text representations</a>. In this work we propose a <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> for <a href=https://en.wikipedia.org/wiki/Text_processing>text processing</a> using <a href=https://en.wikipedia.org/wiki/String_kernel>string kernels</a>, capable of flexibly representing non-contiguous sequences. Specifically, we derive a vectorised version of the string kernel algorithm and their <a href=https://en.wikipedia.org/wiki/Gradient>gradients</a>, allowing efficient <a href=https://en.wikipedia.org/wiki/Hyperparameter_optimization>hyperparameter optimisation</a> as part of a Gaussian Process framework. Experiments on synthetic data and text regression for emotion analysis show the promise of this technique.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1033.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1033 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1033 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P17-1033.Notes.zip data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234951882 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P17-1033" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-1033/>Topically Driven Neural Language Model</a></strong><br><a href=/people/j/jey-han-lau/>Jey Han Lau</a>
|
<a href=/people/t/timothy-baldwin/>Timothy Baldwin</a>
|
<a href=/people/t/trevor-cohn/>Trevor Cohn</a><br><a href=/volumes/P17-1/ class=text-muted>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1033><div class="card-body p-3 small">Language models are typically applied at the sentence level, without access to the broader document context. We present a neural language model that incorporates document context in the form of a topic model-like architecture, thus providing a succinct representation of the broader document context outside of the current sentence. Experiments over a range of datasets demonstrate that our <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> outperforms a pure sentence-based model in terms of language model perplexity, and leads to topics that are potentially more coherent than those produced by a standard LDA topic model. Our <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> also has the ability to generate related sentences for a topic, providing another way to interpret topics.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2033.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2033 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2033 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234946757 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-2033/>A Neural Model for User Geolocation and Lexical Dialectology</a></strong><br><a href=/people/a/afshin-rahimi/>Afshin Rahimi</a>
|
<a href=/people/t/trevor-cohn/>Trevor Cohn</a>
|
<a href=/people/t/timothy-baldwin/>Timothy Baldwin</a><br><a href=/volumes/P17-2/ class=text-muted>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2033><div class="card-body p-3 small">We propose a simple yet effective text-based user geolocation model based on a <a href=https://en.wikipedia.org/wiki/Neural_network>neural network</a> with one hidden layer, which achieves state of the art performance over three Twitter benchmark geolocation datasets, in addition to producing word and phrase embeddings in the hidden layer that we show to be useful for detecting dialectal terms. As part of our analysis of dialectal terms, we release DAREDS, a dataset for evaluating dialect term detection methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2093.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2093 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2093 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P17-2093.Datasets.zip data-toggle=tooltip data-placement=top title=Dataset><i class="fas fa-file-archive"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P17-2093" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P17-2093/>Model Transfer for Tagging Low-resource Languages using a <a href=https://en.wikipedia.org/wiki/Bilingual_dictionary>Bilingual Dictionary</a></a></strong><br><a href=/people/m/meng-fang/>Meng Fang</a>
|
<a href=/people/t/trevor-cohn/>Trevor Cohn</a><br><a href=/volumes/P17-2/ class=text-muted>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2093><div class="card-body p-3 small">Cross-lingual model transfer is a compelling and popular method for predicting annotations in a low-resource language, whereby parallel corpora provide a bridge to a high-resource language, and its associated annotated corpora. However, parallel data is not readily available for many languages, limiting the applicability of these approaches. We address these drawbacks in our <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> which takes advantage of cross-lingual word embeddings trained solely on a high coverage dictionary. We propose a novel neural network model for joint training from both sources of data based on cross-lingual word embeddings, and show substantial empirical improvements over baseline techniques. We also propose several active learning heuristics, which result in improvements over competitive benchmark methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1002.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1002 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1002 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1002/>Decoupling Encoder and Decoder Networks for Abstractive Document Summarization</a></strong><br><a href=/people/y/ying-xu/>Ying Xu</a>
|
<a href=/people/j/jey-han-lau/>Jey Han Lau</a>
|
<a href=/people/t/timothy-baldwin/>Timothy Baldwin</a>
|
<a href=/people/t/trevor-cohn/>Trevor Cohn</a><br><a href=/volumes/W17-10/ class=text-muted>Proceedings of the MultiLing 2017 Workshop on Summarization and Summary Evaluation Across Source Types and Genres</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1002><div class="card-body p-3 small">Abstractive document summarization seeks to automatically generate a summary for a document, based on some abstract understanding of the original document. State-of-the-art techniques traditionally use attentive encoderdecoder architectures. However, due to the large number of parameters in these <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a>, they require large training datasets and long training times. In this paper, we propose decoupling the encoder and decoder networks, and training them separately. We encode documents using an unsupervised document encoder, and then feed the document vector to a <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent neural network decoder</a>. With this decoupled architecture, we decrease the number of parameters in the <a href=https://en.wikipedia.org/wiki/Codec>decoder</a> substantially, and shorten its <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training time</a>. Experiments show that the decoupled model achieves comparable performance with state-of-the-art models for in-domain documents, but less well for out-of-domain documents.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4115.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4115 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4115 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4115/>Word Representation Models for Morphologically Rich Languages in <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a></a></strong><br><a href=/people/e/ekaterina-vylomova/>Ekaterina Vylomova</a>
|
<a href=/people/t/trevor-cohn/>Trevor Cohn</a>
|
<a href=/people/x/xuanli-he/>Xuanli He</a>
|
<a href=/people/g/gholamreza-haffari/>Gholamreza Haffari</a><br><a href=/volumes/W17-41/ class=text-muted>Proceedings of the First Workshop on Subword and Character Level Models in NLP</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4115><div class="card-body p-3 small">Out-of-vocabulary words present a great challenge for <a href=https://en.wikipedia.org/wiki/Machine_translation>Machine Translation</a>. Recently various character-level compositional models were proposed to address this issue. In current research we incorporate two most popular neural architectures, namely LSTM and CNN, into hard- and soft-attentional models of translation for character-level representation of the source. We propose semantic and morphological intrinsic evaluation of encoder-level representations. Our analysis of the learned representations reveals that character-based LSTM seems to be better at capturing morphological aspects compared to character-based CNN. We also show that hard-attentional model provides better character-level representations compared to vanilla one.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5404.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5404 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5404 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5404/>BIBI System Description : Building with CNNs and Breaking with <a href=https://en.wikipedia.org/wiki/Deep_learning>Deep Reinforcement Learning</a><span class=acl-fixed-case>BIBI</span> System Description: Building with <span class=acl-fixed-case>CNN</span>s and Breaking with Deep Reinforcement Learning</a></strong><br><a href=/people/y/yitong-li/>Yitong Li</a>
|
<a href=/people/t/trevor-cohn/>Trevor Cohn</a>
|
<a href=/people/t/timothy-baldwin/>Timothy Baldwin</a><br><a href=/volumes/W17-54/ class=text-muted>Proceedings of the First Workshop on Building Linguistically Generalizable NLP Systems</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5404><div class="card-body p-3 small">This paper describes our submission to the sentiment analysis sub-task of Build It, Break It : The Language Edition (BIBI), on both the builder and breaker sides. As a builder, we use <a href=https://en.wikipedia.org/wiki/Convolutional_neural_network>convolutional neural nets</a>, trained on both phrase and sentence data. As a breaker, we use <a href=https://en.wikipedia.org/wiki/Q-learning>Q-learning</a> to learn minimal change pairs, and apply a token substitution method automatically. We analyse the results to gauge the <a href=https://en.wikipedia.org/wiki/Robustness_(computer_science)>robustness</a> of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP systems</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1014.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1014 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1014 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D17-1014.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/238236392 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D17-1014/>Towards Decoding as Continuous Optimisation in <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a></a></strong><br><a href=/people/c/cong-duy-vu-hoang/>Cong Duy Vu Hoang</a>
|
<a href=/people/g/gholamreza-haffari/>Gholamreza Haffari</a>
|
<a href=/people/t/trevor-cohn/>Trevor Cohn</a><br><a href=/volumes/D17-1/ class=text-muted>Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1014><div class="card-body p-3 small">We propose a novel decoding approach for neural machine translation (NMT) based on continuous optimisation. We reformulate decoding, a discrete optimization problem, into a continuous problem, such that <a href=https://en.wikipedia.org/wiki/Mathematical_optimization>optimization</a> can make use of efficient gradient-based techniques. Our powerful decoding framework allows for more accurate decoding for standard neural machine translation models, as well as enabling decoding in intractable models such as intersection of several different NMT models. Our empirical results show that our decoding framework is effective, and can leads to substantial improvements in translations, especially in situations where <a href=https://en.wikipedia.org/wiki/Greedy_search>greedy search</a> and <a href=https://en.wikipedia.org/wiki/Beam_search>beam search</a> are not feasible. Finally, we show how the <a href=https://en.wikipedia.org/wiki/Methodology>technique</a> is highly competitive with, and complementary to, <a href=https://en.wikipedia.org/wiki/Ranking>reranking</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1016.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1016 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1016 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/238228698 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D17-1016" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/D17-1016/>Continuous Representation of Location for <a href=https://en.wikipedia.org/wiki/Geolocation>Geolocation</a> and Lexical Dialectology using Mixture Density Networks</a></strong><br><a href=/people/a/afshin-rahimi/>Afshin Rahimi</a>
|
<a href=/people/t/timothy-baldwin/>Timothy Baldwin</a>
|
<a href=/people/t/trevor-cohn/>Trevor Cohn</a><br><a href=/volumes/D17-1/ class=text-muted>Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1016><div class="card-body p-3 small">We propose a method for embedding two-dimensional locations in a continuous vector space using a neural network-based model incorporating mixtures of Gaussian distributions, presenting two model variants for text-based geolocation and lexical dialectology. Evaluated over <a href=https://en.wikipedia.org/wiki/Twitter>Twitter data</a>, the proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms conventional regression-based geolocation and provides a better <a href=https://en.wikipedia.org/wiki/Measurement_uncertainty>estimate of uncertainty</a>. We also show the effectiveness of the <a href=https://en.wikipedia.org/wiki/Representation_(mathematics)>representation</a> for predicting words from location in lexical dialectology, and evaluate it using the DARE dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1063.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1063 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1063 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/238234005 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D17-1063" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/D17-1063/>Learning how to Active Learn : A Deep Reinforcement Learning Approach</a></strong><br><a href=/people/m/meng-fang/>Meng Fang</a>
|
<a href=/people/y/yuan-li/>Yuan Li</a>
|
<a href=/people/t/trevor-cohn/>Trevor Cohn</a><br><a href=/volumes/D17-1/ class=text-muted>Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1063><div class="card-body p-3 small">Active learning aims to select a small subset of data for <a href=https://en.wikipedia.org/wiki/Annotation>annotation</a> such that a <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifier</a> learned on the data is highly accurate. This is usually done using heuristic selection methods, however the effectiveness of such methods is limited and moreover, the performance of <a href=https://en.wikipedia.org/wiki/Heuristic>heuristics</a> varies between datasets. To address these shortcomings, we introduce a novel formulation by reframing the <a href=https://en.wikipedia.org/wiki/Active_learning>active learning</a> as a reinforcement learning problem and explicitly learning a data selection policy, where the <a href=https://en.wikipedia.org/wiki/Policy>policy</a> takes the role of the <a href=https://en.wikipedia.org/wiki/Active_learning>active learning heuristic</a>. Importantly, our <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>method</a> allows the selection policy learned using simulation to one language to be transferred to other languages. We demonstrate our method using cross-lingual named entity recognition, observing uniform improvements over traditional active learning algorithms.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1306.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1306 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1306 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/238235659 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D17-1306/>Sequence Effects in Crowdsourced Annotations</a></strong><br><a href=/people/n/nitika-mathur/>Nitika Mathur</a>
|
<a href=/people/t/timothy-baldwin/>Timothy Baldwin</a>
|
<a href=/people/t/trevor-cohn/>Trevor Cohn</a><br><a href=/volumes/D17-1/ class=text-muted>Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1306><div class="card-body p-3 small">Manual data annotation is a vital component of <a href=https://en.wikipedia.org/wiki/Neuro-linguistic_programming>NLP research</a>. When designing <a href=https://en.wikipedia.org/wiki/Annotation>annotation tasks</a>, properties of the annotation interface can unintentionally lead to artefacts in the resulting dataset, biasing the evaluation. In this paper, we explore sequence effects where <a href=https://en.wikipedia.org/wiki/Annotation>annotations</a> of an item are affected by the preceding items. Having assigned one label to an instance, the annotator may be less (or more) likely to assign the same label to the next. During rating tasks, seeing a low quality item may affect the score given to the next item either positively or negatively. We see clear evidence of both types of effects using auto-correlation studies over three different crowdsourced datasets. We then recommend a simple way to minimise sequence effects.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1084.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1084 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1084 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1084/>Multilingual Training of Crosslingual Word Embeddings</a></strong><br><a href=/people/l/long-duong/>Long Duong</a>
|
<a href=/people/h/hiroshi-kanayama/>Hiroshi Kanayama</a>
|
<a href=/people/t/tengfei-ma/>Tengfei Ma</a>
|
<a href=/people/s/steven-bird/>Steven Bird</a>
|
<a href=/people/t/trevor-cohn/>Trevor Cohn</a><br><a href=/volumes/E17-1/ class=text-muted>Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1084><div class="card-body p-3 small">Crosslingual word embeddings represent lexical items from different languages using the same <a href=https://en.wikipedia.org/wiki/Vector_space>vector space</a>, enabling crosslingual transfer. Most prior work constructs <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a> for a pair of languages, with English on one side. We investigate methods for building high quality crosslingual word embeddings for many languages in a unified vector space. In this way, we can exploit and combine strength of many languages. We obtained high performance on bilingual lexicon induction, monolingual similarity and crosslingual document classification tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1088.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1088 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1088 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1088/>Cross-Lingual Word Embeddings for Low-Resource Language Modeling</a></strong><br><a href=/people/o/oliver-adams/>Oliver Adams</a>
|
<a href=/people/a/adam-makarucha/>Adam Makarucha</a>
|
<a href=/people/g/graham-neubig/>Graham Neubig</a>
|
<a href=/people/s/steven-bird/>Steven Bird</a>
|
<a href=/people/t/trevor-cohn/>Trevor Cohn</a><br><a href=/volumes/E17-1/ class=text-muted>Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1088><div class="card-body p-3 small">Most languages have no established <a href=https://en.wikipedia.org/wiki/Writing_system>writing system</a> and minimal written records. However, <a href=https://en.wikipedia.org/wiki/Textual_data>textual data</a> is essential for <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a>, and particularly important for training <a href=https://en.wikipedia.org/wiki/Language_model>language models</a> to support <a href=https://en.wikipedia.org/wiki/Speech_recognition>speech recognition</a>. Even in cases where text data is missing, there are some languages for which bilingual lexicons are available, since creating lexicons is a fundamental task of documentary linguistics. We investigate the use of such <a href=https://en.wikipedia.org/wiki/Lexicon>lexicons</a> to improve <a href=https://en.wikipedia.org/wiki/Language_model>language models</a> when textual training data is limited to as few as a thousand sentences. The method involves learning cross-lingual word embeddings as a preliminary step in training monolingual language models. Results across a number of languages show that <a href=https://en.wikipedia.org/wiki/Language_model>language models</a> are improved by this pre-training. Application to <a href=https://en.wikipedia.org/wiki/Yongning_Na>Yongning Na</a>, a threatened language, highlights challenges in deploying the approach in real low-resource environments.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2019.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2019 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2019 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=E17-2019" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/E17-2019/>Context-Aware Prediction of Derivational Word-forms</a></strong><br><a href=/people/e/ekaterina-vylomova/>Ekaterina Vylomova</a>
|
<a href=/people/r/ryan-cotterell/>Ryan Cotterell</a>
|
<a href=/people/t/timothy-baldwin/>Timothy Baldwin</a>
|
<a href=/people/t/trevor-cohn/>Trevor Cohn</a><br><a href=/volumes/E17-2/ class=text-muted>Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2019><div class="card-body p-3 small">Derivational morphology is a fundamental and complex characteristic of language. In this paper we propose a new task of predicting the <a href=https://en.wikipedia.org/wiki/Derivation_(differential_algebra)>derivational form</a> of a given base-form lemma that is appropriate for a given context. We present an encoder-decoder style neural network to produce a derived form character-by-character, based on its corresponding character-level representation of the base form and the context. We demonstrate that our model is able to generate valid context-sensitive derivations from known base forms, but is less accurate under lexicon agnostic setting.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Trevor+Cohn" title="Search for 'Trevor Cohn' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/t/timothy-baldwin/ class=align-middle>Timothy Baldwin</a>
<span class="badge badge-secondary align-middle ml-2">17</span></li><li class=list-group-item><a href=/people/a/afshin-rahimi/ class=align-middle>Afshin Rahimi</a>
<span class="badge badge-secondary align-middle ml-2">5</span></li><li class=list-group-item><a href=/people/g/gholamreza-haffari/ class=align-middle>Gholamreza Haffari</a>
<span class="badge badge-secondary align-middle ml-2">5</span></li><li class=list-group-item><a href=/people/d/daniel-beck/ class=align-middle>Daniel Beck</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/l/lea-frermann/ class=align-middle>Lea Frermann</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/e/ekaterina-vylomova/ class=align-middle>Ekaterina Vylomova</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/f/fei-liu-unimelb/ class=align-middle>Fei Liu</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/s/shivashankar-subramanian/ class=align-middle>Shivashankar Subramanian</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/y/yulan-he/ class=align-middle>Yulan He</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/y/yang-liu-icsi/ class=align-middle>Yang Liu</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/j/jey-han-lau/ class=align-middle>Jey Han Lau</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/m/meng-fang/ class=align-middle>Meng Fang</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/c/cong-duy-vu-hoang/ class=align-middle>Cong Duy Vu Hoang</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/r/ryan-cotterell/ class=align-middle>Ryan Cotterell</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/s/steven-bird/ class=align-middle>Steven Bird</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/f/fei-liu-utdallas/ class=align-middle>Fei Liu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/b/bonnie-webber/ class=align-middle>Bonnie Webber</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/chunhua-liu/ class=align-middle>Chunhua Liu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/x/xudong-han/ class=align-middle>Xudong Han</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/ying-xu/ class=align-middle>Ying Xu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/x/xuanli-he/ class=align-middle>Xuanli He</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yitong-li/ class=align-middle>Yitong Li</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/minghao-wu/ class=align-middle>Minghao Wu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/dalin-wang/ class=align-middle>Dalin Wang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yuan-li/ class=align-middle>Yuan Li</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/n/nitika-mathur/ class=align-middle>Nitika Mathur</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/taro-miyazaki/ class=align-middle>Taro Miyazaki</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jason-eisner/ class=align-middle>Jason Eisner</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/n/najam-zaidi/ class=align-middle>Najam Zaidi</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/kemal-kurniawan/ class=align-middle>Kemal Kurniawan</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/p/philip-schulz/ class=align-middle>Philip Schulz</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/long-duong/ class=align-middle>Long Duong</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/hiroshi-kanayama/ class=align-middle>Hiroshi Kanayama</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/tengfei-ma/ class=align-middle>Tengfei Ma</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/o/oliver-adams/ class=align-middle>Oliver Adams</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/adam-makarucha/ class=align-middle>Adam Makarucha</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/g/graham-neubig/ class=align-middle>Graham Neubig</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/emnlp/ class=align-middle>EMNLP</a><span class="badge badge-secondary align-middle ml-2">10</span></li><li class=list-group-item><a href=/venues/acl/ class=align-middle>ACL</a><span class="badge badge-secondary align-middle ml-2">5</span></li><li class=list-group-item><a href=/venues/eacl/ class=align-middle>EACL</a><span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/venues/ws/ class=align-middle>WS</a><span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/venues/naacl/ class=align-middle>NAACL</a><span class="badge badge-secondary align-middle ml-2">3</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-venues aria-expanded=false aria-controls=more-venues>show all...</li><div class="collapse border-top" id=more-venues><li class=list-group-item><a href=/venues/ijcnlp/ class=align-middle>IJCNLP</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/alta/ class=align-middle>ALTA</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/conll/ class=align-middle>CoNLL</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/findings/ class=align-middle>Findings</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/semeval/ class=align-middle>SemEval</a><span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>