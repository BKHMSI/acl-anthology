<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Tomoyuki Kajiwara - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Tomoyuki</span> <span class=font-weight-bold>Kajiwara</span></h2><hr><div class=row><div class=col-lg-9><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-srw.24.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-srw--24 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-srw.24 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-srw.24/>Edit Distance Based Curriculum Learning for Paraphrase Generation</a></strong><br><a href=/people/s/sora-kadotani/>Sora Kadotani</a>
|
<a href=/people/t/tomoyuki-kajiwara/>Tomoyuki Kajiwara</a>
|
<a href=/people/y/yuki-arase/>Yuki Arase</a>
|
<a href=/people/m/makoto-onizuka/>Makoto Onizuka</a><br><a href=/volumes/2021.acl-srw/ class=text-muted>Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: Student Research Workshop</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-srw--24><div class="card-body p-3 small">Curriculum learning has improved the quality of <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a>, where only source-side features are considered in the <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> to determine the difficulty of <a href=https://en.wikipedia.org/wiki/Translation>translation</a>. In this study, we apply <a href=https://en.wikipedia.org/wiki/Curriculum>curriculum learning</a> to <a href=https://en.wikipedia.org/wiki/Paraphrase_generation>paraphrase generation</a> for the first time. Different from <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>, <a href=https://en.wikipedia.org/wiki/Paraphrase_generation>paraphrase generation</a> allows a certain level of discrepancy in <a href=https://en.wikipedia.org/wiki/Semantics>semantics</a> between source and target, which results in diverse transformations from <a href=https://en.wikipedia.org/wiki/Lexical_substitution>lexical substitution</a> to reordering of clauses. Hence, the difficulty of transformations requires considering both source and target contexts. Experiments on formality transfer using GYAFC showed that our curriculum learning with <a href=https://en.wikipedia.org/wiki/Edit_distance>edit distance</a> improves the quality of <a href=https://en.wikipedia.org/wiki/Paraphrase_generation>paraphrase generation</a>. Additionally, the proposed <a href=https://en.wikipedia.org/wiki/Methodology>method</a> improves the quality of difficult samples, which was not possible for previous <a href=https://en.wikipedia.org/wiki/Methodology>methods</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wat-1.20.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wat-1--20 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wat-1.20 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wat-1.20/>TMEKU System for the WAT2021 Multimodal Translation Task<span class=acl-fixed-case>TMEKU</span> System for the <span class=acl-fixed-case>WAT</span>2021 Multimodal Translation Task</a></strong><br><a href=/people/y/yuting-zhao/>Yuting Zhao</a>
|
<a href=/people/m/mamoru-komachi/>Mamoru Komachi</a>
|
<a href=/people/t/tomoyuki-kajiwara/>Tomoyuki Kajiwara</a>
|
<a href=/people/c/chenhui-chu/>Chenhui Chu</a><br><a href=/volumes/2021.wat-1/ class=text-muted>Proceedings of the 8th Workshop on Asian Translation (WAT2021)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wat-1--20><div class="card-body p-3 small">We introduce our TMEKU system submitted to the English-Japanese Multimodal Translation Task for WAT 2021. We participated in the Flickr30kEnt-JP task and Ambiguous MSCOCO Multimodal task under the constrained condition using only the officially provided datasets. Our proposed system employs soft alignment of word-region for multimodal neural machine translation (MNMT). The experimental results evaluated on the BLEU metric provided by the WAT 2021 evaluation site show that the TMEKU system has achieved the best performance among all the participated systems. Further analysis of the case study demonstrates that leveraging word-region alignment between the textual and visual modalities is the key to performance enhancement in our TMEKU system, which leads to better visual information use.</div></div><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.eamt-1.12.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--eamt-1--12 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.eamt-1.12 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.eamt-1.12/>Double Attention-based Multimodal Neural Machine Translation with Semantic Image Regions</a></strong><br><a href=/people/y/yuting-zhao/>Yuting Zhao</a>
|
<a href=/people/m/mamoru-komachi/>Mamoru Komachi</a>
|
<a href=/people/t/tomoyuki-kajiwara/>Tomoyuki Kajiwara</a>
|
<a href=/people/c/chenhui-chu/>Chenhui Chu</a><br><a href=/volumes/2020.eamt-1/ class=text-muted>Proceedings of the 22nd Annual Conference of the European Association for Machine Translation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--eamt-1--12><div class="card-body p-3 small">Existing studies on multimodal neural machine translation (MNMT) have mainly focused on the effect of combining visual and textual modalities to improve translations. However, it has been suggested that the <a href=https://en.wikipedia.org/wiki/Visual_system>visual modality</a> is only marginally beneficial. Conventional visual attention mechanisms have been used to select the visual features from equally-sized grids generated by convolutional neural networks (CNNs), and may have had modest effects on aligning the visual concepts associated with textual objects, because the grid visual features do not capture semantic information. In contrast, we propose the application of semantic image regions for MNMT by integrating visual and textual features using two individual attention mechanisms (double attention). We conducted experiments on the Multi30k dataset and achieved an improvement of 0.5 and 0.9 BLEU points for English-German and English-French translation tasks, compared with the MNMT with grid visual features. We also demonstrated concrete improvements on <a href=https://en.wikipedia.org/wiki/Translation>translation</a> performance benefited from semantic image regions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.aacl-srw.22.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--aacl-srw--22 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.aacl-srw.22 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.aacl-srw.22/>Text Simplification with <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>Reinforcement Learning</a> Using <a href=https://en.wikipedia.org/wiki/Supervised_learning>Supervised Rewards</a> on <a href=https://en.wikipedia.org/wiki/Grammaticality>Grammaticality</a>, Meaning Preservation, and <a href=https://en.wikipedia.org/wiki/Simplicity>Simplicity</a></a></strong><br><a href=/people/a/akifumi-nakamachi/>Akifumi Nakamachi</a>
|
<a href=/people/t/tomoyuki-kajiwara/>Tomoyuki Kajiwara</a>
|
<a href=/people/y/yuki-arase/>Yuki Arase</a><br><a href=/volumes/2020.aacl-srw/ class=text-muted>Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing: Student Research Workshop</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--aacl-srw--22><div class="card-body p-3 small">We optimize rewards of <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning</a> in <a href=https://en.wikipedia.org/wiki/Text_simplification>text simplification</a> using <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> that are highly correlated with human-perspectives. To address problems of <a href=https://en.wikipedia.org/wiki/Exposure_bias>exposure bias</a> and loss-evaluation mismatch, text-to-text generation tasks employ <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning</a> that rewards task-specific metrics. Previous studies in <a href=https://en.wikipedia.org/wiki/Text_simplification>text simplification</a> employ the weighted sum of sub-rewards from three perspectives : <a href=https://en.wikipedia.org/wiki/Grammaticality>grammaticality</a>, meaning preservation, and <a href=https://en.wikipedia.org/wiki/Simplicity>simplicity</a>. However, the previous rewards do not align with <a href=https://en.wikipedia.org/wiki/Human_psychology>human-perspectives</a> for these <a href=https://en.wikipedia.org/wiki/Point_of_view_(philosophy)>perspectives</a>. In this study, we propose to use BERT regressors fine-tuned for <a href=https://en.wikipedia.org/wiki/Grammaticality>grammaticality</a>, meaning preservation, and <a href=https://en.wikipedia.org/wiki/Simplicity>simplicity</a> as reward estimators to achieve text simplification conforming to human-perspectives. Experimental results show that <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning</a> with our <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>rewards</a> balances meaning preservation and <a href=https://en.wikipedia.org/wiki/Simplicity>simplicity</a>. Additionally, human evaluation confirmed that simplified texts by our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> are preferred by humans compared to previous studies.</div></div><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5552.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5552 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5552 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5552/>Contextualized context2vec</a></strong><br><a href=/people/k/kazuki-ashihara/>Kazuki Ashihara</a>
|
<a href=/people/t/tomoyuki-kajiwara/>Tomoyuki Kajiwara</a>
|
<a href=/people/y/yuki-arase/>Yuki Arase</a>
|
<a href=/people/s/satoru-uchida/>Satoru Uchida</a><br><a href=/volumes/D19-55/ class=text-muted>Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT 2019)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5552><div class="card-body p-3 small">Lexical substitution ranks substitution candidates from the viewpoint of paraphrasability for a target word in a given sentence. There are two major approaches for <a href=https://en.wikipedia.org/wiki/Lexical_substitution>lexical substitution</a> : (1) generating contextualized word embeddings by assigning multiple embeddings to one word and (2) generating context embeddings using the sentence. Herein we propose a method that combines these two approaches to contextualize word embeddings for <a href=https://en.wikipedia.org/wiki/Lexical_substitution>lexical substitution</a>. Experiments demonstrate that our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> outperforms the current state-of-the-art method. We also create CEFR-LP, a new evaluation dataset for the lexical substitution task. It has a wider coverage of substitution candidates than previous <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> and assigns <a href=https://en.wikipedia.org/wiki/English_as_a_second_or_foreign_language>English proficiency levels</a> to all target words and substitution candidates.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1607.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1607 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1607 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-1607/>Negative Lexically Constrained Decoding for <a href=https://en.wikipedia.org/wiki/Paraphrase_generation>Paraphrase Generation</a></a></strong><br><a href=/people/t/tomoyuki-kajiwara/>Tomoyuki Kajiwara</a><br><a href=/volumes/P19-1/ class=text-muted>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1607><div class="card-body p-3 small">Paraphrase generation can be regarded as monolingual translation. Unlike bilingual machine translation, <a href=https://en.wikipedia.org/wiki/Paraphrase_generation>paraphrase generation</a> rewrites only a limited portion of an input sentence. Hence, previous methods based on <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> often perform conservatively to fail to make necessary rewrites. To solve this problem, we propose a neural model for <a href=https://en.wikipedia.org/wiki/Paraphrase_generation>paraphrase generation</a> that first identifies words in the source sentence that should be paraphrased. Then, these words are paraphrased by the negative lexically constrained decoding that avoids outputting these words as they are. Experiments on text simplification and formality transfer show that our model improves the quality of <a href=https://en.wikipedia.org/wiki/Paraphrase>paraphrasing</a> by making necessary rewrites to an input sentence.</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6456.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6456 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6456 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-6456" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-6456/>RUSE : Regressor Using Sentence Embeddings for Automatic Machine Translation Evaluation<span class=acl-fixed-case>RUSE</span>: Regressor Using Sentence Embeddings for Automatic Machine Translation Evaluation</a></strong><br><a href=/people/h/hiroki-shimanaka/>Hiroki Shimanaka</a>
|
<a href=/people/t/tomoyuki-kajiwara/>Tomoyuki Kajiwara</a>
|
<a href=/people/m/mamoru-komachi/>Mamoru Komachi</a><br><a href=/volumes/W18-64/ class=text-muted>Proceedings of the Third Conference on Machine Translation: Shared Task Papers</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6456><div class="card-body p-3 small">We introduce the RUSE metric for the WMT18 metrics shared task. Sentence embeddings can capture global information that can not be captured by local features based on character or word N-grams. Although training <a href=https://en.wikipedia.org/wiki/Sentence_embedding>sentence embeddings</a> using small-scale translation datasets with manual evaluation is difficult, <a href=https://en.wikipedia.org/wiki/Sentence_embedding>sentence embeddings</a> trained from large-scale data in other tasks can improve the automatic evaluation of <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>. We use a multi-layer perceptron regressor based on three types of <a href=https://en.wikipedia.org/wiki/Sentence_embedding>sentence embeddings</a>. The experimental results of the WMT16 and WMT17 datasets show that the RUSE metric achieves a state-of-the-art performance in both segment- and system-level metrics tasks with embedding features only.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-4015.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-4015 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-4015 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-4015/>Metric for Automatic Machine Translation Evaluation based on Universal Sentence Representations</a></strong><br><a href=/people/h/hiroki-shimanaka/>Hiroki Shimanaka</a>
|
<a href=/people/t/tomoyuki-kajiwara/>Tomoyuki Kajiwara</a>
|
<a href=/people/m/mamoru-komachi/>Mamoru Komachi</a><br><a href=/volumes/N18-4/ class=text-muted>Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Student Research Workshop</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-4015><div class="card-body p-3 small">Sentence representations can capture a wide range of information that can not be captured by local features based on character or word N-grams. This paper examines the usefulness of universal sentence representations for evaluating the quality of <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>. Al-though it is difficult to train sentence representations using small-scale translation datasets with manual evaluation, sentence representations trained from large-scale data in other tasks can improve the automatic evaluation of <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>. Experimental results of the WMT-2016 dataset show that the proposed method achieves state-of-the-art performance with sentence representation features only.</div></div><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1009.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1009 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1009 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=I17-1009" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/I17-1009/>MIPA : Mutual Information Based Paraphrase Acquisition via Bilingual Pivoting<span class=acl-fixed-case>MIPA</span>: Mutual Information Based Paraphrase Acquisition via Bilingual Pivoting</a></strong><br><a href=/people/t/tomoyuki-kajiwara/>Tomoyuki Kajiwara</a>
|
<a href=/people/m/mamoru-komachi/>Mamoru Komachi</a>
|
<a href=/people/d/daichi-mochihashi/>Daichi Mochihashi</a><br><a href=/volumes/I17-1/ class=text-muted>Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1009><div class="card-body p-3 small">We present a pointwise mutual information (PMI)-based approach to formalize paraphrasability and propose a variant of PMI, called MIPA, for the paraphrase acquisition. Our paraphrase acquisition method first acquires lexical paraphrase pairs by bilingual pivoting and then reranks them by PMI and distributional similarity. The complementary nature of information from bilingual corpora and from monolingual corpora makes the proposed method robust. Experimental results show that the proposed method substantially outperforms bilingual pivoting and distributional similarity themselves in terms of metrics such as MRR, MAP, coverage, and Spearman&#8217;s correlation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-2019.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-2019 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-2019 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-2019/>Semantic Features Based on Word Alignments for Estimating Quality of Text Simplification</a></strong><br><a href=/people/t/tomoyuki-kajiwara/>Tomoyuki Kajiwara</a>
|
<a href=/people/a/atsushi-fujita/>Atsushi Fujita</a><br><a href=/volumes/I17-2/ class=text-muted>Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-2019><div class="card-body p-3 small">This paper examines the usefulness of <a href=https://en.wikipedia.org/wiki/Semantic_feature>semantic features</a> based on <a href=https://en.wikipedia.org/wiki/Word_alignment>word alignments</a> for estimating the quality of text simplification. Specifically, we introduce seven types of alignment-based features computed on the basis of <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> and paraphrase lexicons. Through an empirical experiment using the QATS dataset, we confirm that we can achieve the state-of-the-art performance only with these <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5703.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5703 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5703 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5703/>Improving Japanese-to-English Neural Machine Translation by Paraphrasing the Target Language<span class=acl-fixed-case>J</span>apanese-to-<span class=acl-fixed-case>E</span>nglish Neural Machine Translation by Paraphrasing the Target Language</a></strong><br><a href=/people/y/yuuki-sekizawa/>Yuuki Sekizawa</a>
|
<a href=/people/t/tomoyuki-kajiwara/>Tomoyuki Kajiwara</a>
|
<a href=/people/m/mamoru-komachi/>Mamoru Komachi</a><br><a href=/volumes/W17-57/ class=text-muted>Proceedings of the 4th Workshop on Asian Translation (WAT2017)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5703><div class="card-body p-3 small">Neural machine translation (NMT) produces sentences that are more fluent than those produced by <a href=https://en.wikipedia.org/wiki/Statistical_machine_translation>statistical machine translation (SMT)</a>. However, NMT has a very high <a href=https://en.wikipedia.org/wiki/Computational_cost>computational cost</a> because of the high dimensionality of the output layer. Generally, NMT restricts the size of vocabulary, which results in infrequent words being treated as out-of-vocabulary (OOV) and degrades the performance of the <a href=https://en.wikipedia.org/wiki/Translation>translation</a>. In evaluation, we achieved a statistically significant BLEU score improvement of 0.55-0.77 over the baselines including the <a href=https://en.wikipedia.org/wiki/State-of-the-art>state-of-the-art method</a>.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Tomoyuki+Kajiwara" title="Search for 'Tomoyuki Kajiwara' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/m/mamoru-komachi/ class=align-middle>Mamoru Komachi</a>
<span class="badge badge-secondary align-middle ml-2">6</span></li><li class=list-group-item><a href=/people/y/yuki-arase/ class=align-middle>Yuki Arase</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/y/yuting-zhao/ class=align-middle>Yuting Zhao</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/c/chenhui-chu/ class=align-middle>Chenhui Chu</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/h/hiroki-shimanaka/ class=align-middle>Hiroki Shimanaka</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/s/sora-kadotani/ class=align-middle>Sora Kadotani</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/makoto-onizuka/ class=align-middle>Makoto Onizuka</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/daichi-mochihashi/ class=align-middle>Daichi Mochihashi</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/atsushi-fujita/ class=align-middle>Atsushi Fujita</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yuuki-sekizawa/ class=align-middle>Yuuki Sekizawa</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/akifumi-nakamachi/ class=align-middle>Akifumi Nakamachi</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/kazuki-ashihara/ class=align-middle>Kazuki Ashihara</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/satoru-uchida/ class=align-middle>Satoru Uchida</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/acl/ class=align-middle>ACL</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/ijcnlp/ class=align-middle>IJCNLP</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/ws/ class=align-middle>WS</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/eamt/ class=align-middle>EAMT</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/aacl/ class=align-middle>AACL</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-venues aria-expanded=false aria-controls=more-venues>show all...</li><div class="collapse border-top" id=more-venues><li class=list-group-item><a href=/venues/emnlp/ class=align-middle>EMNLP</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/wat/ class=align-middle>WAT</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/naacl/ class=align-middle>NAACL</a><span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>