<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Lu Chen - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Lu</span> <span class=font-weight-bold>Chen</span></h2><hr><div class=row><div class=col-lg-9><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.198.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--198 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.198 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.198" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.198/>LGESQL : Line Graph Enhanced Text-to-SQL Model with Mixed Local and Non-Local Relations<span class=acl-fixed-case>LGESQL</span>: Line Graph Enhanced Text-to-<span class=acl-fixed-case>SQL</span> Model with Mixed Local and Non-Local Relations</a></strong><br><a href=/people/r/ruisheng-cao/>Ruisheng Cao</a>
|
<a href=/people/l/lu-chen/>Lu Chen</a>
|
<a href=/people/z/zhi-chen/>Zhi Chen</a>
|
<a href=/people/y/yanbin-zhao/>Yanbin Zhao</a>
|
<a href=/people/s/su-zhu/>Su Zhu</a>
|
<a href=/people/k/kai-yu/>Kai Yu</a><br><a href=/volumes/2021.acl-long/ class=text-muted>Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--198><div class="card-body p-3 small">This work aims to tackle the challenging heterogeneous graph encoding problem in the text-to-SQL task. Previous methods are typically node-centric and merely utilize different weight matrices to parameterize edge types, which 1) ignore the rich semantics embedded in the topological structure of edges, and 2) fail to distinguish local and non-local relations for each node. To this end, we propose a Line Graph Enhanced Text-to-SQL (LGESQL) model to mine the underlying relational features without constructing meta-paths. By virtue of the <a href=https://en.wikipedia.org/wiki/Line_graph>line graph</a>, messages propagate more efficiently through not only connections between nodes, but also the topology of directed edges. Furthermore, both local and non-local relations are integrated distinctively during the graph iteration. We also design an auxiliary task called graph pruning to improve the discriminative capability of the <a href=https://en.wikipedia.org/wiki/Encoder>encoder</a>. Our <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> achieves state-of-the-art results (62.8 % with Glove, 72.0 % with Electra) on the cross-domain text-to-SQL benchmark Spider at the time of writing.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.343.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--343 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.343 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.343/>WebSRC : A Dataset for Web-Based Structural Reading Comprehension<span class=acl-fixed-case>W</span>eb<span class=acl-fixed-case>SRC</span>: A Dataset for Web-Based Structural Reading Comprehension</a></strong><br><a href=/people/x/xingyu-chen/>Xingyu Chen</a>
|
<a href=/people/z/zihan-zhao/>Zihan Zhao</a>
|
<a href=/people/l/lu-chen/>Lu Chen</a>
|
<a href=/people/j/jiabao-ji/>JiaBao Ji</a>
|
<a href=/people/d/danyang-zhang/>Danyang Zhang</a>
|
<a href=/people/a/ao-luo/>Ao Luo</a>
|
<a href=/people/y/yuxuan-xiong/>Yuxuan Xiong</a>
|
<a href=/people/k/kai-yu/>Kai Yu</a><br><a href=/volumes/2021.emnlp-main/ class=text-muted>Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--343><div class="card-body p-3 small">Web search is an essential way for humans to obtain information, but it&#8217;s still a great challenge for machines to understand the contents of web pages. In this paper, we introduce the task of web-based structural reading comprehension. Given a <a href=https://en.wikipedia.org/wiki/Web_page>web page</a> and a question about it, the task is to find an answer from the web page. This <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> requires a system not only to understand the <a href=https://en.wikipedia.org/wiki/Semantics>semantics of texts</a> but also the <a href=https://en.wikipedia.org/wiki/Web_design>structure of the web page</a>. Moreover, we proposed WebSRC, a novel Web-based Structural Reading Comprehension dataset. WebSRC consists of 400 K question-answer pairs, which are collected from 6.4 K web pages with corresponding HTML source code, <a href=https://en.wikipedia.org/wiki/Screenshot>screenshots</a>, and <a href=https://en.wikipedia.org/wiki/Metadata>metadata</a>. Each question in WebSRC requires a certain structural understanding of a web page to answer, and the answer is either a text span on the web page or yes / no. We evaluate various strong <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a> on our <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> to show the difficulty of our <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. We also investigate the usefulness of structural information and <a href=https://en.wikipedia.org/wiki/Visual_system>visual features</a>. Our <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> and baselines have been publicly available.</div></div><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.acl-main.608.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--acl-main--608 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.acl-main.608 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://slideslive.com/38929390 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.acl-main.608/>Unsupervised Dual Paraphrasing for Two-stage Semantic Parsing</a></strong><br><a href=/people/r/ruisheng-cao/>Ruisheng Cao</a>
|
<a href=/people/s/su-zhu/>Su Zhu</a>
|
<a href=/people/c/chenyu-yang/>Chenyu Yang</a>
|
<a href=/people/c/chen-liu/>Chen Liu</a>
|
<a href=/people/r/rao-ma/>Rao Ma</a>
|
<a href=/people/y/yanbin-zhao/>Yanbin Zhao</a>
|
<a href=/people/l/lu-chen/>Lu Chen</a>
|
<a href=/people/k/kai-yu/>Kai Yu</a><br><a href=/volumes/2020.acl-main/ class=text-muted>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--acl-main--608><div class="card-body p-3 small">One daunting problem for <a href=https://en.wikipedia.org/wiki/Semantic_parsing>semantic parsing</a> is the scarcity of <a href=https://en.wikipedia.org/wiki/Annotation>annotation</a>. Aiming to reduce nontrivial human labor, we propose a two-stage semantic parsing framework, where the first stage utilizes an unsupervised paraphrase model to convert an unlabeled natural language utterance into the canonical utterance. The downstream naive semantic parser accepts the intermediate output and returns the target <a href=https://en.wikipedia.org/wiki/Logical_form>logical form</a>. Furthermore, the entire training process is split into two phases : pre-training and cycle learning. Three tailored self-supervised tasks are introduced throughout training to activate the unsupervised paraphrase model. Experimental results on benchmarks <a href=https://en.wikipedia.org/wiki/Overnight>Overnight</a> and GeoGranno demonstrate that our <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> is effective and compatible with supervised training.</div></div><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1137.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1137 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1137 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1137" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-1137/>DIAG-NRE : A Neural Pattern Diagnosis Framework for Distantly Supervised Neural Relation Extraction<span class=acl-fixed-case>DIAG</span>-<span class=acl-fixed-case>NRE</span>: A Neural Pattern Diagnosis Framework for Distantly Supervised Neural Relation Extraction</a></strong><br><a href=/people/s/shun-zheng/>Shun Zheng</a>
|
<a href=/people/x/xu-han/>Xu Han</a>
|
<a href=/people/y/yankai-lin/>Yankai Lin</a>
|
<a href=/people/p/peilin-yu/>Peilin Yu</a>
|
<a href=/people/l/lu-chen/>Lu Chen</a>
|
<a href=/people/l/ling-huang/>Ling Huang</a>
|
<a href=/people/z/zhiyuan-liu/>Zhiyuan Liu</a>
|
<a href=/people/w/wei-xu/>Wei Xu</a><br><a href=/volumes/P19-1/ class=text-muted>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1137><div class="card-body p-3 small">Pattern-based labeling methods have achieved promising results in alleviating the inevitable labeling noises of distantly supervised neural relation extraction. However, these methods require significant expert labor to write relation-specific patterns, which makes them too sophisticated to generalize quickly. To ease the labor-intensive workload of pattern writing and enable the quick generalization to new relation types, we propose a neural pattern diagnosis framework, DIAG-NRE, that can automatically summarize and refine high-quality relational patterns from noise data with human experts in the loop. To demonstrate the effectiveness of DIAG-NRE, we apply it to two real-world datasets and present both significant and interpretable improvements over state-of-the-art methods.</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1107.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1107 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1107 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1107/>Structured Dialogue Policy with Graph Neural Networks</a></strong><br><a href=/people/l/lu-chen/>Lu Chen</a>
|
<a href=/people/b/bowen-tan/>Bowen Tan</a>
|
<a href=/people/s/sishan-long/>Sishan Long</a>
|
<a href=/people/k/kai-yu/>Kai Yu</a><br><a href=/volumes/C18-1/ class=text-muted>Proceedings of the 27th International Conference on Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1107><div class="card-body p-3 small">Recently, deep reinforcement learning (DRL) has been used for dialogue policy optimization. However, many DRL-based policies are not sample-efficient. Most recent advances focus on improving DRL optimization algorithms to address this issue. Here, we take an alternative route of designing neural network structure that is better suited for DRL-based dialogue management. The proposed structured deep reinforcement learning is based on graph neural networks (GNN), which consists of some sub-networks, each one for a <a href=https://en.wikipedia.org/wiki/Vertex_(graph_theory)>node</a> on a <a href=https://en.wikipedia.org/wiki/Directed_graph>directed graph</a>. The <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph</a> is defined according to the <a href=https://en.wikipedia.org/wiki/Ontology_(information_science)>domain ontology</a> and each node can be considered as a sub-agent. During <a href=https://en.wikipedia.org/wiki/Decision-making>decision making</a>, these sub-agents have internal message exchange between neighbors on the <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph</a>. We also propose an approach to jointly optimize the <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph structure</a> as well as the parameters of GNN. Experiments show that structured DRL significantly outperforms previous state-of-the-art approaches in almost all of the 18 tasks of the PyDial benchmark.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1299.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1299 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1299 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/305944406 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1299/>Towards Universal Dialogue State Tracking</a></strong><br><a href=/people/l/liliang-ren/>Liliang Ren</a>
|
<a href=/people/k/kaige-xie/>Kaige Xie</a>
|
<a href=/people/l/lu-chen/>Lu Chen</a>
|
<a href=/people/k/kai-yu/>Kai Yu</a><br><a href=/volumes/D18-1/ class=text-muted>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1299><div class="card-body p-3 small">Dialogue state tracker is the core part of a <a href=https://en.wikipedia.org/wiki/Spoken_dialogue_system>spoken dialogue system</a>. It estimates the beliefs of possible user&#8217;s goals at every dialogue turn. However, for most current approaches, it&#8217;s difficult to scale to large dialogue domains. They have one or more of following limitations : (a) Some <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> do n&#8217;t work in the situation where slot values in ontology changes dynamically ; (b) The number of model parameters is proportional to the number of slots ; (c) Some <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> extract features based on hand-crafted lexicons. To tackle these challenges, we propose StateNet, a universal dialogue state tracker. It is independent of the number of values, shares parameters across all slots, and uses pre-trained word vectors instead of explicit semantic dictionaries. Our experiments on two datasets show that our approach not only overcomes the limitations, but also significantly outperforms the performance of state-of-the-art approaches.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5022.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5022 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5022 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5022/>Cost-Sensitive Active Learning for Dialogue State Tracking</a></strong><br><a href=/people/k/kaige-xie/>Kaige Xie</a>
|
<a href=/people/c/cheng-chang/>Cheng Chang</a>
|
<a href=/people/l/liliang-ren/>Liliang Ren</a>
|
<a href=/people/l/lu-chen/>Lu Chen</a>
|
<a href=/people/k/kai-yu/>Kai Yu</a><br><a href=/volumes/W18-50/ class=text-muted>Proceedings of the 19th Annual SIGdial Meeting on Discourse and Dialogue</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5022><div class="card-body p-3 small">Dialogue state tracking (DST), when formulated as a <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised learning problem</a>, relies on <a href=https://en.wikipedia.org/wiki/Data_type>labelled data</a>. Since dialogue state annotation usually requires labelling all turns of a single dialogue and utilizing context information, it is very expensive to annotate all available unlabelled data. In this paper, a novel cost-sensitive active learning framework is proposed based on a set of new dialogue-level query strategies. This is the first attempt to apply <a href=https://en.wikipedia.org/wiki/Active_learning>active learning</a> for dialogue state tracking. Experiments on DSTC2 show that active learning with mixed data query strategies can effectively achieve the same DST performance with significantly less data annotation compared to traditional training approaches.</div></div><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1234.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1234 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1234 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D17-1234.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D17-1234/>Affordable On-line Dialogue Policy Learning</a></strong><br><a href=/people/c/cheng-chang/>Cheng Chang</a>
|
<a href=/people/r/runzhe-yang/>Runzhe Yang</a>
|
<a href=/people/l/lu-chen/>Lu Chen</a>
|
<a href=/people/x/xiang-zhou/>Xiang Zhou</a>
|
<a href=/people/k/kai-yu/>Kai Yu</a><br><a href=/volumes/D17-1/ class=text-muted>Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1234><div class="card-body p-3 small">The key to building an evolvable dialogue system in real-world scenarios is to ensure an affordable on-line dialogue policy learning, which requires the on-line learning process to be safe, efficient and economical. But in reality, due to the scarcity of real interaction data, the <a href=https://en.wikipedia.org/wiki/Dialogue_system>dialogue system</a> usually grows slowly. Besides, the poor initial dialogue policy easily leads to bad user experience and incurs a failure of attracting users to contribute training data, so that the learning process is unsustainable. To accurately depict this, two <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>quantitative metrics</a> are proposed to assess safety and efficiency issues. For solving the unsustainable learning problem, we proposed a complete companion teaching framework incorporating the guidance from the human teacher. Since the human teaching is expensive, we compared various teaching schemes answering the question how and when to teach, to economically utilize teaching budget, so that make the online learning process affordable.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1260.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1260 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1260 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D17-1260.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/238231562 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D17-1260/>Agent-Aware Dropout DQN for Safe and Efficient On-line Dialogue Policy Learning<span class=acl-fixed-case>DQN</span> for Safe and Efficient On-line Dialogue Policy Learning</a></strong><br><a href=/people/l/lu-chen/>Lu Chen</a>
|
<a href=/people/x/xiang-zhou/>Xiang Zhou</a>
|
<a href=/people/c/cheng-chang/>Cheng Chang</a>
|
<a href=/people/r/runzhe-yang/>Runzhe Yang</a>
|
<a href=/people/k/kai-yu/>Kai Yu</a><br><a href=/volumes/D17-1/ class=text-muted>Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1260><div class="card-body p-3 small">Hand-crafted rules and reinforcement learning (RL) are two popular choices to obtain dialogue policy. The rule-based policy is often reliable within predefined scope but not self-adaptable, whereas RL is evolvable with data but often suffers from a bad initial performance. We employ a companion learning framework to integrate the two approaches for on-line dialogue policy learning, in which a pre-defined rule-based policy acts as a teacher and guides a data-driven RL system by giving example actions as well as additional rewards. A novel agent-aware dropout Deep Q-Network (AAD-DQN) is proposed to address the problem of when to consult the teacher and how to learn from the teacher&#8217;s experiences. AAD-DQN, as a data-driven student policy, provides (1) two separate experience memories for student and teacher, (2) an uncertainty estimated by <a href=https://en.wikipedia.org/wiki/Dropping_out>dropout</a> to control the timing of consultation and learning. Simulation experiments showed that the proposed approach can significantly improve both safetyand efficiency of on-line policy optimization compared to other companion learning approaches as well as supervised pre-training using static dialogue corpus.<i>companion learning</i> framework to integrate the two approaches for <i>on-line</i> dialogue policy learning, in which a pre-defined rule-based policy acts as a &#8220;teacher&#8221; and guides a data-driven RL system by giving example actions as well as additional rewards. A novel <i>agent-aware dropout</i> Deep Q-Network (AAD-DQN) is proposed to address the problem of when to consult the teacher and how to learn from the teacher&#8217;s experiences. AAD-DQN, as a data-driven student policy, provides (1) two separate experience memories for student and teacher, (2) an uncertainty estimated by dropout to control the timing of consultation and learning. Simulation experiments showed that the proposed approach can significantly improve both <i>safety</i>\n\nand <i>efficiency</i> of on-line policy optimization compared to other companion learning approaches as well as supervised pre-training using static dialogue corpus.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Lu+Chen" title="Search for 'Lu Chen' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/k/kai-yu/ class=align-middle>Kai Yu</a>
<span class="badge badge-secondary align-middle ml-2">8</span></li><li class=list-group-item><a href=/people/c/cheng-chang/ class=align-middle>Cheng Chang</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/r/ruisheng-cao/ class=align-middle>Ruisheng Cao</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/y/yanbin-zhao/ class=align-middle>Yanbin Zhao</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/s/su-zhu/ class=align-middle>Su Zhu</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/l/liliang-ren/ class=align-middle>Liliang Ren</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/k/kaige-xie/ class=align-middle>Kaige Xie</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/r/runzhe-yang/ class=align-middle>Runzhe Yang</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/x/xiang-zhou/ class=align-middle>Xiang Zhou</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/b/bowen-tan/ class=align-middle>Bowen Tan</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/sishan-long/ class=align-middle>Sishan Long</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/zhi-chen/ class=align-middle>Zhi Chen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/chenyu-yang/ class=align-middle>Chenyu Yang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/chen-liu/ class=align-middle>Chen Liu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/rao-ma/ class=align-middle>Rao Ma</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/x/xingyu-chen/ class=align-middle>Xingyu Chen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/zihan-zhao/ class=align-middle>Zihan Zhao</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jiabao-ji/ class=align-middle>Jiabao Ji</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/danyang-zhang/ class=align-middle>Danyang Zhang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/ao-luo/ class=align-middle>Ao Luo</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yuxuan-xiong/ class=align-middle>Yuxuan Xiong</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/shun-zheng/ class=align-middle>Shun Zheng</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/x/xu-han/ class=align-middle>Xu Han</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yankai-lin/ class=align-middle>Yankai Lin</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/p/peilin-yu/ class=align-middle>Peilin Yu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/ling-huang/ class=align-middle>Ling Huang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/zhiyuan-liu/ class=align-middle>Zhiyuan Liu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/w/wei-xu/ class=align-middle>Wei Xu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/emnlp/ class=align-middle>EMNLP</a><span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/venues/acl/ class=align-middle>ACL</a><span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/venues/coling/ class=align-middle>COLING</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/ws/ class=align-middle>WS</a><span class="badge badge-secondary align-middle ml-2">1</span></li></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright Â©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>