<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Luis Espinosa Anke - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Luis</span> <span class=font-weight-bold>Espinosa Anke</span></h2><p class="font-weight-light text-muted"><span class=font-italic>Also published as:</span>
Luis Espinosa <span class=font-weight-normal>Anke</span>,
Luis <span class=font-weight-normal>Espinosa-Anke</span></p><hr><div class=row><div class=col-lg-9><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.280.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--280 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.280 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.280" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.280/>BERT is to NLP what AlexNet is to CV : Can Pre-Trained Language Models Identify Analogies?<span class=acl-fixed-case>BERT</span> is to <span class=acl-fixed-case>NLP</span> what <span class=acl-fixed-case>A</span>lex<span class=acl-fixed-case>N</span>et is to <span class=acl-fixed-case>CV</span>: Can Pre-Trained Language Models Identify Analogies?</a></strong><br><a href=/people/a/asahi-ushio/>Asahi Ushio</a>
|
<a href=/people/l/luis-espinosa-anke/>Luis Espinosa Anke</a>
|
<a href=/people/s/steven-schockaert/>Steven Schockaert</a>
|
<a href=/people/j/jose-camacho-collados/>Jose Camacho-Collados</a><br><a href=/volumes/2021.acl-long/ class=text-muted>Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--280><div class="card-body p-3 small">Analogies play a central role in human commonsense reasoning. The ability to recognize <a href=https://en.wikipedia.org/wiki/Analogy>analogies</a> such as eye is to seeing what ear is to hearing, sometimes referred to as <a href=https://en.wikipedia.org/wiki/Analogy>analogical proportions</a>, shape how we structure knowledge and understand <a href=https://en.wikipedia.org/wiki/Language>language</a>. Surprisingly, however, the task of identifying such <a href=https://en.wikipedia.org/wiki/Analogy>analogies</a> has not yet received much attention in the <a href=https://en.wikipedia.org/wiki/Language_model>language model era</a>. In this paper, we analyze the capabilities of transformer-based language models on this unsupervised task, using <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmarks</a> obtained from educational settings, as well as more commonly used <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>. We find that off-the-shelf language models can identify analogies to a certain extent, but struggle with abstract and complex relations, and results are highly sensitive to model architecture and hyperparameters. Overall the best results were obtained with GPT-2 and RoBERTa, while configurations using BERT were not able to outperform word embedding models. Our results raise important questions for future work about how, and to what extent, pre-trained language models capture knowledge about abstract semantic relations.</div></div><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.mwe-1.1.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--mwe-1--1 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.mwe-1.1 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.mwe-1.1" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.mwe-1.1/>CollFrEn : Rich Bilingual EnglishFrench Collocation Resource<span class=acl-fixed-case>C</span>oll<span class=acl-fixed-case>F</span>r<span class=acl-fixed-case>E</span>n: Rich Bilingual <span class=acl-fixed-case>E</span>nglish–<span class=acl-fixed-case>F</span>rench Collocation Resource</a></strong><br><a href=/people/b/beatriz-fisas/>Beatriz Fisas</a>
|
<a href=/people/l/luis-espinosa-anke/>Luis Espinosa Anke</a>
|
<a href=/people/j/joan-codina-filba/>Joan Codina-Filbá</a>
|
<a href=/people/l/leo-wanner/>Leo Wanner</a><br><a href=/volumes/2020.mwe-1/ class=text-muted>Proceedings of the Joint Workshop on Multiword Expressions and Electronic Lexicons</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--mwe-1--1><div class="card-body p-3 small">Collocations in the sense of idiosyncratic lexical co-occurrences of two syntactically bound words traditionally pose a challenge to language learners and many Natural Language Processing (NLP) applications alike. Reliable ground truth (i.e., ideally manually compiled) resources are thus of high value. We present a manually compiled bilingual EnglishFrench collocation resource with 7,480 collocations in <a href=https://en.wikipedia.org/wiki/English_language>English</a> and 6,733 in <a href=https://en.wikipedia.org/wiki/French_language>French</a>. Each <a href=https://en.wikipedia.org/wiki/Collocation>collocation</a> is enriched with information that facilitates its downstream exploitation in NLP tasks such as <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>, <a href=https://en.wikipedia.org/wiki/Word-sense_disambiguation>word sense disambiguation</a>, <a href=https://en.wikipedia.org/wiki/Natural-language_generation>natural language generation</a>, relation classification, and so forth. Our proposed enrichment covers : the semantic category of the collocation (its lexical function), its vector space representation (for each individual word as well as their joint collocation embedding), a subcategorization pattern of both its elements, as well as their corresponding BabelNet i d, and finally, indices of their occurrences in large scale reference corpora.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.148.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--148 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.148 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.148" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.148/>TweetEval : Unified Benchmark and Comparative Evaluation for Tweet Classification<span class=acl-fixed-case>T</span>weet<span class=acl-fixed-case>E</span>val: Unified Benchmark and Comparative Evaluation for Tweet Classification</a></strong><br><a href=/people/f/francesco-barbieri/>Francesco Barbieri</a>
|
<a href=/people/j/jose-camacho-collados/>Jose Camacho-Collados</a>
|
<a href=/people/l/luis-espinosa-anke/>Luis Espinosa Anke</a>
|
<a href=/people/l/leonardo-neves/>Leonardo Neves</a><br><a href=/volumes/2020.findings-emnlp/ class=text-muted>Findings of the Association for Computational Linguistics: EMNLP 2020</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--148><div class="card-body p-3 small">The experimental landscape in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a> for <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> is too fragmented. Each year, new shared tasks and datasets are proposed, ranging from classics like <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> to irony detection or emoji prediction. Therefore, it is unclear what the current state of the art is, as there is no standardized evaluation protocol, neither a strong set of <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a> trained on such domain-specific data. In this paper, we propose a new evaluation framework (TweetEval) consisting of seven heterogeneous Twitter-specific classification tasks. We also provide a strong set of <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a> as starting point, and compare different language modeling pre-training strategies. Our initial experiments show the effectiveness of starting off with existing pre-trained generic language models, and continue training them on Twitter corpora.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.518.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--518 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.518 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.518/>Do n’t Patronize Me ! An Annotated Dataset with Patronizing and Condescending Language towards Vulnerable Communities</a></strong><br><a href=/people/c/carla-perez-almendros/>Carla Perez Almendros</a>
|
<a href=/people/l/luis-espinosa-anke/>Luis Espinosa Anke</a>
|
<a href=/people/s/steven-schockaert/>Steven Schockaert</a><br><a href=/volumes/2020.coling-main/ class=text-muted>Proceedings of the 28th International Conference on Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--518><div class="card-body p-3 small">In this paper, we introduce a new annotated dataset which is aimed at supporting the development of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP models</a> to identify and categorize language that is patronizing or condescending towards vulnerable communities (e.g. refugees, homeless people, poor families). While the prevalence of such <a href=https://en.wikipedia.org/wiki/Language>language</a> in the general media has long been shown to have harmful effects, it differs from other types of harmful language, in that it is generally used unconsciously and with good intentions. We furthermore believe that the often subtle nature of patronizing and condescending language (PCL) presents an interesting technical challenge for the NLP community. Our analysis of the proposed dataset shows that identifying PCL is hard for standard NLP models, with language models such as BERT achieving the best results.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wnut-1.5.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wnut-1--5 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wnut-1.5 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.wnut-1.5/>Combining BERT with Static Word Embeddings for Categorizing Social Media<span class=acl-fixed-case>BERT</span> with Static Word Embeddings for Categorizing Social Media</a></strong><br><a href=/people/i/israa-alghanmi/>Israa Alghanmi</a>
|
<a href=/people/l/luis-espinosa-anke/>Luis Espinosa Anke</a>
|
<a href=/people/s/steven-schockaert/>Steven Schockaert</a><br><a href=/volumes/2020.wnut-1/ class=text-muted>Proceedings of the Sixth Workshop on Noisy User-generated Text (W-NUT 2020)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wnut-1--5><div class="card-body p-3 small">Pre-trained neural language models (LMs) have achieved impressive results in various natural language processing tasks, across different languages. Surprisingly, this extends to the <a href=https://en.wikipedia.org/wiki/Social_media>social media genre</a>, despite the fact that <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> often has very different characteristics from the language that LMs have seen during training. A particularly striking example is the performance of AraBERT, an LM for the <a href=https://en.wikipedia.org/wiki/Arabic>Arabic language</a>, which is successful in categorizing social media posts in <a href=https://en.wikipedia.org/wiki/Arabic>Arabic dialects</a>, despite only having been trained on Modern Standard <a href=https://en.wikipedia.org/wiki/Arabic>Arabic</a>. Our hypothesis in this paper is that the performance of LMs for <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> can nonetheless be improved by incorporating static word vectors that have been specifically trained on <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>. We show that a simple method for incorporating such word vectors is indeed successful in several Arabic and English benchmarks. Curiously, however, we also find that similar improvements are possible with word vectors that have been trained on traditional text sources (e.g. Wikipedia).</div></div><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5800.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5800/>Proceedings of the 5th Workshop on Semantic Deep Learning (SemDeep-5)</a></strong><br><a href=/people/l/luis-espinosa-anke/>Luis Espinosa-Anke</a>
|
<a href=/people/t/thierry-declerck/>Thierry Declerck</a>
|
<a href=/people/d/dagmar-gromann/>Dagmar Gromann</a>
|
<a href=/people/j/jose-camacho-collados/>Jose Camacho-Collados</a>
|
<a href=/people/m/mohammad-taher-pilehvar/>Mohammad Taher Pilehvar</a><br><a href=/volumes/W19-58/ class=text-muted>Proceedings of the 5th Workshop on Semantic Deep Learning (SemDeep-5)</a></span></p><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S18-1003.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S18-1003 data-toggle=collapse aria-expanded=false aria-controls=abstract-S18-1003 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/S18-1003/>SemEval 2018 Task 2 : Multilingual Emoji Prediction<span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val 2018 Task 2: Multilingual Emoji Prediction</a></strong><br><a href=/people/f/francesco-barbieri/>Francesco Barbieri</a>
|
<a href=/people/j/jose-camacho-collados/>Jose Camacho-Collados</a>
|
<a href=/people/f/francesco-ronzano/>Francesco Ronzano</a>
|
<a href=/people/l/luis-espinosa-anke/>Luis Espinosa-Anke</a>
|
<a href=/people/m/miguel-ballesteros/>Miguel Ballesteros</a>
|
<a href=/people/v/valerio-basile/>Valerio Basile</a>
|
<a href=/people/v/viviana-patti/>Viviana Patti</a>
|
<a href=/people/h/horacio-saggion/>Horacio Saggion</a><br><a href=/volumes/S18-1/ class=text-muted>Proceedings of The 12th International Workshop on Semantic Evaluation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S18-1003><div class="card-body p-3 small">This paper describes the results of the first Shared Task on Multilingual Emoji Prediction, organized as part of SemEval 2018. Given the text of a tweet, the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> consists of predicting the most likely <a href=https://en.wikipedia.org/wiki/Emoji>emoji</a> to be used along such tweet. Two <a href=https://en.wikipedia.org/wiki/Task_(project_management)>subtasks</a> were proposed, one for <a href=https://en.wikipedia.org/wiki/English_language>English</a> and one for <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a>, and participants were allowed to submit a <a href=https://en.wikipedia.org/wiki/System>system</a> run to one or both <a href=https://en.wikipedia.org/wiki/Task_(project_management)>subtasks</a>. In total, 49 teams participated to the <a href=https://en.wikipedia.org/wiki/English_language>English subtask</a> and 22 teams submitted a <a href=https://en.wikipedia.org/wiki/System>system</a> run to the <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish subtask</a>. Evaluation was carried out emoji-wise, and the final <a href=https://en.wikipedia.org/wiki/Ranking>ranking</a> was based on macro F-Score. Data and further information about this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> can be found at.<url>https://competitions.codalab.org/competitions/17344</url>.\n</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S18-1115.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S18-1115 data-toggle=collapse aria-expanded=false aria-controls=abstract-S18-1115 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/S18-1115/>SemEval-2018 Task 9 : Hypernym Discovery<span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2018 Task 9: Hypernym Discovery</a></strong><br><a href=/people/j/jose-camacho-collados/>Jose Camacho-Collados</a>
|
<a href=/people/c/claudio-delli-bovi/>Claudio Delli Bovi</a>
|
<a href=/people/l/luis-espinosa-anke/>Luis Espinosa-Anke</a>
|
<a href=/people/s/sergio-oramas/>Sergio Oramas</a>
|
<a href=/people/t/tommaso-pasini/>Tommaso Pasini</a>
|
<a href=/people/e/enrico-santus/>Enrico Santus</a>
|
<a href=/people/v/vered-shwartz/>Vered Shwartz</a>
|
<a href=/people/r/roberto-navigli/>Roberto Navigli</a>
|
<a href=/people/h/horacio-saggion/>Horacio Saggion</a><br><a href=/volumes/S18-1/ class=text-muted>Proceedings of The 12th International Workshop on Semantic Evaluation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S18-1115><div class="card-body p-3 small">This paper describes the SemEval 2018 Shared Task on Hypernym Discovery. We put forward this task as a complementary benchmark for modeling <a href=https://en.wikipedia.org/wiki/Hypernymy>hypernymy</a>, a problem which has traditionally been cast as a binary classification task, taking a pair of candidate words as input. Instead, our reformulated task is defined as follows : given an input term, retrieve (or discover) its suitable <a href=https://en.wikipedia.org/wiki/Hypernym>hypernyms</a> from a target corpus. We proposed five different subtasks covering three languages (English, Spanish, and Italian), and two specific domains of knowledge in English (Medical and Music). Participants were allowed to compete in any or all of the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>subtasks</a>. Overall, a total of 11 teams participated, with a total of 39 different systems submitted through all subtasks. Data, results and further information about the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> can be found at.<url>https://competitions.codalab.org/competitions/17119</url>.\n</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4000.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4000/>Proceedings of the Third Workshop on Semantic Deep Learning</a></strong><br><a href=/people/l/luis-espinosa-anke/>Luis Espinosa Anke</a>
|
<a href=/people/d/dagmar-gromann/>Dagmar Gromann</a>
|
<a href=/people/t/thierry-declerck/>Thierry Declerck</a><br><a href=/volumes/W18-40/ class=text-muted>Proceedings of the Third Workshop on Semantic Deep Learning</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2061.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2061 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2061 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-2061/>Syntactically Aware Neural Architectures for Definition Extraction</a></strong><br><a href=/people/l/luis-espinosa-anke/>Luis Espinosa-Anke</a>
|
<a href=/people/s/steven-schockaert/>Steven Schockaert</a><br><a href=/volumes/N18-2/ class=text-muted>Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2061><div class="card-body p-3 small">Automatically identifying definitional knowledge in text corpora (Definition Extraction or DE) is an important task with direct applications in, among others, Automatic Glossary Generation, Taxonomy Learning, Question Answering and Semantic Search. It is generally cast as a binary classification problem between definitional and non-definitional sentences. In this paper we present a set of neural architectures combining Convolutional and Recurrent Neural Networks, which are further enriched by incorporating linguistic information via syntactic dependencies. Our experimental results in the task of sentence classification, on two benchmarking DE datasets (one generic, one domain-specific), show that these models obtain consistent state of the art results. Furthermore, we demonstrate that <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> trained on clean Wikipedia-like definitions can successfully be applied to more noisy domain-specific corpora.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-6004.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-6004 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-6004 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/279154260 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N18-6004/>The interplay between lexical resources and <a href=https://en.wikipedia.org/wiki/Natural_language_processing>Natural Language Processing</a></a></strong><br><a href=/people/j/jose-camacho-collados/>Jose Camacho-Collados</a>
|
<a href=/people/l/luis-espinosa-anke/>Luis Espinosa Anke</a>
|
<a href=/people/m/mohammad-taher-pilehvar/>Mohammad Taher Pilehvar</a><br><a href=/volumes/N18-6/ class=text-muted>Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Tutorial Abstracts</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-6004><div class="card-body p-3 small">Incorporating linguistic, world and common sense knowledge into AI / NLP systems is currently an important research area, with several open problems and challenges. At the same time, processing and storing this knowledge in <a href=https://en.wikipedia.org/wiki/Lexical_resource>lexical resources</a> is not a straightforward task. We propose to address these complementary goals from two methodological perspectives : the use of NLP methods to help the process of constructing and enriching lexical resources and the use of lexical resources for improving NLP applications. This tutorial may be useful for two main types of audience : those working on language resources who are interested in becoming acquainted with automatic NLP techniques, with the end goal of speeding and/or easing up the process of resource curation ; and on the other hand, researchers in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> who would like to benefit from the knowledge of lexical resources to improve their systems and models.</div></div><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4402.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4402 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4402 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4402/>Towards the Understanding of Gaming Audiences by Modeling Twitch Emotes</a></strong><br><a href=/people/f/francesco-barbieri/>Francesco Barbieri</a>
|
<a href=/people/l/luis-espinosa-anke/>Luis Espinosa-Anke</a>
|
<a href=/people/m/miguel-ballesteros/>Miguel Ballesteros</a>
|
<a href=/people/j/juan-soler-company/>Juan Soler-Company</a>
|
<a href=/people/h/horacio-saggion/>Horacio Saggion</a><br><a href=/volumes/W17-44/ class=text-muted>Proceedings of the 3rd Workshop on Noisy User-generated Text</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4402><div class="card-body p-3 small">Videogame streaming platforms have become a paramount example of noisy user-generated text. These are websites where <a href=https://en.wikipedia.org/wiki/Video_game>gaming</a> is broadcasted, and allows interaction with viewers via integrated chatrooms. Probably the best known <a href=https://en.wikipedia.org/wiki/Computing_platform>platform</a> of this kind is <a href=https://en.wikipedia.org/wiki/Twitch.tv>Twitch</a>, which has more than 100 million monthly viewers. Despite these numbers, and unlike other <a href=https://en.wikipedia.org/wiki/Computing_platform>platforms</a> featuring short messages (e.g. Twitter), <a href=https://en.wikipedia.org/wiki/Twitch.tv>Twitch</a> has not received much attention from the Natural Language Processing community. In this paper we aim at bridging this gap by proposing two important tasks specific to the Twitch platform, namely (1) Emote prediction ; and (2) Trolling detection. In our experiments, we evaluate three models : a BOW baseline, a logistic supervised classifiers based on word embeddings, and a bidirectional long short-term memory recurrent neural network (LSTM). Our results show that the LSTM model outperforms the other two models, where explicit features with proven effectiveness for similar tasks were encoded.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Luis+Espinosa+Anke" title="Search for 'Luis Espinosa Anke' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/j/jose-camacho-collados/ class=align-middle>Jose Camacho-Collados</a>
<span class="badge badge-secondary align-middle ml-2">6</span></li><li class=list-group-item><a href=/people/s/steven-schockaert/ class=align-middle>Steven Schockaert</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/people/f/francesco-barbieri/ class=align-middle>Francesco Barbieri</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/h/horacio-saggion/ class=align-middle>Horacio Saggion</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/m/miguel-ballesteros/ class=align-middle>Miguel Ballesteros</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/d/dagmar-gromann/ class=align-middle>Dagmar Gromann</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/t/thierry-declerck/ class=align-middle>Thierry Declerck</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/m/mohammad-taher-pilehvar/ class=align-middle>Mohammad Taher Pilehvar</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/a/asahi-ushio/ class=align-middle>Asahi Ushio</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/b/beatriz-fisas/ class=align-middle>Beatriz Fisas</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/joan-codina-filba/ class=align-middle>Joan Codina-Filbá</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/leo-wanner/ class=align-middle>Leo Wanner</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/juan-soler-company/ class=align-middle>Juan Soler-Company</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/leonardo-neves/ class=align-middle>Leonardo Neves</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/f/francesco-ronzano/ class=align-middle>Francesco Ronzano</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/v/valerio-basile/ class=align-middle>Valerio Basile</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/v/viviana-patti/ class=align-middle>Viviana Patti</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/claudio-delli-bovi/ class=align-middle>Claudio Delli Bovi</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/sergio-oramas/ class=align-middle>Sergio Oramas</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/tommaso-pasini/ class=align-middle>Tommaso Pasini</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/e/enrico-santus/ class=align-middle>Enrico Santus</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/v/vered-shwartz/ class=align-middle>Vered Shwartz</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/roberto-navigli/ class=align-middle>Roberto Navigli</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/carla-perez-almendros/ class=align-middle>Carla Perez Almendros</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/i/israa-alghanmi/ class=align-middle>Israa Alghanmi</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/ws/ class=align-middle>WS</a><span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/venues/semeval/ class=align-middle>SemEval</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/naacl/ class=align-middle>NAACL</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/acl/ class=align-middle>ACL</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/mwe/ class=align-middle>MWE</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-venues aria-expanded=false aria-controls=more-venues>show all...</li><div class="collapse border-top" id=more-venues><li class=list-group-item><a href=/venues/findings/ class=align-middle>Findings</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/coling/ class=align-middle>COLING</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/wnut/ class=align-middle>WNUT</a><span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>