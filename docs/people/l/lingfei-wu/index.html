<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Lingfei Wu - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Lingfei</span> <span class=font-weight-bold>Wu</span></h2><hr><div class=row><div class=col-lg-9><h4>2022</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.acl-long.403.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--acl-long--403 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.acl-long.403 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2022.acl-long.403.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.acl-long.403/>Feeding What You Need by Understanding What You Learned</a></strong><br><a href=/people/x/xiaoqiang-wang/>Xiaoqiang Wang</a>
|
<a href=/people/b/bang-liu/>Bang Liu</a>
|
<a href=/people/f/fangli-xu/>Fangli Xu</a>
|
<a href=/people/b/bo-long/>Bo Long</a>
|
<a href=/people/s/siliang-tang/>Siliang Tang</a>
|
<a href=/people/l/lingfei-wu/>Lingfei Wu</a><br><a href=/volumes/2022.acl-long/ class=text-muted>Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--acl-long--403><div class="card-body p-3 small">Machine Reading Comprehension MRC reveals the ability to understand a given text passage and answer questions based on it Existing research works in MRC rely heavily on large size models and corpus to improve the performance evaluated by <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> such as Exact Match EM$ and F_1$. However such a <a href=https://en.wikipedia.org/wiki/Paradigm>paradigm</a> lacks sufficient interpretation to model capability and can not efficiently train a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> with a large corpus In this paper we argue that a deep understanding of <a href=https://en.wikipedia.org/wiki/Statistical_model>model capabilities</a> and data properties can help us feed a <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> with appropriate training data based on its learning status Specifically we design an MRC capability assessment framework that assesses model capabilities in an explainable and multi dimensional manner Based on <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> we further uncover and disentangle the connections between various <a href=https://en.wikipedia.org/wiki/Property_(philosophy)>data properties</a> and model performance Finally to verify the effectiveness of the proposed MRC capability assessment framework we incorporate it into a curriculum learning pipeline and devise a Capability Boundary Breakthrough Curriculum CBBC strategy which performs a model capability based training to maximize the data value and improve training efficiency Extensive experiments demonstrate that our <a href=https://en.wikipedia.org/wiki/Design_of_experiments>approach</a> significantly improves performance achieving up to an 11.22\\% 8.71\\% improvement of EM$ F_1 on <a href=https://en.wikipedia.org/wiki/Functional_magnetic_resonance_imaging>MRC tasks</a><tex-math>EM</tex-math>) and <tex-math>F_1</tex-math>. However, such a paradigm lacks sufficient interpretation to model capability and can not efficiently train a model with a large corpus. In this paper, we argue that a deep understanding of model capabilities and data properties can help us feed a model with appropriate training data based on its learning status. Specifically, we design an MRC capability assessment framework that assesses model capabilities in an explainable and multi-dimensional manner. Based on it, we further uncover and disentangle the connections between various data properties and model performance. Finally, to verify the effectiveness of the proposed MRC capability assessment framework, we incorporate it into a curriculum learning pipeline and devise a Capability Boundary Breakthrough Curriculum (CBBC) strategy, which performs a model capability-based training to maximize the data value and improve training efficiency. Extensive experiments demonstrate that our approach significantly improves performance, achieving up to an 11.22% / 8.71% improvement of <tex-math>EM</tex-math> / <tex-math>F_1</tex-math> on MRC tasks.</div></div><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.432.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--432 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.432 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.432/>Adversarial Attack against Cross-lingual Knowledge Graph Alignment</a></strong><br><a href=/people/z/zeru-zhang/>Zeru Zhang</a>
|
<a href=/people/z/zijie-zhang/>Zijie Zhang</a>
|
<a href=/people/y/yang-zhou/>Yang Zhou</a>
|
<a href=/people/l/lingfei-wu/>Lingfei Wu</a>
|
<a href=/people/s/sixing-wu/>Sixing Wu</a>
|
<a href=/people/x/xiaoying-han/>Xiaoying Han</a>
|
<a href=/people/d/dejing-dou/>Dejing Dou</a>
|
<a href=/people/t/tianshi-che/>Tianshi Che</a>
|
<a href=/people/d/da-yan/>Da Yan</a><br><a href=/volumes/2021.emnlp-main/ class=text-muted>Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--432><div class="card-body p-3 small">Recent literatures have shown that knowledge graph (KG) learning models are highly vulnerable to adversarial attacks. However, there is still a paucity of vulnerability analyses of cross-lingual entity alignment under adversarial attacks. This paper proposes an adversarial attack model with two novel attack techniques to perturb the KG structure and degrade the quality of deep cross-lingual entity alignment. First, an entity density maximization method is employed to hide the attacked entities in dense regions in two KGs, such that the derived perturbations are unnoticeable. Second, an attack signal amplification method is developed to reduce the gradient vanishing issues in the process of adversarial attacks for further improving the attack effectiveness.</div></div><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-demos.13.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-demos--13 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-demos.13 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.emnlp-demos.13.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-demos.13" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.emnlp-demos.13/>A Technical Question Answering System with Transfer Learning</a></strong><br><a href=/people/w/wenhao-yu/>Wenhao Yu</a>
|
<a href=/people/l/lingfei-wu/>Lingfei Wu</a>
|
<a href=/people/y/yu-deng/>Yu Deng</a>
|
<a href=/people/r/ruchi-mahindru/>Ruchi Mahindru</a>
|
<a href=/people/q/qingkai-zeng/>Qingkai Zeng</a>
|
<a href=/people/s/sinem-guven/>Sinem Guven</a>
|
<a href=/people/m/meng-jiang/>Meng Jiang</a><br><a href=/volumes/2020.emnlp-demos/ class=text-muted>Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-demos--13><div class="card-body p-3 small">In recent years, the need for community technical question-answering sites has increased significantly. However, it is often expensive for human experts to provide timely and helpful responses on those forums. We develop TransTQA, which is a novel system that offers automatic responses by retrieving proper answers based on correctly answered similar questions in the past. TransTQA is built upon a siamese ALBERT network, which enables it to respond quickly and accurately. Furthermore, TransTQA adopts a standard deep transfer learning strategy to improve its capability of supporting multiple technical domains.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.acl-main.713.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--acl-main--713 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.acl-main.713 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://slideslive.com/38929257 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.acl-main.713/>A Joint Neural Model for <a href=https://en.wikipedia.org/wiki/Information_extraction>Information Extraction</a> with Global Features</a></strong><br><a href=/people/y/ying-lin/>Ying Lin</a>
|
<a href=/people/h/heng-ji/>Heng Ji</a>
|
<a href=/people/f/fei-huang/>Fei Huang</a>
|
<a href=/people/l/lingfei-wu/>Lingfei Wu</a><br><a href=/volumes/2020.acl-main/ class=text-muted>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--acl-main--713><div class="card-body p-3 small">Most existing joint neural models for Information Extraction (IE) use local task-specific classifiers to predict labels for individual instances (e.g., trigger, relation) regardless of their interactions. For example, a victim of a die event is likely to be a victim of an attack event in the same sentence. In order to capture such cross-subtask and cross-instance inter-dependencies, we propose a joint neural framework, OneIE, that aims to extract the globally optimal IE result as a <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph</a> from an input sentence. OneIE performs end-to-end IE in four stages : (1) Encoding a given sentence as contextualized word representations ; (2) Identifying entity mentions and event triggers as <a href=https://en.wikipedia.org/wiki/Vertex_(graph_theory)>nodes</a> ; (3) Computing label scores for all <a href=https://en.wikipedia.org/wiki/Vertex_(graph_theory)>nodes</a> and their pairwise links using local classifiers ; (4) Searching for the globally optimal graph with a beam decoder. At the decoding stage, we incorporate global features to capture the cross-subtask and cross-instance interactions. Experiments show that adding global features improves the performance of our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> and achieves new state of-the-art on all subtasks. In addition, as OneIE does not use any language-specific feature, we prove it can be easily applied to new languages or trained in a multilingual manner.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.255.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--255 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.255 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.findings-emnlp.255.OptionalSupplementaryMaterial.pdf data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.255" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.255/>Graph-to-Tree Neural Networks for Learning Structured Input-Output Translation with Applications to Semantic Parsing and Math Word Problem</a></strong><br><a href=/people/s/shucheng-li/>Shucheng Li</a>
|
<a href=/people/l/lingfei-wu/>Lingfei Wu</a>
|
<a href=/people/s/shiwei-feng/>Shiwei Feng</a>
|
<a href=/people/f/fangli-xu/>Fangli Xu</a>
|
<a href=/people/f/fengyuan-xu/>Fengyuan Xu</a>
|
<a href=/people/s/sheng-zhong/>Sheng Zhong</a><br><a href=/volumes/2020.findings-emnlp/ class=text-muted>Findings of the Association for Computational Linguistics: EMNLP 2020</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--255><div class="card-body p-3 small">The celebrated Seq2Seq technique and its numerous variants achieve excellent performance on many tasks such as <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a>, <a href=https://en.wikipedia.org/wiki/Semantic_parsing>semantic parsing</a>, and math word problem solving. However, these models either only consider input objects as <a href=https://en.wikipedia.org/wiki/Sequence>sequences</a> while ignoring the important structural information for <a href=https://en.wikipedia.org/wiki/Code>encoding</a>, or they simply treat output objects as sequence outputs instead of structural objects for decoding. In this paper, we present a novel Graph-to-Tree Neural Networks, namely Graph2Tree consisting of a graph encoder and a hierarchical tree decoder, that encodes an augmented graph-structured input and decodes a tree-structured output. In particular, we investigated our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> for solving two problems, neural semantic parsing and math word problem. Our extensive experiments demonstrate that our Graph2Tree model outperforms or matches the performance of other state-of-the-art models on these tasks.</div></div><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5319.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5319 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5319 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5319/>Graph Enhanced Cross-Domain Text-to-SQL Generation<span class=acl-fixed-case>SQL</span> Generation</a></strong><br><a href=/people/s/siyu-huo/>Siyu Huo</a>
|
<a href=/people/t/tengfei-ma/>Tengfei Ma</a>
|
<a href=/people/j/jie-chen/>Jie Chen</a>
|
<a href=/people/m/maria-chang/>Maria Chang</a>
|
<a href=/people/l/lingfei-wu/>Lingfei Wu</a>
|
<a href=/people/m/michael-j-witbrock/>Michael Witbrock</a><br><a href=/volumes/D19-53/ class=text-muted>Proceedings of the Thirteenth Workshop on Graph-Based Methods for Natural Language Processing (TextGraphs-13)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5319><div class="card-body p-3 small">Semantic parsing is a fundamental problem in <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language understanding</a>, as it involves the mapping of natural language to structured forms such as <a href=https://en.wikipedia.org/wiki/Executable>executable queries</a> or <a href=https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning>logic-like knowledge representations</a>. Existing deep learning approaches for <a href=https://en.wikipedia.org/wiki/Semantic_parsing>semantic parsing</a> have shown promise on a variety of benchmark data sets, particularly on text-to-SQL parsing. However, most text-to-SQL parsers do not generalize to unseen data sets in different domains. In this paper, we propose a new cross-domain learning scheme to perform text-to-SQL translation and demonstrate its use on Spider, a large-scale cross-domain text-to-SQL data set. We improve upon a state-of-the-art Spider model, SyntaxSQLNet, by constructing a graph of column names for all databases and using graph neural networks to compute their embeddings. The resulting embeddings offer better cross-domain representations and <a href=https://en.wikipedia.org/wiki/SQL>SQL queries</a>, as evidenced by substantial improvement on the Spider data set compared to SyntaxSQLNet.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1299.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1299 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1299 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/356071812 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1299" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1299/>Bidirectional Attentive Memory Networks for Question Answering over Knowledge Bases</a></strong><br><a href=/people/y/yu-chen/>Yu Chen</a>
|
<a href=/people/l/lingfei-wu/>Lingfei Wu</a>
|
<a href=/people/m/mohammed-j-zaki/>Mohammed J. Zaki</a><br><a href=/volumes/N19-1/ class=text-muted>Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1299><div class="card-body p-3 small">When answering <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language questions</a> over knowledge bases (KBs), different question components and KB aspects play different roles. However, most existing embedding-based methods for knowledge base question answering (KBQA) ignore the subtle inter-relationships between the question and the KB (e.g., <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entity types</a>, <a href=https://en.wikipedia.org/wiki/Relation_(database)>relation paths</a> and <a href=https://en.wikipedia.org/wiki/Context_(language_use)>context</a>). In this work, we propose to directly model the two-way flow of interactions between the questions and the KB via a novel Bidirectional Attentive Memory Network, called BAMnet. Requiring no external resources and only very few hand-crafted features, on the WebQuestions benchmark, our method significantly outperforms existing information-retrieval based methods, and remains competitive with (hand-crafted) semantic parsing based methods. Also, since we use <a href=https://en.wikipedia.org/wiki/Attentional_control>attention mechanisms</a>, our method offers better <a href=https://en.wikipedia.org/wiki/Interpretability>interpretability</a> compared to other baselines.</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1112.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1112 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1112 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/305213739 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1112" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1112/>SQL-to-Text Generation with Graph-to-Sequence Model<span class=acl-fixed-case>SQL</span>-to-Text Generation with Graph-to-Sequence Model</a></strong><br><a href=/people/k/kun-xu/>Kun Xu</a>
|
<a href=/people/l/lingfei-wu/>Lingfei Wu</a>
|
<a href=/people/z/zhiguo-wang/>Zhiguo Wang</a>
|
<a href=/people/y/yansong-feng/>Yansong Feng</a>
|
<a href=/people/v/vadim-sheinin/>Vadim Sheinin</a><br><a href=/volumes/D18-1/ class=text-muted>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1112><div class="card-body p-3 small">Previous work approaches the SQL-to-text generation task using vanilla Seq2Seq models, which may not fully capture the inherent <a href=https://en.wikipedia.org/wiki/Graph_(abstract_data_type)>graph-structured information</a> in <a href=https://en.wikipedia.org/wiki/SQL>SQL query</a>. In this paper, we propose a graph-to-sequence model to encode the global structure information into node embeddings. This <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> can effectively learn the correlation between the SQL query pattern and its interpretation. Experimental results on the WikiSQL dataset and Stackoverflow dataset show that our model outperforms the Seq2Seq and Tree2Seq baselines, achieving the state-of-the-art performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1482.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1482 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1482 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D18-1482.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1482" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D18-1482/>Word Mover’s Embedding : From Word2Vec to Document Embedding<span class=acl-fixed-case>W</span>ord2<span class=acl-fixed-case>V</span>ec to Document Embedding</a></strong><br><a href=/people/l/lingfei-wu/>Lingfei Wu</a>
|
<a href=/people/i/ian-en-hsu-yen/>Ian En-Hsu Yen</a>
|
<a href=/people/k/kun-xu/>Kun Xu</a>
|
<a href=/people/f/fangli-xu/>Fangli Xu</a>
|
<a href=/people/a/avinash-balakrishnan/>Avinash Balakrishnan</a>
|
<a href=/people/p/pin-yu-chen/>Pin-Yu Chen</a>
|
<a href=/people/p/pradeep-ravikumar/>Pradeep Ravikumar</a>
|
<a href=/people/m/michael-j-witbrock/>Michael J. Witbrock</a><br><a href=/volumes/D18-1/ class=text-muted>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1482><div class="card-body p-3 small">While the celebrated Word2Vec technique yields semantically rich representations for individual words, there has been relatively less success in extending to generate unsupervised sentences or documents embeddings. Recent work has demonstrated that a distance measure between documents called Word Mover&#8217;s Distance (WMD) that aligns semantically similar words, yields unprecedented KNN classification accuracy. However, WMD is expensive to compute, and it is hard to extend its use beyond a KNN classifier. In this paper, we propose the Word Mover&#8217;s Embedding (WME), a novel approach to building an unsupervised document (sentence) embedding from pre-trained word embeddings. In our experiments on 9 benchmark text classification datasets and 22 textual similarity tasks, the proposed technique consistently matches or outperforms state-of-the-art techniques, with significantly higher accuracy on problems of short length.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Lingfei+Wu" title="Search for 'Lingfei Wu' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/f/fangli-xu/ class=align-middle>Fangli Xu</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/k/kun-xu/ class=align-middle>Kun Xu</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/m/michael-j-witbrock/ class=align-middle>Michael J. Witbrock</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/w/wenhao-yu/ class=align-middle>Wenhao Yu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yu-deng/ class=align-middle>Yu Deng</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/r/ruchi-mahindru/ class=align-middle>Ruchi Mahindru</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/q/qingkai-zeng/ class=align-middle>Qingkai Zeng</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/sinem-guven/ class=align-middle>Sinem Guven</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/meng-jiang/ class=align-middle>Meng Jiang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/ying-lin/ class=align-middle>Ying Lin</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/heng-ji/ class=align-middle>Heng Ji</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/f/fei-huang/ class=align-middle>Fei Huang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/x/xiaoqiang-wang/ class=align-middle>Xiaoqiang Wang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/b/bang-liu/ class=align-middle>Bang Liu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/b/bo-long/ class=align-middle>Bo Long</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/siliang-tang/ class=align-middle>Siliang Tang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/zhiguo-wang/ class=align-middle>Zhiguo Wang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yansong-feng/ class=align-middle>Yansong Feng</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/v/vadim-sheinin/ class=align-middle>Vadim Sheinin</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/i/ian-en-hsu-yen/ class=align-middle>Ian En-Hsu Yen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/avinash-balakrishnan/ class=align-middle>Avinash Balakrishnan</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/p/pin-yu-chen/ class=align-middle>Pin-Yu Chen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/p/pradeep-ravikumar/ class=align-middle>Pradeep Ravikumar</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/zeru-zhang/ class=align-middle>Zeru Zhang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/zijie-zhang/ class=align-middle>Zijie Zhang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yang-zhou/ class=align-middle>Yang Zhou</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/sixing-wu/ class=align-middle>Sixing Wu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/x/xiaoying-han/ class=align-middle>Xiaoying Han</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/dejing-dou/ class=align-middle>Dejing Dou</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/tianshi-che/ class=align-middle>Tianshi Che</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/da-yan/ class=align-middle>Da Yan</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/siyu-huo/ class=align-middle>Siyu Huo</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/tengfei-ma/ class=align-middle>Tengfei Ma</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jie-chen/ class=align-middle>Jie Chen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/maria-chang/ class=align-middle>Maria Chang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/shucheng-li/ class=align-middle>Shucheng Li</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/shiwei-feng/ class=align-middle>Shiwei Feng</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/f/fengyuan-xu/ class=align-middle>Fengyuan Xu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/sheng-zhong/ class=align-middle>Sheng Zhong</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yu-chen/ class=align-middle>Yu Chen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/mohammed-j-zaki/ class=align-middle>Mohammed J. Zaki</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/emnlp/ class=align-middle>EMNLP</a><span class="badge badge-secondary align-middle ml-2">5</span></li><li class=list-group-item><a href=/venues/acl/ class=align-middle>ACL</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/findings/ class=align-middle>Findings</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/naacl/ class=align-middle>NAACL</a><span class="badge badge-secondary align-middle ml-2">1</span></li></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>