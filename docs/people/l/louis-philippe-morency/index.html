<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Louis-Philippe Morency - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Louis-Philippe</span> <span class=font-weight-bold>Morency</span></h2><hr><div class=row><div class=col-lg-9><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.maiworkshop-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.maiworkshop-1.0/>Proceedings of the Third Workshop on Multimodal Artificial Intelligence</a></strong><br><a href=/people/a/amir-zadeh/>Amir Zadeh</a>
|
<a href=/people/l/louis-philippe-morency/>Louis-Philippe Morency</a>
|
<a href=/people/p/paul-pu-liang/>Paul Pu Liang</a>
|
<a href=/people/c/candace-ross/>Candace Ross</a>
|
<a href=/people/r/ruslan-salakhutdinov/>Ruslan Salakhutdinov</a>
|
<a href=/people/s/soujanya-poria/>Soujanya Poria</a>
|
<a href=/people/e/erik-cambria/>Erik Cambria</a>
|
<a href=/people/k/kelly-shi/>Kelly Shi</a><br><a href=/volumes/2021.maiworkshop-1/ class=text-muted>Proceedings of the Third Workshop on Multimodal Artificial Intelligence</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.79.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--79 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.79 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.naacl-main.79" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.79/>MTAG : Modal-Temporal Attention Graph for Unaligned Human Multimodal Language Sequences<span class=acl-fixed-case>MTAG</span>: Modal-Temporal Attention Graph for Unaligned Human Multimodal Language Sequences</a></strong><br><a href=/people/j/jianing-yang/>Jianing Yang</a>
|
<a href=/people/y/yongxin-wang/>Yongxin Wang</a>
|
<a href=/people/r/ruitao-yi/>Ruitao Yi</a>
|
<a href=/people/y/yuying-zhu/>Yuying Zhu</a>
|
<a href=/people/a/azaan-rehman/>Azaan Rehman</a>
|
<a href=/people/a/amir-zadeh/>Amir Zadeh</a>
|
<a href=/people/s/soujanya-poria/>Soujanya Poria</a>
|
<a href=/people/l/louis-philippe-morency/>Louis-Philippe Morency</a><br><a href=/volumes/2021.naacl-main/ class=text-muted>Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--79><div class="card-body p-3 small">Human communication is multimodal in nature ; it is through multiple modalities such as <a href=https://en.wikipedia.org/wiki/Language>language</a>, <a href=https://en.wikipedia.org/wiki/Human_voice>voice</a>, and <a href=https://en.wikipedia.org/wiki/Facial_expression>facial expressions</a>, that opinions and emotions are expressed. Data in this <a href=https://en.wikipedia.org/wiki/Domain_(biology)>domain</a> exhibits complex multi-relational and temporal interactions. Learning from this <a href=https://en.wikipedia.org/wiki/Data>data</a> is a fundamentally challenging research problem. In this paper, we propose Modal-Temporal Attention Graph (MTAG). MTAG is an interpretable graph-based neural model that provides a suitable framework for analyzing multimodal sequential data. We first introduce a procedure to convert unaligned multimodal sequence data into a <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph</a> with heterogeneous nodes and edges that captures the rich interactions across modalities and through time. Then, a novel graph fusion operation, called MTAG fusion, along with a dynamic pruning and read-out technique, is designed to efficiently process this modal-temporal graph and capture various interactions. By learning to focus only on the important interactions within the <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph</a>, MTAG achieves state-of-the-art performance on multimodal sentiment analysis and emotion recognition benchmarks, while utilizing significantly fewer model parameters.</div></div><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.143.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--143 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.143 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.emnlp-main.143.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938697 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.143" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.143/>Multimodal Routing : Improving Local and Global Interpretability of Multimodal Language Analysis</a></strong><br><a href=/people/y/yao-hung-hubert-tsai/>Yao-Hung Hubert Tsai</a>
|
<a href=/people/m/martin-ma/>Martin Ma</a>
|
<a href=/people/m/muqiao-yang/>Muqiao Yang</a>
|
<a href=/people/r/ruslan-salakhutdinov/>Ruslan Salakhutdinov</a>
|
<a href=/people/l/louis-philippe-morency/>Louis-Philippe Morency</a><br><a href=/volumes/2020.emnlp-main/ class=text-muted>Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--143><div class="card-body p-3 small">The <a href=https://en.wikipedia.org/wiki/Human_language>human language</a> can be expressed through multiple sources of information known as <a href=https://en.wikipedia.org/wiki/Linguistic_modality>modalities</a>, including <a href=https://en.wikipedia.org/wiki/Tone_(linguistics)>tones of voice</a>, facial gestures, and <a href=https://en.wikipedia.org/wiki/Spoken_language>spoken language</a>. Recent <a href=https://en.wikipedia.org/wiki/Multimodal_learning>multimodal learning</a> with strong performances on human-centric tasks such as <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> and <a href=https://en.wikipedia.org/wiki/Emotion_recognition>emotion recognition</a> are often black-box, with very limited interpretability. In this paper we propose, which dynamically adjusts weights between input modalities and output representations differently for each input sample. Multimodal routing can identify relative importance of both individual modalities and cross-modality factors. Moreover, the weight assignment by <a href=https://en.wikipedia.org/wiki/Routing>routing</a> allows us to interpret modality-prediction relationships not only globally (i.e. general trends over the whole dataset), but also locally for each single input sample, meanwhile keeping competitive performance compared to state-of-the-art methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.acl-main.214.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--acl-main--214 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.acl-main.214 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://slideslive.com/38929139 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.acl-main.214" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.acl-main.214/>Integrating Multimodal Information in Large Pretrained Transformers</a></strong><br><a href=/people/w/wasifur-rahman/>Wasifur Rahman</a>
|
<a href=/people/m/md-kamrul-hasan/>Md Kamrul Hasan</a>
|
<a href=/people/s/sangwu-lee/>Sangwu Lee</a>
|
<a href=/people/a/amirali-bagher-zadeh/>AmirAli Bagher Zadeh</a>
|
<a href=/people/c/chengfeng-mao/>Chengfeng Mao</a>
|
<a href=/people/l/louis-philippe-morency/>Louis-Philippe Morency</a>
|
<a href=/people/e/ehsan-hoque/>Ehsan Hoque</a><br><a href=/volumes/2020.acl-main/ class=text-muted>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--acl-main--214><div class="card-body p-3 small">Recent Transformer-based contextual word representations, including <a href=https://en.wikipedia.org/wiki/BERT>BERT</a> and XLNet, have shown state-of-the-art performance in multiple disciplines within <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a>. Fine-tuning the trained contextual models on task-specific datasets has been the key to achieving superior performance downstream. While fine-tuning these pre-trained models is straightforward for lexical applications (applications with only language modality), it is not trivial for multimodal language (a growing area in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> focused on modeling face-to-face communication). More specifically, this is due to the fact that pre-trained models do n&#8217;t have the necessary components to accept two extra modalities of <a href=https://en.wikipedia.org/wiki/Visual_perception>vision</a> and <a href=https://en.wikipedia.org/wiki/Acoustics>acoustic</a>. In this paper, we proposed an attachment to BERT and XLNet called Multimodal Adaptation Gate (MAG). MAG allows BERT and XLNet to accept multimodal nonverbal data during <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a>. It does so by generating a shift to internal representation of BERT and XLNet ; a shift that is conditioned on the visual and acoustic modalities. In our experiments, we study the commonly used CMU-MOSI and CMU-MOSEI datasets for <a href=https://en.wikipedia.org/wiki/Multimodal_sentiment_analysis>multimodal sentiment analysis</a>. Fine-tuning MAG-BERT and MAG-XLNet significantly boosts the <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> performance over previous baselines as well as language-only fine-tuning of <a href=https://en.wikipedia.org/wiki/BERT>BERT</a> and XLNet. On the CMU-MOSI dataset, MAG-XLNet achieves human-level multimodal sentiment analysis performance for the first time in the NLP community.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.acl-main.625.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--acl-main--625 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.acl-main.625 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://slideslive.com/38928907 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.acl-main.625/>Language to Network : Conditional Parameter Adaptation with Natural Language Descriptions</a></strong><br><a href=/people/t/tian-jin/>Tian Jin</a>
|
<a href=/people/z/zhun-liu/>Zhun Liu</a>
|
<a href=/people/s/shengjia-yan/>Shengjia Yan</a>
|
<a href=/people/a/alexandre-eichenberger/>Alexandre Eichenberger</a>
|
<a href=/people/l/louis-philippe-morency/>Louis-Philippe Morency</a><br><a href=/volumes/2020.acl-main/ class=text-muted>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--acl-main--625><div class="card-body p-3 small">Transfer learning using ImageNet pre-trained models has been the de facto approach in a wide range of <a href=https://en.wikipedia.org/wiki/Computer_vision>computer vision tasks</a>. However, <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a> still requires task-specific training data. In this paper, we propose N3 (Neural Networks from Natural Language)-a new paradigm of synthesizing task-specific neural networks from language descriptions and a generic pre-trained model. N3 leverages <a href=https://en.wikipedia.org/wiki/Linguistic_description>language descriptions</a> to generate parameter adaptations as well as a new task-specific classification layer for a pre-trained neural network, effectively fine-tuning the network for a new task using only <a href=https://en.wikipedia.org/wiki/Linguistic_description>language descriptions</a> as input. To the best of our knowledge, N3 is the first method to synthesize entire <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a> from <a href=https://en.wikipedia.org/wiki/Natural_language>natural language</a>. Experimental results show that N3 can out-perform previous natural-language based zero-shot learning methods across 4 different zero-shot image classification benchmarks. We also demonstrate a simple method to help identify keywords in <a href=https://en.wikipedia.org/wiki/Linguistic_description>language descriptions</a> leveraged by N3 when synthesizing <a href=https://en.wikipedia.org/wiki/Parameter_(computer_programming)>model parameters</a>.<b>N3</b> (<b>N</b>eural <b>N</b>etworks from <b>N</b>atural Language) - a new paradigm of synthesizing task-specific neural networks from language descriptions and a generic pre-trained model. <b>N3</b> leverages language descriptions to generate parameter adaptations as well as a new task-specific classification layer for a pre-trained neural network, effectively &#8220;fine-tuning&#8221; the network for a new task using only language descriptions as input. To the best of our knowledge, <b>N3</b> is the first method to synthesize entire neural networks from natural language. Experimental results show that <b>N3</b> can out-perform previous natural-language based zero-shot learning methods across 4 different zero-shot image classification benchmarks. We also demonstrate a simple method to help identify keywords in language descriptions leveraged by <b>N3</b> when synthesizing model parameters.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.challengehml-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.challengehml-1.0/>Second Grand-Challenge and Workshop on Multimodal Language (Challenge-HML)</a></strong><br><a href=/people/a/amir-zadeh/>Amir Zadeh</a>
|
<a href=/people/l/louis-philippe-morency/>Louis-Philippe Morency</a>
|
<a href=/people/p/paul-pu-liang/>Paul Pu Liang</a>
|
<a href=/people/s/soujanya-poria/>Soujanya Poria</a><br><a href=/volumes/2020.challengehml-1/ class=text-muted>Second Grand-Challenge and Workshop on Multimodal Language (Challenge-HML)</a></span></p><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1211.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1211 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1211 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1211/>UR-FUNNY : A Multimodal Language Dataset for Understanding Humor<span class=acl-fixed-case>UR</span>-<span class=acl-fixed-case>FUNNY</span>: A Multimodal Language Dataset for Understanding Humor</a></strong><br><a href=/people/m/md-kamrul-hasan/>Md Kamrul Hasan</a>
|
<a href=/people/w/wasifur-rahman/>Wasifur Rahman</a>
|
<a href=/people/a/amirali-bagher-zadeh/>AmirAli Bagher Zadeh</a>
|
<a href=/people/j/jianyuan-zhong/>Jianyuan Zhong</a>
|
<a href=/people/m/md-iftekhar-tanveer/>Md Iftekhar Tanveer</a>
|
<a href=/people/l/louis-philippe-morency/>Louis-Philippe Morency</a>
|
<a href=/people/m/mohammed-ehsan-hoque/>Mohammed (Ehsan) Hoque</a><br><a href=/volumes/D19-1/ class=text-muted>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1211><div class="card-body p-3 small">Humor is a unique and creative communicative behavior often displayed during <a href=https://en.wikipedia.org/wiki/Social_relation>social interactions</a>. It is produced in a multimodal manner, through the usage of words (text), gestures (visual) and prosodic cues (acoustic). Understanding <a href=https://en.wikipedia.org/wiki/Humour>humor</a> from these three modalities falls within boundaries of multimodal language ; a recent research trend in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a> that models <a href=https://en.wikipedia.org/wiki/Natural_language>natural language</a> as it happens in face-to-face communication. Although humor detection is an established research area in <a href=https://en.wikipedia.org/wiki/Neuro-linguistic_programming>NLP</a>, in a multimodal context it has been understudied. This paper presents a diverse multimodal dataset, called UR-FUNNY, to open the door to understanding multimodal language used in expressing humor. The <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> and accompanying studies, present a <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> in multimodal humor detection for the natural language processing community. UR-FUNNY is publicly available for research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1443.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1443 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1443 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1443/>Transformer Dissection : An Unified Understanding for Transformer’s Attention via the Lens of Kernel</a></strong><br><a href=/people/y/yao-hung-hubert-tsai/>Yao-Hung Hubert Tsai</a>
|
<a href=/people/s/shaojie-bai/>Shaojie Bai</a>
|
<a href=/people/m/makoto-yamada/>Makoto Yamada</a>
|
<a href=/people/l/louis-philippe-morency/>Louis-Philippe Morency</a>
|
<a href=/people/r/ruslan-salakhutdinov/>Ruslan Salakhutdinov</a><br><a href=/volumes/D19-1/ class=text-muted>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1443><div class="card-body p-3 small">Transformer is a powerful architecture that achieves superior performance on various sequence learning tasks, including <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a>, <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>language understanding</a>, and sequence prediction. At the core of the <a href=https://en.wikipedia.org/wiki/Transformer>Transformer</a> is the <a href=https://en.wikipedia.org/wiki/Attentional_control>attention mechanism</a>, which concurrently processes all inputs in the streams. In this paper, we present a new formulation of <a href=https://en.wikipedia.org/wiki/Attention>attention</a> via the lens of the <a href=https://en.wikipedia.org/wiki/Kernel_(linear_algebra)>kernel</a>. To be more precise, we realize that the <a href=https://en.wikipedia.org/wiki/Attention>attention</a> can be seen as applying kernel smoother over the inputs with the kernel scores being the similarities between inputs. This new formulation gives us a better way to understand individual components of the Transformer&#8217;s attention, such as the better way to integrate the positional embedding. Another important advantage of our kernel-based formulation is that it paves the way to a larger space of composing Transformer&#8217;s attention. As an example, we propose a new variant of Transformer&#8217;s attention which models the input as a product of symmetric kernels. This approach achieves competitive performance to the current state of the art <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> with less <a href=https://en.wikipedia.org/wiki/Computation>computation</a>. In our experiments, we empirically study different kernel construction strategies on two widely used tasks : <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a> and sequence prediction.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1267.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1267 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1267 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N19-1267.Supplementary.pdf data-toggle=tooltip data-placement=top title=Supplementary><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/364226255 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1267" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1267/>Strong and Simple Baselines for Multimodal Utterance Embeddings</a></strong><br><a href=/people/p/paul-pu-liang/>Paul Pu Liang</a>
|
<a href=/people/y/yao-chong-lim/>Yao Chong Lim</a>
|
<a href=/people/y/yao-hung-hubert-tsai/>Yao-Hung Hubert Tsai</a>
|
<a href=/people/r/ruslan-salakhutdinov/>Ruslan Salakhutdinov</a>
|
<a href=/people/l/louis-philippe-morency/>Louis-Philippe Morency</a><br><a href=/volumes/N19-1/ class=text-muted>Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1267><div class="card-body p-3 small">Human language is a rich multimodal signal consisting of <a href=https://en.wikipedia.org/wiki/Speech>spoken words</a>, <a href=https://en.wikipedia.org/wiki/Facial_expression>facial expressions</a>, <a href=https://en.wikipedia.org/wiki/Gesture>body gestures</a>, and <a href=https://en.wikipedia.org/wiki/Intonation_(linguistics)>vocal intonations</a>. Learning representations for these spoken utterances is a complex research problem due to the presence of multiple heterogeneous sources of information. Recent advances in <a href=https://en.wikipedia.org/wiki/Multimodal_learning>multimodal learning</a> have followed the general trend of building more complex models that utilize various attention, memory and recurrent components. In this paper, we propose two simple but strong baselines to learn embeddings of multimodal utterances. The first baseline assumes a conditional factorization of the utterance into unimodal factors. Each <a href=https://en.wikipedia.org/wiki/Unimodality>unimodal factor</a> is modeled using the simple form of a <a href=https://en.wikipedia.org/wiki/Likelihood_function>likelihood function</a> obtained via a linear transformation of the embedding. We show that the optimal embedding can be derived in closed form by taking a weighted average of the unimodal features. In order to capture richer representations, our second baseline extends the first by factorizing into unimodal, bimodal, and trimodal factors, while retaining simplicity and efficiency during <a href=https://en.wikipedia.org/wiki/Machine_learning>learning</a> and <a href=https://en.wikipedia.org/wiki/Statistical_inference>inference</a>. From a set of experiments across two tasks, we show strong performance on both supervised and semi-supervised multimodal prediction, as well as significant (10 times) speedups over neural models during <a href=https://en.wikipedia.org/wiki/Statistical_inference>inference</a>. Overall, we believe that our strong baseline models offer new benchmarking options for future research in <a href=https://en.wikipedia.org/wiki/Multimodal_learning>multimodal learning</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1656.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1656 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1656 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1656" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-1656/>Multimodal Transformer for Unaligned Multimodal Language Sequences</a></strong><br><a href=/people/y/yao-hung-hubert-tsai/>Yao-Hung Hubert Tsai</a>
|
<a href=/people/s/shaojie-bai/>Shaojie Bai</a>
|
<a href=/people/p/paul-pu-liang/>Paul Pu Liang</a>
|
<a href=/people/j/j-zico-kolter/>J. Zico Kolter</a>
|
<a href=/people/l/louis-philippe-morency/>Louis-Philippe Morency</a>
|
<a href=/people/r/ruslan-salakhutdinov/>Ruslan Salakhutdinov</a><br><a href=/volumes/P19-1/ class=text-muted>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1656><div class="card-body p-3 small">Human language is often multimodal, which comprehends a mixture of <a href=https://en.wikipedia.org/wiki/Natural_language>natural language</a>, facial gestures, and acoustic behaviors. However, two major challenges in modeling such multimodal human language time-series data exist : 1) inherent data non-alignment due to variable sampling rates for the sequences from each modality ; and 2) long-range dependencies between elements across modalities. In this paper, we introduce the Multimodal Transformer (MulT) to generically address the above issues in an end-to-end manner without explicitly aligning the data. At the heart of our model is the directional pairwise crossmodal attention, which attends to interactions between multimodal sequences across distinct time steps and latently adapt streams from one modality to another. Comprehensive experiments on both aligned and non-aligned multimodal time-series show that our model outperforms state-of-the-art methods by a large margin. In addition, empirical analysis suggests that correlated crossmodal signals are able to be captured by the proposed crossmodal attention mechanism in MulT.</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1014.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1014 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1014 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D18-1014.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/305210831 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1014/>Multimodal Language Analysis with Recurrent Multistage Fusion</a></strong><br><a href=/people/p/paul-pu-liang/>Paul Pu Liang</a>
|
<a href=/people/z/ziyin-liu/>Ziyin Liu</a>
|
<a href=/people/a/amirali-bagher-zadeh/>AmirAli Bagher Zadeh</a>
|
<a href=/people/l/louis-philippe-morency/>Louis-Philippe Morency</a><br><a href=/volumes/D18-1/ class=text-muted>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1014><div class="card-body p-3 small">Computational modeling of human multimodal language is an emerging research area in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a> spanning the <a href=https://en.wikipedia.org/wiki/Language>language</a>, visual and acoustic modalities. Comprehending multimodal language requires modeling not only the interactions within each modality (intra-modal interactions) but more importantly the interactions between modalities (cross-modal interactions). In this paper, we propose the Recurrent Multistage Fusion Network (RMFN) which decomposes the fusion problem into multiple stages, each of them focused on a subset of multimodal signals for specialized, effective fusion. Cross-modal interactions are modeled using this multistage fusion approach which builds upon intermediate representations of previous stages. Temporal and intra-modal interactions are modeled by integrating our proposed fusion approach with a system of recurrent neural networks. The RMFN displays state-of-the-art performance in modeling human multimodal language across three public datasets relating to multimodal sentiment analysis, <a href=https://en.wikipedia.org/wiki/Emotion_recognition>emotion recognition</a>, and speaker traits recognition. We provide visualizations to show that each stage of fusion focuses on a different subset of multimodal signals, learning increasingly discriminative multimodal representations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3300.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3300/>Proceedings of Grand Challenge and Workshop on Human Multimodal Language (Challenge-<span class=acl-fixed-case>HML</span>)</a></strong><br><a href=/people/a/amir-zadeh/>Amir Zadeh</a>
|
<a href=/people/p/paul-pu-liang/>Paul Pu Liang</a>
|
<a href=/people/l/louis-philippe-morency/>Louis-Philippe Morency</a>
|
<a href=/people/s/soujanya-poria/>Soujanya Poria</a>
|
<a href=/people/e/erik-cambria/>Erik Cambria</a>
|
<a href=/people/s/stefan-scherer/>Stefan Scherer</a><br><a href=/volumes/W18-33/ class=text-muted>Proceedings of Grand Challenge and Workshop on Human Multimodal Language (Challenge-HML)</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1208.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1208 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1208 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285805491 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-1208/>Multimodal Language Analysis in the Wild : CMU-MOSEI Dataset and Interpretable Dynamic Fusion Graph<span class=acl-fixed-case>CMU</span>-<span class=acl-fixed-case>MOSEI</span> Dataset and Interpretable Dynamic Fusion Graph</a></strong><br><a href=/people/a/amirali-bagher-zadeh/>AmirAli Bagher Zadeh</a>
|
<a href=/people/p/paul-pu-liang/>Paul Pu Liang</a>
|
<a href=/people/s/soujanya-poria/>Soujanya Poria</a>
|
<a href=/people/e/erik-cambria/>Erik Cambria</a>
|
<a href=/people/l/louis-philippe-morency/>Louis-Philippe Morency</a><br><a href=/volumes/P18-1/ class=text-muted>Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1208><div class="card-body p-3 small">Analyzing human multimodal language is an emerging area of research in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a>. Intrinsically this <a href=https://en.wikipedia.org/wiki/Language>language</a> is multimodal (heterogeneous), sequential and asynchronous ; it consists of the language (words), visual (expressions) and acoustic (paralinguistic) modalities all in the form of asynchronous coordinated sequences. From a resource perspective, there is a genuine need for large scale datasets that allow for in-depth studies of this form of language. In this paper we introduce CMU Multimodal Opinion Sentiment and Emotion Intensity (CMU-MOSEI), the largest dataset of <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> and <a href=https://en.wikipedia.org/wiki/Emotion_recognition>emotion recognition</a> to date. Using data from CMU-MOSEI and a novel multimodal fusion technique called the Dynamic Fusion Graph (DFG), we conduct experimentation to exploit how modalities interact with each other in human multimodal language. Unlike previously proposed fusion techniques, DFG is highly interpretable and achieves competative performance when compared to the previous state of the art.</div></div><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1059.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1059 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1059 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234957188 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-1059/>Affect-LM : A Neural Language Model for Customizable Affective Text Generation<span class=acl-fixed-case>LM</span>: A Neural Language Model for Customizable Affective Text Generation</a></strong><br><a href=/people/s/sayan-ghosh/>Sayan Ghosh</a>
|
<a href=/people/m/mathieu-chollet/>Mathieu Chollet</a>
|
<a href=/people/e/eugene-laksana/>Eugene Laksana</a>
|
<a href=/people/l/louis-philippe-morency/>Louis-Philippe Morency</a>
|
<a href=/people/s/stefan-scherer/>Stefan Scherer</a><br><a href=/volumes/P17-1/ class=text-muted>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1059><div class="card-body p-3 small">Human verbal communication includes <a href=https://en.wikipedia.org/wiki/Affect_(psychology)>affective messages</a> which are conveyed through use of <a href=https://en.wikipedia.org/wiki/Emotion>emotionally colored words</a>. There has been a lot of research effort in this direction but the problem of integrating state-of-the-art neural language models with <a href=https://en.wikipedia.org/wiki/Affect_(psychology)>affective information</a> remains an area ripe for exploration. In this paper, we propose an extension to an LSTM (Long Short-Term Memory) language model for generation of conversational text, conditioned on <a href=https://en.wikipedia.org/wiki/Affect_(psychology)>affect categories</a>. Our proposed model, Affect-LM enables us to customize the degree of emotional content in generated sentences through an additional design parameter. Perception studies conducted using <a href=https://en.wikipedia.org/wiki/Amazon_Mechanical_Turk>Amazon Mechanical Turk</a> show that Affect-LM can generate naturally looking emotional sentences without sacrificing <a href=https://en.wikipedia.org/wiki/Grammaticality>grammatical correctness</a>. Affect-LM also learns affect-discriminative word representations, and perplexity experiments show that additional <a href=https://en.wikipedia.org/wiki/Affect_(psychology)>affective information</a> in conversational text can improve language model prediction.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1081.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1081 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1081 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234955981 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P17-1081" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-1081/>Context-Dependent Sentiment Analysis in <a href=https://en.wikipedia.org/wiki/User-generated_content>User-Generated Videos</a></a></strong><br><a href=/people/s/soujanya-poria/>Soujanya Poria</a>
|
<a href=/people/e/erik-cambria/>Erik Cambria</a>
|
<a href=/people/d/devamanyu-hazarika/>Devamanyu Hazarika</a>
|
<a href=/people/n/navonil-majumder/>Navonil Majumder</a>
|
<a href=/people/a/amir-zadeh/>Amir Zadeh</a>
|
<a href=/people/l/louis-philippe-morency/>Louis-Philippe Morency</a><br><a href=/volumes/P17-1/ class=text-muted>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1081><div class="card-body p-3 small">Multimodal sentiment analysis is a developing area of research, which involves the identification of sentiments in <a href=https://en.wikipedia.org/wiki/Video>videos</a>. Current research considers utterances as independent entities, i.e., ignores the interdependencies and relations among the utterances of a video. In this paper, we propose a LSTM-based model that enables utterances to capture contextual information from their surroundings in the same video, thus aiding the classification process. Our method shows 5-10 % performance improvement over the <a href=https://en.wikipedia.org/wiki/State_of_the_art>state of the art</a> and high robustness to <a href=https://en.wikipedia.org/wiki/Generalizability>generalizability</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1142.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1142 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1142 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-1142/>Combating Human Trafficking with Multimodal Deep Models</a></strong><br><a href=/people/e/edmund-tong/>Edmund Tong</a>
|
<a href=/people/a/amir-zadeh/>Amir Zadeh</a>
|
<a href=/people/c/cara-jones/>Cara Jones</a>
|
<a href=/people/l/louis-philippe-morency/>Louis-Philippe Morency</a><br><a href=/volumes/P17-1/ class=text-muted>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1142><div class="card-body p-3 small">Human trafficking is a global epidemic affecting millions of people across the planet. Sex trafficking, the dominant form of <a href=https://en.wikipedia.org/wiki/Human_trafficking>human trafficking</a>, has seen a significant rise mostly due to the abundance of escort websites, where human traffickers can openly advertise among at-will escort advertisements. In this paper, we take a major step in the automatic detection of advertisements suspected to pertain to <a href=https://en.wikipedia.org/wiki/Human_trafficking>human trafficking</a>. We present a novel <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> called Trafficking-10k, with more than 10,000 advertisements annotated for this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. The <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> contains two sources of information per advertisement : <a href=https://en.wikipedia.org/wiki/Text_(literary_theory)>text</a> and <a href=https://en.wikipedia.org/wiki/Image>images</a>. For the accurate detection of trafficking advertisements, we designed and trained a deep multimodal model called the Human Trafficking Deep Network (HTDN).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-5002.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-5002 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-5002 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-5002/>Multimodal Machine Learning : Integrating Language, <a href=https://en.wikipedia.org/wiki/Visual_perception>Vision</a> and Speech</a></strong><br><a href=/people/l/louis-philippe-morency/>Louis-Philippe Morency</a>
|
<a href=/people/t/tadas-baltrusaitis/>Tadas Baltrušaitis</a><br><a href=/volumes/P17-5/ class=text-muted>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics: Tutorial Abstracts</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-5002><div class="card-body p-3 small">Multimodal machine learning is a vibrant multi-disciplinary research field which addresses some of the original goals of <a href=https://en.wikipedia.org/wiki/Artificial_intelligence>artificial intelligence</a> by integrating and modeling multiple communicative modalities, including linguistic, acoustic and visual messages. With the initial research on audio-visual speech recognition and more recently with image and video captioning projects, this research field brings some unique challenges for multimodal researchers given the heterogeneity of the data and the contingency often found between modalities. This tutorial builds upon a recent course taught at Carnegie Mellon University during the Spring 2016 semester (CMU course 11-777) and two tutorials presented at CVPR 2016 and ICMI 2016. The present tutorial will review fundamental concepts of <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning</a> and deep neural networks before describing the five main challenges in multimodal machine learning : (1) multimodal representation learning, (2) translation & mapping, (3) modality alignment, (4) multimodal fusion and (5) co-learning. The tutorial will also present state-of-the-art <a href=https://en.wikipedia.org/wiki/Algorithm>algorithms</a> that were recently proposed to solve multimodal applications such as image captioning, video descriptions and visual question-answer. We will also discuss the current and upcoming challenges.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1115.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1115 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1115 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/238236114 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D17-1115/>Tensor Fusion Network for Multimodal Sentiment Analysis</a></strong><br><a href=/people/a/amir-zadeh/>Amir Zadeh</a>
|
<a href=/people/m/minghai-chen/>Minghai Chen</a>
|
<a href=/people/s/soujanya-poria/>Soujanya Poria</a>
|
<a href=/people/e/erik-cambria/>Erik Cambria</a>
|
<a href=/people/l/louis-philippe-morency/>Louis-Philippe Morency</a><br><a href=/volumes/D17-1/ class=text-muted>Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1115><div class="card-body p-3 small">Multimodal sentiment analysis is an increasingly popular research area, which extends the conventional language-based definition of sentiment analysis to a multimodal setup where other relevant modalities accompany language. In this paper, we pose the problem of <a href=https://en.wikipedia.org/wiki/Multimodal_sentiment_analysis>multimodal sentiment analysis</a> as modeling intra-modality and inter-modality dynamics. We introduce a novel <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>, termed Tensor Fusion Networks, which learns both such dynamics end-to-end. The proposed approach is tailored for the volatile nature of spoken language in <a href=https://en.wikipedia.org/wiki/Online_video_platform>online videos</a> as well as accompanying gestures and <a href=https://en.wikipedia.org/wiki/Human_voice>voice</a>. In the experiments, our model outperforms state-of-the-art approaches for both multimodal and unimodal sentiment analysis.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Louis-Philippe+Morency" title="Search for 'Louis-Philippe Morency' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/a/amir-zadeh/ class=align-middle>Amir Zadeh</a>
<span class="badge badge-secondary align-middle ml-2">7</span></li><li class=list-group-item><a href=/people/p/paul-pu-liang/ class=align-middle>Paul Pu Liang</a>
<span class="badge badge-secondary align-middle ml-2">7</span></li><li class=list-group-item><a href=/people/s/soujanya-poria/ class=align-middle>Soujanya Poria</a>
<span class="badge badge-secondary align-middle ml-2">7</span></li><li class=list-group-item><a href=/people/r/ruslan-salakhutdinov/ class=align-middle>Ruslan Salakhutdinov</a>
<span class="badge badge-secondary align-middle ml-2">5</span></li><li class=list-group-item><a href=/people/e/erik-cambria/ class=align-middle>Erik Cambria</a>
<span class="badge badge-secondary align-middle ml-2">5</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/y/yao-hung-hubert-tsai/ class=align-middle>Yao-Hung Hubert Tsai</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/people/a/amirali-bagher-zadeh/ class=align-middle>AmirAli Bagher Zadeh</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/people/w/wasifur-rahman/ class=align-middle>Wasifur Rahman</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/m/md-kamrul-hasan/ class=align-middle>Md Kamrul Hasan</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/s/stefan-scherer/ class=align-middle>Stefan Scherer</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/s/shaojie-bai/ class=align-middle>Shaojie Bai</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/c/candace-ross/ class=align-middle>Candace Ross</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/kelly-shi/ class=align-middle>Kelly Shi</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/martin-ma/ class=align-middle>Martin Ma</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/muqiao-yang/ class=align-middle>Muqiao Yang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/sangwu-lee/ class=align-middle>Sangwu Lee</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/chengfeng-mao/ class=align-middle>Chengfeng Mao</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/e/ehsan-hoque/ class=align-middle>Ehsan Hoque</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/tian-jin/ class=align-middle>Tian Jin</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/zhun-liu/ class=align-middle>Zhun Liu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/shengjia-yan/ class=align-middle>Shengjia Yan</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/alexandre-eichenberger/ class=align-middle>Alexandre Eichenberger</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/sayan-ghosh/ class=align-middle>Sayan Ghosh</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/mathieu-chollet/ class=align-middle>Mathieu Chollet</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/e/eugene-laksana/ class=align-middle>Eugene Laksana</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/devamanyu-hazarika/ class=align-middle>Devamanyu Hazarika</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/n/navonil-majumder/ class=align-middle>Navonil Majumder</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/e/edmund-tong/ class=align-middle>Edmund Tong</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/cara-jones/ class=align-middle>Cara Jones</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/tadas-baltrusaitis/ class=align-middle>Tadas Baltrušaitis</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/ziyin-liu/ class=align-middle>Ziyin Liu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jianyuan-zhong/ class=align-middle>Jianyuan Zhong</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/md-iftekhar-tanveer/ class=align-middle>Md Iftekhar Tanveer</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/mohammed-ehsan-hoque/ class=align-middle>Mohammed (Ehsan) Hoque</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/makoto-yamada/ class=align-middle>Makoto Yamada</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/minghai-chen/ class=align-middle>Minghai Chen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jianing-yang/ class=align-middle>Jianing Yang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yongxin-wang/ class=align-middle>Yongxin Wang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/ruitao-yi/ class=align-middle>Ruitao Yi</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yuying-zhu/ class=align-middle>Yuying Zhu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/azaan-rehman/ class=align-middle>Azaan Rehman</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yao-chong-lim/ class=align-middle>Yao Chong Lim</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/j-zico-kolter/ class=align-middle>J. Zico Kolter</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/acl/ class=align-middle>ACL</a><span class="badge badge-secondary align-middle ml-2">8</span></li><li class=list-group-item><a href=/venues/emnlp/ class=align-middle>EMNLP</a><span class="badge badge-secondary align-middle ml-2">5</span></li><li class=list-group-item><a href=/venues/naacl/ class=align-middle>NAACL</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/maiworkshop/ class=align-middle>maiworkshop</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/challengehml/ class=align-middle>Challenge-HML</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-venues aria-expanded=false aria-controls=more-venues>show all...</li><div class="collapse border-top" id=more-venues><li class=list-group-item><a href=/venues/ws/ class=align-middle>WS</a><span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>