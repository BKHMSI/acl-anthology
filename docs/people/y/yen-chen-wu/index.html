<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Yen-Chen Wu - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Yen-Chen</span> <span class=font-weight-bold>Wu</span></h2><p class="font-weight-light text-muted"><span class=font-italic>Also published as:</span>
Yen-chen <span class=font-weight-normal>Wu</span></p><hr><div class=row><div class=col-lg-9><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.267.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--267 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.267 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.267/>Clipping Loops for Sample-Efficient Dialogue Policy Optimisation</a></strong><br><a href=/people/y/yen-chen-wu/>Yen-Chen Wu</a>
|
<a href=/people/c/carl-edward-rasmussen/>Carl Edward Rasmussen</a><br><a href=/volumes/2021.naacl-main/ class=text-muted>Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--267><div class="card-body p-3 small">Training dialogue agents requires a large number of interactions with users : agents have no idea about which responses are bad among a lengthy dialogue. In this paper, we propose loop-clipping policy optimisation (LCPO) to eliminate useless responses. LCPO consists of two stages : loop clipping and advantage clipping. In loop clipping, we clip off useless responses (called loops) from dialogue history (called trajectories). The clipped trajectories are more succinct than the original ones, and the estimation of state-value is more accurate. Second, in advantage clipping, we estimate and clip the advantages of useless responses and normal ones separately. The clipped advantage distinguish useless actions from others and reduce the probabilities of useless actions efficiently. In experiments on Cambridge Restaurant Dialogue System, LCPO uses only 260 training dialogues to achieve 80 % success rate, while PPO baseline requires 2160 dialogues. Besides, LCPO receives 3.7/5 scores in <a href=https://en.wikipedia.org/wiki/Human&#8211;computer_interaction>human evaluation</a> where the agent interactively collects 100 real-user dialogues in training phase.</div></div><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.75.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--75 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.75 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.75/>Actor-Double-Critic : Incorporating Model-Based Critic for Task-Oriented Dialogue Systems</a></strong><br><a href=/people/y/yen-chen-wu/>Yen-chen Wu</a>
|
<a href=/people/b/bo-hsiang-tseng/>Bo-Hsiang Tseng</a>
|
<a href=/people/m/milica-gasic/>Milica Gasic</a><br><a href=/volumes/2020.findings-emnlp/ class=text-muted>Findings of the Association for Computational Linguistics: EMNLP 2020</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--75><div class="card-body p-3 small">In order to improve the sample-efficiency of deep reinforcement learning (DRL), we implemented imagination augmented agent (I2A) in spoken dialogue systems (SDS). Although I2A achieves a higher success rate than <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a> by augmenting predicted future into a policy network, its complicated architecture introduces unwanted instability. In this work, we propose actor-double-critic (ADC) to improve the stability and overall performance of I2A. ADC simplifies the architecture of I2A to reduce excessive parameters and hyper-parameters. More importantly, a separate model-based critic shares parameters between actions and makes <a href=https://en.wikipedia.org/wiki/Backpropagation>back-propagation</a> explicit. In our experiments on Cambridge Restaurant Booking task, ADC enhances success rates considerably and shows robustness to imperfect environment models. In addition, ADC exhibits the stability and sample-efficiency as significantly reducing the baseline standard deviation of success rates and reaching the 80 % success rate with half training data.</div></div><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5920.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5920 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5920 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5920/>Tree-Structured Semantic Encoder with Knowledge Sharing for Domain Adaptation in Natural Language Generation</a></strong><br><a href=/people/b/bo-hsiang-tseng/>Bo-Hsiang Tseng</a>
|
<a href=/people/p/pawel-budzianowski/>Pawe≈Ç Budzianowski</a>
|
<a href=/people/y/yen-chen-wu/>Yen-chen Wu</a>
|
<a href=/people/m/milica-gasic/>Milica Gasic</a><br><a href=/volumes/W19-59/ class=text-muted>Proceedings of the 20th Annual SIGdial Meeting on Discourse and Dialogue</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5920><div class="card-body p-3 small">Domain adaptation in natural language generation (NLG) remains challenging because of the high complexity of input semantics across domains and limited data of a target domain. This is particularly the case for dialogue systems, where we want to be able to seamlessly include new domains into the conversation. Therefore, it is crucial for generation models to share knowledge across domains for the effective <a href=https://en.wikipedia.org/wiki/Adaptation>adaptation</a> from one domain to another. In this study, we exploit a tree-structured semantic encoder to capture the internal structure of complex semantic representations required for multi-domain dialogues in order to facilitate knowledge sharing across domains. In addition, a layer-wise attention mechanism between the <a href=https://en.wikipedia.org/wiki/Tree_traversal>tree encoder</a> and the <a href=https://en.wikipedia.org/wiki/Tree_traversal>decoder</a> is adopted to further improve the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>&#8217;s capability. The automatic evaluation results show that our model outperforms previous methods in terms of the BLEU score and the slot error rate, in particular when the <a href=https://en.wikipedia.org/wiki/Adaptation_data>adaptation data</a> is limited. In subjective evaluation, human judges tend to prefer the sentences generated by our model, rating them more highly on informativeness and <a href=https://en.wikipedia.org/wiki/Naturalness_(philosophy)>naturalness</a> than other <a href=https://en.wikipedia.org/wiki/System>systems</a>.</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5032.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5032 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5032 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W18-5032.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/W18-5032/>Addressing Objects and Their Relations : The Conversational Entity Dialogue Model</a></strong><br><a href=/people/s/stefan-ultes/>Stefan Ultes</a>
|
<a href=/people/p/pawel-budzianowski/>Pawe≈Ç Budzianowski</a>
|
<a href=/people/i/inigo-casanueva/>I√±igo Casanueva</a>
|
<a href=/people/l/lina-m-rojas-barahona/>Lina M. Rojas-Barahona</a>
|
<a href=/people/b/bo-hsiang-tseng/>Bo-Hsiang Tseng</a>
|
<a href=/people/y/yen-chen-wu/>Yen-Chen Wu</a>
|
<a href=/people/s/steve-young/>Steve Young</a>
|
<a href=/people/m/milica-gasic/>Milica Ga≈°iƒá</a><br><a href=/volumes/W18-50/ class=text-muted>Proceedings of the 19th Annual SIGdial Meeting on Discourse and Dialogue</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5032><div class="card-body p-3 small">Statistical spoken dialogue systems usually rely on a single- or multi-domain dialogue model that is restricted in its capabilities of modelling complex dialogue structures, e.g., <a href=https://en.wikipedia.org/wiki/Interpersonal_relationship>relations</a>. In this work, we propose a novel dialogue model that is centred around entities and is able to model <a href=https://en.wikipedia.org/wiki/Binary_relation>relations</a> as well as multiple entities of the same type. We demonstrate in a prototype implementation benefits of relation modelling on the dialogue level and show that a trained <a href=https://en.wikipedia.org/wiki/Policy>policy</a> using these <a href=https://en.wikipedia.org/wiki/Binary_relation>relations</a> outperforms the multi-domain baseline. Furthermore, we show that by modelling the <a href=https://en.wikipedia.org/wiki/Binary_relation>relations</a> on the dialogue level, the <a href=https://en.wikipedia.org/wiki/System>system</a> is capable of processing <a href=https://en.wikipedia.org/wiki/Binary_relation>relations</a> present in the user input and even learns to address them in the system response.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5038.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5038 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5038 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5038/>Feudal Dialogue Management with Jointly Learned Feature Extractors</a></strong><br><a href=/people/i/inigo-casanueva/>I√±igo Casanueva</a>
|
<a href=/people/p/pawel-budzianowski/>Pawe≈Ç Budzianowski</a>
|
<a href=/people/s/stefan-ultes/>Stefan Ultes</a>
|
<a href=/people/f/florian-kreyssig/>Florian Kreyssig</a>
|
<a href=/people/b/bo-hsiang-tseng/>Bo-Hsiang Tseng</a>
|
<a href=/people/y/yen-chen-wu/>Yen-chen Wu</a>
|
<a href=/people/m/milica-gasic/>Milica Ga≈°iƒá</a><br><a href=/volumes/W18-50/ class=text-muted>Proceedings of the 19th Annual SIGdial Meeting on Discourse and Dialogue</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5038><div class="card-body p-3 small">Reinforcement learning (RL) is a promising dialogue policy optimisation approach, but traditional RL algorithms fail to scale to large domains. Recently, Feudal Dialogue Management (FDM), has shown to increase the scalability to large domains by decomposing the dialogue management decision into two steps, making use of the domain ontology to abstract the dialogue state in each step. In order to abstract the <a href=https://en.wikipedia.org/wiki/State_space>state space</a>, however, previous work on <a href=https://en.wikipedia.org/wiki/Finite-state_machine>FDM</a> relies on handcrafted feature functions. In this work, we show that these <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>feature functions</a> can be learned jointly with the <a href=https://en.wikipedia.org/wiki/Policy_model>policy model</a> while obtaining similar performance, even outperforming the handcrafted features in several environments and domains.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5039.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5039 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5039 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5039/>Variational Cross-domain Natural Language Generation for Spoken Dialogue Systems</a></strong><br><a href=/people/b/bo-hsiang-tseng/>Bo-Hsiang Tseng</a>
|
<a href=/people/f/florian-kreyssig/>Florian Kreyssig</a>
|
<a href=/people/p/pawel-budzianowski/>Pawe≈Ç Budzianowski</a>
|
<a href=/people/i/inigo-casanueva/>I√±igo Casanueva</a>
|
<a href=/people/y/yen-chen-wu/>Yen-Chen Wu</a>
|
<a href=/people/s/stefan-ultes/>Stefan Ultes</a>
|
<a href=/people/m/milica-gasic/>Milica Ga≈°iƒá</a><br><a href=/volumes/W18-50/ class=text-muted>Proceedings of the 19th Annual SIGdial Meeting on Discourse and Dialogue</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5039><div class="card-body p-3 small">Cross-domain natural language generation (NLG) is still a difficult task within spoken dialogue modelling. Given a <a href=https://en.wikipedia.org/wiki/Semantics>semantic representation</a> provided by the <a href=https://en.wikipedia.org/wiki/Dialogue_manager>dialogue manager</a>, the language generator should generate sentences that convey desired information. Traditional template-based generators can produce sentences with all necessary information, but these sentences are not sufficiently diverse. With RNN-based models, the diversity of the generated sentences can be high, however, in the process some information is lost. In this work, we improve an RNN-based generator by considering latent information at the sentence level during generation using conditional variational auto-encoder architecture. We demonstrate that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms the original RNN-based generator, while yielding highly diverse sentences. In addition, our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> performs better when the training data is limited.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Yen-Chen+Wu" title="Search for 'Yen-Chen Wu' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/b/bo-hsiang-tseng/ class=align-middle>Bo-Hsiang Tseng</a>
<span class="badge badge-secondary align-middle ml-2">5</span></li><li class=list-group-item><a href=/people/m/milica-gasic/ class=align-middle>Milica Gasic</a>
<span class="badge badge-secondary align-middle ml-2">5</span></li><li class=list-group-item><a href=/people/p/pawel-budzianowski/ class=align-middle>Pawe≈Ç Budzianowski</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/people/s/stefan-ultes/ class=align-middle>Stefan Ultes</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/i/inigo-casanueva/ class=align-middle>I√±igo Casanueva</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/f/florian-kreyssig/ class=align-middle>Florian Kreyssig</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/c/carl-edward-rasmussen/ class=align-middle>Carl Edward Rasmussen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/lina-m-rojas-barahona/ class=align-middle>Lina M. Rojas Barahona</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/steve-young/ class=align-middle>Steve Young</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/ws/ class=align-middle>WS</a><span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/venues/naacl/ class=align-middle>NAACL</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/findings/ class=align-middle>Findings</a><span class="badge badge-secondary align-middle ml-2">1</span></li></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ¬©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>