<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Yasuhide Miura - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Yasuhide</span> <span class=font-weight-bold>Miura</span></h2><hr><div class=row><div class=col-lg-9><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.416.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--416 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.416 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.naacl-main.416" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.416/>Improving Factual Completeness and Consistency of Image-to-Text Radiology Report Generation</a></strong><br><a href=/people/y/yasuhide-miura/>Yasuhide Miura</a>
|
<a href=/people/y/yuhao-zhang/>Yuhao Zhang</a>
|
<a href=/people/e/emily-tsai/>Emily Tsai</a>
|
<a href=/people/c/curtis-langlotz/>Curtis Langlotz</a>
|
<a href=/people/d/dan-jurafsky/>Dan Jurafsky</a><br><a href=/volumes/2021.naacl-main/ class=text-muted>Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--416><div class="card-body p-3 small">Neural image-to-text radiology report generation systems offer the potential to improve radiology reporting by reducing the repetitive process of report drafting and identifying possible <a href=https://en.wikipedia.org/wiki/Medical_error>medical errors</a>. However, existing report generation systems, despite achieving high performances on natural language generation metrics such as CIDEr or <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a>, still suffer from incomplete and inconsistent generations. Here we introduce two new simple rewards to encourage the generation of factually complete and consistent radiology reports : one that encourages the system to generate radiology domain entities consistent with the reference, and one that uses natural language inference to encourage these entities to be described in inferentially consistent ways. We combine these with the novel use of an existing semantic equivalence metric (BERTScore). We further propose a report generation system that optimizes these <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>rewards</a> via <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning</a>. On two open radiology report datasets, our <a href=https://en.wikipedia.org/wiki/System>system</a> substantially improved the F1 score of a clinical information extraction performance by +22.1 (Delta +63.9 %). We further show via a <a href=https://en.wikipedia.org/wiki/Evaluation>human evaluation</a> and a <a href=https://en.wikipedia.org/wiki/Qualitative_property>qualitative analysis</a> that our <a href=https://en.wikipedia.org/wiki/System>system</a> leads to generations that are more factually complete and consistent compared to the <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a>.</div></div><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.aacl-main.32.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--aacl-main--32 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.aacl-main.32 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.aacl-main.32/>Identifying Implicit Quotes for Unsupervised Extractive Summarization of Conversations</a></strong><br><a href=/people/r/ryuji-kano/>Ryuji Kano</a>
|
<a href=/people/y/yasuhide-miura/>Yasuhide Miura</a>
|
<a href=/people/t/tomoki-taniguchi/>Tomoki Taniguchi</a>
|
<a href=/people/t/tomoko-ohkuma/>Tomoko Ohkuma</a><br><a href=/volumes/2020.aacl-main/ class=text-muted>Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--aacl-main--32><div class="card-body p-3 small">We propose Implicit Quote Extractor, an end-to-end unsupervised extractive neural summarization model for conversational texts. When we reply to posts, <a href=https://en.wikipedia.org/wiki/Quotation>quotes</a> are used to highlight important part of texts. We aim to extract quoted sentences as summaries. Most replies do not explicitly include <a href=https://en.wikipedia.org/wiki/Quotation>quotes</a>, so it is difficult to use <a href=https://en.wikipedia.org/wiki/Quotation>quotes</a> as <a href=https://en.wikipedia.org/wiki/Supervisor>supervision</a>. However, even if it is not explicitly shown, replies always refer to certain parts of texts ; we call them implicit quotes. Implicit Quote Extractor aims to extract implicit quotes as summaries. The training task of the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is to predict whether a reply candidate is a true reply to a post. For <a href=https://en.wikipedia.org/wiki/Prediction>prediction</a>, the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> has to choose a few sentences from the post. To predict accurately, the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> learns to extract sentences that replies frequently refer to. We evaluate our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> on two email datasets and one social media dataset, and confirm that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is useful for extractive summarization. We further discuss two topics ; one is whether quote extraction is an important factor for summarization, and the other is whether our model can capture salient sentences that conventional methods can not.</div></div><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5302.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5302 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5302 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5302/>Relation Prediction for Unseen-Entities Using Entity-Word Graphs</a></strong><br><a href=/people/y/yuki-tagawa/>Yuki Tagawa</a>
|
<a href=/people/m/motoki-taniguchi/>Motoki Taniguchi</a>
|
<a href=/people/y/yasuhide-miura/>Yasuhide Miura</a>
|
<a href=/people/t/tomoki-taniguchi/>Tomoki Taniguchi</a>
|
<a href=/people/t/tomoko-ohkuma/>Tomoko Ohkuma</a>
|
<a href=/people/t/takayuki-yamamoto/>Takayuki Yamamoto</a>
|
<a href=/people/k/keiichi-nemoto/>Keiichi Nemoto</a><br><a href=/volumes/D19-53/ class=text-muted>Proceedings of the Thirteenth Workshop on Graph-Based Methods for Natural Language Processing (TextGraphs-13)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5302><div class="card-body p-3 small">Knowledge graphs (KGs) are generally used for various <a href=https://en.wikipedia.org/wiki/Neuro-linguistic_programming>NLP tasks</a>. However, as <a href=https://en.wikipedia.org/wiki/Knowledge_graph>KGs</a> still miss some information, it is necessary to develop Knowledge Graph Completion (KGC) methods. Most KGC researches do not focus on the Out-of-KGs entities (Unseen-entities), we need a method that can predict the relation for the entity pairs containing Unseen-entities to automatically add new entities to the KGs. In this study, we focus on relation prediction and propose a method to learn entity representations via a graph structure that uses Seen-entities, Unseen-entities and words as nodes created from the descriptions of all entities. In the experiments, our method shows a significant improvement in the relation prediction for the entity pairs containing Unseen-entities.</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5506.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5506 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5506 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5506/>Joint Modeling for <a href=https://en.wikipedia.org/wiki/Query_expansion>Query Expansion</a> and <a href=https://en.wikipedia.org/wiki/Information_extraction>Information Extraction</a> with <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>Reinforcement Learning</a></a></strong><br><a href=/people/m/motoki-taniguchi/>Motoki Taniguchi</a>
|
<a href=/people/y/yasuhide-miura/>Yasuhide Miura</a>
|
<a href=/people/t/tomoko-ohkuma/>Tomoko Ohkuma</a><br><a href=/volumes/W18-55/ class=text-muted>Proceedings of the First Workshop on Fact Extraction and VERification (FEVER)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5506><div class="card-body p-3 small">Information extraction about an event can be improved by incorporating <a href=https://en.wikipedia.org/wiki/Empirical_evidence>external evidence</a>. In this study, we propose a joint model for pseudo-relevance feedback based query expansion and <a href=https://en.wikipedia.org/wiki/Information_extraction>information extraction</a> with <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning</a>. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> generates an event-specific query to effectively retrieve documents relevant to the event. We demonstrate that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is comparable or has better performance than the previous <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> in two publicly available datasets. Furthermore, we analyzed the influences of the retrieval effectiveness in our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> on the <a href=https://en.wikipedia.org/wiki/Information_retrieval>extraction</a> performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5520.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5520 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5520 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5520/>Integrating Entity Linking and Evidence Ranking for Fact Extraction and Verification</a></strong><br><a href=/people/m/motoki-taniguchi/>Motoki Taniguchi</a>
|
<a href=/people/t/tomoki-taniguchi/>Tomoki Taniguchi</a>
|
<a href=/people/t/takumi-takahashi/>Takumi Takahashi</a>
|
<a href=/people/y/yasuhide-miura/>Yasuhide Miura</a>
|
<a href=/people/t/tomoko-ohkuma/>Tomoko Ohkuma</a><br><a href=/volumes/W18-55/ class=text-muted>Proceedings of the First Workshop on Fact Extraction and VERification (FEVER)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5520><div class="card-body p-3 small">We describe here our <a href=https://en.wikipedia.org/wiki/System>system</a> and results on the FEVER shared task. We prepared a <a href=https://en.wikipedia.org/wiki/Pipeline_(software)>pipeline system</a> which composes of a document selection, a sentence retrieval, and a recognizing textual entailment (RTE) components. A simple entity linking approach with text match is used as the document selection component, this component identifies relevant documents for a given claim by using mentioned entities as clues. The sentence retrieval component selects relevant sentences as candidate evidence from the documents based on <a href=https://en.wikipedia.org/wiki/TF-IDF>TF-IDF</a>. Finally, the RTE component selects evidence sentences by ranking the sentences and classifies the claim simultaneously. The experimental results show that our <a href=https://en.wikipedia.org/wiki/System>system</a> achieved the <a href=https://en.wikipedia.org/wiki/FEVER>FEVER score</a> of 0.4016 and outperformed the official baseline system.</div></div><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-2045.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-2045 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-2045 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-2045/>Using <a href=https://en.wikipedia.org/wiki/Social_network>Social Networks</a> to Improve Language Variety Identification with Neural Networks</a></strong><br><a href=/people/y/yasuhide-miura/>Yasuhide Miura</a>
|
<a href=/people/t/tomoki-taniguchi/>Tomoki Taniguchi</a>
|
<a href=/people/m/motoki-taniguchi/>Motoki Taniguchi</a>
|
<a href=/people/s/shotaro-misawa/>Shotaro Misawa</a>
|
<a href=/people/t/tomoko-ohkuma/>Tomoko Ohkuma</a><br><a href=/volumes/I17-2/ class=text-muted>Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-2045><div class="card-body p-3 small">We propose a hierarchical neural network model for language variety identification that integrates information from a <a href=https://en.wikipedia.org/wiki/Social_network>social network</a>. Recently, language variety identification has enjoyed heightened popularity as an advanced task of <a href=https://en.wikipedia.org/wiki/Language_identification>language identification</a>. The proposed <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> uses additional texts from a <a href=https://en.wikipedia.org/wiki/Social_network>social network</a> to improve language variety identification from two perspectives. First, they are used to introduce the effects of <a href=https://en.wikipedia.org/wiki/Homophily>homophily</a>. Secondly, they are used as expanded training data for shared layers of the proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>. By introducing information from <a href=https://en.wikipedia.org/wiki/List_of_social_networking_websites>social networks</a>, the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> improved its <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> by 1.67-5.56. Compared to state-of-the-art baselines, these improved performances are better in <a href=https://en.wikipedia.org/wiki/English_language>English</a> and comparable in <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a>. Furthermore, we analyzed the cases of <a href=https://en.wikipedia.org/wiki/Portuguese_language>Portuguese</a> and <a href=https://en.wikipedia.org/wiki/Arabic>Arabic</a> when the model showed weak performances, and found that the effect of <a href=https://en.wikipedia.org/wiki/Homophily>homophily</a> is likely to be weak due to sparsity and noises compared to languages with the strong performances.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1116.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1116 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1116 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P17-1116.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234945928 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-1116/>Unifying Text, <a href=https://en.wikipedia.org/wiki/Metadata>Metadata</a>, and User Network Representations with a <a href=https://en.wikipedia.org/wiki/Neural_network>Neural Network</a> for Geolocation Prediction</a></strong><br><a href=/people/y/yasuhide-miura/>Yasuhide Miura</a>
|
<a href=/people/m/motoki-taniguchi/>Motoki Taniguchi</a>
|
<a href=/people/t/tomoki-taniguchi/>Tomoki Taniguchi</a>
|
<a href=/people/t/tomoko-ohkuma/>Tomoko Ohkuma</a><br><a href=/volumes/P17-1/ class=text-muted>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1116><div class="card-body p-3 small">We propose a novel geolocation prediction model using a complex neural network. Geolocation prediction in <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> has attracted many researchers to use information of various types. Our model unifies <a href=https://en.wikipedia.org/wiki/Text_(literary_theory)>text</a>, <a href=https://en.wikipedia.org/wiki/Metadata>metadata</a>, and user network representations with an <a href=https://en.wikipedia.org/wiki/Attentional_control>attention mechanism</a> to overcome previous ensemble approaches. In an evaluation using two open datasets, the proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> exhibited a maximum 3.8 % increase in <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> and a maximum of 6.6 % increase in accuracy@161 against previous <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>. We further analyzed several intermediate layers of our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>, which revealed that their states capture some statistical characteristics of the datasets.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Yasuhide+Miura" title="Search for 'Yasuhide Miura' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/t/tomoko-ohkuma/ class=align-middle>Tomoko Ohkuma</a>
<span class="badge badge-secondary align-middle ml-2">6</span></li><li class=list-group-item><a href=/people/t/tomoki-taniguchi/ class=align-middle>Tomoki Taniguchi</a>
<span class="badge badge-secondary align-middle ml-2">5</span></li><li class=list-group-item><a href=/people/m/motoki-taniguchi/ class=align-middle>Motoki Taniguchi</a>
<span class="badge badge-secondary align-middle ml-2">5</span></li><li class=list-group-item><a href=/people/s/shotaro-misawa/ class=align-middle>Shotaro Misawa</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/ryuji-kano/ class=align-middle>Ryuji Kano</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/y/yuki-tagawa/ class=align-middle>Yuki Tagawa</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/takayuki-yamamoto/ class=align-middle>Takayuki Yamamoto</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/keiichi-nemoto/ class=align-middle>Keiichi Nemoto</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yuhao-zhang/ class=align-middle>Yuhao Zhang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/e/emily-tsai/ class=align-middle>Emily Tsai</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/curtis-langlotz/ class=align-middle>Curtis Langlotz</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/dan-jurafsky/ class=align-middle>Dan Jurafsky</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/takumi-takahashi/ class=align-middle>Takumi Takahashi</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/ws/ class=align-middle>WS</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/ijcnlp/ class=align-middle>IJCNLP</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/acl/ class=align-middle>ACL</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/aacl/ class=align-middle>AACL</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/emnlp/ class=align-middle>EMNLP</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-venues aria-expanded=false aria-controls=more-venues>show all...</li><div class="collapse border-top" id=more-venues><li class=list-group-item><a href=/venues/naacl/ class=align-middle>NAACL</a><span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright Â©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>