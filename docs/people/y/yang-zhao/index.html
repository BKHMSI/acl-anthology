<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Yang Zhao - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Yang</span> <span class=font-weight-bold>Zhao</span></h2><hr><div class=row><div class=col-lg-9><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.34.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--34 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.34 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938716 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.34" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.34/>Q-learning with <a href=https://en.wikipedia.org/wiki/Language_model>Language Model</a> for Edit-based Unsupervised Summarization<span class=acl-fixed-case>Q</span>-learning with Language Model for Edit-based Unsupervised Summarization</a></strong><br><a href=/people/r/ryosuke-kohita/>Ryosuke Kohita</a>
|
<a href=/people/a/akifumi-wachi/>Akifumi Wachi</a>
|
<a href=/people/y/yang-zhao/>Yang Zhao</a>
|
<a href=/people/r/ryuki-tachibana/>Ryuki Tachibana</a><br><a href=/volumes/2020.emnlp-main/ class=text-muted>Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--34><div class="card-body p-3 small">Unsupervised methods are promising for abstractive textsummarization in that the <a href=https://en.wikipedia.org/wiki/Parallel_text>parallel corpora</a> is not required. However, their performance is still far from being satisfied, therefore research on promising solutions is on-going. In this paper, we propose a new approach based on <a href=https://en.wikipedia.org/wiki/Q-learning>Q-learning</a> with an edit-based summarization. The method combines two key <a href=https://en.wikipedia.org/wiki/Modular_programming>modules</a> to form an Editorial Agent and Language Model converter (EALM). The <a href=https://en.wikipedia.org/wiki/Agency_(philosophy)>agent</a> predicts edit actions (e.t., delete, keep, and replace), and then the LM converter deterministically generates a summary on the basis of the action signals. Q-learning is leveraged to train the <a href=https://en.wikipedia.org/wiki/Intelligent_agent>agent</a> to produce proper edit actions. Experimental results show that EALM delivered competitive performance compared with the previous encoder-decoder-based methods, even with truly zero paired data (i.e., no validation set). Defining the task as <a href=https://en.wikipedia.org/wiki/Q-learning>Q-learning</a> enables us not only to develop a competitive method but also to make the latest techniques in <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning</a> available for unsupervised summarization. We also conduct <a href=https://en.wikipedia.org/wiki/Qualitative_research>qualitative analysis</a>, providing insights into future study on unsupervised summarizers.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.368.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--368 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.368 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.coling-main.368" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.368/>Deconstruct to Reconstruct a Configurable Evaluation Metric for Open-Domain Dialogue Systems</a></strong><br><a href=/people/v/vitou-phy/>Vitou Phy</a>
|
<a href=/people/y/yang-zhao/>Yang Zhao</a>
|
<a href=/people/a/akiko-aizawa/>Akiko Aizawa</a><br><a href=/volumes/2020.coling-main/ class=text-muted>Proceedings of the 28th International Conference on Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--368><div class="card-body p-3 small">Many automatic evaluation metrics have been proposed to score the overall quality of a response in open-domain dialogue. Generally, the overall quality is comprised of various aspects, such as relevancy, <a href=https://en.wikipedia.org/wiki/Sensitivity_and_specificity>specificity</a>, and <a href=https://en.wikipedia.org/wiki/Empathy>empathy</a>, and the importance of each aspect differs according to the task. For instance, <a href=https://en.wikipedia.org/wiki/Sensitivity_and_specificity>specificity</a> is mandatory in a food-ordering dialogue task, whereas <a href=https://en.wikipedia.org/wiki/Fluency>fluency</a> is preferred in a language-teaching dialogue system. However, existing <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> are not designed to cope with such flexibility. For example, BLEU score fundamentally relies only on word overlapping, whereas BERTScore relies on <a href=https://en.wikipedia.org/wiki/Semantic_similarity>semantic similarity</a> between reference and candidate response. Thus, <a href=https://en.wikipedia.org/wiki/Copula_(linguistics)>they</a> are not guaranteed to capture the required <a href=https://en.wikipedia.org/wiki/Complex_system>aspects</a>, i.e., <a href=https://en.wikipedia.org/wiki/Sensitivity_and_specificity>specificity</a>. To design a metric that is flexible to a task, we first propose making these qualities manageable by grouping them into three groups : understandability, sensibleness, and likability, where likability is a combination of qualities that are essential for a task. We also propose a simple method to composite metrics of each aspect to obtain a single <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metric</a> called USL-H, which stands for Understandability, Sensibleness, and Likability in Hierarchy. We demonstrated that USL-H score achieves good correlations with human judgment and maintains its configurability towards different aspects and metrics.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.iwslt-1.15.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--iwslt-1--15 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.iwslt-1.15 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://slideslive.com/38929589 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.iwslt-1.15/>CASIA’s System for IWSLT 2020 Open Domain Translation<span class=acl-fixed-case>CASIA</span>’s System for <span class=acl-fixed-case>IWSLT</span> 2020 Open Domain Translation</a></strong><br><a href=/people/q/qian-wang/>Qian Wang</a>
|
<a href=/people/y/yuchen-liu/>Yuchen Liu</a>
|
<a href=/people/c/cong-ma/>Cong Ma</a>
|
<a href=/people/y/yu-lu/>Yu Lu</a>
|
<a href=/people/y/yining-wang/>Yining Wang</a>
|
<a href=/people/l/long-zhou/>Long Zhou</a>
|
<a href=/people/y/yang-zhao/>Yang Zhao</a>
|
<a href=/people/j/jiajun-zhang/>Jiajun Zhang</a>
|
<a href=/people/c/chengqing-zong/>Chengqing Zong</a><br><a href=/volumes/2020.iwslt-1/ class=text-muted>Proceedings of the 17th International Conference on Spoken Language Translation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--iwslt-1--15><div class="card-body p-3 small">This paper describes the CASIA&#8217;s system for the IWSLT 2020 open domain translation task. This year we participate in both ChineseJapanese and JapaneseChinese translation tasks. Our <a href=https://en.wikipedia.org/wiki/System>system</a> is neural machine translation system based on Transformer model. We augment the training data with knowledge distillation and back translation to improve the <a href=https://en.wikipedia.org/wiki/Translation>translation</a> performance. Domain data classification and weighted domain model ensemble are introduced to generate the final translation result. We compare and analyze the performance on <a href=https://en.wikipedia.org/wiki/Software_development_process>development data</a> with different model settings and different <a href=https://en.wikipedia.org/wiki/Data_processing>data processing techniques</a>.</div></div><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1390.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1390 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1390 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1390.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D19-1390/>Improving Latent Alignment in Text Summarization by Generalizing the Pointer Generator</a></strong><br><a href=/people/x/xiaoyu-shen/>Xiaoyu Shen</a>
|
<a href=/people/y/yang-zhao/>Yang Zhao</a>
|
<a href=/people/h/hui-su/>Hui Su</a>
|
<a href=/people/d/dietrich-klakow/>Dietrich Klakow</a><br><a href=/volumes/D19-1/ class=text-muted>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1390><div class="card-body p-3 small">Pointer Generators have been the de facto standard for modern summarization systems. However, this architecture faces two major drawbacks : Firstly, the <a href=https://en.wikipedia.org/wiki/Pointer_(computer_programming)>pointer</a> is limited to copying the exact words while ignoring possible <a href=https://en.wikipedia.org/wiki/Inflection_point>inflections</a> or <a href=https://en.wikipedia.org/wiki/Abstraction_(computer_science)>abstractions</a>, which restricts its power of capturing richer latent alignment. Secondly, the copy mechanism results in a strong bias towards extractive generations, where most sentences are produced by simply copying from the source text. In this paper, we address these problems by allowing the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> to edit pointed tokens instead of always hard copying them. The editing is performed by transforming the pointed word vector into a target space with a learned <a href=https://en.wikipedia.org/wiki/Glossary_of_graph_theory_terms>relation embedding</a>. On three large-scale summarization dataset, we show the model is able to (1) capture more latent alignment relations than exact word matches, (2) improve word alignment accuracy, allowing for better model interpretation and controlling, (3) generate higher-quality summaries validated by both qualitative and quantitative evaluations and (4) bring more abstraction to the generated summaries.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1216.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1216 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1216 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P19-1216.Supplementary.pdf data-toggle=tooltip data-placement=top title=Supplementary><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1216/>Unsupervised Rewriter for Multi-Sentence Compression</a></strong><br><a href=/people/y/yang-zhao/>Yang Zhao</a>
|
<a href=/people/x/xiaoyu-shen/>Xiaoyu Shen</a>
|
<a href=/people/w/wei-bi/>Wei Bi</a>
|
<a href=/people/a/akiko-aizawa/>Akiko Aizawa</a><br><a href=/volumes/P19-1/ class=text-muted>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1216><div class="card-body p-3 small">Multi-sentence compression (MSC) aims to generate a grammatical but reduced compression from multiple input sentences while retaining their key information. Previous dominating approach for <a href=https://en.wikipedia.org/wiki/Microsoft_SQL_Server>MSC</a> is the extraction-based word graph approach. A few variants further leveraged <a href=https://en.wikipedia.org/wiki/Lexical_substitution>lexical substitution</a> to yield more abstractive compression. However, two limitations exist. First, the word graph approach that simply concatenates fragments from multiple sentences may yield non-fluent or ungrammatical compression. Second, <a href=https://en.wikipedia.org/wiki/Lexical_substitution>lexical substitution</a> is often inappropriate without the consideration of <a href=https://en.wikipedia.org/wiki/Context_(language_use)>context information</a>. To tackle the above-mentioned issues, we present a neural rewriter for multi-sentence compression that does not need any <a href=https://en.wikipedia.org/wiki/Parallel_text>parallel corpus</a>. Empirical studies have shown that our approach achieves comparable results upon automatic evaluation and improves the <a href=https://en.wikipedia.org/wiki/Grammaticality>grammaticality</a> of compression based on human evaluation. A <a href=https://en.wikipedia.org/wiki/Parallel_corpus>parallel corpus</a> with more than 140,000 (sentence group, compression) pairs is also constructed as a by-product for future research.</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1036.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1036 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1036 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-1036/>Addressing Troublesome Words in <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a></a></strong><br><a href=/people/y/yang-zhao/>Yang Zhao</a>
|
<a href=/people/j/jiajun-zhang/>Jiajun Zhang</a>
|
<a href=/people/z/zhongjun-he/>Zhongjun He</a>
|
<a href=/people/c/chengqing-zong/>Chengqing Zong</a>
|
<a href=/people/h/hua-wu/>Hua Wu</a><br><a href=/volumes/D18-1/ class=text-muted>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1036><div class="card-body p-3 small">One of the weaknesses of Neural Machine Translation (NMT) is in handling lowfrequency and ambiguous words, which we refer as troublesome words. To address this problem, we propose a novel memoryenhanced NMT method. First, we investigate different <a href=https://en.wikipedia.org/wiki/Strategy>strategies</a> to define and detect the troublesome words. Then, a contextual memory is constructed to memorize which target words should be produced in what situations. Finally, we design a hybrid model to dynamically access the contextual memory so as to correctly translate the troublesome words. The extensive experiments on Chinese-to-English and English-to-German translation tasks demonstrate that our method significantly outperforms the strong baseline models in translation quality, especially in handling troublesome words.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2028.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2028 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2028 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2028.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2028.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-2028/>A Language Model based Evaluator for Sentence Compression</a></strong><br><a href=/people/y/yang-zhao/>Yang Zhao</a>
|
<a href=/people/z/zhiyuan-luo/>Zhiyuan Luo</a>
|
<a href=/people/a/akiko-aizawa/>Akiko Aizawa</a><br><a href=/volumes/P18-2/ class=text-muted>Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2028><div class="card-body p-3 small">We herein present a language-model-based evaluator for deletion-based sentence compression and view this task as a series of deletion-and-evaluation operations using the evaluator. More specifically, the evaluator is a syntactic neural language model that is first built by learning the syntactic and structural collocation among words. Subsequently, a series of trial-and-error deletion operations are conducted on the source sentences via a reinforcement learning framework to obtain the best target compression. An empirical study shows that the proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> can effectively generate more readable compression, comparable or superior to several strong <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a>. Furthermore, we introduce a 200-sentence test set for a large-scale dataset, setting a new baseline for the future research.</div></div><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1039.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1039 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1039 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-1039/>Towards <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a> with Partially Aligned Corpora</a></strong><br><a href=/people/y/yining-wang/>Yining Wang</a>
|
<a href=/people/y/yang-zhao/>Yang Zhao</a>
|
<a href=/people/j/jiajun-zhang/>Jiajun Zhang</a>
|
<a href=/people/c/chengqing-zong/>Chengqing Zong</a>
|
<a href=/people/z/zhengshan-xue/>Zhengshan Xue</a><br><a href=/volumes/I17-1/ class=text-muted>Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1039><div class="card-body p-3 small">While neural machine translation (NMT) has become the new paradigm, the parameter optimization requires large-scale parallel data which is scarce in many domains and language pairs. In this paper, we address a new translation scenario in which there only exists monolingual corpora and phrase pairs. We propose a new method towards <a href=https://en.wikipedia.org/wiki/Translation>translation</a> with partially aligned sentence pairs which are derived from the phrase pairs and <a href=https://en.wikipedia.org/wiki/Text_corpus>monolingual corpora</a>. To make full use of the partially aligned corpora, we adapt the conventional NMT training method in two aspects. On one hand, different generation strategies are designed for aligned and unaligned target words. On the other hand, a different <a href=https://en.wikipedia.org/wiki/Loss_function>objective function</a> is designed to model the partially aligned parts. The experiments demonstrate that our method can achieve a relatively good result in such a translation scenario, and tiny bitexts can boost translation quality to a large extent.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2080.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2080 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2080 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-2080/>A Conditional Variational Framework for Dialog Generation</a></strong><br><a href=/people/x/xiaoyu-shen/>Xiaoyu Shen</a>
|
<a href=/people/h/hui-su/>Hui Su</a>
|
<a href=/people/y/yanran-li/>Yanran Li</a>
|
<a href=/people/w/wenjie-li/>Wenjie Li</a>
|
<a href=/people/s/shuzi-niu/>Shuzi Niu</a>
|
<a href=/people/y/yang-zhao/>Yang Zhao</a>
|
<a href=/people/a/akiko-aizawa/>Akiko Aizawa</a>
|
<a href=/people/g/guoping-long/>Guoping Long</a><br><a href=/volumes/P17-2/ class=text-muted>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2080><div class="card-body p-3 small">Deep latent variable models have been shown to facilitate the response generation for open-domain dialog systems. However, these latent variables are highly randomized, leading to uncontrollable generated responses. In this paper, we propose a <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> allowing conditional response generation based on specific attributes. These <a href=https://en.wikipedia.org/wiki/Attribute_(computing)>attributes</a> can be either manually assigned or automatically detected. Moreover, the dialog states for both speakers are modeled separately in order to reflect personal features. We validate this <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> on two different scenarios, where the attribute refers to <a href=https://en.wikipedia.org/wiki/Generic_property>genericness</a> and <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment states</a> respectively. The experiment result testified the potential of our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>, where meaningful responses can be generated in accordance with the specified attributes.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Yang+Zhao" title="Search for 'Yang Zhao' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/a/akiko-aizawa/ class=align-middle>Akiko Aizawa</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/people/j/jiajun-zhang/ class=align-middle>Jiajun Zhang</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/c/chengqing-zong/ class=align-middle>Chengqing Zong</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/x/xiaoyu-shen/ class=align-middle>Xiaoyu Shen</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/y/yining-wang/ class=align-middle>Yining Wang</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/h/hui-su/ class=align-middle>Hui Su</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/z/zhengshan-xue/ class=align-middle>Zhengshan Xue</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/ryosuke-kohita/ class=align-middle>Ryosuke Kohita</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/akifumi-wachi/ class=align-middle>Akifumi Wachi</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/ryuki-tachibana/ class=align-middle>Ryuki Tachibana</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yanran-li/ class=align-middle>Yanran Li</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/w/wenjie-li/ class=align-middle>Wenjie Li</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/shuzi-niu/ class=align-middle>Shuzi Niu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/g/guoping-long/ class=align-middle>Guoping Long</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/zhongjun-he/ class=align-middle>Zhongjun He</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/hua-wu/ class=align-middle>Hua Wu (吴华)</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/dietrich-klakow/ class=align-middle>Dietrich Klakow</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/v/vitou-phy/ class=align-middle>Vitou Phy</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/zhiyuan-luo/ class=align-middle>Zhiyuan Luo</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/w/wei-bi/ class=align-middle>Wei Bi</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/q/qian-wang/ class=align-middle>Qian Wang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yuchen-liu/ class=align-middle>Yuchen Liu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/cong-ma/ class=align-middle>Cong Ma</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yu-lu/ class=align-middle>Yu Lu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/long-zhou/ class=align-middle>Long Zhou</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/emnlp/ class=align-middle>EMNLP</a><span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/venues/acl/ class=align-middle>ACL</a><span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/venues/ijcnlp/ class=align-middle>IJCNLP</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/coling/ class=align-middle>COLING</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/iwslt/ class=align-middle>IWSLT</a><span class="badge badge-secondary align-middle ml-2">1</span></li></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>