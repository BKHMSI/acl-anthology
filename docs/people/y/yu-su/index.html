<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Yu Su - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Yu</span> <span class=font-weight-bold>Su</span></h2><hr><div class=row><div class=col-lg-9><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.nlp4prog-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.nlp4prog-1.0/>Proceedings of the 1st Workshop on Natural Language Processing for Programming (NLP4Prog 2021)</a></strong><br><a href=/people/r/royi-lachmy/>Royi Lachmy</a>
|
<a href=/people/z/ziyu-yao/>Ziyu Yao</a>
|
<a href=/people/g/greg-durrett/>Greg Durrett</a>
|
<a href=/people/m/milos-gligoric/>Milos Gligoric</a>
|
<a href=/people/j/junyi-jessy-li/>Junyi Jessy Li</a>
|
<a href=/people/r/ray-mooney/>Ray Mooney</a>
|
<a href=/people/g/graham-neubig/>Graham Neubig</a>
|
<a href=/people/y/yu-su/>Yu Su</a>
|
<a href=/people/h/huan-sun/>Huan Sun</a>
|
<a href=/people/r/reut-tsarfaty/>Reut Tsarfaty</a><br><a href=/volumes/2021.nlp4prog-1/ class=text-muted>Proceedings of the 1st Workshop on Natural Language Processing for Programming (NLP4Prog 2021)</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.225.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--225 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.225 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.225/>Compositional Generalization for Neural Semantic Parsing via Span-level Supervised Attention</a></strong><br><a href=/people/p/pengcheng-yin/>Pengcheng Yin</a>
|
<a href=/people/h/hao-fang/>Hao Fang</a>
|
<a href=/people/g/graham-neubig/>Graham Neubig</a>
|
<a href=/people/a/adam-pauls/>Adam Pauls</a>
|
<a href=/people/e/emmanouil-antonios-platanios/>Emmanouil Antonios Platanios</a>
|
<a href=/people/y/yu-su/>Yu Su</a>
|
<a href=/people/s/sam-thomson/>Sam Thomson</a>
|
<a href=/people/j/jacob-andreas/>Jacob Andreas</a><br><a href=/volumes/2021.naacl-main/ class=text-muted>Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--225><div class="card-body p-3 small">We describe a span-level supervised attention loss that improves compositional generalization in <a href=https://en.wikipedia.org/wiki/Semantic_parser>semantic parsers</a>. Our approach builds on existing losses that encourage attention maps in neural sequence-to-sequence models to imitate the output of classical word alignment algorithms. Where past work has used word-level alignments, we focus on spans ; borrowing ideas from phrase-based machine translation, we align subtrees in semantic parses to spans of input sentences, and encourage neural attention mechanisms to mimic these alignments. This method improves the performance of transformers, <a href=https://en.wikipedia.org/wiki/Radio-frequency_identification>RNNs</a>, and structured decoders on three benchmarks of compositional generalization.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-srw.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-srw.0/>Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Student Research Workshop</a></strong><br><a href=/people/e/esin-durmus/>Esin Durmus</a>
|
<a href=/people/v/vivek-gupta/>Vivek Gupta</a>
|
<a href=/people/n/nelson-f-liu/>Nelson Liu</a>
|
<a href=/people/n/nanyun-peng/>Nanyun Peng</a>
|
<a href=/people/y/yu-su/>Yu Su</a><br><a href=/volumes/2021.naacl-srw/ class=text-muted>Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Student Research Workshop</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.blackboxnlp-1.34.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--blackboxnlp-1--34 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.blackboxnlp-1.34 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.blackboxnlp-1.34" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.blackboxnlp-1.34/>An Investigation of Language Model Interpretability via Sentence Editing</a></strong><br><a href=/people/s/samuel-stevens/>Samuel Stevens</a>
|
<a href=/people/y/yu-su/>Yu Su</a><br><a href=/volumes/2021.blackboxnlp-1/ class=text-muted>Proceedings of the Fourth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--blackboxnlp-1--34><div class="card-body p-3 small">Pre-trained language models (PLMs) like BERT are being used for almost all language-related tasks, but interpreting their behavior still remains a significant challenge and many important questions remain largely unanswered. In this work, we re-purpose a sentence editing dataset, where faithful high-quality human rationales can be automatically extracted and compared with extracted model rationales, as a new testbed for interpretability. This enables us to conduct a systematic investigation on an array of questions regarding PLMs&#8217; interpretability, including the role of pre-training procedure, comparison of rationale extraction methods, and different layers in the PLM. The investigation generates new insights, for example, contrary to the common understanding, we find that attention weights correlate well with <a href=https://en.wikipedia.org/wiki/Rationality>human rationales</a> and work better than gradient-based saliency in extracting model rationales. Both the <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> and code will be released to facilitate future interpretability research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.semeval-1.59.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--semeval-1--59 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.semeval-1.59 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.semeval-1.59/>ITNLP at SemEval-2021 Task 11 : Boosting BERT with Sampling and Adversarial Training for Knowledge Extraction<span class=acl-fixed-case>ITNLP</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2021 Task 11: Boosting <span class=acl-fixed-case>BERT</span> with Sampling and Adversarial Training for Knowledge Extraction</a></strong><br><a href=/people/g/genyu-zhang/>Genyu Zhang</a>
|
<a href=/people/y/yu-su/>Yu Su</a>
|
<a href=/people/c/changhong-he/>Changhong He</a>
|
<a href=/people/l/lei-lin/>Lei Lin</a>
|
<a href=/people/c/cheng-jie-sun/>Chengjie Sun</a>
|
<a href=/people/l/lili-shan/>Lili Shan</a><br><a href=/volumes/2021.semeval-1/ class=text-muted>Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--semeval-1--59><div class="card-body p-3 small">This paper describes the winning system in the End-to-end Pipeline phase for the NLPContributionGraph task. The system is composed of three BERT-based models and the three <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> are used to extract sentences, entities and triples respectively. Experiments show that sampling and adversarial training can greatly boost the <a href=https://en.wikipedia.org/wiki/System>system</a>. In End-to-end Pipeline phase, our <a href=https://en.wikipedia.org/wiki/System>system</a> got an average F1 of 0.4703, significantly higher than the second-placed system which got an average F1 of 0.3828.</div></div><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.697.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--697 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.697 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938913 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.697" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.697/>KGPT : Knowledge-Grounded Pre-Training for Data-to-Text Generation<span class=acl-fixed-case>KGPT</span>: Knowledge-Grounded Pre-Training for Data-to-Text Generation</a></strong><br><a href=/people/w/wenhu-chen/>Wenhu Chen</a>
|
<a href=/people/y/yu-su/>Yu Su</a>
|
<a href=/people/x/xifeng-yan/>Xifeng Yan</a>
|
<a href=/people/w/william-yang-wang/>William Yang Wang</a><br><a href=/volumes/2020.emnlp-main/ class=text-muted>Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--697><div class="card-body p-3 small">Data-to-text generation has recently attracted substantial interests due to its wide applications. Existing methods have shown impressive performance on an array of <a href=https://en.wikipedia.org/wiki/Task_(computing)>tasks</a>. However, they rely on a significant amount of labeled data for each <a href=https://en.wikipedia.org/wiki/Task_(computing)>task</a>, which is costly to acquire and thus limits their application to new <a href=https://en.wikipedia.org/wiki/Task_(computing)>tasks</a> and domains. In this paper, we propose to leverage pre-training and transfer learning to address this issue. We propose a knowledge-grounded pre-training (KGPT), which consists of two parts, 1) a general knowledge-grounded generation model to generate knowledge-enriched text. 2) a pre-training paradigm on a massive knowledge-grounded text corpus crawled from the <a href=https://en.wikipedia.org/wiki/World_Wide_Web>web</a>. The pre-trained model can be fine-tuned on various data-to-text generation tasks to generate task-specific text. We adopt three settings, namely fully-supervised, zero-shot, few-shot to evaluate its effectiveness. Under the fully-supervised setting, our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> can achieve remarkable gains over the known baselines. Under zero-shot setting, our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> without seeing any examples achieves over 30 ROUGE-L on WebNLG while all other baselines fail. Under the few-shot setting, our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> only needs about one-fifteenth as many labeled examples to achieve the same level of performance as baseline models. These experiments consistently prove the strong generalization ability of our proposed <a href=https://en.wikipedia.org/wiki/Conceptual_framework>framework</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.nli-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.nli-1.0/>Proceedings of the First Workshop on Natural Language Interfaces</a></strong><br><a href=/people/a/ahmed-hassan/>Ahmed Hassan Awadallah</a>
|
<a href=/people/y/yu-su/>Yu Su</a>
|
<a href=/people/h/huan-sun/>Huan Sun</a>
|
<a href=/people/w/wen-tau-yih/>Scott Wen-tau Yih</a><br><a href=/volumes/2020.nli-1/ class=text-muted>Proceedings of the First Workshop on Natural Language Interfaces</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.332.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--332 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.332 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.332" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.332/>Document Classification for COVID-19 Literature<span class=acl-fixed-case>COVID</span>-19 Literature</a></strong><br><a href=/people/b/bernal-jimenez-gutierrez/>Bernal Jimenez Gutierrez</a>
|
<a href=/people/j/jucheng-zeng/>Jucheng Zeng</a>
|
<a href=/people/d/dongdong-zhang/>Dongdong Zhang</a>
|
<a href=/people/p/ping-zhang/>Ping Zhang</a>
|
<a href=/people/y/yu-su/>Yu Su</a><br><a href=/volumes/2020.findings-emnlp/ class=text-muted>Findings of the Association for Computational Linguistics: EMNLP 2020</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--332><div class="card-body p-3 small">The global pandemic has made it more important than ever to quickly and accurately retrieve relevant scientific literature for effective consumption by researchers in a wide range of fields. We provide an analysis of several multi-label document classification models on the LitCovid dataset, a growing collection of 23,000 research papers regarding the novel 2019 coronavirus. We find that pre-trained language models fine-tuned on this dataset outperform all other baselines and that BioBERT surpasses the others by a small margin with micro-F1 and accuracy scores of around 86 % and 75 % respectively on the test set. We evaluate the <a href=https://en.wikipedia.org/wiki/Data_efficiency>data efficiency</a> and generalizability of these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> as essential features of any system prepared to deal with an urgent situation like the current <a href=https://en.wikipedia.org/wiki/Health_crisis>health crisis</a>. We perform a data ablation study to determine how important article titles are for achieving reasonable performance on this <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>. Finally, we explore 50 errors made by the best performing models on LitCovid documents and find that they often (1) correlate certain labels too closely together and (2) fail to focus on discriminative sections of the articles ; both of which are important issues to address in future work. Both data and code are available on GitHub.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.nlpcovid19-acl.3.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--nlpcovid19-acl--3 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.nlpcovid19-acl.3 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.nlpcovid19-acl.3/>Document Classification for COVID-19 Literature<span class=acl-fixed-case>COVID-19</span> Literature</a></strong><br><a href=/people/b/bernal-jimenez-gutierrez/>Bernal Jiménez Gutiérrez</a>
|
<a href=/people/j/juncheng-zeng/>Juncheng Zeng</a>
|
<a href=/people/d/dongdong-zhang/>Dongdong Zhang</a>
|
<a href=/people/p/ping-zhang/>Ping Zhang</a>
|
<a href=/people/y/yu-su/>Yu Su</a><br><a href=/volumes/2020.nlpcovid19-acl/ class=text-muted>Proceedings of the 1st Workshop on NLP for COVID-19 at ACL 2020</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--nlpcovid19-acl--3><div class="card-body p-3 small">The global pandemic has made it more important than ever to quickly and accurately retrieve relevant scientific literature for effective consumption by researchers in a wide range of fields. We provide an analysis of several multi-label document classification models on the LitCovid dataset. We find that pre-trained language models outperform other <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> in both low and high data regimes, achieving a maximum F1 score of around 86 %. We note that even the highest performing models still struggle with label correlation, distraction from introductory text and CORD-19 generalization. Both data and code are available on GitHub.</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1038.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1038 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1038 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D18-1038.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1038" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D18-1038/>XL-NBT : A Cross-lingual Neural Belief Tracking Framework<span class=acl-fixed-case>XL</span>-<span class=acl-fixed-case>NBT</span>: A Cross-lingual Neural Belief Tracking Framework</a></strong><br><a href=/people/w/wenhu-chen/>Wenhu Chen</a>
|
<a href=/people/j/jianshu-chen/>Jianshu Chen</a>
|
<a href=/people/y/yu-su/>Yu Su</a>
|
<a href=/people/x/xin-wang/>Xin Wang</a>
|
<a href=/people/d/dong-yu/>Dong Yu</a>
|
<a href=/people/x/xifeng-yan/>Xifeng Yan</a>
|
<a href=/people/w/william-yang-wang/>William Yang Wang</a><br><a href=/volumes/D18-1/ class=text-muted>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1038><div class="card-body p-3 small">Task-oriented dialog systems are becoming pervasive, and many companies heavily rely on them to complement <a href=https://en.wikipedia.org/wiki/Intelligent_agent>human agents</a> for customer service in call centers. With globalization, the need for providing cross-lingual customer support becomes more urgent than ever. However, cross-lingual support poses great challengesit requires a large amount of additional annotated data from <a href=https://en.wikipedia.org/wiki/First_language>native speakers</a>. In order to bypass the expensive human annotation and achieve the first step towards the ultimate goal of building a universal dialog system, we set out to build a cross-lingual state tracking framework. Specifically, we assume that there exists a source language with dialog belief tracking annotations while the target languages have no annotated dialog data of any form. Then, we pre-train a state tracker for the source language as a teacher, which is able to exploit easy-to-access parallel data. We then distill and transfer its own knowledge to the student state tracker in target languages. We specifically discuss two types of common parallel resources : <a href=https://en.wikipedia.org/wiki/Bilingual_corpus>bilingual corpus</a> and <a href=https://en.wikipedia.org/wiki/Bilingual_dictionary>bilingual dictionary</a>, and design different transfer learning strategies accordingly. Experimentally, we successfully use English state tracker as the teacher to transfer its knowledge to both Italian and German trackers and achieve promising results.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1197.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1197 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1197 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-1197/>What It Takes to Achieve 100 % Condition Accuracy on WikiSQL<span class=acl-fixed-case>W</span>iki<span class=acl-fixed-case>SQL</span></a></strong><br><a href=/people/s/semih-yavuz/>Semih Yavuz</a>
|
<a href=/people/i/izzeddin-gur/>Izzeddin Gur</a>
|
<a href=/people/y/yu-su/>Yu Su</a>
|
<a href=/people/x/xifeng-yan/>Xifeng Yan</a><br><a href=/volumes/D18-1/ class=text-muted>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1197><div class="card-body p-3 small">WikiSQL is a newly released dataset for studying the <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language sequence</a> to SQL translation problem. The SQL queries in WikiSQL are simple : Each involves one relation and does not have any <a href=https://en.wikipedia.org/wiki/Join_(SQL)>join operation</a>. Despite of its simplicity, none of the publicly reported structured query generation models can achieve an <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> beyond 62 %, which is still far from enough for practical use. In this paper, we ask two questions, Why is the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> still low for such simple <a href=https://en.wikipedia.org/wiki/Information_retrieval>queries</a>? and What does it take to achieve 100 % accuracy on WikiSQL? To limit the scope of our study, we focus on the WHERE clause in <a href=https://en.wikipedia.org/wiki/SQL>SQL</a>. The answers will help us gain insights about the directions we should explore in order to further improve the translation accuracy. We will then investigate alternative solutions to realize the potential ceiling performance on WikiSQL. Our proposed <a href=https://en.wikipedia.org/wiki/Solution>solution</a> can reach up to 88.6 % <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>condition accuracy</a> on the WikiSQL dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1075.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1075 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1075 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N18-1075" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N18-1075/>Global Relation Embedding for <a href=https://en.wikipedia.org/wiki/Relation_extraction>Relation Extraction</a></a></strong><br><a href=/people/y/yu-su/>Yu Su</a>
|
<a href=/people/h/honglei-liu/>Honglei Liu</a>
|
<a href=/people/s/semih-yavuz/>Semih Yavuz</a>
|
<a href=/people/i/izzeddin-gur/>Izzeddin Gür</a>
|
<a href=/people/h/huan-sun/>Huan Sun</a>
|
<a href=/people/x/xifeng-yan/>Xifeng Yan</a><br><a href=/volumes/N18-1/ class=text-muted>Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1075><div class="card-body p-3 small">We study the problem of textual relation embedding with distant supervision. To combat the wrong labeling problem of distant supervision, we propose to embed textual relations with global statistics of relations, i.e., the co-occurrence statistics of textual and knowledge base relations collected from the entire corpus. This approach turns out to be more robust to the training noise introduced by distant supervision. On a popular relation extraction dataset, we show that the learned textual relation embedding can be used to augment existing relation extraction models and significantly improve their performance. Most remarkably, for the top 1,000 relational facts discovered by the best existing <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>, the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>precision</a> can be improved from 83.9 % to 89.3 %.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1124.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1124 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1124 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285803553 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-1124/>DialSQL : Dialogue Based Structured Query Generation<span class=acl-fixed-case>D</span>ial<span class=acl-fixed-case>SQL</span>: Dialogue Based Structured Query Generation</a></strong><br><a href=/people/i/izzeddin-gur/>Izzeddin Gur</a>
|
<a href=/people/s/semih-yavuz/>Semih Yavuz</a>
|
<a href=/people/y/yu-su/>Yu Su</a>
|
<a href=/people/x/xifeng-yan/>Xifeng Yan</a><br><a href=/volumes/P18-1/ class=text-muted>Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1124><div class="card-body p-3 small">The recent advance in <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning</a> and <a href=https://en.wikipedia.org/wiki/Semantic_parsing>semantic parsing</a> has significantly improved the translation accuracy of <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language questions</a> to structured queries. However, further improvement of the existing <a href=https://en.wikipedia.org/wiki/Scientific_method>approaches</a> turns out to be quite challenging. Rather than solely relying on algorithmic innovations, in this work, we introduce DialSQL, a dialogue-based structured query generation framework that leverages <a href=https://en.wikipedia.org/wiki/Human_intelligence>human intelligence</a> to boost the performance of existing <a href=https://en.wikipedia.org/wiki/Algorithm>algorithms</a> via user interaction. DialSQL is capable of identifying potential errors in a generated <a href=https://en.wikipedia.org/wiki/SQL>SQL query</a> and asking users for validation via simple <a href=https://en.wikipedia.org/wiki/Multiple_choice>multi-choice questions</a>. User feedback is then leveraged to revise the query. We design a generic simulator to bootstrap synthetic training dialogues and evaluate the performance of DialSQL on the WikiSQL dataset. Using SQLNet as a black box query generation tool, DialSQL improves its performance from 61.3 % to 69.0 % using only 2.4 validation questions per dialogue.</div></div><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1094.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1094 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1094 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D17-1094.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D17-1094/>Recovering Question Answering Errors via Query Revision</a></strong><br><a href=/people/s/semih-yavuz/>Semih Yavuz</a>
|
<a href=/people/i/izzeddin-gur/>Izzeddin Gur</a>
|
<a href=/people/y/yu-su/>Yu Su</a>
|
<a href=/people/x/xifeng-yan/>Xifeng Yan</a><br><a href=/volumes/D17-1/ class=text-muted>Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1094><div class="card-body p-3 small">The existing factoid QA systems often lack a post-inspection component that can help models recover from their own mistakes. In this work, we propose to crosscheck the corresponding <a href=https://en.wikipedia.org/wiki/Binary_relation>KB relations</a> behind the predicted answers and identify potential inconsistencies. Instead of developing a new <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> that accepts evidences collected from these relations, we choose to plug them back to the original questions directly and check if the revised question makes sense or not. A bidirectional LSTM is applied to encode revised questions. We develop a scoring mechanism over the revised question encodings to refine the predictions of a base QA system. This approach can improve the <a href=https://en.wikipedia.org/wiki/F1_score>F1 score</a> of <a href=https://en.wikipedia.org/wiki/STAGG>STAGG</a> (Yih et al., 2015), one of the leading QA systems, from 52.5 % to 53.9 % on WEBQUESTIONS data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1131.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1131 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1131 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1131/>An End-to-End Deep Framework for Answer Triggering with a Novel Group-Level Objective</a></strong><br><a href=/people/j/jie-zhao/>Jie Zhao</a>
|
<a href=/people/y/yu-su/>Yu Su</a>
|
<a href=/people/z/ziyu-guan/>Ziyu Guan</a>
|
<a href=/people/h/huan-sun/>Huan Sun</a><br><a href=/volumes/D17-1/ class=text-muted>Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1131><div class="card-body p-3 small">Given a question and a set of answer candidates, answer triggering determines whether the candidate set contains any correct answers. If yes, it then outputs a correct one. In contrast to existing pipeline methods which first consider individual candidate answers separately and then make a prediction based on a threshold, we propose an end-to-end deep neural network framework, which is trained by a novel group-level objective function that directly optimizes the answer triggering performance. Our <a href=https://en.wikipedia.org/wiki/Loss_function>objective function</a> penalizes three potential types of error and allows training the <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> in an end-to-end manner. Experimental results on the WikiQA benchmark show that our <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> outperforms the <a href=https://en.wikipedia.org/wiki/State_(polity)>state</a> of the arts by a 6.6 % absolute gain under F1 measure.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Yu+Su" title="Search for 'Yu Su' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/x/xifeng-yan/ class=align-middle>Xifeng Yan</a>
<span class="badge badge-secondary align-middle ml-2">6</span></li><li class=list-group-item><a href=/people/h/huan-sun/ class=align-middle>Huan Sun</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/people/s/semih-yavuz/ class=align-middle>Semih Yavuz</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/people/i/izzeddin-gur/ class=align-middle>Izzeddin Gür</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/people/w/wenhu-chen/ class=align-middle>Wenhu Chen</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/w/william-yang-wang/ class=align-middle>William Yang Wang</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/g/graham-neubig/ class=align-middle>Graham Neubig</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/b/bernal-jimenez-gutierrez/ class=align-middle>Bernal Jiménez Gutiérrez</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/d/dongdong-zhang/ class=align-middle>Dongdong Zhang</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/p/ping-zhang/ class=align-middle>Ping Zhang</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/r/royi-lachmy/ class=align-middle>Royi Lachmy</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/ziyu-yao/ class=align-middle>Ziyu Yao</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/g/greg-durrett/ class=align-middle>Greg Durrett</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/milos-gligoric/ class=align-middle>Milos Gligoric</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/junyi-jessy-li/ class=align-middle>Junyi Jessy Li</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/ray-mooney/ class=align-middle>Ray Mooney</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/reut-tsarfaty/ class=align-middle>Reut Tsarfaty</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/ahmed-hassan/ class=align-middle>Ahmed Hassan</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/w/wen-tau-yih/ class=align-middle>Wen-tau Yih</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jianshu-chen/ class=align-middle>Jianshu Chen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/x/xin-wang/ class=align-middle>Xin Wang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/dong-yu/ class=align-middle>Dong Yu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jie-zhao/ class=align-middle>Jie Zhao</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/ziyu-guan/ class=align-middle>Ziyu Guan</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/p/pengcheng-yin/ class=align-middle>Pengcheng Yin</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/hao-fang/ class=align-middle>Hao Fang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/adam-pauls/ class=align-middle>Adam Pauls</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/e/emmanouil-antonios-platanios/ class=align-middle>Emmanouil Antonios Platanios</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/sam-thomson/ class=align-middle>Sam Thomson</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jacob-andreas/ class=align-middle>Jacob Andreas</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/e/esin-durmus/ class=align-middle>Esin Durmus</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/v/vivek-gupta/ class=align-middle>Vivek Gupta</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/n/nelson-f-liu/ class=align-middle>Nelson F. Liu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/n/nanyun-peng/ class=align-middle>Nanyun Peng</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jucheng-zeng/ class=align-middle>Jucheng Zeng</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/samuel-stevens/ class=align-middle>Samuel Stevens</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/honglei-liu/ class=align-middle>Honglei Liu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/juncheng-zeng/ class=align-middle>Juncheng Zeng</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/g/genyu-zhang/ class=align-middle>Genyu Zhang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/changhong-he/ class=align-middle>Changhong He</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/lei-lin/ class=align-middle>Lei Lin</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/cheng-jie-sun/ class=align-middle>Cheng-Jie Sun</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/lili-shan/ class=align-middle>Lili Shan</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/emnlp/ class=align-middle>EMNLP</a><span class="badge badge-secondary align-middle ml-2">5</span></li><li class=list-group-item><a href=/venues/naacl/ class=align-middle>NAACL</a><span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/venues/nlp4prog/ class=align-middle>NLP4Prog</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/nli/ class=align-middle>NLI</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/findings/ class=align-middle>Findings</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-venues aria-expanded=false aria-controls=more-venues>show all...</li><div class="collapse border-top" id=more-venues><li class=list-group-item><a href=/venues/blackboxnlp/ class=align-middle>BlackboxNLP</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/nlpcovid19/ class=align-middle>NLP-COVID19</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/semeval/ class=align-middle>SemEval</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/acl/ class=align-middle>ACL</a><span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>