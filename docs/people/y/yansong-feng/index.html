<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Yansong Feng - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Yansong</span> <span class=font-weight-bold>Feng</span></h2><hr><div class=row><div class=col-lg-9><h4>2022</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.acl-long.406.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--acl-long--406 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.acl-long.406 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.acl-long.406" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2022.acl-long.406/>Entailment Graph Learning with <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>Textual Entailment</a> and Soft Transitivity</a></strong><br><a href=/people/z/zhibin-chen/>Zhibin Chen</a>
|
<a href=/people/y/yansong-feng/>Yansong Feng</a>
|
<a href=/people/d/dongyan-zhao/>Dongyan Zhao</a><br><a href=/volumes/2022.acl-long/ class=text-muted>Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--acl-long--406><div class="card-body p-3 small">Typed entailment graphs try to learn the entailment relations between predicates from text and model them as edges between predicate nodes The construction of entailment graphs usually suffers from severe sparsity and unreliability of distributional similarity We propose a two stage method Entailment Graph with Textual Entailment and Transitivity EGT2 EGT2 learns the local entailment relations by recognizing the textual entailment between template sentences formed by typed CCG parsed predicates Based on the generated local graph EGT2 then uses three novel soft transitivity constraints to consider the logical transitivity in <a href=https://en.wikipedia.org/wiki/Logical_consequence>entailment structures</a> Experiments on benchmark datasets show that EGT2 can well model the transitivity in entailment graph to alleviate the sparsity and leads to signifcant improvement over current state of the art methods</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.acl-long.432.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--acl-long--432 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.acl-long.432 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.acl-long.432" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2022.acl-long.432/>Does Recommend-Revise Produce Reliable Annotations? An Analysis on Missing Instances in <span class=acl-fixed-case>D</span>oc<span class=acl-fixed-case>RED</span></a></strong><br><a href=/people/q/quzhe-huang/>Quzhe Huang</a>
|
<a href=/people/s/shibo-hao/>Shibo Hao</a>
|
<a href=/people/y/yuan-ye/>Yuan Ye</a>
|
<a href=/people/s/shengqi-zhu/>Shengqi Zhu</a>
|
<a href=/people/y/yansong-feng/>Yansong Feng</a>
|
<a href=/people/d/dongyan-zhao/>Dongyan Zhao</a><br><a href=/volumes/2022.acl-long/ class=text-muted>Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--acl-long--432><div class="card-body p-3 small">DocRED is a widely used dataset for document-level relation extraction. In the large-scale annotation, a recommend-revise scheme is adopted to reduce the workload. Within this scheme, annotators are provided with candidate relation instances from distant supervision, and they then manually supplement and remove relational facts based on the recommendations. However, when comparing DocRED with a subset relabeled from scratch, we find that this scheme results in a considerable amount of false negative samples and an obvious bias towards popular entities and relations. Furthermore, we observe that the models trained on DocRED have low recall on our relabeled dataset and inherit the same bias in the training data. Through the analysis of annotators&#8217; behaviors, we figure out the underlying reason for the problems above: the scheme actually discourages annotators from supplementing adequate instances in the revision phase. We appeal to future research to take into consideration the issues with the recommend-revise scheme when designing new models and annotation schemes. The relabeled dataset is released at https://github.com/AndrewZhe/Revisit-DocRED, to serve as a more reliable test set of document RE models.</div></div><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.433.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--433 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.433 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.433/>Exploring Distantly-Labeled Rationales in Neural Network Models</a></strong><br><a href=/people/q/quzhe-huang/>Quzhe Huang</a>
|
<a href=/people/s/shengqi-zhu/>Shengqi Zhu</a>
|
<a href=/people/y/yansong-feng/>Yansong Feng</a>
|
<a href=/people/d/dongyan-zhao/>Dongyan Zhao</a><br><a href=/volumes/2021.acl-long/ class=text-muted>Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--433><div class="card-body p-3 small">Recent studies strive to incorporate various <a href=https://en.wikipedia.org/wiki/Rationality>human rationales</a> into <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a> to improve <a href=https://en.wikipedia.org/wiki/Computer_simulation>model</a> performance, but few pay attention to the quality of the rationales. Most existing methods distribute their models&#8217; focus to distantly-labeled rationale words entirely and equally, while ignoring the potential important non-rationale words and not distinguishing the importance of different rationale words. In this paper, we propose two novel auxiliary loss functions to make better use of distantly-labeled rationales, which encourage models to maintain their focus on important words beyond labeled rationales (PINs) and alleviate redundant training on non-helpful rationales (NoIRs). Experiments on two representative classification tasks show that our proposed methods can push a classification model to effectively learn crucial clues from non-perfect rationales while maintaining the ability to spread its focus to other unlabeled important words, thus significantly outperform existing methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.137.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--137 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.137 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.naacl-main.137" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.137/>Lattice-BERT : Leveraging Multi-Granularity Representations in Chinese Pre-trained Language Models<span class=acl-fixed-case>BERT</span>: Leveraging Multi-Granularity Representations in <span class=acl-fixed-case>C</span>hinese Pre-trained Language Models</a></strong><br><a href=/people/y/yuxuan-lai/>Yuxuan Lai</a>
|
<a href=/people/y/yijia-liu/>Yijia Liu</a>
|
<a href=/people/y/yansong-feng/>Yansong Feng</a>
|
<a href=/people/s/songfang-huang/>Songfang Huang</a>
|
<a href=/people/d/dongyan-zhao/>Dongyan Zhao</a><br><a href=/volumes/2021.naacl-main/ class=text-muted>Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--137><div class="card-body p-3 small">Chinese pre-trained language models usually process text as a sequence of characters, while ignoring more <a href=https://en.wikipedia.org/wiki/Granularity>coarse granularity</a>, e.g., <a href=https://en.wikipedia.org/wiki/Word>words</a>. In this work, we propose a novel pre-training paradigm for Chinese Lattice-BERT, which explicitly incorporates word representations along with <a href=https://en.wikipedia.org/wiki/Chinese_characters>characters</a>, thus can model a sentence in a multi-granularity manner. Specifically, we construct a <a href=https://en.wikipedia.org/wiki/Lattice_graph>lattice graph</a> from the <a href=https://en.wikipedia.org/wiki/Character_(computing)>characters</a> and words in a sentence and feed all these text units into transformers. We design a lattice position attention mechanism to exploit the <a href=https://en.wikipedia.org/wiki/Lattice_model_(physics)>lattice structures</a> in self-attention layers. We further propose a masked segment prediction task to push the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> to learn from rich but redundant information inherent in <a href=https://en.wikipedia.org/wiki/Lattice_model_(physics)>lattices</a>, while avoiding learning unexpected tricks. Experiments on 11 Chinese natural language understanding tasks show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> can bring an average increase of 1.5 % under the 12-layer setting, which achieves new state-of-the-art among base-size models on the CLUE benchmarks. Further analysis shows that Lattice-BERT can harness the <a href=https://en.wikipedia.org/wiki/Lattice_model_(physics)>lattice structures</a>, and the improvement comes from the exploration of <a href=https://en.wikipedia.org/wiki/Redundancy_(information_theory)>redundant information</a> and multi-granularity representations. Our code will be available at https://github.com/alibaba/pretrained-language-models/LatticeBERT.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.155.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--155 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.155 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.naacl-main.155" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.155/>Everything Has a Cause : Leveraging <a href=https://en.wikipedia.org/wiki/Causal_inference>Causal Inference</a> in Legal Text Analysis</a></strong><br><a href=/people/x/xiao-liu/>Xiao Liu</a>
|
<a href=/people/d/da-yin/>Da Yin</a>
|
<a href=/people/y/yansong-feng/>Yansong Feng</a>
|
<a href=/people/y/yuting-wu/>Yuting Wu</a>
|
<a href=/people/d/dongyan-zhao/>Dongyan Zhao</a><br><a href=/volumes/2021.naacl-main/ class=text-muted>Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--155><div class="card-body p-3 small">Causal inference is the process of capturing cause-effect relationship among variables. Most existing works focus on dealing with <a href=https://en.wikipedia.org/wiki/Structured_data>structured data</a>, while mining causal relationship among factors from <a href=https://en.wikipedia.org/wiki/Unstructured_data>unstructured data</a>, like text, has been less examined, but is of great importance, especially in the <a href=https://en.wikipedia.org/wiki/Legal_research>legal domain</a>. In this paper, we propose a novel Graph-based Causal Inference (GCI) framework, which builds causal graphs from fact descriptions without much human involvement and enables causal inference to facilitate legal practitioners to make proper decisions. We evaluate the <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> on a challenging similar charge disambiguation task. Experimental results show that GCI can capture the nuance from fact descriptions among multiple confusing charges and provide explainable discrimination, especially in few-shot settings. We also observe that the <a href=https://en.wikipedia.org/wiki/Causality>causal knowledge</a> contained in GCI can be effectively injected into powerful <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a> for better performance and <a href=https://en.wikipedia.org/wiki/Interpretability>interpretability</a>.</div></div><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.228.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--228 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.228 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.coling-main.228" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.228/>Exploring Question-Specific Rewards for Generating Deep Questions</a></strong><br><a href=/people/y/yuxi-xie/>Yuxi Xie</a>
|
<a href=/people/l/liangming-pan/>Liangming Pan</a>
|
<a href=/people/d/dongzhe-wang/>Dongzhe Wang</a>
|
<a href=/people/m/min-yen-kan/>Min-Yen Kan</a>
|
<a href=/people/y/yansong-feng/>Yansong Feng</a><br><a href=/volumes/2020.coling-main/ class=text-muted>Proceedings of the 28th International Conference on Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--228><div class="card-body p-3 small">Recent question generation (QG) approaches often utilize the sequence-to-sequence framework (Seq2Seq) to optimize the log likelihood of ground-truth questions using teacher forcing. However, this training objective is inconsistent with actual question quality, which is often reflected by certain global properties such as whether the question can be answered by the document. As such, we directly optimize for QG-specific objectives via <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning</a> to improve question quality. We design three different rewards that target to improve the <a href=https://en.wikipedia.org/wiki/Fluency>fluency</a>, <a href=https://en.wikipedia.org/wiki/Relevance>relevance</a>, and answerability of generated questions. We conduct both automatic and human evaluations in addition to thorough analysis to explore the effect of each QG-specific reward. We find that optimizing on question-specific rewards generally leads to better performance in automatic evaluation metrics. However, only the <a href=https://en.wikipedia.org/wiki/Motivational_salience>rewards</a> that correlate well with <a href=https://en.wikipedia.org/wiki/Judgement>human judgement</a> (e.g., relevance) lead to real improvement in question quality. Optimizing for the others, especially answerability, introduces incorrect bias to the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>, resulting in poorer question quality. The code is publicly available at https://github.com/YuxiXie/RL-for-Question-Generation.</div></div><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1398.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1398 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1398 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1398/>Easy First Relation Extraction with <a href=https://en.wikipedia.org/wiki/Redundancy_(information_theory)>Information Redundancy</a></a></strong><br><a href=/people/s/shuai-ma/>Shuai Ma</a>
|
<a href=/people/g/gang-wang/>Gang Wang</a>
|
<a href=/people/y/yansong-feng/>Yansong Feng</a>
|
<a href=/people/j/jinpeng-huai/>Jinpeng Huai</a><br><a href=/volumes/D19-1/ class=text-muted>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1398><div class="card-body p-3 small">Many existing relation extraction (RE) models make decisions globally using integer linear programming (ILP). However, it is nontrivial to make use of <a href=https://en.wikipedia.org/wiki/Integer_linear_programming>integer linear programming</a> as a blackbox solver for <a href=https://en.wikipedia.org/wiki/RE_(complexity)>RE</a>. Its cost of time and memory may become unacceptable with the increase of data scale, and <a href=https://en.wikipedia.org/wiki/Redundancy_(information_theory)>redundant information</a> needs to be encoded cautiously for ILP. In this paper, we propose an easy first approach for relation extraction with information redundancies, embedded in the results produced by local sentence level extractors, during which conflict decisions are resolved with domain and uniqueness constraints. Information redundancies are leveraged to support both easy first collective inference for easy decisions in the first stage and ILP for hard decisions in a subsequent stage. Experimental study shows that our approach improves the <a href=https://en.wikipedia.org/wiki/Efficiency>efficiency</a> and <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of RE, and outperforms both ILP and neural network-based methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1301.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1301 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1301 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/356088995 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1301/>Enhancing Key-Value Memory Neural Networks for Knowledge Based Question Answering</a></strong><br><a href=/people/k/kun-xu/>Kun Xu</a>
|
<a href=/people/y/yuxuan-lai/>Yuxuan Lai</a>
|
<a href=/people/y/yansong-feng/>Yansong Feng</a>
|
<a href=/people/z/zhiguo-wang/>Zhiguo Wang</a><br><a href=/volumes/N19-1/ class=text-muted>Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1301><div class="card-body p-3 small">Traditional Key-value Memory Neural Networks (KV-MemNNs) are proved to be effective to support shallow reasoning over a collection of documents in domain specific Question Answering or Reading Comprehension tasks. However, extending KV-MemNNs to Knowledge Based Question Answering (KB-QA) is not trivia, which should properly decompose a complex question into a sequence of queries against the <a href=https://en.wikipedia.org/wiki/Random-access_memory>memory</a>, and update the query representations to support multi-hop reasoning over the <a href=https://en.wikipedia.org/wiki/Random-access_memory>memory</a>. In this paper, we propose a novel mechanism to enable conventional KV-MemNNs models to perform interpretable reasoning for complex questions. To achieve this, we design a new query updating strategy to mask previously-addressed memory information from the query representations, and introduce a novel STOP strategy to avoid invalid or repeated memory reading without strong annotation signals. This also enables KV-MemNNs to produce structured queries and work in a semantic parsing fashion. Experimental results on benchmark datasets show that our solution, trained with question-answer pairs only, can provide conventional KV-MemNNs models with better reasoning abilities on complex questions, and achieve state-of-art performances.</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1112.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1112 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1112 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/305213739 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1112" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1112/>SQL-to-Text Generation with Graph-to-Sequence Model<span class=acl-fixed-case>SQL</span>-to-Text Generation with Graph-to-Sequence Model</a></strong><br><a href=/people/k/kun-xu/>Kun Xu</a>
|
<a href=/people/l/lingfei-wu/>Lingfei Wu</a>
|
<a href=/people/z/zhiguo-wang/>Zhiguo Wang</a>
|
<a href=/people/y/yansong-feng/>Yansong Feng</a>
|
<a href=/people/v/vadim-sheinin/>Vadim Sheinin</a><br><a href=/volumes/D18-1/ class=text-muted>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1112><div class="card-body p-3 small">Previous work approaches the SQL-to-text generation task using vanilla Seq2Seq models, which may not fully capture the inherent <a href=https://en.wikipedia.org/wiki/Graph_(abstract_data_type)>graph-structured information</a> in <a href=https://en.wikipedia.org/wiki/SQL>SQL query</a>. In this paper, we propose a graph-to-sequence model to encode the global structure information into node embeddings. This <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> can effectively learn the correlation between the SQL query pattern and its interpretation. Experimental results on the WikiSQL dataset and Stackoverflow dataset show that our model outperforms the Seq2Seq and Tree2Seq baselines, achieving the state-of-the-art performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1194.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1194 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1194 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1194.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285805276 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-1194/>Marrying Up <a href=https://en.wikipedia.org/wiki/Regular_expression>Regular Expressions</a> with <a href=https://en.wikipedia.org/wiki/Neural_network>Neural Networks</a> : A Case Study for Spoken Language Understanding</a></strong><br><a href=/people/b/bingfeng-luo/>Bingfeng Luo</a>
|
<a href=/people/y/yansong-feng/>Yansong Feng</a>
|
<a href=/people/z/zheng-wang/>Zheng Wang</a>
|
<a href=/people/s/songfang-huang/>Songfang Huang</a>
|
<a href=/people/r/rui-yan/>Rui Yan</a>
|
<a href=/people/d/dongyan-zhao/>Dongyan Zhao</a><br><a href=/volumes/P18-1/ class=text-muted>Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1194><div class="card-body p-3 small">The success of many natural language processing (NLP) tasks is bound by the number and quality of annotated data, but there is often a shortage of such training data. In this paper, we ask the question : Can we combine a neural network (NN) with regular expressions (RE) to improve <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised learning</a> for <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a>?. In answer, we develop novel methods to exploit the rich expressiveness of REs at different levels within a NN, showing that the combination significantly enhances the learning effectiveness when a small number of training examples are available. We evaluate our approach by applying it to spoken language understanding for intent detection and slot filling. Experimental results show that our approach is highly effective in exploiting the available <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training data</a>, giving a clear boost to the RE-unaware NN.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2070.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2070 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2070 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2070.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285804139 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-2070/>Modeling discourse cohesion for discourse parsing via memory network</a></strong><br><a href=/people/y/yanyan-jia/>Yanyan Jia</a>
|
<a href=/people/y/yuan-ye/>Yuan Ye</a>
|
<a href=/people/y/yansong-feng/>Yansong Feng</a>
|
<a href=/people/y/yuxuan-lai/>Yuxuan Lai</a>
|
<a href=/people/r/rui-yan/>Rui Yan</a>
|
<a href=/people/d/dongyan-zhao/>Dongyan Zhao</a><br><a href=/volumes/P18-2/ class=text-muted>Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2070><div class="card-body p-3 small">Identifying long-span dependencies between discourse units is crucial to improve discourse parsing performance. Most existing approaches design sophisticated <a href=https://en.wikipedia.org/wiki/Software_feature>features</a> or exploit various off-the-shelf tools, but achieve little success. In this paper, we propose a new transition-based discourse parser that makes use of memory networks to take discourse cohesion into account. The automatically captured discourse cohesion benefits discourse parsing, especially for long span scenarios. Experiments on the RST discourse treebank show that our method outperforms traditional featured based methods, and the memory based discourse cohesion can improve the overall parsing performance significantly.</div></div><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2036.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2036 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2036 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-2036/>How to Make Context More Useful? An Empirical Study on Context-Aware Neural Conversational Models</a></strong><br><a href=/people/z/zhiliang-tian/>Zhiliang Tian</a>
|
<a href=/people/r/rui-yan/>Rui Yan</a>
|
<a href=/people/l/lili-mou/>Lili Mou</a>
|
<a href=/people/y/yiping-song/>Yiping Song</a>
|
<a href=/people/y/yansong-feng/>Yansong Feng</a>
|
<a href=/people/d/dongyan-zhao/>Dongyan Zhao</a><br><a href=/volumes/P17-2/ class=text-muted>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2036><div class="card-body p-3 small">Generative conversational systems are attracting increasing attention in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing (NLP)</a>. Recently, researchers have noticed the importance of <a href=https://en.wikipedia.org/wiki/Context_(language_use)>context information</a> in dialog processing, and built various <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> to utilize <a href=https://en.wikipedia.org/wiki/Context_(language_use)>context</a>. However, there is no systematic comparison to analyze how to use <a href=https://en.wikipedia.org/wiki/Context_(language_use)>context</a> effectively. In this paper, we conduct an empirical study to compare various <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> and investigate the effect of <a href=https://en.wikipedia.org/wiki/Context_(language_use)>context information</a> in <a href=https://en.wikipedia.org/wiki/Dialogue>dialog systems</a>. We also propose a variant that explicitly weights context vectors by context-query relevance, outperforming the other baselines.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1233.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1233 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1233 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1233/>Towards Implicit Content-Introducing for Generative Short-Text Conversation Systems</a></strong><br><a href=/people/l/lili-yao/>Lili Yao</a>
|
<a href=/people/y/yaoyuan-zhang/>Yaoyuan Zhang</a>
|
<a href=/people/y/yansong-feng/>Yansong Feng</a>
|
<a href=/people/d/dongyan-zhao/>Dongyan Zhao</a>
|
<a href=/people/r/rui-yan/>Rui Yan</a><br><a href=/volumes/D17-1/ class=text-muted>Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1233><div class="card-body p-3 small">The study on human-computer conversation systems is a hot research topic nowadays. One of the prevailing methods to build the system is using the generative Sequence-to-Sequence (Seq2Seq) model through <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a>. However, the standard Seq2Seq model is prone to generate <a href=https://en.wikipedia.org/wiki/Triviality_(mathematics)>trivial responses</a>. In this paper, we aim to generate a more meaningful and informative reply when answering a given question. We propose an implicit content-introducing method which incorporates additional information into the Seq2Seq model in a flexible way. Specifically, we fuse the general decoding and the auxiliary cue word information through our proposed hierarchical gated fusion unit. Experiments on real-life data demonstrate that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> consistently outperforms a set of competitive baselines in terms of BLEU scores and human evaluation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1289.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1289 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1289 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1289/>Learning to Predict Charges for Criminal Cases with Legal Basis</a></strong><br><a href=/people/b/bingfeng-luo/>Bingfeng Luo</a>
|
<a href=/people/y/yansong-feng/>Yansong Feng</a>
|
<a href=/people/j/jianbo-xu/>Jianbo Xu</a>
|
<a href=/people/x/xiang-zhang/>Xiang Zhang</a>
|
<a href=/people/d/dongyan-zhao/>Dongyan Zhao</a><br><a href=/volumes/D17-1/ class=text-muted>Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1289><div class="card-body p-3 small">The charge prediction task is to determine appropriate charges for a given case, which is helpful for legal assistant systems where the user input is fact description. We argue that relevant law articles play an important role in this task, and therefore propose an attention-based neural network method to jointly model the charge prediction task and the relevant article extraction task in a unified framework. The experimental results show that, besides providing legal basis, the relevant articles can also clearly improve the charge prediction results, and our full model can effectively predict appropriate charges for cases with different expression styles.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Yansong+Feng" title="Search for 'Yansong Feng' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/d/dongyan-zhao/ class=align-middle>Dongyan Zhao</a>
<span class="badge badge-secondary align-middle ml-2">10</span></li><li class=list-group-item><a href=/people/r/rui-yan/ class=align-middle>Rui Yan</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/people/y/yuxuan-lai/ class=align-middle>Yuxuan Lai</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/q/quzhe-huang/ class=align-middle>Quzhe Huang</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/s/shengqi-zhu/ class=align-middle>Shengqi Zhu</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/y/yuan-ye/ class=align-middle>Yuan Ye</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/k/kun-xu/ class=align-middle>Kun Xu</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/z/zhiguo-wang/ class=align-middle>Zhiguo Wang</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/b/bingfeng-luo/ class=align-middle>Bingfeng Luo</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/s/songfang-huang/ class=align-middle>Songfang Huang</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/z/zhiliang-tian/ class=align-middle>Zhiliang Tian</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/lili-mou/ class=align-middle>Lili Mou</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yiping-song/ class=align-middle>Yiping Song</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/zhibin-chen/ class=align-middle>Zhibin Chen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/shibo-hao/ class=align-middle>Shibo Hao</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/lingfei-wu/ class=align-middle>Lingfei Wu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/v/vadim-sheinin/ class=align-middle>Vadim Sheinin</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/shuai-ma/ class=align-middle>Shuai Ma</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/g/gang-wang/ class=align-middle>Gang Wang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jinpeng-huai/ class=align-middle>Jinpeng Huai</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/lili-yao/ class=align-middle>Lili Yao</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yaoyuan-zhang/ class=align-middle>Yaoyuan Zhang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jianbo-xu/ class=align-middle>Jianbo Xu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/x/xiang-zhang/ class=align-middle>Xiang Zhang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yijia-liu/ class=align-middle>Yijia Liu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/x/xiao-liu/ class=align-middle>Xiao Liu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/da-yin/ class=align-middle>Da Yin</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yuting-wu/ class=align-middle>Yuting Wu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yuxi-xie/ class=align-middle>Yuxi Xie</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/liangming-pan/ class=align-middle>Liangming Pan</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/dongzhe-wang/ class=align-middle>Dongzhe Wang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/min-yen-kan/ class=align-middle>Min-Yen Kan</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/zheng-wang/ class=align-middle>Zheng Wang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yanyan-jia/ class=align-middle>Yanyan Jia</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/acl/ class=align-middle>ACL</a><span class="badge badge-secondary align-middle ml-2">6</span></li><li class=list-group-item><a href=/venues/emnlp/ class=align-middle>EMNLP</a><span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/venues/naacl/ class=align-middle>NAACL</a><span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/venues/coling/ class=align-middle>COLING</a><span class="badge badge-secondary align-middle ml-2">1</span></li></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright Â©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>