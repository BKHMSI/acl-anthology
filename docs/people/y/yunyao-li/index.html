<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Yunyao Li - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Yunyao</span> <span class=font-weight-bold>Li</span></h2><hr><div class=row><div class=col-lg-9><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.dash-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.dash-1.0/>Proceedings of the Second Workshop on Data Science with Human in the Loop: Language Advances</a></strong><br><a href=/people/e/eduard-dragut/>Eduard Dragut</a>
|
<a href=/people/y/yunyao-li/>Yunyao Li</a>
|
<a href=/people/l/lucian-popa/>Lucian Popa</a>
|
<a href=/people/s/slobodan-vucetic/>Slobodan Vucetic</a><br><a href=/volumes/2021.dash-1/ class=text-muted>Proceedings of the Second Workshop on Data Science with Human in the Loop: Language Advances</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.repl4nlp-1.1.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--repl4nlp-1--1 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.repl4nlp-1.1 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.repl4nlp-1.1/>Improving Cross-lingual Text Classification with Zero-shot Instance-Weighting</a></strong><br><a href=/people/i/irene-li/>Irene Li</a>
|
<a href=/people/p/prithviraj-sen/>Prithviraj Sen</a>
|
<a href=/people/h/huaiyu-zhu/>Huaiyu Zhu</a>
|
<a href=/people/y/yunyao-li/>Yunyao Li</a>
|
<a href=/people/d/dragomir-radev/>Dragomir Radev</a><br><a href=/volumes/2021.repl4nlp-1/ class=text-muted>Proceedings of the 6th Workshop on Representation Learning for NLP (RepL4NLP-2021)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--repl4nlp-1--1><div class="card-body p-3 small">Cross-lingual text classification (CLTC) is a challenging task made even harder still due to the lack of labeled data in low-resource languages. In this paper, we propose zero-shot instance-weighting, a general model-agnostic zero-shot learning framework for improving CLTC by leveraging source instance weighting. It adds a module on top of pre-trained language models for similarity computation of instance weights, thus aligning each source instance to the target language. During <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training</a>, the framework utilizes <a href=https://en.wikipedia.org/wiki/Gradient_descent>gradient descent</a> that is weighted by instance weights to update parameters. We evaluate this <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> over seven target languages on three fundamental tasks and show its effectiveness and extensibility, by improving on F1 score up to 4 % in single-source transfer and 8 % in multi-source transfer. To the best of our knowledge, our method is the first to apply instance weighting in zero-shot CLTC. It is simple yet effective and easily extensible into multi-source transfer.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-industry.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-industry.0/>Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Industry Papers</a></strong><br><a href=/people/y/young-bum-kim/>Young-bum Kim</a>
|
<a href=/people/y/yunyao-li/>Yunyao Li</a>
|
<a href=/people/o/owen-rambow/>Owen Rambow</a><br><a href=/volumes/2021.naacl-industry/ class=text-muted>Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Industry Papers</a></span></p><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.345.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--345 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.345 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.345/>Learning Explainable Linguistic Expressions with Neural Inductive Logic Programming for Sentence Classification</a></strong><br><a href=/people/p/prithviraj-sen/>Prithviraj Sen</a>
|
<a href=/people/m/marina-danilevsky/>Marina Danilevsky</a>
|
<a href=/people/y/yunyao-li/>Yunyao Li</a>
|
<a href=/people/s/siddhartha-brahma/>Siddhartha Brahma</a>
|
<a href=/people/m/matthias-boehm/>Matthias Boehm</a>
|
<a href=/people/l/laura-chiticariu/>Laura Chiticariu</a>
|
<a href=/people/r/rajasekar-krishnamurthy/>Rajasekar Krishnamurthy</a><br><a href=/volumes/2020.emnlp-main/ class=text-muted>Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--345><div class="card-body p-3 small">Interpretability of <a href=https://en.wikipedia.org/wiki/Predictive_modelling>predictive models</a> is becoming increasingly important with growing adoption in the real-world. We present RuleNN, a neural network architecture for learning transparent models for sentence classification. The models are in the form of rules expressed in <a href=https://en.wikipedia.org/wiki/First-order_logic>first-order logic</a>, a <a href=https://en.wikipedia.org/wiki/Dialect>dialect</a> with well-defined, human-understandable semantics. More precisely, RuleNN learns linguistic expressions (LE) built on top of <a href=https://en.wikipedia.org/wiki/Predicate_(grammar)>predicates</a> extracted using shallow natural language understanding. Our experimental results show that RuleNN outperforms statistical relational learning and other neuro-symbolic methods, and performs comparably with black-box recurrent neural networks. Our user studies confirm that the learned LEs are explainable and capture domain semantics. Moreover, allowing domain experts to modify LEs and instill more <a href=https://en.wikipedia.org/wiki/Domain_knowledge>domain knowledge</a> leads to human-machine co-creation of models with better performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.517.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--517 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.517 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938677 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.517" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.517/>Learning Structured Representations of Entity Names using Active Learning and Weak Supervision<span class=acl-fixed-case>A</span>ctive <span class=acl-fixed-case>L</span>earning and Weak Supervision</a></strong><br><a href=/people/k/kun-qian/>Kun Qian</a>
|
<a href=/people/p/poornima-chozhiyath-raman/>Poornima Chozhiyath Raman</a>
|
<a href=/people/y/yunyao-li/>Yunyao Li</a>
|
<a href=/people/l/lucian-popa/>Lucian Popa</a><br><a href=/volumes/2020.emnlp-main/ class=text-muted>Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--517><div class="card-body p-3 small">Structured representations of entity names are useful for many entity-related tasks such as <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entity normalization</a> and variant generation. Learning the implicit structured representations of entity names without context and external knowledge is particularly challenging. In this paper, we present a novel learning framework that combines <a href=https://en.wikipedia.org/wiki/Active_learning>active learning</a> and weak supervision to solve this problem. Our experimental evaluation show that this <a href=https://en.wikipedia.org/wiki/Conceptual_framework>framework</a> enables the learning of high-quality models from merely a dozen or so labeled examples.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.nli-1.1.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--nli-1--1 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.nli-1.1 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://slideslive.com/38929797 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.nli-1.1/>Answering Complex Questions by Combining Information from Curated and Extracted Knowledge Bases</a></strong><br><a href=/people/n/nikita-bhutani/>Nikita Bhutani</a>
|
<a href=/people/x/xinyi-zheng/>Xinyi Zheng</a>
|
<a href=/people/k/kun-qian/>Kun Qian</a>
|
<a href=/people/y/yunyao-li/>Yunyao Li</a>
|
<a href=/people/h/h-jagadish/>H. Jagadish</a><br><a href=/volumes/2020.nli-1/ class=text-muted>Proceedings of the First Workshop on Natural Language Interfaces</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--nli-1--1><div class="card-body p-3 small">Knowledge-based question answering (KB_QA) has long focused on simple questions that can be answered from a single knowledge source, a manually curated or an automatically extracted KB. In this work, we look at answering complex questions which often require combining information from multiple sources. We present a novel KB-QA system, Multique, which can map a complex question to a complex query pattern using a sequence of simple queries each targeted at a specific KB. It finds simple queries using a neural-network based model capable of collective inference over textual relations in extracted KB and <a href=https://en.wikipedia.org/wiki/Ontology_(information_science)>ontological relations</a> in curated KB. Experiments show that our proposed system outperforms previous KB-QA systems on benchmark datasets, ComplexWebQuestions and WebQuestionsSP.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.38.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--38 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.38 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.findings-emnlp.38.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.38" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.38/>A Novel Workflow for Accurately and Efficiently Crowdsourcing Predicate Senses and Argument Labels</a></strong><br><a href=/people/y/youxuan-jiang/>Youxuan Jiang</a>
|
<a href=/people/h/huaiyu-zhu/>Huaiyu Zhu</a>
|
<a href=/people/j/jonathan-k-kummerfeld/>Jonathan K. Kummerfeld</a>
|
<a href=/people/y/yunyao-li/>Yunyao Li</a>
|
<a href=/people/w/walter-lasecki/>Walter Lasecki</a><br><a href=/volumes/2020.findings-emnlp/ class=text-muted>Findings of the Association for Computational Linguistics: EMNLP 2020</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--38><div class="card-body p-3 small">Resources for Semantic Role Labeling (SRL) are typically annotated by experts at great expense. Prior attempts to develop <a href=https://en.wikipedia.org/wiki/Crowdsourcing>crowdsourcing methods</a> have either had low <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> or required substantial <a href=https://en.wikipedia.org/wiki/Annotation>expert annotation</a>. We propose a new multi-stage crowd workflow that substantially reduces expert involvement without sacrificing <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>. In particular, we introduce a unique filter stage based on the key observation that crowd workers are able to almost perfectly filter out incorrect options for labels. Our three-stage workflow produces annotations with 95 % <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> for predicate labels and 93 % for argument labels, which is comparable to expert agreement. Compared to prior work on <a href=https://en.wikipedia.org/wiki/Crowdsourcing>crowdsourcing</a> for SRL, we decrease expert effort by 4x, from 56 % to 14 % of cases. Our approach enables more scalable annotation of SRL, and could enable annotation of NLP tasks that have previously been considered too complex to effectively crowdsource.</div></div><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3320.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3320 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3320 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3320/>Towards Universal Semantic Representation</a></strong><br><a href=/people/h/huaiyu-zhu/>Huaiyu Zhu</a>
|
<a href=/people/y/yunyao-li/>Yunyao Li</a>
|
<a href=/people/l/laura-chiticariu/>Laura Chiticariu</a><br><a href=/volumes/W19-33/ class=text-muted>Proceedings of the First International Workshop on Designing Meaning Representations</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3320><div class="card-body p-3 small">Natural language understanding at the <a href=https://en.wikipedia.org/wiki/Semantics>semantic level</a> and independent of <a href=https://en.wikipedia.org/wiki/Variation_(linguistics)>language variations</a> is of great practical value. Existing approaches such as semantic role labeling (SRL) and abstract meaning representation (AMR) still have features related to the peculiarities of the particular language. In this work we describe various challenges and possible solutions in designing a semantic representation that is universal across a variety of languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1586.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1586 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1586 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/385226574 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1586/>Low-resource Deep Entity Resolution with Transfer and Active Learning</a></strong><br><a href=/people/j/jungo-kasai/>Jungo Kasai</a>
|
<a href=/people/k/kun-qian/>Kun Qian</a>
|
<a href=/people/s/sairam-gurajada/>Sairam Gurajada</a>
|
<a href=/people/y/yunyao-li/>Yunyao Li</a>
|
<a href=/people/l/lucian-popa/>Lucian Popa</a><br><a href=/volumes/P19-1/ class=text-muted>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1586><div class="card-body p-3 small">Entity resolution (ER) is the task of identifying different representations of the same real-world entities across databases. It is a key step for <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge base creation</a> and <a href=https://en.wikipedia.org/wiki/Text_mining>text mining</a>. Recent adaptation of deep learning methods for ER mitigates the need for dataset-specific feature engineering by constructing distributed representations of entity records. While these methods achieve state-of-the-art performance over benchmark data, they require large amounts of labeled data, which are typically unavailable in realistic ER applications. In this paper, we develop a deep learning-based method that targets low-resource settings for ER through a novel combination of <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a> and <a href=https://en.wikipedia.org/wiki/Active_learning>active learning</a>. We design an <a href=https://en.wikipedia.org/wiki/Software_architecture>architecture</a> that allows us to learn a transferable model from a high-resource setting to a low-resource one. To further adapt to the target dataset, we incorporate active learning that carefully selects a few informative examples to fine-tune the transferred model. Empirical evaluation demonstrates that our method achieves comparable, if not better, performance compared to state-of-the-art learning-based methods while using an order of magnitude fewer labels.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-3023.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-3023 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-3023 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-3023/>HEIDL : Learning <a href=https://en.wikipedia.org/wiki/Linguistic_description>Linguistic Expressions</a> with <a href=https://en.wikipedia.org/wiki/Deep_learning>Deep Learning</a> and Human-in-the-Loop<span class=acl-fixed-case>HEIDL</span>: Learning Linguistic Expressions with Deep Learning and Human-in-the-Loop</a></strong><br><a href=/people/p/prithviraj-sen/>Prithviraj Sen</a>
|
<a href=/people/y/yunyao-li/>Yunyao Li</a>
|
<a href=/people/e/eser-kandogan/>Eser Kandogan</a>
|
<a href=/people/y/yiwei-yang/>Yiwei Yang</a>
|
<a href=/people/w/walter-lasecki/>Walter Lasecki</a><br><a href=/volumes/P19-3/ class=text-muted>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: System Demonstrations</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-3023><div class="card-body p-3 small">While the role of humans is increasingly recognized in machine learning community, representation of and interaction with <a href=https://en.wikipedia.org/wiki/Conceptual_model_(computer_science)>models</a> in current human-in-the-loop machine learning (HITL-ML) approaches are too low-level and far-removed from human&#8217;s conceptual models. We demonstrate HEIDL, a prototype HITL-ML system that exposes the machine-learned model through high-level, explainable linguistic expressions formed of predicates representing semantic structure of text. In HEIDL, human&#8217;s role is elevated from simply evaluating model predictions to interpreting and even updating the model logic directly by enabling interaction with rule predicates themselves. Raising the currency of interaction to such semantic levels calls for new interaction paradigms between humans and machines that result in improved productivity for text analytics model development process. Moreover, by involving humans in the process, the human-machine co-created models generalize better to unseen data as domain experts are able to instill their expertise by extrapolating from what has been learned by automated algorithms from few labelled data.</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1058.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1058 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1058 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1058/>Exploiting Structure in Representation of Named Entities using <a href=https://en.wikipedia.org/wiki/Active_learning>Active Learning</a></a></strong><br><a href=/people/n/nikita-bhutani/>Nikita Bhutani</a>
|
<a href=/people/k/kun-qian/>Kun Qian</a>
|
<a href=/people/y/yunyao-li/>Yunyao Li</a>
|
<a href=/people/h/h-v-jagadish/>H. V. Jagadish</a>
|
<a href=/people/m/mauricio-hernandez/>Mauricio Hernandez</a>
|
<a href=/people/m/mitesh-vasa/>Mitesh Vasa</a><br><a href=/volumes/C18-1/ class=text-muted>Proceedings of the 27th International Conference on Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1058><div class="card-body p-3 small">Fundamental to several knowledge-centric applications is the need to identify <a href=https://en.wikipedia.org/wiki/Named_entity>named entities</a> from their textual mentions. However, entities lack a unique representation and their mentions can differ greatly. These variations arise in complex ways that can not be captured using textual similarity metrics. However, entities have underlying structures, typically shared by entities of the same entity type, that can help reason over their name variations. Discovering, learning and manipulating these structures typically requires high manual effort in the form of large amounts of labeled training data and handwritten transformation programs. In this work, we propose an active-learning based framework that drastically reduces the labeled data required to learn the structures of entities. We show that <a href=https://en.wikipedia.org/wiki/Computer_program>programs</a> for mapping entity mentions to their structures can be automatically generated using human-comprehensible labels. Our experiments show that our <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> consistently outperforms both <a href=https://en.wikipedia.org/wiki/Handwriting>handwritten programs</a> and <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised learning models</a>. We also demonstrate the utility of our <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> in relation extraction and entity resolution tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-3000.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-3000/>Proceedings of the 2018 Conference of the North <span class=acl-fixed-case>A</span>merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 3 (Industry Papers)</a></strong><br><a href=/people/s/srinivas-bangalore/>Srinivas Bangalore</a>
|
<a href=/people/j/jennifer-chu-carroll/>Jennifer Chu-Carroll</a>
|
<a href=/people/y/yunyao-li/>Yunyao Li</a><br><a href=/volumes/N18-3/ class=text-muted>Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 3 (Industry Papers)</a></span></p><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1205.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1205 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1205 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1205/>CROWD-IN-THE-LOOP : A Hybrid Approach for Annotating Semantic Roles<span class=acl-fixed-case>CROWD</span>-<span class=acl-fixed-case>IN</span>-<span class=acl-fixed-case>THE</span>-<span class=acl-fixed-case>LOOP</span>: A Hybrid Approach for Annotating Semantic Roles</a></strong><br><a href=/people/c/chenguang-wang/>Chenguang Wang</a>
|
<a href=/people/a/alan-akbik/>Alan Akbik</a>
|
<a href=/people/l/laura-chiticariu/>Laura Chiticariu</a>
|
<a href=/people/y/yunyao-li/>Yunyao Li</a>
|
<a href=/people/f/fei-xia/>Fei Xia</a>
|
<a href=/people/a/anbang-xu/>Anbang Xu</a><br><a href=/volumes/D17-1/ class=text-muted>Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1205><div class="card-body p-3 small">Crowdsourcing has proven to be an effective method for generating <a href=https://en.wikipedia.org/wiki/Labeled_data>labeled data</a> for a range of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP tasks</a>. However, multiple recent attempts of using <a href=https://en.wikipedia.org/wiki/Crowdsourcing>crowdsourcing</a> to generate gold-labeled training data for semantic role labeling (SRL) reported only modest results, indicating that SRL is perhaps too difficult a task to be effectively crowdsourced. In this paper, we postulate that while producing SRL annotation does require expert involvement in general, a large subset of SRL labeling tasks is in fact appropriate for the crowd. We present a novel <a href=https://en.wikipedia.org/wiki/Workflow>workflow</a> in which we employ a classifier to identify difficult annotation tasks and route each task either to experts or crowd workers according to their difficulties. Our experimental evaluation shows that the proposed approach reduces the workload for experts by over two-thirds, and thus significantly reduces the cost of producing SRL annotation at little loss in quality.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Yunyao+Li" title="Search for 'Yunyao Li' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/k/kun-qian/ class=align-middle>Kun Qian</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/people/p/prithviraj-sen/ class=align-middle>Prithviraj Sen</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/l/laura-chiticariu/ class=align-middle>Laura Chiticariu</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/l/lucian-popa/ class=align-middle>Lucian Popa</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/h/huaiyu-zhu/ class=align-middle>Huaiyu Zhu</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/n/nikita-bhutani/ class=align-middle>Nikita Bhutani</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/w/walter-lasecki/ class=align-middle>Walter Lasecki</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/h/h-v-jagadish/ class=align-middle>H. V. Jagadish</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/mauricio-hernandez/ class=align-middle>Mauricio Hernandez</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/mitesh-vasa/ class=align-middle>Mitesh Vasa</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/marina-danilevsky/ class=align-middle>Marina Danilevsky</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/siddhartha-brahma/ class=align-middle>Siddhartha Brahma</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/matthias-boehm/ class=align-middle>Matthias Boehm</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/rajasekar-krishnamurthy/ class=align-middle>Rajasekar Krishnamurthy</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/p/poornima-chozhiyath-raman/ class=align-middle>Poornima Chozhiyath Raman</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/e/eduard-dragut/ class=align-middle>Eduard Dragut</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/slobodan-vucetic/ class=align-middle>Slobodan Vucetic</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/i/irene-li/ class=align-middle>Irene Li</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/dragomir-radev/ class=align-middle>Dragomir Radev</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/x/xinyi-zheng/ class=align-middle>Xinyi Zheng</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/h-jagadish/ class=align-middle>H. Jagadish</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/chenguang-wang/ class=align-middle>Chenguang Wang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/alan-akbik/ class=align-middle>Alan Akbik</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/f/fei-xia/ class=align-middle>Fei Xia</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/anbang-xu/ class=align-middle>Anbang Xu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/young-bum-kim/ class=align-middle>Young-Bum Kim</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/o/owen-rambow/ class=align-middle>Owen Rambow</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/youxuan-jiang/ class=align-middle>Youxuan Jiang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jonathan-k-kummerfeld/ class=align-middle>Jonathan K. Kummerfeld</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/srinivas-bangalore/ class=align-middle>Srinivas Bangalore</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jennifer-chu-carroll/ class=align-middle>Jennifer Chu-Carroll</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jungo-kasai/ class=align-middle>Jungo Kasai</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/sairam-gurajada/ class=align-middle>Sairam Gurajada</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/e/eser-kandogan/ class=align-middle>Eser Kandogan</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yiwei-yang/ class=align-middle>Yiwei Yang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/emnlp/ class=align-middle>EMNLP</a><span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/venues/naacl/ class=align-middle>NAACL</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/acl/ class=align-middle>ACL</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/coling/ class=align-middle>COLING</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/dash/ class=align-middle>DaSH</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-venues aria-expanded=false aria-controls=more-venues>show all...</li><div class="collapse border-top" id=more-venues><li class=list-group-item><a href=/venues/repl4nlp/ class=align-middle>RepL4NLP</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/nli/ class=align-middle>NLI</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/findings/ class=align-middle>Findings</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/ws/ class=align-middle>WS</a><span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>