<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Yang Liu - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Yang</span> <span class=font-weight-bold>Liu</span></h2><p class="font-weight-light text-muted">刘扬; Ph.D Purdue; ICSI, Dallas, Facebook, Liulishuo, Amazon</p><p class="font-weight-light text-muted"><span class=font-italic>Other people with similar names:</span>
<a href=/people/y/yang-liu/>Yang Liu</a>
(May refer to several people),
<a href=/people/y/yang-liu-3m/>Yang Liu</a>
(3M Health Information Systems),
<a href=/people/y/yang-liu-Helsinki/>Yang Liu</a>
(University of Helsinki),
<a href=/people/y/yang-liu-dt/>Yang Liu</a>
(National University of Defense Technology),
<a href=/people/y/yang-liu-edinburgh/>Yang Liu</a>
(Edinburgh),
<a href=/people/y/yang-liu-georgetown/>Yang (Janet) Liu</a>
(刘洋; Georgetown),
<a href=/people/y/yang-liu-gt/>Yang Liu</a>
(Georgetown University),
<a href=/people/y/yang-liu-hk/>Yang Liu</a>
(The Chinese University of Hong Kong (Shenzhen)),
<a href=/people/y/yang-liu-ict/>Yang Liu</a>
(刘洋; ICT, Tsinghua, Beijing Academy of Artificial Intelligence),
<a href=/people/y/yang-liu-microsoft/>Yang Liu</a>
(Microsoft Cognitive Services Research),
<a href=/people/y/yang-liu-pk/>Yang Liu</a>
(Peking University),
<a href=/people/y/yang-liu-umich/>Yang Liu</a>
(Univ. of Michigan, UC Santa Cruz)</p><hr><div class=row><div class=col-lg-9><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.nlp4convai-1.27.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--nlp4convai-1--27 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.nlp4convai-1.27 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.nlp4convai-1.27" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.nlp4convai-1.27/>Towards Zero and Few-shot Knowledge-seeking Turn Detection in Task-orientated Dialogue Systems</a></strong><br><a href=/people/d/di-jin/>Di Jin</a>
|
<a href=/people/s/shuyang-gao/>Shuyang Gao</a>
|
<a href=/people/s/seokhwan-kim/>Seokhwan Kim</a>
|
<a href=/people/y/yang-liu-icsi/>Yang Liu</a>
|
<a href=/people/d/dilek-hakkani-tur/>Dilek Hakkani-Tur</a><br><a href=/volumes/2021.nlp4convai-1/ class=text-muted>Proceedings of the 3rd Workshop on Natural Language Processing for Conversational AI</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--nlp4convai-1--27><div class="card-body p-3 small">Most prior work on task-oriented dialogue systems is restricted to supporting domain APIs. However, users may have requests that are out of the scope of these <a href=https://en.wikipedia.org/wiki/Application_programming_interface>APIs</a>. This work focuses on identifying such <a href=https://en.wikipedia.org/wiki/User_(computing)>user requests</a>. Existing methods for this task mainly rely on fine-tuning pre-trained models on large annotated data. We propose a novel method, REDE, based on adaptive representation learning and <a href=https://en.wikipedia.org/wiki/Density_estimation>density estimation</a>. REDE can be applied to zero-shot cases, and quickly learns a high-performing detector with only a few shots by updating less than 3 K parameters. We demonstrate <a href=https://en.wikipedia.org/wiki/Rede>REDE</a>&#8217;s competitive performance on DSTC9 data and our newly collected test set.</div></div><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.0/>Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</a></strong><br><a href=/people/b/bonnie-webber/>Bonnie Webber</a>
|
<a href=/people/t/trevor-cohn/>Trevor Cohn</a>
|
<a href=/people/y/yulan-he/>Yulan He</a>
|
<a href=/people/y/yang-liu-icsi/>Yang Liu</a><br><a href=/volumes/2020.emnlp-main/ class=text-muted>Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.0/>Findings of the Association for Computational Linguistics: EMNLP 2020</a></strong><br><a href=/people/t/trevor-cohn/>Trevor Cohn</a>
|
<a href=/people/y/yulan-he/>Yulan He</a>
|
<a href=/people/y/yang-liu-icsi/>Yang Liu</a><br><a href=/volumes/2020.findings-emnlp/ class=text-muted>Findings of the Association for Computational Linguistics: EMNLP 2020</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.inlg-1.46.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--inlg-1--46 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.inlg-1.46 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.inlg-1.46/>Policy-Driven Neural Response Generation for Knowledge-Grounded Dialog Systems</a></strong><br><a href=/people/b/behnam-hedayatnia/>Behnam Hedayatnia</a>
|
<a href=/people/k/karthik-gopalakrishnan/>Karthik Gopalakrishnan</a>
|
<a href=/people/s/seokhwan-kim/>Seokhwan Kim</a>
|
<a href=/people/y/yang-liu-icsi/>Yang Liu</a>
|
<a href=/people/m/mihail-eric/>Mihail Eric</a>
|
<a href=/people/d/dilek-hakkani-tur/>Dilek Hakkani-Tur</a><br><a href=/volumes/2020.inlg-1/ class=text-muted>Proceedings of the 13th International Conference on Natural Language Generation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--inlg-1--46><div class="card-body p-3 small">Open-domain dialog systems aim to generate relevant, informative and engaging responses. In this paper, we propose using a dialog policy to plan the content and style of target, open domain responses in the form of an action plan, which includes knowledge sentences related to the dialog context, targeted dialog acts, topic information, etc. For <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training</a>, the attributes within the <a href=https://en.wikipedia.org/wiki/Action_plan>action plan</a> are obtained by automatically annotating the publicly released Topical-Chat dataset. We condition neural response generators on the <a href=https://en.wikipedia.org/wiki/Action_plan>action plan</a> which is then realized as target utterances at the turn and sentence levels. We also investigate different dialog policy models to predict an <a href=https://en.wikipedia.org/wiki/Action_plan>action plan</a> given the dialog context. Through automated and human evaluation, we measure the appropriateness of the generated responses and check if the generation models indeed learn to realize the given <a href=https://en.wikipedia.org/wiki/Action_plan>action plans</a>. We demonstrate that a basic dialog policy that operates at the sentence level generates better responses in comparison to turn level generation as well as baseline models with no <a href=https://en.wikipedia.org/wiki/Action_plan>action plan</a>. Additionally the basic dialog policy has the added benefit of <a href=https://en.wikipedia.org/wiki/Controllability>controllability</a>.</div></div><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4416.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4416 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4416 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4416/>The LAIX Systems in the BEA-2019 GEC Shared Task<span class=acl-fixed-case>LAIX</span> Systems in the <span class=acl-fixed-case>BEA</span>-2019 <span class=acl-fixed-case>GEC</span> Shared Task</a></strong><br><a href=/people/r/ruobing-li/>Ruobing Li</a>
|
<a href=/people/c/chuan-wang/>Chuan Wang</a>
|
<a href=/people/y/yefei-zha/>Yefei Zha</a>
|
<a href=/people/y/yonghong-yu/>Yonghong Yu</a>
|
<a href=/people/s/shiman-guo/>Shiman Guo</a>
|
<a href=/people/q/qiang-wang/>Qiang Wang</a>
|
<a href=/people/y/yang-liu-icsi/>Yang Liu</a>
|
<a href=/people/h/hui-lin/>Hui Lin</a><br><a href=/volumes/W19-44/ class=text-muted>Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4416><div class="card-body p-3 small">In this paper, we describe two <a href=https://en.wikipedia.org/wiki/System>systems</a> we developed for the three tracks we have participated in the BEA-2019 GEC Shared Task. We investigate competitive classification models with bi-directional recurrent neural networks (Bi-RNN) and neural machine translation (NMT) models. For different tracks, we use <a href=https://en.wikipedia.org/wiki/Ensemble_learning>ensemble systems</a> to selectively combine the NMT models, the classification models, and some rules, and demonstrate that an ensemble solution can effectively improve GEC performance over single systems. Our GEC systems ranked the first in the Unrestricted Track, and the third in both the Restricted Track and the Low Resource Track.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4450.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4450 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4450 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4450/>Automated Essay Scoring with Discourse-Aware Neural Models</a></strong><br><a href=/people/f/farah-nadeem/>Farah Nadeem</a>
|
<a href=/people/h/huy-nguyen/>Huy Nguyen</a>
|
<a href=/people/y/yang-liu-icsi/>Yang Liu</a>
|
<a href=/people/m/mari-ostendorf/>Mari Ostendorf</a><br><a href=/volumes/W19-44/ class=text-muted>Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4450><div class="card-body p-3 small">Automated essay scoring systems typically rely on hand-crafted features to predict essay quality, but such systems are limited by the cost of <a href=https://en.wikipedia.org/wiki/Feature_engineering>feature engineering</a>. Neural networks offer an alternative to <a href=https://en.wikipedia.org/wiki/Feature_engineering>feature engineering</a>, but they typically require more annotated data. This paper explores network structures, contextualized embeddings and pre-training strategies aimed at capturing discourse characteristics of essays. Experiments on three essay scoring tasks show benefits from all three <a href=https://en.wikipedia.org/wiki/Strategy>strategies</a> in different combinations, with simpler <a href=https://en.wikipedia.org/wiki/Computer_architecture>architectures</a> being more effective when less <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training data</a> is available.</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1150.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1150 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1150 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1150/>A Reinforcement Learning Framework for Natural Question Generation using Bi-discriminators</a></strong><br><a href=/people/z/zhihao-fan/>Zhihao Fan</a>
|
<a href=/people/z/zhongyu-wei/>Zhongyu Wei</a>
|
<a href=/people/s/siyuan-wang/>Siyuan Wang</a>
|
<a href=/people/y/yang-liu-icsi/>Yang Liu</a>
|
<a href=/people/x/xuan-jing-huang/>Xuanjing Huang</a><br><a href=/volumes/C18-1/ class=text-muted>Proceedings of the 27th International Conference on Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1150><div class="card-body p-3 small">Visual Question Generation (VQG) aims to ask natural questions about an <a href=https://en.wikipedia.org/wiki/Image>image</a> automatically. Existing research focus on training model to fit the annotated data set that makes it indifferent from other language generation tasks. We argue that natural questions need to have two specific attributes from the perspectives of content and linguistic respectively, namely, natural and human-written. Inspired by the setting of <a href=https://en.wikipedia.org/wiki/Discriminator>discriminator</a> in <a href=https://en.wikipedia.org/wiki/Adversarial_learning>adversarial learning</a>, we propose two discriminators, one for each attribute, to enhance the training. We then use the reinforcement learning framework to incorporate scores from the two discriminators as the reward to guide the training of the question generator. Experimental results on a benchmark VQG dataset show the effectiveness and robustness of our model compared to some state-of-the-art models in terms of both automatic and human evaluation metrics.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-5000.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-5000/>Proceedings of the 2018 Conference of the North <span class=acl-fixed-case>A</span>merican Chapter of the Association for Computational Linguistics: Demonstrations</a></strong><br><a href=/people/y/yang-liu-icsi/>Yang Liu</a>
|
<a href=/people/t/tim-paek/>Tim Paek</a>
|
<a href=/people/m/manasi-patwardhan/>Manasi Patwardhan</a><br><a href=/volumes/N18-5/ class=text-muted>Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Demonstrations</a></span></p><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K17-3015.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K17-3015 data-toggle=collapse aria-expanded=false aria-controls=abstract-K17-3015 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/K17-3015/>A non-DNN Feature Engineering Approach to Dependency Parsing FBAML at CoNLL 2017 Shared Task<span class=acl-fixed-case>DNN</span> Feature Engineering Approach to Dependency Parsing – <span class=acl-fixed-case>FBAML</span> at <span class=acl-fixed-case>C</span>o<span class=acl-fixed-case>NLL</span> 2017 Shared Task</a></strong><br><a href=/people/x/xian-qian/>Xian Qian</a>
|
<a href=/people/y/yang-liu-icsi/>Yang Liu</a><br><a href=/volumes/K17-3/ class=text-muted>Proceedings of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K17-3015><div class="card-body p-3 small">For this year&#8217;s multilingual dependency parsing shared task, we developed a <a href=https://en.wikipedia.org/wiki/Pipeline_(software)>pipeline system</a>, which uses a variety of features for each of its <a href=https://en.wikipedia.org/wiki/Component-based_software_engineering>components</a>. Unlike the recent popular deep learning approaches that learn low dimensional dense features using non-linear classifier, our system uses structured linear classifiers to learn millions of <a href=https://en.wikipedia.org/wiki/Sparse_matrix>sparse features</a>. Specifically, we trained a <a href=https://en.wikipedia.org/wiki/Linear_classifier>linear classifier</a> for sentence boundary prediction, linear chain conditional random fields (CRFs) for <a href=https://en.wikipedia.org/wiki/Lexical_analysis>tokenization</a>, <a href=https://en.wikipedia.org/wiki/Part-of-speech_tagging>part-of-speech tagging</a> and morph analysis. A second order graph based parser learns the tree structure (without relations), and fa linear tree CRF then assigns relations to the dependencies in the <a href=https://en.wikipedia.org/wiki/Tree_(graph_theory)>tree</a>. Our <a href=https://en.wikipedia.org/wiki/System>system</a> achieves reasonable performance 67.87 % official averaged macro F1 score</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Yang+Liu" title="Search for 'Yang Liu' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/t/trevor-cohn/ class=align-middle>Trevor Cohn</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/y/yulan-he/ class=align-middle>Yulan He</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/s/seokhwan-kim/ class=align-middle>Seokhwan Kim</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/d/dilek-hakkani-tur/ class=align-middle>Dilek Hakkani-Tur</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/z/zhihao-fan/ class=align-middle>Zhihao Fan</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/z/zhongyu-wei/ class=align-middle>Zhongyu Wei</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/siyuan-wang/ class=align-middle>Siyuan Wang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/x/xuan-jing-huang/ class=align-middle>Xuan-Jing Huang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/b/bonnie-webber/ class=align-middle>Bonnie Webber</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/x/xian-qian/ class=align-middle>Xian Qian</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/di-jin/ class=align-middle>Di Jin</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/shuyang-gao/ class=align-middle>Shuyang Gao</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/ruobing-li/ class=align-middle>Ruobing Li</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/chuan-wang/ class=align-middle>Chuan Wang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yefei-zha/ class=align-middle>Yefei Zha</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yonghong-yu/ class=align-middle>Yonghong Yu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/shiman-guo/ class=align-middle>Shiman Guo</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/q/qiang-wang/ class=align-middle>Qiang Wang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/hui-lin/ class=align-middle>Hui Lin</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/f/farah-nadeem/ class=align-middle>Farah Nadeem</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/huy-nguyen/ class=align-middle>Huy Nguyen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/mari-ostendorf/ class=align-middle>Mari Ostendorf</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/b/behnam-hedayatnia/ class=align-middle>Behnam Hedayatnia</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/karthik-gopalakrishnan/ class=align-middle>Karthik Gopalakrishnan</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/mihail-eric/ class=align-middle>Mihail Eric</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/tim-paek/ class=align-middle>Tim Paek</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/manasi-patwardhan/ class=align-middle>Manasi Patwardhan</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/ws/ class=align-middle>WS</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/coling/ class=align-middle>COLING</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/emnlp/ class=align-middle>EMNLP</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/conll/ class=align-middle>CoNLL</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/nlp4convai/ class=align-middle>NLP4ConvAI</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-venues aria-expanded=false aria-controls=more-venues>show all...</li><div class="collapse border-top" id=more-venues><li class=list-group-item><a href=/venues/findings/ class=align-middle>Findings</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/inlg/ class=align-middle>INLG</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/naacl/ class=align-middle>NAACL</a><span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>