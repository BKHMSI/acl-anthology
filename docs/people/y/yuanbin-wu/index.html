<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Yuanbin Wu - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Yuanbin</span> <span class=font-weight-bold>Wu</span></h2><hr><div class=row><div class=col-lg-9><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.19.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--19 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.19 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.19" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.19/>UniRE : A Unified Label Space for Entity Relation Extraction<span class=acl-fixed-case>U</span>ni<span class=acl-fixed-case>RE</span>: A Unified Label Space for Entity Relation Extraction</a></strong><br><a href=/people/y/yijun-wang/>Yijun Wang</a>
|
<a href=/people/c/changzhi-sun/>Changzhi Sun</a>
|
<a href=/people/y/yuanbin-wu/>Yuanbin Wu</a>
|
<a href=/people/h/hao-zhou/>Hao Zhou</a>
|
<a href=/people/l/lei-li/>Lei Li</a>
|
<a href=/people/j/junchi-yan/>Junchi Yan</a><br><a href=/volumes/2021.acl-long/ class=text-muted>Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--19><div class="card-body p-3 small">Many joint entity relation extraction models setup two separated label spaces for the two <a href=https://en.wikipedia.org/wiki/Task_(project_management)>sub-tasks</a> (i.e., <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entity detection</a> and relation classification). We argue that this setting may hinder the <a href=https://en.wikipedia.org/wiki/Information_exchange>information interaction</a> between entities and relations. In this work, we propose to eliminate the different treatment on the two sub-tasks&#8217; label spaces. The input of our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is a table containing all word pairs from a sentence. Entities and relations are represented by squares and rectangles in the table. We apply a unified classifier to predict each cell&#8217;s label, which unifies the learning of two sub-tasks. For testing, an effective (yet fast) approximate decoder is proposed for finding squares and rectangles from tables. Experiments on three benchmarks (ACE04, ACE05, SciERC) show that, using only half the number of parameters, our model achieves competitive accuracy with the best extractor, and is faster.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.251.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--251 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.251 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.eacl-main.251" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.251/>ENPAR : Enhancing Entity and Entity Pair Representations for Joint Entity Relation Extraction<span class=acl-fixed-case>ENPAR</span>:Enhancing Entity and Entity Pair Representations for Joint Entity Relation Extraction</a></strong><br><a href=/people/y/yijun-wang/>Yijun Wang</a>
|
<a href=/people/c/changzhi-sun/>Changzhi Sun</a>
|
<a href=/people/y/yuanbin-wu/>Yuanbin Wu</a>
|
<a href=/people/h/hao-zhou/>Hao Zhou</a>
|
<a href=/people/l/lei-li/>Lei Li</a>
|
<a href=/people/j/junchi-yan/>Junchi Yan</a><br><a href=/volumes/2021.eacl-main/ class=text-muted>Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--251><div class="card-body p-3 small">Current state-of-the-art systems for joint entity relation extraction (Luan et al., 2019 ; Wad-den et al., 2019) usually adopt the multi-task learning framework. However, <a href=https://en.wikipedia.org/wiki/Annotation>annotations</a> for these additional tasks such as <a href=https://en.wikipedia.org/wiki/Coreference_resolution>coreference resolution</a> and <a href=https://en.wikipedia.org/wiki/Event_extraction>event extraction</a> are always equally hard (or even harder) to obtain. In this work, we propose a pre-training method ENPAR to improve the joint extraction performance. ENPAR requires only the additional entity annotations that are much easier to collect. Unlike most existing works that only consider incorporating <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entity information</a> into the sentence encoder, we further utilize the <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entity pair information</a>. Specifically, we devise four novel <a href=https://en.wikipedia.org/wiki/Goal>objectives</a>, i.e., masked entity typing, masked entity prediction, adversarial context discrimination, and permutation prediction, to pre-train an <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entity encoder</a> and an <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entity pair encoder</a>. Comprehensive experiments show that the proposed pre-training method achieves significant improvement over BERT on ACE05, SciERC, and NYT, and outperforms current state-of-the-art on ACE05.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.339.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--339 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.339 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.emnlp-main.339.Software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.339/>A Unified Encoding of Structures in Transition Systems</a></strong><br><a href=/people/t/tao-ji/>Tao Ji</a>
|
<a href=/people/y/yong-jiang/>Yong Jiang</a>
|
<a href=/people/t/tao-wang/>Tao Wang</a>
|
<a href=/people/z/zhongqiang-huang/>Zhongqiang Huang</a>
|
<a href=/people/f/fei-huang/>Fei Huang</a>
|
<a href=/people/y/yuanbin-wu/>Yuanbin Wu</a>
|
<a href=/people/x/xiaoling-wang/>Xiaoling Wang</a><br><a href=/volumes/2021.emnlp-main/ class=text-muted>Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--339><div class="card-body p-3 small">Transition systems usually contain various <a href=https://en.wikipedia.org/wiki/Dynamical_system>dynamic structures</a> (e.g., <a href=https://en.wikipedia.org/wiki/Stack_(abstract_data_type)>stacks</a>, buffers). An ideal transition-based model should encode these <a href=https://en.wikipedia.org/wiki/Mathematical_structure>structures</a> completely and efficiently. Previous works relying on <a href=https://en.wikipedia.org/wiki/Template_processor>templates</a> or neural network structures either only encode partial structure information or suffer from computation efficiency. In this paper, we propose a novel attention-based encoder unifying representation of all structures in a <a href=https://en.wikipedia.org/wiki/Transition_system>transition system</a>. Specifically, we separate two views of items on structures, namely structure-invariant view and structure-dependent view. With the help of parallel-friendly attention network, we are able to encoding <a href=https://en.wikipedia.org/wiki/Transition_state>transition states</a> with O(1) additional complexity (with respect to basic feature extractors). Experiments on the PTB and UD show that our proposed method significantly improves the test speed and achieves the best transition-based model, and is comparable to state-of-the-art methods.</div></div><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.acl-main.299.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--acl-main--299 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.acl-main.299 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://slideslive.com/38928752 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.acl-main.299" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.acl-main.299/>A Span-based Linearization for Constituent Trees</a></strong><br><a href=/people/y/yang-wei/>Yang Wei</a>
|
<a href=/people/y/yuanbin-wu/>Yuanbin Wu</a>
|
<a href=/people/m/man-lan/>Man Lan</a><br><a href=/volumes/2020.acl-main/ class=text-muted>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--acl-main--299><div class="card-body p-3 small">We propose a novel linearization of a constituent tree, together with a new locally normalized model. For each split point in a sentence, our model computes the <a href=https://en.wikipedia.org/wiki/Normalization_(statistics)>normalizer</a> on all spans ending with that split point, and then predicts a tree span from them. Compared with global models, our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is fast and parallelizable. Different from previous local models, our linearization method is tied on the spans directly and considers more local features when performing span prediction, which is more interpretable and effective. Experiments on PTB (95.8 F1) and CTB (92.4 F1) show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> significantly outperforms existing local models and efficiently achieves competitive results with global models.</div></div><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1237.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1237 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1237 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1237" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-1237/>Graph-based Dependency Parsing with Graph Neural Networks</a></strong><br><a href=/people/t/tao-ji/>Tao Ji</a>
|
<a href=/people/y/yuanbin-wu/>Yuanbin Wu</a>
|
<a href=/people/m/man-lan/>Man Lan</a><br><a href=/volumes/P19-1/ class=text-muted>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1237><div class="card-body p-3 small">We investigate the problem of efficiently incorporating <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>high-order features</a> into neural graph-based dependency parsing. Instead of explicitly extracting high-order features from intermediate parse trees, we develop a more powerful dependency tree node representation which captures high-order information concisely and efficiently. We use graph neural networks (GNNs) to learn the representations and discuss several new configurations of GNN&#8217;s updating and aggregation functions. Experiments on <a href=https://en.wikipedia.org/wiki/Parsing>PTB</a> show that our <a href=https://en.wikipedia.org/wiki/Parsing>parser</a> achieves the best UAS and LAS on <a href=https://en.wikipedia.org/wiki/Parsing>PTB</a> (96.0 %, 94.3 %) among systems without using any external resources.</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S18-1035.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S18-1035 data-toggle=collapse aria-expanded=false aria-controls=abstract-S18-1035 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/S18-1035/>ECNU at SemEval-2018 Task 1 : Emotion Intensity Prediction Using Effective Features and Machine Learning Models<span class=acl-fixed-case>ECNU</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2018 Task 1: Emotion Intensity Prediction Using Effective Features and Machine Learning Models</a></strong><br><a href=/people/h/huimin-xu/>Huimin Xu</a>
|
<a href=/people/m/man-lan/>Man Lan</a>
|
<a href=/people/y/yuanbin-wu/>Yuanbin Wu</a><br><a href=/volumes/S18-1/ class=text-muted>Proceedings of The 12th International Workshop on Semantic Evaluation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S18-1035><div class="card-body p-3 small">This paper describes our submissions to SemEval 2018 task 1. The <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> is affect intensity prediction in <a href=https://en.wikipedia.org/wiki/Twitter>tweets</a>, including five <a href=https://en.wikipedia.org/wiki/Task_(project_management)>subtasks</a>. We participated in all subtasks of <a href=https://en.wikipedia.org/wiki/Twitter>English tweets</a>. We extracted several traditional <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a>, sentiment lexicon, emotion lexicon and domain specific features from tweets, adopted supervised machine learning algorithms to perform emotion intensity prediction.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S18-1184.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S18-1184 data-toggle=collapse aria-expanded=false aria-controls=abstract-S18-1184 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/S18-1184/>ECNU at SemEval-2018 Task 12 : An End-to-End Attention-based Neural Network for the Argument Reasoning Comprehension Task<span class=acl-fixed-case>ECNU</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2018 Task 12: An End-to-End Attention-based Neural Network for the Argument Reasoning Comprehension Task</a></strong><br><a href=/people/j/junfeng-tian/>Junfeng Tian</a>
|
<a href=/people/m/man-lan/>Man Lan</a>
|
<a href=/people/y/yuanbin-wu/>Yuanbin Wu</a><br><a href=/volumes/S18-1/ class=text-muted>Proceedings of The 12th International Workshop on Semantic Evaluation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S18-1184><div class="card-body p-3 small">This paper presents our submissions to SemEval 2018 Task 12 : the Argument Reasoning Comprehension Task. We investigate an end-to-end attention-based neural network to represent the two lexically close candidate warrants. On the one hand, we extract their different parts as attention vectors to obtain distinguishable representations. On the other hand, we use their surrounds (i.e., claim, reason, debate context) as another <a href=https://en.wikipedia.org/wiki/Attention>attention vectors</a> to get contextual representations, which work as final clues to select the correct warrant. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieves 60.4 % <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> and ranks 3rd among 22 participating systems.</div></div><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K17-3025.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K17-3025 data-toggle=collapse aria-expanded=false aria-controls=abstract-K17-3025 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/K17-3025/>A Fast and Lightweight System for Multilingual Dependency Parsing</a></strong><br><a href=/people/t/tao-ji/>Tao Ji</a>
|
<a href=/people/y/yuanbin-wu/>Yuanbin Wu</a>
|
<a href=/people/m/man-lan/>Man Lan</a><br><a href=/volumes/K17-3/ class=text-muted>Proceedings of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K17-3025><div class="card-body p-3 small">We present a multilingual dependency parser with a bidirectional-LSTM (BiLSTM) feature extractor and a multi-layer perceptron (MLP) classifier. We trained our transition-based projective parser in UD version 2.0 datasets without any additional data. The <a href=https://en.wikipedia.org/wiki/Parsing>parser</a> is fast, lightweight and effective on <a href=https://en.wikipedia.org/wiki/Treebank>big treebanks</a>. In the CoNLL 2017 Shared Task : Multilingual Parsing from Raw Text to Universal Dependencies, the official results show that the macro-averaged LAS F1 score of our system Mengest is 61.33 %.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S17-2028.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S17-2028 data-toggle=collapse aria-expanded=false aria-controls=abstract-S17-2028 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/S17-2028/>ECNU at SemEval-2017 Task 1 : Leverage Kernel-based Traditional NLP features and Neural Networks to Build a Universal Model for Multilingual and Cross-lingual Semantic Textual Similarity<span class=acl-fixed-case>ECNU</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2017 Task 1: Leverage Kernel-based Traditional <span class=acl-fixed-case>NLP</span> features and Neural Networks to Build a Universal Model for Multilingual and Cross-lingual Semantic Textual Similarity</a></strong><br><a href=/people/j/junfeng-tian/>Junfeng Tian</a>
|
<a href=/people/z/zhiheng-zhou/>Zhiheng Zhou</a>
|
<a href=/people/m/man-lan/>Man Lan</a>
|
<a href=/people/y/yuanbin-wu/>Yuanbin Wu</a><br><a href=/volumes/S17-2/ class=text-muted>Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S17-2028><div class="card-body p-3 small">To address semantic similarity on multilingual and cross-lingual sentences, we firstly translate other foreign languages into English, and then feed our monolingual English system with various interactive features. Our system is further supported by combining with deep learning semantic similarity and our best run achieves the mean Pearson correlation 73.16 % in primary track.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S17-2060.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S17-2060 data-toggle=collapse aria-expanded=false aria-controls=abstract-S17-2060 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/S17-2060/>ECNU at SemEval-2017 Task 3 : Using Traditional and Deep Learning Methods to Address Community Question Answering Task<span class=acl-fixed-case>ECNU</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2017 Task 3: Using Traditional and Deep Learning Methods to Address Community Question Answering Task</a></strong><br><a href=/people/g/guoshun-wu/>Guoshun Wu</a>
|
<a href=/people/y/yixuan-sheng/>Yixuan Sheng</a>
|
<a href=/people/m/man-lan/>Man Lan</a>
|
<a href=/people/y/yuanbin-wu/>Yuanbin Wu</a><br><a href=/volumes/S17-2/ class=text-muted>Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S17-2060><div class="card-body p-3 small">This paper describes the <a href=https://en.wikipedia.org/wiki/System>systems</a> we submitted to the task 3 (Community Question Answering) in SemEval 2017 which contains three subtasks on English corpora, i.e., subtask A : Question-Comment Similarity, subtask B : Question-Question Similarity, and subtask C : Question-External Comment Similarity. For subtask A, we combined two different <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> to represent question-comment pair, i.e., <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised model</a> using traditional <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> and <a href=https://en.wikipedia.org/wiki/Convolutional_neural_network>Convolutional Neural Network</a>. For subtask B, we utilized the information of snippets returned from <a href=https://en.wikipedia.org/wiki/Web_search_engine>Search Engine</a> with question subject as query. For subtask C, we ranked the comments by multiplying the probability of the pair related question comment being Good by the reciprocal rank of the related question.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S17-2078.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S17-2078 data-toggle=collapse aria-expanded=false aria-controls=abstract-S17-2078 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/S17-2078/>ECNU at SemEval-2017 Task 7 : Using Supervised and Unsupervised Methods to Detect and Locate English Puns<span class=acl-fixed-case>ECNU</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2017 Task 7: Using Supervised and Unsupervised Methods to Detect and Locate <span class=acl-fixed-case>E</span>nglish Puns</a></strong><br><a href=/people/y/yuhuan-xiu/>Yuhuan Xiu</a>
|
<a href=/people/m/man-lan/>Man Lan</a>
|
<a href=/people/y/yuanbin-wu/>Yuanbin Wu</a><br><a href=/volumes/S17-2/ class=text-muted>Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S17-2078><div class="card-body p-3 small">This paper describes our submissions to task 7 in SemEval 2017, i.e., <a href=https://en.wikipedia.org/wiki/Detection_theory>Detection</a> and <a href=https://en.wikipedia.org/wiki/Interpretation_(linguistics)>Interpretation of English Puns</a>. We participated in the first two subtasks, which are to detect and locate English puns respectively. For subtask 1, we presented a <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised system</a> to determine whether or not a sentence contains a <a href=https://en.wikipedia.org/wiki/Pun>pun</a> using similarity features calculated on sense vectors or cluster center vectors. For subtask 2, we established an <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised system</a> to locate the <a href=https://en.wikipedia.org/wiki/Pun>pun</a> by scoring each word in the sentence and we assumed that the word with the smallest score is the <a href=https://en.wikipedia.org/wiki/Pun>pun</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S17-2086.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S17-2086 data-toggle=collapse aria-expanded=false aria-controls=abstract-S17-2086 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/S17-2086/>ECNU at SemEval-2017 Task 8 : Rumour Evaluation Using Effective Features and Supervised Ensemble Models<span class=acl-fixed-case>ECNU</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2017 Task 8: Rumour Evaluation Using Effective Features and Supervised Ensemble Models</a></strong><br><a href=/people/f/feixiang-wang/>Feixiang Wang</a>
|
<a href=/people/m/man-lan/>Man Lan</a>
|
<a href=/people/y/yuanbin-wu/>Yuanbin Wu</a><br><a href=/volumes/S17-2/ class=text-muted>Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S17-2086><div class="card-body p-3 small">This paper describes our submissions to task 8 in SemEval 2017, i.e., Determining rumour veracity and support for <a href=https://en.wikipedia.org/wiki/Rumor>rumours</a>. Given a rumoured tweet and a lot of reply tweets, the subtask A is to label whether these tweets are support, deny, query or comment, and the subtask B aims to predict the veracity (i.e., true, false, and unverified) with a confidence (in range of 0-1) of the given rumoured tweet. For both subtasks, we adopted supervised machine learning methods, incorporating <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>rich features</a>. Since training data is imbalanced, we specifically designed a two-step classifier to address subtask A.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S17-2137.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S17-2137 data-toggle=collapse aria-expanded=false aria-controls=abstract-S17-2137 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/S17-2137/>ECNU at SemEval-2017 Task 4 : Evaluating Effective Features on Machine Learning Methods for Twitter Message Polarity Classification<span class=acl-fixed-case>ECNU</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2017 Task 4: Evaluating Effective Features on Machine Learning Methods for <span class=acl-fixed-case>T</span>witter Message Polarity Classification</a></strong><br><a href=/people/y/yunxiao-zhou/>Yunxiao Zhou</a>
|
<a href=/people/m/man-lan/>Man Lan</a>
|
<a href=/people/y/yuanbin-wu/>Yuanbin Wu</a><br><a href=/volumes/S17-2/ class=text-muted>Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S17-2137><div class="card-body p-3 small">This paper reports our submission to subtask A of task 4 (Sentiment Analysis in Twitter, SAT) in SemEval 2017, i.e., Message Polarity Classification. We investigated several traditional <a href=https://en.wikipedia.org/wiki/Natural_language_processing>Natural Language Processing (NLP) features</a>, domain specific features and word embedding features together with <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised machine learning methods</a> to address this task. Officially released results showed that our <a href=https://en.wikipedia.org/wiki/System>system</a> ranked above average.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S17-2152.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S17-2152 data-toggle=collapse aria-expanded=false aria-controls=abstract-S17-2152 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/S17-2152/>ECNU at SemEval-2017 Task 5 : An Ensemble of Regression Algorithms with Effective Features for Fine-Grained Sentiment Analysis in Financial Domain<span class=acl-fixed-case>ECNU</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2017 Task 5: An Ensemble of Regression Algorithms with Effective Features for Fine-Grained Sentiment Analysis in Financial Domain</a></strong><br><a href=/people/m/mengxiao-jiang/>Mengxiao Jiang</a>
|
<a href=/people/m/man-lan/>Man Lan</a>
|
<a href=/people/y/yuanbin-wu/>Yuanbin Wu</a><br><a href=/volumes/S17-2/ class=text-muted>Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S17-2152><div class="card-body p-3 small">This paper describes our systems submitted to the Fine-Grained Sentiment Analysis on Financial Microblogs and News task (i.e., Task 5) in SemEval-2017. This <a href=https://en.wikipedia.org/wiki/Task_(computing)>task</a> includes two subtasks in <a href=https://en.wikipedia.org/wiki/Microblogging>microblogs</a> and news headline domain respectively. To settle this problem, we extract four types of effective <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a>, including linguistic features, sentiment lexicon features, domain-specific features and word embedding features. Then we employ these <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> to construct <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> by using ensemble regression algorithms. Our submissions rank 1st and rank 5th in subtask 1 and subtask 2 respectively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1134.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1134 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1134 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1134/>Multi-task Attention-based Neural Networks for Implicit Discourse Relationship Representation and Identification</a></strong><br><a href=/people/m/man-lan/>Man Lan</a>
|
<a href=/people/j/jianxiang-wang/>Jianxiang Wang</a>
|
<a href=/people/y/yuanbin-wu/>Yuanbin Wu</a>
|
<a href=/people/z/zheng-yu-niu/>Zheng-Yu Niu</a>
|
<a href=/people/h/haifeng-wang/>Haifeng Wang</a><br><a href=/volumes/D17-1/ class=text-muted>Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1134><div class="card-body p-3 small">We present a novel multi-task attention based neural network model to address implicit discourse relationship representation and identification through two types of <a href=https://en.wikipedia.org/wiki/Representation_learning>representation learning</a>, an attention based neural network for learning discourse relationship representation with two arguments and a multi-task framework for learning knowledge from annotated and unannotated corpora. The extensive experiments have been performed on two benchmark corpora (i.e., PDTB and CoNLL-2016 datasets). Experimental results show that our proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms the state-of-the-art systems on benchmark corpora.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1097.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1097 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1097 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1097/>Large-scale Opinion Relation Extraction with Distantly Supervised Neural Network</a></strong><br><a href=/people/c/changzhi-sun/>Changzhi Sun</a>
|
<a href=/people/y/yuanbin-wu/>Yuanbin Wu</a>
|
<a href=/people/m/man-lan/>Man Lan</a>
|
<a href=/people/s/shiliang-sun/>Shiliang Sun</a>
|
<a href=/people/q/qi-zhang/>Qi Zhang</a><br><a href=/volumes/E17-1/ class=text-muted>Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1097><div class="card-body p-3 small">We investigate the task of open domain opinion relation extraction. Different from works on manually labeled corpus, we propose an efficient distantly supervised framework based on <a href=https://en.wikipedia.org/wiki/Pattern_matching>pattern matching</a> and neural network classifiers. The <a href=https://en.wikipedia.org/wiki/Pattern_recognition>patterns</a> are designed to automatically generate training data, and the <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning model</a> is design to capture various lexical and syntactic features. The result <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> is fast and scalable on large-scale corpus. We test the <a href=https://en.wikipedia.org/wiki/System>system</a> on the <a href=https://en.wikipedia.org/wiki/Amazon_(company)>Amazon online review dataset</a>. The result shows that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is able to achieve promising performances without any human annotations.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Yuanbin+Wu" title="Search for 'Yuanbin Wu' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/m/man-lan/ class=align-middle>Man Lan</a>
<span class="badge badge-secondary align-middle ml-2">13</span></li><li class=list-group-item><a href=/people/c/changzhi-sun/ class=align-middle>Changzhi Sun</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/t/tao-ji/ class=align-middle>Tao Ji</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/y/yijun-wang/ class=align-middle>Yijun Wang</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/h/hao-zhou/ class=align-middle>Hao Zhou</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/l/lei-li/ class=align-middle>Lei Li</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/j/junchi-yan/ class=align-middle>Junchi Yan</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/j/junfeng-tian/ class=align-middle>Junfeng Tian</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/y/yang-wei/ class=align-middle>Yang Wei</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yong-jiang/ class=align-middle>Yong Jiang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/tao-wang/ class=align-middle>Tao Wang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/zhongqiang-huang/ class=align-middle>Zhongqiang Huang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/f/fei-huang/ class=align-middle>Fei Huang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/x/xiaoling-wang/ class=align-middle>Xiaoling Wang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/zhiheng-zhou/ class=align-middle>Zhiheng Zhou</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/g/guoshun-wu/ class=align-middle>Guoshun Wu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yixuan-sheng/ class=align-middle>Yixuan Sheng</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yuhuan-xiu/ class=align-middle>Yuhuan Xiu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/f/feixiang-wang/ class=align-middle>Feixiang Wang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yunxiao-zhou/ class=align-middle>Yunxiao Zhou</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/mengxiao-jiang/ class=align-middle>Mengxiao Jiang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jianxiang-wang/ class=align-middle>JianXiang Wang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/zheng-yu-niu/ class=align-middle>Zheng-Yu Niu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/haifeng-wang/ class=align-middle>Haifeng Wang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/huimin-xu/ class=align-middle>Huimin Xu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/shiliang-sun/ class=align-middle>Shiliang Sun</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/q/qi-zhang/ class=align-middle>Qi Zhang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/semeval/ class=align-middle>SemEval</a><span class="badge badge-secondary align-middle ml-2">8</span></li><li class=list-group-item><a href=/venues/acl/ class=align-middle>ACL</a><span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/venues/eacl/ class=align-middle>EACL</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/emnlp/ class=align-middle>EMNLP</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/conll/ class=align-middle>CoNLL</a><span class="badge badge-secondary align-middle ml-2">1</span></li></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>