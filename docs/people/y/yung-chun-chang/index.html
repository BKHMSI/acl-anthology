<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Yung-Chun Chang - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Yung-Chun</span> <span class=font-weight-bold>Chang</span></h2><hr><div class=row><div class=col-lg-9><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.rocling-1.27.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--rocling-1--27 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.rocling-1.27 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.rocling-1.27/>Using Valence and Arousal-infused Bi-LSTM for <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>Sentiment Analysis</a> in Social Media Product Reviews<span class=acl-fixed-case>B</span>i-<span class=acl-fixed-case>LSTM</span> for Sentiment Analysis in Social Media Product Reviews</a></strong><br><a href=/people/y/yu-ya-cheng/>Yu-Ya Cheng</a>
|
<a href=/people/w/wen-chao-yeh/>Wen-Chao Yeh</a>
|
<a href=/people/y/yan-ming-chen/>Yan-Ming Chen</a>
|
<a href=/people/y/yung-chun-chang/>Yung-Chun Chang</a><br><a href=/volumes/2021.rocling-1/ class=text-muted>Proceedings of the 33rd Conference on Computational Linguistics and Speech Processing (ROCLING 2021)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--rocling-1--27><div class="card-body p-3 small">With the popularity of the current Internet age, online social platforms have provided a bridge for communication between private companies, public organizations, and the public. The purpose of this research is to understand the user&#8217;s experience of the product by analyzing product review data in different fields. We propose a BiLSTM-based neural network which infused rich emotional information. In addition to consider <a href=https://en.wikipedia.org/wiki/Valence_(psychology)>Valence</a> and Arousal which is the smallest morpheme of emotional information, the dependence relationship between texts is also integrated into the deep learning model to analyze the <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment</a>. The experimental results show that this <a href=https://en.wikipedia.org/wiki/Research>research</a> can achieve good performance in predicting the vocabulary Valence and Arousal. In addition, the integration of VA and dependency information into the BiLSTM model can have excellent performance for social text sentiment analysis, which verifies that this <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> is effective in emotion recognition of social medial short text.</div></div><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-2014.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-2014 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-2014 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-2014/>MONPA : Multi-objective Named-entity and Part-of-speech Annotator for Chinese using <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>Recurrent Neural Network</a><span class=acl-fixed-case>MONPA</span>: Multi-objective Named-entity and Part-of-speech Annotator for <span class=acl-fixed-case>C</span>hinese using Recurrent Neural Network</a></strong><br><a href=/people/y/yu-lun-hsieh/>Yu-Lun Hsieh</a>
|
<a href=/people/y/yung-chun-chang/>Yung-Chun Chang</a>
|
<a href=/people/y/yi-jie-huang/>Yi-Jie Huang</a>
|
<a href=/people/s/shu-hao-yeh/>Shu-Hao Yeh</a>
|
<a href=/people/c/chun-hung-chen/>Chun-Hung Chen</a>
|
<a href=/people/w/wen-lian-hsu/>Wen-Lian Hsu</a><br><a href=/volumes/I17-2/ class=text-muted>Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-2014><div class="card-body p-3 small">Part-of-speech (POS) tagging and named entity recognition (NER) are crucial steps in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a>. In addition, the difficulty of <a href=https://en.wikipedia.org/wiki/Word_segmentation>word segmentation</a> places additional burden on those who intend to deal with languages such as <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a>, and pipelined systems often suffer from <a href=https://en.wikipedia.org/wiki/Propagation_of_error>error propagation</a>. This work proposes an end-to-end model using character-based recurrent neural network (RNN) to jointly accomplish segmentation, POS tagging and NER of a Chinese sentence. Experiments on previous word segmentation and NER datasets show that a single model with the proposed architecture is comparable to those trained specifically for each task, and outperforms freely-available softwares. Moreover, we provide a web-based interface for the public to easily access this resource.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-2041.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-2041 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-2041 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-2041/>Identifying Protein-protein Interactions in Biomedical Literature using Recurrent Neural Networks with Long Short-Term Memory</a></strong><br><a href=/people/y/yu-lun-hsieh/>Yu-Lun Hsieh</a>
|
<a href=/people/y/yung-chun-chang/>Yung-Chun Chang</a>
|
<a href=/people/n/nai-wen-chang/>Nai-Wen Chang</a>
|
<a href=/people/w/wen-lian-hsu/>Wen-Lian Hsu</a><br><a href=/volumes/I17-2/ class=text-muted>Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-2041><div class="card-body p-3 small">In this paper, we propose a <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent neural network model</a> for identifying protein-protein interactions in <a href=https://en.wikipedia.org/wiki/Medical_literature>biomedical literature</a>. Experiments on two largest public benchmark datasets, AIMed and BioInfer, demonstrate that our approach significantly surpasses state-of-the-art methods with relative improvements of 10 % and 18 %, respectively. Cross-corpus evaluation also demonstrate that the proposed <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> remains robust despite using different training data. These results suggest that RNN can effectively capture semantic relationships among proteins as well as generalizes over different corpora, without any <a href=https://en.wikipedia.org/wiki/Feature_engineering>feature engineering</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-4015.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-4015 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-4015 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-4015/>CIAL at IJCNLP-2017 Task 2 : An Ensemble Valence-Arousal Analysis System for Chinese Words and Phrases<span class=acl-fixed-case>CIAL</span> at <span class=acl-fixed-case>IJCNLP</span>-2017 Task 2: An Ensemble Valence-Arousal Analysis System for <span class=acl-fixed-case>C</span>hinese Words and Phrases</a></strong><br><a href=/people/z/zheng-wen-lin/>Zheng-Wen Lin</a>
|
<a href=/people/y/yung-chun-chang/>Yung-Chun Chang</a>
|
<a href=/people/c/chen-ann-wang/>Chen-Ann Wang</a>
|
<a href=/people/y/yu-lun-hsieh/>Yu-Lun Hsieh</a>
|
<a href=/people/w/wen-lian-hsu/>Wen-Lian Hsu</a><br><a href=/volumes/I17-4/ class=text-muted>Proceedings of the IJCNLP 2017, Shared Tasks</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-4015><div class="card-body p-3 small">Sentiment lexicon is very helpful in dimensional sentiment applications. Because of countless <a href=https://en.wikipedia.org/wiki/List_of_Chinese_words_of_English_origin>Chinese words</a>, developing a method to predict unseen <a href=https://en.wikipedia.org/wiki/List_of_Chinese_words_of_English_origin>Chinese words</a> is required. The proposed method can handle both words and phrases by using an ADVWeight List for <a href=https://en.wikipedia.org/wiki/Word_prediction>word prediction</a>, which in turn improves our performance at phrase level. The evaluation results demonstrate that our <a href=https://en.wikipedia.org/wiki/System>system</a> is effective in dimensional sentiment analysis for Chinese phrases. The Mean Absolute Error (MAE) and Pearson&#8217;s Correlation Coefficient (PCC) for <a href=https://en.wikipedia.org/wiki/Valence_(psychology)>Valence</a> are 0.723 and 0.835, respectively, and those for <a href=https://en.wikipedia.org/wiki/Arousal>Arousal</a> are 0.914 and 0.756, respectively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5800.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5800/>Proceedings of the International Workshop on Digital Disease Detection using Social Media 2017 (<span class=acl-fixed-case>DDDSM</span>-2017)</a></strong><br><a href=/people/j/jitendra-jonnagaddala/>Jitendra Jonnagaddala</a>
|
<a href=/people/h/hong-jie-dai/>Hong-Jie Dai</a>
|
<a href=/people/y/yung-chun-chang/>Yung-Chun Chang</a><br><a href=/volumes/W17-58/ class=text-muted>Proceedings of the International Workshop on Digital Disease Detection using Social Media 2017 (DDDSM-2017)</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5809.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5809 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5809 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5809/>Chemical-Induced Disease Detection Using Invariance-based Pattern Learning Model</a></strong><br><a href=/people/n/neha-warikoo/>Neha Warikoo</a>
|
<a href=/people/y/yung-chun-chang/>Yung-Chun Chang</a>
|
<a href=/people/w/wen-lian-hsu/>Wen-Lian Hsu</a><br><a href=/volumes/W17-58/ class=text-muted>Proceedings of the International Workshop on Digital Disease Detection using Social Media 2017 (DDDSM-2017)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5809><div class="card-body p-3 small">In this work, we introduce a novel feature engineering approach named algebraic invariance to identify discriminative patterns for learning relation pair features for the chemical-disease relation (CDR) task of BioCreative V. Our method exploits the existing structural similarity of the key concepts of relation descriptions from the CDR corpus to generate robust linguistic patterns for SVM tree kernel-based learning. Preprocessing of the training data classifies the entity pairs as either related or unrelated to build instance types for both inter-sentential and intra-sentential scenarios. An <a href=https://en.wikipedia.org/wiki/Invariant_(mathematics)>invariant function</a> is proposed to process and optimally cluster similar patterns for both positive and negative instances. The learning model for CDR pairs is based on the SVM tree kernel approach, which generates feature trees and vectors and is modeled on suitable invariance based patterns, bringing brevity, precision and context to the identifier features. Results demonstrate that our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> outperformed other compared approaches, achieved a high <a href=https://en.wikipedia.org/wiki/Recall_(memory)>recall rate</a> of 85.08 %, and averaged an F1-score of 54.34 % without the use of any additional <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge bases</a>.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Yung-Chun+Chang" title="Search for 'Yung-Chun Chang' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/w/wen-lian-hsu/ class=align-middle>Wen-Lian Hsu</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/people/y/yu-lun-hsieh/ class=align-middle>Yu-Lun Hsieh</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/y/yi-jie-huang/ class=align-middle>Yi-Jie Huang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/shu-hao-yeh/ class=align-middle>Shu-Hao Yeh</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/chun-hung-chen/ class=align-middle>Chun-Hung Chen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/n/nai-wen-chang/ class=align-middle>Nai-Wen Chang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/zheng-wen-lin/ class=align-middle>Zheng-Wen Lin</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/chen-ann-wang/ class=align-middle>Chen-Ann Wang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jitendra-jonnagaddala/ class=align-middle>Jitendra Jonnagaddala</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/hong-jie-dai/ class=align-middle>Hong-Jie Dai</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/n/neha-warikoo/ class=align-middle>Neha Warikoo</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yu-ya-cheng/ class=align-middle>Yu-Ya Cheng</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/w/wen-chao-yeh/ class=align-middle>Wen-Chao Yeh</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yan-ming-chen/ class=align-middle>Yan-Ming Chen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/ijcnlp/ class=align-middle>IJCNLP</a><span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/venues/ws/ class=align-middle>WS</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/rocling/ class=align-middle>ROCLING</a><span class="badge badge-secondary align-middle ml-2">1</span></li></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>