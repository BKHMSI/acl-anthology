<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Yulia Tsvetkov - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Yulia</span> <span class=font-weight-bold>Tsvetkov</span></h2><hr><div class=row><div class=col-lg-9><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.204.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--204 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.204 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.eacl-main.204" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.204/>Cross-Cultural Similarity Features for Cross-Lingual Transfer Learning of Pragmatically Motivated Tasks</a></strong><br><a href=/people/j/jimin-sun/>Jimin Sun</a>
|
<a href=/people/h/hwijeen-ahn/>Hwijeen Ahn</a>
|
<a href=/people/c/chan-young-park/>Chan Young Park</a>
|
<a href=/people/y/yulia-tsvetkov/>Yulia Tsvetkov</a>
|
<a href=/people/d/david-r-mortensen/>David R. Mortensen</a><br><a href=/volumes/2021.eacl-main/ class=text-muted>Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--204><div class="card-body p-3 small">Much work in cross-lingual transfer learning explored how to select better transfer languages for multilingual tasks, primarily focusing on typological and genealogical similarities between languages. We hypothesize that these measures of linguistic proximity are not enough when working with pragmatically-motivated tasks, such as <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a>. As an alternative, we introduce three linguistic features that capture cross-cultural similarities that manifest in linguistic patterns and quantify distinct aspects of <a href=https://en.wikipedia.org/wiki/Pragmatics>language pragmatics</a> : <a href=https://en.wikipedia.org/wiki/Context_(language_use)>language context-level</a>, <a href=https://en.wikipedia.org/wiki/Literal_and_figurative_language>figurative language</a>, and the lexification of emotion concepts. Our analyses show that the proposed pragmatic features do capture cross-cultural similarities and align well with existing work in <a href=https://en.wikipedia.org/wiki/Sociolinguistics>sociolinguistics</a> and <a href=https://en.wikipedia.org/wiki/Linguistic_anthropology>linguistic anthropology</a>. We further corroborate the effectiveness of pragmatically-driven transfer in the downstream task of choosing transfer languages for cross-lingual sentiment analysis.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.220.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--220 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.220 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.eacl-main.220" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.220/>StructSum : <a href=https://en.wikipedia.org/wiki/Summarization>Summarization</a> via Structured Representations<span class=acl-fixed-case>S</span>truct<span class=acl-fixed-case>S</span>um: Summarization via Structured Representations</a></strong><br><a href=/people/v/vidhisha-balachandran/>Vidhisha Balachandran</a>
|
<a href=/people/a/artidoro-pagnoni/>Artidoro Pagnoni</a>
|
<a href=/people/j/jay-yoon-lee/>Jay Yoon Lee</a>
|
<a href=/people/d/dheeraj-rajagopal/>Dheeraj Rajagopal</a>
|
<a href=/people/j/jaime-g-carbonell/>Jaime Carbonell</a>
|
<a href=/people/y/yulia-tsvetkov/>Yulia Tsvetkov</a><br><a href=/volumes/2021.eacl-main/ class=text-muted>Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--220><div class="card-body p-3 small">Abstractive text summarization aims at compressing the information of a long source document into a rephrased, condensed summary. Despite advances in modeling techniques, abstractive summarization models still suffer from several key challenges : (i) layout bias : they overfit to the style of training corpora ; (ii) limited abstractiveness : they are optimized to copying <a href=https://en.wikipedia.org/wiki/N-gram>n-grams</a> from the source rather than generating novel abstractive summaries ; (iii) lack of transparency : they are not interpretable. In this work, we propose a <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> based on document-level structure induction for summarization to address these challenges. To this end, we propose incorporating latent and explicit dependencies across sentences in the source document into end-to-end single-document summarization models. Our framework complements standard encoder-decoder summarization models by augmenting them with rich structure-aware document representations based on implicitly learned (latent) structures and externally-derived linguistic (explicit) structures. We show that our summarization framework, trained on the CNN / DM dataset, improves the coverage of content in the source documents, generates more abstractive summaries by generating more novel <a href=https://en.wikipedia.org/wiki/N-gram>n-grams</a>, and incorporates interpretable sentence-level structures, while performing on par with standard baselines.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.570.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--570 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.570 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.570" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.570/>Evaluating the Morphosyntactic Well-formedness of Generated Texts</a></strong><br><a href=/people/a/adithya-pratapa/>Adithya Pratapa</a>
|
<a href=/people/a/antonios-anastasopoulos/>Antonios Anastasopoulos</a>
|
<a href=/people/s/shruti-rijhwani/>Shruti Rijhwani</a>
|
<a href=/people/a/aditi-chaudhary/>Aditi Chaudhary</a>
|
<a href=/people/d/david-r-mortensen/>David R. Mortensen</a>
|
<a href=/people/g/graham-neubig/>Graham Neubig</a>
|
<a href=/people/y/yulia-tsvetkov/>Yulia Tsvetkov</a><br><a href=/volumes/2021.emnlp-main/ class=text-muted>Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--570><div class="card-body p-3 small">Text generation systems are ubiquitous in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing applications</a>. However, evaluation of these <a href=https://en.wikipedia.org/wiki/System>systems</a> remains a challenge, especially in multilingual settings. In this paper, we propose L&#8217;AMBRE a <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metric</a> to evaluate the morphosyntactic well-formedness of text using its dependency parse and morphosyntactic rules of the language. We present a way to automatically extract various rules governing morphosyntax directly from dependency treebanks. To tackle the noisy outputs from text generation systems, we propose a simple <a href=https://en.wikipedia.org/wiki/Methodology>methodology</a> to train robust parsers. We show the effectiveness of our <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metric</a> on the task of <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> through a diachronic study of systems translating into morphologically-rich languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.383.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--383 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.383 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.naacl-main.383" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.383/>Understanding Factuality in Abstractive Summarization with FRANK : A Benchmark for Factuality Metrics<span class=acl-fixed-case>FRANK</span>: A Benchmark for Factuality Metrics</a></strong><br><a href=/people/a/artidoro-pagnoni/>Artidoro Pagnoni</a>
|
<a href=/people/v/vidhisha-balachandran/>Vidhisha Balachandran</a>
|
<a href=/people/y/yulia-tsvetkov/>Yulia Tsvetkov</a><br><a href=/volumes/2021.naacl-main/ class=text-muted>Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--383><div class="card-body p-3 small">Modern <a href=https://en.wikipedia.org/wiki/Automatic_summarization>summarization models</a> generate highly fluent but often factually unreliable outputs. This motivated a surge of <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> attempting to measure the <a href=https://en.wikipedia.org/wiki/Fact>factuality</a> of automatically generated summaries. Due to the lack of common benchmarks, these <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> can not be compared. Moreover, all these methods treat <a href=https://en.wikipedia.org/wiki/Factuality>factuality</a> as a binary concept and fail to provide deeper insights on the kinds of inconsistencies made by different systems. To address these limitations, we devise a typology of factual errors and use it to collect human annotations of generated summaries from state-of-the-art summarization systems for the CNN / DM and XSum datasets. Through these annotations we identify the proportion of different categories of factual errors and benchmark factuality metrics, showing their correlation with human judgement as well as their specific strengths and weaknesses.</div></div><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.44.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--44 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.44 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938782 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.44" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.44/>Unsupervised Discovery of Implicit Gender Bias</a></strong><br><a href=/people/a/anjalie-field/>Anjalie Field</a>
|
<a href=/people/y/yulia-tsvetkov/>Yulia Tsvetkov</a><br><a href=/volumes/2020.emnlp-main/ class=text-muted>Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--44><div class="card-body p-3 small">Despite their prevalence in society, social biases are difficult to identify, primarily because <a href=https://en.wikipedia.org/wiki/Judgement>human judgements</a> in this domain can be unreliable. We take an <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised approach</a> to identifying gender bias against women at a comment level and present a model that can surface text likely to contain <a href=https://en.wikipedia.org/wiki/Bias>bias</a>. Our main challenge is forcing the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> to focus on signs of <a href=https://en.wikipedia.org/wiki/Implicit_stereotype>implicit bias</a>, rather than other artifacts in the data. Thus, our <a href=https://en.wikipedia.org/wiki/Methodology>methodology</a> involves reducing the influence of <a href=https://en.wikipedia.org/wiki/Confounding>confounds</a> through propensity matching and <a href=https://en.wikipedia.org/wiki/Adversarial_learning>adversarial learning</a>. Our analysis shows how biased comments directed towards female politicians contain mixed criticisms, while comments directed towards other female public figures focus on <a href=https://en.wikipedia.org/wiki/Human_physical_appearance>appearance</a> and <a href=https://en.wikipedia.org/wiki/Sexualization>sexualization</a>. Ultimately, our work offers a way to capture subtle biases in various domains without relying on subjective human judgements.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.422.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--422 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.422 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939038 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.422" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.422/>Automatic Extraction of <a href=https://en.wikipedia.org/wiki/Rule_of_inference>Rules</a> Governing Morphological Agreement</a></strong><br><a href=/people/a/aditi-chaudhary/>Aditi Chaudhary</a>
|
<a href=/people/a/antonios-anastasopoulos/>Antonios Anastasopoulos</a>
|
<a href=/people/a/adithya-pratapa/>Adithya Pratapa</a>
|
<a href=/people/d/david-r-mortensen/>David R. Mortensen</a>
|
<a href=/people/z/zaid-sheikh/>Zaid Sheikh</a>
|
<a href=/people/y/yulia-tsvetkov/>Yulia Tsvetkov</a>
|
<a href=/people/g/graham-neubig/>Graham Neubig</a><br><a href=/volumes/2020.emnlp-main/ class=text-muted>Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--422><div class="card-body p-3 small">Creating a <a href=https://en.wikipedia.org/wiki/Descriptive_grammar>descriptive grammar</a> of a language is an indispensable step for language documentation and preservation. However, at the same time it is a tedious, time-consuming task. In this paper, we take steps towards automating this process by devising an automated framework for extracting a first-pass grammatical specification from raw text in a concise, human- and machine-readable format. We focus on extracting rules describing <a href=https://en.wikipedia.org/wiki/Agreement_(linguistics)>agreement</a>, a morphosyntactic phenomenon at the core of the <a href=https://en.wikipedia.org/wiki/Grammar>grammars</a> of many of the world&#8217;s languages. We apply our <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> to all languages included in the Universal Dependencies project, with promising results. Using cross-lingual transfer, even with no expert annotations in the language of interest, our framework extracts a grammatical specification which is nearly equivalent to those created with large amounts of gold-standard annotated data. We confirm this finding with human expert evaluations of the <a href=https://en.wikipedia.org/wiki/Rule_of_inference>rules</a> that our <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> produces, which have an average <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of 78 %. We release an interface demonstrating the extracted <a href=https://en.wikipedia.org/wiki/Rule-based_programming>rules</a> at https://neulab.github.io/lase/</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.acl-main.492.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--acl-main--492 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.acl-main.492 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://slideslive.com/38929233 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.acl-main.492" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.acl-main.492/>Explaining Black Box Predictions and Unveiling Data Artifacts through Influence Functions</a></strong><br><a href=/people/x/xiaochuang-han/>Xiaochuang Han</a>
|
<a href=/people/b/byron-c-wallace/>Byron C. Wallace</a>
|
<a href=/people/y/yulia-tsvetkov/>Yulia Tsvetkov</a><br><a href=/volumes/2020.acl-main/ class=text-muted>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--acl-main--492><div class="card-body p-3 small">Modern deep learning models for <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> are notoriously opaque. This has motivated the development of methods for interpreting such <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a>, e.g., via gradient-based saliency maps or the visualization of attention weights. Such approaches aim to provide explanations for a particular model prediction by highlighting important words in the corresponding input text. While this might be useful for tasks where decisions are explicitly influenced by individual tokens in the input, we suspect that such highlighting is not suitable for tasks where model decisions should be driven by more complex reasoning. In this work, we investigate the use of influence functions for <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a>, providing an alternative approach to interpreting neural text classifiers. Influence functions explain the decisions of a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> by identifying influential training examples. Despite the promise of this approach, influence functions have not yet been extensively evaluated in the context of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a>, a gap addressed by this work. We conduct a comparison between <a href=https://en.wikipedia.org/wiki/Influence_function>influence functions</a> and common word-saliency methods on representative tasks. As suspected, we find that influence functions are particularly useful for <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language inference</a>, a task in which &#8216;saliency maps&#8217; may not have clear interpretation. Furthermore, we develop a new quantitative measure based on <a href=https://en.wikipedia.org/wiki/Correlation_and_dependence>influence functions</a> that can reveal artifacts in training data.</div></div><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1176.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1176 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1176 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1176.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D19-1176/>Finding Microaggressions in the Wild : A Case for Locating Elusive Phenomena in Social Media Posts</a></strong><br><a href=/people/l/luke-breitfeller/>Luke Breitfeller</a>
|
<a href=/people/e/emily-ahn/>Emily Ahn</a>
|
<a href=/people/d/david-jurgens/>David Jurgens</a>
|
<a href=/people/y/yulia-tsvetkov/>Yulia Tsvetkov</a><br><a href=/volumes/D19-1/ class=text-muted>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1176><div class="card-body p-3 small">Microaggressions are subtle, often veiled, manifestations of <a href=https://en.wikipedia.org/wiki/Bias>human biases</a>. These uncivil interactions can have a powerful negative impact on people by marginalizing minorities and disadvantaged groups. The linguistic subtlety of <a href=https://en.wikipedia.org/wiki/Microaggression>microaggressions</a> in <a href=https://en.wikipedia.org/wiki/Communication>communication</a> has made it difficult for researchers to analyze their exact nature, and to quantify and extract <a href=https://en.wikipedia.org/wiki/Microaggression>microaggressions</a> automatically. Specifically, the lack of a corpus of real-world microaggressions and objective criteria for annotating them have prevented researchers from addressing these problems at scale. In this paper, we devise a general but nuanced, computationally operationalizable typology of <a href=https://en.wikipedia.org/wiki/Microaggression>microaggressions</a> based on a small subset of data that we have. We then create two <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> : one with examples of diverse types of <a href=https://en.wikipedia.org/wiki/Microaggression>microaggressions</a> recollected by their targets, and another with gender-based microaggressions in public conversations on <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>. We introduce a new, more objective, criterion for <a href=https://en.wikipedia.org/wiki/Annotation>annotation</a> and an active-learning based procedure that increases the likelihood of surfacing posts containing <a href=https://en.wikipedia.org/wiki/Microaggression>microaggressions</a>. Finally, we analyze the trends that emerge from these new <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1425.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1425 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1425 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1425" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1425/>Topics to Avoid : Demoting Latent Confounds in <a href=https://en.wikipedia.org/wiki/Text_classification>Text Classification</a></a></strong><br><a href=/people/s/sachin-kumar/>Sachin Kumar</a>
|
<a href=/people/s/shuly-wintner/>Shuly Wintner</a>
|
<a href=/people/n/noah-a-smith/>Noah A. Smith</a>
|
<a href=/people/y/yulia-tsvetkov/>Yulia Tsvetkov</a><br><a href=/volumes/D19-1/ class=text-muted>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1425><div class="card-body p-3 small">Despite impressive performance on many text classification tasks, <a href=https://en.wikipedia.org/wiki/Deep_learning>deep neural networks</a> tend to learn frequent superficial patterns that are specific to the training data and do not always generalize well. In this work, we observe this limitation with respect to the task of native language identification. We find that standard text classifiers which perform well on the test set end up learning topical features which are confounds of the <a href=https://en.wikipedia.org/wiki/Prediction>prediction task</a> (e.g., if the input text mentions Sweden, the <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifier</a> predicts that the author&#8217;s native language is Swedish). We propose a method that represents the latent topical confounds and a model which unlearns confounding features by predicting both the label of the input text and the confound ; but we train the two predictors adversarially in an alternating fashion to learn a text representation that predicts the correct label but is less prone to using information about the confound. We show that this <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> generalizes better and learns features that are indicative of the writing style rather than the content.<i>native language identification</i>. We find that standard text classifiers which perform well on the test set end up learning topical features which are confounds of the prediction task (e.g., if the input text mentions Sweden, the classifier predicts that the author&#8217;s native language is Swedish). We propose a method that represents the latent topical confounds and a model which &#8220;unlearns&#8221; confounding features by predicting both the label of the input text and the confound; but we train the two predictors adversarially in an alternating fashion to learn a text representation that predicts the correct label but is less prone to using information about the confound. We show that this model generalizes better and learns features that are indicative of the writing style rather than the content.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5621.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5621 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5621 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5621/>A Margin-based Loss with Synthetic Negative Samples for Continuous-output Machine Translation</a></strong><br><a href=/people/g/gayatri-bhat/>Gayatri Bhat</a>
|
<a href=/people/s/sachin-kumar/>Sachin Kumar</a>
|
<a href=/people/y/yulia-tsvetkov/>Yulia Tsvetkov</a><br><a href=/volumes/D19-56/ class=text-muted>Proceedings of the 3rd Workshop on Neural Generation and Translation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5621><div class="card-body p-3 small">Neural models that eliminate the softmax bottleneck by generating word embeddings (rather than <a href=https://en.wikipedia.org/wiki/Multinomial_distribution>multinomial distributions</a> over a vocabulary) attain faster training with fewer learnable parameters. These models are currently trained by maximizing densities of pretrained target embeddings under von Mises-Fisher distributions parameterized by corresponding model-predicted embeddings. This work explores the utility of margin-based loss functions in optimizing such <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>. We present syn-margin loss, a novel margin-based loss that uses a synthetic negative sample constructed from only the predicted and target embeddings at every step. The <a href=https://en.wikipedia.org/wiki/Profit_(accounting)>loss</a> is efficient to compute, and we use a <a href=https://en.wikipedia.org/wiki/Geometric_analysis>geometric analysis</a> to argue that it is more consistent and interpretable than other margin-based losses. Empirically, we find that syn-margin provides small but significant improvements over both vMF and standard margin-based losses in continuous-output neural machine translation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4208.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4208 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4208 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-4208" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-4208/>CMU-01 at the SIGMORPHON 2019 Shared Task on Crosslinguality and Context in Morphology<span class=acl-fixed-case>CMU</span>-01 at the <span class=acl-fixed-case>SIGMORPHON</span> 2019 Shared Task on Crosslinguality and Context in Morphology</a></strong><br><a href=/people/a/aditi-chaudhary/>Aditi Chaudhary</a>
|
<a href=/people/e/elizabeth-salesky/>Elizabeth Salesky</a>
|
<a href=/people/g/gayatri-bhat/>Gayatri Bhat</a>
|
<a href=/people/d/david-r-mortensen/>David R. Mortensen</a>
|
<a href=/people/j/jaime-g-carbonell/>Jaime Carbonell</a>
|
<a href=/people/y/yulia-tsvetkov/>Yulia Tsvetkov</a><br><a href=/volumes/W19-42/ class=text-muted>Proceedings of the 16th Workshop on Computational Research in Phonetics, Phonology, and Morphology</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4208><div class="card-body p-3 small">This paper presents the submission by the CMU-01 team to the SIGMORPHON 2019 task 2 of Morphological Analysis and <a href=https://en.wikipedia.org/wiki/Lemmatization>Lemmatization</a> in Context. This task requires us to produce the lemma and morpho-syntactic description of each token in a sequence, for 107 <a href=https://en.wikipedia.org/wiki/Treebank>treebanks</a>. We approach this task with a hierarchical neural conditional random field (CRF) model which predicts each coarse-grained feature (eg. POS, <a href=https://en.wikipedia.org/wiki/Case_(disambiguation)>Case</a>, etc.) independently. However, most <a href=https://en.wikipedia.org/wiki/Treebank>treebanks</a> are under-resourced, thus making it challenging to train <a href=https://en.wikipedia.org/wiki/Deep_learning>deep neural models</a> for them. Hence, we propose a multi-lingual transfer training regime where we transfer from multiple related languages that share similar typology.</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1393.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1393 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1393 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D18-1393.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1393/>Framing and Agenda-setting in Russian News : a Computational Analysis of Intricate Political Strategies<span class=acl-fixed-case>R</span>ussian News: a Computational Analysis of Intricate Political Strategies</a></strong><br><a href=/people/a/anjalie-field/>Anjalie Field</a>
|
<a href=/people/d/doron-kliger/>Doron Kliger</a>
|
<a href=/people/s/shuly-wintner/>Shuly Wintner</a>
|
<a href=/people/j/jennifer-pan/>Jennifer Pan</a>
|
<a href=/people/d/dan-jurafsky/>Dan Jurafsky</a>
|
<a href=/people/y/yulia-tsvetkov/>Yulia Tsvetkov</a><br><a href=/volumes/D18-1/ class=text-muted>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1393><div class="card-body p-3 small">Amidst growing concern over <a href=https://en.wikipedia.org/wiki/Media_manipulation>media manipulation</a>, NLP attention has focused on overt strategies like <a href=https://en.wikipedia.org/wiki/Censorship>censorship</a> and <a href=https://en.wikipedia.org/wiki/Fake_news>fake news</a>. Here, we draw on two concepts from political science literature to explore subtler strategies for government media manipulation : <a href=https://en.wikipedia.org/wiki/Agenda-setting_theory>agenda-setting</a> (selecting what topics to cover) and <a href=https://en.wikipedia.org/wiki/Framing_(social_sciences)>framing</a> (deciding how topics are covered). We analyze 13 years (100 K articles) of the <a href=https://en.wikipedia.org/wiki/List_of_newspapers_in_Russia>Russian newspaper</a> Izvestia and identify a strategy of distraction : articles mention the U.S. more frequently in the month directly following an economic downturn in Russia. We introduce embedding-based methods for cross-lingually projecting English frames to <a href=https://en.wikipedia.org/wiki/Russian_language>Russian</a>, and discover that these articles emphasize U.S. moral failings and threats to the U.S. Our work offers new ways to identify subtle media manipulation strategies at the intersection of agenda-setting and framing.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/Q18-1024.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-Q18-1024 data-toggle=collapse aria-expanded=false aria-controls=abstract-Q18-1024 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285802410 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=Q18-1024" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/Q18-1024/>Native Language Cognate Effects on Second Language Lexical Choice</a></strong><br><a href=/people/e/ella-rabinovich/>Ella Rabinovich</a>
|
<a href=/people/y/yulia-tsvetkov/>Yulia Tsvetkov</a>
|
<a href=/people/s/shuly-wintner/>Shuly Wintner</a><br><a href=/volumes/Q18-1/ class=text-muted>Transactions of the Association for Computational Linguistics, Volume 6</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-Q18-1024><div class="card-body p-3 small">We present a computational analysis of cognate effects on the spontaneous linguistic productions of advanced non-native speakers. Introducing a large corpus of highly competent non-native English speakers, and using a set of carefully selected lexical items, we show that the lexical choices of non-natives are affected by cognates in their native language. This effect is so powerful that we are able to reconstruct the <a href=https://en.wikipedia.org/wiki/Phylogenetic_tree>phylogenetic language tree</a> of the <a href=https://en.wikipedia.org/wiki/Indo-European_languages>Indo-European language family</a> solely from the frequencies of specific lexical items in the English of authors with various native languages. We quantitatively analyze non-native lexical choice, highlighting cognate facilitation as one of the important phenomena shaping the language of non-native speakers.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-1200.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-1200/>Proceedings of the Second Workshop on Subword/Character <span class=acl-fixed-case>LE</span>vel Models</a></strong><br><a href=/people/m/manaal-faruqui/>Manaal Faruqui</a>
|
<a href=/people/h/hinrich-schutze/>Hinrich Sch√ºtze</a>
|
<a href=/people/i/isabel-trancoso/>Isabel Trancoso</a>
|
<a href=/people/y/yulia-tsvetkov/>Yulia Tsvetkov</a>
|
<a href=/people/y/yadollah-yaghoobzadeh/>Yadollah Yaghoobzadeh</a><br><a href=/volumes/W18-12/ class=text-muted>Proceedings of the Second Workshop on Subword/Character LEvel Models</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-6005.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-6005 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-6005 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-6005/>Socially Responsible NLP<span class=acl-fixed-case>NLP</span></a></strong><br><a href=/people/y/yulia-tsvetkov/>Yulia Tsvetkov</a>
|
<a href=/people/v/vinodkumar-prabhakaran/>Vinodkumar Prabhakaran</a>
|
<a href=/people/r/rob-voigt/>Rob Voigt</a><br><a href=/volumes/N18-6/ class=text-muted>Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Tutorial Abstracts</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-6005><div class="card-body p-3 small">As <a href=https://en.wikipedia.org/wiki/Language_technology>language technologies</a> have become increasingly prevalent, there is a growing awareness that decisions we make about our data, methods, and tools are often tied up with their impact on people and societies. This tutorial will provide an overview of real-world applications of <a href=https://en.wikipedia.org/wiki/Language_technology>language technologies</a> and the potential ethical implications associated with them. We will discuss philosophical foundations of ethical research along with state of the art techniques. Through this tutorial, we intend to provide the NLP researcher with an overview of tools to ensure that the data, algorithms, and models that they build are socially responsible. These tools will include a checklist of common pitfalls that one should avoid (e.g., demographic bias in data collection), as well as <a href=https://en.wikipedia.org/wiki/Sampling_(statistics)>methods</a> to adequately mitigate these issues (e.g., adjusting <a href=https://en.wikipedia.org/wiki/Sampling_(statistics)>sampling rates</a> or de-biasing through regularization). The tutorial is based on a new course on <a href=https://en.wikipedia.org/wiki/Ethics>Ethics</a> and <a href=https://en.wikipedia.org/wiki/Neuro-linguistic_programming>NLP</a> developed at Carnegie Mellon University.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1080.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1080 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1080 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1080.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285801126 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-1080" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-1080/>Style Transfer Through Back-Translation</a></strong><br><a href=/people/s/shrimai-prabhumoye/>Shrimai Prabhumoye</a>
|
<a href=/people/y/yulia-tsvetkov/>Yulia Tsvetkov</a>
|
<a href=/people/r/ruslan-salakhutdinov/>Ruslan Salakhutdinov</a>
|
<a href=/people/a/alan-w-black/>Alan W Black</a><br><a href=/volumes/P18-1/ class=text-muted>Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1080><div class="card-body p-3 small">Style transfer is the task of rephrasing the text to contain specific stylistic properties without changing the intent or affect within the context. This paper introduces a new <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>method</a> for automatic style transfer. We first learn a latent representation of the input sentence which is grounded in a language translation model in order to better preserve the meaning of the sentence while reducing stylistic properties. Then adversarial generation techniques are used to make the output match the desired style. We evaluate this technique on three different style transformations : <a href=https://en.wikipedia.org/wiki/Sentimentality>sentiment</a>, <a href=https://en.wikipedia.org/wiki/Gender>gender</a> and political slant. Compared to two state-of-the-art style transfer modeling techniques we show improvements both in automatic evaluation of style transfer and in manual evaluation of meaning preservation and fluency.</div></div><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2009.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2009 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2009 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P17-2009.Notes.zip data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234952193 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-2009/>Incorporating Dialectal Variability for Socially Equitable Language Identification</a></strong><br><a href=/people/d/david-jurgens/>David Jurgens</a>
|
<a href=/people/y/yulia-tsvetkov/>Yulia Tsvetkov</a>
|
<a href=/people/d/dan-jurafsky/>Dan Jurafsky</a><br><a href=/volumes/P17-2/ class=text-muted>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2009><div class="card-body p-3 small">Language identification (LID) is a critical first step for processing <a href=https://en.wikipedia.org/wiki/Multilingualism>multilingual text</a>. Yet most LID systems are not designed to handle the linguistic diversity of global platforms like <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a>, where local dialects and rampant code-switching lead language classifiers to systematically miss minority dialect speakers and multilingual speakers. We propose a new dataset and a character-based sequence-to-sequence model for LID designed to support dialectal and multilingual language varieties. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieves state-of-the-art performance on multiple LID benchmarks. Furthermore, in a case study using <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a> for health tracking, our method substantially increases the availability of texts written by underrepresented populations, enabling the development of socially inclusive NLP tools.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Yulia+Tsvetkov" title="Search for 'Yulia Tsvetkov' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/d/david-r-mortensen/ class=align-middle>David R. Mortensen</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/people/a/aditi-chaudhary/ class=align-middle>Aditi Chaudhary</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/s/shuly-wintner/ class=align-middle>Shuly Wintner</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/a/anjalie-field/ class=align-middle>Anjalie Field</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/a/antonios-anastasopoulos/ class=align-middle>Antonios Anastasopoulos</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/a/adithya-pratapa/ class=align-middle>Adithya Pratapa</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/g/graham-neubig/ class=align-middle>Graham Neubig</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/d/david-jurgens/ class=align-middle>David Jurgens</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/d/dan-jurafsky/ class=align-middle>Dan Jurafsky</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/v/vidhisha-balachandran/ class=align-middle>Vidhisha Balachandran</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/a/artidoro-pagnoni/ class=align-middle>Artidoro Pagnoni</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/j/jaime-g-carbonell/ class=align-middle>Jaime G. Carbonell</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/s/sachin-kumar/ class=align-middle>Sachin Kumar</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/g/gayatri-bhat/ class=align-middle>Gayatri Bhat</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/z/zaid-sheikh/ class=align-middle>Zaid Sheikh</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/x/xiaochuang-han/ class=align-middle>Xiaochuang Han</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/b/byron-c-wallace/ class=align-middle>Byron C. Wallace</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jimin-sun/ class=align-middle>Jimin Sun</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/hwijeen-ahn/ class=align-middle>Hwijeen Ahn</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/chan-young-park/ class=align-middle>Chan Young Park</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jay-yoon-lee/ class=align-middle>Jay Yoon Lee</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/dheeraj-rajagopal/ class=align-middle>Dheeraj Rajagopal</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/doron-kliger/ class=align-middle>Doron Kliger</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jennifer-pan/ class=align-middle>Jennifer Pan</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/shruti-rijhwani/ class=align-middle>Shruti Rijhwani</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/luke-breitfeller/ class=align-middle>Luke Breitfeller</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/e/emily-ahn/ class=align-middle>Emily Ahn</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/n/noah-a-smith/ class=align-middle>Noah A. Smith</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/e/ella-rabinovich/ class=align-middle>Ella Rabinovich</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/manaal-faruqui/ class=align-middle>Manaal Faruqui</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/hinrich-schutze/ class=align-middle>Hinrich Sch√ºtze</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/i/isabel-trancoso/ class=align-middle>Isabel Trancoso</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yadollah-yaghoobzadeh/ class=align-middle>Yadollah Yaghoobzadeh</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/e/elizabeth-salesky/ class=align-middle>Elizabeth Salesky</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/v/vinodkumar-prabhakaran/ class=align-middle>Vinodkumar Prabhakaran</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/rob-voigt/ class=align-middle>Rob Voigt</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/shrimai-prabhumoye/ class=align-middle>Shrimai Prabhumoye</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/ruslan-salakhutdinov/ class=align-middle>Ruslan Salakhutdinov</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/alan-w-black/ class=align-middle>Alan W. Black</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/emnlp/ class=align-middle>EMNLP</a><span class="badge badge-secondary align-middle ml-2">7</span></li><li class=list-group-item><a href=/venues/acl/ class=align-middle>ACL</a><span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/venues/eacl/ class=align-middle>EACL</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/naacl/ class=align-middle>NAACL</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/ws/ class=align-middle>WS</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-venues aria-expanded=false aria-controls=more-venues>show all...</li><div class="collapse border-top" id=more-venues><li class=list-group-item><a href=/venues/tacl/ class=align-middle>TACL</a><span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ¬©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>