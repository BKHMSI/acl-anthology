<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Yilin Shen - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Yilin</span> <span class=font-weight-bold>Shen</span></h2><hr><div class=row><div class=col-lg-9><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.352.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--352 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.352 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939280 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.352/>Generating Dialogue Responses from a Semantic Latent Space</a></strong><br><a href=/people/w/wei-jen-ko/>Wei-Jen Ko</a>
|
<a href=/people/a/avik-ray/>Avik Ray</a>
|
<a href=/people/y/yilin-shen/>Yilin Shen</a>
|
<a href=/people/h/hongxia-jin/>Hongxia Jin</a><br><a href=/volumes/2020.emnlp-main/ class=text-muted>Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--352><div class="card-body p-3 small">Existing open-domain dialogue generation models are usually trained to mimic the gold response in the training set using cross-entropy loss on the vocabulary. However, a good response does not need to resemble the gold response, since there are multiple possible responses to a given prompt. In this work, we hypothesize that the current models are unable to integrate information from multiple semantically similar valid responses of a prompt, resulting in the generation of generic and uninformative responses. To address this issue, we propose an alternative to the end-to-end classification on <a href=https://en.wikipedia.org/wiki/Vocabulary>vocabulary</a>. We learn the pair relationship between the prompts and responses as a <a href=https://en.wikipedia.org/wiki/Regression_analysis>regression task</a> on a latent space instead. In our novel dialog generation model, the representations of semantically related sentences are close to each other on the latent space. Human evaluation showed that learning the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> on a <a href=https://en.wikipedia.org/wiki/Continuous_or_discrete_variable>continuous space</a> can generate responses that are both relevant and informative.</div></div><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1126.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1126 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1126 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1126/>A Progressive Model to Enable Continual Learning for Semantic Slot Filling</a></strong><br><a href=/people/y/yilin-shen/>Yilin Shen</a>
|
<a href=/people/x/xiangyu-zeng/>Xiangyu Zeng</a>
|
<a href=/people/h/hongxia-jin/>Hongxia Jin</a><br><a href=/volumes/D19-1/ class=text-muted>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1126><div class="card-body p-3 small">Semantic slot filling is one of the major tasks in spoken language understanding (SLU). After a slot filling model is trained on precollected data, it is crucial to continually improve the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> after deployment to learn users&#8217; new expressions. As the data amount grows, it becomes infeasible to either store such huge data and repeatedly retrain the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> on all data or fine tune the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> only on new data without forgetting old expressions. In this paper, we introduce a novel progressive slot filling model, ProgModel. ProgModel consists of a novel context gate that transfers previously learned knowledge to a small size expanded component ; and meanwhile enables this new <a href=https://en.wikipedia.org/wiki/Component-based_software_engineering>component</a> to be fast trained to learn from new data. As such, ProgModel learns the new knowledge by only using new data at each time and meanwhile preserves the previously learned expressions. Our experiments show that ProgModel needs much less training time and smaller model size to outperform various model fine tuning competitors by up to 4.24 % and 3.03 % on two benchmark datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-6111.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-6111 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-6111 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-6111/>Fast Domain Adaptation of Semantic Parsers via Paraphrase Attention</a></strong><br><a href=/people/a/avik-ray/>Avik Ray</a>
|
<a href=/people/y/yilin-shen/>Yilin Shen</a>
|
<a href=/people/h/hongxia-jin/>Hongxia Jin</a><br><a href=/volumes/D19-61/ class=text-muted>Proceedings of the 2nd Workshop on Deep Learning Approaches for Low-Resource NLP (DeepLo 2019)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-6111><div class="card-body p-3 small">Semantic parsers are used to convert user&#8217;s natural language commands to executable logical form in intelligent personal agents. Labeled datasets required to train such <a href=https://en.wikipedia.org/wiki/Parsing>parsers</a> are expensive to collect, and are never comprehensive. As a result, for effective post-deployment domain adaptation and personalization, semantic parsers are continuously retrained to learn new user vocabulary and paraphrase variety. However, state-of-the art attention based neural parsers are slow to retrain which inhibits real time domain adaptation. Secondly, these <a href=https://en.wikipedia.org/wiki/Parsing>parsers</a> do not leverage numerous <a href=https://en.wikipedia.org/wiki/Paraphrase>paraphrases</a> already present in the training dataset. Designing <a href=https://en.wikipedia.org/wiki/Parsing>parsers</a> which can simultaneously maintain high <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> and fast retraining time is challenging. In this paper, we present novel paraphrase attention based sequence-to-sequence / tree parsers which support fast near real time retraining. In addition, our <a href=https://en.wikipedia.org/wiki/Parsing>parsers</a> often boost <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> by jointly modeling the semantic dependencies of paraphrases. We evaluate our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> on benchmark datasets to demonstrate upto 9X speedup in retraining time compared to existing <a href=https://en.wikipedia.org/wiki/Parsing>parsers</a>, as well as achieving state-of-the-art <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>.</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2050.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2050 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2050 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-2050/>A Bi-Model Based RNN Semantic Frame Parsing Model for Intent Detection and Slot Filling<span class=acl-fixed-case>RNN</span> Semantic Frame Parsing Model for Intent Detection and Slot Filling</a></strong><br><a href=/people/y/yu-wang/>Yu Wang</a>
|
<a href=/people/y/yilin-shen/>Yilin Shen</a>
|
<a href=/people/h/hongxia-jin/>Hongxia Jin</a><br><a href=/volumes/N18-2/ class=text-muted>Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2050><div class="card-body p-3 small">Intent detection and slot filling are two main tasks for building a spoken language understanding(SLU) system. Multiple <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning based models</a> have demonstrated good results on these <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a>. The most effective algorithms are based on the structures of sequence to sequence models (or encoder-decoder models), and generate the intents and semantic tags either using separate models. Most of the previous studies, however, either treat the <a href=https://en.wikipedia.org/wiki/Intention>intent detection</a> and slot filling as two separate parallel tasks, or use a sequence to sequence model to generate both semantic tags and <a href=https://en.wikipedia.org/wiki/Intention>intent</a>. None of the approaches consider the cross-impact between the intent detection task and the slot filling task. In this paper, new Bi-model based RNN semantic frame parsing network structures are designed to perform the intent detection and slot filling tasks jointly, by considering their cross-impact to each other using two correlated bidirectional LSTMs (BLSTM). Our Bi-model structure with a <a href=https://en.wikipedia.org/wiki/Code>decoder</a> achieves state-of-art result on the benchmark <a href=https://en.wikipedia.org/wiki/Automated_airport_weather_station>ATIS data</a>, with about 0.5 % intent accuracy improvement and 0.9 % slot filling improvement.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Yilin+Shen" title="Search for 'Yilin Shen' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/h/hongxia-jin/ class=align-middle>Hongxia Jin</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/people/a/avik-ray/ class=align-middle>Avik Ray</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/w/wei-jen-ko/ class=align-middle>Wei-Jen Ko</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/x/xiangyu-zeng/ class=align-middle>Xiangyu Zeng</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yu-wang/ class=align-middle>Yu Wang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/emnlp/ class=align-middle>EMNLP</a><span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/venues/naacl/ class=align-middle>NAACL</a><span class="badge badge-secondary align-middle ml-2">1</span></li></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>