<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Chuhan Wu - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Chuhan</span> <span class=font-weight-bold>Wu</span></h2><hr><div class=row><div class=col-lg-9><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.423.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--423 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.423 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.423/>HieRec : Hierarchical User Interest Modeling for Personalized News Recommendation<span class=acl-fixed-case>H</span>ie<span class=acl-fixed-case>R</span>ec: Hierarchical User Interest Modeling for Personalized News Recommendation</a></strong><br><a href=/people/t/tao-qi/>Tao Qi</a>
|
<a href=/people/f/fangzhao-wu/>Fangzhao Wu</a>
|
<a href=/people/c/chuhan-wu/>Chuhan Wu</a>
|
<a href=/people/p/peiru-yang/>Peiru Yang</a>
|
<a href=/people/y/yang-yu/>Yang Yu</a>
|
<a href=/people/x/xing-xie/>Xing Xie</a>
|
<a href=/people/y/yongfeng-huang/>Yongfeng Huang</a><br><a href=/volumes/2021.acl-long/ class=text-muted>Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--423><div class="card-body p-3 small">User interest modeling is critical for personalized news recommendation. Existing news recommendation methods usually learn a single user embedding for each user from their previous behaviors to represent their overall interest. However, user interest is usually diverse and multi-grained, which is difficult to be accurately modeled by a single user embedding. In this paper, we propose a news recommendation method with hierarchical user interest modeling, named HieRec. Instead of a single user embedding, in our <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>method</a> each user is represented in a hierarchical interest tree to better capture their diverse and multi-grained interest in news. We use a three-level hierarchy to represent 1) overall user interest ; 2) <a href=https://en.wikipedia.org/wiki/Interest_(emotion)>user interest</a> in coarse-grained topics like <a href=https://en.wikipedia.org/wiki/Sport>sports</a> ; and 3) <a href=https://en.wikipedia.org/wiki/Interest_(emotion)>user interest</a> in fine-grained topics like <a href=https://en.wikipedia.org/wiki/Association_football>football</a>. Moreover, we propose a hierarchical user interest matching framework to match candidate news with different levels of user interest for more accurate user interest targeting. Extensive experiments on two real-world datasets validate our method can effectively improve the performance of user modeling for personalized news recommendation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.424.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--424 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.424 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.424/>PP-Rec : News Recommendation with Personalized User Interest and Time-aware News Popularity<span class=acl-fixed-case>PP</span>-Rec: News Recommendation with Personalized User Interest and Time-aware News Popularity</a></strong><br><a href=/people/t/tao-qi/>Tao Qi</a>
|
<a href=/people/f/fangzhao-wu/>Fangzhao Wu</a>
|
<a href=/people/c/chuhan-wu/>Chuhan Wu</a>
|
<a href=/people/y/yongfeng-huang/>Yongfeng Huang</a><br><a href=/volumes/2021.acl-long/ class=text-muted>Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--424><div class="card-body p-3 small">Personalized news recommendation methods are widely used in <a href=https://en.wikipedia.org/wiki/Online_newspaper>online news services</a>. These methods usually recommend news based on the matching between <a href=https://en.wikipedia.org/wiki/Content_(media)>news content</a> and <a href=https://en.wikipedia.org/wiki/Interest_(emotion)>user interest</a> inferred from historical behaviors. However, these methods usually have difficulties in making accurate recommendations to cold-start users, and tend to recommend similar news with those users have read. In general, popular news usually contain important information and can attract users with different interests. Besides, they are usually diverse in content and topic. Thus, in this paper we propose to incorporate news popularity information to alleviate the cold-start and diversity problems for personalized news recommendation. In our method, the ranking score for recommending a candidate news to a target user is the combination of a personalized matching score and a news popularity score. The former is used to capture the personalized user interest in news. The latter is used to measure time-aware popularity of candidate news, which is predicted based on news content, recency, and real-time CTR using a unified framework. Besides, we propose a popularity-aware user encoder to eliminate the popularity bias in user behaviors for accurate interest modeling. Experiments on two real-world datasets show our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> can effectively improve the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> and <a href=https://en.wikipedia.org/wiki/Diversity_(politics)>diversity</a> for <a href=https://en.wikipedia.org/wiki/News_aggregator>news recommendation</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.223.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--223 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.223 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.223" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.223/>Efficient-FedRec : Efficient Federated Learning Framework for Privacy-Preserving News Recommendation<span class=acl-fixed-case>F</span>ed<span class=acl-fixed-case>R</span>ec: Efficient Federated Learning Framework for Privacy-Preserving News Recommendation</a></strong><br><a href=/people/j/jingwei-yi/>Jingwei Yi</a>
|
<a href=/people/f/fangzhao-wu/>Fangzhao Wu</a>
|
<a href=/people/c/chuhan-wu/>Chuhan Wu</a>
|
<a href=/people/r/ruixuan-liu/>Ruixuan Liu</a>
|
<a href=/people/g/guangzhong-sun/>Guangzhong Sun</a>
|
<a href=/people/x/xing-xie/>Xing Xie</a><br><a href=/volumes/2021.emnlp-main/ class=text-muted>Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--223><div class="card-body p-3 small">News recommendation is critical for personalized news access. Most existing news recommendation methods rely on centralized storage of users&#8217; historical news click behavior data, which may lead to privacy concerns and hazards. Federated Learning is a privacy-preserving framework for multiple clients to collaboratively train models without sharing their private data. However, the computation and communication cost of directly learning many existing news recommendation models in a federated way are unacceptable for user clients. In this paper, we propose an efficient federated learning framework for privacy-preserving news recommendation. Instead of training and communicating the whole <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a>, we decompose the news recommendation model into a large news model maintained in the server and a light-weight user model shared on both server and clients, where news representations and user model are communicated between server and clients. More specifically, the clients request the <a href=https://en.wikipedia.org/wiki/User_model>user model</a> and <a href=https://en.wikipedia.org/wiki/News>news representations</a> from the server, and send their locally computed gradients to the server for <a href=https://en.wikipedia.org/wiki/News_aggregator>aggregation</a>. The server updates its global user model with the aggregated gradients, and further updates its news model to infer updated news representations. Since the local gradients may contain private information, we propose a secure aggregation method to aggregate gradients in a privacy-preserving way. Experiments on two real-world datasets show that our method can reduce the computation and communication cost on clients while keep promising model performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.166.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--166 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.166 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.166/>DA-Transformer : Distance-aware Transformer<span class=acl-fixed-case>DA</span>-Transformer: Distance-aware Transformer</a></strong><br><a href=/people/c/chuhan-wu/>Chuhan Wu</a>
|
<a href=/people/f/fangzhao-wu/>Fangzhao Wu</a>
|
<a href=/people/y/yongfeng-huang/>Yongfeng Huang</a><br><a href=/volumes/2021.naacl-main/ class=text-muted>Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--166><div class="card-body p-3 small">Transformer has achieved great success in the NLP field by composing various advanced <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> like BERT and GPT. However, Transformer and its existing variants may not be optimal in capturing token distances because the position or distance embeddings used by these methods usually can not keep the precise information of real distances, which may not be beneficial for modeling the orders and relations of contexts. In this paper, we propose DA-Transformer, which is a distance-aware Transformer that can exploit the real distance. We propose to incorporate the real distances between tokens to re-scale the raw self-attention weights, which are computed by the <a href=https://en.wikipedia.org/wiki/Relevance_(information_retrieval)>relevance</a> between attention query and key. Concretely, in different self-attention heads the relative distance between each pair of tokens is weighted by different learnable parameters, which control the different preferences on long- or short-term information of these heads. Since the raw weighted real distances may not be optimal for adjusting self-attention weights, we propose a learnable sigmoid function to map them into re-scaled coefficients that have proper ranges. We first clip the raw self-attention weights via the ReLU function to keep non-negativity and introduce sparsity, and then multiply them with the re-scaled coefficients to encode real distance information into self-attention. Extensive experiments on five benchmark datasets show that DA-Transformer can effectively improve the performance of many tasks and outperform the vanilla Transformer and its several variants.</div></div><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.ccl-1.106.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--ccl-1--106 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.ccl-1.106 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.ccl-1.106/>Clickbait Detection with Style-aware Title Modeling and Co-attention</a></strong><br><a href=/people/c/chuhan-wu/>Chuhan Wu</a>
|
<a href=/people/f/fangzhao-wu/>Fangzhao Wu</a>
|
<a href=/people/t/tao-qi/>Tao Qi</a>
|
<a href=/people/y/yongfeng-huang/>Yongfeng Huang</a><br><a href=/volumes/2020.ccl-1/ class=text-muted>Proceedings of the 19th Chinese National Conference on Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--ccl-1--106><div class="card-body p-3 small">Clickbait is a form of <a href=https://en.wikipedia.org/wiki/Web_content>web content</a> designed to attract attention and entice users to click on specific hyperlinks. The detection of clickbaits is an important task for online platforms to improve the quality of web content and the satisfaction of users. Clickbait detection is typically formed as a binary classification task based on the title and body of a webpage, and existing methods are mainly based on the content of title and the relevance between title and body. However, these methods ignore the stylistic patterns of titles, which can provide important clues on identifying <a href=https://en.wikipedia.org/wiki/Clickbait>clickbaits</a>. In addition, they do not consider the interactions between the contexts within title and body, which are very important for measuring their <a href=https://en.wikipedia.org/wiki/Relevance>relevance</a> for clickbait detection. In this paper, we propose a clickbait detection approach with style-aware title modeling and co-attention. Specifically, we use Transformers to learn content representations of title and body, and respectively compute two content-based clickbait scores for title and body based on their <a href=https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning>representations</a>. In addition, we propose to use a character-level Transformer to learn a style-aware title representation by capturing the stylistic patterns of title, and we compute a title stylistic score based on this representation. Besides, we propose to use a co-attention network to model the relatedness between the contexts within title and body, and further enhance their representations by encoding the interaction information. We compute a title-body matching score based on the representations of title and body enhanced by their interactions. The final clickbait score is predicted by a weighted summation of the aforementioned four kinds of scores.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.aacl-main.22.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--aacl-main--22 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.aacl-main.22 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.aacl-main.22/>Named Entity Recognition in Multi-level Contexts</a></strong><br><a href=/people/y/yubo-chen/>Yubo Chen</a>
|
<a href=/people/c/chuhan-wu/>Chuhan Wu</a>
|
<a href=/people/t/tao-qi/>Tao Qi</a>
|
<a href=/people/z/zhigang-yuan/>Zhigang Yuan</a>
|
<a href=/people/y/yongfeng-huang/>Yongfeng Huang</a><br><a href=/volumes/2020.aacl-main/ class=text-muted>Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--aacl-main--22><div class="card-body p-3 small">Named entity recognition is a critical task in the <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing field</a>. Most existing <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> for this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> can only exploit <a href=https://en.wikipedia.org/wiki/Context_(language_use)>contextual information</a> within a sentence. However, their performance on recognizing entities in limited or ambiguous sentence-level contexts is usually unsatisfactory. Fortunately, other sentences in the same document can provide supplementary document-level contexts to help recognize these entities. In addition, words themselves contain word-level contextual information since they usually have different preferences of entity type and relative position from named entities. In this paper, we propose a unified framework to incorporate <a href=https://en.wikipedia.org/wiki/Context_(computing)>multi-level contexts</a> for <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>named entity recognition</a>. We use TagLM as our basic <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> to capture sentence-level contexts. To incorporate document-level contexts, we propose to capture interactions between sentences via a multi-head self attention network. To mine word-level contexts, we propose an auxiliary task to predict the type of each word to capture its type preference. We jointly train our model in entity recognition and the auxiliary classification task via <a href=https://en.wikipedia.org/wiki/Multi-task_learning>multi-task learning</a>. The experimental results on several <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmark datasets</a> validate the effectiveness of our <a href=https://en.wikipedia.org/wiki/Methodology>method</a>.</div></div><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1493.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1493 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1493 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1493/>Neural News Recommendation with Heterogeneous User Behavior</a></strong><br><a href=/people/c/chuhan-wu/>Chuhan Wu</a>
|
<a href=/people/f/fangzhao-wu/>Fangzhao Wu</a>
|
<a href=/people/m/mingxiao-an/>Mingxiao An</a>
|
<a href=/people/t/tao-qi/>Tao Qi</a>
|
<a href=/people/j/jianqiang-huang/>Jianqiang Huang</a>
|
<a href=/people/y/yongfeng-huang/>Yongfeng Huang</a>
|
<a href=/people/x/xing-xie/>Xing Xie</a><br><a href=/volumes/D19-1/ class=text-muted>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1493><div class="card-body p-3 small">News recommendation is important for <a href=https://en.wikipedia.org/wiki/Online_newspaper>online news platforms</a> to help users find interested news and alleviate <a href=https://en.wikipedia.org/wiki/Information_overload>information overload</a>. Existing news recommendation methods usually rely on the news click history to model <a href=https://en.wikipedia.org/wiki/Interest_(emotion)>user interest</a>. However, these methods may suffer from the data sparsity problem, since the news click behaviors of many users in online news platforms are usually very limited. Fortunately, some other kinds of user behaviors such as <a href=https://en.wikipedia.org/wiki/Web_navigation>webpage browsing</a> and <a href=https://en.wikipedia.org/wiki/Web_search_query>search queries</a> can also provide useful clues of users&#8217; news reading interest. In this paper, we propose a neural news recommendation approach which can exploit heterogeneous user behaviors. Our approach contains two major <a href=https://en.wikipedia.org/wiki/Modular_programming>modules</a>, i.e., <a href=https://en.wikipedia.org/wiki/News_media>news representation</a> and <a href=https://en.wikipedia.org/wiki/User_interface>user representation</a>. In the news representation module, we learn representations of news from their titles via CNN networks, and apply attention networks to select important words. In the user representation module, we propose an attentive multi-view learning framework to learn unified representations of users from their heterogeneous behaviors such as <a href=https://en.wikipedia.org/wiki/Web_search_query>search queries</a>, clicked news and browsed webpages. In addition, we use word- and record-level attentions to select informative words and behavior records. Experiments on a real-world dataset validate the effectiveness of our approach.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3214.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3214 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3214 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3214/>Detecting and Extracting of Adverse Drug Reaction Mentioning Tweets with Multi-Head Self Attention</a></strong><br><a href=/people/s/suyu-ge/>Suyu Ge</a>
|
<a href=/people/t/tao-qi/>Tao Qi</a>
|
<a href=/people/c/chuhan-wu/>Chuhan Wu</a>
|
<a href=/people/y/yongfeng-huang/>Yongfeng Huang</a><br><a href=/volumes/W19-32/ class=text-muted>Proceedings of the Fourth Social Media Mining for Health Applications (#SMM4H) Workshop & Shared Task</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3214><div class="card-body p-3 small">This paper describes our <a href=https://en.wikipedia.org/wiki/System>system</a> for the first and second shared tasks of the fourth Social Media Mining for Health Applications (SMM4H) workshop. We enhance tweet representation with a <a href=https://en.wikipedia.org/wiki/Language_model>language model</a> and distinguish the importance of different words with Multi-Head Self-Attention. In addition, <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a> is exploited to make up for the data shortage. Our system achieved competitive results on both <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> with an F1-score of 0.5718 for task 1 and 0.653 (overlap) / 0.357 (strict) for task 2.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1033.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1033 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1033 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-1033/>Neural News Recommendation with Long- and Short-term User Representations</a></strong><br><a href=/people/m/mingxiao-an/>Mingxiao An</a>
|
<a href=/people/f/fangzhao-wu/>Fangzhao Wu</a>
|
<a href=/people/c/chuhan-wu/>Chuhan Wu</a>
|
<a href=/people/k/kun-zhang/>Kun Zhang</a>
|
<a href=/people/z/zheng-liu/>Zheng Liu</a>
|
<a href=/people/x/xing-xie/>Xing Xie</a><br><a href=/volumes/P19-1/ class=text-muted>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1033><div class="card-body p-3 small">Personalized news recommendation is important to help users find their interested news and improve reading experience. A key problem in news recommendation is learning accurate user representations to capture their interests. Users usually have both long-term preferences and short-term interests. However, existing news recommendation methods usually learn single representations of users, which may be insufficient. In this paper, we propose a neural news recommendation approach which can learn both long- and short-term user representations. The core of our approach is a news encoder and a <a href=https://en.wikipedia.org/wiki/User-generated_content>user encoder</a>. In the news encoder, we learn representations of news from their titles and topic categories, and use attention network to select important words. In the user encoder, we propose to learn long-term user representations from the embeddings of their IDs. In addition, we propose to learn short-term user representations from their recently browsed news via GRU network. Besides, we propose two <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> to combine long-term and short-term user representations. The first one is using the long-term user representation to initialize the hidden state of the GRU network in short-term user representation. The second one is concatenating both long- and short-term user representations as a unified user vector. Extensive experiments on a real-world dataset show our approach can effectively improve the performance of neural news recommendation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1110.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1110 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1110 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-1110/>Neural News Recommendation with Topic-Aware News Representation</a></strong><br><a href=/people/c/chuhan-wu/>Chuhan Wu</a>
|
<a href=/people/f/fangzhao-wu/>Fangzhao Wu</a>
|
<a href=/people/m/mingxiao-an/>Mingxiao An</a>
|
<a href=/people/y/yongfeng-huang/>Yongfeng Huang</a>
|
<a href=/people/x/xing-xie/>Xing Xie</a><br><a href=/volumes/P19-1/ class=text-muted>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1110><div class="card-body p-3 small">News recommendation can help users find interested news and alleviate <a href=https://en.wikipedia.org/wiki/Information_overload>information overload</a>. The topic information of news is critical for learning accurate news and user representations for <a href=https://en.wikipedia.org/wiki/News_aggregator>news recommendation</a>. However, <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> is not considered in many existing news recommendation methods. In this paper, we propose a neural news recommendation approach with topic-aware news representations. The core of our approach is a topic-aware news encoder and a user encoder. In the news encoder we learn representations of news from their titles via CNN networks and apply attention networks to select important words. In addition, we propose to learn topic-aware news representations by jointly training the news encoder with an auxiliary topic classification task. In the user encoder we learn the representations of users from their browsed news and use attention networks to select informative news for user representation learning. Extensive experiments on a real-world dataset validate the effectiveness of our approach.</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S18-1006.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S18-1006 data-toggle=collapse aria-expanded=false aria-controls=abstract-S18-1006 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/S18-1006/>THU_NGN at SemEval-2018 Task 3 : Tweet Irony Detection with Densely connected LSTM and Multi-task Learning<span class=acl-fixed-case>THU</span>_<span class=acl-fixed-case>NGN</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2018 Task 3: Tweet Irony Detection with Densely connected <span class=acl-fixed-case>LSTM</span> and Multi-task Learning</a></strong><br><a href=/people/c/chuhan-wu/>Chuhan Wu</a>
|
<a href=/people/f/fangzhao-wu/>Fangzhao Wu</a>
|
<a href=/people/s/sixing-wu/>Sixing Wu</a>
|
<a href=/people/j/junxin-liu/>Junxin Liu</a>
|
<a href=/people/z/zhigang-yuan/>Zhigang Yuan</a>
|
<a href=/people/y/yongfeng-huang/>Yongfeng Huang</a><br><a href=/volumes/S18-1/ class=text-muted>Proceedings of The 12th International Workshop on Semantic Evaluation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S18-1006><div class="card-body p-3 small">Detecting irony is an important task to mine fine-grained information from social web messages. Therefore, the Semeval-2018 task 3 is aimed to detect the ironic tweets (subtask A) and their ironic types (subtask B). In order to address this task, we propose a <a href=https://en.wikipedia.org/wiki/System>system</a> based on a densely connected LSTM network with multi-task learning strategy. In our dense LSTM model, each layer will take all outputs from previous layers as input. The last LSTM layer will output the hidden representations of texts, and they will be used in three classification task. In addition, we incorporate several types of features to improve the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> performance. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieved an <a href=https://en.wikipedia.org/wiki/F-score>F-score</a> of 70.54 (ranked 2/43) in the subtask A and 49.47 (ranked 3/29) in the subtask B. The experimental results validate the effectiveness of our <a href=https://en.wikipedia.org/wiki/System>system</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S18-1028.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S18-1028 data-toggle=collapse aria-expanded=false aria-controls=abstract-S18-1028 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/S18-1028/>THU_NGN at SemEval-2018 Task 1 : Fine-grained Tweet Sentiment Intensity Analysis with Attention CNN-LSTM<span class=acl-fixed-case>THU</span>_<span class=acl-fixed-case>NGN</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2018 Task 1: Fine-grained Tweet Sentiment Intensity Analysis with Attention <span class=acl-fixed-case>CNN</span>-<span class=acl-fixed-case>LSTM</span></a></strong><br><a href=/people/c/chuhan-wu/>Chuhan Wu</a>
|
<a href=/people/f/fangzhao-wu/>Fangzhao Wu</a>
|
<a href=/people/j/junxin-liu/>Junxin Liu</a>
|
<a href=/people/z/zhigang-yuan/>Zhigang Yuan</a>
|
<a href=/people/s/sixing-wu/>Sixing Wu</a>
|
<a href=/people/y/yongfeng-huang/>Yongfeng Huang</a><br><a href=/volumes/S18-1/ class=text-muted>Proceedings of The 12th International Workshop on Semantic Evaluation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S18-1028><div class="card-body p-3 small">Traditional sentiment analysis approaches mainly focus on classifying the sentiment polarities or emotion categories of texts. However, <a href=https://en.wikipedia.org/wiki/Information_technology>they</a> ca n&#8217;t exploit the <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment intensity information</a>. Therefore, the SemEval-2018 Task 1 is aimed to automatically determine the intensity of emotions or sentiment of tweets to mine fine-grained sentiment information. In order to address this task, we propose a <a href=https://en.wikipedia.org/wiki/System>system</a> based on an attention CNN-LSTM model. In our <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a>, LSTM is used to extract the long-term contextual information from texts. We apply <a href=https://en.wikipedia.org/wiki/Attentional_control>attention techniques</a> to selecting this <a href=https://en.wikipedia.org/wiki/Information>information</a>. A CNN layer with different size of kernels is used to extract local features. The dense layers take the pooled CNN feature maps and predict the intensity scores. Our system reaches average Pearson correlation score of 0.722 (ranked 12/48) in emotion intensity regression task, and 0.810 in valence regression task (ranked 15/38). It indicates that our <a href=https://en.wikipedia.org/wiki/System>system</a> can be further extended.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S18-1063.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S18-1063 data-toggle=collapse aria-expanded=false aria-controls=abstract-S18-1063 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/S18-1063/>THU_NGN at SemEval-2018 Task 2 : Residual CNN-LSTM Network with Attention for English Emoji Prediction<span class=acl-fixed-case>THU</span>_<span class=acl-fixed-case>NGN</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2018 Task 2: Residual <span class=acl-fixed-case>CNN</span>-<span class=acl-fixed-case>LSTM</span> Network with Attention for <span class=acl-fixed-case>E</span>nglish Emoji Prediction</a></strong><br><a href=/people/c/chuhan-wu/>Chuhan Wu</a>
|
<a href=/people/f/fangzhao-wu/>Fangzhao Wu</a>
|
<a href=/people/s/sixing-wu/>Sixing Wu</a>
|
<a href=/people/z/zhigang-yuan/>Zhigang Yuan</a>
|
<a href=/people/j/junxin-liu/>Junxin Liu</a>
|
<a href=/people/y/yongfeng-huang/>Yongfeng Huang</a><br><a href=/volumes/S18-1/ class=text-muted>Proceedings of The 12th International Workshop on Semantic Evaluation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S18-1063><div class="card-body p-3 small">Emojis are widely used by <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> and social network users when posting their messages. It is important to study the relationships between messages and <a href=https://en.wikipedia.org/wiki/Emoji>emojis</a>. Thus, in SemEval-2018 Task 2 an interesting and challenging <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> is proposed, i.e., predicting which <a href=https://en.wikipedia.org/wiki/Emoji>emojis</a> are evoked by <a href=https://en.wikipedia.org/wiki/Twitter>text-based tweets</a>. We propose a residual CNN-LSTM with attention (RCLA) model for this task. Our model combines CNN and LSTM layers to capture both local and long-range contextual information for tweet representation. In addition, attention mechanism is used to select important components. Besides, residual connection is applied to CNN layers to facilitate the training of <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a>. We also incorporated additional <a href=https://en.wikipedia.org/wiki/Software_feature>features</a> such as POS tags and sentiment features extracted from <a href=https://en.wikipedia.org/wiki/Lexicon>lexicons</a>. Our model achieved 30.25 % macro-averaged F-score in the first subtask (i.e., emoji prediction in English), ranking 7th out of 48 participants.<b>RCLA</b>) model\n for this task. Our model combines CNN and LSTM layers to capture\n both local and long-range contextual information for tweet\n representation. In addition, attention mechanism is used to\n select important components. Besides, residual connection is\n applied to CNN layers to facilitate the training of neural\n networks. We also incorporated additional features such as POS\n tags and sentiment features extracted from lexicons. Our model\n achieved 30.25% macro-averaged F-score in the first subtask\n (i.e., emoji prediction in English), ranking 7th out of 48\n participants.\n</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S18-1157.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S18-1157 data-toggle=collapse aria-expanded=false aria-controls=abstract-S18-1157 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/S18-1157/>THU_NGN at SemEval-2018 Task 10 : Capturing Discriminative Attributes with MLP-CNN model<span class=acl-fixed-case>THU</span>_<span class=acl-fixed-case>NGN</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2018 Task 10: Capturing Discriminative Attributes with <span class=acl-fixed-case>MLP</span>-<span class=acl-fixed-case>CNN</span> model</a></strong><br><a href=/people/c/chuhan-wu/>Chuhan Wu</a>
|
<a href=/people/f/fangzhao-wu/>Fangzhao Wu</a>
|
<a href=/people/s/sixing-wu/>Sixing Wu</a>
|
<a href=/people/z/zhigang-yuan/>Zhigang Yuan</a>
|
<a href=/people/y/yongfeng-huang/>Yongfeng Huang</a><br><a href=/volumes/S18-1/ class=text-muted>Proceedings of The 12th International Workshop on Semantic Evaluation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S18-1157><div class="card-body p-3 small">Existing semantic models are capable of identifying the semantic similarity of words. However, it&#8217;s hard for these <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> to discriminate between a word and another similar word. Thus, the aim of SemEval-2018 Task 10 is to predict whether a word is a discriminative attribute between two concepts. In this task, we apply a multilayer perceptron (MLP)-convolutional neural network (CNN) model to identify whether an attribute is discriminative. The CNNs are used to extract low-level features from the inputs. The MLP takes both the flatten CNN maps and inputs to predict the labels. The evaluation F-score of our <a href=https://en.wikipedia.org/wiki/System>system</a> on the test set is 0.629 (ranked 15th), which indicates that our <a href=https://en.wikipedia.org/wiki/System>system</a> still needs to be improved. However, the behaviours of our <a href=https://en.wikipedia.org/wiki/System>system</a> in our experiments provide useful information, which can help to improve the collective understanding of this novel <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0913.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0913 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0913 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0913/>Neural Metaphor Detecting with CNN-LSTM Model<span class=acl-fixed-case>CNN</span>-<span class=acl-fixed-case>LSTM</span> Model</a></strong><br><a href=/people/c/chuhan-wu/>Chuhan Wu</a>
|
<a href=/people/f/fangzhao-wu/>Fangzhao Wu</a>
|
<a href=/people/y/yubo-chen/>Yubo Chen</a>
|
<a href=/people/s/sixing-wu/>Sixing Wu</a>
|
<a href=/people/z/zhigang-yuan/>Zhigang Yuan</a>
|
<a href=/people/y/yongfeng-huang/>Yongfeng Huang</a><br><a href=/volumes/W18-09/ class=text-muted>Proceedings of the Workshop on Figurative Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0913><div class="card-body p-3 small">Metaphors are <a href=https://en.wikipedia.org/wiki/Literal_and_figurative_language>figurative languages</a> widely used in daily life and literatures. It&#8217;s an important task to detect the <a href=https://en.wikipedia.org/wiki/Metaphor>metaphors</a> evoked by texts. Thus, the metaphor shared task is aimed to extract <a href=https://en.wikipedia.org/wiki/Metaphor>metaphors</a> from plain texts at <a href=https://en.wikipedia.org/wiki/Literal_and_figurative_language>word level</a>. We propose to use a CNN-LSTM model for this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. Our model combines CNN and LSTM layers to utilize both local and long-range contextual information for identifying metaphorical information. In addition, we compare the performance of the softmax classifier and conditional random field (CRF) for sequential labeling in this task. We also incorporated some additional features such as part of speech (POS) tags and word cluster to improve the performance of model. Our best <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> achieved 65.06 % <a href=https://en.wikipedia.org/wiki/F-score>F-score</a> in the all POS testing subtask and 67.15 % in the verbs testing subtask.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5909.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5909 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5909 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-5909" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-5909/>Detecting Tweets Mentioning Drug Name and Adverse Drug Reaction with Hierarchical Tweet Representation and Multi-Head Self-Attention</a></strong><br><a href=/people/c/chuhan-wu/>Chuhan Wu</a>
|
<a href=/people/f/fangzhao-wu/>Fangzhao Wu</a>
|
<a href=/people/j/junxin-liu/>Junxin Liu</a>
|
<a href=/people/s/sixing-wu/>Sixing Wu</a>
|
<a href=/people/y/yongfeng-huang/>Yongfeng Huang</a>
|
<a href=/people/x/xing-xie/>Xing Xie</a><br><a href=/volumes/W18-59/ class=text-muted>Proceedings of the 2018 EMNLP Workshop SMM4H: The 3rd Social Media Mining for Health Applications Workshop & Shared Task</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5909><div class="card-body p-3 small">This paper describes our system for the first and third shared tasks of the third Social Media Mining for Health Applications (SMM4H) workshop, which aims to detect the tweets mentioning drug names and adverse drug reactions. In our system we propose a neural approach with hierarchical tweet representation and multi-head self-attention (HTR-MSA) for both tasks. Our <a href=https://en.wikipedia.org/wiki/System>system</a> achieved the first place in both the first and third shared tasks of SMM4H with an <a href=https://en.wikipedia.org/wiki/F-score>F-score</a> of 91.83 % and 52.20 % respectively.</div></div><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-4007.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-4007 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-4007 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-4007/>THU_NGN at IJCNLP-2017 Task 2 : Dimensional Sentiment Analysis for Chinese Phrases with Deep LSTM<span class=acl-fixed-case>THU</span>_<span class=acl-fixed-case>NGN</span> at <span class=acl-fixed-case>IJCNLP</span>-2017 Task 2: Dimensional Sentiment Analysis for <span class=acl-fixed-case>C</span>hinese Phrases with Deep <span class=acl-fixed-case>LSTM</span></a></strong><br><a href=/people/c/chuhan-wu/>Chuhan Wu</a>
|
<a href=/people/f/fangzhao-wu/>Fangzhao Wu</a>
|
<a href=/people/y/yongfeng-huang/>Yongfeng Huang</a>
|
<a href=/people/s/sixing-wu/>Sixing Wu</a>
|
<a href=/people/z/zhigang-yuan/>Zhigang Yuan</a><br><a href=/volumes/I17-4/ class=text-muted>Proceedings of the IJCNLP 2017, Shared Tasks</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-4007><div class="card-body p-3 small">Predicting valence-arousal ratings for words and phrases is very useful for constructing affective resources for dimensional sentiment analysis. Since the existing valence-arousal resources of <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a> are mainly in word-level and there is a lack of phrase-level ones, the Dimensional Sentiment Analysis for Chinese Phrases (DSAP) task aims to predict the valence-arousal ratings for <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese affective words and phrases</a> automatically. In this task, we propose an approach using a densely connected LSTM network and word features to identify dimensional sentiment on valence and arousal for words and phrases jointly. We use <a href=https://en.wikipedia.org/wiki/Word_embedding>word embedding</a> as major feature and choose part of speech (POS) and word clusters as additional features to train the dense LSTM network. The evaluation results of our submissions (1st and 2nd in average performance) validate the effectiveness of our system to predict valence and arousal dimensions for Chinese words and phrases.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Chuhan+Wu" title="Search for 'Chuhan Wu' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/f/fangzhao-wu/ class=align-middle>Fangzhao Wu</a>
<span class="badge badge-secondary align-middle ml-2">15</span></li><li class=list-group-item><a href=/people/y/yongfeng-huang/ class=align-middle>Yongfeng Huang</a>
<span class="badge badge-secondary align-middle ml-2">15</span></li><li class=list-group-item><a href=/people/s/sixing-wu/ class=align-middle>Sixing Wu</a>
<span class="badge badge-secondary align-middle ml-2">7</span></li><li class=list-group-item><a href=/people/z/zhigang-yuan/ class=align-middle>Zhigang Yuan</a>
<span class="badge badge-secondary align-middle ml-2">7</span></li><li class=list-group-item><a href=/people/t/tao-qi/ class=align-middle>Tao Qi</a>
<span class="badge badge-secondary align-middle ml-2">6</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/x/xing-xie/ class=align-middle>Xing Xie</a>
<span class="badge badge-secondary align-middle ml-2">6</span></li><li class=list-group-item><a href=/people/j/junxin-liu/ class=align-middle>Junxin Liu</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/people/m/mingxiao-an/ class=align-middle>Mingxiao An</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/y/yubo-chen/ class=align-middle>Yubo Chen</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/p/peiru-yang/ class=align-middle>Peiru Yang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yang-yu/ class=align-middle>Yang Yu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jingwei-yi/ class=align-middle>Jingwei Yi</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/ruixuan-liu/ class=align-middle>Ruixuan Liu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/g/guangzhong-sun/ class=align-middle>Guangzhong Sun</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jianqiang-huang/ class=align-middle>Jianqiang Huang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/suyu-ge/ class=align-middle>Suyu Ge</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/kun-zhang/ class=align-middle>Kun Zhang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/zheng-liu/ class=align-middle>Zheng Liu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/acl/ class=align-middle>ACL</a><span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/venues/semeval/ class=align-middle>SemEval</a><span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/venues/ws/ class=align-middle>WS</a><span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/venues/emnlp/ class=align-middle>EMNLP</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/ijcnlp/ class=align-middle>IJCNLP</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-venues aria-expanded=false aria-controls=more-venues>show all...</li><div class="collapse border-top" id=more-venues><li class=list-group-item><a href=/venues/ccl/ class=align-middle>CCL</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/aacl/ class=align-middle>AACL</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/naacl/ class=align-middle>NAACL</a><span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>