<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Chris Biemann - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Chris</span> <span class=font-weight-bold>Biemann</span></h2><hr><div class=row><div class=col-lg-9><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-demos.8.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-demos--8 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-demos.8 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-demos.8/>Forum 4.0 : An Open-Source User Comment Analysis Framework</a></strong><br><a href=/people/m/marlo-haering/>Marlo Haering</a>
|
<a href=/people/j/jakob-smedegaard-andersen/>Jakob Smedegaard Andersen</a>
|
<a href=/people/c/chris-biemann/>Chris Biemann</a>
|
<a href=/people/w/wiebke-loosen/>Wiebke Loosen</a>
|
<a href=/people/b/benjamin-milde/>Benjamin Milde</a>
|
<a href=/people/t/tim-pietz/>Tim Pietz</a>
|
<a href=/people/c/christian-stocker/>Christian Stöcker</a>
|
<a href=/people/g/gregor-wiedemann/>Gregor Wiedemann</a>
|
<a href=/people/o/olaf-zukunft/>Olaf Zukunft</a>
|
<a href=/people/w/walid-maalej/>Walid Maalej</a><br><a href=/volumes/2021.eacl-demos/ class=text-muted>Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: System Demonstrations</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-demos--8><div class="card-body p-3 small">With the increasing number of user comments in diverse domains, including comments on <a href=https://en.wikipedia.org/wiki/Digital_journalism>online journalism</a> and <a href=https://en.wikipedia.org/wiki/E-commerce>e-commerce websites</a>, the manual content analysis of these comments becomes time-consuming and challenging. However, research showed that user comments contain useful information for different domain experts, which is thus worth finding and utilizing. This paper introduces Forum 4.0, an open-source framework to semi-automatically analyze, aggregate, and visualize user comments based on labels defined by domain experts. We demonstrate the applicability of Forum 4.0 with comments analytics scenarios within the domains of <a href=https://en.wikipedia.org/wiki/Digital_journalism>online journalism</a> and <a href=https://en.wikipedia.org/wiki/App_store>app stores</a>. We outline the underlying container architecture, including the <a href=https://en.wikipedia.org/wiki/Web_interface>web-based user interface</a>, the <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning component</a>, and the <a href=https://en.wikipedia.org/wiki/Task_manager>task manager</a> for time-consuming tasks. We finally conduct <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning</a> experiments with simulated annotations and different <a href=https://en.wikipedia.org/wiki/Sampling_(statistics)>sampling strategies</a> on existing <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> from both domains to evaluate Forum 4.0&#8217;s performance. Forum 4.0 achieves promising classification results (ROC-AUC 0.9 with 100 annotated samples), utilizing transformer-based embeddings with a lightweight <a href=https://en.wikipedia.org/wiki/Logistic_regression>logistic regression model</a>. We explain how Forum 4.0&#8217;s architecture is applicable for millions of user comments in real-time, yet at feasible training and classification costs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-demos.36.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-demos--36 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-demos.36 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-demos.36/>Which is Better for <a href=https://en.wikipedia.org/wiki/Deep_learning>Deep Learning</a> : <a href=https://en.wikipedia.org/wiki/Python_(programming_language)>Python</a> or <a href=https://en.wikipedia.org/wiki/MATLAB>MATLAB</a>? Answering Comparative Questions in Natural Language<span class=acl-fixed-case>MATLAB</span>? Answering Comparative Questions in Natural Language</a></strong><br><a href=/people/v/viktoriia-chekalina/>Viktoriia Chekalina</a>
|
<a href=/people/a/alexander-bondarenko/>Alexander Bondarenko</a>
|
<a href=/people/c/chris-biemann/>Chris Biemann</a>
|
<a href=/people/m/meriem-beloucif/>Meriem Beloucif</a>
|
<a href=/people/v/varvara-logacheva/>Varvara Logacheva</a>
|
<a href=/people/a/alexander-panchenko/>Alexander Panchenko</a><br><a href=/volumes/2021.eacl-demos/ class=text-muted>Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: System Demonstrations</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-demos--36><div class="card-body p-3 small">We present a <a href=https://en.wikipedia.org/wiki/System>system</a> for answering comparative questions (Is X better than Y with respect to Z?) in <a href=https://en.wikipedia.org/wiki/Natural_language>natural language</a>. Answering such questions is important for assisting humans in making informed decisions. The key component of our system is a <a href=https://en.wikipedia.org/wiki/Natural-language_user_interface>natural language interface</a> for comparative QA that can be used in <a href=https://en.wikipedia.org/wiki/Personal_assistant>personal assistants</a>, <a href=https://en.wikipedia.org/wiki/Chatbot>chatbots</a>, and similar <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP devices</a>. Comparative QA is a challenging NLP task, since it requires collecting support evidence from many different sources, and direct comparisons of rare objects may be not available even on the entire Web. We take the first step towards a solution for such a task offering a testbed for comparative QA in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language</a> by probing several methods, making the three best ones available as an online demo.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-demos.12.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-demos--12 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-demos.12 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.naacl-demos.12.Supplementary.pdf data-toggle=tooltip data-placement=top title=Supplementary><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2021.naacl-demos.12/>ActiveAnno : General-Purpose Document-Level Annotation Tool with Active Learning Integration<span class=acl-fixed-case>A</span>ctive<span class=acl-fixed-case>A</span>nno: General-Purpose Document-Level Annotation Tool with Active Learning Integration</a></strong><br><a href=/people/m/max-wiechmann/>Max Wiechmann</a>
|
<a href=/people/s/seid-muhie-yimam/>Seid Muhie Yimam</a>
|
<a href=/people/c/chris-biemann/>Chris Biemann</a><br><a href=/volumes/2021.naacl-demos/ class=text-muted>Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Demonstrations</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-demos--12><div class="card-body p-3 small">ActiveAnno is an <a href=https://en.wikipedia.org/wiki/Annotation>annotation tool</a> focused on document-level annotation tasks developed both for industry and research settings. It is designed to be a general-purpose tool with a wide variety of use cases. It features a modern and responsive <a href=https://en.wikipedia.org/wiki/User_interface>web UI</a> for creating annotation projects, conducting <a href=https://en.wikipedia.org/wiki/Annotation>annotations</a>, adjudicating disagreements, and analyzing annotation results. ActiveAnno embeds a highly configurable and interactive user interface. The tool also integrates a RESTful API that enables integration into other <a href=https://en.wikipedia.org/wiki/Software_system>software systems</a>, including an <a href=https://en.wikipedia.org/wiki/Application_programming_interface>API</a> for machine learning integration. ActiveAnno is built with extensible design and easy deployment in mind, all to enable users to perform annotation tasks with high efficiency and high-quality annotation results.</div></div><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.pam-1.13.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--pam-1--13 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.pam-1.13 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.pam-1.13/>Generating Lexical Representations of Frames using Lexical Substitution</a></strong><br><a href=/people/s/saba-anwar/>Saba Anwar</a>
|
<a href=/people/a/artem-shelmanov/>Artem Shelmanov</a>
|
<a href=/people/a/alexander-panchenko/>Alexander Panchenko</a>
|
<a href=/people/c/chris-biemann/>Chris Biemann</a><br><a href=/volumes/2020.pam-1/ class=text-muted>Proceedings of the Probability and Meaning Conference (PaM 2020)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--pam-1--13><div class="card-body p-3 small">Semantic frames are <a href=https://en.wikipedia.org/wiki/Formal_language>formal linguistic structures</a> describing situations / actions / events, e.g. Commercial transfer of goods. Each frame provides a set of <a href=https://en.wikipedia.org/wiki/Character_(arts)>roles</a> corresponding to the situation participants, e.g. Buyer and Goods, and lexical units (LUs) words and phrases that can evoke this particular <a href=https://en.wikipedia.org/wiki/Frame_story>frame</a> in texts, e.g. Sell. The scarcity of annotated resources hinders wider adoption of <a href=https://en.wikipedia.org/wiki/Frame_semantics_(linguistics)>frame semantics</a> across languages and domains. We investigate a simple yet effective method, <a href=https://en.wikipedia.org/wiki/Lexical_substitution>lexical substitution</a> with word representation models, to automatically expand a small set of frame-annotated sentences with new words for their respective roles and LUs. We evaluate the expansion quality using <a href=https://en.wikipedia.org/wiki/FrameNet>FrameNet</a>. Contextualized models demonstrate overall superior performance compared to the non-contextualized ones on roles. However, the latter show comparable performance on the task of LU expansion.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.cogalex-1.1.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--cogalex-1--1 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.cogalex-1.1 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.cogalex-1.1/>Individual corpora predict fast memory retrieval during reading</a></strong><br><a href=/people/m/markus-j-hofmann/>Markus J. Hofmann</a>
|
<a href=/people/l/lara-muller/>Lara Müller</a>
|
<a href=/people/a/andre-rolke/>Andre Rölke</a>
|
<a href=/people/r/ralph-radach/>Ralph Radach</a>
|
<a href=/people/c/chris-biemann/>Chris Biemann</a><br><a href=/volumes/2020.cogalex-1/ class=text-muted>Proceedings of the Workshop on the Cognitive Aspects of the Lexicon</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--cogalex-1--1><div class="card-body p-3 small">The <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a>, from which a <a href=https://en.wikipedia.org/wiki/Predictive_modelling>predictive language model</a> is trained, can be considered the experience of a semantic system. We recorded everyday reading of two participants for two months on a tablet, generating individual corpus samples of 300/500 K tokens. Then we trained word2vec models from individual corpora and a 70 million-sentence newspaper corpus to obtain individual and norm-based long-term memory structure. To test whether individual corpora can make better predictions for a cognitive task of long-term memory retrieval, we generated stimulus materials consisting of 134 sentences with uncorrelated individual and norm-based word probabilities. For the subsequent eye tracking study 1-2 months later, our <a href=https://en.wikipedia.org/wiki/Regression_analysis>regression analyses</a> revealed that individual, but not norm-corpus-based word probabilities can account for first-fixation duration and first-pass gaze duration. Word length additionally affected gaze duration and total viewing duration. The results suggest that corpora representative for an individual&#8217;s <a href=https://en.wikipedia.org/wiki/Long-term_memory>long-term memory structure</a> can better explain reading performance than a norm corpus, and that recently acquired information is lexically accessed rapidly.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.213.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--213 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.213 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.213/>UHH-LT at SemEval-2020 Task 12 : Fine-Tuning of Pre-Trained Transformer Networks for Offensive Language Detection<span class=acl-fixed-case>UHH</span>-<span class=acl-fixed-case>LT</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 12: Fine-Tuning of Pre-Trained Transformer Networks for Offensive Language Detection</a></strong><br><a href=/people/g/gregor-wiedemann/>Gregor Wiedemann</a>
|
<a href=/people/s/seid-muhie-yimam/>Seid Muhie Yimam</a>
|
<a href=/people/c/chris-biemann/>Chris Biemann</a><br><a href=/volumes/2020.semeval-1/ class=text-muted>Proceedings of the Fourteenth Workshop on Semantic Evaluation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--213><div class="card-body p-3 small">Fine-tuning of pre-trained transformer networks such as BERT yield state-of-the-art results for text classification tasks. Typically, <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a> is performed on <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>task-specific training datasets</a> in a supervised manner. One can also fine-tune in unsupervised manner beforehand by further pre-training the masked language modeling (MLM) task. Hereby, in-domain data for <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised MLM</a> resembling the actual classification target dataset allows for domain adaptation of the <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a>. In this paper, we compare current pre-trained transformer networks with and without MLM fine-tuning on their performance for offensive language detection. Our MLM fine-tuned RoBERTa-based classifier officially ranks 1st in the SemEval 2020 Shared Task 12 for the <a href=https://en.wikipedia.org/wiki/English_language>English language</a>. Further experiments with the ALBERT model even surpass this result.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.91.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--91 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.91 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.91/>Exploring Amharic Sentiment Analysis from Social Media Texts : Building Annotation Tools and Classification Models<span class=acl-fixed-case>A</span>mharic Sentiment Analysis from Social Media Texts: Building Annotation Tools and Classification Models</a></strong><br><a href=/people/s/seid-muhie-yimam/>Seid Muhie Yimam</a>
|
<a href=/people/h/hizkiel-mitiku-alemayehu/>Hizkiel Mitiku Alemayehu</a>
|
<a href=/people/a/abinew-ayele/>Abinew Ayele</a>
|
<a href=/people/c/chris-biemann/>Chris Biemann</a><br><a href=/volumes/2020.coling-main/ class=text-muted>Proceedings of the 28th International Conference on Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--91><div class="card-body p-3 small">This paper presents the study of <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> for Amharic social media texts. As the number of social media users is ever-increasing, social media platforms would like to understand the latent meaning and sentiments of a text to enhance decision-making procedures. However, low-resource languages such as <a href=https://en.wikipedia.org/wiki/Amharic>Amharic</a> have received less attention due to several reasons such as lack of well-annotated datasets, unavailability of computing resources, and fewer or no expert researchers in the area. This research addresses three main research questions. We first explore the suitability of existing <a href=https://en.wikipedia.org/wiki/Programming_tool>tools</a> for the <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis task</a>. Annotation tools are scarce to support large-scale annotation tasks in <a href=https://en.wikipedia.org/wiki/Amharic>Amharic</a>. Also, the existing <a href=https://en.wikipedia.org/wiki/Crowdsourcing>crowdsourcing platforms</a> do not support Amharic text annotation. Hence, we build a social-network-friendly annotation tool called &#8216;ASAB&#8217; using the <a href=https://en.wikipedia.org/wiki/Telegram_(software)>Telegram bot</a>. We collect 9.4k tweets, where each tweet is annotated by three Telegram users. Moreover, we explore the suitability of machine learning approaches for Amharic sentiment analysis. The FLAIR deep learning text classifier, based on network embeddings that are computed from a distributional thesaurus, outperforms other <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised classifiers</a>. We further investigate the challenges in building a sentiment analysis system for <a href=https://en.wikipedia.org/wiki/Amharic>Amharic</a> and we found that the widespread usage of <a href=https://en.wikipedia.org/wiki/Sarcasm>sarcasm</a> and <a href=https://en.wikipedia.org/wiki/Literal_and_figurative_language>figurative speech</a> are the main issues in dealing with the problem. To advance the <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> research in <a href=https://en.wikipedia.org/wiki/Amharic>Amharic</a> and other related low-resource languages, we release the dataset, the annotation tool, source code, and models publicly under a permissive.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.728.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--728 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.728 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.728/>Word Sense Disambiguation for 158 Languages using Word Embeddings Only</a></strong><br><a href=/people/v/varvara-logacheva/>Varvara Logacheva</a>
|
<a href=/people/d/denis-teslenko/>Denis Teslenko</a>
|
<a href=/people/a/artem-shelmanov/>Artem Shelmanov</a>
|
<a href=/people/s/steffen-remus/>Steffen Remus</a>
|
<a href=/people/d/dmitry-ustalov/>Dmitry Ustalov</a>
|
<a href=/people/a/andrey-kutuzov/>Andrey Kutuzov</a>
|
<a href=/people/e/ekaterina-artemova/>Ekaterina Artemova</a>
|
<a href=/people/c/chris-biemann/>Chris Biemann</a>
|
<a href=/people/s/simone-paolo-ponzetto/>Simone Paolo Ponzetto</a>
|
<a href=/people/a/alexander-panchenko/>Alexander Panchenko</a><br><a href=/volumes/2020.lrec-1/ class=text-muted>Proceedings of the 12th Language Resources and Evaluation Conference</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--728><div class="card-body p-3 small">Disambiguation of word senses in context is easy for humans, but is a major challenge for automatic approaches. Sophisticated supervised and knowledge-based models were developed to solve this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. However, (i) the inherent <a href=https://en.wikipedia.org/wiki/Zipfian_distribution>Zipfian distribution</a> of supervised training instances for a given word and/or (ii) the quality of linguistic knowledge representations motivate the development of completely unsupervised and knowledge-free approaches to word sense disambiguation (WSD). They are particularly useful for under-resourced languages which do not have any resources for building either supervised and/or knowledge-based models. In this paper, we present a method that takes as input a standard pre-trained word embedding model and induces a fully-fledged word sense inventory, which can be used for disambiguation in context. We use this method to induce a collection of sense inventories for 158 languages on the basis of the original pre-trained fastText word embeddings by Grave et al., (2018), enabling WSD in these <a href=https://en.wikipedia.org/wiki/Language>languages</a>. Models and system are available online.</div></div><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S19-2137.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S19-2137 data-toggle=collapse aria-expanded=false aria-controls=abstract-S19-2137 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/S19-2137/>UHH-LT at SemEval-2019 Task 6 : Supervised vs. Unsupervised Transfer Learning for Offensive Language Detection<span class=acl-fixed-case>UHH</span>-<span class=acl-fixed-case>LT</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2019 Task 6: Supervised vs. Unsupervised Transfer Learning for Offensive Language Detection</a></strong><br><a href=/people/g/gregor-wiedemann/>Gregor Wiedemann</a>
|
<a href=/people/e/eugen-ruppert/>Eugen Ruppert</a>
|
<a href=/people/c/chris-biemann/>Chris Biemann</a><br><a href=/volumes/S19-2/ class=text-muted>Proceedings of the 13th International Workshop on Semantic Evaluation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S19-2137><div class="card-body p-3 small">We present a neural network based approach of <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a> for offensive language detection. For our <a href=https://en.wikipedia.org/wiki/System>system</a>, we compare two types of <a href=https://en.wikipedia.org/wiki/Knowledge_transfer>knowledge transfer</a> : supervised and unsupervised pre-training. Supervised pre-training of our bidirectional GRU-3-CNN architecture is performed as multi-task learning of parallel training of five different tasks. The selected tasks are supervised classification problems from public NLP resources with some overlap to offensive language such as sentiment detection, emoji classification, and aggressive language classification. Unsupervised transfer learning is performed with a thematic clustering of 40 M unlabeled tweets via <a href=https://en.wikipedia.org/wiki/Linear_predictive_coding>LDA</a>. Based on this <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>, pre-training is performed by predicting the main topic of a tweet. Results indicate that unsupervised transfer from large datasets performs slightly better than <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised training</a> on small &#8216;near target category&#8217; datasets. In the SemEval Task, our <a href=https://en.wikipedia.org/wiki/System>system</a> ranks 14 out of 103 participants.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4516.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4516 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4516 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-4516" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-4516/>Categorizing Comparative Sentences</a></strong><br><a href=/people/a/alexander-panchenko/>Alexander Panchenko</a>
|
<a href=/people/a/alexander-bondarenko/>Alexander Bondarenko</a>
|
<a href=/people/m/mirco-franzek/>Mirco Franzek</a>
|
<a href=/people/m/matthias-hagen/>Matthias Hagen</a>
|
<a href=/people/c/chris-biemann/>Chris Biemann</a><br><a href=/volumes/W19-45/ class=text-muted>Proceedings of the 6th Workshop on Argument Mining</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4516><div class="card-body p-3 small">We tackle the tasks of automatically identifying comparative sentences and categorizing the intended preference (e.g., <a href=https://en.wikipedia.org/wiki/Python_(programming_language)>Python</a> has better NLP libraries than <a href=https://en.wikipedia.org/wiki/MATLAB>MATLAB Python</a>, better, <a href=https://en.wikipedia.org/wiki/MATLAB>MATLAB</a>). To this end, we manually annotate 7,199 sentences for 217 distinct target item pairs from several domains (27 % of the sentences contain an oriented comparison in the sense of better or worse). A gradient boosting model based on pre-trained sentence embeddings reaches an F1 score of 85 % in our experimental evaluation. The model can be used to extract comparative sentences for pro / con argumentation in comparative / argument search engines or debating technologies.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-4017.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-4017 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-4017 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-4017" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N19-4017/>LT Expertfinder : An Evaluation Framework for Expert Finding Methods<span class=acl-fixed-case>LT</span> Expertfinder: An Evaluation Framework for Expert Finding Methods</a></strong><br><a href=/people/t/tim-fischer/>Tim Fischer</a>
|
<a href=/people/s/steffen-remus/>Steffen Remus</a>
|
<a href=/people/c/chris-biemann/>Chris Biemann</a><br><a href=/volumes/N19-4/ class=text-muted>Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-4017><div class="card-body p-3 small">Expert finding is the task of ranking persons for a predefined topic or search query. Finding experts for a specified area is an important task and has attracted much attention in the <a href=https://en.wikipedia.org/wiki/Information_retrieval>information retrieval community</a>. Most approaches for this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> are evaluated in a supervised fashion, which depend on predefined topics of interest as well as gold standard expert rankings. Famous representatives of such datasets are enriched versions of <a href=https://en.wikipedia.org/wiki/DBLP>DBLP</a> provided by the ArnetMiner projet or the W3C Corpus of TREC. However, manually ranking experts can be considered highly subjective and detailed rankings are hardly distinguishable. Evaluating these <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> does not necessarily guarantee a good or bad performance of the <a href=https://en.wikipedia.org/wiki/System>system</a>. Particularly for <a href=https://en.wikipedia.org/wiki/Dynamical_system>dynamic systems</a>, where topics are not predefined but formulated as a <a href=https://en.wikipedia.org/wiki/Web_search_query>search query</a>, we believe a more informative approach is to perform <a href=https://en.wikipedia.org/wiki/User_study>user studies</a> for directly comparing different methods in the same view. In order to accomplish this in a user-friendly way, we present the LT Expert Finder web-application, which is equipped with various query-based expert finding methods that can be easily extended, a detailed expert profile view, detailed evidence in form of relevant documents and statistics, and an evaluation component that allows the qualitative comparison between different rankings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1316.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1316 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1316 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-1316/>On the Compositionality Prediction of Noun Phrases using Poincar Embeddings</a></strong><br><a href=/people/a/abhik-jana/>Abhik Jana</a>
|
<a href=/people/d/dima-puzyrev/>Dima Puzyrev</a>
|
<a href=/people/a/alexander-panchenko/>Alexander Panchenko</a>
|
<a href=/people/p/pawan-goyal/>Pawan Goyal</a>
|
<a href=/people/c/chris-biemann/>Chris Biemann</a>
|
<a href=/people/a/animesh-mukherjee/>Animesh Mukherjee</a><br><a href=/volumes/P19-1/ class=text-muted>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1316><div class="card-body p-3 small">The compositionality degree of multiword expressions indicates to what extent the meaning of a phrase can be derived from the meaning of its constituents and their <a href=https://en.wikipedia.org/wiki/Grammatical_relation>grammatical relations</a>. Prediction of (non)-compositionality is a task that has been frequently addressed with distributional semantic models. We introduce a novel technique to blend hierarchical information with <a href=https://en.wikipedia.org/wiki/Distribution_(mathematics)>distributional information</a> for predicting compositionality. In particular, we use hypernymy information of the multiword and its constituents encoded in the form of the recently introduced Poincar embeddings in addition to the distributional information to detect compositionality for <a href=https://en.wikipedia.org/wiki/Noun_phrase>noun phrases</a>. Using a <a href=https://en.wikipedia.org/wiki/Weighted_arithmetic_mean>weighted average</a> of the distributional similarity and a Poincar similarity function, we obtain consistent and substantial, statistically significant improvement across three gold standard datasets over state-of-the-art models based on distributional information only. Unlike traditional approaches that solely use an <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised setting</a>, we have also framed the <a href=https://en.wikipedia.org/wiki/Problem_solving>problem</a> as a <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised task</a>, obtaining comparable improvements. Further, we publicly release our Poincar embeddings, which are trained on the output of handcrafted lexical-syntactic patterns on a large corpus.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1325.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1325 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1325 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1325" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-1325/>Making Fast Graph-based Algorithms with Graph Metric Embeddings</a></strong><br><a href=/people/a/andrey-kutuzov/>Andrey Kutuzov</a>
|
<a href=/people/m/mohammad-dorgham/>Mohammad Dorgham</a>
|
<a href=/people/o/oleksiy-oliynyk/>Oleksiy Oliynyk</a>
|
<a href=/people/c/chris-biemann/>Chris Biemann</a>
|
<a href=/people/a/alexander-panchenko/>Alexander Panchenko</a><br><a href=/volumes/P19-1/ class=text-muted>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1325><div class="card-body p-3 small">Graph measures, such as node distances, are inefficient to compute. We explore dense vector representations as an effective way to approximate the same information. We introduce a simple yet efficient and effective approach for learning <a href=https://en.wikipedia.org/wiki/Graph_embedding>graph embeddings</a>. Instead of directly operating on the <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph structure</a>, our method takes structural measures of pairwise node similarities into account and learns dense node representations reflecting user-defined graph distance measures, such as e.g. the shortest path distance or distance measures that take information beyond the <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph structure</a> into account. We demonstrate a speed-up of several orders of magnitude when predicting word similarity by vector operations on our embeddings as opposed to directly computing the respective path-based measures, while outperforming various other graph embeddings on semantic similarity and word sense disambiguation tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-2044.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-2044 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-2044 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-2044/>Improving Neural Entity Disambiguation with Graph Embeddings</a></strong><br><a href=/people/o/ozge-sevgili/>Özge Sevgili</a>
|
<a href=/people/a/alexander-panchenko/>Alexander Panchenko</a>
|
<a href=/people/c/chris-biemann/>Chris Biemann</a><br><a href=/volumes/P19-2/ class=text-muted>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-2044><div class="card-body p-3 small">Entity Disambiguation (ED) is the task of linking an ambiguous entity mention to a corresponding entry in a <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge base</a>. Current methods have mostly focused on unstructured text data to learn representations of entities, however, there is structured information in the knowledge base itself that should be useful to disambiguate entities. In this work, we propose a method that uses <a href=https://en.wikipedia.org/wiki/Graph_embedding>graph embeddings</a> for integrating structured information from the <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge base</a> with <a href=https://en.wikipedia.org/wiki/Unstructured_data>unstructured information</a> from text-based representations. Our experiments confirm that graph embeddings trained on a graph of hyperlinks between Wikipedia articles improve the performances of simple feed-forward neural ED model and a state-of-the-art neural ED system.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-3031.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-3031 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-3031 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-3031" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-3031/>TARGER : Neural Argument Mining at Your Fingertips<span class=acl-fixed-case>TARGER</span>: Neural Argument Mining at Your Fingertips</a></strong><br><a href=/people/a/artem-chernodub/>Artem Chernodub</a>
|
<a href=/people/o/oleksiy-oliynyk/>Oleksiy Oliynyk</a>
|
<a href=/people/p/philipp-heidenreich/>Philipp Heidenreich</a>
|
<a href=/people/a/alexander-bondarenko/>Alexander Bondarenko</a>
|
<a href=/people/m/matthias-hagen/>Matthias Hagen</a>
|
<a href=/people/c/chris-biemann/>Chris Biemann</a>
|
<a href=/people/a/alexander-panchenko/>Alexander Panchenko</a><br><a href=/volumes/P19-3/ class=text-muted>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: System Demonstrations</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-3031><div class="card-body p-3 small">We present TARGER, an open source neural argument mining framework for tagging arguments in free input texts and for keyword-based retrieval of arguments from an argument-tagged web-scale corpus. The currently available models are pre-trained on three recent argument mining datasets and enable the use of neural argument mining without any reproducibility effort on the user&#8217;s side. The open source code ensures portability to other domains and use cases.</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-2009.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-2009 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-2009 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-2009/>Demonstrating Par4Sem-A Semantic Writing Aid with Adaptive Paraphrasing<span class=acl-fixed-case>P</span>ar4<span class=acl-fixed-case>S</span>em - A Semantic Writing Aid with Adaptive Paraphrasing</a></strong><br><a href=/people/s/seid-muhie-yimam/>Seid Muhie Yimam</a>
|
<a href=/people/c/chris-biemann/>Chris Biemann</a><br><a href=/volumes/D18-2/ class=text-muted>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-2009><div class="card-body p-3 small">In this paper, we present Par4Sem, a semantic writing aid tool based on adaptive paraphrasing. Unlike many annotation tools that are primarily used to collect training examples, Par4Sem is integrated into a real word application, in this case a writing aid tool, in order to collect training examples from usage data. Par4Sem is a tool, which supports an adaptive, iterative, and interactive process where the underlying <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning models</a> are updated for each iteration using new training examples from usage data. After motivating the use of ever-learning tools in NLP applications, we evaluate Par4Sem by adopting it to a text simplification task through mere usage.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-2014.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-2014 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-2014 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-2014/>A Multilingual Information Extraction Pipeline for Investigative Journalism</a></strong><br><a href=/people/g/gregor-wiedemann/>Gregor Wiedemann</a>
|
<a href=/people/s/seid-muhie-yimam/>Seid Muhie Yimam</a>
|
<a href=/people/c/chris-biemann/>Chris Biemann</a><br><a href=/volumes/D18-2/ class=text-muted>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-2014><div class="card-body p-3 small">We introduce an advanced information extraction pipeline to automatically process very large collections of <a href=https://en.wikipedia.org/wiki/Unstructured_data>unstructured textual data</a> for the purpose of <a href=https://en.wikipedia.org/wiki/Investigative_journalism>investigative journalism</a>. The <a href=https://en.wikipedia.org/wiki/Pipeline_(computing)>pipeline</a> serves as a new input processor for the upcoming major release of our New / s / leak 2.0 software, which we develop in cooperation with a large German news organization. The use case is that journalists receive a large collection of files up to several Gigabytes containing unknown contents. Collections may originate either from official disclosures of documents, e.g. Freedom of Information Act requests, or unofficial data leaks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/J18-3005.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-J18-3005 data-toggle=collapse aria-expanded=false aria-controls=abstract-J18-3005 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/J18-3005/>Using <a href=https://en.wikipedia.org/wiki/Semantics>Semantics</a> for Granularities of Tokenization</a></strong><br><a href=/people/m/martin-riedl/>Martin Riedl</a>
|
<a href=/people/c/chris-biemann/>Chris Biemann</a><br><a href=/volumes/J18-3/ class=text-muted>Computational Linguistics, Volume 44, Issue 3 - September 2018</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-J18-3005><div class="card-body p-3 small">Depending on downstream applications, it is advisable to extend the notion of <a href=https://en.wikipedia.org/wiki/Lexical_analysis>tokenization</a> from low-level character-based token boundary detection to identification of meaningful and useful language units. This entails both identifying units composed of several single words that form a several single words that form a, as well as splitting single-word compounds into their meaningful parts. In this article, we introduce unsupervised and knowledge-free methods for these two <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a>. The main novelty of our research is based on the fact that methods are primarily based on distributional similarity, of which we use two flavors : a sparse count-based and a dense neural-based distributional semantic model. First, we introduce DRUID, which is a method for detecting MWEs. The evaluation on MWE-annotated data sets in two languages and newly extracted evaluation data sets for 32 languages shows that DRUID compares favorably over previous methods not utilizing distributional information. Second, we present SECOS, an <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> for decompounding close compounds. In an evaluation of four dedicated decompounding data sets across four languages and on data sets extracted from <a href=https://en.wikipedia.org/wiki/Wiktionary>Wiktionary</a> for 14 languages, we demonstrate the superiority of our approach over unsupervised baselines, sometimes even matching the performance of previous language-specific and supervised methods. In a final experiment, we show how both decompounding and MWE information can be used in <a href=https://en.wikipedia.org/wiki/Information_retrieval>information retrieval</a>. Here, we obtain the best results when combining <a href=https://en.wikipedia.org/wiki/Word>word information</a> with MWEs and the <a href=https://en.wikipedia.org/wiki/Compound_(linguistics)>compound parts</a> in a bag-of-words retrieval set-up.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S18-1163.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S18-1163 data-toggle=collapse aria-expanded=false aria-controls=abstract-S18-1163 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/S18-1163/>BomJi at SemEval-2018 Task 10 : Combining Vector-, Pattern- and Graph-based Information to Identify Discriminative Attributes<span class=acl-fixed-case>B</span>om<span class=acl-fixed-case>J</span>i at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2018 Task 10: Combining Vector-, Pattern- and Graph-based Information to Identify Discriminative Attributes</a></strong><br><a href=/people/e/enrico-santus/>Enrico Santus</a>
|
<a href=/people/c/chris-biemann/>Chris Biemann</a>
|
<a href=/people/e/emmanuele-chersoni/>Emmanuele Chersoni</a><br><a href=/volumes/S18-1/ class=text-muted>Proceedings of The 12th International Workshop on Semantic Evaluation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S18-1163><div class="card-body p-3 small">This paper describes BomJi, a <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised system</a> for capturing discriminative attributes in word pairs (e.g. yellow as discriminative for banana over watermelon). The system relies on an XGB classifier trained on carefully engineered graph-, pattern- and word embedding-based features. It participated in the SemEval-2018 Task 10 on Capturing Discriminative Attributes, achieving an F1 score of 0.73 and ranking 2nd out of 26 participant systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0507.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0507 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0507 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0507/>A Report on the Complex Word Identification Shared Task 2018</a></strong><br><a href=/people/s/seid-muhie-yimam/>Seid Muhie Yimam</a>
|
<a href=/people/c/chris-biemann/>Chris Biemann</a>
|
<a href=/people/s/shervin-malmasi/>Shervin Malmasi</a>
|
<a href=/people/g/gustavo-paetzold/>Gustavo Paetzold</a>
|
<a href=/people/l/lucia-specia/>Lucia Specia</a>
|
<a href=/people/s/sanja-stajner/>Sanja Štajner</a>
|
<a href=/people/a/anais-tack/>Anaïs Tack</a>
|
<a href=/people/m/marcos-zampieri/>Marcos Zampieri</a><br><a href=/volumes/W18-05/ class=text-muted>Proceedings of the Thirteenth Workshop on Innovative Use of NLP for Building Educational Applications</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0507><div class="card-body p-3 small">We report the findings of the second Complex Word Identification (CWI) shared task organized as part of the BEA workshop co-located with NAACL-HLT&#8217;2018. The second CWI shared task featured multilingual and multi-genre datasets divided into four tracks : English monolingual, German monolingual, Spanish monolingual, and a multilingual track with a French test set, and two tasks : <a href=https://en.wikipedia.org/wiki/Binary_classification>binary classification</a> and probabilistic classification. A total of 12 teams submitted their results in different task / track combinations and 11 of them wrote system description papers that are referred to in this report and appear in the BEA workshop proceedings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-3027.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-3027 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-3027 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-3027/>Document-based Recommender System for Job Postings using Dense Representations</a></strong><br><a href=/people/a/ahmed-elsafty/>Ahmed Elsafty</a>
|
<a href=/people/m/martin-riedl/>Martin Riedl</a>
|
<a href=/people/c/chris-biemann/>Chris Biemann</a><br><a href=/volumes/N18-3/ class=text-muted>Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 3 (Industry Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-3027><div class="card-body p-3 small">Job boards and professional social networks heavily use <a href=https://en.wikipedia.org/wiki/Recommender_system>recommender systems</a> in order to better support users in exploring job advertisements. Detecting the similarity between job advertisements is important for job recommendation systems as it allows, for example, the application of item-to-item based recommendations. In this work, we research the usage of dense vector representations to enhance a large-scale job recommendation system and to rank German job advertisements regarding their similarity. We follow a two-folded evaluation scheme : (1) we exploit historic user interactions to automatically create a <a href=https://en.wikipedia.org/wiki/Data_set>dataset of similar jobs</a> that enables an offline evaluation. (2) In addition, we conduct an online A / B test and evaluate the best performing method on our platform reaching more than 1 million users. We achieve the best results by combining <a href=https://en.wikipedia.org/wiki/International_Standard_Classification_of_Occupations>job titles</a> with <a href=https://en.wikipedia.org/wiki/International_Standard_Classification_of_Occupations>full-text job descriptions</a>. In particular, this method builds dense document representation using words of the titles to weigh the importance of words of the full-text description. In the online evaluation, this approach allows us to increase the <a href=https://en.wikipedia.org/wiki/Click-through_rate>click-through rate</a> on <a href=https://en.wikipedia.org/wiki/Job_description>job recommendations</a> for active users by 8.0 %.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2010.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2010 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2010 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2010.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2010.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-2010" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P18-2010/>Unsupervised Semantic Frame Induction using Triclustering</a></strong><br><a href=/people/d/dmitry-ustalov/>Dmitry Ustalov</a>
|
<a href=/people/a/alexander-panchenko/>Alexander Panchenko</a>
|
<a href=/people/a/andrey-kutuzov/>Andrey Kutuzov</a>
|
<a href=/people/c/chris-biemann/>Chris Biemann</a>
|
<a href=/people/s/simone-paolo-ponzetto/>Simone Paolo Ponzetto</a><br><a href=/volumes/P18-2/ class=text-muted>Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2010><div class="card-body p-3 small">We use dependency triples automatically extracted from a Web-scale corpus to perform unsupervised semantic frame induction. We cast the frame induction problem as a triclustering problem that is a generalization of <a href=https://en.wikipedia.org/wiki/Cluster_analysis>clustering</a> for triadic data. Our replicable benchmarks demonstrate that the proposed graph-based approach, Triframes, shows state-of-the art results on this task on a FrameNet-derived dataset and performing on par with competitive methods on a verb class clustering task.</div></div><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-2068.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-2068 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-2068 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-2068/>CWIG3G2-Complex Word Identification Task across Three Text Genres and Two User Groups<span class=acl-fixed-case>CWIG</span>3<span class=acl-fixed-case>G</span>2 - Complex Word Identification Task across Three Text Genres and Two User Groups</a></strong><br><a href=/people/s/seid-muhie-yimam/>Seid Muhie Yimam</a>
|
<a href=/people/s/sanja-stajner/>Sanja Štajner</a>
|
<a href=/people/m/martin-riedl/>Martin Riedl</a>
|
<a href=/people/c/chris-biemann/>Chris Biemann</a><br><a href=/volumes/I17-2/ class=text-muted>Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-2068><div class="card-body p-3 small">Complex word identification (CWI) is an important task in <a href=https://en.wikipedia.org/wiki/Accessibility>text accessibility</a>. However, due to the scarcity of CWI datasets, previous studies have only addressed this problem on Wikipedia sentences and have solely taken into account the needs of non-native English speakers. We collect a new CWI dataset (CWIG3G2) covering three text genres News, WikiNews, and Wikipedia) annotated by both native and non-native English speakers. Unlike previous <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>, we cover <a href=https://en.wikipedia.org/wiki/Phrase>single words</a>, as well as <a href=https://en.wikipedia.org/wiki/Phrase>complex phrases</a>, and present them for judgment in a paragraph context. We present the first study on cross-genre and cross-group CWI, showing measurable influences in native language and genre types.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1145.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1145 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1145 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P17-1145" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P17-1145/>Watset : Automatic Induction of Synsets from a Graph of Synonyms<span class=acl-fixed-case>W</span>atset: Automatic Induction of Synsets from a Graph of Synonyms</a></strong><br><a href=/people/d/dmitry-ustalov/>Dmitry Ustalov</a>
|
<a href=/people/a/alexander-panchenko/>Alexander Panchenko</a>
|
<a href=/people/c/chris-biemann/>Chris Biemann</a><br><a href=/volumes/P17-1/ class=text-muted>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1145><div class="card-body p-3 small">This paper presents a new graph-based approach that induces synsets using <a href=https://en.wikipedia.org/wiki/Synonym>synonymy dictionaries</a> and <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a>. First, we build a weighted graph of synonyms extracted from commonly available resources, such as <a href=https://en.wikipedia.org/wiki/Wiktionary>Wiktionary</a>. Second, we apply <a href=https://en.wikipedia.org/wiki/Word-sense_induction>word sense induction</a> to deal with ambiguous words. Finally, we cluster the disambiguated version of the ambiguous input graph into <a href=https://en.wikipedia.org/wiki/Synset>synsets</a>. Our meta-clustering approach lets us use an efficient hard clustering algorithm to perform a fuzzy clustering of the graph. Despite its simplicity, our approach shows excellent results, outperforming five competitive state-of-the-art methods in terms of <a href=https://en.wikipedia.org/wiki/F-score>F-score</a> on three gold standard datasets for <a href=https://en.wikipedia.org/wiki/English_language>English</a> and <a href=https://en.wikipedia.org/wiki/Russian_language>Russian</a> derived from large-scale manually constructed lexical resources.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S17-2009.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S17-2009 data-toggle=collapse aria-expanded=false aria-controls=abstract-S17-2009 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=S17-2009" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/S17-2009/>IIT-UHH at SemEval-2017 Task 3 : Exploring Multiple Features for Community Question Answering and Implicit Dialogue Identification<span class=acl-fixed-case>IIT</span>-<span class=acl-fixed-case>UHH</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2017 Task 3: Exploring Multiple Features for Community Question Answering and Implicit Dialogue Identification</a></strong><br><a href=/people/t/titas-nandi/>Titas Nandi</a>
|
<a href=/people/c/chris-biemann/>Chris Biemann</a>
|
<a href=/people/s/seid-muhie-yimam/>Seid Muhie Yimam</a>
|
<a href=/people/d/deepak-gupta/>Deepak Gupta</a>
|
<a href=/people/s/sarah-kohail/>Sarah Kohail</a>
|
<a href=/people/a/asif-ekbal/>Asif Ekbal</a>
|
<a href=/people/p/pushpak-bhattacharyya/>Pushpak Bhattacharyya</a><br><a href=/volumes/S17-2/ class=text-muted>Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S17-2009><div class="card-body p-3 small">In this paper we present the system for Answer Selection and Ranking in Community Question Answering, which we build as part of our participation in SemEval-2017 Task 3. We develop a Support Vector Machine (SVM) based system that makes use of textual, domain-specific, word-embedding and topic-modeling features. In addition, we propose a novel <a href=https://en.wikipedia.org/wiki/Methodology>method</a> for dialogue chain identification in <a href=https://en.wikipedia.org/wiki/Internet_forum>comment threads</a>. Our primary submission won subtask C, outperforming other systems in all the primary evaluation metrics. We performed well in other English subtasks, ranking third in subtask A and eighth in subtask B. We also developed open source toolkits for all the three English subtasks by the name cQARank [ ].<url>https://github.com/TitasNandi/cQARank</url>].\n</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S17-2025.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S17-2025 data-toggle=collapse aria-expanded=false aria-controls=abstract-S17-2025 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/S17-2025/>STS-UHH at SemEval-2017 Task 1 : Scoring Semantic Textual Similarity Using Supervised and Unsupervised Ensemble<span class=acl-fixed-case>STS</span>-<span class=acl-fixed-case>UHH</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2017 Task 1: Scoring Semantic Textual Similarity Using Supervised and Unsupervised Ensemble</a></strong><br><a href=/people/s/sarah-kohail/>Sarah Kohail</a>
|
<a href=/people/a/amr-rekaby-salama/>Amr Rekaby Salama</a>
|
<a href=/people/c/chris-biemann/>Chris Biemann</a><br><a href=/volumes/S17-2/ class=text-muted>Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S17-2025><div class="card-body p-3 small">This paper reports the STS-UHH participation in the SemEval 2017 shared Task 1 of Semantic Textual Similarity (STS). Overall, we submitted 3 runs covering monolingual and cross-lingual STS tracks. Our participation involves two approaches : unsupervised approach, which estimates a word alignment-based similarity score, and supervised approach, which combines dependency graph similarity and coverage features with lexical similarity measures using regression methods. We also present a way on ensembling both <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>. Out of 84 submitted runs, our team best multi-lingual run has been ranked 12th in overall performance with correlation of 0.61, 7th among 31 participating teams.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-2016.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-2016 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-2016 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D17-2016" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D17-2016/>Unsupervised, Knowledge-Free, and Interpretable Word Sense Disambiguation</a></strong><br><a href=/people/a/alexander-panchenko/>Alexander Panchenko</a>
|
<a href=/people/f/fide-marten/>Fide Marten</a>
|
<a href=/people/e/eugen-ruppert/>Eugen Ruppert</a>
|
<a href=/people/s/stefano-faralli/>Stefano Faralli</a>
|
<a href=/people/d/dmitry-ustalov/>Dmitry Ustalov</a>
|
<a href=/people/s/simone-paolo-ponzetto/>Simone Paolo Ponzetto</a>
|
<a href=/people/c/chris-biemann/>Chris Biemann</a><br><a href=/volumes/D17-2/ class=text-muted>Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-2016><div class="card-body p-3 small">Interpretability of a <a href=https://en.wikipedia.org/wiki/Predictive_modelling>predictive model</a> is a powerful feature that gains the trust of users in the correctness of the predictions. In word sense disambiguation (WSD), knowledge-based systems tend to be much more interpretable than knowledge-free counterparts as they rely on the wealth of manually-encoded elements representing word senses, such as hypernyms, usage examples, and images. We present a WSD system that bridges the gap between these two so far disconnected groups of methods. Namely, our <a href=https://en.wikipedia.org/wiki/System>system</a>, providing access to several state-of-the-art WSD models, aims to be interpretable as a knowledge-based system while it remains completely unsupervised and knowledge-free. The presented tool features a Web interface for all-word disambiguation of texts that makes the sense predictions human readable by providing interpretable word sense inventories, sense representations, and disambiguation results. We provide a public API, enabling seamless integration.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1009.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1009 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1009 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1009/>Unsupervised Does Not Mean Uninterpretable : The Case for Word Sense Induction and Disambiguation</a></strong><br><a href=/people/a/alexander-panchenko/>Alexander Panchenko</a>
|
<a href=/people/e/eugen-ruppert/>Eugen Ruppert</a>
|
<a href=/people/s/stefano-faralli/>Stefano Faralli</a>
|
<a href=/people/s/simone-paolo-ponzetto/>Simone Paolo Ponzetto</a>
|
<a href=/people/c/chris-biemann/>Chris Biemann</a><br><a href=/volumes/E17-1/ class=text-muted>Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1009><div class="card-body p-3 small">The current trend in <a href=https://en.wikipedia.org/wiki/Neuro-linguistic_programming>NLP</a> is the use of highly opaque models, e.g. neural networks and <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a>. While these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> yield state-of-the-art results on a range of <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a>, their drawback is poor interpretability. On the example of word sense induction and disambiguation (WSID), we show that it is possible to develop an interpretable <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> that matches the state-of-the-art models in <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>. Namely, we present an unsupervised, knowledge-free WSID approach, which is interpretable at three levels : word sense inventory, sense feature representations, and disambiguation procedure. Experiments show that our model performs on par with state-of-the-art word sense embeddings and other <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised systems</a> while offering the possibility to justify its decisions in <a href=https://en.wikipedia.org/wiki/Human-readable_medium>human-readable form</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1056.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1056 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1056 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1056/>The ContrastMedium Algorithm : Taxonomy Induction From Noisy Knowledge Graphs With Just A Few Links<span class=acl-fixed-case>C</span>ontrast<span class=acl-fixed-case>M</span>edium Algorithm: Taxonomy Induction From Noisy Knowledge Graphs With Just A Few Links</a></strong><br><a href=/people/s/stefano-faralli/>Stefano Faralli</a>
|
<a href=/people/a/alexander-panchenko/>Alexander Panchenko</a>
|
<a href=/people/c/chris-biemann/>Chris Biemann</a>
|
<a href=/people/s/simone-paolo-ponzetto/>Simone Paolo Ponzetto</a><br><a href=/volumes/E17-1/ class=text-muted>Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1056><div class="card-body p-3 small">In this paper, we present ContrastMedium, an <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> that transforms noisy semantic networks into full-fledged, clean taxonomies. ContrastMedium is able to identify the embedded taxonomy structure from a noisy knowledge graph without explicit human supervision such as, for instance, a set of manually selected input root and leaf concepts. This is achieved by leveraging structural information from a companion reference taxonomy, to which the input <a href=https://en.wikipedia.org/wiki/Knowledge_graph>knowledge graph</a> is linked (either automatically or manually). When used in conjunction with methods for hypernym acquisition and knowledge base linking, our methodology provides a complete solution for end-to-end taxonomy induction. We conduct experiments using automatically acquired knowledge graphs, as well as a SemEval benchmark, and show that our method is able to achieve high performance on the task of taxonomy induction.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2087.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2087 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2087 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=E17-2087" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/E17-2087/>Negative Sampling Improves Hypernymy Extraction Based on Projection Learning</a></strong><br><a href=/people/d/dmitry-ustalov/>Dmitry Ustalov</a>
|
<a href=/people/n/nikolay-arefyev/>Nikolay Arefyev</a>
|
<a href=/people/c/chris-biemann/>Chris Biemann</a>
|
<a href=/people/a/alexander-panchenko/>Alexander Panchenko</a><br><a href=/volumes/E17-2/ class=text-muted>Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2087><div class="card-body p-3 small">We present a new approach to extraction of hypernyms based on projection learning and <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a>. In contrast to <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification-based approaches</a>, <a href=https://en.wikipedia.org/wiki/Projection_(linear_algebra)>projection-based methods</a> require no candidate <a href=https://en.wikipedia.org/wiki/Hyponymy_and_hypernymy>hyponym-hypernym pairs</a>. While it is natural to use both positive and negative training examples in supervised relation extraction, the impact of positive examples on hypernym prediction was not studied so far. In this paper, we show that explicit negative examples used for <a href=https://en.wikipedia.org/wiki/Regularization_(mathematics)>regularization</a> of the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> significantly improve performance compared to the state-of-the-art approach of Fu et al. (2014) on three <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> from different languages.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Chris+Biemann" title="Search for 'Chris Biemann' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/a/alexander-panchenko/ class=align-middle>Alexander Panchenko</a>
<span class="badge badge-secondary align-middle ml-2">14</span></li><li class=list-group-item><a href=/people/s/seid-muhie-yimam/ class=align-middle>Seid Muhie Yimam</a>
<span class="badge badge-secondary align-middle ml-2">8</span></li><li class=list-group-item><a href=/people/d/dmitry-ustalov/ class=align-middle>Dmitry Ustalov</a>
<span class="badge badge-secondary align-middle ml-2">5</span></li><li class=list-group-item><a href=/people/s/simone-paolo-ponzetto/ class=align-middle>Simone Paolo Ponzetto</a>
<span class="badge badge-secondary align-middle ml-2">5</span></li><li class=list-group-item><a href=/people/g/gregor-wiedemann/ class=align-middle>Gregor Wiedemann</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/m/martin-riedl/ class=align-middle>Martin Riedl</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/a/alexander-bondarenko/ class=align-middle>Alexander Bondarenko</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/e/eugen-ruppert/ class=align-middle>Eugen Ruppert</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/s/stefano-faralli/ class=align-middle>Stefano Faralli</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/a/andrey-kutuzov/ class=align-middle>Andrey Kutuzov</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/s/sanja-stajner/ class=align-middle>Sanja Štajner</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/a/artem-shelmanov/ class=align-middle>Artem Shelmanov</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/v/varvara-logacheva/ class=align-middle>Varvara Logacheva</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/s/sarah-kohail/ class=align-middle>Sarah Kohail</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/m/matthias-hagen/ class=align-middle>Matthias Hagen</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/s/steffen-remus/ class=align-middle>Steffen Remus</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/o/oleksiy-oliynyk/ class=align-middle>Oleksiy Oliynyk</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/s/saba-anwar/ class=align-middle>Saba Anwar</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/marlo-haering/ class=align-middle>Marlo Haering</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jakob-smedegaard-andersen/ class=align-middle>Jakob Smedegaard Andersen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/w/wiebke-loosen/ class=align-middle>Wiebke Loosen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/b/benjamin-milde/ class=align-middle>Benjamin Milde</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/tim-pietz/ class=align-middle>Tim Pietz</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/christian-stocker/ class=align-middle>Christian Stöcker</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/o/olaf-zukunft/ class=align-middle>Olaf Zukunft</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/w/walid-maalej/ class=align-middle>Walid Maalej</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/v/viktoriia-chekalina/ class=align-middle>Viktoriia Chekalina</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/meriem-beloucif/ class=align-middle>Meriem Beloucif</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/markus-j-hofmann/ class=align-middle>Markus J. Hofmann</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/lara-muller/ class=align-middle>Lara Müller</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/andre-rolke/ class=align-middle>Andre Rölke</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/ralph-radach/ class=align-middle>Ralph Radach</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/titas-nandi/ class=align-middle>Titas Nandi</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/deepak-gupta/ class=align-middle>Deepak Gupta</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/asif-ekbal/ class=align-middle>Asif Ekbal</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/p/pushpak-bhattacharyya/ class=align-middle>Pushpak Bhattacharyya</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/amr-rekaby-salama/ class=align-middle>Amr Rekaby Salama</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/f/fide-marten/ class=align-middle>Fide Marten</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/max-wiechmann/ class=align-middle>Max Wiechmann</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/e/enrico-santus/ class=align-middle>Enrico Santus</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/e/emmanuele-chersoni/ class=align-middle>Emmanuele Chersoni</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/shervin-malmasi/ class=align-middle>Shervin Malmasi</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/g/gustavo-paetzold/ class=align-middle>Gustavo Paetzold</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/lucia-specia/ class=align-middle>Lucia Specia</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/anais-tack/ class=align-middle>Anaïs Tack</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/marcos-zampieri/ class=align-middle>Marcos Zampieri</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/mirco-franzek/ class=align-middle>Mirco Franzek</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/hizkiel-mitiku-alemayehu/ class=align-middle>Hizkiel Mitiku Alemayehu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/abinew-ayele/ class=align-middle>Abinew Ayele</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/tim-fischer/ class=align-middle>Tim Fischer</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/ahmed-elsafty/ class=align-middle>Ahmed Elsafty</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/denis-teslenko/ class=align-middle>Denis Teslenko</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/e/ekaterina-artemova/ class=align-middle>Ekaterina Artemova</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/n/nikolay-arefyev/ class=align-middle>Nikolay Arefyev</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/abhik-jana/ class=align-middle>Abhik Jana</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/dima-puzyrev/ class=align-middle>Dima Puzyrev</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/p/pawan-goyal/ class=align-middle>Pawan Goyal</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/animesh-mukherjee/ class=align-middle>Animesh Mukherjee</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/mohammad-dorgham/ class=align-middle>Mohammad Dorgham</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/o/ozge-sevgili/ class=align-middle>Özge Sevgili</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/artem-chernodub/ class=align-middle>Artem Chernodub</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/p/philipp-heidenreich/ class=align-middle>Philipp Heidenreich</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/acl/ class=align-middle>ACL</a><span class="badge badge-secondary align-middle ml-2">6</span></li><li class=list-group-item><a href=/venues/eacl/ class=align-middle>EACL</a><span class="badge badge-secondary align-middle ml-2">5</span></li><li class=list-group-item><a href=/venues/semeval/ class=align-middle>SemEval</a><span class="badge badge-secondary align-middle ml-2">5</span></li><li class=list-group-item><a href=/venues/emnlp/ class=align-middle>EMNLP</a><span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/venues/naacl/ class=align-middle>NAACL</a><span class="badge badge-secondary align-middle ml-2">3</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-venues aria-expanded=false aria-controls=more-venues>show all...</li><div class="collapse border-top" id=more-venues><li class=list-group-item><a href=/venues/ws/ class=align-middle>WS</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/ijcnlp/ class=align-middle>IJCNLP</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/pam/ class=align-middle>PaM</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/cogalex/ class=align-middle>CogALex</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/cl/ class=align-middle>CL</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/coling/ class=align-middle>COLING</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/lrec/ class=align-middle>LREC</a><span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>