<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Chris Callison-Burch - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Chris</span> <span class=font-weight-bold>Callison-Burch</span></h2><hr><div class=row><div class=col-lg-9><h4>2022</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.acl-long.577.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--acl-long--577 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.acl-long.577 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.acl-long.577" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2022.acl-long.577/>Deduplicating Training Data Makes <a href=https://en.wikipedia.org/wiki/Language_model>Language Models</a> Better</a></strong><br><a href=/people/k/katherine-lee/>Katherine Lee</a>
|
<a href=/people/d/daphne-ippolito/>Daphne Ippolito</a>
|
<a href=/people/a/andrew-nystrom/>Andrew Nystrom</a>
|
<a href=/people/c/chiyuan-zhang/>Chiyuan Zhang</a>
|
<a href=/people/d/douglas-eck/>Douglas Eck</a>
|
<a href=/people/c/chris-callison-burch/>Chris Callison-Burch</a>
|
<a href=/people/n/nicholas-carlini/>Nicholas Carlini</a><br><a href=/volumes/2022.acl-long/ class=text-muted>Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--acl-long--577><div class="card-body p-3 small">We find that existing language modeling datasets contain many near duplicate examples and long repetitive substrings As a result over of the unprompted output of language models trained on these <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> is copied verbatim from the training data We develop two tools that allow us to deduplicate training datasets --- for example removing from C4 a single word English sentence that is repeated over 60,000 times Deduplication allows us to train models that emit memorized text ten times less frequently and require fewer training steps to achieve the same or better <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> We can also reduce train test overlap which affects over of the validation set of standard datasets thus allowing for more accurate evaluation Code for <a href=https://en.wikipedia.org/wiki/Data_deduplication>deduplication</a> is released at https://github.com/google-research/deduplicate-text-datasets</div></div><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.dash-1.14.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--dash-1--14 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.dash-1.14 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.dash-1.14" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.dash-1.14/>TopGuNN : Fast NLP Training Data Augmentation using Large Corpora<span class=acl-fixed-case>T</span>op<span class=acl-fixed-case>G</span>u<span class=acl-fixed-case>NN</span>: Fast <span class=acl-fixed-case>NLP</span> Training Data Augmentation using Large Corpora</a></strong><br><a href=/people/r/rebecca-iglesias-flores/>Rebecca Iglesias-Flores</a>
|
<a href=/people/m/megha-mishra/>Megha Mishra</a>
|
<a href=/people/a/ajay-patel/>Ajay Patel</a>
|
<a href=/people/a/akanksha-malhotra/>Akanksha Malhotra</a>
|
<a href=/people/r/reno-kriz/>Reno Kriz</a>
|
<a href=/people/m/martha-palmer/>Martha Palmer</a>
|
<a href=/people/c/chris-callison-burch/>Chris Callison-Burch</a><br><a href=/volumes/2021.dash-1/ class=text-muted>Proceedings of the Second Workshop on Data Science with Human in the Loop: Language Advances</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--dash-1--14><div class="card-body p-3 small">Acquiring training data for <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing systems</a> can be expensive and time-consuming. Given a few training examples crafted by experts, large corpora can be mined for thousands of semantically similar examples that provide useful variability to improve model generalization. We present TopGuNN, a fast contextualized k-NN retrieval system that can efficiently index and search over contextual embeddings generated from large corpora. TopGuNN is demonstrated for a training data augmentation use case over the Gigaword corpus. Using approximate k-NN and an efficient <a href=https://en.wikipedia.org/wiki/Computer_architecture>architecture</a>, TopGuNN performs queries over an <a href=https://en.wikipedia.org/wiki/Glossary_of_graph_theory_terms>embedding space</a> of 4.63 TB (approximately 1.5B embeddings) in less than a day.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.500.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--500 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.500 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.500" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.500/>BiSECT : Learning to Split and Rephrase Sentences with Bitexts<span class=acl-fixed-case>B</span>i<span class=acl-fixed-case>SECT</span>: Learning to Split and Rephrase Sentences with Bitexts</a></strong><br><a href=/people/j/joongwon-kim/>Joongwon Kim</a>
|
<a href=/people/m/mounica-maddela/>Mounica Maddela</a>
|
<a href=/people/r/reno-kriz/>Reno Kriz</a>
|
<a href=/people/w/wei-xu/>Wei Xu</a>
|
<a href=/people/c/chris-callison-burch/>Chris Callison-Burch</a><br><a href=/volumes/2021.emnlp-main/ class=text-muted>Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--500><div class="card-body p-3 small">An important task in NLP applications such as <a href=https://en.wikipedia.org/wiki/Sentence_simplification>sentence simplification</a> is the ability to take a long, complex sentence and split it into shorter sentences, rephrasing as necessary. We introduce a novel <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> and a new <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> for this &#8216;split and rephrase&#8217; task. Our BiSECT training data consists of 1 million long English sentences paired with shorter, meaning-equivalent English sentences. We obtain these by extracting 1-2 sentence alignments in bilingual parallel corpora and then using <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> to convert both sides of the <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> into the same language. BiSECT contains higher quality training examples than the previous Split and Rephrase corpora, with sentence splits that require more significant modifications. We categorize examples in our <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> and use these categories in a novel model that allows us to target specific regions of the input sentence to be split and edited. Moreover, we show that models trained on BiSECT can perform a wider variety of split operations and improve upon previous state-of-the-art approaches in automatic and human evaluations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-demos.16.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-demos--16 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-demos.16 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.naacl-demos.16" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-demos.16/>RESIN : A Dockerized Schema-Guided Cross-document Cross-lingual Cross-media Information Extraction and Event Tracking System<span class=acl-fixed-case>RESIN</span>: A Dockerized Schema-Guided Cross-document Cross-lingual Cross-media Information Extraction and Event Tracking System</a></strong><br><a href=/people/h/haoyang-wen/>Haoyang Wen</a>
|
<a href=/people/y/ying-lin/>Ying Lin</a>
|
<a href=/people/t/tuan-lai/>Tuan Lai</a>
|
<a href=/people/x/xiaoman-pan/>Xiaoman Pan</a>
|
<a href=/people/s/sha-li/>Sha Li</a>
|
<a href=/people/x/xudong-lin/>Xudong Lin</a>
|
<a href=/people/b/ben-zhou/>Ben Zhou</a>
|
<a href=/people/m/manling-li/>Manling Li</a>
|
<a href=/people/h/haoyu-wang/>Haoyu Wang</a>
|
<a href=/people/h/hongming-zhang/>Hongming Zhang</a>
|
<a href=/people/x/xiaodong-yu/>Xiaodong Yu</a>
|
<a href=/people/a/alexander-dong/>Alexander Dong</a>
|
<a href=/people/z/zhenhailong-wang/>Zhenhailong Wang</a>
|
<a href=/people/y/yi-fung/>Yi Fung</a>
|
<a href=/people/p/piyush-mishra/>Piyush Mishra</a>
|
<a href=/people/q/qing-lyu/>Qing Lyu</a>
|
<a href=/people/d/didac-suris/>Dídac Surís</a>
|
<a href=/people/b/brian-chen/>Brian Chen</a>
|
<a href=/people/s/susan-windisch-brown/>Susan Windisch Brown</a>
|
<a href=/people/m/martha-palmer/>Martha Palmer</a>
|
<a href=/people/c/chris-callison-burch/>Chris Callison-Burch</a>
|
<a href=/people/c/carl-vondrick/>Carl Vondrick</a>
|
<a href=/people/j/jiawei-han/>Jiawei Han</a>
|
<a href=/people/d/dan-roth/>Dan Roth</a>
|
<a href=/people/s/shih-fu-chang/>Shih-Fu Chang</a>
|
<a href=/people/h/heng-ji/>Heng Ji</a><br><a href=/volumes/2021.naacl-demos/ class=text-muted>Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Demonstrations</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-demos--16><div class="card-body p-3 small">We present a new information extraction system that can automatically construct temporal event graphs from a collection of news documents from multiple sources, multiple languages (English and Spanish for our experiment), and multiple data modalities (speech, text, image and video). The system advances state-of-the-art from two aspects : (1) extending from sentence-level event extraction to cross-document cross-lingual cross-media event extraction, coreference resolution and temporal event tracking ; (2) using human curated event schema library to match and enhance the extraction output. We have made the dockerlized system publicly available for research purpose at GitHub, with a demo video.</div></div><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.acl-main.164.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--acl-main--164 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.acl-main.164 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://slideslive.com/38928914 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.acl-main.164" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.acl-main.164/>Automatic Detection of Generated Text is Easiest when Humans are Fooled</a></strong><br><a href=/people/d/daphne-ippolito/>Daphne Ippolito</a>
|
<a href=/people/d/daniel-duckworth/>Daniel Duckworth</a>
|
<a href=/people/c/chris-callison-burch/>Chris Callison-Burch</a>
|
<a href=/people/d/douglas-eck/>Douglas Eck</a><br><a href=/volumes/2020.acl-main/ class=text-muted>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--acl-main--164><div class="card-body p-3 small">Recent advancements in neural language modelling make it possible to rapidly generate vast amounts of human-sounding text. The capabilities of <a href=https://en.wikipedia.org/wiki/Human>humans</a> and <a href=https://en.wikipedia.org/wiki/Discriminator>automatic discriminators</a> to detect machine-generated text have been a large source of research interest, but humans and machines rely on different cues to make their decisions. Here, we perform careful benchmarking and analysis of three popular sampling-based decoding strategiestop-_k _, nucleus sampling, and untruncated random samplingand show that improvements in decoding methods have primarily optimized for fooling humans. This comes at the expense of introducing <a href=https://en.wikipedia.org/wiki/Statistical_significance>statistical abnormalities</a> that make <a href=https://en.wikipedia.org/wiki/Detection>detection</a> easy for <a href=https://en.wikipedia.org/wiki/Automation>automatic systems</a>. We also show that though both human and automatic detector performance improve with longer excerpt length, even multi-sentence excerpts can fool expert human raters over 30 % of the time. Our findings reveal the importance of using both human and automatic detectors to assess the humanness of text generation systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.acl-main.666.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--acl-main--666 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.acl-main.666 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://slideslive.com/38928916 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.acl-main.666" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.acl-main.666/>Toward Better Storylines with Sentence-Level Language Models</a></strong><br><a href=/people/d/daphne-ippolito/>Daphne Ippolito</a>
|
<a href=/people/d/david-grangier/>David Grangier</a>
|
<a href=/people/d/douglas-eck/>Douglas Eck</a>
|
<a href=/people/c/chris-callison-burch/>Chris Callison-Burch</a><br><a href=/volumes/2020.acl-main/ class=text-muted>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--acl-main--666><div class="card-body p-3 small">We propose a sentence-level language model which selects the next sentence in a story from a finite set of fluent alternatives. Since it does not need to model <a href=https://en.wikipedia.org/wiki/Fluency>fluency</a>, the sentence-level language model can focus on longer range dependencies, which are crucial for <a href=https://en.wikipedia.org/wiki/Coherence_(linguistics)>multi-sentence coherence</a>. Rather than dealing with individual words, our method treats the story so far as a list of pre-trained sentence embeddings and predicts an embedding for the next sentence, which is more efficient than predicting word embeddings. Notably this allows us to consider a large number of candidates for the next sentence during training. We demonstrate the effectiveness of our approach with state-of-the-art <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> on the unsupervised Story Cloze task and with promising results on larger-scale next sentence prediction tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.cllrd-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.cllrd-1.0/>Proceedings of the LREC 2020 Workshop on "Citizen Linguistics in Language Resource Development"</a></strong><br><a href=/people/j/james-fiumara/>James Fiumara</a>
|
<a href=/people/c/christopher-cieri/>Christopher Cieri</a>
|
<a href=/people/m/mark-liberman/>Mark Liberman</a>
|
<a href=/people/c/chris-callison-burch/>Chris Callison-Burch</a><br><a href=/volumes/2020.cllrd-1/ class=text-muted>Proceedings of the LREC 2020 Workshop on "Citizen Linguistics in Language Resource Development"</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.crac-1.14.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--crac-1--14 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.crac-1.14 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.crac-1.14/>Resolving Pronouns in Twitter Streams : Context can Help !<span class=acl-fixed-case>T</span>witter Streams: Context can Help!</a></strong><br><a href=/people/a/anietie-andy/>Anietie Andy</a>
|
<a href=/people/c/chris-callison-burch/>Chris Callison-Burch</a>
|
<a href=/people/d/derry-tanti-wijaya/>Derry Tanti Wijaya</a><br><a href=/volumes/2020.crac-1/ class=text-muted>Proceedings of the Third Workshop on Computational Models of Reference, Anaphora and Coreference</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--crac-1--14><div class="card-body p-3 small">Many people live-tweet televised events like <a href=https://en.wikipedia.org/wiki/United_States_presidential_debates>Presidential debates</a> and popular TV-shows and discuss people or characters in the event. Naturally, many tweets make pronominal reference to these people / characters. We propose an <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> for resolving <a href=https://en.wikipedia.org/wiki/Personal_pronoun>personal pronouns</a> that make reference to people involved in an event, in tweet streams collected during the event.</div></div><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1053.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1053 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1053 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1053" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N19-1053/>Seeing Things from a Different Angle : Discovering Diverse Perspectives about Claims</a></strong><br><a href=/people/s/sihao-chen/>Sihao Chen</a>
|
<a href=/people/d/daniel-khashabi/>Daniel Khashabi</a>
|
<a href=/people/w/wenpeng-yin/>Wenpeng Yin</a>
|
<a href=/people/c/chris-callison-burch/>Chris Callison-Burch</a>
|
<a href=/people/d/dan-roth/>Dan Roth</a><br><a href=/volumes/N19-1/ class=text-muted>Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1053><div class="card-body p-3 small">One key consequence of the <a href=https://en.wikipedia.org/wiki/Information_revolution>information revolution</a> is a significant increase and a contamination of our information supply. The practice of <a href=https://en.wikipedia.org/wiki/Fact-checking>fact checking</a> wo n&#8217;t suffice to eliminate the biases in text data we observe, as the degree of <a href=https://en.wikipedia.org/wiki/Fact>factuality</a> alone does not determine whether biases exist in the spectrum of opinions visible to us. To better understand controversial issues, one needs to view them from a diverse yet comprehensive set of perspectives. For example, there are many ways to respond to a claim such as animals should have lawful rights, and these responses form a spectrum of perspectives, each with a stance relative to this claim and, ideally, with evidence supporting it. Inherently, this is a <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language understanding task</a>, and we propose to address it as such. Specifically, we propose the task of substantiated perspective discovery where, given a claim, a system is expected to discover a diverse set of well-corroborated perspectives that take a stance with respect to the claim. Each perspective should be substantiated by evidence paragraphs which summarize pertinent results and facts. We construct PERSPECTRUM, a <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> of claims, perspectives and evidence, making use of online debate websites to create the initial <a href=https://en.wikipedia.org/wiki/Data_collection>data collection</a>, and augmenting it using <a href=https://en.wikipedia.org/wiki/Web_search_engine>search engines</a> in order to expand and diversify our <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>. We use <a href=https://en.wikipedia.org/wiki/Crowdsourcing>crowd-sourcing</a> to filter out noise and ensure high-quality data. Our dataset contains 1k claims, accompanied with pools of 10k and 8k perspective sentences and evidence paragraphs, respectively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1365.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1365 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1365 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1365" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-1365/>Comparison of Diverse Decoding Methods from <a href=https://en.wikipedia.org/wiki/Conditional_(computer_programming)>Conditional Language Models</a></a></strong><br><a href=/people/d/daphne-ippolito/>Daphne Ippolito</a>
|
<a href=/people/r/reno-kriz/>Reno Kriz</a>
|
<a href=/people/j/joao-sedoc/>João Sedoc</a>
|
<a href=/people/m/maria-kustikova/>Maria Kustikova</a>
|
<a href=/people/c/chris-callison-burch/>Chris Callison-Burch</a><br><a href=/volumes/P19-1/ class=text-muted>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1365><div class="card-body p-3 small">While conditional language models have greatly improved in their ability to output high quality natural language, many NLP applications benefit from being able to generate a diverse set of candidate sequences. Diverse decoding strategies aim to, within a given-sized candidate list, cover as much of the space of high-quality outputs as possible, leading to improvements for tasks that rerank and combine candidate outputs. Standard decoding methods, such as <a href=https://en.wikipedia.org/wiki/Beam_search>beam search</a>, optimize for generating high likelihood sequences rather than diverse ones, though recent work has focused on increasing diversity in these methods. In this work, we perform an extensive survey of decoding-time strategies for generating diverse outputs from a <a href=https://en.wikipedia.org/wiki/Conditional_(computer_programming)>conditional language model</a>. In addition, we present a novel method where we over-sample candidates, then use <a href=https://en.wikipedia.org/wiki/Cluster_analysis>clustering</a> to remove similar sequences, thus achieving high diversity without sacrificing quality.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-3022.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-3022 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-3022 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-3022" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-3022/>PerspectroScope : A Window to the World of Diverse Perspectives<span class=acl-fixed-case>P</span>erspectro<span class=acl-fixed-case>S</span>cope: A Window to the World of Diverse Perspectives</a></strong><br><a href=/people/s/sihao-chen/>Sihao Chen</a>
|
<a href=/people/d/daniel-khashabi/>Daniel Khashabi</a>
|
<a href=/people/c/chris-callison-burch/>Chris Callison-Burch</a>
|
<a href=/people/d/dan-roth/>Dan Roth</a><br><a href=/volumes/P19-3/ class=text-muted>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: System Demonstrations</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-3022><div class="card-body p-3 small">This work presents PerspectroScope, a web-based system which lets users query a discussion-worthy natural language claim, and extract and visualize various perspectives in support or against the claim, along with evidence supporting each perspective. The <a href=https://en.wikipedia.org/wiki/System>system</a> thus lets users explore various perspectives that could touch upon aspects of the issue at hand. The <a href=https://en.wikipedia.org/wiki/System>system</a> is built as a combination of retrieval engines and learned textual-entailment-like classifiers built using a few recent developments in <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language understanding</a>. To make the <a href=https://en.wikipedia.org/wiki/System>system</a> more adaptive, expand its coverage, and improve its decisions over time, our <a href=https://en.wikipedia.org/wiki/Computing_platform>platform</a> employs various mechanisms to get corrections from the users. PerspectroScope is available at github.com/CogComp/perspectroscope Web demo link : http://orwell.seas.upenn.edu:4002/ Link to demo video : https://www.youtube.com/watch?v=MXBTR1Sp3Bs</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-2021.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-2021 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-2021 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-2021" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D18-2021/>Magnitude : A Fast, Efficient Universal Vector Embedding Utility Package<span class=acl-fixed-case>M</span>agnitude: A Fast, Efficient Universal Vector Embedding Utility Package</a></strong><br><a href=/people/a/ajay-patel/>Ajay Patel</a>
|
<a href=/people/a/alexander-sands/>Alexander Sands</a>
|
<a href=/people/c/chris-callison-burch/>Chris Callison-Burch</a>
|
<a href=/people/m/marianna-apidianaki/>Marianna Apidianaki</a><br><a href=/volumes/D18-2/ class=text-muted>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-2021><div class="card-body p-3 small">Vector space embedding models like <a href=https://en.wikipedia.org/wiki/Word2vec>word2vec</a>, <a href=https://en.wikipedia.org/wiki/GloVe_(machine_learning)>GloVe</a>, and <a href=https://en.wikipedia.org/wiki/FastText>fastText</a> are extremely popular representations in natural language processing (NLP) applications. We present Magnitude, a fast, lightweight tool for utilizing and processing <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a>. Magnitude is an open source <a href=https://en.wikipedia.org/wiki/Python_(programming_language)>Python package</a> with a compact vector storage file format that allows for efficient manipulation of huge numbers of <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a>. Magnitude performs common <a href=https://en.wikipedia.org/wiki/Instruction_set_architecture>operations</a> up to 60 to 6,000 times faster than <a href=https://en.wikipedia.org/wiki/Gensim>Gensim</a>. Magnitude introduces several novel <a href=https://en.wikipedia.org/wiki/Software_feature>features</a> for improved <a href=https://en.wikipedia.org/wiki/Robustness_(computer_science)>robustness</a> like out-of-vocabulary lookups.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2077.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2077 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2077 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-2077/>Automated Paraphrase Lattice Creation for HyTER Machine Translation Evaluation<span class=acl-fixed-case>H</span>y<span class=acl-fixed-case>TER</span> Machine Translation Evaluation</a></strong><br><a href=/people/m/marianna-apidianaki/>Marianna Apidianaki</a>
|
<a href=/people/g/guillaume-wisniewski/>Guillaume Wisniewski</a>
|
<a href=/people/a/anne-cocos/>Anne Cocos</a>
|
<a href=/people/c/chris-callison-burch/>Chris Callison-Burch</a><br><a href=/volumes/N18-2/ class=text-muted>Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2077><div class="card-body p-3 small">We propose a variant of a well-known machine translation (MT) evaluation metric, HyTER (Dreyer and Marcu, 2012), which exploits reference translations enriched with meaning equivalent expressions. The original HyTER metric relied on hand-crafted paraphrase networks which restricted its applicability to new data. We test, for the first time, HyTER with automatically built paraphrase lattices. We show that although the <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metric</a> obtains good results on small and carefully curated data with both manually and automatically selected substitutes, it achieves medium performance on much larger and noisier datasets, demonstrating the limits of the <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metric</a> for tuning and evaluation of current MT systems.</div></div><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1914.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1914 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1914 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1914/>Word Sense Filtering Improves Embedding-Based Lexical Substitution</a></strong><br><a href=/people/a/anne-cocos/>Anne Cocos</a>
|
<a href=/people/m/marianna-apidianaki/>Marianna Apidianaki</a>
|
<a href=/people/c/chris-callison-burch/>Chris Callison-Burch</a><br><a href=/volumes/W17-19/ class=text-muted>Proceedings of the 1st Workshop on Sense, Concept and Entity Representations and their Applications</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1914><div class="card-body p-3 small">The role of <a href=https://en.wikipedia.org/wiki/Word_sense_disambiguation>word sense disambiguation</a> in <a href=https://en.wikipedia.org/wiki/Lexical_substitution>lexical substitution</a> has been questioned due to the high performance of vector space models which propose good substitutes without explicitly accounting for <a href=https://en.wikipedia.org/wiki/Word_sense>sense</a>. We show that a filtering mechanism based on a sense inventory optimized for <a href=https://en.wikipedia.org/wiki/Substituent>substitutability</a> can improve the results of these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>. Our sense inventory is constructed using a clustering method which generates paraphrase clusters that are congruent with lexical substitution annotations in a development set. The results show that <a href=https://en.wikipedia.org/wiki/Lexical_substitution>lexical substitution</a> can still benefit from <a href=https://en.wikipedia.org/wiki/Word_sense>senses</a> which can improve the output of vector space paraphrase ranking models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4405.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4405 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4405 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4405/>Constructing an Alias List for Named Entities during an Event</a></strong><br><a href=/people/a/anietie-andy/>Anietie Andy</a>
|
<a href=/people/m/mark-dredze/>Mark Dredze</a>
|
<a href=/people/m/mugizi-rwebangira/>Mugizi Rwebangira</a>
|
<a href=/people/c/chris-callison-burch/>Chris Callison-Burch</a><br><a href=/volumes/W17-44/ class=text-muted>Proceedings of the 3rd Workshop on Noisy User-generated Text</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4405><div class="card-body p-3 small">In certain fields, real-time knowledge from events can help in making informed decisions. In order to extract pertinent real-time knowledge related to an event, it is important to identify the <a href=https://en.wikipedia.org/wiki/Named_entity>named entities</a> and their corresponding <a href=https://en.wikipedia.org/wiki/Pseudonym>aliases</a> related to the event. The problem of identifying aliases of named entities that spike has remained unexplored. In this paper, we introduce an <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a>, EntitySpike, that identifies entities that spike in popularity in tweets from a given time period, and constructs an alias list for these spiked entities. EntitySpike uses a temporal heuristic to identify named entities with similar context that occur in the same time period (within minutes) during an event. Each entity is encoded as a vector using this temporal heuristic. We show how these entity-vectors can be used to create a named entity alias list. We evaluated our <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> on a dataset of temporally ordered tweets from a single event, the 2013 <a href=https://en.wikipedia.org/wiki/55th_Annual_Grammy_Awards>Grammy Awards show</a>. We carried out various experiments on tweets that were published in the same time period and show that our <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> identifies most entity name aliases and outperforms a competitive baseline.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5039.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5039 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5039 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W17-5039" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W17-5039/>Systematically Adapting <a href=https://en.wikipedia.org/wiki/Machine_translation>Machine Translation</a> for Grammatical Error Correction</a></strong><br><a href=/people/c/courtney-napoles/>Courtney Napoles</a>
|
<a href=/people/c/chris-callison-burch/>Chris Callison-Burch</a><br><a href=/volumes/W17-50/ class=text-muted>Proceedings of the 12th Workshop on Innovative Use of NLP for Building Educational Applications</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5039><div class="card-body p-3 small">n this work we adapt machine translation (MT) to grammatical error correction, identifying how components of the statistical MT pipeline can be modified for this task and analyzing how each modification impacts system performance. We evaluate the contribution of each of these components with standard evaluation metrics and automatically characterize the morphological and lexical transformations made in system output. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> rivals the current state of the art using a fraction of the training data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/J17-2001.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-J17-2001 data-toggle=collapse aria-expanded=false aria-controls=abstract-J17-2001 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/J17-2001/>A Comprehensive Analysis of Bilingual Lexicon Induction</a></strong><br><a href=/people/a/ann-irvine/>Ann Irvine</a>
|
<a href=/people/c/chris-callison-burch/>Chris Callison-Burch</a><br><a href=/volumes/J17-2/ class=text-muted>Computational Linguistics, Volume 43, Issue 2 - June 2017</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-J17-2001><div class="card-body p-3 small">Bilingual lexicon induction is the task of inducing word translations from monolingual corpora in two languages. In this article we present the most comprehensive analysis of bilingual lexicon induction to date. We present experiments on a wide range of languages and data sizes. We examine translation into English from 25 foreign languages : <a href=https://en.wikipedia.org/wiki/Albanian_language>Albanian</a>, <a href=https://en.wikipedia.org/wiki/Azerbaijani_language>Azeri</a>, <a href=https://en.wikipedia.org/wiki/Bengali_language>Bengali</a>, <a href=https://en.wikipedia.org/wiki/Bosnian_language>Bosnian</a>, <a href=https://en.wikipedia.org/wiki/Bulgarian_language>Bulgarian</a>, <a href=https://en.wikipedia.org/wiki/Cebuano_language>Cebuano</a>, <a href=https://en.wikipedia.org/wiki/Gujarati_language>Gujarati</a>, <a href=https://en.wikipedia.org/wiki/Hindi>Hindi</a>, <a href=https://en.wikipedia.org/wiki/Hungarian_language>Hungarian</a>, <a href=https://en.wikipedia.org/wiki/Indonesian_language>Indonesian</a>, <a href=https://en.wikipedia.org/wiki/Latvian_language>Latvian</a>, <a href=https://en.wikipedia.org/wiki/Nepali_language>Nepali</a>, <a href=https://en.wikipedia.org/wiki/Romanian_language>Romanian</a>, <a href=https://en.wikipedia.org/wiki/Serbian_language>Serbian</a>, <a href=https://en.wikipedia.org/wiki/Slovak_language>Slovak</a>, <a href=https://en.wikipedia.org/wiki/Somali_language>Somali</a>, <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a>, <a href=https://en.wikipedia.org/wiki/Swedish_language>Swedish</a>, <a href=https://en.wikipedia.org/wiki/Tamil_language>Tamil</a>, <a href=https://en.wikipedia.org/wiki/Telugu_language>Telugu</a>, <a href=https://en.wikipedia.org/wiki/Turkish_language>Turkish</a>, Ukrainian, <a href=https://en.wikipedia.org/wiki/Uzbek_language>Uzbek</a>, <a href=https://en.wikipedia.org/wiki/Vietnamese_language>Vietnamese</a>, and <a href=https://en.wikipedia.org/wiki/Welsh_language>Welsh</a>. We analyze the behavior of bilingual lexicon induction on low-frequency words, rather than testing solely on high-frequency words, as previous research has done. Low-frequency words are more relevant to <a href=https://en.wikipedia.org/wiki/Statistical_machine_translation>statistical machine translation</a>, where systems typically lack translations of rare words that fall outside of their training data. We systematically explore a wide range of <a href=https://en.wikipedia.org/wiki/Feature_(linguistics)>features</a> and phenomena that affect the quality of the translations discovered by bilingual lexicon induction. We provide illustrative examples of the highest ranking translations for orthogonal signals of translation equivalence like contextual similarity and temporal similarity. We analyze the effects of frequency and burstiness, and the sizes of the seed bilingual dictionaries and the monolingual training corpora. Additionally, we introduce a novel discriminative approach to bilingual lexicon induction. Our <a href=https://en.wikipedia.org/wiki/Discriminative_model>discriminative model</a> is capable of combining a wide variety of <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> that individually provide only weak indications of translation equivalence.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S17-1009.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S17-1009 data-toggle=collapse aria-expanded=false aria-controls=abstract-S17-1009 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/S17-1009/>Mapping the Paraphrase Database to <a href=https://en.wikipedia.org/wiki/WordNet>WordNet</a><span class=acl-fixed-case>W</span>ord<span class=acl-fixed-case>N</span>et</a></strong><br><a href=/people/a/anne-cocos/>Anne Cocos</a>
|
<a href=/people/m/marianna-apidianaki/>Marianna Apidianaki</a>
|
<a href=/people/c/chris-callison-burch/>Chris Callison-Burch</a><br><a href=/volumes/S17-1/ class=text-muted>Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (*SEM 2017)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S17-1009><div class="card-body p-3 small">WordNet has facilitated important research in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a> but its usefulness is somewhat limited by its relatively small lexical coverage. The Paraphrase Database (PPDB) covers 650 times more words, but lacks the semantic structure of <a href=https://en.wikipedia.org/wiki/WordNet>WordNet</a> that would make it more directly useful for downstream tasks. We present a method for mapping words from PPDB to WordNet synsets with 89 % <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>. The mapping also lays important groundwork for incorporating WordNet&#8217;s relations into PPDB so as to increase its utility for semantic reasoning in applications.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-2007.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-2007 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-2007 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-2007/>KnowYourNyms? A Game of Semantic Relationships<span class=acl-fixed-case>K</span>now<span class=acl-fixed-case>Y</span>our<span class=acl-fixed-case>N</span>yms? A Game of Semantic Relationships</a></strong><br><a href=/people/r/ross-mechanic/>Ross Mechanic</a>
|
<a href=/people/d/dean-fulgoni/>Dean Fulgoni</a>
|
<a href=/people/h/hannah-cutler/>Hannah Cutler</a>
|
<a href=/people/s/sneha-rajana/>Sneha Rajana</a>
|
<a href=/people/z/zheyuan-liu/>Zheyuan Liu</a>
|
<a href=/people/b/bradley-jackson/>Bradley Jackson</a>
|
<a href=/people/a/anne-cocos/>Anne Cocos</a>
|
<a href=/people/c/chris-callison-burch/>Chris Callison-Burch</a>
|
<a href=/people/m/marianna-apidianaki/>Marianna Apidianaki</a><br><a href=/volumes/D17-2/ class=text-muted>Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-2007><div class="card-body p-3 small">Semantic relation knowledge is crucial for <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language understanding</a>. We introduce KnowYourNyms?, a <a href=https://en.wikipedia.org/wiki/Browser_game>web-based game</a> for learning <a href=https://en.wikipedia.org/wiki/Semantic_Web>semantic relations</a>. While providing users with an engaging experience, the <a href=https://en.wikipedia.org/wiki/Application_software>application</a> collects large amounts of data that can be used to improve semantic relation classifiers. The data also broadly informs us of how people perceive the relationships between words, providing useful insights for research in <a href=https://en.wikipedia.org/wiki/Psychology>psychology</a> and <a href=https://en.wikipedia.org/wiki/Linguistics>linguistics</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2016.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2016 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2016 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2016/>The Language of Place : Semantic Value from <a href=https://en.wikipedia.org/wiki/Geographic_data_and_information>Geospatial Context</a></a></strong><br><a href=/people/a/anne-cocos/>Anne Cocos</a>
|
<a href=/people/c/chris-callison-burch/>Chris Callison-Burch</a><br><a href=/volumes/E17-2/ class=text-muted>Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2016><div class="card-body p-3 small">There is a relationship between what we say and where we say it. Word embeddings are usually trained assuming that semantically-similar words occur within the same textual contexts. We investigate the extent to which <a href=https://en.wikipedia.org/wiki/Semantic_similarity>semantically-similar words</a> occur within the same <a href=https://en.wikipedia.org/wiki/Geographic_data_and_information>geospatial contexts</a>. We enrich a corpus of geolocated Twitter posts with physical data derived from <a href=https://en.wikipedia.org/wiki/Google_Places>Google Places</a> and <a href=https://en.wikipedia.org/wiki/OpenStreetMap>OpenStreetMap</a>, and train word embeddings using the resulting geospatial contexts. Intrinsic evaluation of the resulting vectors shows that <a href=https://en.wikipedia.org/wiki/Context_(language_use)>geographic context</a> alone does provide useful information about <a href=https://en.wikipedia.org/wiki/Semantic_similarity>semantic relatedness</a>.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Chris+Callison-Burch" title="Search for 'Chris Callison-Burch' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/a/anne-cocos/ class=align-middle>Anne Cocos</a>
<span class="badge badge-secondary align-middle ml-2">5</span></li><li class=list-group-item><a href=/people/m/marianna-apidianaki/ class=align-middle>Marianna Apidianaki</a>
<span class="badge badge-secondary align-middle ml-2">5</span></li><li class=list-group-item><a href=/people/d/daphne-ippolito/ class=align-middle>Daphne Ippolito</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/people/d/douglas-eck/ class=align-middle>Douglas Eck</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/r/reno-kriz/ class=align-middle>Reno Kriz</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/d/dan-roth/ class=align-middle>Dan Roth</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/a/ajay-patel/ class=align-middle>Ajay Patel</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/m/martha-palmer/ class=align-middle>Martha Palmer</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/a/anietie-andy/ class=align-middle>Anietie Andy</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/s/sihao-chen/ class=align-middle>Sihao Chen</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/d/daniel-khashabi/ class=align-middle>Daniel Khashabi</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/d/daniel-duckworth/ class=align-middle>Daniel Duckworth</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/david-grangier/ class=align-middle>David Grangier</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/rebecca-iglesias-flores/ class=align-middle>Rebecca Iglesias-Flores</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/megha-mishra/ class=align-middle>Megha Mishra</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/akanksha-malhotra/ class=align-middle>Akanksha Malhotra</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/mark-dredze/ class=align-middle>Mark Dredze</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/mugizi-rwebangira/ class=align-middle>Mugizi Rwebangira</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/courtney-napoles/ class=align-middle>Courtney Napoles</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/katherine-lee/ class=align-middle>Katherine Lee</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/andrew-nystrom/ class=align-middle>Andrew Nystrom</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/chiyuan-zhang/ class=align-middle>Chiyuan Zhang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/n/nicholas-carlini/ class=align-middle>Nicholas Carlini</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/ann-irvine/ class=align-middle>Ann Irvine</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/alexander-sands/ class=align-middle>Alexander Sands</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/james-fiumara/ class=align-middle>James Fiumara</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/christopher-cieri/ class=align-middle>Christopher Cieri</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/mark-liberman/ class=align-middle>Mark Liberman</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/joongwon-kim/ class=align-middle>Joongwon Kim</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/mounica-maddela/ class=align-middle>Mounica Maddela</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/w/wei-xu/ class=align-middle>Wei Xu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/ross-mechanic/ class=align-middle>Ross Mechanic</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/dean-fulgoni/ class=align-middle>Dean Fulgoni</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/hannah-cutler/ class=align-middle>Hannah Cutler</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/sneha-rajana/ class=align-middle>Sneha Rajana</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/zheyuan-liu/ class=align-middle>Zheyuan Liu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/b/bradley-jackson/ class=align-middle>Bradley Jackson</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/derry-tanti-wijaya/ class=align-middle>Derry Tanti Wijaya</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/haoyang-wen/ class=align-middle>Haoyang Wen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/ying-lin/ class=align-middle>Ying Lin</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/tuan-lai/ class=align-middle>Tuan Lai</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/x/xiaoman-pan/ class=align-middle>Xiaoman Pan</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/sha-li/ class=align-middle>Sha Li</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/x/xudong-lin/ class=align-middle>Xudong Lin</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/b/ben-zhou/ class=align-middle>Ben Zhou</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/manling-li/ class=align-middle>Manling Li</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/haoyu-wang/ class=align-middle>Haoyu Wang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/hongming-zhang/ class=align-middle>Hongming Zhang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/x/xiaodong-yu/ class=align-middle>Xiaodong Yu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/alexander-dong/ class=align-middle>Alexander Dong</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/zhenhailong-wang/ class=align-middle>Zhenhailong Wang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yi-fung/ class=align-middle>Yi Fung</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/p/piyush-mishra/ class=align-middle>Piyush Mishra</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/q/qing-lyu/ class=align-middle>Qing Lyu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/didac-suris/ class=align-middle>Dídac Surís</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/b/brian-chen/ class=align-middle>Brian Chen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/susan-windisch-brown/ class=align-middle>Susan Windisch Brown</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/carl-vondrick/ class=align-middle>Carl Vondrick</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jiawei-han/ class=align-middle>Jiawei Han</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/shih-fu-chang/ class=align-middle>Shih-Fu Chang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/heng-ji/ class=align-middle>Heng Ji</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/w/wenpeng-yin/ class=align-middle>Wenpeng Yin</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/g/guillaume-wisniewski/ class=align-middle>Guillaume Wisniewski</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/joao-sedoc/ class=align-middle>João Sedoc</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/maria-kustikova/ class=align-middle>Maria Kustikova</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/acl/ class=align-middle>ACL</a><span class="badge badge-secondary align-middle ml-2">5</span></li><li class=list-group-item><a href=/venues/ws/ class=align-middle>WS</a><span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/venues/emnlp/ class=align-middle>EMNLP</a><span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/venues/naacl/ class=align-middle>NAACL</a><span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/venues/dash/ class=align-middle>DaSH</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-venues aria-expanded=false aria-controls=more-venues>show all...</li><div class="collapse border-top" id=more-venues><li class=list-group-item><a href=/venues/cl/ class=align-middle>CL</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/cllrd/ class=align-middle>CLLRD</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/semeval/ class=align-middle>SemEval</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/crac/ class=align-middle>CRAC</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/eacl/ class=align-middle>EACL</a><span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>