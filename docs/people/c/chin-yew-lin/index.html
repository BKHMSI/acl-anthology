<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Chin-Yew Lin - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Chin-Yew</span> <span class=font-weight-bold>Lin</span></h2><hr><div class=row><div class=col-lg-9><h4>2022</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-acl.124.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-acl--124 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-acl.124 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2022.findings-acl.124.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.findings-acl.124" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2022.findings-acl.124/>Decomposed Meta-Learning for Few-Shot Named Entity Recognition</a></strong><br><a href=/people/t/tingting-ma/>Tingting Ma</a>
|
<a href=/people/h/huiqiang-jiang/>Huiqiang Jiang</a>
|
<a href=/people/q/qianhui-wu/>Qianhui Wu</a>
|
<a href=/people/t/tiejun-zhao/>Tiejun Zhao</a>
|
<a href=/people/c/chin-yew-lin/>Chin-Yew Lin</a><br><a href=/volumes/2022.findings-acl/ class=text-muted>Findings of the Association for Computational Linguistics: ACL 2022</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-acl--124><div class="card-body p-3 small">Few-shot named entity recognition (NER) systems aim at recognizing novel-class named entities based on only a few labeled examples. In this paper, we present a decomposed meta-learning approach which addresses the problem of few-shot NER by sequentially tackling few-shot span detection and few-shot entity typing using meta-learning. In particular, we take the few-shot span detection as a sequence labeling problem and train the span detector by introducing the model-agnostic meta-learning (MAML) algorithm to find a good model parameter initialization that could fast adapt to new entity classes. For few-shot entity typing, we propose MAML-ProtoNet, i.e., MAML-enhanced prototypical networks to find a good embedding space that can better distinguish text span representations from different entity classes. Extensive experiments on various benchmarks show that our approach achieves superior performance over prior methods.</div></div><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-demo.39.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-demo--39 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-demo.39 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-demo.39" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-demo.39/>ReTraCk : A Flexible and Efficient Framework for Knowledge Base Question Answering<span class=acl-fixed-case>R</span>e<span class=acl-fixed-case>T</span>ra<span class=acl-fixed-case>C</span>k: A Flexible and Efficient Framework for Knowledge Base Question Answering</a></strong><br><a href=/people/s/shuang-chen/>Shuang Chen</a>
|
<a href=/people/q/qian-liu/>Qian Liu</a>
|
<a href=/people/z/zhiwei-yu/>Zhiwei Yu</a>
|
<a href=/people/c/chin-yew-lin/>Chin-Yew Lin</a>
|
<a href=/people/j/jian-guang-lou/>Jian-Guang Lou</a>
|
<a href=/people/f/feng-jiang/>Feng Jiang</a><br><a href=/volumes/2021.acl-demo/ class=text-muted>Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-demo--39><div class="card-body p-3 small">We present Retriever-Transducer-Checker (ReTraCk), a neural semantic parsing framework for large scale knowledge base question answering (KBQA). ReTraCk is designed as a <a href=https://en.wikipedia.org/wiki/Modular_programming>modular framework</a> to maintain high flexibility. It includes a retriever to retrieve relevant KB items efficiently, a transducer to generate <a href=https://en.wikipedia.org/wiki/Logical_form>logical form</a> with syntax correctness guarantees and a checker to improve transduction procedure. ReTraCk is ranked at top1 overall performance on the GrailQA leaderboard and obtains highly competitive performance on the typical WebQuestionsSP benchmark. Our <a href=https://en.wikipedia.org/wiki/System>system</a> can interact with users timely, demonstrating the efficiency of the proposed <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a>.</div></div><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1299.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1299 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1299 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1299/>Enhancing Neural Data-To-Text Generation Models with External Background Knowledge</a></strong><br><a href=/people/s/shuang-chen/>Shuang Chen</a>
|
<a href=/people/j/jinpeng-wang/>Jinpeng Wang</a>
|
<a href=/people/x/xiaocheng-feng/>Xiaocheng Feng</a>
|
<a href=/people/f/feng-jiang/>Feng Jiang</a>
|
<a href=/people/b/bing-qin/>Bing Qin</a>
|
<a href=/people/c/chin-yew-lin/>Chin-Yew Lin</a><br><a href=/volumes/D19-1/ class=text-muted>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1299><div class="card-body p-3 small">Recent neural models for data-to-text generation rely on massive parallel pairs of data and text to learn the writing knowledge. They often assume that writing knowledge can be acquired from the training data alone. However, when people are writing, they not only rely on the data but also consider <a href=https://en.wikipedia.org/wiki/Common_knowledge_(logic)>related knowledge</a>. In this paper, we enhance neural data-to-text models with external knowledge in a simple but effective way to improve the fidelity of generated text. Besides relying on parallel data and text as in previous work, our model attends to relevant external knowledge, encoded as a temporary memory, and combines this <a href=https://en.wikipedia.org/wiki/Knowledge>knowledge</a> with the context representation of data before generating words. This allows the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> to infer relevant facts which are not explicitly stated in the data table from an external knowledge source. Experimental results on twenty-one Wikipedia infobox-to-text datasets show our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>, KBAtt, consistently improves a state-of-the-art <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> on most of the datasets. In addition, to quantify when and why <a href=https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning>external knowledge</a> is effective, we design a <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metric</a>, KBGain, which shows a strong correlation with the observed performance boost. This result demonstrates the relevance of external knowledge and sparseness of original data are the main factors affecting system performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1256.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1256 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1256 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/384728744 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1256/>A Simple Recipe towards Reducing Hallucination in Neural Surface Realisation</a></strong><br><a href=/people/f/feng-nie/>Feng Nie</a>
|
<a href=/people/j/jin-ge-yao/>Jin-Ge Yao</a>
|
<a href=/people/j/jinpeng-wang/>Jinpeng Wang</a>
|
<a href=/people/r/rong-pan/>Rong Pan</a>
|
<a href=/people/c/chin-yew-lin/>Chin-Yew Lin</a><br><a href=/volumes/P19-1/ class=text-muted>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1256><div class="card-body p-3 small">Recent neural language generation systems often hallucinate contents (i.e., producing irrelevant or contradicted facts), especially when trained on loosely corresponding pairs of the input structure and text. To mitigate this issue, we propose to integrate a language understanding module for data refinement with self-training iterations to effectively induce strong equivalence between the input data and the paired text. Experiments on the E2E challenge dataset show that our proposed framework can reduce more than 50 % relative unaligned noise from the original data-text pairs. A vanilla sequence-to-sequence neural NLG model trained on the refined data has improved on content correctness compared with the current state-of-the-art ensemble generator.<i>hallucinate</i> contents (i.e., producing irrelevant or contradicted facts), especially when trained on loosely corresponding pairs of the input structure and text. To mitigate this issue, we propose to integrate a language understanding module for data refinement with self-training iterations to effectively induce strong equivalence between the input data and the paired text. Experiments on the E2E challenge dataset show that our proposed framework can reduce more than 50% relative unaligned noise from the original data-text pairs. A vanilla sequence-to-sequence neural NLG model trained on the refined data has improved on content correctness compared with the current state-of-the-art ensemble generator.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1524.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1524 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1524 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P19-1524.Supplementary.zip data-toggle=tooltip data-placement=top title=Supplementary><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1524" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-1524/>Towards Improving Neural Named Entity Recognition with Gazetteers</a></strong><br><a href=/people/t/tianyu-liu/>Tianyu Liu</a>
|
<a href=/people/j/jin-ge-yao/>Jin-Ge Yao</a>
|
<a href=/people/c/chin-yew-lin/>Chin-Yew Lin</a><br><a href=/volumes/P19-1/ class=text-muted>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1524><div class="card-body p-3 small">Most of the recently proposed neural models for <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>named entity recognition</a> have been purely data-driven, with a strong emphasis on getting rid of the efforts for collecting external resources or designing hand-crafted features. This could increase the chance of <a href=https://en.wikipedia.org/wiki/Overfitting>overfitting</a> since the models can not access any supervision signal beyond the small amount of annotated data, limiting their power to generalize beyond the annotated entities. In this work, we show that properly utilizing external gazetteers could benefit segmental neural NER models. We add a simple <a href=https://en.wikipedia.org/wiki/Modular_programming>module</a> on the recently proposed hybrid semi-Markov CRF architecture and observe some promising results.</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1018.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1018 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1018 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1018/>Neural Math Word Problem Solver with <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>Reinforcement Learning</a></a></strong><br><a href=/people/d/danqing-huang/>Danqing Huang</a>
|
<a href=/people/j/jing-liu/>Jing Liu</a>
|
<a href=/people/c/chin-yew-lin/>Chin-Yew Lin</a>
|
<a href=/people/j/jian-yin/>Jian Yin</a><br><a href=/volumes/C18-1/ class=text-muted>Proceedings of the 27th International Conference on Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1018><div class="card-body p-3 small">Sequence-to-sequence model has been applied to solve <a href=https://en.wikipedia.org/wiki/Word_problem_(mathematics)>math word problems</a>. The <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> takes math problem descriptions as input and generates equations as output. The advantage of sequence-to-sequence model requires no <a href=https://en.wikipedia.org/wiki/Feature_engineering>feature engineering</a> and can generate equations that do not exist in training data. However, our experimental analysis reveals that this <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> suffers from two shortcomings : (1) generate spurious numbers ; (2) generate numbers at wrong positions. In this paper, we propose incorporating copy and alignment mechanism to the sequence-to-sequence model (namely CASS) to address these shortcomings. To train our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>, we apply <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning</a> to directly optimize the solution accuracy. It overcomes the train-test discrepancy issue of <a href=https://en.wikipedia.org/wiki/Maximum_likelihood_estimation>maximum likelihood estimation</a>, which uses the surrogate objective of maximizing equation likelihood during training while the evaluation metric is solution accuracy (non-differentiable) at test time. Furthermore, to explore the effectiveness of our neural model, we use our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> output as a <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>feature</a> and incorporate it into the feature-based model. Experimental results show that (1) The copy and alignment mechanism is effective to address the two issues ; (2) <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>Reinforcement learning</a> leads to better performance than <a href=https://en.wikipedia.org/wiki/Maximum_likelihood_estimation>maximum likelihood</a> on this task ; (3) Our neural model is complementary to the feature-based model and their combination significantly outperforms the state-of-the-art results.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1422.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1422 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1422 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-1422/>Operation-guided Neural Networks for High Fidelity Data-To-Text Generation</a></strong><br><a href=/people/f/feng-nie/>Feng Nie</a>
|
<a href=/people/j/jinpeng-wang/>Jinpeng Wang</a>
|
<a href=/people/j/jin-ge-yao/>Jin-Ge Yao</a>
|
<a href=/people/r/rong-pan/>Rong Pan</a>
|
<a href=/people/c/chin-yew-lin/>Chin-Yew Lin</a><br><a href=/volumes/D18-1/ class=text-muted>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1422><div class="card-body p-3 small">Recent neural models for data-to-text generation are mostly based on data-driven end-to-end training over encoder-decoder networks. Even though the generated texts are mostly fluent and informative, they often generate descriptions that are not consistent with the input structured data. This is a critical issue especially in domains that require <a href=https://en.wikipedia.org/wiki/Inference>inference</a> or calculations over <a href=https://en.wikipedia.org/wiki/Raw_data>raw data</a>. In this paper, we attempt to improve the <a href=https://en.wikipedia.org/wiki/Fidelity>fidelity</a> of neural data-to-text generation by utilizing pre-executed symbolic operations. We propose a framework called Operation-guided Attention-based sequence-to-sequence network (OpAtt), with a specifically designed gating mechanism as well as a quantization module for operation results to utilize information from pre-executed operations. Experiments on two sports datasets show our proposed method clearly improves the <a href=https://en.wikipedia.org/wiki/Fidelity>fidelity</a> of the generated texts to the input structured data.</div></div><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-2032.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-2032 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-2032 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-2032/>A Statistical Framework for Product Description Generation</a></strong><br><a href=/people/j/jinpeng-wang/>Jinpeng Wang</a>
|
<a href=/people/y/yutai-hou/>Yutai Hou</a>
|
<a href=/people/j/jing-liu/>Jing Liu</a>
|
<a href=/people/y/yunbo-cao/>Yunbo Cao</a>
|
<a href=/people/c/chin-yew-lin/>Chin-Yew Lin</a><br><a href=/volumes/I17-2/ class=text-muted>Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-2032><div class="card-body p-3 small">We present in this paper a statistical framework that generates accurate and fluent product description from product attributes. Specifically, after extracting templates and learning writing knowledge from attribute-description parallel data, we use the learned knowledge to decide what to say and how to say for product description generation. To evaluate accuracy and fluency for the generated descriptions, in addition to BLEU and Recall, we propose to measure what to say (in terms of attribute coverage) and to measure how to say (by attribute-specified generation) separately. Experimental results show that our <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> is effective.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2085.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2085 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2085 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-2085/>List-only Entity Linking</a></strong><br><a href=/people/y/ying-lin/>Ying Lin</a>
|
<a href=/people/c/chin-yew-lin/>Chin-Yew Lin</a>
|
<a href=/people/h/heng-ji/>Heng Ji</a><br><a href=/volumes/P17-2/ class=text-muted>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2085><div class="card-body p-3 small">Traditional Entity Linking (EL) technologies rely on rich structures and properties in the target knowledge base (KB). However, in many applications, the <a href=https://en.wikipedia.org/wiki/Kibibyte>KB</a> may be as simple and sparse as lists of names of the same type (e.g., lists of products). We call <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> as List-only Entity Linking problem. Fortunately, some mentions may have more cues for linking, which can be used as seed mentions to bridge other mentions and the uninformative entities. In this work, we select most linkable mentions as <a href=https://en.wikipedia.org/wiki/List_of_Latin_phrases_(S)>seed mentions</a> and disambiguate other mentions by comparing them with the <a href=https://en.wikipedia.org/wiki/List_of_Latin_phrases_(S)>seed mentions</a> rather than directly with the entities. Our experiments on linking mentions to seven automatically mined lists show promising results and demonstrate the effectiveness of our approach.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1084.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1084 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1084 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1084/>Learning Fine-Grained Expressions to Solve Math Word Problems</a></strong><br><a href=/people/d/danqing-huang/>Danqing Huang</a>
|
<a href=/people/s/shuming-shi/>Shuming Shi</a>
|
<a href=/people/c/chin-yew-lin/>Chin-Yew Lin</a>
|
<a href=/people/j/jian-yin/>Jian Yin</a><br><a href=/volumes/D17-1/ class=text-muted>Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1084><div class="card-body p-3 small">This paper presents a novel template-based method to solve math word problems. This method learns the mappings between math concept phrases in math word problems and their math expressions from training data. For each equation template, we automatically construct a rich template sketch by aggregating information from various problems with the same <a href=https://en.wikipedia.org/wiki/Template_processor>template</a>. Our approach is implemented in a two-stage system. It first retrieves a few relevant equation system templates and aligns numbers in math word problems to those <a href=https://en.wikipedia.org/wiki/Template_processor>templates</a> for candidate equation generation. It then does a fine-grained inference to obtain the final answer. Experiment results show that our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> achieves an <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of 28.4 % on the linear Dolphin18 K benchmark, which is 10 % (54 % relative) higher than previous state-of-the-art systems while achieving an accuracy increase of 12 % (59 % relative) on the TS6 benchmark subset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1078.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1078 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1078 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1078/>Trust, but Verify ! Better Entity Linking through Automatic Verification</a></strong><br><a href=/people/b/benjamin-heinzerling/>Benjamin Heinzerling</a>
|
<a href=/people/m/michael-strube/>Michael Strube</a>
|
<a href=/people/c/chin-yew-lin/>Chin-Yew Lin</a><br><a href=/volumes/E17-1/ class=text-muted>Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1078><div class="card-body p-3 small">We introduce automatic verification as a post-processing step for entity linking (EL). The proposed method trusts EL system results collectively, by assuming entity mentions are mostly linked correctly, in order to create a semantic profile of the given text using geospatial and temporal information, as well as fine-grained entity types. This profile is then used to automatically verify each linked mention individually, i.e., to predict whether it has been linked correctly or not. Verification allows leveraging a rich set of global and pairwise features that would be prohibitively expensive for EL systems employing global inference. Evaluation shows consistent improvements across datasets and systems. In particular, when applied to state-of-the-art systems, our method yields an absolute improvement in <a href=https://en.wikipedia.org/wiki/Linker_(computing)>linking</a> performance of up to 1.7 F1 on AIDA / CoNLL&#8217;03 and up to 2.4 F1 on the English TAC KBP 2015 TEDL dataset.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Chin-Yew+Lin" title="Search for 'Chin-Yew Lin' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/j/jinpeng-wang/ class=align-middle>Jinpeng Wang</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/people/j/jin-ge-yao/ class=align-middle>Jin-ge Yao</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/d/danqing-huang/ class=align-middle>Danqing Huang</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/j/jing-liu/ class=align-middle>Jing Liu</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/j/jian-yin/ class=align-middle>Jian Yin</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/s/shuang-chen/ class=align-middle>Shuang Chen</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/f/feng-jiang/ class=align-middle>Feng Jiang</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/f/feng-nie/ class=align-middle>Feng Nie</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/r/rong-pan/ class=align-middle>Rong Pan</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/q/qian-liu/ class=align-middle>Qian Liu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/zhiwei-yu/ class=align-middle>Zhiwei Yu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jian-guang-lou/ class=align-middle>Jian-Guang Lou</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yutai-hou/ class=align-middle>Yutai Hou</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yunbo-cao/ class=align-middle>Yunbo Cao</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/ying-lin/ class=align-middle>Ying Lin</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/heng-ji/ class=align-middle>Heng Ji</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/x/xiaocheng-feng/ class=align-middle>Xiaocheng Feng</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/b/bing-qin/ class=align-middle>Bing Qin</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/tingting-ma/ class=align-middle>Tingting Ma</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/huiqiang-jiang/ class=align-middle>Huiqiang Jiang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/q/qianhui-wu/ class=align-middle>Qianhui Wu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/tiejun-zhao/ class=align-middle>Tiejun Zhao</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/shuming-shi/ class=align-middle>Shuming Shi</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/b/benjamin-heinzerling/ class=align-middle>Benjamin Heinzerling</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/michael-strube/ class=align-middle>Michael Strube</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/tianyu-liu/ class=align-middle>Tianyu Liu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/acl/ class=align-middle>ACL</a><span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/venues/emnlp/ class=align-middle>EMNLP</a><span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/venues/coling/ class=align-middle>COLING</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/ijcnlp/ class=align-middle>IJCNLP</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/findings/ class=align-middle>Findings</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-venues aria-expanded=false aria-controls=more-venues>show all...</li><div class="collapse border-top" id=more-venues><li class=list-group-item><a href=/venues/eacl/ class=align-middle>EACL</a><span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>