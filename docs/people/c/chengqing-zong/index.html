<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Chengqing Zong - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Chengqing</span> <span class=font-weight-bold>Zong</span></h2><hr><div class=row><div class=col-lg-9><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.0/>Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</a></strong><br><a href=/people/c/chengqing-zong/>Chengqing Zong</a>
|
<a href=/people/f/fei-xia/>Fei Xia</a>
|
<a href=/people/w/wenjie-li/>Wenjie Li</a>
|
<a href=/people/r/roberto-navigli/>Roberto Navigli</a><br><a href=/volumes/2021.acl-long/ class=text-muted>Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.184.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--184 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.184 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.184/>Distributed Representations of Emotion Categories in Emotion Space</a></strong><br><a href=/people/x/xiangyu-wang/>Xiangyu Wang</a>
|
<a href=/people/c/chengqing-zong/>Chengqing Zong</a><br><a href=/volumes/2021.acl-long/ class=text-muted>Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--184><div class="card-body p-3 small">Emotion category is usually divided into different ones by human beings, but it is indeed difficult to clearly distinguish and define the boundaries between different emotion categories. The existing studies working on <a href=https://en.wikipedia.org/wiki/Emotion_detection>emotion detection</a> usually focus on how to improve the performance of <a href=https://en.wikipedia.org/wiki/Prediction>model prediction</a>, in which <a href=https://en.wikipedia.org/wiki/Emotion>emotions</a> are represented with one-hot vectors. However, emotion relations are ignored in one-hot representations. In this article, we first propose a general framework to learn the <a href=https://en.wikipedia.org/wiki/Distributed_representation>distributed representations</a> for <a href=https://en.wikipedia.org/wiki/Emotion_classification>emotion categories</a> in emotion space from a given <a href=https://en.wikipedia.org/wiki/Emotion_classification>emotion classification dataset</a>. Furthermore, based on the soft labels predicted by the pre-trained neural network model, we derive a simple and effective <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a>. Experiments have validated that the proposed representations in emotion space can express emotion relations much better than word vectors in <a href=https://en.wikipedia.org/wiki/Semantic_space>semantic space</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-short.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-short.0/>Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers)</a></strong><br><a href=/people/c/chengqing-zong/>Chengqing Zong</a>
|
<a href=/people/f/fei-xia/>Fei Xia</a>
|
<a href=/people/w/wenjie-li/>Wenjie Li</a>
|
<a href=/people/r/roberto-navigli/>Roberto Navigli</a><br><a href=/volumes/2021.acl-short/ class=text-muted>Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers)</a></span></p><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.116.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--116 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.116 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939383 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.116/>A Knowledge-driven Generative Model for Multi-implication Chinese Medical Procedure Entity Normalization<span class=acl-fixed-case>C</span>hinese Medical Procedure Entity Normalization</a></strong><br><a href=/people/j/jinghui-yan/>Jinghui Yan</a>
|
<a href=/people/y/yining-wang/>Yining Wang</a>
|
<a href=/people/l/lu-xiang/>Lu Xiang</a>
|
<a href=/people/y/yu-zhou/>Yu Zhou</a>
|
<a href=/people/c/chengqing-zong/>Chengqing Zong</a><br><a href=/volumes/2020.emnlp-main/ class=text-muted>Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--116><div class="card-body p-3 small">Medical entity normalization, which links medical mentions in the text to entities in <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge bases</a>, is an important research topic in medical natural language processing. In this paper, we focus on Chinese medical procedure entity normalization. However, nonstandard Chinese expressions and combined procedures present challenges in our <a href=https://en.wikipedia.org/wiki/Problem_solving>problem</a>. The existing <a href=https://en.wikipedia.org/wiki/Strategy>strategies</a> relying on the <a href=https://en.wikipedia.org/wiki/Discriminative_model>discriminative model</a> are poorly to cope with normalizing combined procedure mentions. We propose a sequence generative framework to directly generate all the corresponding medical procedure entities. we adopt two strategies : category-based constraint decoding and category-based model refining to avoid unrealistic results. The method is capable of linking entities when a mention contains multiple procedure concepts and our comprehensive experiments demonstrate that the proposed model can achieve remarkable improvements over existing baselines, particularly significant in the case of multi-implication Chinese medical procedures.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.acl-main.121.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--acl-main--121 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.acl-main.121 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://slideslive.com/38928726 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.acl-main.121/>Attend, Translate and Summarize : An Efficient Method for Neural Cross-Lingual Summarization</a></strong><br><a href=/people/j/junnan-zhu/>Junnan Zhu</a>
|
<a href=/people/y/yu-zhou/>Yu Zhou</a>
|
<a href=/people/j/jiajun-zhang/>Jiajun Zhang</a>
|
<a href=/people/c/chengqing-zong/>Chengqing Zong</a><br><a href=/volumes/2020.acl-main/ class=text-muted>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--acl-main--121><div class="card-body p-3 small">Cross-lingual summarization aims at summarizing a document in one language (e.g., Chinese) into another language (e.g., English). In this paper, we propose a novel method inspired by the translation pattern in the process of obtaining a cross-lingual summary. We first attend to some words in the source text, then translate them into the target language, and summarize to get the final summary. Specifically, we first employ the encoder-decoder attention distribution to attend to the source words. Second, we present three strategies to acquire the translation probability, which helps obtain the translation candidates for each source word. Finally, each summary word is generated either from the neural distribution or from the translation candidates of source words. Experimental results on Chinese-to-English and English-to-Chinese summarization tasks have shown that our proposed method can significantly outperform the baselines, achieving comparable performance with the state-of-the-art.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.aacl-main.1.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--aacl-main--1 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.aacl-main.1 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.aacl-main.1/>Touch Editing : A Flexible One-Time Interaction Approach for <a href=https://en.wikipedia.org/wiki/Translation>Translation</a></a></strong><br><a href=/people/q/qian-wang/>Qian Wang</a>
|
<a href=/people/j/jiajun-zhang/>Jiajun Zhang</a>
|
<a href=/people/l/lemao-liu/>Lemao Liu</a>
|
<a href=/people/g/guoping-huang/>Guoping Huang</a>
|
<a href=/people/c/chengqing-zong/>Chengqing Zong</a><br><a href=/volumes/2020.aacl-main/ class=text-muted>Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--aacl-main--1><div class="card-body p-3 small">We propose a touch-based editing method for <a href=https://en.wikipedia.org/wiki/Translation>translation</a>, which is more flexible than traditional keyboard-mouse-based translation postediting. This approach relies on <a href=https://en.wikipedia.org/wiki/Somatosensory_system>touch actions</a> that users perform to indicate translation errors. We present a dual-encoder model to handle the actions and generate refined translations. To mimic the user feedback, we adopt the TER algorithm comparing between draft translations and references to automatically extract the simulated actions for training data construction. Experiments on translation datasets with simulated editing actions show that our <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>method</a> significantly improves original translation of Transformer (up to 25.31 BLEU) and outperforms existing interactive translation methods (up to 16.64 BLEU). We also conduct experiments on post-editing dataset to further prove the robustness and effectiveness of our <a href=https://en.wikipedia.org/wiki/Methodology>method</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.0/>Proceedings of the 28th International Conference on Computational Linguistics</a></strong><br><a href=/people/d/donia-scott/>Donia Scott</a>
|
<a href=/people/n/nuria-bel/>Nuria Bel</a>
|
<a href=/people/c/chengqing-zong/>Chengqing Zong</a><br><a href=/volumes/2020.coling-main/ class=text-muted>Proceedings of the 28th International Conference on Computational Linguistics</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.496.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--496 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.496 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.496/>Multimodal Sentence Summarization via Multimodal Selective Encoding</a></strong><br><a href=/people/h/haoran-li/>Haoran Li</a>
|
<a href=/people/j/junnan-zhu/>Junnan Zhu</a>
|
<a href=/people/j/jiajun-zhang/>Jiajun Zhang</a>
|
<a href=/people/x/xiaodong-he/>Xiaodong He</a>
|
<a href=/people/c/chengqing-zong/>Chengqing Zong</a><br><a href=/volumes/2020.coling-main/ class=text-muted>Proceedings of the 28th International Conference on Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--496><div class="card-body p-3 small">This paper studies the problem of generating a summary for a given sentence-image pair. Existing multimodal sequence-to-sequence approaches mainly focus on enhancing the <a href=https://en.wikipedia.org/wiki/Codec>decoder</a> by visual signals, while ignoring that the <a href=https://en.wikipedia.org/wiki/Image>image</a> can improve the ability of the <a href=https://en.wikipedia.org/wiki/Encoder>encoder</a> to identify highlights of a news event or a document. Thus, we propose a multimodal selective gate network that considers reciprocal relationships between textual and multi-level visual features, including global image descriptor, activation grids, and object proposals, to select highlights of the event when encoding the source sentence. In addition, we introduce a modality regularization to encourage the summary to capture the highlights embedded in the image more accurately. To verify the generalization of our model, we adopt the multimodal selective gate to the text-based decoder and multimodal-based decoder. Experimental results on a public multimodal sentence summarization dataset demonstrate the advantage of our <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> over baselines. Further analysis suggests that our proposed multimodal selective gate network can effectively select important information in the input sentence.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.iwslt-1.15.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--iwslt-1--15 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.iwslt-1.15 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://slideslive.com/38929589 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.iwslt-1.15/>CASIA’s System for IWSLT 2020 Open Domain Translation<span class=acl-fixed-case>CASIA</span>’s System for <span class=acl-fixed-case>IWSLT</span> 2020 Open Domain Translation</a></strong><br><a href=/people/q/qian-wang/>Qian Wang</a>
|
<a href=/people/y/yuchen-liu/>Yuchen Liu</a>
|
<a href=/people/c/cong-ma/>Cong Ma</a>
|
<a href=/people/y/yu-lu/>Yu Lu</a>
|
<a href=/people/y/yining-wang/>Yining Wang</a>
|
<a href=/people/l/long-zhou/>Long Zhou</a>
|
<a href=/people/y/yang-zhao/>Yang Zhao</a>
|
<a href=/people/j/jiajun-zhang/>Jiajun Zhang</a>
|
<a href=/people/c/chengqing-zong/>Chengqing Zong</a><br><a href=/volumes/2020.iwslt-1/ class=text-muted>Proceedings of the 17th International Conference on Spoken Language Translation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--iwslt-1--15><div class="card-body p-3 small">This paper describes the CASIA&#8217;s system for the IWSLT 2020 open domain translation task. This year we participate in both ChineseJapanese and JapaneseChinese translation tasks. Our <a href=https://en.wikipedia.org/wiki/System>system</a> is neural machine translation system based on Transformer model. We augment the training data with knowledge distillation and back translation to improve the <a href=https://en.wikipedia.org/wiki/Translation>translation</a> performance. Domain data classification and weighted domain model ensemble are introduced to generate the final translation result. We compare and analyze the performance on <a href=https://en.wikipedia.org/wiki/Software_development_process>development data</a> with different model settings and different <a href=https://en.wikipedia.org/wiki/Data_processing>data processing techniques</a>.</div></div><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1185.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1185 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1185 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1185.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1185" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1185/>Are You for Real? Detecting Identity Fraud via Dialogue Interactions</a></strong><br><a href=/people/w/weikang-wang/>Weikang Wang</a>
|
<a href=/people/j/jiajun-zhang/>Jiajun Zhang</a>
|
<a href=/people/q/qian-li/>Qian Li</a>
|
<a href=/people/c/chengqing-zong/>Chengqing Zong</a>
|
<a href=/people/z/zhifei-li/>Zhifei Li</a><br><a href=/volumes/D19-1/ class=text-muted>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1185><div class="card-body p-3 small">Identity fraud detection is of great importance in many real-world scenarios such as the <a href=https://en.wikipedia.org/wiki/Financial_services>financial industry</a>. However, few studies addressed this problem before. In this paper, we focus on identity fraud detection in <a href=https://en.wikipedia.org/wiki/Loan>loan applications</a> and propose to solve this problem with a novel interactive dialogue system which consists of two <a href=https://en.wikipedia.org/wiki/Modular_programming>modules</a>. One is the knowledge graph (KG) constructor organizing the <a href=https://en.wikipedia.org/wiki/Personal_data>personal information</a> for each loan applicant. The other is structured dialogue management that can dynamically generate a series of questions based on the personal KG to ask the applicants and determine their <a href=https://en.wikipedia.org/wiki/Identity_(social_science)>identity states</a>. We also present a heuristic user simulator based on <a href=https://en.wikipedia.org/wiki/Problem_analysis>problem analysis</a> to evaluate our <a href=https://en.wikipedia.org/wiki/Methodology>method</a>. Experiments have shown that the trainable dialogue system can effectively detect <a href=https://en.wikipedia.org/wiki/Fraud>fraudsters</a>, and achieve higher recognition accuracy compared with rule-based systems. Furthermore, our learned dialogue strategies are interpretable and flexible, which can help promote real-world applications.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1302.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1302 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1302 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1302/>NCLS : Neural Cross-Lingual Summarization<span class=acl-fixed-case>NCLS</span>: Neural Cross-Lingual Summarization</a></strong><br><a href=/people/j/junnan-zhu/>Junnan Zhu</a>
|
<a href=/people/q/qian-wang/>Qian Wang</a>
|
<a href=/people/y/yining-wang/>Yining Wang</a>
|
<a href=/people/y/yu-zhou/>Yu Zhou</a>
|
<a href=/people/j/jiajun-zhang/>Jiajun Zhang</a>
|
<a href=/people/s/shaonan-wang/>Shaonan Wang</a>
|
<a href=/people/c/chengqing-zong/>Chengqing Zong</a><br><a href=/volumes/D19-1/ class=text-muted>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1302><div class="card-body p-3 small">Cross-lingual summarization (CLS) is the task to produce a summary in one particular language for a source document in a different language. Existing methods simply divide this task into two steps : <a href=https://en.wikipedia.org/wiki/Automatic_summarization>summarization</a> and <a href=https://en.wikipedia.org/wiki/Automatic_translation>translation</a>, leading to the problem of <a href=https://en.wikipedia.org/wiki/Error_propagation>error propagation</a>. To handle that, we present an end-to-end CLS framework, which we refer to as Neural Cross-Lingual Summarization (NCLS), for the first time. Moreover, we propose to further improve NCLS by incorporating two related tasks, monolingual summarization and <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>, into the training process of CLS under <a href=https://en.wikipedia.org/wiki/Multi-task_learning>multi-task learning</a>. Due to the lack of supervised CLS data, we propose a round-trip translation strategy to acquire two high-quality large-scale CLS datasets based on existing monolingual summarization datasets. Experimental results have shown that our NCLS achieves remarkable improvement over traditional pipeline methods on both English-to-Chinese and Chinese-to-English CLS human-corrected test sets. In addition, NCLS with <a href=https://en.wikipedia.org/wiki/Multi-task_learning>multi-task learning</a> can further significantly improve the quality of generated summaries. We make our dataset and code publicly available here : http://www.nlpr.ia.ac.cn/cip/dataset.htm.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1330.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1330 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1330 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1330/>Synchronously Generating Two Languages with Interactive Decoding</a></strong><br><a href=/people/y/yining-wang/>Yining Wang</a>
|
<a href=/people/j/jiajun-zhang/>Jiajun Zhang</a>
|
<a href=/people/l/long-zhou/>Long Zhou</a>
|
<a href=/people/y/yuchen-liu/>Yuchen Liu</a>
|
<a href=/people/c/chengqing-zong/>Chengqing Zong</a><br><a href=/volumes/D19-1/ class=text-muted>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1330><div class="card-body p-3 small">In this paper, we introduce a novel interactive approach to translate a source language into two different languages simultaneously and interactively. Specifically, the generation of one language relies on not only previously generated outputs by itself, but also the outputs predicted in the other language. Experimental results on IWSLT and WMT datasets demonstrate that our method can obtain significant improvements over both conventional Neural Machine Translation (NMT) model and multilingual NMT model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/Q19-1006.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-Q19-1006 data-toggle=collapse aria-expanded=false aria-controls=abstract-Q19-1006 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/385255892 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=Q19-1006" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/Q19-1006/>Synchronous Bidirectional Neural Machine Translation</a></strong><br><a href=/people/l/long-zhou/>Long Zhou</a>
|
<a href=/people/j/jiajun-zhang/>Jiajun Zhang</a>
|
<a href=/people/c/chengqing-zong/>Chengqing Zong</a><br><a href=/volumes/Q19-1/ class=text-muted>Transactions of the Association for Computational Linguistics, Volume 7</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-Q19-1006><div class="card-body p-3 small">Existing approaches to neural machine translation (NMT) generate the target language sequence token-by-token from left to right. However, this kind of unidirectional decoding framework can not make full use of the target-side future contexts which can be produced in a right-to-left decoding direction, and thus suffers from the issue of unbalanced outputs. In this paper, we introduce a synchronous bidirectionalneural machine translation (SB-NMT) that predicts its outputs using left-to-right and right-to-left decoding simultaneously and interactively, in order to leverage both of the history and future information at the same time. Specifically, we first propose a new <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> that enables synchronous bidirectional decoding in a single model. Then, we present an interactive decoding model in which left-to-right (right-to-left) generation does not only depend on its previously generated outputs, but also relies on future contexts predicted by right-to-left (left-to-right) decoding. We extensively evaluate the proposed SB-NMT model on large-scale NIST ChineseEnglish, WMT14 EnglishGerman, and WMT18 RussianEnglish translation tasks. Experimental results demonstrate that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieves significant improvements over the strong Transformer model by 3.92, 1.49, and 1.04 BLEU points, respectively, and obtains the state-of-the-art performance on ChineseEnglish and EnglishGerman translation tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1117.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1117 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1117 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-1117/>A Compact and Language-Sensitive Multilingual Translation Method</a></strong><br><a href=/people/y/yining-wang/>Yining Wang</a>
|
<a href=/people/l/long-zhou/>Long Zhou</a>
|
<a href=/people/j/jiajun-zhang/>Jiajun Zhang</a>
|
<a href=/people/f/feifei-zhai/>Feifei Zhai</a>
|
<a href=/people/j/jingfang-xu/>Jingfang Xu</a>
|
<a href=/people/c/chengqing-zong/>Chengqing Zong</a><br><a href=/volumes/P19-1/ class=text-muted>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1117><div class="card-body p-3 small">Multilingual neural machine translation (Multi-NMT) with one encoder-decoder model has made remarkable progress due to its simple deployment. However, this multilingual translation paradigm does not make full use of <a href=https://en.wikipedia.org/wiki/Language_family>language commonality</a> and parameter sharing between encoder and decoder. Furthermore, this kind of <a href=https://en.wikipedia.org/wiki/Paradigm>paradigm</a> can not outperform the individual <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> trained on bilingual corpus in most cases. In this paper, we propose a compact and language-sensitive method for multilingual translation. To maximize parameter sharing, we first present a universal representor to replace both encoder and decoder models. To make the representor sensitive for specific languages, we further introduce language-sensitive embedding, <a href=https://en.wikipedia.org/wiki/Attention>attention</a>, and <a href=https://en.wikipedia.org/wiki/Discriminator>discriminator</a> with the ability to enhance model performance. We verify our methods on various translation scenarios, including one-to-many, many-to-many and zero-shot. Extensive experiments demonstrate that our proposed methods remarkably outperform strong standard multilingual translation systems on WMT and IWSLT datasets. Moreover, we find that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is especially helpful in low-resource and zero-shot translation scenarios.</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1035.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1035 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1035 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1035/>Adopting the Word-Pair-Dependency-Triplets with Individual Comparison for Natural Language Inference</a></strong><br><a href=/people/q/qianlong-du/>Qianlong Du</a>
|
<a href=/people/c/chengqing-zong/>Chengqing Zong</a>
|
<a href=/people/k/keh-yih-su/>Keh-Yih Su</a><br><a href=/volumes/C18-1/ class=text-muted>Proceedings of the 27th International Conference on Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1035><div class="card-body p-3 small">This paper proposes to perform natural language inference with Word-Pair-Dependency-Triplets. Most previous DNN-based approaches either ignore syntactic dependency among words, or directly use tree-LSTM to generate sentence representation with irrelevant information. To overcome the problems mentioned above, we adopt Word-Pair-Dependency-Triplets to improve <a href=https://en.wikipedia.org/wiki/Sequence_alignment>alignment</a> and <a href=https://en.wikipedia.org/wiki/Statistical_inference>inference judgment</a>. To be specific, instead of comparing each <a href=https://en.wikipedia.org/wiki/Tuplet>triplet</a> from one passage with the merged information of another passage, we first propose to perform comparison directly between the <a href=https://en.wikipedia.org/wiki/Tuplet>triplets</a> of the given passage-pair to make the judgement more interpretable. Experimental results show that the performance of our approach is better than most of the approaches that use tree structures, and is comparable to other state-of-the-art approaches.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1079.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1079 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1079 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=C18-1079" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/C18-1079/>Document-level Multi-aspect Sentiment Classification by Jointly Modeling Users, Aspects, and Overall Ratings</a></strong><br><a href=/people/j/junjie-li/>Junjie Li</a>
|
<a href=/people/h/haitong-yang/>Haitong Yang</a>
|
<a href=/people/c/chengqing-zong/>Chengqing Zong</a><br><a href=/volumes/C18-1/ class=text-muted>Proceedings of the 27th International Conference on Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1079><div class="card-body p-3 small">Document-level multi-aspect sentiment classification aims to predict user&#8217;s sentiment polarities for different aspects of a product in a review. Existing approaches mainly focus on <a href=https://en.wikipedia.org/wiki/Text_(literary_theory)>text information</a>. However, the authors (i.e. users) and overall ratings of reviews are ignored, both of which are proved to be significant on interpreting the sentiments of different aspects in this paper. Therefore, we propose a <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> called Hierarchical User Aspect Rating Network (HUARN) to consider user preference and overall ratings jointly. Specifically, HUARN adopts a <a href=https://en.wikipedia.org/wiki/Hierarchical_organization>hierarchical architecture</a> to encode word, sentence, and document level information. Then, user attention and aspect attention are introduced into building sentence and document level representation. The document representation is combined with user and overall rating information to predict aspect ratings of a review. Diverse aspects are treated differently and a multi-task framework is adopted. Empirical results on two real-world datasets show that HUARN achieves state-of-the-art performances.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1305.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1305 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1305 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1305/>Source Critical Reinforcement Learning for Transferring Spoken Language Understanding to a New Language</a></strong><br><a href=/people/h/he-bai/>He Bai</a>
|
<a href=/people/y/yu-zhou/>Yu Zhou</a>
|
<a href=/people/j/jiajun-zhang/>Jiajun Zhang</a>
|
<a href=/people/l/liang-zhao/>Liang Zhao</a>
|
<a href=/people/m/mei-yuh-hwang/>Mei-Yuh Hwang</a>
|
<a href=/people/c/chengqing-zong/>Chengqing Zong</a><br><a href=/volumes/C18-1/ class=text-muted>Proceedings of the 27th International Conference on Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1305><div class="card-body p-3 small">To deploy a spoken language understanding (SLU) model to a new language, language transferring is desired to avoid the trouble of acquiring and labeling a new big SLU corpus. An SLU corpus is a monolingual corpus with domain / intent / slot labels. Translating the original SLU corpus into the target language is an attractive strategy. However, SLU corpora consist of plenty of semantic labels (slots), which general-purpose translators can not handle well, not to mention additional culture differences. This paper focuses on the language transferring task given a small in-domain parallel SLU corpus. The in-domain parallel corpus can be used as the first adaptation on the general translator. But more importantly, we show how to use <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning (RL)</a> to further adapt the adapted <a href=https://en.wikipedia.org/wiki/Translation>translator</a>, where translated sentences with more proper slot tags receive higher rewards. Our <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reward</a> is derived from the source input sentence exclusively, unlike <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reward</a> via actor-critical methods or computing <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reward</a> with a ground truth target sentence. Hence we can adapt the <a href=https://en.wikipedia.org/wiki/Translation>translator</a> the second time, using the big monolingual SLU corpus from the source language. We evaluate our approach on Chinese to English language transferring for SLU systems. The experimental results show that the generated English SLU corpus via adaptation and <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning</a> gives us over 97 % in the slot F1 score and over 84 % accuracy in domain classification. It demonstrates the effectiveness of the proposed language transferring method. Compared with naive translation, our proposed method improves domain classification accuracy by relatively 22 %, and the slot filling F1 score by relatively more than 71 %.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1011.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1011 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1011 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/305209813 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1011" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1011/>Associative Multichannel Autoencoder for Multimodal Word Representation</a></strong><br><a href=/people/s/shaonan-wang/>Shaonan Wang</a>
|
<a href=/people/j/jiajun-zhang/>Jiajun Zhang</a>
|
<a href=/people/c/chengqing-zong/>Chengqing Zong</a><br><a href=/volumes/D18-1/ class=text-muted>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1011><div class="card-body p-3 small">In this paper we address the problem of learning multimodal word representations by integrating textual, visual and auditory inputs. Inspired by the re-constructive and associative nature of human memory, we propose a novel associative multichannel autoencoder (AMA). Our model first learns the associations between textual and perceptual modalities, so as to predict the missing perceptual information of concepts. Then the textual and predicted perceptual representations are fused through reconstructing their original and associated embeddings. Using a gating mechanism our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> assigns different weights to each modality according to the different concepts. Results on six benchmark concept similarity tests show that the proposed method significantly outperforms strong unimodal baselines and state-of-the-art multimodal models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1036.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1036 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1036 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-1036/>Addressing Troublesome Words in <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a></a></strong><br><a href=/people/y/yang-zhao/>Yang Zhao</a>
|
<a href=/people/j/jiajun-zhang/>Jiajun Zhang</a>
|
<a href=/people/z/zhongjun-he/>Zhongjun He</a>
|
<a href=/people/c/chengqing-zong/>Chengqing Zong</a>
|
<a href=/people/h/hua-wu/>Hua Wu</a><br><a href=/volumes/D18-1/ class=text-muted>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1036><div class="card-body p-3 small">One of the weaknesses of Neural Machine Translation (NMT) is in handling lowfrequency and ambiguous words, which we refer as troublesome words. To address this problem, we propose a novel memoryenhanced NMT method. First, we investigate different <a href=https://en.wikipedia.org/wiki/Strategy>strategies</a> to define and detect the troublesome words. Then, a contextual memory is constructed to memorize which target words should be produced in what situations. Finally, we design a hybrid model to dynamically access the contextual memory so as to correctly translate the troublesome words. The extensive experiments on Chinese-to-English and English-to-German translation tasks demonstrate that our method significantly outperforms the strong baseline models in translation quality, especially in handling troublesome words.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1326.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1326 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1326 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-1326/>Three Strategies to Improve One-to-Many Multilingual Translation</a></strong><br><a href=/people/y/yining-wang/>Yining Wang</a>
|
<a href=/people/j/jiajun-zhang/>Jiajun Zhang</a>
|
<a href=/people/f/feifei-zhai/>Feifei Zhai</a>
|
<a href=/people/j/jingfang-xu/>Jingfang Xu</a>
|
<a href=/people/c/chengqing-zong/>Chengqing Zong</a><br><a href=/volumes/D18-1/ class=text-muted>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1326><div class="card-body p-3 small">Due to the benefits of model compactness, multilingual translation (including many-to-one, many-to-many and one-to-many) based on a universal encoder-decoder architecture attracts more and more attention. However, previous studies show that one-to-many translation based on this <a href=https://en.wikipedia.org/wiki/Conceptual_framework>framework</a> can not perform on par with the individually trained models. In this work, we introduce three <a href=https://en.wikipedia.org/wiki/Strategy>strategies</a> to improve one-to-many multilingual translation by balancing the shared and unique features. Within the architecture of one <a href=https://en.wikipedia.org/wiki/Codec>decoder</a> for all target languages, we first exploit the use of unique initial states for different target languages. Then, we employ language-dependent positional embeddings. Finally and especially, we propose to divide the hidden cells of the <a href=https://en.wikipedia.org/wiki/Code>decoder</a> into shared and language-dependent ones. The extensive experiments demonstrate that our proposed methods can obtain remarkable improvements over the strong <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a>. Moreover, our <a href=https://en.wikipedia.org/wiki/Strategy>strategies</a> can achieve comparable or even better performance than the individually trained <a href=https://en.wikipedia.org/wiki/Translation_(biology)>translation models</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1448.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1448 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1448 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D18-1448.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1448/>MSMO : Multimodal Summarization with Multimodal Output<span class=acl-fixed-case>MSMO</span>: Multimodal Summarization with Multimodal Output</a></strong><br><a href=/people/j/junnan-zhu/>Junnan Zhu</a>
|
<a href=/people/h/haoran-li/>Haoran Li</a>
|
<a href=/people/t/tianshang-liu/>Tianshang Liu</a>
|
<a href=/people/y/yu-zhou/>Yu Zhou</a>
|
<a href=/people/j/jiajun-zhang/>Jiajun Zhang</a>
|
<a href=/people/c/chengqing-zong/>Chengqing Zong</a><br><a href=/volumes/D18-1/ class=text-muted>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1448><div class="card-body p-3 small">Multimodal summarization has drawn much attention due to the rapid growth of <a href=https://en.wikipedia.org/wiki/Multimedia>multimedia data</a>. The output of the current multimodal summarization systems is usually represented in texts. However, we have found through experiments that multimodal output can significantly improve user satisfaction for informativeness of summaries. In this paper, we propose a novel task, multimodal summarization with multimodal output (MSMO). To handle this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>, we first collect a <a href=https://en.wikipedia.org/wiki/Data_set>large-scale dataset</a> for MSMO research. We then propose a multimodal attention model to jointly generate text and select the most relevant image from the multimodal input. Finally, to evaluate multimodal outputs, we construct a novel multimodal automatic evaluation (MMAE) method which considers both intra-modality salience and inter-modality relevance. The experimental results show the effectiveness of <a href=https://en.wikipedia.org/wiki/MMAE>MMAE</a>.</div></div><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1039.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1039 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1039 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-1039/>Towards <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a> with Partially Aligned Corpora</a></strong><br><a href=/people/y/yining-wang/>Yining Wang</a>
|
<a href=/people/y/yang-zhao/>Yang Zhao</a>
|
<a href=/people/j/jiajun-zhang/>Jiajun Zhang</a>
|
<a href=/people/c/chengqing-zong/>Chengqing Zong</a>
|
<a href=/people/z/zhengshan-xue/>Zhengshan Xue</a><br><a href=/volumes/I17-1/ class=text-muted>Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1039><div class="card-body p-3 small">While neural machine translation (NMT) has become the new paradigm, the parameter optimization requires large-scale parallel data which is scarce in many domains and language pairs. In this paper, we address a new translation scenario in which there only exists monolingual corpora and phrase pairs. We propose a new method towards <a href=https://en.wikipedia.org/wiki/Translation>translation</a> with partially aligned sentence pairs which are derived from the phrase pairs and <a href=https://en.wikipedia.org/wiki/Text_corpus>monolingual corpora</a>. To make full use of the partially aligned corpora, we adapt the conventional NMT training method in two aspects. On one hand, different generation strategies are designed for aligned and unaligned target words. On the other hand, a different <a href=https://en.wikipedia.org/wiki/Loss_function>objective function</a> is designed to model the partially aligned parts. The experiments demonstrate that our method can achieve a relatively good result in such a translation scenario, and tiny bitexts can boost translation quality to a large extent.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2060.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2060 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2060 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P17-2060.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-2060/>Neural System Combination for <a href=https://en.wikipedia.org/wiki/Machine_translation>Machine Translation</a></a></strong><br><a href=/people/l/long-zhou/>Long Zhou</a>
|
<a href=/people/w/wenpeng-hu/>Wenpeng Hu</a>
|
<a href=/people/j/jiajun-zhang/>Jiajun Zhang</a>
|
<a href=/people/c/chengqing-zong/>Chengqing Zong</a><br><a href=/volumes/P17-2/ class=text-muted>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2060><div class="card-body p-3 small">Neural machine translation (NMT) becomes a new approach to <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> and generates much more fluent results compared to statistical machine translation (SMT). However, SMT is usually better than NMT in translation adequacy. It is therefore a promising direction to combine the advantages of both <a href=https://en.wikipedia.org/wiki/Network_topology>NMT</a> and <a href=https://en.wikipedia.org/wiki/Network_topology>SMT</a>. In this paper, we propose a neural system combination framework leveraging multi-source NMT, which takes as input the outputs of NMT and SMT systems and produces the final translation. Extensive experiments on the Chinese-to-English translation task show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> archives significant improvement by 5.3 BLEU points over the best single system output and 3.4 BLEU points over the state-of-the-art traditional system combination methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-6005.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-6005 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-6005 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-6005/>Learning from Parenthetical Sentences for Term Translation in Machine Translation</a></strong><br><a href=/people/g/guoping-huang/>Guoping Huang</a>
|
<a href=/people/j/jiajun-zhang/>Jiajun Zhang</a>
|
<a href=/people/y/yu-zhou/>Yu Zhou</a>
|
<a href=/people/c/chengqing-zong/>Chengqing Zong</a><br><a href=/volumes/W17-60/ class=text-muted>Proceedings of the 9th SIGHAN Workshop on Chinese Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-6005><div class="card-body p-3 small">Terms extensively exist in specific domains, and term translation plays a critical role in domain-specific machine translation (MT) tasks. However, it&#8217;s a challenging task to translate them correctly for the huge number of pre-existing terms and the endless new terms. To achieve better term translation quality, it is necessary to inject external term knowledge into the underlying MT system. Fortunately, there are plenty of term translation knowledge in parenthetical sentences on the Internet. In this paper, we propose a simple, straightforward and effective <a href=https://en.wikipedia.org/wiki/Conceptual_framework>framework</a> to improve term translation by learning from <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>parenthetical sentences</a>. This framework includes : (1) a focused web crawler ; (2) a parenthetical sentence filter, acquiring parenthetical sentences including bilingual term pairs ; (3) a term translation knowledge extractor, extracting bilingual term translation candidates ; (4) a probability learner, generating the term translation table for MT decoders. The extensive experiments demonstrate that our proposed <a href=https://en.wikipedia.org/wiki/Conceptual_framework>framework</a> significantly improves the translation quality of terms and sentences.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1029.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1029 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1029 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1029/>Exploiting Word Internal Structures for Generic Chinese Sentence Representation<span class=acl-fixed-case>C</span>hinese Sentence Representation</a></strong><br><a href=/people/s/shaonan-wang/>Shaonan Wang</a>
|
<a href=/people/j/jiajun-zhang/>Jiajun Zhang</a>
|
<a href=/people/c/chengqing-zong/>Chengqing Zong</a><br><a href=/volumes/D17-1/ class=text-muted>Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1029><div class="card-body p-3 small">We introduce a novel mixed characterword architecture to improve Chinese sentence representations, by utilizing rich semantic information of word internal structures. Our <a href=https://en.wikipedia.org/wiki/Architecture>architecture</a> uses two key strategies. The first is a mask gate on characters, learning the relation among characters in a word. The second is a maxpooling operation on words, adaptively finding the optimal mixture of the atomic and compositional word representations. Finally, the proposed <a href=https://en.wikipedia.org/wiki/Software_architecture>architecture</a> is applied to various sentence composition models, which achieves substantial performance gains over baseline models on sentence similarity task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1114.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1114 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1114 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/238234442 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D17-1114/>Multi-modal Summarization for Asynchronous Collection of Text, Image, Audio and Video</a></strong><br><a href=/people/h/haoran-li/>Haoran Li</a>
|
<a href=/people/j/junnan-zhu/>Junnan Zhu</a>
|
<a href=/people/c/cong-ma/>Cong Ma</a>
|
<a href=/people/j/jiajun-zhang/>Jiajun Zhang</a>
|
<a href=/people/c/chengqing-zong/>Chengqing Zong</a><br><a href=/volumes/D17-1/ class=text-muted>Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1114><div class="card-body p-3 small">The rapid increase of the multimedia data over the <a href=https://en.wikipedia.org/wiki/Internet>Internet</a> necessitates multi-modal summarization from collections of text, image, audio and video. In this work, we propose an extractive Multi-modal Summarization (MMS) method which can automatically generate a textual summary given a set of <a href=https://en.wikipedia.org/wiki/Document>documents</a>, <a href=https://en.wikipedia.org/wiki/Image>images</a>, audios and <a href=https://en.wikipedia.org/wiki/Video>videos</a> related to a specific topic. The key idea is to bridge the semantic gaps between multi-modal contents. For <a href=https://en.wikipedia.org/wiki/Audio_signal>audio information</a>, we design an approach to selectively use its <a href=https://en.wikipedia.org/wiki/Transcription_(biology)>transcription</a>. For vision information, we learn joint representations of texts and images using a <a href=https://en.wikipedia.org/wiki/Neural_network>neural network</a>. Finally, all the multi-modal aspects are considered to generate the textural summary by maximizing the salience, <a href=https://en.wikipedia.org/wiki/Redundancy_(engineering)>non-redundancy</a>, <a href=https://en.wikipedia.org/wiki/Readability>readability</a> and <a href=https://en.wikipedia.org/wiki/Coverage_(statistics)>coverage</a> through budgeted optimization of submodular functions. We further introduce an <a href=https://en.wikipedia.org/wiki/Multimedia_Messaging_Service>MMS corpus</a> in <a href=https://en.wikipedia.org/wiki/English_language>English</a> and <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a>. The experimental results on this <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> demonstrate that our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> outperforms other competitive baseline methods.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Chengqing+Zong" title="Search for 'Chengqing Zong' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/j/jiajun-zhang/ class=align-middle>Jiajun Zhang</a>
<span class="badge badge-secondary align-middle ml-2">19</span></li><li class=list-group-item><a href=/people/y/yining-wang/ class=align-middle>Yining Wang</a>
<span class="badge badge-secondary align-middle ml-2">7</span></li><li class=list-group-item><a href=/people/y/yu-zhou/ class=align-middle>Yu Zhou</a>
<span class="badge badge-secondary align-middle ml-2">6</span></li><li class=list-group-item><a href=/people/j/junnan-zhu/ class=align-middle>Junnan Zhu</a>
<span class="badge badge-secondary align-middle ml-2">5</span></li><li class=list-group-item><a href=/people/l/long-zhou/ class=align-middle>Long Zhou</a>
<span class="badge badge-secondary align-middle ml-2">5</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/y/yang-zhao/ class=align-middle>Yang Zhao</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/q/qian-wang/ class=align-middle>Qian Wang</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/s/shaonan-wang/ class=align-middle>Shaonan Wang</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/h/haoran-li/ class=align-middle>Haoran Li</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/f/fei-xia/ class=align-middle>Fei Xia</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/w/wenjie-li/ class=align-middle>Wenjie Li</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/r/roberto-navigli/ class=align-middle>Roberto Navigli</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/g/guoping-huang/ class=align-middle>Guoping Huang</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/f/feifei-zhai/ class=align-middle>Feifei Zhai</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/j/jingfang-xu/ class=align-middle>Jingfang Xu</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/y/yuchen-liu/ class=align-middle>Yuchen Liu</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/c/cong-ma/ class=align-middle>Cong Ma</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/q/qianlong-du/ class=align-middle>Qianlong Du</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/keh-yih-su/ class=align-middle>Keh-Yih Su</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/junjie-li/ class=align-middle>Junjie Li</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/haitong-yang/ class=align-middle>Haitong Yang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/he-bai/ class=align-middle>He Bai</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/liang-zhao/ class=align-middle>Liang Zhao</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/mei-yuh-hwang/ class=align-middle>Mei-Yuh Hwang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/x/xiangyu-wang/ class=align-middle>Xiangyu Wang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/zhengshan-xue/ class=align-middle>Zhengshan Xue</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jinghui-yan/ class=align-middle>Jinghui Yan</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/lu-xiang/ class=align-middle>Lu Xiang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/w/wenpeng-hu/ class=align-middle>Wenpeng Hu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/lemao-liu/ class=align-middle>Lemao Liu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/zhongjun-he/ class=align-middle>Zhongjun He</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/hua-wu/ class=align-middle>Hua Wu (吴华)</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/tianshang-liu/ class=align-middle>Tianshang Liu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/w/weikang-wang/ class=align-middle>Weikang Wang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/q/qian-li/ class=align-middle>Qian Li</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/zhifei-li/ class=align-middle>Zhifei Li</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/donia-scott/ class=align-middle>Donia Scott</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/n/nuria-bel/ class=align-middle>Núria Bel</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/x/xiaodong-he/ class=align-middle>Xiaodong He</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yu-lu/ class=align-middle>Yu Lu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/emnlp/ class=align-middle>EMNLP</a><span class="badge badge-secondary align-middle ml-2">10</span></li><li class=list-group-item><a href=/venues/acl/ class=align-middle>ACL</a><span class="badge badge-secondary align-middle ml-2">6</span></li><li class=list-group-item><a href=/venues/coling/ class=align-middle>COLING</a><span class="badge badge-secondary align-middle ml-2">5</span></li><li class=list-group-item><a href=/venues/ijcnlp/ class=align-middle>IJCNLP</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/ws/ class=align-middle>WS</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-venues aria-expanded=false aria-controls=more-venues>show all...</li><div class="collapse border-top" id=more-venues><li class=list-group-item><a href=/venues/aacl/ class=align-middle>AACL</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/tacl/ class=align-middle>TACL</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/iwslt/ class=align-middle>IWSLT</a><span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>