<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Chu-Ren Huang - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Chu-Ren</span> <span class=font-weight-bold>Huang</span></h2><hr><div class=row><div class=col-lg-9><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.sigdial-1.26.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--sigdial-1--26 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.sigdial-1.26 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href="https://www.youtube.com/watch?v=yNtYLKCo3xI" data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2021.sigdial-1.26/>Scikit-talk : A toolkit for processing real-world conversational speech data<span class=acl-fixed-case>S</span>cikit-talk: A toolkit for processing real-world conversational speech data</a></strong><br><a href=/people/a/andreas-liesenfeld/>Andreas Liesenfeld</a>
|
<a href=/people/g/gabor-parti/>Gabor Parti</a>
|
<a href=/people/c/chu-ren-huang/>Chu-Ren Huang</a><br><a href=/volumes/2021.sigdial-1/ class=text-muted>Proceedings of the 22nd Annual Meeting of the Special Interest Group on Discourse and Dialogue</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--sigdial-1--26><div class="card-body p-3 small">We present Scikit-talk, an open-source toolkit for processing collections of real-world conversational speech in <a href=https://en.wikipedia.org/wiki/Python_(programming_language)>Python</a>. First of its kind, the <a href=https://en.wikipedia.org/wiki/List_of_toolkits>toolkit</a> equips those interested in studying or modeling conversations with an easy-to-use interface to build and explore large collections of transcriptions and annotations of talk-in-interaction. Designed for applications in <a href=https://en.wikipedia.org/wiki/Speech_processing>speech processing</a> and Conversational AI, Scikit-talk provides tools to custom-build datasets for tasks such as intent prototyping, dialog flow testing, and conversation design. Its preprocessor module comes with several pre-built interfaces for common transcription formats, which aim to make working across multiple data sources more accessible. The explorer module provides a collection of tools to explore and analyse this data type via <a href=https://en.wikipedia.org/wiki/String_matching>string matching</a> and unsupervised machine learning techniques. Scikit-talk serves as a platform to collect and connect different transcription formats and representations of talk, enabling the user to quickly build multilingual datasets of varying detail and granularity. Thus, the toolkit aims to make working with authentic conversational speech data in <a href=https://en.wikipedia.org/wiki/Python_(programming_language)>Python</a> more accessible and to provide the user with comprehensive options to work with representations of talk in appropriate detail for any downstream task. For the latest updates and information on currently supported languages and language resources, please refer to : https://pypi.org/project/scikit-talk/<i>preprocessor</i> module comes with several pre-built interfaces for common transcription formats, which aim to make working across multiple data sources more accessible. The <i>explorer</i> module provides a collection of tools to explore and analyse this data type via string matching and unsupervised machine learning techniques. Scikit-talk serves as a platform to collect and connect different transcription formats and representations of talk, enabling the user to quickly build multilingual datasets of varying detail and granularity. Thus, the toolkit aims to make working with authentic conversational speech data in Python more accessible and to provide the user with comprehensive options to work with representations of talk in appropriate detail for any downstream task. For the latest updates and information on currently supported languages and language resources, please refer to: https://pypi.org/project/scikit-talk/</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.cl-3.20.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--cl-3--20 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.cl-3.20 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.cl-3.20/>Decoding Word Embeddings with Brain-Based Semantic Features</a></strong><br><a href=/people/e/emmanuele-chersoni/>Emmanuele Chersoni</a>
|
<a href=/people/e/enrico-santus/>Enrico Santus</a>
|
<a href=/people/c/chu-ren-huang/>Chu-Ren Huang</a>
|
<a href=/people/a/alessandro-lenci/>Alessandro Lenci</a><br><a href=/volumes/2021.cl-3/ class=text-muted>Computational Linguistics, Volume 47, Issue 3 - November 2021</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--cl-3--20><div class="card-body p-3 small">Word embeddings are vectorial semantic representations built with either counting or predicting techniques aimed at capturing shades of meaning from word co-occurrences. Since their introduction, these <a href=https://en.wikipedia.org/wiki/Representation_(arts)>representations</a> have been criticized for lacking interpretable dimensions. This property of <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> limits our understanding of the <a href=https://en.wikipedia.org/wiki/Semantic_feature>semantic features</a> they actually encode. Moreover, it contributes to the black box nature of the tasks in which they are used, since the reasons for <a href=https://en.wikipedia.org/wiki/Word_embedding>word embedding</a> performance often remain opaque to humans. In this contribution, we explore the semantic properties encoded in word embeddings by mapping them onto interpretable vectors, consisting of explicit and neurobiologically motivated semantic features (Binder et al. Our exploration takes into account different types of embeddings, including factorized count vectors and predict models (Skip-Gram, GloVe, etc.), as well as the most recent contextualized representations (i.e., ELMo and BERT). In our analysis, we first evaluate the quality of the <a href=https://en.wikipedia.org/wiki/Map_(mathematics)>mapping</a> in a retrieval task, then we shed light on the semantic features that are better encoded in each embedding type. A large number of probing tasks is finally set to assess how the original and the mapped embeddings perform in discriminating semantic categories. For each probing task, we identify the most relevant semantic features and we show that there is a correlation between the <a href=https://en.wikipedia.org/wiki/Embedding>embedding</a> performance and how they encode those <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a>. This study sets itself as a step forward in understanding which aspects of meaning are captured by <a href=https://en.wikipedia.org/wiki/Vector_space>vector spaces</a>, by proposing a new and simple method to carve human-interpretable semantic representations from <a href=https://en.wikipedia.org/wiki/Distribution_(mathematics)>distributional vectors</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.rocling-1.51.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--rocling-1--51 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.rocling-1.51 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.rocling-1.51/>ROCLING-2021 Shared Task : <a href=https://en.wikipedia.org/wiki/Dimensional_analysis>Dimensional Sentiment Analysis</a> for Educational Texts<span class=acl-fixed-case>ROCLING</span>-2021 Shared Task: Dimensional Sentiment Analysis for Educational Texts</a></strong><br><a href=/people/l/liang-chih-yu/>Liang-Chih Yu</a>
|
<a href=/people/j/jin-wang/>Jin Wang</a>
|
<a href=/people/b/bo-peng/>Bo Peng</a>
|
<a href=/people/c/chu-ren-huang/>Chu-Ren Huang</a><br><a href=/volumes/2021.rocling-1/ class=text-muted>Proceedings of the 33rd Conference on Computational Linguistics and Speech Processing (ROCLING 2021)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--rocling-1--51><div class="card-body p-3 small">This paper presents the ROCLING 2021 shared task on dimensional sentiment analysis for educational texts which seeks to identify a real-value sentiment score of self-evaluation comments written by Chinese students in the both valence and arousal dimensions. Valence represents the degree of pleasant and unpleasant (or positive and negative) feelings, and <a href=https://en.wikipedia.org/wiki/Arousal>arousal</a> represents the degree of excitement and calm. Of the 7 teams registered for this shared task for two-dimensional sentiment analysis, 6 submitted results. We expected that this evaluation campaign could produce more advanced dimensional sentiment analysis techniques for the educational domain. All data sets with gold standards and scoring script are made publicly available to researchers.</div></div><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.dmr-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.dmr-1.0/>Proceedings of the Second International Workshop on Designing Meaning Representations</a></strong><br><a href=/people/n/nianwen-xue/>Nianwen Xue</a>
|
<a href=/people/j/johan-bos/>Johan Bos</a>
|
<a href=/people/w/william-croft/>William Croft</a>
|
<a href=/people/j/jan-hajic/>Jan Hajič</a>
|
<a href=/people/c/chu-ren-huang/>Chu-Ren Huang</a>
|
<a href=/people/s/stephan-oepen/>Stephan Oepen</a>
|
<a href=/people/m/martha-palmer/>Martha Palmer</a>
|
<a href=/people/j/james-pustejovsky/>James Pustejovsky</a><br><a href=/volumes/2020.dmr-1/ class=text-muted>Proceedings of the Second International Workshop on Designing Meaning Representations</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.figlang-1.16.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--figlang-1--16 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.figlang-1.16 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://slideslive.com/38929723 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.figlang-1.16/>Using Conceptual Norms for Metaphor Detection</a></strong><br><a href=/people/m/mingyu-wan/>Mingyu Wan</a>
|
<a href=/people/k/kathleen-ahrens/>Kathleen Ahrens</a>
|
<a href=/people/e/emmanuele-chersoni/>Emmanuele Chersoni</a>
|
<a href=/people/m/menghan-jiang/>Menghan Jiang</a>
|
<a href=/people/q/qi-su/>Qi Su</a>
|
<a href=/people/r/rong-xiang/>Rong Xiang</a>
|
<a href=/people/c/chu-ren-huang/>Chu-Ren Huang</a><br><a href=/volumes/2020.figlang-1/ class=text-muted>Proceedings of the Second Workshop on Figurative Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--figlang-1--16><div class="card-body p-3 small">This paper reports a linguistically-enriched method of detecting token-level metaphors for the second shared task on Metaphor Detection. We participate in all four phases of competition with both <a href=https://en.wikipedia.org/wiki/Digital_data>datasets</a>, i.e. Verbs and AllPOS on the VUA and the TOFEL datasets. We use the modality exclusivity and embodiment norms for constructing a conceptual representation of the nodes and the context. Our <a href=https://en.wikipedia.org/wiki/System>system</a> obtains an <a href=https://en.wikipedia.org/wiki/International_Federation_of_the_Phonographic_Industry>F-score</a> of 0.652 for the VUA Verbs track, which is 5 % higher than the strong baselines. The experimental results across models and datasets indicate the salient contribution of using modality exclusivity and modality shift information for predicting <a href=https://en.wikipedia.org/wiki/Metaphor>metaphoricity</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lincr-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lincr-1.0/>Proceedings of the Second Workshop on Linguistic and Neurocognitive Resources</a></strong><br><a href=/people/e/emmanuele-chersoni/>Emmanuele Chersoni</a>
|
<a href=/people/b/barry-devereux/>Barry Devereux</a>
|
<a href=/people/c/chu-ren-huang/>Chu-Ren Huang</a><br><a href=/volumes/2020.lincr-1/ class=text-muted>Proceedings of the Second Workshop on Linguistic and Neurocognitive Resources</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.14.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--14 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.14 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.14/>Affection Driven Neural Networks for Sentiment Analysis</a></strong><br><a href=/people/r/rong-xiang/>Rong Xiang</a>
|
<a href=/people/y/yunfei-long/>Yunfei Long</a>
|
<a href=/people/m/mingyu-wan/>Mingyu Wan</a>
|
<a href=/people/j/jinghang-gu/>Jinghang Gu</a>
|
<a href=/people/q/qin-lu/>Qin Lu</a>
|
<a href=/people/c/chu-ren-huang/>Chu-Ren Huang</a><br><a href=/volumes/2020.lrec-1/ class=text-muted>Proceedings of the 12th Language Resources and Evaluation Conference</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--14><div class="card-body p-3 small">Deep neural network models have played a critical role in <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> with promising results in the recent decade. One of the essential challenges, however, is how external sentiment knowledge can be effectively utilized. In this work, we propose a novel affection-driven approach to incorporating affective knowledge into <a href=https://en.wikipedia.org/wiki/Neural_circuit>neural network models</a>. The affective knowledge is obtained in the form of a lexicon under the Affect Control Theory (ACT), which is represented by vectors of three-dimensional attributes in Evaluation, Potency, and Activity (EPA). The EPA vectors are mapped to an affective influence value and then integrated into Long Short-term Memory (LSTM) models to highlight affective terms. Experimental results show a consistent improvement of our approach over conventional LSTM models by 1.0 % to 1.5 % in <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> on three large benchmark datasets. Evaluations across a variety of <a href=https://en.wikipedia.org/wiki/Algorithm>algorithms</a> have also proven the effectiveness of leveraging affective terms for deep model enhancement.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.700.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--700 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.700 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.700/>Are Word Embeddings Really a Bad Fit for the Estimation of Thematic Fit?</a></strong><br><a href=/people/e/emmanuele-chersoni/>Emmanuele Chersoni</a>
|
<a href=/people/l/ludovica-pannitto/>Ludovica Pannitto</a>
|
<a href=/people/e/enrico-santus/>Enrico Santus</a>
|
<a href=/people/a/alessandro-lenci/>Alessandro Lenci</a>
|
<a href=/people/c/chu-ren-huang/>Chu-Ren Huang</a><br><a href=/volumes/2020.lrec-1/ class=text-muted>Proceedings of the 12th Language Resources and Evaluation Conference</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--700><div class="card-body p-3 small">While neural embeddings represent a popular choice for word representation in a wide variety of NLP tasks, their usage for thematic fit modeling has been limited, as they have been reported to lag behind syntax-based count models. In this paper, we propose a complete evaluation of count models and <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> on thematic fit estimation, by taking into account a larger number of parameters and verb roles and introducing also dependency-based embeddings in the comparison. Our results show a complex scenario, where a determinant factor for the performance seems to be the availability to the model of reliable syntactic information for building the distributional representations of the roles.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.701.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--701 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.701 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.701/>Ciron : a New Benchmark Dataset for Chinese Irony Detection<span class=acl-fixed-case>C</span>iron: a New Benchmark Dataset for <span class=acl-fixed-case>C</span>hinese Irony Detection</a></strong><br><a href=/people/r/rong-xiang/>Rong Xiang</a>
|
<a href=/people/x/xuefeng-gao/>Xuefeng Gao</a>
|
<a href=/people/y/yunfei-long/>Yunfei Long</a>
|
<a href=/people/a/anran-li/>Anran Li</a>
|
<a href=/people/e/emmanuele-chersoni/>Emmanuele Chersoni</a>
|
<a href=/people/q/qin-lu/>Qin Lu</a>
|
<a href=/people/c/chu-ren-huang/>Chu-Ren Huang</a><br><a href=/volumes/2020.lrec-1/ class=text-muted>Proceedings of the 12th Language Resources and Evaluation Conference</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--701><div class="card-body p-3 small">Automatic Chinese irony detection is a challenging task, and it has a strong impact on <a href=https://en.wikipedia.org/wiki/Linguistics>linguistic research</a>. However, Chinese irony detection often lacks labeled benchmark datasets. In this paper, we introduce <a href=https://en.wikipedia.org/wiki/Iron>Ciron</a>, the first Chinese benchmark dataset available for irony detection for <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning models</a>. Ciron includes more than 8.7 K posts, collected from <a href=https://en.wikipedia.org/wiki/Sina_Weibo>Weibo</a>, a <a href=https://en.wikipedia.org/wiki/Microblogging_in_China>micro blogging platform</a>. Most importantly, <a href=https://en.wikipedia.org/wiki/Ciron>Ciron</a> is collected with no pre-conditions to ensure a much wider coverage. Evaluation on seven different <a href=https://en.wikipedia.org/wiki/Statistical_classification>machine learning classifiers</a> proves the usefulness of <a href=https://en.wikipedia.org/wiki/Ciron>Ciron</a> as an important resource for Chinese irony detection.</div></div><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3300.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3300/>Proceedings of the First International Workshop on Designing Meaning Representations</a></strong><br><a href=/people/n/nianwen-xue/>Nianwen Xue</a>
|
<a href=/people/w/william-croft/>William Croft</a>
|
<a href=/people/j/jan-hajic/>Jan Hajic</a>
|
<a href=/people/c/chu-ren-huang/>Chu-Ren Huang</a>
|
<a href=/people/s/stephan-oepen/>Stephan Oepen</a>
|
<a href=/people/m/martha-palmer/>Martha Palmer</a>
|
<a href=/people/j/james-pustejovksy/>James Pustejovksy</a><br><a href=/volumes/W19-33/ class=text-muted>Proceedings of the First International Workshop on Designing Meaning Representations</a></span></p><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6220.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6220 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6220 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6220/>Dual Memory Network Model for Biased Product Review Classification</a></strong><br><a href=/people/y/yunfei-long/>Yunfei Long</a>
|
<a href=/people/m/mingyu-ma/>Mingyu Ma</a>
|
<a href=/people/q/qin-lu/>Qin Lu</a>
|
<a href=/people/r/rong-xiang/>Rong Xiang</a>
|
<a href=/people/c/chu-ren-huang/>Chu-Ren Huang</a><br><a href=/volumes/W18-62/ class=text-muted>Proceedings of the 9th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6220><div class="card-body p-3 small">In sentiment analysis (SA) of product reviews, both user and product information are proven to be useful. Current tasks handle <a href=https://en.wikipedia.org/wiki/User_profile>user profile</a> and product information in a unified model which may not be able to learn salient features of users and products effectively. In this work, we propose a dual user and product memory network (DUPMN) model to learn user profiles and product reviews using separate memory networks. Then, the two <a href=https://en.wikipedia.org/wiki/Representation_(mathematics)>representations</a> are used jointly for <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment prediction</a>. The use of separate <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> aims to capture <a href=https://en.wikipedia.org/wiki/User_profile>user profiles</a> and <a href=https://en.wikipedia.org/wiki/Product_information>product information</a> more effectively. Compared to state-of-the-art unified prediction models, the evaluations on three benchmark datasets, <a href=https://en.wikipedia.org/wiki/Internet_Movie_Database>IMDB</a>, Yelp13, and Yelp14, show that our dual learning model gives performance gain of 0.6 %, 1.2 %, and 0.9 %, respectively. The improvements are also deemed very significant measured by <a href=https://en.wikipedia.org/wiki/P-value>p-values</a>.<i>p-values</i>.</div></div><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-2043.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-2043 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-2043 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-2043/>Fake News Detection Through Multi-Perspective Speaker Profiles</a></strong><br><a href=/people/y/yunfei-long/>Yunfei Long</a>
|
<a href=/people/q/qin-lu/>Qin Lu</a>
|
<a href=/people/r/rong-xiang/>Rong Xiang</a>
|
<a href=/people/m/minglei-li/>Minglei Li</a>
|
<a href=/people/c/chu-ren-huang/>Chu-Ren Huang</a><br><a href=/volumes/I17-2/ class=text-muted>Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-2043><div class="card-body p-3 small">Automatic fake news detection is an important, yet very challenging topic. Traditional <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> using lexical features have only very limited success. This paper proposes a novel method to incorporate <a href=https://en.wikipedia.org/wiki/Public_speaking>speaker profiles</a> into an attention based LSTM model for <a href=https://en.wikipedia.org/wiki/Fake_news>fake news detection</a>. Speaker profiles contribute to the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> in two ways. One is to include <a href=https://en.wikipedia.org/wiki/Them>them</a> in the attention model. The other includes <a href=https://en.wikipedia.org/wiki/List_of_Latin_phrases_(M)>them</a> as additional input data. By adding speaker profiles such as <a href=https://en.wikipedia.org/wiki/Political_party>party affiliation</a>, speaker title, location and <a href=https://en.wikipedia.org/wiki/Credit_history>credit history</a>, our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms the <a href=https://en.wikipedia.org/wiki/State-of-the-art>state-of-the-art method</a> by 14.5 % in <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> using a benchmark fake news detection dataset. This proves that speaker profiles provide valuable information to validate the credibility of news articles.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Chu-Ren+Huang" title="Search for 'Chu-Ren Huang' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/r/rong-xiang/ class=align-middle>Rong Xiang</a>
<span class="badge badge-secondary align-middle ml-2">5</span></li><li class=list-group-item><a href=/people/e/emmanuele-chersoni/ class=align-middle>Emmanuele Chersoni</a>
<span class="badge badge-secondary align-middle ml-2">5</span></li><li class=list-group-item><a href=/people/y/yunfei-long/ class=align-middle>Yunfei Long</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/people/q/qin-lu/ class=align-middle>Qin Lu</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/people/e/enrico-santus/ class=align-middle>Enrico Santus</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/a/alessandro-lenci/ class=align-middle>Alessandro Lenci</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/n/nianwen-xue/ class=align-middle>Nianwen Xue</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/w/william-croft/ class=align-middle>William Croft</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/j/jan-hajic/ class=align-middle>Jan Hajic</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/s/stephan-oepen/ class=align-middle>Stephan Oepen</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/m/martha-palmer/ class=align-middle>Martha Palmer</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/m/mingyu-wan/ class=align-middle>Mingyu Wan</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/m/minglei-li/ class=align-middle>Minglei Li</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/andreas-liesenfeld/ class=align-middle>Andreas Liesenfeld</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/g/gabor-parti/ class=align-middle>Gabor Parti</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/johan-bos/ class=align-middle>Johan Bos</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/james-pustejovsky/ class=align-middle>James Pustejovsky</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/kathleen-ahrens/ class=align-middle>Kathleen Ahrens</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/menghan-jiang/ class=align-middle>Menghan Jiang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/q/qi-su/ class=align-middle>Qi Su</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/mingyu-ma/ class=align-middle>Mingyu Ma</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/james-pustejovksy/ class=align-middle>James Pustejovksy</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/b/barry-devereux/ class=align-middle>Barry Devereux</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jinghang-gu/ class=align-middle>Jinghang Gu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/ludovica-pannitto/ class=align-middle>Ludovica Pannitto</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/x/xuefeng-gao/ class=align-middle>Xuefeng Gao</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/anran-li/ class=align-middle>Anran Li</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/liang-chih-yu/ class=align-middle>Liang-Chih Yu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jin-wang/ class=align-middle>Jin Wang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/b/bo-peng/ class=align-middle>Bo Peng</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/lrec/ class=align-middle>LREC</a><span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/venues/ws/ class=align-middle>WS</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/ijcnlp/ class=align-middle>IJCNLP</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/sigdial/ class=align-middle>SIGDIAL</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/cl/ class=align-middle>CL</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-venues aria-expanded=false aria-controls=more-venues>show all...</li><div class="collapse border-top" id=more-venues><li class=list-group-item><a href=/venues/dmr/ class=align-middle>DMR</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/figlang/ class=align-middle>Fig-Lang</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/lincr/ class=align-middle>LiNCr</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/rocling/ class=align-middle>ROCLING</a><span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>