<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Chunyuan Li - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Chunyuan</span> <span class=font-weight-bold>Li</span></h2><hr><div class=row><div class=col-lg-9><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.341.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--341 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.341 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.341/>RADDLE : An Evaluation Benchmark and Analysis Platform for Robust Task-oriented Dialog Systems<span class=acl-fixed-case>RADDLE</span>: An Evaluation Benchmark and Analysis Platform for Robust Task-oriented Dialog Systems</a></strong><br><a href=/people/b/baolin-peng/>Baolin Peng</a>
|
<a href=/people/c/chunyuan-li/>Chunyuan Li</a>
|
<a href=/people/z/zhu-zhang/>Zhu Zhang</a>
|
<a href=/people/c/chenguang-zhu/>Chenguang Zhu</a>
|
<a href=/people/j/jinchao-li/>Jinchao Li</a>
|
<a href=/people/j/jianfeng-gao/>Jianfeng Gao</a><br><a href=/volumes/2021.acl-long/ class=text-muted>Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--341><div class="card-body p-3 small">For task-oriented dialog systems to be maximally useful, it must be able to process conversations in a way that is (1) generalizable with a small number of training examples for new task domains, and (2) robust to user input in various styles, modalities, or domains. In pursuit of these goals, we introduce the RADDLE benchmark, a collection of corpora and tools for evaluating the performance of <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> across a diverse set of domains. By including tasks with limited training data, RADDLE is designed to favor and encourage <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> with a strong <a href=https://en.wikipedia.org/wiki/Generalization>generalization ability</a>. RADDLE also includes a diagnostic checklist that facilitates detailed robustness analysis in aspects such as <a href=https://en.wikipedia.org/wiki/Variation_(linguistics)>language variations</a>, <a href=https://en.wikipedia.org/wiki/Speech_error>speech errors</a>, unseen entities, and out-of-domain utterances. We evaluate recent state-of-the-art systems based on pre-training and fine-tuning, and find that grounded pre-training on heterogeneous dialog corpora performs better than training a separate model per domain. Adversarial training is also proposed to improve <a href=https://en.wikipedia.org/wiki/Robust_statistics>model robustness</a> against <a href=https://en.wikipedia.org/wiki/Noise_(signal_processing)>noisy inputs</a>. Overall, existing <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> are less than satisfactory in robustness evaluation, which suggests opportunities for future improvement.</div></div><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.17.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--17 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.17 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938801 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.17/>Repulsive Attention : Rethinking Multi-head Attention as <a href=https://en.wikipedia.org/wiki/Bayesian_inference>Bayesian Inference</a><span class=acl-fixed-case>B</span>ayesian Inference</a></strong><br><a href=/people/b/bang-an/>Bang An</a>
|
<a href=/people/j/jie-lyu/>Jie Lyu</a>
|
<a href=/people/z/zhenyi-wang/>Zhenyi Wang</a>
|
<a href=/people/c/chunyuan-li/>Chunyuan Li</a>
|
<a href=/people/c/changwei-hu/>Changwei Hu</a>
|
<a href=/people/f/fei-tan/>Fei Tan</a>
|
<a href=/people/r/ruiyi-zhang/>Ruiyi Zhang</a>
|
<a href=/people/y/yifan-hu/>Yifan Hu</a>
|
<a href=/people/c/changyou-chen/>Changyou Chen</a><br><a href=/volumes/2020.emnlp-main/ class=text-muted>Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--17><div class="card-body p-3 small">The neural attention mechanism plays an important role in many <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing applications</a>. In particular, multi-head attention extends single-head attention by allowing a <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> to jointly attend information from different perspectives. However, without explicit constraining, multi-head attention may suffer from attention collapse, an issue that makes different heads extract similar attentive features, thus limiting the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model&#8217;s representation power</a>. In this paper, for the first time, we provide a novel understanding of multi-head attention from a <a href=https://en.wikipedia.org/wiki/Bayesian_inference>Bayesian perspective</a>. Based on the recently developed particle-optimization sampling techniques, we propose a non-parametric approach that explicitly improves the repulsiveness in multi-head attention and consequently strengthens <a href=https://en.wikipedia.org/wiki/Mathematical_model>model&#8217;s expressiveness</a>. Remarkably, our Bayesian interpretation provides theoretical inspirations on the not-well-understood questions : why and how one uses multi-head attention. Extensive experiments on various attention models and applications demonstrate that the proposed repulsive attention can improve the learned feature diversity, leading to more informative representations with consistent performance improvement on multiple tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.378.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--378 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.378 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938906 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.378" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.378/>Optimus : Organizing Sentences via Pre-trained Modeling of a Latent Space</a></strong><br><a href=/people/c/chunyuan-li/>Chunyuan Li</a>
|
<a href=/people/x/xiang-gao/>Xiang Gao</a>
|
<a href=/people/y/yuan-li/>Yuan Li</a>
|
<a href=/people/b/baolin-peng/>Baolin Peng</a>
|
<a href=/people/x/xiujun-li/>Xiujun Li</a>
|
<a href=/people/y/yizhe-zhang/>Yizhe Zhang</a>
|
<a href=/people/j/jianfeng-gao/>Jianfeng Gao</a><br><a href=/volumes/2020.emnlp-main/ class=text-muted>Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--378><div class="card-body p-3 small">When trained effectively, the Variational Autoencoder (VAE) can be both a powerful <a href=https://en.wikipedia.org/wiki/Generative_model>generative model</a> and an effective representation learning framework for <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language</a>. In this paper, we propose the first large-scale language VAE model Optimus (Organizing sentences via Pre-Trained Modeling of a Universal Space). A universal latent embedding space for sentences is first pre-trained on large text corpus, and then fine-tuned for various language generation and understanding tasks. Compared with GPT-2, Optimus enables guided language generation from an abstract level using the <a href=https://en.wikipedia.org/wiki/Latent_vector>latent vectors</a>. Compared with BERT, Optimus can generalize better on low-resource language understanding tasks due to the smooth latent space structure. Extensive experimental results on a wide range of language tasks demonstrate the effectiveness of Optimus. It achieves new state-of-the-art on VAE language modeling benchmarks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.698.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--698 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.698 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938973 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.698" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.698/>POINTER : Constrained Progressive Text Generation via Insertion-based Generative Pre-training<span class=acl-fixed-case>POINTER</span>: Constrained Progressive Text Generation via Insertion-based Generative Pre-training</a></strong><br><a href=/people/y/yizhe-zhang/>Yizhe Zhang</a>
|
<a href=/people/g/guoyin-wang/>Guoyin Wang</a>
|
<a href=/people/c/chunyuan-li/>Chunyuan Li</a>
|
<a href=/people/z/zhe-gan/>Zhe Gan</a>
|
<a href=/people/c/chris-brockett/>Chris Brockett</a>
|
<a href=/people/w/william-b-dolan/>Bill Dolan</a><br><a href=/volumes/2020.emnlp-main/ class=text-muted>Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--698><div class="card-body p-3 small">Large-scale pre-trained language models, such as BERT and GPT-2, have achieved excellent performance in language representation learning and free-form text generation. However, these <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> can not be directly employed to generate text under specified <a href=https://en.wikipedia.org/wiki/Lexical_analysis>lexical constraints</a>. To address this challenge, we present POINTER (PrOgressive INsertion-based TransformER), a simple yet novel insertion-based approach for hard-constrained text generation. The proposed <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>method</a> operates by progressively inserting new tokens between existing tokens in a parallel manner. This procedure is recursively applied until a sequence is completed. The resulting coarse-to-fine hierarchy makes the generation process intuitive and interpretable. We pre-train our model with the proposed progressive insertion-based objective on a 12 GB Wikipedia dataset, and fine-tune it on downstream hard-constrained generation tasks. Non-autoregressive decoding yields a <a href=https://en.wikipedia.org/wiki/Time_complexity>logarithmic time complexity</a> during <a href=https://en.wikipedia.org/wiki/Time_complexity>inference time</a>. Experimental results on both News and Yelp datasets demonstrate that Pointer achieves state-of-the-art performance on constrained text generation. We released the <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>pre-trained models</a> and the source code to facilitate future research.</div></div><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1159.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1159 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1159 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1159.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1159" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1159/>Robust Navigation with Language Pretraining and Stochastic Sampling</a></strong><br><a href=/people/x/xiujun-li/>Xiujun Li</a>
|
<a href=/people/c/chunyuan-li/>Chunyuan Li</a>
|
<a href=/people/q/qiaolin-xia/>Qiaolin Xia</a>
|
<a href=/people/y/yonatan-bisk/>Yonatan Bisk</a>
|
<a href=/people/a/asli-celikyilmaz/>Asli Celikyilmaz</a>
|
<a href=/people/j/jianfeng-gao/>Jianfeng Gao</a>
|
<a href=/people/n/noah-a-smith/>Noah A. Smith</a>
|
<a href=/people/y/yejin-choi/>Yejin Choi</a><br><a href=/volumes/D19-1/ class=text-muted>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1159><div class="card-body p-3 small">Core to the vision-and-language navigation (VLN) challenge is building robust instruction representations and action decoding schemes, which can generalize well to previously unseen instructions and environments. In this paper, we report two simple but highly effective methods to address these challenges and lead to a new state-of-the-art performance. First, we adapt large-scale pretrained language models to learn text representations that generalize better to previously unseen instructions. Second, we propose a stochastic sampling scheme to reduce the considerable gap between the expert actions in training and sampled actions in test, so that the agent can learn to correct its own mistakes during long sequential action decoding. Combining the two techniques, we achieve a new state of the art on the Room-to-Room benchmark with 6 % absolute gain over the previous best result (47 %-53 %) on the Success Rate weighted by Path Length metric.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1021.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1021 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1021 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N19-1021.Supplementary.pdf data-toggle=tooltip data-placement=top title=Supplementary><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1021" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N19-1021/>Cyclical Annealing Schedule : A Simple Approach to Mitigating KL Vanishing<span class=acl-fixed-case>KL</span> Vanishing</a></strong><br><a href=/people/h/hao-fu/>Hao Fu</a>
|
<a href=/people/c/chunyuan-li/>Chunyuan Li</a>
|
<a href=/people/x/xiaodong-liu/>Xiaodong Liu</a>
|
<a href=/people/j/jianfeng-gao/>Jianfeng Gao</a>
|
<a href=/people/a/asli-celikyilmaz/>Asli Celikyilmaz</a>
|
<a href=/people/l/lawrence-carin/>Lawrence Carin</a><br><a href=/volumes/N19-1/ class=text-muted>Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1021><div class="card-body p-3 small">Variational autoencoders (VAE) with an auto-regressive decoder have been applied for many natural language processing (NLP) tasks. VAE objective consists of two terms, the KL regularization term and the reconstruction term, balanced by a weighting hyper-parameter. One notorious training difficulty is that the KL term tends to vanish. In this paper we study different scheduling schemes for, and show that KL vanishing is caused by the lack of good latent codes in training decoder at the beginning of <a href=https://en.wikipedia.org/wiki/Mathematical_optimization>optimization</a>. To remedy the issue, we propose a cyclical annealing schedule, which simply repeats the process of increasing multiple times. This new procedure allows us to learn more meaningful latent codes progressively by leveraging the results of previous learning cycles as warm re-restart. The effectiveness of cyclical annealing schedule is validated on a broad range of NLP tasks, including <a href=https://en.wikipedia.org/wiki/Language_model>language modeling</a>, dialog response generation and <a href=https://en.wikipedia.org/wiki/Semi-supervised_learning>semi-supervised text classification</a>.<tex-math>\\beta</tex-math>. One notorious training difficulty is that the KL term tends to vanish. In this paper we study different scheduling schemes for <tex-math>\\beta</tex-math>, and show that KL vanishing is caused by the lack of good latent codes in training decoder at the beginning of optimization. To remedy the issue, we propose a cyclical annealing schedule, which simply repeats the process of increasing <tex-math>\\beta</tex-math> multiple times. This new procedure allows us to learn more meaningful latent codes progressively by leveraging the results of previous learning cycles as warm re-restart. The effectiveness of cyclical annealing schedule is validated on a broad range of NLP tasks, including language modeling, dialog response generation and semi-supervised text classification.</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1041.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1041 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1041 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1041.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1041.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-1041" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P18-1041/>Baseline Needs More Love : On Simple Word-Embedding-Based Models and Associated Pooling Mechanisms</a></strong><br><a href=/people/d/dinghan-shen/>Dinghan Shen</a>
|
<a href=/people/g/guoyin-wang/>Guoyin Wang</a>
|
<a href=/people/w/wenlin-wang/>Wenlin Wang</a>
|
<a href=/people/m/martin-renqiang-min/>Martin Renqiang Min</a>
|
<a href=/people/q/qinliang-su/>Qinliang Su</a>
|
<a href=/people/y/yizhe-zhang/>Yizhe Zhang</a>
|
<a href=/people/c/chunyuan-li/>Chunyuan Li</a>
|
<a href=/people/r/ricardo-henao/>Ricardo Henao</a>
|
<a href=/people/l/lawrence-carin/>Lawrence Carin</a><br><a href=/volumes/P18-1/ class=text-muted>Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1041><div class="card-body p-3 small">Many deep learning architectures have been proposed to model the compositionality in text sequences, requiring substantial number of <a href=https://en.wikipedia.org/wiki/Parameter_(computer_programming)>parameters</a> and expensive computations. However, there has not been a rigorous evaluation regarding the added value of sophisticated <a href=https://en.wikipedia.org/wiki/Function_(mathematics)>compositional functions</a>. In this paper, we conduct a point-by-point comparative study between Simple Word-Embedding-based Models (SWEMs), consisting of parameter-free pooling operations, relative to word-embedding-based RNN / CNN models. Surprisingly, SWEMs exhibit comparable or even superior performance in the majority of cases considered. Based upon this understanding, we propose two additional pooling strategies over learned word embeddings : (i) a max-pooling operation for improved interpretability ; and (ii) a hierarchical pooling operation, which preserves spatial (n-gram) information within text sequences. We present experiments on 17 datasets encompassing three <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> : (i) (long) document classification ; (ii) text sequence matching ; and (iii) short text tasks, including classification and tagging.<i>compositionality</i> in text sequences, requiring substantial number of parameters and expensive computations. However, there has not been a rigorous evaluation regarding the added value of sophisticated compositional functions. In this paper, we conduct a point-by-point comparative study between Simple Word-Embedding-based Models (SWEMs), consisting of parameter-free pooling operations, relative to word-embedding-based RNN/CNN models. Surprisingly, SWEMs exhibit comparable or even superior performance in the majority of cases considered. Based upon this understanding, we propose two additional pooling strategies over learned word embeddings: (<i>i</i>) a max-pooling operation for improved interpretability; and (<i>ii</i>) a hierarchical pooling operation, which preserves spatial (<tex-math>n</tex-math>-gram) information within text sequences. We present experiments on 17 datasets encompassing three tasks: (<i>i</i>) (long) document classification; (<i>ii</i>) text sequence matching; and (<i>iii</i>) short text tasks, including classification and tagging.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1216.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1216 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1216 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1216.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-1216" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P18-1216/>Joint Embedding of Words and Labels for Text Classification</a></strong><br><a href=/people/g/guoyin-wang/>Guoyin Wang</a>
|
<a href=/people/c/chunyuan-li/>Chunyuan Li</a>
|
<a href=/people/w/wenlin-wang/>Wenlin Wang</a>
|
<a href=/people/y/yizhe-zhang/>Yizhe Zhang</a>
|
<a href=/people/d/dinghan-shen/>Dinghan Shen</a>
|
<a href=/people/x/xinyuan-zhang/>Xinyuan Zhang</a>
|
<a href=/people/r/ricardo-henao/>Ricardo Henao</a>
|
<a href=/people/l/lawrence-carin/>Lawrence Carin</a><br><a href=/volumes/P18-1/ class=text-muted>Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1216><div class="card-body p-3 small">Word embeddings are effective intermediate representations for capturing semantic regularities between words, when learning the representations of text sequences. We propose to view <a href=https://en.wikipedia.org/wiki/Text_classification>text classification</a> as a label-word joint embedding problem : each label is embedded in the same space with the word vectors. We introduce an attention framework that measures the compatibility of embeddings between <a href=https://en.wikipedia.org/wiki/String_(computer_science)>text sequences</a> and labels. The <a href=https://en.wikipedia.org/wiki/Attention>attention</a> is learned on a training set of labeled samples to ensure that, given a text sequence, the relevant words are weighted higher than the irrelevant ones. Our method maintains the interpretability of word embeddings, and enjoys a built-in ability to leverage alternative sources of information, in addition to input text sequences. Extensive results on the several large text datasets show that the proposed framework outperforms the state-of-the-art methods by a large margin, in terms of both <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> and <a href=https://en.wikipedia.org/wiki/Speed>speed</a>.</div></div><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1254.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1254 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1254 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/238233944 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D17-1254/>Learning Generic Sentence Representations Using <a href=https://en.wikipedia.org/wiki/Convolutional_neural_network>Convolutional Neural Networks</a></a></strong><br><a href=/people/z/zhe-gan/>Zhe Gan</a>
|
<a href=/people/y/yunchen-pu/>Yunchen Pu</a>
|
<a href=/people/r/ricardo-henao/>Ricardo Henao</a>
|
<a href=/people/c/chunyuan-li/>Chunyuan Li</a>
|
<a href=/people/x/xiaodong-he/>Xiaodong He</a>
|
<a href=/people/l/lawrence-carin/>Lawrence Carin</a><br><a href=/volumes/D17-1/ class=text-muted>Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1254><div class="card-body p-3 small">We propose a new encoder-decoder approach to learn distributed sentence representations that are applicable to multiple purposes. The model is learned by using a <a href=https://en.wikipedia.org/wiki/Convolutional_neural_network>convolutional neural network</a> as an <a href=https://en.wikipedia.org/wiki/Encoder>encoder</a> to map an input sentence into a <a href=https://en.wikipedia.org/wiki/Continuous_or_discrete_variable>continuous vector</a>, and using a <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>long short-term memory recurrent neural network</a> as a decoder. Several <a href=https://en.wikipedia.org/wiki/Task_(computing)>tasks</a> are considered, including sentence reconstruction and future sentence prediction. Further, a hierarchical encoder-decoder model is proposed to encode a sentence to predict multiple future sentences. By training our <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> on a large collection of <a href=https://en.wikipedia.org/wiki/Novel>novels</a>, we obtain a highly generic convolutional sentence encoder that performs well in practice. Experimental results on several benchmark datasets, and across a broad range of applications, demonstrate the superiority of the proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> over competing methods.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Chunyuan+Li" title="Search for 'Chunyuan Li' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/j/jianfeng-gao/ class=align-middle>Jianfeng Gao</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/people/y/yizhe-zhang/ class=align-middle>Yizhe Zhang</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/people/l/lawrence-carin/ class=align-middle>Lawrence Carin</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/people/g/guoyin-wang/ class=align-middle>Guoyin Wang</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/r/ricardo-henao/ class=align-middle>Ricardo Henao</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/b/baolin-peng/ class=align-middle>Baolin Peng</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/x/xiujun-li/ class=align-middle>Xiujun Li</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/z/zhe-gan/ class=align-middle>Zhe Gan</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/a/asli-celikyilmaz/ class=align-middle>Asli Celikyilmaz</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/d/dinghan-shen/ class=align-middle>Dinghan Shen</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/w/wenlin-wang/ class=align-middle>Wenlin Wang</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/z/zhu-zhang/ class=align-middle>Zhu Zhang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/chenguang-zhu/ class=align-middle>Chenguang Zhu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jinchao-li/ class=align-middle>Jinchao Li</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/b/bang-an/ class=align-middle>Bang An</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jie-lyu/ class=align-middle>Jie Lyu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/zhenyi-wang/ class=align-middle>Zhenyi Wang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/changwei-hu/ class=align-middle>Changwei Hu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/f/fei-tan/ class=align-middle>Fei Tan</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/ruiyi-zhang/ class=align-middle>Ruiyi Zhang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yifan-hu/ class=align-middle>Yifan Hu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/changyou-chen/ class=align-middle>Changyou Chen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/x/xiang-gao/ class=align-middle>Xiang Gao</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yuan-li/ class=align-middle>Yuan Li</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/chris-brockett/ class=align-middle>Chris Brockett</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/w/william-b-dolan/ class=align-middle>William B. Dolan</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/q/qiaolin-xia/ class=align-middle>Qiaolin Xia</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yonatan-bisk/ class=align-middle>Yonatan Bisk</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/n/noah-a-smith/ class=align-middle>Noah A. Smith</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yejin-choi/ class=align-middle>Yejin Choi</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yunchen-pu/ class=align-middle>Yunchen Pu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/x/xiaodong-he/ class=align-middle>Xiaodong He</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/hao-fu/ class=align-middle>Hao Fu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/x/xiaodong-liu/ class=align-middle>Xiaodong Liu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/martin-renqiang-min/ class=align-middle>Martin Renqiang Min</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/q/qinliang-su/ class=align-middle>Qinliang Su</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/x/xinyuan-zhang/ class=align-middle>Xinyuan Zhang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/emnlp/ class=align-middle>EMNLP</a><span class="badge badge-secondary align-middle ml-2">5</span></li><li class=list-group-item><a href=/venues/acl/ class=align-middle>ACL</a><span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/venues/naacl/ class=align-middle>NAACL</a><span class="badge badge-secondary align-middle ml-2">1</span></li></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>