<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Caiming Xiong - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Caiming</span> <span class=font-weight-bold>Xiong</span></h2><hr><div class=row><div class=col-lg-9><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.85.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--85 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.85 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.85" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.85/>Unsupervised Out-of-Domain Detection via Pre-trained Transformers</a></strong><br><a href=/people/k/keyang-xu/>Keyang Xu</a>
|
<a href=/people/t/tongzheng-ren/>Tongzheng Ren</a>
|
<a href=/people/s/shikun-zhang/>Shikun Zhang</a>
|
<a href=/people/y/yihao-feng/>Yihao Feng</a>
|
<a href=/people/c/caiming-xiong/>Caiming Xiong</a><br><a href=/volumes/2021.acl-long/ class=text-muted>Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--85><div class="card-body p-3 small">Deployed real-world machine learning applications are often subject to uncontrolled and even potentially malicious inputs. Those out-of-domain inputs can lead to unpredictable outputs and sometimes catastrophic safety issues. Prior studies on out-of-domain detection require in-domain task labels and are limited to supervised classification scenarios. Our work tackles the problem of detecting out-of-domain samples with only <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised in-domain data</a>. We utilize the latent representations of pre-trained transformers and propose a simple yet effective method to transform <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> across all layers to construct out-of-domain detectors efficiently. Two domain-specific fine-tuning approaches are further proposed to boost detection accuracy. Our empirical evaluations of related methods on two datasets validate that our method greatly improves out-of-domain detection ability in a more general scenario.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.417.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--417 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.417 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.417/>Unsupervised Paraphrasing with Pretrained Language Models</a></strong><br><a href=/people/t/tong-niu/>Tong Niu</a>
|
<a href=/people/s/semih-yavuz/>Semih Yavuz</a>
|
<a href=/people/y/yingbo-zhou/>Yingbo Zhou</a>
|
<a href=/people/n/nitish-shirish-keskar/>Nitish Shirish Keskar</a>
|
<a href=/people/h/huan-wang/>Huan Wang</a>
|
<a href=/people/c/caiming-xiong/>Caiming Xiong</a><br><a href=/volumes/2021.emnlp-main/ class=text-muted>Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--417><div class="card-body p-3 small">Paraphrase generation has benefited extensively from recent progress in the designing of training objectives and model architectures. However, previous explorations have largely focused on <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised methods</a>, which require a large amount of labeled data that is costly to collect. To address this drawback, we adopt a transfer learning approach and propose a <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training pipeline</a> that enables pre-trained language models to generate high-quality paraphrases in an <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised setting</a>. Our recipe consists of task-adaptation, self-supervision, and a novel decoding algorithm named Dynamic Blocking (DB). To enforce a <a href=https://en.wikipedia.org/wiki/Surface_form>surface form</a> dissimilar from the input, whenever the <a href=https://en.wikipedia.org/wiki/Language_model>language model</a> emits a token contained in the source sequence, DB prevents the model from outputting the subsequent source token for the next generation step. We show with automatic and human evaluations that our approach achieves state-of-the-art performance on both the Quora Question Pair (QQP) and the ParaNMT datasets and is robust to domain shift between the two datasets of distinct distributions. We also demonstrate that our <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> transfers to <a href=https://en.wikipedia.org/wiki/Paraphrasing>paraphrasing</a> in other languages without any additional <a href=https://en.wikipedia.org/wiki/Finetuning>finetuning</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.37.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--37 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.37 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.naacl-main.37" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.37/>DART : Open-Domain Structured Data Record to Text Generation<span class=acl-fixed-case>DART</span>: Open-Domain Structured Data Record to Text Generation</a></strong><br><a href=/people/l/linyong-nan/>Linyong Nan</a>
|
<a href=/people/d/dragomir-radev/>Dragomir Radev</a>
|
<a href=/people/r/rui-zhang/>Rui Zhang</a>
|
<a href=/people/a/amrit-rau/>Amrit Rau</a>
|
<a href=/people/a/abhinand-sivaprasad/>Abhinand Sivaprasad</a>
|
<a href=/people/c/chiachun-hsieh/>Chiachun Hsieh</a>
|
<a href=/people/x/xiangru-tang/>Xiangru Tang</a>
|
<a href=/people/a/aadit-vyas/>Aadit Vyas</a>
|
<a href=/people/n/neha-verma/>Neha Verma</a>
|
<a href=/people/p/pranav-krishna/>Pranav Krishna</a>
|
<a href=/people/y/yangxiaokang-liu/>Yangxiaokang Liu</a>
|
<a href=/people/n/nadia-irwanto/>Nadia Irwanto</a>
|
<a href=/people/j/jessica-pan/>Jessica Pan</a>
|
<a href=/people/f/faiaz-rahman/>Faiaz Rahman</a>
|
<a href=/people/a/ahmad-zaidi/>Ahmad Zaidi</a>
|
<a href=/people/m/mutethia-mutuma/>Mutethia Mutuma</a>
|
<a href=/people/y/yasin-tarabar/>Yasin Tarabar</a>
|
<a href=/people/a/ankit-gupta/>Ankit Gupta</a>
|
<a href=/people/t/tao-yu/>Tao Yu</a>
|
<a href=/people/y/yi-chern-tan/>Yi Chern Tan</a>
|
<a href=/people/x/xi-victoria-lin/>Xi Victoria Lin</a>
|
<a href=/people/c/caiming-xiong/>Caiming Xiong</a>
|
<a href=/people/r/richard-socher/>Richard Socher</a>
|
<a href=/people/n/nazneen-fatema-rajani/>Nazneen Fatema Rajani</a><br><a href=/volumes/2021.naacl-main/ class=text-muted>Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--37><div class="card-body p-3 small">We present DART, an open domain structured DAta Record to Text generation dataset with over 82k instances (DARTs). Data-to-text annotations can be a costly process, especially when dealing with tables which are the major source of structured data and contain nontrivial structures. To this end, we propose a procedure of extracting semantic triples from tables that encodes their structures by exploiting the semantic dependencies among table headers and the table title. Our dataset construction framework effectively merged heterogeneous sources from open domain semantic parsing and spoken dialogue systems by utilizing techniques including tree ontology annotation, question-answer pair to declarative sentence conversion, and predicate unification, all with minimum post-editing. We present systematic evaluation on DART as well as new state-of-the-art results on WebNLG 2017 to show that DART (1) poses new challenges to existing data-to-text datasets and (2) facilitates out-of-domain generalization. Our data and code can be found at https://github.com/Yale-LILY/dart.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.220.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--220 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.220 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.naacl-main.220" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.220/>Learning to Synthesize Data for Semantic Parsing</a></strong><br><a href=/people/b/bailin-wang/>Bailin Wang</a>
|
<a href=/people/w/wenpeng-yin/>Wenpeng Yin</a>
|
<a href=/people/x/xi-victoria-lin/>Xi Victoria Lin</a>
|
<a href=/people/c/caiming-xiong/>Caiming Xiong</a><br><a href=/volumes/2021.naacl-main/ class=text-muted>Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--220><div class="card-body p-3 small">Synthesizing data for <a href=https://en.wikipedia.org/wiki/Semantic_parsing>semantic parsing</a> has gained increasing attention recently. However, most methods require handcrafted (high-precision) rules in their generative process, hindering the exploration of diverse unseen data. In this work, we propose a <a href=https://en.wikipedia.org/wiki/Generative_model>generative model</a> which features a (non-neural) PCFG that models the composition of programs (e.g., SQL), and a BART-based translation model that maps a program to an utterance. Due to the simplicity of PCFG and pre-trained BART, our <a href=https://en.wikipedia.org/wiki/Generative_model>generative model</a> can be efficiently learned from existing data at hand. Moreover, explicitly modeling compositions using PCFG leads to better exploration of unseen programs, thus generate more diverse data. We evaluate our method in both in-domain and out-of-domain settings of text-to-SQL parsing on the standard benchmarks of GeoQuery and Spider, respectively. Our empirical results show that the synthesized data generated from our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> can substantially help a <a href=https://en.wikipedia.org/wiki/Semantic_parser>semantic parser</a> achieve better compositional and domain generalization.</div></div><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.66.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--66 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.66 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938861 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.66" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.66/>TOD-BERT : Pre-trained Natural Language Understanding for Task-Oriented Dialogue<span class=acl-fixed-case>TOD</span>-<span class=acl-fixed-case>BERT</span>: Pre-trained Natural Language Understanding for Task-Oriented Dialogue</a></strong><br><a href=/people/c/chien-sheng-wu/>Chien-Sheng Wu</a>
|
<a href=/people/s/steven-c-h-hoi/>Steven C.H. Hoi</a>
|
<a href=/people/r/richard-socher/>Richard Socher</a>
|
<a href=/people/c/caiming-xiong/>Caiming Xiong</a><br><a href=/volumes/2020.emnlp-main/ class=text-muted>Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--66><div class="card-body p-3 small">The underlying difference of linguistic patterns between general text and task-oriented dialogue makes existing pre-trained language models less useful in practice. In this work, we unify nine human-human and multi-turn task-oriented dialogue datasets for <a href=https://en.wikipedia.org/wiki/Language_model>language modeling</a>. To better model dialogue behavior during pre-training, we incorporate user and system tokens into the masked language modeling. We propose a contrastive objective function to simulate the response selection task. Our pre-trained task-oriented dialogue BERT (TOD-BERT) outperforms strong baselines like BERT on four downstream task-oriented dialogue applications, including intention recognition, dialogue state tracking, dialogue act prediction, and response selection. We also show that TOD-BERT has a stronger few-shot ability that can mitigate the data scarcity problem for task-oriented dialogue.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.412.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--412 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.412 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939326 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.412/>Simple Data Augmentation with the Mask Token Improves Domain Adaptation for Dialog Act Tagging</a></strong><br><a href=/people/s/semih-yavuz/>Semih Yavuz</a>
|
<a href=/people/k/kazuma-hashimoto/>Kazuma Hashimoto</a>
|
<a href=/people/w/wenhao-liu/>Wenhao Liu</a>
|
<a href=/people/n/nitish-shirish-keskar/>Nitish Shirish Keskar</a>
|
<a href=/people/r/richard-socher/>Richard Socher</a>
|
<a href=/people/c/caiming-xiong/>Caiming Xiong</a><br><a href=/volumes/2020.emnlp-main/ class=text-muted>Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--412><div class="card-body p-3 small">The concept of Dialogue Act (DA) is universal across different task-oriented dialogue domains-the act of request carries the same speaker intention whether it is for <a href=https://en.wikipedia.org/wiki/Table_reservation>restaurant reservation</a> or flight booking. However, DA taggers trained on one domain do not generalize well to other domains, which leaves us with the expensive need for a large amount of annotated data in the target domain. In this work, we investigate how to better adapt DA taggers to desired target domains with only unlabeled data. We propose MaskAugment, a controllable mechanism that augments text input by leveraging the pre-trained Mask token from BERT model. Inspired by consistency regularization, we use MaskAugment to introduce an unsupervised teacher-student learning scheme to examine the <a href=https://en.wikipedia.org/wiki/Domain_adaptation>domain adaptation</a> of DA taggers. Our extensive experiments on the Simulated Dialogue (GSim) and Schema-Guided Dialogue (SGD) datasets show that MaskAugment is useful in improving the cross-domain generalization for DA tagging.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.501.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--501 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.501 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938724 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.501/>The Thieves on Sesame Street are Polyglots-Extracting Multilingual Models from Monolingual APIs<span class=acl-fixed-case>API</span>s</a></strong><br><a href=/people/n/nitish-shirish-keskar/>Nitish Shirish Keskar</a>
|
<a href=/people/b/bryan-mccann/>Bryan McCann</a>
|
<a href=/people/c/caiming-xiong/>Caiming Xiong</a>
|
<a href=/people/r/richard-socher/>Richard Socher</a><br><a href=/volumes/2020.emnlp-main/ class=text-muted>Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--501><div class="card-body p-3 small">Pre-training in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a> makes it easier for an adversary with only query access to a victim model to reconstruct a local copy of the victim by training with gibberish input data paired with the victim&#8217;s labels for that data. We discover that this extraction process extends to local copies initialized from a pre-trained, multilingual model while the victim remains monolingual. The extracted <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> learns the task from the monolingual victim, but it generalizes far better than the victim to several other languages. This is done without ever showing the multilingual, extracted model a well-formed input in any of the languages for the target <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. We also demonstrate that a few real examples can greatly improve performance, and we analyze how these results shed light on how such extraction methods succeed.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.acl-main.88.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--acl-main--88 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.acl-main.88 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://slideslive.com/38928988 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.acl-main.88" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.acl-main.88/>Explicit Memory Tracker with Coarse-to-Fine Reasoning for Conversational Machine Reading</a></strong><br><a href=/people/y/yifan-gao/>Yifan Gao</a>
|
<a href=/people/c/chien-sheng-wu/>Chien-Sheng Wu</a>
|
<a href=/people/s/shafiq-joty/>Shafiq Joty</a>
|
<a href=/people/c/caiming-xiong/>Caiming Xiong</a>
|
<a href=/people/r/richard-socher/>Richard Socher</a>
|
<a href=/people/i/irwin-king/>Irwin King</a>
|
<a href=/people/m/michael-lyu/>Michael Lyu</a>
|
<a href=/people/s/steven-c-h-hoi/>Steven C.H. Hoi</a><br><a href=/volumes/2020.acl-main/ class=text-muted>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--acl-main--88><div class="card-body p-3 small">The goal of conversational machine reading is to answer user questions given a knowledge base text which may require asking clarification questions. Existing approaches are limited in their decision making due to struggles in extracting question-related rules and reasoning about them. In this paper, we present a new framework of conversational machine reading that comprises a novel Explicit Memory Tracker (EMT) to track whether conditions listed in the rule text have already been satisfied to make a decision. Moreover, our framework generates clarification questions by adopting a coarse-to-fine reasoning strategy, utilizing sentence-level entailment scores to weight token-level distributions. On the ShARC benchmark (blind, held-out) testset, <a href=https://en.wikipedia.org/wiki/Emergency_medical_technician>EMT</a> achieves new state-of-the-art results of 74.6 % micro-averaged decision accuracy and 49.5 BLEU4. We also show that <a href=https://en.wikipedia.org/wiki/Emergency_medical_technician>EMT</a> is more interpretable by visualizing the entailment-oriented reasoning process as the conversation flows. Code and models are released at https://github.com/Yifan-Gao/explicit_memory_tracker.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.acl-main.484.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--acl-main--484 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.acl-main.484 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://slideslive.com/38929050 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.acl-main.484" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.acl-main.484/>Double-Hard Debias : Tailoring Word Embeddings for Gender Bias Mitigation</a></strong><br><a href=/people/t/tianlu-wang/>Tianlu Wang</a>
|
<a href=/people/x/xi-victoria-lin/>Xi Victoria Lin</a>
|
<a href=/people/n/nazneen-fatema-rajani/>Nazneen Fatema Rajani</a>
|
<a href=/people/b/bryan-mccann/>Bryan McCann</a>
|
<a href=/people/v/vicente-ordonez/>Vicente Ordonez</a>
|
<a href=/people/c/caiming-xiong/>Caiming Xiong</a><br><a href=/volumes/2020.acl-main/ class=text-muted>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--acl-main--484><div class="card-body p-3 small">Word embeddings derived from human-generated corpora inherit strong gender bias which can be further amplified by downstream models. Some commonly adopted debiasing approaches, including the seminal Hard Debias algorithm, apply post-processing procedures that project pre-trained word embeddings into a subspace orthogonal to an inferred gender subspace. We discover that semantic-agnostic corpus regularities such as <a href=https://en.wikipedia.org/wiki/Word_frequency>word frequency</a> captured by the <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> negatively impact the performance of these algorithms. We propose a simple but effective technique, Double Hard Debias, which purifies the word embeddings against such corpus regularities prior to inferring and removing the gender subspace. Experiments on three bias mitigation benchmarks show that our approach preserves the distributional semantics of the pre-trained word embeddings while reducing gender bias to a significantly larger degree than prior approaches.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.acl-demos.24.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--acl-demos--24 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.acl-demos.24 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://slideslive.com/38928624 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.acl-demos.24/>Photon : A Robust Cross-Domain Text-to-SQL System<span class=acl-fixed-case>P</span>hoton: A Robust Cross-Domain Text-to-<span class=acl-fixed-case>SQL</span> System</a></strong><br><a href=/people/j/jichuan-zeng/>Jichuan Zeng</a>
|
<a href=/people/x/xi-victoria-lin/>Xi Victoria Lin</a>
|
<a href=/people/s/steven-c-h-hoi/>Steven C.H. Hoi</a>
|
<a href=/people/r/richard-socher/>Richard Socher</a>
|
<a href=/people/c/caiming-xiong/>Caiming Xiong</a>
|
<a href=/people/m/michael-lyu/>Michael Lyu</a>
|
<a href=/people/i/irwin-king/>Irwin King</a><br><a href=/volumes/2020.acl-demos/ class=text-muted>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--acl-demos--24><div class="card-body p-3 small">Natural language interfaces to databases(NLIDB) democratize end user access to <a href=https://en.wikipedia.org/wiki/Relational_model>relational data</a>. Due to fundamental differences between <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language communication</a> and <a href=https://en.wikipedia.org/wiki/Computer_programming>programming</a>, it is common for end users to issue questions that are ambiguous to the system or fall outside the semantic scope of its underlying <a href=https://en.wikipedia.org/wiki/Query_language>query language</a>. We present PHOTON, a robust, modular, cross-domain NLIDB that can flag natural language input to which a SQL mapping can not be immediately determined. PHOTON consists of a strong neural semantic parser (63.2 % structure accuracy on the Spider dev benchmark), a human-in-the-loop question corrector, a <a href=https://en.wikipedia.org/wiki/Executable>SQL executor</a> and a response generator. The question corrector isa discriminative neural sequence editor which detects confusion span(s) in the input question and suggests rephrasing until a translatable input is given by the user or a maximum number of iterations are conducted. Experiments on simulated data show that the proposed method effectively improves the robustness of text-to-SQL system against untranslatable user input. The live demo of our system is available at http://www.naturalsql.com</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.intexsempar-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.intexsempar-1.0/>Proceedings of the First Workshop on Interactive and Executable Semantic Parsing</a></strong><br><a href=/people/b/ben-bogin/>Ben Bogin</a>
|
<a href=/people/s/srinivasan-iyer/>Srinivasan Iyer</a>
|
<a href=/people/x/xi-victoria-lin/>Victoria Lin</a>
|
<a href=/people/d/dragomir-radev/>Dragomir Radev</a>
|
<a href=/people/a/alane-suhr/>Alane Suhr</a>
|
<a href=/people/p/panupong/>Panupong</a>
|
<a href=/people/c/caiming-xiong/>Caiming Xiong</a>
|
<a href=/people/p/pengcheng-yin/>Pengcheng Yin</a>
|
<a href=/people/t/tao-yu/>Tao Yu</a>
|
<a href=/people/r/rui-zhang/>Rui Zhang</a>
|
<a href=/people/v/victor-zhong/>Victor Zhong</a><br><a href=/volumes/2020.intexsempar-1/ class=text-muted>Proceedings of the First Workshop on Interactive and Executable Semantic Parsing</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.400.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--400 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.400 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.findings-emnlp.400.OptionalSupplementaryMaterial.pdf data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.400/>Improving Limited Labeled Dialogue State Tracking with Self-Supervision</a></strong><br><a href=/people/c/chien-sheng-wu/>Chien-Sheng Wu</a>
|
<a href=/people/s/steven-c-h-hoi/>Steven C.H. Hoi</a>
|
<a href=/people/c/caiming-xiong/>Caiming Xiong</a><br><a href=/volumes/2020.findings-emnlp/ class=text-muted>Findings of the Association for Computational Linguistics: EMNLP 2020</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--400><div class="card-body p-3 small">Existing dialogue state tracking (DST) models require plenty of labeled data. However, collecting high-quality labels is costly, especially when the number of domains increases. In this paper, we address a practical <a href=https://en.wikipedia.org/wiki/Discrete-time_stochastic_process>DST problem</a> that is rarely discussed, i.e., learning efficiently with limited labeled data. We present and investigate two self-supervised objectives : preserving latent consistency and modeling conversational behavior. We encourage a DST model to have consistent latent distributions given a perturbed input, making it more robust to an unseen scenario. We also add an auxiliary utterance generation task, modeling a potential correlation between conversational behavior and dialogue states. The experimental results show that our proposed self-supervised signals can improve <a href=https://en.wikipedia.org/wiki/Common_cause_and_special_cause_(statistics)>joint goal accuracy</a> by 8.95 % when only 1 % labeled data is used on the MultiWOZ dataset. We can achieve an additional 1.76 % improvement if some unlabeled data is jointly trained as <a href=https://en.wikipedia.org/wiki/Semi-supervised_learning>semi-supervised learning</a>. We analyze and visualize how our proposed self-supervised signals help the DST task and hope to stimulate future data-efficient DST research.</div></div><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1051.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1051 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1051 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1051.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D19-1051/>Neural Text Summarization : A Critical Evaluation</a></strong><br><a href=/people/w/wojciech-kryscinski/>Wojciech Kryscinski</a>
|
<a href=/people/n/nitish-shirish-keskar/>Nitish Shirish Keskar</a>
|
<a href=/people/b/bryan-mccann/>Bryan McCann</a>
|
<a href=/people/c/caiming-xiong/>Caiming Xiong</a>
|
<a href=/people/r/richard-socher/>Richard Socher</a><br><a href=/volumes/D19-1/ class=text-muted>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1051><div class="card-body p-3 small">Text summarization aims at compressing long documents into a shorter form that conveys the most important parts of the original document. Despite increased interest in the community and notable research effort, progress on <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmark datasets</a> has stagnated. We critically evaluate key ingredients of the current research setup : <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>, evaluation metrics, and <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a>, and highlight three primary shortcomings : 1) automatically collected datasets leave the task underconstrained and may contain noise detrimental to training and evaluation, 2) current evaluation protocol is weakly correlated with human judgment and does not account for important characteristics such as factual correctness, 3) models overfit to layout biases of current datasets and offer limited diversity in their outputs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1537.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1537 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1537 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1537" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1537/>Editing-Based SQL Query Generation for Cross-Domain Context-Dependent Questions<span class=acl-fixed-case>SQL</span> Query Generation for Cross-Domain Context-Dependent Questions</a></strong><br><a href=/people/r/rui-zhang/>Rui Zhang</a>
|
<a href=/people/t/tao-yu/>Tao Yu</a>
|
<a href=/people/h/heyang-er/>Heyang Er</a>
|
<a href=/people/s/sungrok-shim/>Sungrok Shim</a>
|
<a href=/people/e/eric-xue/>Eric Xue</a>
|
<a href=/people/x/xi-victoria-lin/>Xi Victoria Lin</a>
|
<a href=/people/t/tianze-shi/>Tianze Shi</a>
|
<a href=/people/c/caiming-xiong/>Caiming Xiong</a>
|
<a href=/people/r/richard-socher/>Richard Socher</a>
|
<a href=/people/d/dragomir-radev/>Dragomir Radev</a><br><a href=/volumes/D19-1/ class=text-muted>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1537><div class="card-body p-3 small">We focus on the cross-domain context-dependent text-to-SQL generation task. Based on the observation that adjacent natural language questions are often linguistically dependent and their corresponding SQL queries tend to overlap, we utilize the interaction history by editing the previous predicted query to improve the generation quality. Our editing mechanism views SQL as sequences and reuses generation results at the token level in a simple manner. It is flexible to change individual tokens and robust to <a href=https://en.wikipedia.org/wiki/Propagation_of_uncertainty>error propagation</a>. Furthermore, to deal with complex table structures in different domains, we employ an utterance-table encoder and a table-aware decoder to incorporate the context of the user utterance and the table schema. We evaluate our approach on the SParC dataset and demonstrate the benefit of editing compared with the state-of-the-art baselines which generate SQL from scratch. Our code is available at.<url>https://github.com/ryanzhumich/sparc_atis_pytorch</url>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5212.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5212 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5212 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W19-5212.Supplementary.pdf data-toggle=tooltip data-placement=top title=Supplementary><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/W19-5212/>A High-Quality Multilingual Dataset for Structured Documentation Translation</a></strong><br><a href=/people/k/kazuma-hashimoto/>Kazuma Hashimoto</a>
|
<a href=/people/r/raffaella-buschiazzo/>Raffaella Buschiazzo</a>
|
<a href=/people/j/james-bradbury/>James Bradbury</a>
|
<a href=/people/t/teresa-marshall/>Teresa Marshall</a>
|
<a href=/people/r/richard-socher/>Richard Socher</a>
|
<a href=/people/c/caiming-xiong/>Caiming Xiong</a><br><a href=/volumes/W19-52/ class=text-muted>Proceedings of the Fourth Conference on Machine Translation (Volume 1: Research Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5212><div class="card-body p-3 small">This paper presents a high-quality multilingual dataset for the documentation domain to advance research on localization of structured text. Unlike widely-used datasets for translation of plain text, we collect XML-structured parallel text segments from the online documentation for an enterprise software platform. These <a href=https://en.wikipedia.org/wiki/Web_page>Web pages</a> have been professionally translated from English into 16 languages and maintained by domain experts, and around 100,000 text segments are available for each language pair. We build and evaluate translation models for seven target languages from <a href=https://en.wikipedia.org/wiki/English_language>English</a>, with several different copy mechanisms and an XML-constrained beam search. We also experiment with a non-English pair to show that our dataset has the potential to explicitly enable 17 16 translation settings. Our experiments show that learning to translate with the <a href=https://en.wikipedia.org/wiki/Tag_(metadata)>XML tags</a> improves translation accuracy, and the <a href=https://en.wikipedia.org/wiki/Beam_search>beam search</a> accurately generates <a href=https://en.wikipedia.org/wiki/XML>XML structures</a>. We also discuss trade-offs of using the copy mechanisms by focusing on translation of numerical words and <a href=https://en.wikipedia.org/wiki/Named_entity>named entities</a>. We further provide a detailed human analysis of gaps between the model output and human translations for real-world applications, including suitability for <a href=https://en.wikipedia.org/wiki/Post-editing>post-editing</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1078.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1078 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1078 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><span class="align-middle mr-1" data-toggle=tooltip data-placement=bottom title="Outstanding Paper"><i class="fas fa-award"></i></span><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/384011409 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1078" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1078/>Transferable Multi-Domain State Generator for Task-Oriented Dialogue Systems</a></strong><br><a href=/people/c/chien-sheng-wu/>Chien-Sheng Wu</a>
|
<a href=/people/a/andrea-madotto/>Andrea Madotto</a>
|
<a href=/people/e/ehsan-hosseini-asl/>Ehsan Hosseini-Asl</a>
|
<a href=/people/c/caiming-xiong/>Caiming Xiong</a>
|
<a href=/people/r/richard-socher/>Richard Socher</a>
|
<a href=/people/p/pascale-fung/>Pascale Fung</a><br><a href=/volumes/P19-1/ class=text-muted>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1078><div class="card-body p-3 small">Over-dependence on <a href=https://en.wikipedia.org/wiki/Ontology_(information_science)>domain ontology</a> and lack of sharing knowledge across domains are two practical and yet less studied problems of dialogue state tracking. Existing approaches generally fall short when tracking unknown slot values during <a href=https://en.wikipedia.org/wiki/Inference>inference</a> and often have difficulties in adapting to new domains. In this paper, we propose a Transferable Dialogue State Generator (TRADE) that generates dialogue states from utterances using copy mechanism, facilitating transfer when predicting (domain, slot, value) triplets not encountered during training. Our model is composed of an utterance encoder, a slot gate, and a state generator, which are shared across domains. Empirical results demonstrate that TRADE achieves state-of-the-art 48.62 % joint goal accuracy for the five domains of MultiWOZ, a human-human dialogue dataset. In addition, we show the transferring ability by simulating zero-shot and few-shot dialogue state tracking for unseen domains. TRADE achieves 60.58 % joint goal accuracy in one of the zero-shot domains, and is able to adapt to few-shot cases without forgetting already trained domains.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1487.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1487 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1487 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/385213801 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1487/>Explain Yourself ! Leveraging <a href=https://en.wikipedia.org/wiki/Language_model>Language Models</a> for Commonsense Reasoning</a></strong><br><a href=/people/n/nazneen-fatema-rajani/>Nazneen Fatema Rajani</a>
|
<a href=/people/b/bryan-mccann/>Bryan McCann</a>
|
<a href=/people/c/caiming-xiong/>Caiming Xiong</a>
|
<a href=/people/r/richard-socher/>Richard Socher</a><br><a href=/volumes/P19-1/ class=text-muted>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1487><div class="card-body p-3 small">Deep learning models perform poorly on tasks that require <a href=https://en.wikipedia.org/wiki/Commonsense_reasoning>commonsense reasoning</a>, which often necessitates some form of world-knowledge or reasoning over information not immediately present in the input. We collect human explanations for <a href=https://en.wikipedia.org/wiki/Commonsense_reasoning>commonsense reasoning</a> in the form of <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language sequences</a> and highlighted annotations in a new <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> called Common Sense Explanations (CoS-E). We use CoS-E to train language models to automatically generate explanations that can be used during <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training</a> and <a href=https://en.wikipedia.org/wiki/Statistical_inference>inference</a> in a novel Commonsense Auto-Generated Explanation (CAGE) framework. CAGE improves the state-of-the-art by 10 % on the challenging CommonsenseQA task. We further study <a href=https://en.wikipedia.org/wiki/Commonsense_reasoning>commonsense reasoning</a> in DNNs using both human and auto-generated explanations including transfer to out-of-domain tasks. Empirical results indicate that we can effectively leverage <a href=https://en.wikipedia.org/wiki/Language_model>language models</a> for <a href=https://en.wikipedia.org/wiki/Commonsense_reasoning>commonsense reasoning</a>.</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1207.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1207 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1207 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/305886179 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1207/>Improving Abstraction in Text Summarization</a></strong><br><a href=/people/w/wojciech-kryscinski/>Wojciech Kryściński</a>
|
<a href=/people/r/romain-paulus/>Romain Paulus</a>
|
<a href=/people/c/caiming-xiong/>Caiming Xiong</a>
|
<a href=/people/r/richard-socher/>Richard Socher</a><br><a href=/volumes/D18-1/ class=text-muted>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1207><div class="card-body p-3 small">Abstractive text summarization aims to shorten long text documents into a human readable form that contains the most important facts from the original document. However, the level of actual abstraction as measured by novel phrases that do not appear in the source document remains low in existing approaches. We propose two techniques to improve the level of abstraction of generated summaries. First, we decompose the decoder into a contextual network that retrieves relevant parts of the source document, and a pretrained language model that incorporates prior knowledge about language generation. Second, we propose a novelty metric that is optimized directly through <a href=https://en.wikipedia.org/wiki/Policy_learning>policy learning</a> to encourage the generation of novel phrases. Our model achieves results comparable to state-of-the-art models, as determined by ROUGE scores and human evaluations, while achieving a significantly higher level of abstraction as measured by n-gram overlap with the source document.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1160.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1160 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1160 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1160.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1160.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-1160/>Efficient and Robust <a href=https://en.wikipedia.org/wiki/Question_answering>Question Answering</a> from Minimal Context over Documents</a></strong><br><a href=/people/s/sewon-min/>Sewon Min</a>
|
<a href=/people/v/victor-zhong/>Victor Zhong</a>
|
<a href=/people/r/richard-socher/>Richard Socher</a>
|
<a href=/people/c/caiming-xiong/>Caiming Xiong</a><br><a href=/volumes/P18-1/ class=text-muted>Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1160><div class="card-body p-3 small">Neural models for <a href=https://en.wikipedia.org/wiki/Question_answering>question answering (QA)</a> over <a href=https://en.wikipedia.org/wiki/Document>documents</a> have achieved significant performance improvements. Although effective, these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> do not scale to large corpora due to their complex modeling of interactions between the document and the question. Moreover, recent work has shown that such <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> are sensitive to adversarial inputs. In this paper, we study the minimal context required to answer the question, and find that most questions in existing <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> can be answered with a small set of sentences. Inspired by this observation, we propose a simple sentence selector to select the minimal set of sentences to feed into the QA model. Our overall system achieves significant reductions in training (up to 15 times) and inference times (up to 13 times), with accuracy comparable to or better than the state-of-the-art on SQuAD, NewsQA, TriviaQA and SQuAD-Open. Furthermore, our experimental results and analyses show that our approach is more robust to adversarial inputs.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Caiming+Xiong" title="Search for 'Caiming Xiong' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/r/richard-socher/ class=align-middle>Richard Socher</a>
<span class="badge badge-secondary align-middle ml-2">13</span></li><li class=list-group-item><a href=/people/x/xi-victoria-lin/ class=align-middle>Xi Victoria Lin</a>
<span class="badge badge-secondary align-middle ml-2">6</span></li><li class=list-group-item><a href=/people/c/chien-sheng-wu/ class=align-middle>Chien-Sheng Wu</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/people/s/steven-c-h-hoi/ class=align-middle>Steven C.H. Hoi</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/people/n/nitish-shirish-keskar/ class=align-middle>Nitish Shirish Keskar</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/b/bryan-mccann/ class=align-middle>Bryan McCann</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/people/n/nazneen-fatema-rajani/ class=align-middle>Nazneen Fatema Rajani</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/d/dragomir-radev/ class=align-middle>Dragomir Radev</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/t/tao-yu/ class=align-middle>Tao Yu</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/r/rui-zhang/ class=align-middle>Rui Zhang</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/s/semih-yavuz/ class=align-middle>Semih Yavuz</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/k/kazuma-hashimoto/ class=align-middle>Kazuma Hashimoto</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/i/irwin-king/ class=align-middle>Irwin King</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/m/michael-lyu/ class=align-middle>Michael Lyu</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/v/victor-zhong/ class=align-middle>Victor Zhong</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/w/wojciech-kryscinski/ class=align-middle>Wojciech Kryściński</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/k/keyang-xu/ class=align-middle>Keyang Xu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/tongzheng-ren/ class=align-middle>Tongzheng Ren</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/shikun-zhang/ class=align-middle>Shikun Zhang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yihao-feng/ class=align-middle>Yihao Feng</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/w/wenhao-liu/ class=align-middle>Wenhao Liu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yifan-gao/ class=align-middle>Yifan Gao</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/shafiq-joty/ class=align-middle>Shafiq Joty</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/tianlu-wang/ class=align-middle>Tianlu Wang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/v/vicente-ordonez/ class=align-middle>Vicente Ordonez</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jichuan-zeng/ class=align-middle>Jichuan Zeng</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/b/ben-bogin/ class=align-middle>Ben Bogin</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/srinivasan-iyer/ class=align-middle>Srinivasan Iyer</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/alane-suhr/ class=align-middle>Alane Suhr</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/p/panupong/ class=align-middle>Panupong</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/p/pengcheng-yin/ class=align-middle>Pengcheng Yin</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/romain-paulus/ class=align-middle>Romain Paulus</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/tong-niu/ class=align-middle>Tong Niu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yingbo-zhou/ class=align-middle>Yingbo Zhou</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/huan-wang/ class=align-middle>Huan Wang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/heyang-er/ class=align-middle>Heyang Er</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/sungrok-shim/ class=align-middle>Sungrok Shim</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/e/eric-xue/ class=align-middle>Eric Xue</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/tianze-shi/ class=align-middle>Tianze Shi</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/linyong-nan/ class=align-middle>Linyong Nan</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/amrit-rau/ class=align-middle>Amrit Rau</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/abhinand-sivaprasad/ class=align-middle>Abhinand Sivaprasad</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/chiachun-hsieh/ class=align-middle>Chiachun Hsieh</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/x/xiangru-tang/ class=align-middle>Xiangru Tang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/aadit-vyas/ class=align-middle>Aadit Vyas</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/n/neha-verma/ class=align-middle>Neha Verma</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/p/pranav-krishna/ class=align-middle>Pranav Krishna</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yangxiaokang-liu/ class=align-middle>Yangxiaokang Liu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/n/nadia-irwanto/ class=align-middle>Nadia Irwanto</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jessica-pan/ class=align-middle>Jessica Pan</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/f/faiaz-rahman/ class=align-middle>Faiaz Rahman</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/ahmad-zaidi/ class=align-middle>Ahmad Zaidi</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/mutethia-mutuma/ class=align-middle>Mutethia Mutuma</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yasin-tarabar/ class=align-middle>Yasin Tarabar</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/ankit-gupta/ class=align-middle>Ankit Gupta</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yi-chern-tan/ class=align-middle>Yi Chern Tan</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/b/bailin-wang/ class=align-middle>Bailin Wang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/w/wenpeng-yin/ class=align-middle>Wenpeng Yin</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/raffaella-buschiazzo/ class=align-middle>Raffaella Buschiazzo</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/james-bradbury/ class=align-middle>James Bradbury</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/teresa-marshall/ class=align-middle>Teresa Marshall</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/sewon-min/ class=align-middle>Sewon Min</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/andrea-madotto/ class=align-middle>Andrea Madotto</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/e/ehsan-hosseini-asl/ class=align-middle>Ehsan Hosseini-Asl</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/p/pascale-fung/ class=align-middle>Pascale Fung</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/acl/ class=align-middle>ACL</a><span class="badge badge-secondary align-middle ml-2">7</span></li><li class=list-group-item><a href=/venues/emnlp/ class=align-middle>EMNLP</a><span class="badge badge-secondary align-middle ml-2">7</span></li><li class=list-group-item><a href=/venues/naacl/ class=align-middle>NAACL</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/intexsempar/ class=align-middle>intexsempar</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/findings/ class=align-middle>Findings</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-venues aria-expanded=false aria-controls=more-venues>show all...</li><div class="collapse border-top" id=more-venues><li class=list-group-item><a href=/venues/ws/ class=align-middle>WS</a><span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>