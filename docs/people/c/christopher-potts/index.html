<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Christopher Potts - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Christopher</span> <span class=font-weight-bold>Potts</span></h2><hr><div class=row><div class=col-lg-9><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.324.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--324 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.324 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.324/>Dynabench : Rethinking Benchmarking in NLP<span class=acl-fixed-case>NLP</span></a></strong><br><a href=/people/d/douwe-kiela/>Douwe Kiela</a>
|
<a href=/people/m/max-bartolo/>Max Bartolo</a>
|
<a href=/people/y/yixin-nie/>Yixin Nie</a>
|
<a href=/people/d/divyansh-kaushik/>Divyansh Kaushik</a>
|
<a href=/people/a/atticus-geiger/>Atticus Geiger</a>
|
<a href=/people/z/zhengxuan-wu/>Zhengxuan Wu</a>
|
<a href=/people/b/bertie-vidgen/>Bertie Vidgen</a>
|
<a href=/people/g/grusha-prasad/>Grusha Prasad</a>
|
<a href=/people/a/amanpreet-singh/>Amanpreet Singh</a>
|
<a href=/people/p/pratik-ringshia/>Pratik Ringshia</a>
|
<a href=/people/z/zhiyi-ma/>Zhiyi Ma</a>
|
<a href=/people/t/tristan-thrush/>Tristan Thrush</a>
|
<a href=/people/s/sebastian-riedel/>Sebastian Riedel</a>
|
<a href=/people/z/zeerak-waseem/>Zeerak Waseem</a>
|
<a href=/people/p/pontus-stenetorp/>Pontus Stenetorp</a>
|
<a href=/people/r/robin-jia/>Robin Jia</a>
|
<a href=/people/m/mohit-bansal/>Mohit Bansal</a>
|
<a href=/people/c/christopher-potts/>Christopher Potts</a>
|
<a href=/people/a/adina-williams/>Adina Williams</a><br><a href=/volumes/2021.naacl-main/ class=text-muted>Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--324><div class="card-body p-3 small">We introduce Dynabench, an open-source platform for dynamic dataset creation and model benchmarking. Dynabench runs in a web browser and supports human-and-model-in-the-loop dataset creation : annotators seek to create examples that a target model will misclassify, but that another person will not. In this paper, we argue that Dynabench addresses a critical need in our community : contemporary models quickly achieve outstanding performance on benchmark tasks but nonetheless fail on simple challenge examples and falter in real-world scenarios. With Dynabench, dataset creation, model development, and model assessment can directly inform each other, leading to more robust and informative benchmarks. We report on four initial NLP tasks, illustrating these concepts and highlighting the promise of the <a href=https://en.wikipedia.org/wiki/Computing_platform>platform</a>, and address potential objections to dynamic benchmarking as a new standard for the field.</div></div><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.173.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--173 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.173 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.173" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.173/>Pragmatic Issue-Sensitive Image Captioning</a></strong><br><a href=/people/a/allen-nie/>Allen Nie</a>
|
<a href=/people/r/reuben-cohn-gordon/>Reuben Cohn-Gordon</a>
|
<a href=/people/c/christopher-potts/>Christopher Potts</a><br><a href=/volumes/2020.findings-emnlp/ class=text-muted>Findings of the Association for Computational Linguistics: EMNLP 2020</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--173><div class="card-body p-3 small">Image captioning systems need to produce texts that are not only true but also relevant in that they are properly aligned with the current issues. For instance, in a <a href=https://en.wikipedia.org/wiki/Article_(publishing)>newspaper article</a> about a <a href=https://en.wikipedia.org/wiki/Sport>sports event</a>, a caption that not only identifies the player in a picture but also comments on their ethnicity could create unwanted reader reactions. To address this, we propose Issue-Sensitive Image Captioning (ISIC). In ISIC, the captioner is given a target image and an issue, which is a set of images partitioned in a way that specifies what information is relevant. For the sports article, we could construct a <a href=https://en.wikipedia.org/wiki/Partition_of_a_set>partition</a> that places images into <a href=https://en.wikipedia.org/wiki/Equivalence_class>equivalence classes</a> based on player position. To model this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>, we use an extension of the Rational Speech Acts model. Our extension is built on top of state-of-the-art pretrained neural image captioners and explicitly uses image partitions to control caption generation. In both automatic and human evaluations, we show that these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> generate captions that are descriptive and issue-sensitive. Finally, we show how ISIC can complement and enrich the related task of Visual Question Answering.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.blackboxnlp-1.16.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--blackboxnlp-1--16 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.blackboxnlp-1.16 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.blackboxnlp-1.16.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.blackboxnlp-1.16" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.blackboxnlp-1.16/>Neural Natural Language Inference Models Partially Embed Theories of Lexical Entailment and Negation</a></strong><br><a href=/people/a/atticus-geiger/>Atticus Geiger</a>
|
<a href=/people/k/kyle-richardson/>Kyle Richardson</a>
|
<a href=/people/c/christopher-potts/>Christopher Potts</a><br><a href=/volumes/2020.blackboxnlp-1/ class=text-muted>Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--blackboxnlp-1--16><div class="card-body p-3 small">We address whether neural models for Natural Language Inference (NLI) can learn the compositional interactions between lexical entailment and negation, using four methods : the behavioral evaluation methods of (1) challenge test sets and (2) systematic generalization tasks, and the structural evaluation methods of (3) probes and (4) interventions. To facilitate this holistic evaluation, we present Monotonicity NLI (MoNLI), a new naturalistic dataset focused on <a href=https://en.wikipedia.org/wiki/Logical_consequence>lexical entailment</a> and <a href=https://en.wikipedia.org/wiki/Negation>negation</a>. In our behavioral evaluations, we find that models trained on general-purpose NLI datasets fail systematically on MoNLI examples containing <a href=https://en.wikipedia.org/wiki/Negation>negation</a>, but that MoNLI fine-tuning addresses this failure. In our structural evaluations, we look for evidence that our top-performing BERT-based model has learned to implement the monotonicity algorithm behind MoNLI. Probes yield evidence consistent with this conclusion, and our intervention experiments bolster this, showing that the <a href=https://en.wikipedia.org/wiki/Causal_model>causal dynamics</a> of the model mirror the <a href=https://en.wikipedia.org/wiki/Causal_model>causal dynamics</a> of this <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> on subsets of MoNLI. This suggests that the BERT model at least partially embeds a theory of lexical entailment and negation at an algorithmic level.</div></div><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1365.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1365 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1365 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/356167288 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1365/>Recursive Routing Networks : Learning to Compose Modules for Language Understanding</a></strong><br><a href=/people/i/ignacio-cases/>Ignacio Cases</a>
|
<a href=/people/c/clemens-rosenbaum/>Clemens Rosenbaum</a>
|
<a href=/people/m/matthew-riemer/>Matthew Riemer</a>
|
<a href=/people/a/atticus-geiger/>Atticus Geiger</a>
|
<a href=/people/t/tim-klinger/>Tim Klinger</a>
|
<a href=/people/a/alex-tamkin/>Alex Tamkin</a>
|
<a href=/people/o/olivia-li/>Olivia Li</a>
|
<a href=/people/s/sandhini-agarwal/>Sandhini Agarwal</a>
|
<a href=/people/j/joshua-d-greene/>Joshua D. Greene</a>
|
<a href=/people/d/dan-jurafsky/>Dan Jurafsky</a>
|
<a href=/people/c/christopher-potts/>Christopher Potts</a>
|
<a href=/people/l/lauri-karttunen/>Lauri Karttunen</a><br><a href=/volumes/N19-1/ class=text-muted>Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1365><div class="card-body p-3 small">We introduce Recursive Routing Networks (RRNs), which are modular, adaptable models that learn effectively in diverse environments. RRNs consist of a set of <a href=https://en.wikipedia.org/wiki/Subroutine>functions</a>, typically organized into a <a href=https://en.wikipedia.org/wiki/Grid_(spatial_index)>grid</a>, and a meta-learner decision-making component called the <a href=https://en.wikipedia.org/wiki/Router_(computing)>router</a>. The <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> jointly optimizes the parameters of the <a href=https://en.wikipedia.org/wiki/Function_(mathematics)>functions</a> and the meta-learner&#8217;s policy for routing inputs through those <a href=https://en.wikipedia.org/wiki/Function_(mathematics)>functions</a>. RRNs can be incorporated into existing architectures in a number of ways ; we explore adding them to word representation layers, recurrent network hidden layers, and classifier layers. Our evaluation task is natural language inference (NLI). Using the MultiNLI corpus, we show that an RRN&#8217;s routing decisions reflect the high-level genre structure of that <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a>. To show that RRNs can learn to specialize to more fine-grained semantic distinctions, we introduce a new corpus of NLI examples involving implicative predicates, and show that the model components become fine-tuned to the inferential signatures that are characteristic of these <a href=https://en.wikipedia.org/wiki/Predicate_(grammar)>predicates</a>.</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1205.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1205 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1205 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=C18-1205" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/C18-1205/>Retrofitting <a href=https://en.wikipedia.org/wiki/Distribution_(computing)>Distributional Embeddings</a> to <a href=https://en.wikipedia.org/wiki/Knowledge_graph>Knowledge Graphs</a> with Functional Relations</a></strong><br><a href=/people/b/ben-lengerich/>Ben Lengerich</a>
|
<a href=/people/a/andrew-maas/>Andrew Maas</a>
|
<a href=/people/c/christopher-potts/>Christopher Potts</a><br><a href=/volumes/C18-1/ class=text-muted>Proceedings of the 27th International Conference on Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1205><div class="card-body p-3 small">Knowledge graphs are a versatile framework to encode richly structured data relationships, but it can be challenging to combine these <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graphs</a> with <a href=https://en.wikipedia.org/wiki/Unstructured_data>unstructured data</a>. Methods for retrofitting pre-trained entity representations to the structure of a <a href=https://en.wikipedia.org/wiki/Knowledge_graph>knowledge graph</a> typically assume that entities are embedded in a <a href=https://en.wikipedia.org/wiki/Connectivity_(graph_theory)>connected space</a> and that relations imply similarity. However, useful <a href=https://en.wikipedia.org/wiki/Knowledge_graph>knowledge graphs</a> often contain diverse entities and relations (with potentially disjoint underlying corpora) which do not accord with these assumptions. To overcome these limitations, we present Functional Retrofitting, a framework that generalizes current retrofitting methods by explicitly modeling pairwise relations. Our <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> can directly incorporate a variety of pairwise penalty functions previously developed for knowledge graph completion. Further, <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> allows users to encode, learn, and extract information about relation semantics. We present both linear and neural instantiations of the <a href=https://en.wikipedia.org/wiki/Conceptual_framework>framework</a>. Functional Retrofitting significantly outperforms existing retrofitting methods on complex knowledge graphs and loses no accuracy on simpler graphs (in which relations do imply similarity). Finally, we demonstrate the utility of the framework by predicting new drugdisease treatment pairs in a large, complex health knowledge graph.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1140.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1140 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1140 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D18-1140.Attachment.tgz data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1140" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D18-1140/>Representing Social Media Users for Sarcasm Detection</a></strong><br><a href=/people/y/y-alex-kolchinski/>Y. Alex Kolchinski</a>
|
<a href=/people/c/christopher-potts/>Christopher Potts</a><br><a href=/volumes/D18-1/ class=text-muted>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1140><div class="card-body p-3 small">We explore two methods for representing <a href=https://en.wikipedia.org/wiki/Author>authors</a> in the context of textual sarcasm detection : a Bayesian approach that directly represents authors&#8217; propensities to be sarcastic, and a dense embedding approach that can learn interactions between the author and the text. Using the SARC dataset of Reddit comments, we show that augmenting a bidirectional RNN with these representations improves performance ; the Bayesian approach suffices in homogeneous contexts, whereas the added power of the dense embeddings proves valuable in more diverse ones.</div></div><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/Q17-1023.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-Q17-1023 data-toggle=collapse aria-expanded=false aria-controls=abstract-Q17-1023 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/238230459 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/Q17-1023/>Colors in Context : A Pragmatic Neural Model for Grounded Language Understanding</a></strong><br><a href=/people/w/will-monroe/>Will Monroe</a>
|
<a href=/people/r/robert-x-d-hawkins/>Robert X.D. Hawkins</a>
|
<a href=/people/n/noah-goodman/>Noah D. Goodman</a>
|
<a href=/people/c/christopher-potts/>Christopher Potts</a><br><a href=/volumes/Q17-1/ class=text-muted>Transactions of the Association for Computational Linguistics, Volume 5</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-Q17-1023><div class="card-body p-3 small">We present a model of pragmatic referring expression interpretation in a grounded communication task (identifying colors from descriptions) that draws upon predictions from two recurrent neural network classifiers, a speaker and a listener, unified by a recursive pragmatic reasoning framework. Experiments show that this combined pragmatic model interprets color descriptions more accurately than the <a href=https://en.wikipedia.org/wiki/Classifier_(linguistics)>classifiers</a> from which it is built, and that much of this improvement results from combining the speaker and listener perspectives. We observe that pragmatic reasoning helps primarily in the hardest cases : when the model must distinguish very similar colors, or when few utterances adequately express the target color. Our findings make use of a newly-collected corpus of human utterances in color reference games, which exhibit a variety of pragmatic behaviors. We also show that the embedded speaker model reproduces many of these pragmatic behaviors.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Christopher+Potts" title="Search for 'Christopher Potts' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/a/atticus-geiger/ class=align-middle>Atticus Geiger</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/b/ben-lengerich/ class=align-middle>Ben Lengerich</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/andrew-maas/ class=align-middle>Andrew Maas</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/w/will-monroe/ class=align-middle>Will Monroe</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/robert-x-d-hawkins/ class=align-middle>Robert X.D. Hawkins</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/n/noah-goodman/ class=align-middle>Noah Goodman</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/y-alex-kolchinski/ class=align-middle>Y. Alex Kolchinski</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/douwe-kiela/ class=align-middle>Douwe Kiela</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/max-bartolo/ class=align-middle>Max Bartolo</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yixin-nie/ class=align-middle>Yixin Nie</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/divyansh-kaushik/ class=align-middle>Divyansh Kaushik</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/zhengxuan-wu/ class=align-middle>Zhengxuan Wu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/b/bertie-vidgen/ class=align-middle>Bertie Vidgen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/g/grusha-prasad/ class=align-middle>Grusha Prasad</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/amanpreet-singh/ class=align-middle>Amanpreet Singh</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/p/pratik-ringshia/ class=align-middle>Pratik Ringshia</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/zhiyi-ma/ class=align-middle>Zhiyi Ma</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/tristan-thrush/ class=align-middle>Tristan Thrush</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/sebastian-riedel/ class=align-middle>Sebastian Riedel</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/zeerak-waseem/ class=align-middle>Zeerak Waseem</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/p/pontus-stenetorp/ class=align-middle>Pontus Stenetorp</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/robin-jia/ class=align-middle>Robin Jia</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/mohit-bansal/ class=align-middle>Mohit Bansal</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/adina-williams/ class=align-middle>Adina Williams</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/allen-nie/ class=align-middle>Allen Nie</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/reuben-cohn-gordon/ class=align-middle>Reuben Cohn-Gordon</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/i/ignacio-cases/ class=align-middle>Ignacio Cases</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/clemens-rosenbaum/ class=align-middle>Clemens Rosenbaum</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/matthew-riemer/ class=align-middle>Matthew Riemer</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/tim-klinger/ class=align-middle>Tim Klinger</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/alex-tamkin/ class=align-middle>Alex Tamkin</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/o/olivia-li/ class=align-middle>Olivia Li</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/sandhini-agarwal/ class=align-middle>Sandhini Agarwal</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/joshua-d-greene/ class=align-middle>Joshua D. Greene</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/dan-jurafsky/ class=align-middle>Dan Jurafsky</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/lauri-karttunen/ class=align-middle>Lauri Karttunen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/kyle-richardson/ class=align-middle>Kyle Richardson</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/naacl/ class=align-middle>NAACL</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/coling/ class=align-middle>COLING</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/tacl/ class=align-middle>TACL</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/emnlp/ class=align-middle>EMNLP</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/findings/ class=align-middle>Findings</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-venues aria-expanded=false aria-controls=more-venues>show all...</li><div class="collapse border-top" id=more-venues><li class=list-group-item><a href=/venues/blackboxnlp/ class=align-middle>BlackboxNLP</a><span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright Â©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>