<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Ines Rehbein - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Ines</span> <span class=font-weight-bold>Rehbein</span></h2><hr><div class=row><div class=col-lg-9><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.615.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--615 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.615 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.615/>Come hither or go away? Recognising pre-electoral coalition signals in the news</a></strong><br><a href=/people/i/ines-rehbein/>Ines Rehbein</a>
|
<a href=/people/s/simone-paolo-ponzetto/>Simone Paolo Ponzetto</a>
|
<a href=/people/a/anna-adendorf/>Anna Adendorf</a>
|
<a href=/people/o/oke-bahnsen/>Oke Bahnsen</a>
|
<a href=/people/l/lukas-stoetzer/>Lukas Stoetzer</a>
|
<a href=/people/h/heiner-stuckenschmidt/>Heiner Stuckenschmidt</a><br><a href=/volumes/2021.emnlp-main/ class=text-muted>Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--615><div class="card-body p-3 small">In this paper, we introduce the task of political coalition signal prediction from text, that is, the task of recognizing from the <a href=https://en.wikipedia.org/wiki/News_media>news coverage</a> leading up to an election the (un)willingness of political parties to form a <a href=https://en.wikipedia.org/wiki/Coalition_government>government coalition</a>. We decompose our problem into two related, but distinct tasks : (i) predicting whether a reported statement from a politician or a journalist refers to a potential <a href=https://en.wikipedia.org/wiki/Coalition>coalition</a> and (ii) predicting the polarity of the signal namely, whether the speaker is in favour of or against the coalition. For this, we explore the benefits of <a href=https://en.wikipedia.org/wiki/Multi-task_learning>multi-task learning</a> and investigate which setup and task formulation is best suited for each <a href=https://en.wikipedia.org/wiki/Task_(project_management)>sub-task</a>. We evaluate our approach, based on hand-coded newspaper articles, covering elections in three countries (Ireland, <a href=https://en.wikipedia.org/wiki/Germany>Germany</a>, Austria) and two languages (English, German). Our results show that the multi-task learning approach can further improve results over a strong monolingual transfer learning baseline.</div></div><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.acl-main.379.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--acl-main--379 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.acl-main.379 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://slideslive.com/38929031 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.acl-main.379/>Neural Reranking for Dependency Parsing : An Evaluation</a></strong><br><a href=/people/b/bich-ngoc-do/>Bich-Ngoc Do</a>
|
<a href=/people/i/ines-rehbein/>Ines Rehbein</a><br><a href=/volumes/2020.acl-main/ class=text-muted>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--acl-main--379><div class="card-body p-3 small">Recent work has shown that neural rerankers can improve results for dependency parsing over the top k trees produced by a base <a href=https://en.wikipedia.org/wiki/Parsing>parser</a>. However, all neural rerankers so far have been evaluated on <a href=https://en.wikipedia.org/wiki/English_language>English</a> and <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a> only, both languages with a configurational word order and poor morphology. In the paper, we re-assess the potential of successful neural reranking models from the literature on <a href=https://en.wikipedia.org/wiki/English_language>English</a> and on two morphologically rich(er) languages, <a href=https://en.wikipedia.org/wiki/German_language>German</a> and <a href=https://en.wikipedia.org/wiki/Czech_language>Czech</a>. In addition, we introduce a new variation of a discriminative reranker based on graph convolutional networks (GCNs). We show that the GCN not only outperforms previous models on <a href=https://en.wikipedia.org/wiki/English_language>English</a> but is the only <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> that is able to improve results over the baselines on <a href=https://en.wikipedia.org/wiki/German_language>German</a> and <a href=https://en.wikipedia.org/wiki/Czech_language>Czech</a>. We explain the differences in reranking performance based on an analysis of a) the gold tree ratio and b) the variety in the k-best lists.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.878.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--878 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.878 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.878/>Improving Sentence Boundary Detection for Spoken Language Transcripts</a></strong><br><a href=/people/i/ines-rehbein/>Ines Rehbein</a>
|
<a href=/people/j/josef-ruppenhofer/>Josef Ruppenhofer</a>
|
<a href=/people/t/thomas-schmidt/>Thomas Schmidt</a><br><a href=/volumes/2020.lrec-1/ class=text-muted>Proceedings of the 12th Language Resources and Evaluation Conference</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--878><div class="card-body p-3 small">This paper presents experiments on sentence boundary detection in transcripts of spoken dialogues. Segmenting <a href=https://en.wikipedia.org/wiki/Spoken_language>spoken language</a> into sentence-like units is a challenging task, due to disfluencies, ungrammatical or fragmented structures and the lack of <a href=https://en.wikipedia.org/wiki/Punctuation>punctuation</a>. In addition, one of the main bottlenecks for many NLP applications for <a href=https://en.wikipedia.org/wiki/Spoken_language>spoken language</a> is the small size of the training data, as the transcription and annotation of spoken language is by far more time-consuming and labour-intensive than processing <a href=https://en.wikipedia.org/wiki/Written_language>written language</a>. We therefore investigate the benefits of data expansion and <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a> and test different ML architectures for this task. Our results show that data expansion is not straightforward and even data from the same domain does not always improve results. They also highlight the importance of <a href=https://en.wikipedia.org/wiki/Model_(person)>modelling</a>, i.e. of finding the best <a href=https://en.wikipedia.org/wiki/Architecture>architecture</a> and <a href=https://en.wikipedia.org/wiki/Data_(computing)>data representation</a> for the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> at hand. For the detection of boundaries in spoken language transcripts, we achieve a substantial improvement when framing the boundary detection problem assentence pair classification task, as compared to a sequence tagging approach.</div></div><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2505.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-2505 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-2505 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-2505/>Automatic Alignment and Annotation Projection for Literary Texts</a></strong><br><a href=/people/u/uli-steinbach/>Uli Steinbach</a>
|
<a href=/people/i/ines-rehbein/>Ines Rehbein</a><br><a href=/volumes/W19-25/ class=text-muted>Proceedings of the 3rd Joint SIGHUM Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-2505><div class="card-body p-3 small">This paper presents a modular NLP pipeline for the creation of a parallel literature corpus, followed by annotation transfer from the source to the target language. The test case we use to evaluate our <a href=https://en.wikipedia.org/wiki/Pipeline_(software)>pipeline</a> is the automatic transfer of quote and speaker mention annotations from <a href=https://en.wikipedia.org/wiki/English_language>English</a> to <a href=https://en.wikipedia.org/wiki/German_language>German</a>. We evaluate the different components of the <a href=https://en.wikipedia.org/wiki/Pipeline_transport>pipeline</a> and discuss challenges specific to <a href=https://en.wikipedia.org/wiki/Literature>literary texts</a>. Our experiments show that after applying a reasonable amount of semi-automatic postprocessing we can obtain high-quality aligned and annotated resources for a new language.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4017.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4017 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4017 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4017/>On the role of <a href=https://en.wikipedia.org/wiki/Discourse_analysis>discourse relations</a> in persuasive texts</a></strong><br><a href=/people/i/ines-rehbein/>Ines Rehbein</a><br><a href=/volumes/W19-40/ class=text-muted>Proceedings of the 13th Linguistic Annotation Workshop</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4017><div class="card-body p-3 small">This paper investigates the use of explicitly signalled discourse relations in persuasive texts. We present a corpus study where we control for speaker and topic and show that the distribution of different discourse connectives varies considerably across different discourse settings. While this variation can be explained by genre differences, we also observe variation regarding the distribution of discourse relations across different settings. This variation, however, can not be easily explained by <a href=https://en.wikipedia.org/wiki/Genre>genre differences</a>. We argue that the differences regarding the use of discourse relations reflects different strategies of persuasion and that these might be due to <a href=https://en.wikipedia.org/wiki/Audience_design>audience design</a>.</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1010.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1010 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1010 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1010/>Sprucing up the trees Error detection in treebanks</a></strong><br><a href=/people/i/ines-rehbein/>Ines Rehbein</a>
|
<a href=/people/j/josef-ruppenhofer/>Josef Ruppenhofer</a><br><a href=/volumes/C18-1/ class=text-muted>Proceedings of the 27th International Conference on Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1010><div class="card-body p-3 small">We present a method for detecting annotation errors in manually and automatically annotated dependency parse trees, based on ensemble parsing in combination with <a href=https://en.wikipedia.org/wiki/Bayesian_inference>Bayesian inference</a>, guided by <a href=https://en.wikipedia.org/wiki/Active_learning>active learning</a>. We evaluate our method in different scenarios : (i) for <a href=https://en.wikipedia.org/wiki/Error_detection_and_correction>error detection</a> in dependency treebanks and (ii) for improving parsing accuracy on in- and out-of-domain data.</div></div><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1107.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1107 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1107 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-1107/>Detecting annotation noise in automatically labelled data</a></strong><br><a href=/people/i/ines-rehbein/>Ines Rehbein</a>
|
<a href=/people/j/josef-ruppenhofer/>Josef Ruppenhofer</a><br><a href=/volumes/P17-1/ class=text-muted>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1107><div class="card-body p-3 small">We introduce a method for <a href=https://en.wikipedia.org/wiki/Error_detection_and_correction>error detection</a> in automatically annotated text, aimed at supporting the creation of high-quality language resources at affordable cost. Our method combines an <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised generative model</a> with <a href=https://en.wikipedia.org/wiki/Supervisor>human supervision</a> from <a href=https://en.wikipedia.org/wiki/Active_learning>active learning</a>. We test our approach on in-domain and out-of-domain data in two languages, in AL simulations and in a real world setting. For all settings, the results show that our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> is able to detect annotation errors with high <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>precision</a> and high <a href=https://en.wikipedia.org/wiki/Recall_(memory)>recall</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-0813.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-0813 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-0813 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-0813/>Catching the Common Cause : Extraction and Annotation of Causal Relations and their Participants</a></strong><br><a href=/people/i/ines-rehbein/>Ines Rehbein</a>
|
<a href=/people/j/josef-ruppenhofer/>Josef Ruppenhofer</a><br><a href=/volumes/W17-08/ class=text-muted>Proceedings of the 11th Linguistic Annotation Workshop</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-0813><div class="card-body p-3 small">In this paper, we present a simple, yet effective method for the automatic identification and extraction of causal relations from <a href=https://en.wikipedia.org/wiki/Text_(literary_theory)>text</a>, based on a large English-German parallel corpus. The goal of this effort is to create a <a href=https://en.wikipedia.org/wiki/Lexical_resource>lexical resource</a> for German causal relations. The resource will consist of a lexicon that describes constructions that trigger causality as well as the participants of the causal event, and will be augmented by a <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> with annotated instances for each entry, that can be used as training data to develop a system for automatic classification of causal relations. Focusing on verbs, our method harvested a set of 100 different lexical triggers of causality, including support verb constructions. At the moment, our <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> includes over 1,000 annotated instances. The <a href=https://en.wikipedia.org/wiki/Lexicon>lexicon</a> and the annotated data will be made available to the research community.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4117.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4117 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4117 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4117/>What do we need to know about an unknown word when parsing German<span class=acl-fixed-case>G</span>erman</a></strong><br><a href=/people/b/bich-ngoc-do/>Bich-Ngoc Do</a>
|
<a href=/people/i/ines-rehbein/>Ines Rehbein</a>
|
<a href=/people/a/anette-frank/>Anette Frank</a><br><a href=/volumes/W17-41/ class=text-muted>Proceedings of the First Workshop on Subword and Character Level Models in NLP</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4117><div class="card-body p-3 small">We propose a new type of subword embedding designed to provide more information about unknown compounds, a major source for OOV words in <a href=https://en.wikipedia.org/wiki/German_language>German</a>. We present an extrinsic evaluation where we use the compound embeddings as input to a neural dependency parser and compare the results to the ones obtained with other types of <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a>. Our evaluation shows that adding compound embeddings yields a significant improvement of 2 % LAS over using word embeddings when no POS information is available. When adding POS embeddings to the input, however, the effect levels out. This suggests that it is not the missing information about the <a href=https://en.wikipedia.org/wiki/Semantics>semantics</a> of the unknown words that causes problems for parsing <a href=https://en.wikipedia.org/wiki/German_language>German</a>, but the lack of morphological information for unknown words. To augment our evaluation, we also test the new embeddings in a language modelling task that requires both syntactic and semantic information.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4907.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4907 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4907 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4907/>Authorship Attribution with <a href=https://en.wikipedia.org/wiki/Convolutional_neural_network>Convolutional Neural Networks</a> and POS-Eliding<span class=acl-fixed-case>POS</span>-Eliding</a></strong><br><a href=/people/j/julian-hitschler/>Julian Hitschler</a>
|
<a href=/people/e/esther-van-den-berg/>Esther van den Berg</a>
|
<a href=/people/i/ines-rehbein/>Ines Rehbein</a><br><a href=/volumes/W17-49/ class=text-muted>Proceedings of the Workshop on Stylistic Variation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4907><div class="card-body p-3 small">We use a <a href=https://en.wikipedia.org/wiki/Convolutional_neural_network>convolutional neural network</a> to perform authorship identification on a very homogeneous dataset of scientific publications. In order to investigate the effect of domain biases, we obscure words below a certain frequency threshold, retaining only their POS-tags. This <a href=https://en.wikipedia.org/wiki/Procedure_(term)>procedure</a> improves test performance due to better <a href=https://en.wikipedia.org/wiki/Generalization>generalization</a> on unseen data. Using our <a href=https://en.wikipedia.org/wiki/Methodology>method</a>, we are able to predict the authors of scientific publications in the same discipline at levels well above chance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-6318.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-6318 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-6318 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-6318/>Evaluating LSTM models for grammatical function labelling<span class=acl-fixed-case>LSTM</span> models for grammatical function labelling</a></strong><br><a href=/people/b/bich-ngoc-do/>Bich-Ngoc Do</a>
|
<a href=/people/i/ines-rehbein/>Ines Rehbein</a><br><a href=/volumes/W17-63/ class=text-muted>Proceedings of the 15th International Conference on Parsing Technologies</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-6318><div class="card-body p-3 small">To improve grammatical function labelling for <a href=https://en.wikipedia.org/wiki/German_language>German</a>, we augment the labelling component of a neural dependency parser with a decision history. We present different ways to encode the history, using different LSTM architectures, and show that our models yield significant improvements, resulting in a LAS for <a href=https://en.wikipedia.org/wiki/German_language>German</a> that is close to the best result from the SPMRL 2014 shared task (without the reranker).</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Ines+Rehbein" title="Search for 'Ines Rehbein' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/j/josef-ruppenhofer/ class=align-middle>Josef Ruppenhofer</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/people/b/bich-ngoc-do/ class=align-middle>Bich-Ngoc Do</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/a/anette-frank/ class=align-middle>Anette Frank</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/julian-hitschler/ class=align-middle>Julian Hitschler</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/e/esther-van-den-berg/ class=align-middle>Esther van den Berg</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/s/simone-paolo-ponzetto/ class=align-middle>Simone Paolo Ponzetto</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/anna-adendorf/ class=align-middle>Anna Adendorf</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/o/oke-bahnsen/ class=align-middle>Oke Bahnsen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/lukas-stoetzer/ class=align-middle>Lukas Stoetzer</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/heiner-stuckenschmidt/ class=align-middle>Heiner Stuckenschmidt</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/u/uli-steinbach/ class=align-middle>Uli Steinbach</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/thomas-schmidt/ class=align-middle>Thomas Schmidt</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/ws/ class=align-middle>WS</a><span class="badge badge-secondary align-middle ml-2">6</span></li><li class=list-group-item><a href=/venues/acl/ class=align-middle>ACL</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/coling/ class=align-middle>COLING</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/emnlp/ class=align-middle>EMNLP</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/lrec/ class=align-middle>LREC</a><span class="badge badge-secondary align-middle ml-2">1</span></li></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>