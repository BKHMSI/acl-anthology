<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Iryna Gurevych - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Iryna</span> <span class=font-weight-bold>Gurevych</span></h2><hr><div class=row><div class=col-lg-9><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.243.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--243 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.243 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.243" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.243/>How Good is Your <a href=https://en.wikipedia.org/wiki/Tokenizer>Tokenizer</a>? On the Monolingual Performance of Multilingual Language Models</a></strong><br><a href=/people/p/phillip-rust/>Phillip Rust</a>
|
<a href=/people/j/jonas-pfeiffer/>Jonas Pfeiffer</a>
|
<a href=/people/i/ivan-vulic/>Ivan VuliÄ‡</a>
|
<a href=/people/s/sebastian-ruder/>Sebastian Ruder</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a><br><a href=/volumes/2021.acl-long/ class=text-muted>Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--243><div class="card-body p-3 small">In this work, we provide a systematic and comprehensive empirical comparison of pretrained multilingual language models versus their monolingual counterparts with regard to their monolingual task performance. We study a set of nine typologically diverse languages with readily available pretrained monolingual models on a set of five diverse monolingual downstream tasks. We first aim to establish, via fair and controlled comparisons, if a gap between the multilingual and the corresponding monolingual representation of that language exists, and subsequently investigate the reason for any performance difference. To disentangle conflating factors, we train new monolingual models on the same data, with monolingually and multilingually trained tokenizers. We find that while the pretraining data size is an important factor, a designated monolingual tokenizer plays an equally important role in the downstream performance. Our results show that languages that are adequately represented in the multilingual model&#8217;s vocabulary exhibit negligible performance decreases over their monolingual counterparts. We further find that replacing the original multilingual tokenizer with the specialized monolingual tokenizer improves the downstream performance of the multilingual model for almost every task and language.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.448.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--448 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.448 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.acl-long.448.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.448" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.448/>Coreference Reasoning in Machine Reading Comprehension</a></strong><br><a href=/people/m/mingzhu-wu/>Mingzhu Wu</a>
|
<a href=/people/n/nafise-sadat-moosavi/>Nafise Sadat Moosavi</a>
|
<a href=/people/d/dan-roth/>Dan Roth</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a><br><a href=/volumes/2021.acl-long/ class=text-muted>Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--448><div class="card-body p-3 small">Coreference resolution is essential for <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language understanding</a> and has been long studied in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a>. In recent years, as the format of Question Answering (QA) became a standard for machine reading comprehension (MRC), there have been data collection efforts, e.g., Dasigi et al. (2019), that attempt to evaluate the ability of MRC models to reason about <a href=https://en.wikipedia.org/wiki/Coreference>coreference</a>. However, as we show, coreference reasoning in <a href=https://en.wikipedia.org/wiki/Medical_classification>MRC</a> is a greater challenge than earlier thought ; <a href=https://en.wikipedia.org/wiki/Medical_classification>MRC datasets</a> do not reflect the natural distribution and, consequently, the challenges of coreference reasoning. Specifically, success on these <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> does not reflect a model&#8217;s proficiency in <a href=https://en.wikipedia.org/wiki/Coreference>coreference reasoning</a>. We propose a <a href=https://en.wikipedia.org/wiki/Methodology>methodology</a> for creating MRC datasets that better reflect the challenges of coreference reasoning and use it to create a sample evaluation set. The results on our <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> show that state-of-the-art models still struggle with these phenomena. Furthermore, we develop an effective way to use naturally occurring coreference phenomena from existing coreference resolution datasets when training MRC models. This allows us to show an improvement in the coreference reasoning abilities of state-of-the-art models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-short.77.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-short--77 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-short.77 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-short.77/>The Curse of Dense Low-Dimensional Information Retrieval for Large Index Sizes</a></strong><br><a href=/people/n/nils-reimers/>Nils Reimers</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a><br><a href=/volumes/2021.acl-short/ class=text-muted>Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-short--77><div class="card-body p-3 small">Information Retrieval using dense low-dimensional representations recently became popular and showed out-performance to traditional sparse-representations like <a href=https://en.wikipedia.org/wiki/BM25>BM25</a>. However, no previous work investigated how dense representations perform with large index sizes. We show theoretically and empirically that the performance for <a href=https://en.wikipedia.org/wiki/Sparse_matrix>dense representations</a> decreases quicker than <a href=https://en.wikipedia.org/wiki/Sparse_matrix>sparse representations</a> for increasing index sizes. In extreme cases, this can even lead to a tipping point where at a certain index size <a href=https://en.wikipedia.org/wiki/Sparse_matrix>sparse representations</a> outperform <a href=https://en.wikipedia.org/wiki/Dense_matrix>dense representations</a>. We show that this behavior is tightly connected to the number of dimensions of the <a href=https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning>representations</a> : The lower the <a href=https://en.wikipedia.org/wiki/Dimension>dimension</a>, the higher the chance for <a href=https://en.wikipedia.org/wiki/False_positives_and_false_negatives>false positives</a>, i.e. returning irrelevant documents</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.sustainlp-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.sustainlp-1.0/>Proceedings of the Second Workshop on Simple and Efficient Natural Language Processing</a></strong><br><a href=/people/n/nafise-sadat-moosavi/>Nafise Sadat Moosavi</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a>
|
<a href=/people/a/angela-fan/>Angela Fan</a>
|
<a href=/people/t/thomas-wolf/>Thomas Wolf</a>
|
<a href=/people/y/yufang-hou/>Yufang Hou</a>
|
<a href=/people/a/ana-marasovic/>Ana MarasoviÄ‡</a>
|
<a href=/people/s/sujith-ravi/>Sujith Ravi</a><br><a href=/volumes/2021.sustainlp-1/ class=text-muted>Proceedings of the Second Workshop on Simple and Efficient Natural Language Processing</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.44.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--44 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.44 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.44/>Improving Factual Consistency Between a Response and Persona Facts</a></strong><br><a href=/people/m/mohsen-mesgar/>Mohsen Mesgar</a>
|
<a href=/people/e/edwin-simpson/>Edwin Simpson</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a><br><a href=/volumes/2021.eacl-main/ class=text-muted>Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--44><div class="card-body p-3 small">Neural models for response generation produce responses that are semantically plausible but not necessarily factually consistent with facts describing the speaker&#8217;s persona. These models are trained with fully <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised learning</a> where the <a href=https://en.wikipedia.org/wiki/Loss_function>objective function</a> barely captures factual consistency. We propose to fine-tune these models by <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning</a> and an efficient <a href=https://en.wikipedia.org/wiki/Reward_system>reward function</a> that explicitly captures the consistency between a response and persona facts as well as semantic plausibility. Our automatic and human evaluations on the PersonaChat corpus confirm that our approach increases the rate of responses that are factually consistent with persona facts over its supervised counterpart while retains the language quality of responses.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.38.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--38 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.38 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.38" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.38/>Event Coreference Data (Almost) for Free : Mining Hyperlinks from Online News<span class=acl-fixed-case>E</span>vent Coreference Data (Almost) for Free: <span class=acl-fixed-case>M</span>ining Hyperlinks from Online News</a></strong><br><a href=/people/m/michael-bugert/>Michael Bugert</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a><br><a href=/volumes/2021.emnlp-main/ class=text-muted>Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--38><div class="card-body p-3 small">Cross-document event coreference resolution (CDCR) is the task of identifying which event mentions refer to the same events throughout a collection of documents. Annotating CDCR data is an arduous and expensive process, explaining why existing corpora are small and lack domain coverage. To overcome this bottleneck, we automatically extract event coreference data from <a href=https://en.wikipedia.org/wiki/Hyperlink>hyperlinks</a> in online news : When referring to a significant real-world event, writers often add a <a href=https://en.wikipedia.org/wiki/Hyperlink>hyperlink</a> to another article covering this event. We demonstrate that collecting <a href=https://en.wikipedia.org/wiki/Hyperlink>hyperlinks</a> which point to the same article(s) produces extensive and high-quality CDCR data and create a <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> of 2 M documents and 2.7 M silver-standard event mentions called HyperCoref. We evaluate a state-of-the-art system on three CDCR corpora and find that <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> trained on small subsets of HyperCoref are highly competitive, with performance similar to <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> trained on gold-standard data. With our work, we free CDCR research from depending on costly human-annotated training data and open up possibilities for research beyond English CDCR, as our data extraction approach can be easily adapted to other languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.713.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--713 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.713 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.713" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.713/>Avoiding Inference Heuristics in Few-shot Prompt-based Finetuning</a></strong><br><a href=/people/p/prasetya-utama/>Prasetya Utama</a>
|
<a href=/people/n/nafise-sadat-moosavi/>Nafise Sadat Moosavi</a>
|
<a href=/people/v/victor-sanh/>Victor Sanh</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a><br><a href=/volumes/2021.emnlp-main/ class=text-muted>Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--713><div class="card-body p-3 small">Recent prompt-based approaches allow pretrained language models to achieve strong performances on few-shot finetuning by reformulating downstream tasks as a language modeling problem. In this work, we demonstrate that, despite its advantages on low data regimes, finetuned prompt-based models for sentence pair classification tasks still suffer from a common pitfall of adopting <a href=https://en.wikipedia.org/wiki/Heuristics_in_judgment_and_decision-making>inference heuristics</a> based on <a href=https://en.wikipedia.org/wiki/Lexical_similarity>lexical overlap</a>, e.g., models incorrectly assuming a sentence pair is of the same meaning because they consist of the same set of words. Interestingly, we find that this particular inference heuristic is significantly less present in the zero-shot evaluation of the prompt-based model, indicating how <a href=https://en.wikipedia.org/wiki/Finetuning>finetuning</a> can be destructive to useful knowledge learned during the pretraining. We then show that adding a <a href=https://en.wikipedia.org/wiki/Regularization_(mathematics)>regularization</a> that preserves pretraining weights is effective in mitigating this destructive tendency of few-shot finetuning. Our evaluation on three datasets demonstrates promising improvements on the three corresponding challenge datasets used to diagnose the inference heuristics.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.800.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--800 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.800 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.800" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.800/>UNKs Everywhere : Adapting Multilingual Language Models to New Scripts<span class=acl-fixed-case>UNK</span>s Everywhere: <span class=acl-fixed-case>A</span>dapting Multilingual Language Models to New Scripts</a></strong><br><a href=/people/j/jonas-pfeiffer/>Jonas Pfeiffer</a>
|
<a href=/people/i/ivan-vulic/>Ivan VuliÄ‡</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a>
|
<a href=/people/s/sebastian-ruder/>Sebastian Ruder</a><br><a href=/volumes/2021.emnlp-main/ class=text-muted>Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--800><div class="card-body p-3 small">Massively multilingual language models such as multilingual BERT offer state-of-the-art cross-lingual transfer performance on a range of NLP tasks. However, due to limited capacity and large differences in pretraining data sizes, there is a profound performance gap between resource-rich and resource-poor target languages. The ultimate challenge is dealing with under-resourced languages not covered at all by the <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> and written in scripts unseen during pretraining. In this work, we propose a series of novel data-efficient methods that enable quick and effective adaptation of pretrained multilingual models to such low-resource languages and unseen scripts. Relying on matrix factorization, our methods capitalize on the existing latent knowledge about multiple languages already available in the pretrained model&#8217;s embedding matrix. Furthermore, we show that learning of the new dedicated embedding matrix in the target language can be improved by leveraging a small number of vocabulary items (i.e., the so-called lexically overlapping tokens) shared between mBERT&#8217;s and target language vocabulary. Our adaptation techniques offer substantial performance gains for languages with unseen scripts. We also demonstrate that they can yield improvements for low-resource languages written in scripts covered by the pretrained model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.internlp-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.internlp-1.0/>Proceedings of the First Workshop on Interactive Learning for Natural Language Processing</a></strong><br><a href=/people/k/kiante-brantley/>KiantÃ© Brantley</a>
|
<a href=/people/s/soham-dan/>Soham Dan</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a>
|
<a href=/people/j/ji-ung-lee/>Ji-Ung Lee</a>
|
<a href=/people/f/filip-radlinski/>Filip Radlinski</a>
|
<a href=/people/h/hinrich-schutze/>Hinrich SchÃ¼tze</a>
|
<a href=/people/e/edwin-simpson/>Edwin Simpson</a>
|
<a href=/people/l/lili-yu/>Lili Yu</a><br><a href=/volumes/2021.internlp-1/ class=text-muted>Proceedings of the First Workshop on Interactive Learning for Natural Language Processing</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.28.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--28 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.28 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.naacl-main.28" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.28/>Augmented SBERT : Data Augmentation Method for Improving Bi-Encoders for Pairwise Sentence Scoring Tasks<span class=acl-fixed-case>SBERT</span>: Data Augmentation Method for Improving Bi-Encoders for Pairwise Sentence Scoring Tasks</a></strong><br><a href=/people/n/nandan-thakur/>Nandan Thakur</a>
|
<a href=/people/n/nils-reimers/>Nils Reimers</a>
|
<a href=/people/j/johannes-daxenberger/>Johannes Daxenberger</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a><br><a href=/volumes/2021.naacl-main/ class=text-muted>Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--28><div class="card-body p-3 small">There are two approaches for pairwise sentence scoring : Cross-encoders, which perform full-attention over the input pair, and Bi-encoders, which map each input independently to a dense vector space. While cross-encoders often achieve higher performance, they are too slow for many practical use cases. Bi-encoders, on the other hand, require substantial training data and <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a> over the target task to achieve competitive performance. We present a simple yet efficient data augmentation strategy called Augmented SBERT, where we use the cross-encoder to label a larger set of input pairs to augment the training data for the bi-encoder. We show that, in this process, selecting the sentence pairs is non-trivial and crucial for the success of the method. We evaluate our approach on multiple tasks (in-domain) as well as on a domain adaptation task. Augmented SBERT achieves an improvement of up to 6 points for in-domain and of up to 37 points for domain adaptation tasks compared to the original bi-encoder performance.</div></div><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.starsem-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.starsem-1.0/>Proceedings of the Ninth Joint Conference on Lexical and Computational Semantics</a></strong><br><a href=/people/i/iryna-gurevych/>Iryna Gurevych</a>
|
<a href=/people/m/marianna-apidianaki/>Marianna Apidianaki</a>
|
<a href=/people/m/manaal-faruqui/>Manaal Faruqui</a><br><a href=/volumes/2020.starsem-1/ class=text-muted>Proceedings of the Ninth Joint Conference on Lexical and Computational Semantics</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.194.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--194 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.194 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938833 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.194" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.194/>MultiCQA : Zero-Shot Transfer of Self-Supervised Text Matching Models on a Massive Scale<span class=acl-fixed-case>M</span>ulti<span class=acl-fixed-case>CQA</span>: Zero-Shot Transfer of Self-Supervised Text Matching Models on a Massive Scale</a></strong><br><a href=/people/a/andreas-ruckle/>Andreas RÃ¼cklÃ©</a>
|
<a href=/people/j/jonas-pfeiffer/>Jonas Pfeiffer</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a><br><a href=/volumes/2020.emnlp-main/ class=text-muted>Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--194><div class="card-body p-3 small">We study the zero-shot transfer capabilities of text matching models on a massive scale, by self-supervised training on 140 source domains from community question answering forums in English. We investigate the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> performances on nine benchmarks of answer selection and question similarity tasks, and show that all 140 models transfer surprisingly well, where the large majority of <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> substantially outperforms common IR baselines. We also demonstrate that considering a broad selection of source domains is crucial for obtaining the best zero-shot transfer performances, which contrasts the standard procedure that merely relies on the largest and most similar domains. In addition, we extensively study how to best combine multiple source domains. We propose to incorporate self-supervised with supervised multi-task learning on all available source domains. Our best zero-shot transfer model considerably outperforms in-domain BERT and the previous state of the art on six benchmarks. Fine-tuning of our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> with in-domain data results in additional large gains and achieves the new state of the art on all nine benchmarks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.cl-2.4.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--cl-2--4 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.cl-2.4 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.cl-2.4" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.cl-2.4/>LINSPECTOR : Multilingual Probing Tasks for Word Representations<span class=acl-fixed-case>LINSPECTOR</span>: Multilingual Probing Tasks for Word Representations</a></strong><br><a href=/people/g/gozde-gul-sahin/>GÃ¶zde GÃ¼l Åžahin</a>
|
<a href=/people/c/clara-vania/>Clara Vania</a>
|
<a href=/people/i/ilia-kuznetsov/>Ilia Kuznetsov</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a><br><a href=/volumes/2020.cl-2/ class=text-muted>Computational Linguistics, Volume 46, Issue 2 - June 2020</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--cl-2--4><div class="card-body p-3 small">Despite an ever-growing number of word representation models introduced for a large number of languages, there is a lack of a standardized technique to provide insights into what is captured by these <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a>. Such insights would help the community to get an estimate of the downstream task performance, as well as to design more informed neural architectures, while avoiding extensive experimentation that requires substantial computational resources not all researchers have access to. A recent development in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> is to use simple classification tasks, also called probing tasks, that test for a single <a href=https://en.wikipedia.org/wiki/Feature_(linguistics)>linguistic feature</a> such as <a href=https://en.wikipedia.org/wiki/Part_of_speech>part-of-speech</a>. Existing studies mostly focus on exploring the <a href=https://en.wikipedia.org/wiki/Linguistic_description>linguistic information</a> encoded by the continuous representations of English text. However, from a typological perspective the morphologically poor English is rather an outlier : The information encoded by the <a href=https://en.wikipedia.org/wiki/Word_order>word order</a> and function words in <a href=https://en.wikipedia.org/wiki/English_language>English</a> is often stored on a subword, morphological level in other languages. To address this, we introduce 15 type-level probing tasks such as <a href=https://en.wikipedia.org/wiki/Grammatical_case>case marking</a>, <a href=https://en.wikipedia.org/wiki/Possession_(linguistics)>possession</a>, <a href=https://en.wikipedia.org/wiki/Word_length>word length</a>, morphological tag count, and pseudoword identification for 24 languages. We present a reusable <a href=https://en.wikipedia.org/wiki/Methodology>methodology</a> for creation and evaluation of such <a href=https://en.wikipedia.org/wiki/Test_(assessment)>tests</a> in a multilingual setting, which is challenging because of a lack of resources, lower quality of tools, and differences among languages. We then present experiments on several diverse multilingual word embedding models, in which we relate the probing task performance for a diverse set of languages to a range of five classic NLP tasks : POS-tagging, dependency parsing, semantic role labeling, <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>named entity recognition</a>, and natural language inference. We find that a number of probing tests have significantly high positive correlation to the downstream tasks, especially for morphologically rich languages. We show that our tests can be used to explore <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> or black-box neural models for linguistic cues in a multilingual setting. We release the probing data sets and the evaluation suite LINSPECTOR with https://github.com/UKPLab/linspector.</div></div><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1101.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1101 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1101 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1101" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1101/>A Bayesian Approach for Sequence Tagging with Crowds<span class=acl-fixed-case>B</span>ayesian Approach for Sequence Tagging with Crowds</a></strong><br><a href=/people/e/edwin-simpson/>Edwin Simpson</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a><br><a href=/volumes/D19-1/ class=text-muted>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1101><div class="card-body p-3 small">Current methods for sequence tagging, a core task in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a>, are data hungry, which motivates the use of <a href=https://en.wikipedia.org/wiki/Crowdsourcing>crowdsourcing</a> as a cheap way to obtain labelled data. However, annotators are often unreliable and current aggregation methods can not capture common types of span annotation error. To address this, we propose a <a href=https://en.wikipedia.org/wiki/Bayesian_inference>Bayesian method</a> for aggregating sequence tags that reduces errors by modelling sequential dependencies between the annotations as well as the ground-truth labels. By taking a Bayesian approach, we account for uncertainty in the model due to both annotator errors and the lack of data for modelling annotators who complete few tasks. We evaluate our model on crowdsourced data for <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>named entity recognition</a>, <a href=https://en.wikipedia.org/wiki/Information_extraction>information extraction</a> and <a href=https://en.wikipedia.org/wiki/Argument_mining>argument mining</a>, showing that our sequential model outperforms the previous state of the art, and that Bayesian approaches outperform non-Bayesian alternatives. We also find that our approach can reduce crowdsourcing costs through more effective <a href=https://en.wikipedia.org/wiki/Active_learning>active learning</a>, as it better captures uncertainty in the sequence labels when there are few annotations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1171.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1171 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1171 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1171.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1171" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1171/>Neural Duplicate Question Detection without Labeled Training Data</a></strong><br><a href=/people/a/andreas-ruckle/>Andreas RÃ¼cklÃ©</a>
|
<a href=/people/n/nafise-sadat-moosavi/>Nafise Sadat Moosavi</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a><br><a href=/volumes/D19-1/ class=text-muted>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1171><div class="card-body p-3 small">Supervised training of neural models to duplicate question detection in community Question Answering (CQA) requires large amounts of labeled question pairs, which can be costly to obtain. To minimize this cost, recent works thus often used alternative methods, e.g., adversarial domain adaptation. In this work, we propose two novel methodsweak supervision using the title and body of a question, and the automatic generation of duplicate questionsand show that both can achieve improved performances even though they do not require any labeled data. We provide a comparison of popular training strategies and show that our proposed approaches are more effective in many cases because they can utilize larger amounts of data from the CQA forums. Finally, we show that weak supervision with question title and body information is also an effective method to train CQA answer selection models without direct answer supervision.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1314.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1314 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1314 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1314.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1314" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1314/>Enhancing AMR-to-Text Generation with Dual Graph Representations<span class=acl-fixed-case>AMR</span>-to-Text Generation with Dual Graph Representations</a></strong><br><a href=/people/l/leonardo-f-r-ribeiro/>Leonardo F. R. Ribeiro</a>
|
<a href=/people/c/claire-gardent/>Claire Gardent</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a><br><a href=/volumes/D19-1/ class=text-muted>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1314><div class="card-body p-3 small">Generating text from <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph-based data</a>, such as Abstract Meaning Representation (AMR), is a challenging task due to the inherent difficulty in how to properly encode the structure of a <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph</a> with labeled edges. To address this difficulty, we propose a novel graph-to-sequence model that encodes different but complementary perspectives of the structural information contained in the AMR graph. The <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> learns parallel top-down and bottom-up representations of nodes capturing contrasting views of the graph. We also investigate the use of different node message passing strategies, employing different state-of-the-art graph encoders to compute node representations based on incoming and outgoing perspectives. In our experiments, we demonstrate that the dual graph representation leads to improvements in AMR-to-text generation, achieving state-of-the-art results on two AMR datasets</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1410.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1410 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1410 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1410" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1410/>Sentence-BERT : Sentence Embeddings using Siamese BERT-Networks<span class=acl-fixed-case>BERT</span>: Sentence Embeddings using <span class=acl-fixed-case>S</span>iamese <span class=acl-fixed-case>BERT</span>-Networks</a></strong><br><a href=/people/n/nils-reimers/>Nils Reimers</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a><br><a href=/volumes/D19-1/ class=text-muted>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1410><div class="card-body p-3 small">BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). However, it requires that both sentences are fed into the network, which causes a massive computational overhead : Finding the most similar pair in a collection of 10,000 sentences requires about 50 million inference computations (~65 hours) with BERT. The construction of BERT makes it unsuitable for semantic similarity search as well as for <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised tasks</a> like <a href=https://en.wikipedia.org/wiki/Cluster_analysis>clustering</a>. In this publication, we present Sentence-BERT (SBERT), a modification of the pretrained BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. This reduces the effort for finding the most similar pair from 65 hours with BERT / RoBERTa to about 5 seconds with SBERT, while maintaining the accuracy from BERT. We evaluate SBERT and SRoBERTa on common STS tasks and transfer learning tasks, where it outperforms other state-of-the-art sentence embeddings methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-3022.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-3022 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-3022 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-3022" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-3022/>LINSPECTOR WEB : A Multilingual Probing Suite for Word Representations<span class=acl-fixed-case>LINSPECTOR</span> <span class=acl-fixed-case>WEB</span>: A Multilingual Probing Suite for Word Representations</a></strong><br><a href=/people/m/max-eichler/>Max Eichler</a>
|
<a href=/people/g/gozde-gul-sahin/>GÃ¶zde GÃ¼l Åžahin</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a><br><a href=/volumes/D19-3/ class=text-muted>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP): System Demonstrations</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-3022><div class="card-body p-3 small">We present LINSPECTOR WEB, an open source multilingual inspector to analyze word representations. Our system provides researchers working in low-resource settings with an easily accessible web based probing tool to gain quick insights into their <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> especially outside of the <a href=https://en.wikipedia.org/wiki/English_language>English language</a>. To do this we employ 16 simple linguistic probing tasks such as <a href=https://en.wikipedia.org/wiki/Grammatical_gender>gender</a>, <a href=https://en.wikipedia.org/wiki/Grammatical_case>case marking</a>, and <a href=https://en.wikipedia.org/wiki/Grammatical_tense>tense</a> for a diverse set of 28 languages. We support probing of static word embeddings along with pretrained AllenNLP models that are commonly used for NLP downstream tasks such as <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>named entity recognition</a>, natural language inference and dependency parsing. The results are visualized in a <a href=https://en.wikipedia.org/wiki/Polar_chart>polar chart</a> and also provided as a table. LINSPECTOR WEB is available as an offline tool or at https://linspector.ukp.informatik.tu-darmstadt.de.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-8635.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-8635 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-8635 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-8635/>Revisiting the Binary Linearization Technique for Surface Realization</a></strong><br><a href=/people/y/yevgeniy-puzikov/>Yevgeniy Puzikov</a>
|
<a href=/people/c/claire-gardent/>Claire Gardent</a>
|
<a href=/people/i/ido-dagan/>Ido Dagan</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a><br><a href=/volumes/W19-86/ class=text-muted>Proceedings of the 12th International Conference on Natural Language Generation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-8635><div class="card-body p-3 small">End-to-end neural approaches have achieved state-of-the-art performance in many natural language processing (NLP) tasks. Yet, they often lack transparency of the underlying <a href=https://en.wikipedia.org/wiki/Decision-making>decision-making process</a>, hindering error analysis and certain model improvements. In this work, we revisit the binary linearization approach to surface realization, which exhibits more interpretable behavior, but was falling short in terms of prediction accuracy. We show how enriching the training data to better capture word order constraints almost doubles the performance of the <a href=https://en.wikipedia.org/wiki/System>system</a>. We further demonstrate that encoding both local and global prediction contexts yields another considerable performance boost. With the proposed modifications, the <a href=https://en.wikipedia.org/wiki/System>system</a> which ranked low in the latest shared task on multilingual surface realization now achieves best results in five out of ten languages, while being on par with the state-of-the-art approaches in others.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1213.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1213 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1213 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-1213/>Ranking Generated Summaries by Correctness : An Interesting but Challenging Application for Natural Language Inference</a></strong><br><a href=/people/t/tobias-falke/>Tobias Falke</a>
|
<a href=/people/l/leonardo-f-r-ribeiro/>Leonardo F. R. Ribeiro</a>
|
<a href=/people/p/prasetya-ajie-utama/>Prasetya Ajie Utama</a>
|
<a href=/people/i/ido-dagan/>Ido Dagan</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a><br><a href=/volumes/P19-1/ class=text-muted>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1213><div class="card-body p-3 small">While recent progress on abstractive summarization has led to remarkably fluent summaries, factual errors in generated summaries still severely limit their use in practice. In this paper, we evaluate summaries produced by state-of-the-art <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> via <a href=https://en.wikipedia.org/wiki/Crowdsourcing>crowdsourcing</a> and show that such <a href=https://en.wikipedia.org/wiki/Errors-in-variables_models>errors</a> occur frequently, in particular with more abstractive models. We study whether textual entailment predictions can be used to detect such errors and if they can be reduced by reranking alternative predicted summaries. That leads to an interesting downstream application for <a href=https://en.wikipedia.org/wiki/Logical_consequence>entailment models</a>. In our experiments, we find that out-of-the-box entailment models trained on NLI datasets do not yet offer the desired performance for the downstream task and we therefore release our annotations as additional test data for future extrinsic evaluations of NLI.</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1071.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1071 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1071 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=C18-1071" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/C18-1071/>Cross-lingual Argumentation Mining : <a href=https://en.wikipedia.org/wiki/Machine_translation>Machine Translation</a> (and a bit of Projection) is All You Need !</a></strong><br><a href=/people/s/steffen-eger/>Steffen Eger</a>
|
<a href=/people/j/johannes-daxenberger/>Johannes Daxenberger</a>
|
<a href=/people/c/christian-stab/>Christian Stab</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a><br><a href=/volumes/C18-1/ class=text-muted>Proceedings of the 27th International Conference on Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1071><div class="card-body p-3 small">Argumentation mining (AM) requires the identification of complex discourse structures and has lately been applied with success monolingually. In this work, we show that the existing resources are, however, not adequate for assessing cross-lingual AM, due to their heterogeneity or lack of complexity. We therefore create suitable parallel corpora by (human and machine) translating a popular AM dataset consisting of persuasive student essays into <a href=https://en.wikipedia.org/wiki/German_language>German</a>, <a href=https://en.wikipedia.org/wiki/French_language>French</a>, <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a>, and <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a>. We then compare (i) annotation projection and (ii) bilingual word embeddings based direct transfer strategies for cross-lingual AM, finding that the former performs considerably better and almost eliminates the loss from cross-lingual transfer. Moreover, we find that annotation projection works equally well when using either costly human or cheap machine translations. Our code and data are available at http://github.com/UKPLab/coling2018-xling_argument_mining.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1197.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1197 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1197 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=C18-1197" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/C18-1197/>Multimodal Grounding for <a href=https://en.wikipedia.org/wiki/Language_processing>Language Processing</a></a></strong><br><a href=/people/l/lisa-beinborn/>Lisa Beinborn</a>
|
<a href=/people/t/teresa-botschen/>Teresa Botschen</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a><br><a href=/volumes/C18-1/ class=text-muted>Proceedings of the 27th International Conference on Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1197><div class="card-body p-3 small">This survey discusses how recent developments in multimodal processing facilitate conceptual grounding of language. We categorize the information flow in multimodal processing with respect to cognitive models of human information processing and analyze different methods for combining multimodal representations. Based on this methodological inventory, we discuss the benefit of multimodal grounding for a variety of language processing tasks and the challenges that arise. We particularly focus on multimodal grounding of verbs which play a crucial role for the compositional power of language.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1280.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1280 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1280 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1280/>Modeling <a href=https://en.wikipedia.org/wiki/Semantics>Semantics</a> with Gated Graph Neural Networks for Knowledge Base Question Answering</a></strong><br><a href=/people/d/daniil-sorokin/>Daniil Sorokin</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a><br><a href=/volumes/C18-1/ class=text-muted>Proceedings of the 27th International Conference on Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1280><div class="card-body p-3 small">The most approaches to Knowledge Base Question Answering are based on <a href=https://en.wikipedia.org/wiki/Semantic_parsing>semantic parsing</a>. In this paper, we address the problem of learning vector representations for complex semantic parses that consist of multiple entities and relations. Previous work largely focused on selecting the correct semantic relations for a question and disregarded the structure of the semantic parse : the connections between entities and the directions of the relations. We propose to use Gated Graph Neural Networks to encode the graph structure of the semantic parse. We show on two <a href=https://en.wikipedia.org/wiki/Data_set>data sets</a> that the <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph networks</a> outperform all baseline models that do not explicitly model the <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>structure</a>. The <a href=https://en.wikipedia.org/wiki/Error_detection_and_correction>error analysis</a> confirms that our approach can successfully process complex semantic parses.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-2002.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-2002 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-2002 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-2002/>The INCEpTION Platform : Machine-Assisted and Knowledge-Oriented Interactive Annotation<span class=acl-fixed-case>INCE</span>p<span class=acl-fixed-case>TION</span> Platform: Machine-Assisted and Knowledge-Oriented Interactive Annotation</a></strong><br><a href=/people/j/jan-christoph-klie/>Jan-Christoph Klie</a>
|
<a href=/people/m/michael-bugert/>Michael Bugert</a>
|
<a href=/people/b/beto-boullosa/>Beto Boullosa</a>
|
<a href=/people/r/richard-eckart-de-castilho/>Richard Eckart de Castilho</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a><br><a href=/volumes/C18-2/ class=text-muted>Proceedings of the 27th International Conference on Computational Linguistics: System Demonstrations</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-2002><div class="card-body p-3 small">We introduce INCEpTION, a new annotation platform for tasks including <a href=https://en.wikipedia.org/wiki/Semantic_annotation>interactive and semantic annotation</a> (e.g., concept linking, fact linking, <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge base population</a>, semantic frame annotation). These <a href=https://en.wikipedia.org/wiki/Task_(computing)>tasks</a> are very time consuming and demanding for annotators, especially when <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge bases</a> are used. We address these issues by developing an annotation platform that incorporates machine learning capabilities which actively assist and guide annotators. The <a href=https://en.wikipedia.org/wiki/Computing_platform>platform</a> is both generic and modular. It targets a range of research domains in need of semantic annotation, such as <a href=https://en.wikipedia.org/wiki/Digital_humanities>digital humanities</a>, <a href=https://en.wikipedia.org/wiki/Bioinformatics>bioinformatics</a>, or <a href=https://en.wikipedia.org/wiki/Linguistics>linguistics</a>. INCEpTION is publicly available as <a href=https://en.wikipedia.org/wiki/Open-source_software>open-source software</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2018.gwc-1.20.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2018--gwc-1--20 data-toggle=collapse aria-expanded=false aria-controls=abstract-2018.gwc-1.20 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2018.gwc-1.20/>Lexical-semantic resources : yet powerful resources for automatic personality classification</a></strong><br><a href=/people/x/xuan-son-vu/>Xuan-Son Vu</a>
|
<a href=/people/l/lucie-flekova/>Lucie Flekova</a>
|
<a href=/people/l/lili-jiang/>Lili Jiang</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a><br><a href=/volumes/2018.gwc-1/ class=text-muted>Proceedings of the 9th Global Wordnet Conference</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2018--gwc-1--20><div class="card-body p-3 small">In this paper, we aim to reveal the impact of lexical-semantic resources, used in particular for <a href=https://en.wikipedia.org/wiki/Word-sense_disambiguation>word sense disambiguation</a> and sense-level semantic categorization, on automatic personality classification task. While <a href=https://en.wikipedia.org/wiki/Style_(sociolinguistics)>stylistic features</a> (e.g., part-of-speech counts) have been shown their power in this task, the impact of <a href=https://en.wikipedia.org/wiki/Semantics>semantics</a> beyond targeted word lists is relatively unexplored. We propose and extract three types of lexical-semantic features, which capture high-level concepts and <a href=https://en.wikipedia.org/wiki/Emotion>emotions</a>, overcoming the lexical gap of word n-grams. Our experimental results are comparable to state-of-the-art methods, while no personality-specific resources are required.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1171.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1171 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1171 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1171" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D18-1171/>Weeding out Conventionalized Metaphors : A Corpus of Novel Metaphor Annotations</a></strong><br><a href=/people/e/erik-lan-do-dinh/>Erik-LÃ¢n Do Dinh</a>
|
<a href=/people/h/hannah-wieland/>Hannah Wieland</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a><br><a href=/volumes/D18-1/ class=text-muted>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1171><div class="card-body p-3 small">We encounter <a href=https://en.wikipedia.org/wiki/Metaphor>metaphors</a> every day, but only a few jump out on us and make us stumble. However, little effort has been devoted to investigating more novel metaphors in comparison to general metaphor detection efforts. We attribute this gap primarily to the lack of larger datasets that distinguish between conventionalized, i.e., very common, and novel metaphors. The goal of this paper is to alleviate this situation by introducing a crowdsourced novel metaphor annotation layer for an existing metaphor corpus. Further, we analyze our <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> and investigate correlations between novelty and features that are typically used in metaphor detection, such as concreteness ratings and more semantic features like the Potential for <a href=https://en.wikipedia.org/wiki/Metaphor>Metaphoricity</a>. Finally, we present a <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline approach</a> to assess novelty in metaphors based on our <a href=https://en.wikipedia.org/wiki/Annotation>annotations</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1472.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1472 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1472 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D18-1472.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1472" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D18-1472/>Is it Time to Swish? Comparing Deep Learning Activation Functions Across NLP tasks<span class=acl-fixed-case>NLP</span> tasks</a></strong><br><a href=/people/s/steffen-eger/>Steffen Eger</a>
|
<a href=/people/p/paul-youssef/>Paul Youssef</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a><br><a href=/volumes/D18-1/ class=text-muted>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1472><div class="card-body p-3 small">Activation functions play a crucial role in <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a> because they are the <a href=https://en.wikipedia.org/wiki/Nonlinear_system>nonlinearities</a> which have been attributed to the success story of <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning</a>. One of the currently most popular activation functions is <a href=https://en.wikipedia.org/wiki/ReLU>ReLU</a>, but several competitors have recently been proposed or &#8216;discovered&#8217;, including LReLU functions and swish. While most works compare newly proposed <a href=https://en.wikipedia.org/wiki/Activation_function>activation functions</a> on few tasks (usually from image classification) and against few competitors (usually ReLU), we perform the first largescale comparison of 21 <a href=https://en.wikipedia.org/wiki/Activation_function>activation functions</a> across eight different NLP tasks. We find that a largely unknown activation function performs most stably across all tasks, the so-called penalized tanh function. We also show that it can successfully replace the sigmoid and tanh gates in LSTM cells, leading to a 2 percentage point (pp) improvement over the standard choices on a challenging NLP task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-2020.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-2020 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-2020 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-2020" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D18-2020/>Interactive Instance-based Evaluation of Knowledge Base Question Answering</a></strong><br><a href=/people/d/daniil-sorokin/>Daniil Sorokin</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a><br><a href=/volumes/D18-2/ class=text-muted>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-2020><div class="card-body p-3 small">Most approaches to Knowledge Base Question Answering are based on <a href=https://en.wikipedia.org/wiki/Semantic_parsing>semantic parsing</a>. In this paper, we present a tool that aids in debugging of question answering systems that construct a structured semantic representation for the input question. Previous work has largely focused on building <a href=https://en.wikipedia.org/wiki/Question_answering>question answering interfaces</a> or evaluation frameworks that unify multiple data sets. The primary objective of our <a href=https://en.wikipedia.org/wiki/System>system</a> is to enable interactive debugging of model predictions on individual instances (questions) and to simplify manual error analysis. Our interactive interface helps researchers to understand the shortcomings of a particular <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>, qualitatively analyze the complete <a href=https://en.wikipedia.org/wiki/Pipeline_transport>pipeline</a> and compare different <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>. A set of sit-by sessions was used to validate our <a href=https://en.wikipedia.org/wiki/User_interface_design>interface design</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-2022.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-2022 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-2022 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-2022/>Integrating Knowledge-Supported Search into the INCEpTION Annotation Platform<span class=acl-fixed-case>INCE</span>p<span class=acl-fixed-case>TION</span> Annotation Platform</a></strong><br><a href=/people/b/beto-boullosa/>Beto Boullosa</a>
|
<a href=/people/r/richard-eckart-de-castilho/>Richard Eckart de Castilho</a>
|
<a href=/people/n/naveen-kumar-laskari/>Naveen Kumar</a>
|
<a href=/people/j/jan-christoph-klie/>Jan-Christoph Klie</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a><br><a href=/volumes/D18-2/ class=text-muted>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-2022><div class="card-body p-3 small">Annotating entity mentions and linking them to a knowledge resource are essential tasks in many domains. It disambiguates mentions, introduces cross-document coreferences, and the resources contribute extra information, e.g. taxonomic relations. Such tasks benefit from <a href=https://en.wikipedia.org/wiki/Text_(literary_theory)>text annotation tools</a> that integrate a <a href=https://en.wikipedia.org/wiki/Search_engine_technology>search</a> which covers the <a href=https://en.wikipedia.org/wiki/Text_(literary_theory)>text</a>, the <a href=https://en.wikipedia.org/wiki/Annotation>annotations</a>, as well as the knowledge resource. However, to the best of our knowledge, no current tools integrate knowledge-supported search as well as entity linking support. We address this gap by introducing knowledge-supported search functionality into the INCEpTION text annotation platform. In our approach, cross-document references are created by linking entity mentions to a <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge base</a> in the form of a structured hierarchical vocabulary. The resulting <a href=https://en.wikipedia.org/wiki/Annotation>annotations</a> are then indexed to enable fast and yet complex <a href=https://en.wikipedia.org/wiki/Information_retrieval>queries</a> taking into account the text, the annotations, and the vocabulary structure.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S18-1121.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S18-1121 data-toggle=collapse aria-expanded=false aria-controls=abstract-S18-1121 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/S18-1121/>SemEval-2018 Task 12 : The Argument Reasoning Comprehension Task<span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2018 Task 12: The Argument Reasoning Comprehension Task</a></strong><br><a href=/people/i/ivan-habernal/>Ivan Habernal</a>
|
<a href=/people/h/henning-wachsmuth/>Henning Wachsmuth</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a>
|
<a href=/people/b/benno-stein/>Benno Stein</a><br><a href=/volumes/S18-1/ class=text-muted>Proceedings of The 12th International Workshop on Semantic Evaluation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S18-1121><div class="card-body p-3 small">A natural language argument is composed of a claim as well as reasons given as premises for the claim. The warrant explaining the reasoning is usually left implicit, as it is clear from the context and common sense. This makes a <a href=https://en.wikipedia.org/wiki/Comprehension_(logic)>comprehension of arguments</a> easy for humans but hard for machines. This paper summarizes the first shared <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> on argument reasoning comprehension. Given a premise and a claim along with some topic information, the goal was to automatically identify the correct warrant among two candidates that are plausible and lexically close, but in fact imply opposite claims. We describe the <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> with 1970 instances that we built for the task, and we outline the 21 computational approaches that participated, most of which used <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a>. The results reveal the <a href=https://en.wikipedia.org/wiki/Computational_complexity_theory>complexity</a> of the <a href=https://en.wikipedia.org/wiki/Computational_complexity_theory>task</a>, with many approaches hardly improving over the <a href=https://en.wikipedia.org/wiki/Random_variable>random accuracy</a> of about 0.5. Still, the best observed accuracy (0.712) underlines the principle feasibility of <a href=https://en.wikipedia.org/wiki/Warrant_(law)>identifying warrants</a>. Our analysis indicates that an inclusion of <a href=https://en.wikipedia.org/wiki/Knowledge>external knowledge</a> is key to <a href=https://en.wikipedia.org/wiki/Understanding>reasoning comprehension</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S18-2007.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S18-2007 data-toggle=collapse aria-expanded=false aria-controls=abstract-S18-2007 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=S18-2007" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/S18-2007/>Mixing Context Granularities for Improved Entity Linking on Question Answering Data across Entity Categories</a></strong><br><a href=/people/d/daniil-sorokin/>Daniil Sorokin</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a><br><a href=/volumes/S18-2/ class=text-muted>Proceedings of the Seventh Joint Conference on Lexical and Computational Semantics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S18-2007><div class="card-body p-3 small">The first stage of every knowledge base question answering approach is to link entities in the input question. We investigate <a href=https://en.wikipedia.org/wiki/Entity_linking>entity linking</a> in the context of <a href=https://en.wikipedia.org/wiki/Question_answering>question answering task</a> and present a jointly optimized neural architecture for entity mention detection and entity disambiguation that models the surrounding context on different levels of granularity. We use the Wikidata knowledge base and available question answering datasets to create benchmarks for <a href=https://en.wikipedia.org/wiki/Entity_linking>entity linking</a> on question answering data. Our approach outperforms the previous state-of-the-art <a href=https://en.wikipedia.org/wiki/System>system</a> on this <a href=https://en.wikipedia.org/wiki/Data>data</a>, resulting in an average 8 % improvement of the final score. We further demonstrate that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> delivers a strong performance across different entity categories.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3602.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3602 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3602 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3602/>BinLin : A Simple Method of Dependency Tree Linearization<span class=acl-fixed-case>B</span>in<span class=acl-fixed-case>L</span>in: A Simple Method of Dependency Tree Linearization</a></strong><br><a href=/people/y/yevgeniy-puzikov/>Yevgeniy Puzikov</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a><br><a href=/volumes/W18-36/ class=text-muted>Proceedings of the First Workshop on Multilingual Surface Realisation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3602><div class="card-body p-3 small">Surface Realization Shared Task 2018 is a workshop on generating sentences from lemmatized sets of dependency triples. This paper describes the results of our participation in the challenge. We develop a data-driven pipeline system which first orders the lemmas and then conjugates the words to finish the surface realization process. Our contribution is a novel sequential method of ordering lemmas, which, despite its simplicity, achieves promising results. We demonstrate the effectiveness of the proposed approach, describe its limitations and outline ways to improve it.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5211.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5211 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5211 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-5211" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-5211/>Frame- and Entity-Based Knowledge for Common-Sense Argumentative Reasoning</a></strong><br><a href=/people/t/teresa-botschen/>Teresa Botschen</a>
|
<a href=/people/d/daniil-sorokin/>Daniil Sorokin</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a><br><a href=/volumes/W18-52/ class=text-muted>Proceedings of the 5th Workshop on Argument Mining</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5211><div class="card-body p-3 small">Common-sense argumentative reasoning is a challenging task that requires holistic understanding of the argumentation where external knowledge about the world is hypothesized to play a key role. We explore the idea of using event knowledge about prototypical situations from <a href=https://en.wikipedia.org/wiki/FrameNet>FrameNet</a> and fact knowledge about concrete entities from <a href=https://en.wikipedia.org/wiki/Wikidata>Wikidata</a> to solve the task. We find that both resources can contribute to an improvement over the non-enriched approach and point out two persisting challenges : first, integration of many <a href=https://en.wikipedia.org/wiki/Annotation>annotations</a> of the same type, and second, fusion of complementary annotations. After our explorations, we question the key role of external world knowledge with respect to the argumentative reasoning task and rather point towards a logic-based analysis of the chain of reasoning.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5216.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5216 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5216 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-5216" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-5216/>PD3 : Better Low-Resource Cross-Lingual Transfer By Combining Direct Transfer and Annotation Projection<span class=acl-fixed-case>PD</span>3: Better Low-Resource Cross-Lingual Transfer By Combining Direct Transfer and Annotation Projection</a></strong><br><a href=/people/s/steffen-eger/>Steffen Eger</a>
|
<a href=/people/a/andreas-ruckle/>Andreas RÃ¼cklÃ©</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a><br><a href=/volumes/W18-52/ class=text-muted>Proceedings of the 5th Workshop on Argument Mining</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5216><div class="card-body p-3 small">We consider unsupervised cross-lingual transfer on two tasks, viz., sentence-level argumentation mining and standard POS tagging. We combine direct transfer using bilingual embeddings with annotation projection, which projects labels across unlabeled parallel data. We do so by either merging respective source and target language datasets or alternatively by using <a href=https://en.wikipedia.org/wiki/Multi-task_learning>multi-task learning</a>. Our combination strategy considerably improves upon both direct transfer and <a href=https://en.wikipedia.org/wiki/Projection_(linear_algebra)>projection</a> with few available parallel sentences, the most realistic scenario for many low-resource target languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5516.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5516 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5516 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-5516" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-5516/>UKP-Athene : Multi-Sentence Textual Entailment for Claim Verification<span class=acl-fixed-case>UKP</span>-Athene: Multi-Sentence Textual Entailment for Claim Verification</a></strong><br><a href=/people/a/andreas-hanselowski/>Andreas Hanselowski</a>
|
<a href=/people/h/hao-zhang/>Hao Zhang</a>
|
<a href=/people/z/zile-li/>Zile Li</a>
|
<a href=/people/d/daniil-sorokin/>Daniil Sorokin</a>
|
<a href=/people/b/benjamin-schiller/>Benjamin Schiller</a>
|
<a href=/people/c/claudia-schulz/>Claudia Schulz</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a><br><a href=/volumes/W18-55/ class=text-muted>Proceedings of the First Workshop on Fact Extraction and VERification (FEVER)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5516><div class="card-body p-3 small">The Fact Extraction and VERification (FEVER) shared task was launched to support the development of systems able to verify claims by extracting supporting or refuting facts from raw text. The shared task organizers provide a large-scale dataset for the consecutive steps involved in <a href=https://en.wikipedia.org/wiki/Verification_and_validation>claim verification</a>, in particular, <a href=https://en.wikipedia.org/wiki/Document_retrieval>document retrieval</a>, fact extraction, and <a href=https://en.wikipedia.org/wiki/Statistical_classification>claim classification</a>. In this paper, we present our claim verification pipeline approach, which, according to the preliminary results, scored third in the shared task, out of 23 competing systems. For the <a href=https://en.wikipedia.org/wiki/Document_retrieval>document retrieval</a>, we implemented a new entity linking approach. In order to be able to rank candidate facts and classify a claim on the basis of several selected facts, we introduce two extensions to the Enhanced LSTM (ESIM).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6557.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6557 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6557 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6557/>E2E NLG Challenge : Neural Models vs. Templates<span class=acl-fixed-case>E</span>2<span class=acl-fixed-case>E</span> <span class=acl-fixed-case>NLG</span> Challenge: Neural Models vs. Templates</a></strong><br><a href=/people/y/yevgeniy-puzikov/>Yevgeniy Puzikov</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a><br><a href=/volumes/W18-65/ class=text-muted>Proceedings of the 11th International Conference on Natural Language Generation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6557><div class="card-body p-3 small">E2E NLG Challenge is a shared task on generating restaurant descriptions from sets of key-value pairs. This paper describes the results of our participation in the challenge. We develop a simple, yet effective neural encoder-decoder model which produces fluent restaurant descriptions and outperforms a strong <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline</a>. We further analyze the data provided by the organizers and conclude that the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> can also be approached with a template-based model developed in just a few hours.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1036.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1036 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1036 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N18-1036.Datasets.zip data-toggle=tooltip data-placement=top title=Dataset><i class="fas fa-file-archive"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=http://vimeo.com/276425357 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N18-1036" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/N18-1036/>Before Name-Calling : Dynamics and Triggers of Ad Hominem Fallacies in Web Argumentation</a></strong><br><a href=/people/i/ivan-habernal/>Ivan Habernal</a>
|
<a href=/people/h/henning-wachsmuth/>Henning Wachsmuth</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a>
|
<a href=/people/b/benno-stein/>Benno Stein</a><br><a href=/volumes/N18-1/ class=text-muted>Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1036><div class="card-body p-3 small">Arguing without committing a fallacy is one of the main requirements of an ideal debate. But even when debating rules are strictly enforced and fallacious arguments punished, arguers often lapse into attacking the opponent by an ad hominem argument. As existing research lacks solid empirical investigation of the typology of <a href=https://en.wikipedia.org/wiki/Ad_hominem>ad hominem arguments</a> as well as their potential causes, this paper fills this gap by (1) performing several large-scale annotation studies, (2) experimenting with various neural architectures and validating our working hypotheses, such as controversy or reasonableness, and (3) providing linguistic insights into triggers of <a href=https://en.wikipedia.org/wiki/Ad_hominem>ad hominem</a> using explainable neural network architectures.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2006.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2006 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2006 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N18-2006" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N18-2006/>Multi-Task Learning for Argumentation Mining in Low-Resource Settings</a></strong><br><a href=/people/c/claudia-schulz/>Claudia Schulz</a>
|
<a href=/people/s/steffen-eger/>Steffen Eger</a>
|
<a href=/people/j/johannes-daxenberger/>Johannes Daxenberger</a>
|
<a href=/people/t/tobias-kahse/>Tobias Kahse</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a><br><a href=/volumes/N18-2/ class=text-muted>Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2006><div class="card-body p-3 small">We investigate whether and where <a href=https://en.wikipedia.org/wiki/Multi-task_learning>multi-task learning (MTL)</a> can improve performance on NLP problems related to argumentation mining (AM), in particular argument component identification. Our results show that MTL performs particularly well (and better than single-task learning) when little training data is available for the main task, a common scenario in AM. Our findings challenge previous assumptions that conceptualizations across AM datasets are divergent and that MTL is difficult for semantic or higher-level tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2103.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2103 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2103 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-2103/>Objective Function Learning to Match Human Judgements for Optimization-Based Summarization</a></strong><br><a href=/people/m/maxime-peyrard/>Maxime Peyrard</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a><br><a href=/volumes/N18-2/ class=text-muted>Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2103><div class="card-body p-3 small">Supervised summarization systems usually rely on supervision at the sentence or n-gram level provided by automatic metrics like ROUGE, which act as noisy proxies for human judgments. In this work, we learn a summary-level scoring function including human judgments as supervision and automatically generated data as <a href=https://en.wikipedia.org/wiki/Regularization_(mathematics)>regularization</a>. We extract summaries with a <a href=https://en.wikipedia.org/wiki/Genetic_algorithm>genetic algorithm</a> using as a <a href=https://en.wikipedia.org/wiki/Fitness_function>fitness function</a>. We observe strong and promising performances across datasets in both automatic and manual evaluation.<tex-math>\\theta</tex-math> including human judgments as supervision and automatically generated data as regularization. We extract summaries with a genetic algorithm using <tex-math>\\theta</tex-math> as a fitness function. We observe strong and promising performances across datasets in both automatic and manual evaluation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1000.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P18-1000/>Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></strong><br><a href=/people/i/iryna-gurevych/>Iryna Gurevych</a>
|
<a href=/people/y/yusuke-miyao/>Yusuke Miyao</a><br><a href=/volumes/P18-1/ class=text-muted>Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2000.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P18-2000/>Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</a></strong><br><a href=/people/i/iryna-gurevych/>Iryna Gurevych</a>
|
<a href=/people/y/yusuke-miyao/>Yusuke Miyao</a><br><a href=/volumes/P18-2/ class=text-muted>Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</a></span></p><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1081.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1081 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1081 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-1081/>Concept-Map-Based Multi-Document Summarization using Concept Coreference Resolution and Global Importance Optimization</a></strong><br><a href=/people/t/tobias-falke/>Tobias Falke</a>
|
<a href=/people/c/christian-m-meyer/>Christian M. Meyer</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a><br><a href=/volumes/I17-1/ class=text-muted>Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1081><div class="card-body p-3 small">Concept-map-based multi-document summarization is a variant of traditional <a href=https://en.wikipedia.org/wiki/Automatic_summarization>summarization</a> that produces structured summaries in the form of <a href=https://en.wikipedia.org/wiki/Concept_map>concept maps</a>. In this work, we propose a new <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> for the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> that addresses several issues in previous methods. It learns to identify and merge coreferent concepts to reduce redundancy, determines their importance with a strong <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised model</a> and finds an optimal summary concept map via <a href=https://en.wikipedia.org/wiki/Integer_linear_programming>integer linear programming</a>. It is also computationally more efficient than previous methods, allowing us to summarize larger document sets. We evaluate the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> on two datasets, finding that it outperforms several approaches from previous work.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1002.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1002 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1002 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P17-1002.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234953034 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P17-1002" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-1002/>Neural End-to-End Learning for Computational Argumentation Mining</a></strong><br><a href=/people/s/steffen-eger/>Steffen Eger</a>
|
<a href=/people/j/johannes-daxenberger/>Johannes Daxenberger</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a><br><a href=/volumes/P17-1/ class=text-muted>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1002><div class="card-body p-3 small">We investigate <a href=https://en.wikipedia.org/wiki/Artificial_neural_network>neural techniques</a> for end-to-end computational argumentation mining (AM). We frame AM both as a token-based dependency parsing and as a token-based sequence tagging problem, including a multi-task learning setup. Contrary to models that operate on the argument component level, we find that framing AM as dependency parsing leads to subpar performance results. In contrast, less complex (local) tagging models based on BiLSTMs perform robustly across classification scenarios, being able to catch long-range dependencies inherent to the AM problem. Moreover, we find that jointly learning &#8216;natural&#8217; subtasks, in a multi-task learning setup, improves performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2039.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2039 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2039 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-2039/>Argumentation Quality Assessment : Theory vs. Practice</a></strong><br><a href=/people/h/henning-wachsmuth/>Henning Wachsmuth</a>
|
<a href=/people/n/nona-naderi/>Nona Naderi</a>
|
<a href=/people/i/ivan-habernal/>Ivan Habernal</a>
|
<a href=/people/y/yufang-hou/>Yufang Hou</a>
|
<a href=/people/g/graeme-hirst/>Graeme Hirst</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a>
|
<a href=/people/b/benno-stein/>Benno Stein</a><br><a href=/volumes/P17-2/ class=text-muted>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2039><div class="card-body p-3 small">Argumentation quality is viewed differently in <a href=https://en.wikipedia.org/wiki/Argumentation_theory>argumentation theory</a> and in practical assessment approaches. This paper studies to what extent the views match empirically. We find that most observations on quality phrased spontaneously are in fact adequately represented by <a href=https://en.wikipedia.org/wiki/Theory>theory</a>. Even more, relative comparisons of arguments in practice correlate with absolute quality ratings based on theory. Our results clarify how the two views can learn from each other.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2056.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2056 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2056 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-2056/>Integrating <a href=https://en.wikipedia.org/wiki/Deep_learning>Deep Linguistic Features</a> in Factuality Prediction over Unified Datasets</a></strong><br><a href=/people/g/gabriel-stanovsky/>Gabriel Stanovsky</a>
|
<a href=/people/j/judith-eckle-kohler/>Judith Eckle-Kohler</a>
|
<a href=/people/y/yevgeniy-puzikov/>Yevgeniy Puzikov</a>
|
<a href=/people/i/ido-dagan/>Ido Dagan</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a><br><a href=/volumes/P17-2/ class=text-muted>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2056><div class="card-body p-3 small">Previous models for the assessment of commitment towards a predicate in a sentence (also known as factuality prediction) were trained and tested against a specific annotated dataset, subsequently limiting the generality of their results. In this work we propose an intuitive method for mapping three previously annotated corpora onto a single factuality scale, thereby enabling <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> to be tested across these <a href=https://en.wikipedia.org/wiki/Text_corpus>corpora</a>. In addition, we design a novel model for factuality prediction by first extending a previous rule-based factuality prediction system and applying it over an abstraction of dependency trees, and then using the output of this system in a <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised classifier</a>. We show that this <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms previous <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> on all three <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>. We make both the unified factuality corpus and our new model publicly available.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-0814.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-0814 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-0814 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-0814/>Assessing SRL Frameworks with Automatic Training Data Expansion<span class=acl-fixed-case>SRL</span> Frameworks with Automatic Training Data Expansion</a></strong><br><a href=/people/s/silvana-hartmann/>Silvana Hartmann</a>
|
<a href=/people/e/eva-mujdricza-maydt/>Ã‰va MÃºjdricza-Maydt</a>
|
<a href=/people/i/ilia-kuznetsov/>Ilia Kuznetsov</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a>
|
<a href=/people/a/anette-frank/>Anette Frank</a><br><a href=/volumes/W17-08/ class=text-muted>Proceedings of the 11th Linguistic Annotation Workshop</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-0814><div class="card-body p-3 small">We present the first experiment-based study that explicitly contrasts the three major semantic role labeling frameworks. As a prerequisite, we create a dataset labeled with parallel FrameNet-, PropBank-, and VerbNet-style labels for <a href=https://en.wikipedia.org/wiki/German_language>German</a>. We train a state-of-the-art SRL tool for <a href=https://en.wikipedia.org/wiki/German_language>German</a> for the different annotation styles and provide a comparative analysis across frameworks. We further explore the behavior of the <a href=https://en.wikipedia.org/wiki/Software_framework>frameworks</a> with automatic training data generation. VerbNet provides larger semantic expressivity than <a href=https://en.wikipedia.org/wiki/PropBank>PropBank</a>, and we find that its generalization capacity approaches <a href=https://en.wikipedia.org/wiki/PropBank>PropBank</a> in SRL training, but it benefits less from training data expansion than the sparse-data affected FrameNet.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-0902.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-0902 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-0902 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W17-0902" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W17-0902/>A Consolidated Open Knowledge Representation for Multiple Texts</a></strong><br><a href=/people/r/rachel-wities/>Rachel Wities</a>
|
<a href=/people/v/vered-shwartz/>Vered Shwartz</a>
|
<a href=/people/g/gabriel-stanovsky/>Gabriel Stanovsky</a>
|
<a href=/people/m/meni-adler/>Meni Adler</a>
|
<a href=/people/o/ori-shapira/>Ori Shapira</a>
|
<a href=/people/s/shyam-upadhyay/>Shyam Upadhyay</a>
|
<a href=/people/d/dan-roth/>Dan Roth</a>
|
<a href=/people/e/eugenio-martinez-camara/>Eugenio Martinez Camara</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a>
|
<a href=/people/i/ido-dagan/>Ido Dagan</a><br><a href=/volumes/W17-09/ class=text-muted>Proceedings of the 2nd Workshop on Linking Models of Lexical, Sentential and Discourse-level Semantics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-0902><div class="card-body p-3 small">We propose to move from Open Information Extraction (OIE) ahead to Open Knowledge Representation (OKR), aiming to represent information conveyed jointly in a set of texts in an open text-based manner. We do so by consolidating OIE extractions using entity and predicate coreference, while modeling information containment between coreferring elements via lexical entailment. We suggest that generating OKR structures can be a useful step in the NLP pipeline, to give semantic applications an easy handle on consolidated information across multiple texts.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2618.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2618 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2618 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-2618/>Prediction of Frame-to-Frame Relations in the FrameNet Hierarchy with Frame Embeddings<span class=acl-fixed-case>F</span>rame<span class=acl-fixed-case>N</span>et Hierarchy with Frame Embeddings</a></strong><br><a href=/people/t/teresa-botschen/>Teresa Botschen</a>
|
<a href=/people/h/hatem-mousselly-sergieh/>Hatem Mousselly-Sergieh</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a><br><a href=/volumes/W17-26/ class=text-muted>Proceedings of the 2nd Workshop on Representation Learning for NLP</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2618><div class="card-body p-3 small">Automatic completion of frame-to-frame (F2F) relations in the FrameNet (FN) hierarchy has received little attention, although they incorporate meta-level commonsense knowledge and are used in downstream approaches. We address the problem of sparsely annotated F2F relations. First, we examine whether the manually defined F2F relations emerge from text by learning text-based frame embeddings. Our analysis reveals insights about the difficulty of reconstructing F2F relations purely from <a href=https://en.wikipedia.org/wiki/Text_(literary_theory)>text</a>. Second, we present different systems for predicting F2F relations ; our best-performing one uses the FN hierarchy to train on and to ground embeddings in. A comparison of systems and <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a> exposes the crucial influence of knowledge-based embeddings to a <a href=https://en.wikipedia.org/wiki/System>system</a>&#8217;s performance in predicting F2F relations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4510.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4510 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4510 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4510/>Learning to Score System Summaries for Better Content Selection Evaluation.</a></strong><br><a href=/people/m/maxime-peyrard/>Maxime Peyrard</a>
|
<a href=/people/t/teresa-botschen/>Teresa Botschen</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a><br><a href=/volumes/W17-45/ class=text-muted>Proceedings of the Workshop on New Frontiers in Summarization</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4510><div class="card-body p-3 small">The evaluation of summaries is a challenging but crucial task of the summarization field. In this work, we propose to learn an automatic scoring metric based on the human judgements available as part of classical summarization datasets like TAC-2008 and TAC-2009. Any existing automatic scoring metrics can be included as <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a>, the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> learns the combination exhibiting the best correlation with human judgments. The <a href=https://en.wikipedia.org/wiki/Reliability_(statistics)>reliability</a> of the new <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metric</a> is tested in a further manual evaluation where we ask humans to evaluate summaries covering the whole scoring spectrum of the <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metric</a>. We release the trained <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metric</a> as an open-source tool.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5100.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5100/>Proceedings of the 4th Workshop on Argument Mining</a></strong><br><a href=/people/i/ivan-habernal/>Ivan Habernal</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a>
|
<a href=/people/k/kevin-d-ashley/>Kevin Ashley</a>
|
<a href=/people/c/claire-cardie/>Claire Cardie</a>
|
<a href=/people/n/nancy-green/>Nancy Green</a>
|
<a href=/people/d/diane-litman/>Diane Litman</a>
|
<a href=/people/g/georgios-petasis/>Georgios Petasis</a>
|
<a href=/people/c/chris-reed/>Chris Reed</a>
|
<a href=/people/n/noam-slonim/>Noam Slonim</a>
|
<a href=/people/v/vern-walker/>Vern Walker</a><br><a href=/volumes/W17-51/ class=text-muted>Proceedings of the 4th Workshop on Argument Mining</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/J17-1004.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-J17-1004 data-toggle=collapse aria-expanded=false aria-controls=abstract-J17-1004 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/J17-1004/>Argumentation Mining in User-Generated Web Discourse</a></strong><br><a href=/people/i/ivan-habernal/>Ivan Habernal</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a><br><a href=/volumes/J17-1/ class=text-muted>Computational Linguistics, Volume 43, Issue 1 - April 2017</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-J17-1004><div class="card-body p-3 small">The goal of argumentation mining, an evolving research field in <a href=https://en.wikipedia.org/wiki/Computational_linguistics>computational linguistics</a>, is to design methods capable of analyzing people&#8217;s argumentation. In this article, we go beyond the state of the art in several ways. (i) We deal with actual Web data and take up the challenges given by the variety of registers, multiple domains, and unrestricted noisy user-generated Web discourse. (ii) We bridge the gap between normative argumentation theories and argumentation phenomena encountered in actual data by adapting an argumentation model tested in an extensive annotation study. (iii) We create a new gold standard corpus (90k tokens in 340 documents) and experiment with several machine learning methods to identify argument components. We offer the <a href=https://en.wikipedia.org/wiki/Data>data</a>, source codes, and annotation guidelines to the community under free licenses. Our findings show that argumentation mining in user-generated Web discourse is a feasible but challenging task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/J17-3005.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-J17-3005 data-toggle=collapse aria-expanded=false aria-controls=abstract-J17-3005 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/J17-3005/>Parsing Argumentation Structures in Persuasive Essays</a></strong><br><a href=/people/c/christian-stab/>Christian Stab</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a><br><a href=/volumes/J17-3/ class=text-muted>Computational Linguistics, Volume 43, Issue 3 - September 2017</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-J17-3005><div class="card-body p-3 small">In this article, we present a novel approach for parsing argumentation structures. We identify argument components using <a href=https://en.wikipedia.org/wiki/Sequence_labeling>sequence labeling</a> at the token level and apply a new joint model for detecting argumentation structures. The proposed model globally optimizes argument component types and argumentative relations using <a href=https://en.wikipedia.org/wiki/Integer_linear_programming>Integer Linear Programming</a>. We show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> significantly outperforms challenging heuristic baselines on two different types of <a href=https://en.wikipedia.org/wiki/Discourse>discourse</a>. Moreover, we introduce a novel corpus of persuasive essays annotated with <a href=https://en.wikipedia.org/wiki/Argumentation_theory>argumentation structures</a>. We show that our annotation scheme and annotation guidelines successfully guide human annotators to substantial agreement.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S17-2005.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S17-2005 data-toggle=collapse aria-expanded=false aria-controls=abstract-S17-2005 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/S17-2005/>SemEval-2017 Task 7 : Detection and Interpretation of English Puns<span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2017 Task 7: Detection and Interpretation of <span class=acl-fixed-case>E</span>nglish Puns</a></strong><br><a href=/people/t/tristan-miller/>Tristan Miller</a>
|
<a href=/people/c/christian-f-hempelmann/>Christian Hempelmann</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a><br><a href=/volumes/S17-2/ class=text-muted>Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S17-2005><div class="card-body p-3 small">A pun is a form of <a href=https://en.wikipedia.org/wiki/Word_play>wordplay</a> in which a word suggests two or more meanings by exploiting <a href=https://en.wikipedia.org/wiki/Polysemy>polysemy</a>, <a href=https://en.wikipedia.org/wiki/Homonym>homonymy</a>, or phonological similarity to another word, for an intended humorous or rhetorical effect. Though a recurrent and expected feature in many discourse types, puns stymie traditional approaches to computational lexical semantics because they violate their one-sense-per-context assumption. This paper describes the first competitive evaluation for the automatic detection, location, and interpretation of puns. We describe the motivation for these <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a>, the evaluation methods, and the manually annotated data set. Finally, we present an overview and discussion of the participating <a href=https://en.wikipedia.org/wiki/System>systems</a>&#8217; methodologies, resources, and results.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S17-2163.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S17-2163 data-toggle=collapse aria-expanded=false aria-controls=abstract-S17-2163 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=S17-2163" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/S17-2163/>EELECTION at SemEval-2017 Task 10 : Ensemble of nEural Learners for kEyphrase ClassificaTION<span class=acl-fixed-case>EELECTION</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2017 Task 10: Ensemble of n<span class=acl-fixed-case>E</span>ural Learners for k<span class=acl-fixed-case>E</span>yphrase <span class=acl-fixed-case>C</span>lassifica<span class=acl-fixed-case>TION</span></a></strong><br><a href=/people/s/steffen-eger/>Steffen Eger</a>
|
<a href=/people/e/erik-lan-do-dinh/>Erik-LÃ¢n Do Dinh</a>
|
<a href=/people/i/ilia-kuznetsov/>Ilia Kuznetsov</a>
|
<a href=/people/m/masoud-kiaeeha/>Masoud Kiaeeha</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a><br><a href=/volumes/S17-2/ class=text-muted>Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S17-2163><div class="card-body p-3 small">This paper describes our approach to the SemEval 2017 Task 10 : Extracting Keyphrases and Relations from Scientific Publications, specifically to Subtask (B): Classification of identified keyphrases. We explored three different deep learning approaches : a character-level convolutional neural network (CNN), a stacked learner with an MLP meta-classifier, and an attention based Bi-LSTM. From these approaches, we created an ensemble of differently hyper-parameterized systems, achieving a micro-F_1-score of 0.63 on the test data. Our <a href=https://en.wikipedia.org/wiki/Glossary_of_French_expressions_in_English>approach</a> ranks 2nd (score of 1st placed system : 0.64) out of four according to this official score. However, we erroneously trained 2 out of 3 <a href=https://en.wikipedia.org/wiki/Artificial_neural_network>neural nets</a> (the stacker and the CNN) on only roughly 15 % of the full data, namely, the original development set. When trained on the full data (training+development), our <a href=https://en.wikipedia.org/wiki/Statistical_ensemble_(mathematical_physics)>ensemble</a> has a micro-F_1-score of 0.69. Our code is available from.<tex-math>F_1</tex-math>-score of 0.63 on the test data. Our approach ranks 2nd (score of 1st placed system: 0.64) out of four according to this official score. However, we erroneously trained 2 out of 3 neural nets (the stacker and the CNN) on only roughly 15% of the full data, namely, the original development set. When trained on the full data (training+development), our ensemble has a micro-<tex-math>F_{1}</tex-math>-score of 0.69. Our code is available from <url>https://github.com/UKPLab/semeval2017-scienceie</url>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1035.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1035 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1035 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D17-1035" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D17-1035/>Reporting Score Distributions Makes a Difference : Performance Study of LSTM-networks for Sequence Tagging<span class=acl-fixed-case>LSTM</span>-networks for Sequence Tagging</a></strong><br><a href=/people/n/nils-reimers/>Nils Reimers</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a><br><a href=/volumes/D17-1/ class=text-muted>Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1035><div class="card-body p-3 small">In this paper we show that reporting a single performance score is insufficient to compare <a href=https://en.wikipedia.org/wiki/Non-deterministic_algorithm>non-deterministic approaches</a>. We demonstrate for common sequence tagging tasks that the seed value for the <a href=https://en.wikipedia.org/wiki/Random_number_generation>random number generator</a> can result in statistically significant (p &lt; 10 ^ -4) differences for state-of-the-art systems. For two recent <a href=https://en.wikipedia.org/wiki/System>systems</a> for NER, we observe an absolute difference of one percentage point <a href=https://en.wikipedia.org/wiki/F-score>F-score</a> depending on the selected seed value, making these <a href=https://en.wikipedia.org/wiki/System>systems</a> perceived either as state-of-the-art or mediocre. Instead of publishing and reporting single performance scores, we propose to compare score distributions based on multiple executions. Based on the evaluation of 50.000 LSTM-networks for five sequence tagging tasks, we present <a href=https://en.wikipedia.org/wiki/Network_architecture>network architectures</a> that produce both superior performance as well as are more stable with respect to the remaining hyperparameters.<tex-math>p &lt; 10^{-4}</tex-math>) differences for state-of-the-art systems. For two recent systems for NER, we observe an absolute difference of one percentage point F&#8321;-score depending on the selected seed value, making these systems perceived either as state-of-the-art or mediocre. Instead of publishing and reporting single performance scores, we propose to compare score distributions based on multiple executions. Based on the evaluation of 50.000 LSTM-networks for five sequence tagging tasks, we present network architectures that produce both superior performance as well as are more stable with respect to the remaining hyperparameters.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1188.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1188 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1188 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1188/>Context-Aware Representations for Knowledge Base Relation Extraction</a></strong><br><a href=/people/d/daniil-sorokin/>Daniil Sorokin</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a><br><a href=/volumes/D17-1/ class=text-muted>Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1188><div class="card-body p-3 small">We demonstrate that for sentence-level relation extraction it is beneficial to consider other relations in the sentential context while predicting the target relation. Our architecture uses an LSTM-based encoder to jointly learn representations for all relations in a single sentence. We combine the <a href=https://en.wikipedia.org/wiki/Context_(language_use)>context representations</a> with an <a href=https://en.wikipedia.org/wiki/Attentional_control>attention mechanism</a> to make the final prediction. We use the <a href=https://en.wikipedia.org/wiki/Wikidata>Wikidata knowledge base</a> to construct a <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> of multiple relations per sentence and to evaluate our approach. Compared to a baseline system, our method results in an average error reduction of 24 on a held-out set of relations. The code and the dataset to replicate the experiments are made available at.<url>https://github.com/ukplab/</url>.\n</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1218.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1218 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1218 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D17-1218.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D17-1218" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D17-1218/>What is the Essence of a Claim? Cross-Domain Claim Identification</a></strong><br><a href=/people/j/johannes-daxenberger/>Johannes Daxenberger</a>
|
<a href=/people/s/steffen-eger/>Steffen Eger</a>
|
<a href=/people/i/ivan-habernal/>Ivan Habernal</a>
|
<a href=/people/c/christian-stab/>Christian Stab</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a><br><a href=/volumes/D17-1/ class=text-muted>Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1218><div class="card-body p-3 small">Argument mining has become a popular research area in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a>. It typically includes the identification of argumentative components, e.g. claims, as the central component of an argument. We perform a qualitative analysis across six different datasets and show that these appear to conceptualize claims quite differently. To learn about the consequences of such different conceptualizations of claim for practical applications, we carried out extensive experiments using state-of-the-art feature-rich and deep learning systems, to identify claims in a cross-domain fashion. While the divergent conceptualization of claims in different datasets is indeed harmful to cross-domain classification, we show that there are shared properties on the lexical level as well as system configurations that can help to overcome these gaps.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1320.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1320 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1320 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D17-1320" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D17-1320/>Bringing Structure into Summaries : Crowdsourcing a Benchmark Corpus of Concept Maps</a></strong><br><a href=/people/t/tobias-falke/>Tobias Falke</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a><br><a href=/volumes/D17-1/ class=text-muted>Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1320><div class="card-body p-3 small">Concept maps can be used to concisely represent important information and bring structure into <a href=https://en.wikipedia.org/wiki/Collection_(artwork)>large document collections</a>. Therefore, we study a variant of <a href=https://en.wikipedia.org/wiki/Multi-document_summarization>multi-document summarization</a> that produces summaries in the form of concept maps. However, suitable evaluation datasets for this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> are currently missing. To close this gap, we present a newly created corpus of concept maps that summarize heterogeneous collections of web documents on educational topics. It was created using a novel crowdsourcing approach that allows us to efficiently determine important elements in large document collections. We release the <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> along with a <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline system</a> and proposed evaluation protocol to enable further research on this variant of <a href=https://en.wikipedia.org/wiki/Automatic_summarization>summarization</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-2002.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-2002 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-2002 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D17-2002" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D17-2002/>Argotario : Computational Argumentation Meets Serious Games<span class=acl-fixed-case>A</span>rgotario: Computational Argumentation Meets Serious Games</a></strong><br><a href=/people/i/ivan-habernal/>Ivan Habernal</a>
|
<a href=/people/r/raffael-hannemann/>Raffael Hannemann</a>
|
<a href=/people/c/christian-pollak/>Christian Pollak</a>
|
<a href=/people/c/christopher-klamm/>Christopher Klamm</a>
|
<a href=/people/p/patrick-pauli/>Patrick Pauli</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a><br><a href=/volumes/D17-2/ class=text-muted>Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-2002><div class="card-body p-3 small">An important skill in <a href=https://en.wikipedia.org/wiki/Critical_thinking>critical thinking</a> and <a href=https://en.wikipedia.org/wiki/Argumentation_theory>argumentation</a> is the ability to spot and recognize <a href=https://en.wikipedia.org/wiki/Fallacy>fallacies</a>. Fallacious arguments, omnipresent in argumentative discourse, can be deceptive, manipulative, or simply leading to &#8216;wrong moves&#8217; in a discussion. Despite their importance, argumentation scholars and NLP researchers with focus on argumentation quality have not yet investigated <a href=https://en.wikipedia.org/wiki/Fallacy>fallacies</a> empirically. The nonexistence of resources dealing with <a href=https://en.wikipedia.org/wiki/Fallacy>fallacious argumentation</a> calls for scalable approaches to data acquisition and annotation, for which the serious games methodology offers an appealing, yet unexplored, alternative. We present Argotario, a <a href=https://en.wikipedia.org/wiki/Serious_game>serious game</a> that deals with fallacies in everyday argumentation. Argotario is a multilingual, open-source, platform-independent application with strong educational aspects, accessible at.<url>www.argotario.net</url>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-2004.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-2004 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-2004 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-2004/>GraphDocExplore : A Framework for the Experimental Comparison of Graph-based Document Exploration Techniques<span class=acl-fixed-case>G</span>raph<span class=acl-fixed-case>D</span>oc<span class=acl-fixed-case>E</span>xplore: A Framework for the Experimental Comparison of Graph-based Document Exploration Techniques</a></strong><br><a href=/people/t/tobias-falke/>Tobias Falke</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a><br><a href=/volumes/D17-2/ class=text-muted>Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-2004><div class="card-body p-3 small">Graphs have long been proposed as a tool to browse and navigate in a collection of documents in order to support <a href=https://en.wikipedia.org/wiki/Exploratory_search>exploratory search</a>. Many techniques to automatically extract different types of <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graphs</a>, showing for example <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entities</a> or <a href=https://en.wikipedia.org/wiki/Concept>concepts</a> and different relationships between them, have been suggested. While experimental evidence that they are indeed helpful exists for some of them, it is largely unknown which type of <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph</a> is most helpful for a specific exploratory task. However, carrying out experimental comparisons with human subjects is challenging and time-consuming. Towards this end, we present the GraphDocExplore framework. It provides an intuitive web interface for graph-based document exploration that is optimized for experimental user studies. Through a generic <a href=https://en.wikipedia.org/wiki/Graph_(abstract_data_type)>graph interface</a>, different methods to extract <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graphs</a> from text can be plugged into the <a href=https://en.wikipedia.org/wiki/System>system</a>. Hence, they can be compared at minimal implementation effort in an environment that ensures controlled comparisons. The <a href=https://en.wikipedia.org/wiki/System>system</a> is publicly available under an open-source license.<i>GraphDocExplore</i>\n framework. It provides an intuitive web interface for graph-based document\n exploration that is optimized for experimental user studies. Through a\n generic graph interface, different methods to extract graphs from text can\n be plugged into the system. Hence, they can be compared at minimal\n implementation effort in an environment that ensures controlled\n comparisons. The system is publicly available under an open-source\n license.\n</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1045.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1045 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1045 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1045/>Out-of-domain FrameNet Semantic Role Labeling<span class=acl-fixed-case>F</span>rame<span class=acl-fixed-case>N</span>et Semantic Role Labeling</a></strong><br><a href=/people/s/silvana-hartmann/>Silvana Hartmann</a>
|
<a href=/people/i/ilia-kuznetsov/>Ilia Kuznetsov</a>
|
<a href=/people/m/m-teresa-martin-valdivia/>Teresa Martin</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a><br><a href=/volumes/E17-1/ class=text-muted>Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1045><div class="card-body p-3 small">Domain dependence of NLP systems is one of the major obstacles to their application in large-scale text analysis, also restricting the applicability of FrameNet semantic role labeling (SRL) systems. Yet, current FrameNet SRL systems are still only evaluated on a single in-domain test set. For the first time, we study the domain dependence of FrameNet SRL on a wide range of <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmark sets</a>. We create a novel test set for FrameNet SRL based on user-generated web text and find that the major bottleneck for out-of-domain FrameNet SRL is the frame identification step. To address this problem, we develop a simple, yet efficient <a href=https://en.wikipedia.org/wiki/System>system</a> based on distributed word representations. Our <a href=https://en.wikipedia.org/wiki/System>system</a> closely approaches the state-of-the-art in-domain while outperforming the best available frame identification system out-of-domain. We publish our <a href=https://en.wikipedia.org/wiki/System>system</a> and test data for research purposes.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1082.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1082 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1082 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1082/>Metaheuristic Approaches to Lexical Substitution and Simplification</a></strong><br><a href=/people/s/sallam-abualhaija/>Sallam Abualhaija</a>
|
<a href=/people/t/tristan-miller/>Tristan Miller</a>
|
<a href=/people/j/judith-eckle-kohler/>Judith Eckle-Kohler</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a>
|
<a href=/people/k/karl-heinz-zimmermann/>Karl-Heinz Zimmermann</a><br><a href=/volumes/E17-1/ class=text-muted>Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1082><div class="card-body p-3 small">In this paper, we propose using metaheuristicsin particular, simulated annealing and the new D-Bees algorithmto solve <a href=https://en.wikipedia.org/wiki/Word-sense_disambiguation>word sense disambiguation</a> as an optimization problem within a knowledge-based lexical substitution system. We are the first to perform such an extrinsic evaluation of metaheuristics, for which we use two standard lexical substitution datasets, one <a href=https://en.wikipedia.org/wiki/English_language>English</a> and one <a href=https://en.wikipedia.org/wiki/German_language>German</a>. We find that D-Bees has robust performance for both languages, and performs better than <a href=https://en.wikipedia.org/wiki/Simulated_annealing>simulated annealing</a>, though both achieve good results. Moreover, the D-Beesbased lexical substitution system outperforms state-of-the-art systems on several evaluation metrics. We also show that D-Bees achieves competitive performance in <a href=https://en.wikipedia.org/wiki/Lexical_simplification>lexical simplification</a>, a variant of <a href=https://en.wikipedia.org/wiki/Lexical_substitution>lexical substitution</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1092.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1092 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1092 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1092/>Recognizing Insufficiently Supported Arguments in Argumentative Essays</a></strong><br><a href=/people/c/christian-stab/>Christian Stab</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a><br><a href=/volumes/E17-1/ class=text-muted>Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1092><div class="card-body p-3 small">In this paper, we propose a new <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> for assessing the quality of <a href=https://en.wikipedia.org/wiki/Argument_(linguistics)>natural language arguments</a>. The premises of a well-reasoned argument should provide enough evidence for accepting or rejecting its claim. Although this criterion, known as <a href=https://en.wikipedia.org/wiki/Necessity_and_sufficiency>sufficiency</a>, is widely adopted in <a href=https://en.wikipedia.org/wiki/Argumentation_theory>argumentation theory</a>, there are no empirical studies on its applicability to real arguments. In this work, we show that human annotators substantially agree on the sufficiency criterion and introduce a novel annotated corpus. Furthermore, we experiment with feature-rich SVMs and <a href=https://en.wikipedia.org/wiki/Convolutional_neural_network>Convolutional Neural Networks</a> and achieve 84 % accuracy for automatically identifying insufficiently supported arguments. The final <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> as well as the annotation guideline are freely available for encouraging future research on argument quality.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-3018.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-3018 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-3018 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-3018/>A tool for extracting sense-disambiguated example sentences through user feedback</a></strong><br><a href=/people/b/beto-boullosa/>Beto Boullosa</a>
|
<a href=/people/r/richard-eckart-de-castilho/>Richard Eckart de Castilho</a>
|
<a href=/people/a/alexander-geyken/>Alexander Geyken</a>
|
<a href=/people/l/lothar-lemnitzer/>Lothar Lemnitzer</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a><br><a href=/volumes/E17-3/ class=text-muted>Proceedings of the Software Demonstrations of the 15th Conference of the European Chapter of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-3018><div class="card-body p-3 small">This paper describes an application system aimed to help <a href=https://en.wikipedia.org/wiki/Lexicography>lexicographers</a> in the extraction of example sentences for a given headword based on its different senses. The <a href=https://en.wikipedia.org/wiki/Tool>tool</a> uses classification and clustering methods and incorporates user feedback to refine its results.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Iryna+Gurevych" title="Search for 'Iryna Gurevych' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/s/steffen-eger/ class=align-middle>Steffen Eger</a>
<span class="badge badge-secondary align-middle ml-2">7</span></li><li class=list-group-item><a href=/people/i/ivan-habernal/ class=align-middle>Ivan Habernal</a>
<span class="badge badge-secondary align-middle ml-2">7</span></li><li class=list-group-item><a href=/people/d/daniil-sorokin/ class=align-middle>Daniil Sorokin</a>
<span class="badge badge-secondary align-middle ml-2">6</span></li><li class=list-group-item><a href=/people/j/johannes-daxenberger/ class=align-middle>Johannes Daxenberger</a>
<span class="badge badge-secondary align-middle ml-2">5</span></li><li class=list-group-item><a href=/people/c/christian-stab/ class=align-middle>Christian Stab</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/t/teresa-botschen/ class=align-middle>Teresa Botschen</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/people/n/nafise-sadat-moosavi/ class=align-middle>Nafise Sadat Moosavi</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/people/n/nils-reimers/ class=align-middle>Nils Reimers</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/people/t/tobias-falke/ class=align-middle>Tobias Falke</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/people/y/yevgeniy-puzikov/ class=align-middle>Yevgeniy Puzikov</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/people/i/ido-dagan/ class=align-middle>Ido Dagan</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/people/i/ilia-kuznetsov/ class=align-middle>Ilia Kuznetsov</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/people/b/beto-boullosa/ class=align-middle>Beto Boullosa</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/r/richard-eckart-de-castilho/ class=align-middle>Richard Eckart de Castilho</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/j/jonas-pfeiffer/ class=align-middle>Jonas Pfeiffer</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/a/andreas-ruckle/ class=align-middle>Andreas RÃ¼cklÃ©</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/h/henning-wachsmuth/ class=align-middle>Henning Wachsmuth</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/b/benno-stein/ class=align-middle>Benno Stein</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/e/edwin-simpson/ class=align-middle>Edwin Simpson</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/j/jan-christoph-klie/ class=align-middle>Jan-Christoph Klie</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/m/michael-bugert/ class=align-middle>Michael Bugert</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/i/ivan-vulic/ class=align-middle>Ivan VuliÄ‡</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/s/sebastian-ruder/ class=align-middle>Sebastian Ruder</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/d/dan-roth/ class=align-middle>Dan Roth</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/y/yufang-hou/ class=align-middle>Yufang Hou</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/g/gabriel-stanovsky/ class=align-middle>Gabriel Stanovsky</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/j/judith-eckle-kohler/ class=align-middle>Judith Eckle-Kohler</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/s/silvana-hartmann/ class=align-middle>Silvana Hartmann</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/m/maxime-peyrard/ class=align-middle>Maxime Peyrard</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/e/erik-lan-do-dinh/ class=align-middle>Erik-LÃ¢n Do Dinh</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/l/leonardo-f-r-ribeiro/ class=align-middle>Leonardo F. R. Ribeiro</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/c/claire-gardent/ class=align-middle>Claire Gardent</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/g/gozde-gul-sahin/ class=align-middle>GÃ¶zde GÃ¼l Åžahin</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/t/tristan-miller/ class=align-middle>Tristan Miller</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/c/claudia-schulz/ class=align-middle>Claudia Schulz</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/y/yusuke-miyao/ class=align-middle>Yusuke Miyao</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/l/lisa-beinborn/ class=align-middle>Lisa Beinborn</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/p/phillip-rust/ class=align-middle>Phillip Rust</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/mingzhu-wu/ class=align-middle>Mingzhu Wu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/marianna-apidianaki/ class=align-middle>Marianna Apidianaki</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/manaal-faruqui/ class=align-middle>Manaal Faruqui</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/christian-m-meyer/ class=align-middle>Christian M. Meyer</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/angela-fan/ class=align-middle>Angela Fan</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/thomas-wolf/ class=align-middle>Thomas Wolf</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/ana-marasovic/ class=align-middle>Ana MarasoviÄ‡</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/sujith-ravi/ class=align-middle>Sujith Ravi</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/n/nona-naderi/ class=align-middle>Nona Naderi</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/g/graeme-hirst/ class=align-middle>Graeme Hirst</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/mohsen-mesgar/ class=align-middle>Mohsen Mesgar</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/e/eva-mujdricza-maydt/ class=align-middle>Ã‰va MÃºjdricza-Maydt</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/anette-frank/ class=align-middle>Anette Frank</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/rachel-wities/ class=align-middle>Rachel Wities</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/v/vered-shwartz/ class=align-middle>Vered Shwartz</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/meni-adler/ class=align-middle>Meni Adler</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/o/ori-shapira/ class=align-middle>Ori Shapira</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/shyam-upadhyay/ class=align-middle>Shyam Upadhyay</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/e/eugenio-martinez-camara/ class=align-middle>Eugenio MartÃ­nez-CÃ¡mara</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/hatem-mousselly-sergieh/ class=align-middle>Hatem Mousselly-Sergieh</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/kevin-d-ashley/ class=align-middle>Kevin D. Ashley</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/claire-cardie/ class=align-middle>Claire Cardie</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/n/nancy-green/ class=align-middle>Nancy Green</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/diane-litman/ class=align-middle>Diane Litman</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/g/georgios-petasis/ class=align-middle>Georgios Petasis</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/chris-reed/ class=align-middle>Chris Reed</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/n/noam-slonim/ class=align-middle>Noam Slonim</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/v/vern-walker/ class=align-middle>Vern Walker</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/x/xuan-son-vu/ class=align-middle>Xuan-Son Vu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/lucie-flekova/ class=align-middle>Lucie Flekova</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/lili-jiang/ class=align-middle>Lili Jiang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/hannah-wieland/ class=align-middle>Hannah Wieland</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/p/paul-youssef/ class=align-middle>Paul Youssef</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/n/naveen-kumar-laskari/ class=align-middle>Naveen Kumar Laskari</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/p/prasetya-utama/ class=align-middle>Prasetya Utama</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/v/victor-sanh/ class=align-middle>Victor Sanh</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/max-eichler/ class=align-middle>Max Eichler</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/christian-f-hempelmann/ class=align-middle>Christian F. Hempelmann</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/masoud-kiaeeha/ class=align-middle>Masoud Kiaeeha</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/raffael-hannemann/ class=align-middle>Raffael Hannemann</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/christian-pollak/ class=align-middle>Christian Pollak</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/christopher-klamm/ class=align-middle>Christopher Klamm</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/p/patrick-pauli/ class=align-middle>Patrick Pauli</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/kiante-brantley/ class=align-middle>KiantÃ© Brantley</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/soham-dan/ class=align-middle>Soham Dan</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/ji-ung-lee/ class=align-middle>Ji-Ung Lee</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/f/filip-radlinski/ class=align-middle>Filip Radlinski</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/hinrich-schutze/ class=align-middle>Hinrich SchÃ¼tze</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/lili-yu/ class=align-middle>Lili Yu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/n/nandan-thakur/ class=align-middle>Nandan Thakur</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/andreas-hanselowski/ class=align-middle>Andreas Hanselowski</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/hao-zhang/ class=align-middle>Hao Zhang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/zile-li/ class=align-middle>Zile Li</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/b/benjamin-schiller/ class=align-middle>Benjamin Schiller</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/tobias-kahse/ class=align-middle>Tobias Kahse</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/clara-vania/ class=align-middle>Clara Vania</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/m-teresa-martin-valdivia/ class=align-middle>M. Teresa MartÃ­n-Valdivia</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/sallam-abualhaija/ class=align-middle>Sallam Abualhaija</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/karl-heinz-zimmermann/ class=align-middle>Karl-Heinz Zimmermann</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/alexander-geyken/ class=align-middle>Alexander Geyken</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/lothar-lemnitzer/ class=align-middle>Lothar Lemnitzer</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/p/prasetya-ajie-utama/ class=align-middle>Prasetya Ajie Utama</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/emnlp/ class=align-middle>EMNLP</a><span class="badge badge-secondary align-middle ml-2">19</span></li><li class=list-group-item><a href=/venues/ws/ class=align-middle>WS</a><span class="badge badge-secondary align-middle ml-2">11</span></li><li class=list-group-item><a href=/venues/acl/ class=align-middle>ACL</a><span class="badge badge-secondary align-middle ml-2">9</span></li><li class=list-group-item><a href=/venues/eacl/ class=align-middle>EACL</a><span class="badge badge-secondary align-middle ml-2">5</span></li><li class=list-group-item><a href=/venues/coling/ class=align-middle>COLING</a><span class="badge badge-secondary align-middle ml-2">4</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-venues aria-expanded=false aria-controls=more-venues>show all...</li><div class="collapse border-top" id=more-venues><li class=list-group-item><a href=/venues/semeval/ class=align-middle>SemEval</a><span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/venues/naacl/ class=align-middle>NAACL</a><span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/venues/cl/ class=align-middle>CL</a><span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/venues/starsem/ class=align-middle>*SEM</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/ijcnlp/ class=align-middle>IJCNLP</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/sustainlp/ class=align-middle>sustainlp</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/gwc/ class=align-middle>GWC</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/internlp/ class=align-middle>InterNLP</a><span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright Â©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>