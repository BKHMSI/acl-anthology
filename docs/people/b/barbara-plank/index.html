<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Barbara Plank - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Barbara</span> <span class=font-weight-bold>Plank</span></h2><hr><div class=row><div class=col-lg-9><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.nodalida-main.21.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--nodalida-main--21 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.nodalida-main.21 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.nodalida-main.21" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.nodalida-main.21/>De-identification of Privacy-related Entities in Job Postings</a></strong><br><a href=/people/k/kristian-norgaard-jensen/>Kristian Nørgaard Jensen</a>
|
<a href=/people/m/mike-zhang/>Mike Zhang</a>
|
<a href=/people/b/barbara-plank/>Barbara Plank</a><br><a href=/volumes/2021.nodalida-main/ class=text-muted>Proceedings of the 23rd Nordic Conference on Computational Linguistics (NoDaLiDa)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--nodalida-main--21><div class="card-body p-3 small">De-identification is the task of detecting privacy-related entities in text, such as person names, <a href=https://en.wikipedia.org/wiki/Email>emails</a> and contact data. It has been well-studied within the <a href=https://en.wikipedia.org/wiki/Medicine>medical domain</a>. The need for de-identification technology is increasing, as privacy-preserving data handling is in high demand in many domains. In this paper, we focus on <a href=https://en.wikipedia.org/wiki/Employment_website>job postings</a>. We present JobStack, a new <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> for de-identification of personal data in job vacancies on <a href=https://en.wikipedia.org/wiki/Stackoverflow>Stackoverflow</a>. We introduce <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a>, comparing Long-Short Term Memory (LSTM) and Transformer models. To improve these baselines, we experiment with BERT representations, and distantly related auxiliary data via <a href=https://en.wikipedia.org/wiki/Multi-task_learning>multi-task learning</a>. Our results show that auxiliary data helps to improve <a href=https://en.wikipedia.org/wiki/De-identification>de-identification</a> performance. While BERT representations improve performance, surprisingly vanilla BERT turned out to be more effective than BERT trained on <a href=https://en.wikipedia.org/wiki/Stackoverflow>Stackoverflow-related data</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-demos.22.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-demos--22 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-demos.22 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.eacl-demos.22" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-demos.22/>Massive Choice, Ample Tasks (MaChAmp): A Toolkit for Multi-task Learning in NLP<span class=acl-fixed-case>M</span>a<span class=acl-fixed-case>C</span>h<span class=acl-fixed-case>A</span>mp): A Toolkit for Multi-task Learning in <span class=acl-fixed-case>NLP</span></a></strong><br><a href=/people/r/rob-van-der-goot/>Rob van der Goot</a>
|
<a href=/people/a/ahmet-ustun/>Ahmet Üstün</a>
|
<a href=/people/a/alan-ramponi/>Alan Ramponi</a>
|
<a href=/people/i/ibrahim-sharaf/>Ibrahim Sharaf</a>
|
<a href=/people/b/barbara-plank/>Barbara Plank</a><br><a href=/volumes/2021.eacl-demos/ class=text-muted>Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: System Demonstrations</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-demos--22><div class="card-body p-3 small">Transfer learning, particularly approaches that combine <a href=https://en.wikipedia.org/wiki/Multi-task_learning>multi-task learning</a> with pre-trained contextualized embeddings and <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a>, have advanced the field of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>Natural Language Processing</a> tremendously in recent years. In this paper we present MaChAmp, a toolkit for easy fine-tuning of contextualized embeddings in multi-task settings. The benefits of MaChAmp are its flexible configuration options, and the support of a variety of natural language processing tasks in a uniform toolkit, from text classification and sequence labeling to dependency parsing, masked language modeling, and text generation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.197.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--197 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.197 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.naacl-main.197" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.197/>From Masked Language Modeling to <a href=https://en.wikipedia.org/wiki/Translation>Translation</a> : Non-English Auxiliary Tasks Improve Zero-shot Spoken Language Understanding<span class=acl-fixed-case>E</span>nglish Auxiliary Tasks Improve Zero-shot Spoken Language Understanding</a></strong><br><a href=/people/r/rob-van-der-goot/>Rob van der Goot</a>
|
<a href=/people/i/ibrahim-sharaf/>Ibrahim Sharaf</a>
|
<a href=/people/a/aizhan-imankulova/>Aizhan Imankulova</a>
|
<a href=/people/a/ahmet-ustun/>Ahmet Üstün</a>
|
<a href=/people/m/marija-stepanovic/>Marija Stepanović</a>
|
<a href=/people/a/alan-ramponi/>Alan Ramponi</a>
|
<a href=/people/s/siti-oryza-khairunnisa/>Siti Oryza Khairunnisa</a>
|
<a href=/people/m/mamoru-komachi/>Mamoru Komachi</a>
|
<a href=/people/b/barbara-plank/>Barbara Plank</a><br><a href=/volumes/2021.naacl-main/ class=text-muted>Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--197><div class="card-body p-3 small">The lack of publicly available evaluation data for low-resource languages limits progress in Spoken Language Understanding (SLU). As key tasks like intent classification and slot filling require abundant training data, it is desirable to reuse existing <a href=https://en.wikipedia.org/wiki/Data>data</a> in high-resource languages to develop models for low-resource scenarios. We introduce xSID, a new benchmark for cross-lingual (x) Slot and Intent Detection in 13 languages from 6 language families, including a very low-resource dialect. To tackle the challenge, we propose a joint learning approach, with English SLU training data and non-English auxiliary tasks from raw text, <a href=https://en.wikipedia.org/wiki/Syntax>syntax</a> and <a href=https://en.wikipedia.org/wiki/Translation>translation</a> for transfer. We study two setups which differ by type and language coverage of the pre-trained embeddings. Our results show that jointly learning the main tasks with masked language modeling is effective for slots, while machine translation transfer works best for intent classification.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.204.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--204 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.204 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.204/>Beyond Black & White : Leveraging Annotator Disagreement via Soft-Label Multi-Task Learning</a></strong><br><a href=/people/t/tommaso-fornaciari/>Tommaso Fornaciari</a>
|
<a href=/people/a/alexandra-uma/>Alexandra Uma</a>
|
<a href=/people/s/silviu-paun/>Silviu Paun</a>
|
<a href=/people/b/barbara-plank/>Barbara Plank</a>
|
<a href=/people/d/dirk-hovy/>Dirk Hovy</a>
|
<a href=/people/m/massimo-poesio/>Massimo Poesio</a><br><a href=/volumes/2021.naacl-main/ class=text-muted>Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--204><div class="card-body p-3 small">Supervised learning assumes that a ground truth label exists. However, the <a href=https://en.wikipedia.org/wiki/Reliability_(statistics)>reliability</a> of this ground truth depends on human annotators, who often disagree. Prior work has shown that this disagreement can be helpful in training <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>. We propose a novel method to incorporate this disagreement as information : in addition to the standard error computation, we use soft-labels (i.e., probability distributions over the annotator labels) as an auxiliary task in a multi-task neural network. We measure the <a href=https://en.wikipedia.org/wiki/Divergence>divergence</a> between the predictions and the target soft-labels with several <a href=https://en.wikipedia.org/wiki/Loss_function>loss-functions</a> and evaluate the models on various <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP tasks</a>. We find that the soft-label prediction auxiliary task reduces the penalty for errors on ambiguous entities, and thereby mitigates <a href=https://en.wikipedia.org/wiki/Overfitting>overfitting</a>. It significantly improves performance across <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a>, beyond the standard approach and prior work.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.adaptnlp-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.adaptnlp-1.0/>Proceedings of the Second Workshop on Domain Adaptation for NLP</a></strong><br><a href=/people/e/eyal-ben-david/>Eyal Ben-David</a>
|
<a href=/people/s/shay-b-cohen/>Shay Cohen</a>
|
<a href=/people/r/ryan-mcdonald/>Ryan McDonald</a>
|
<a href=/people/b/barbara-plank/>Barbara Plank</a>
|
<a href=/people/r/roi-reichart/>Roi Reichart</a>
|
<a href=/people/g/guy-rotman/>Guy Rotman</a>
|
<a href=/people/y/yftah-ziser/>Yftah Ziser</a><br><a href=/volumes/2021.adaptnlp-1/ class=text-muted>Proceedings of the Second Workshop on Domain Adaptation for NLP</a></span></p><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.431.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--431 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.431 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939154 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.431/>Biomedical Event Extraction as Sequence Labeling</a></strong><br><a href=/people/a/alan-ramponi/>Alan Ramponi</a>
|
<a href=/people/r/rob-van-der-goot/>Rob van der Goot</a>
|
<a href=/people/r/rosario-lombardo/>Rosario Lombardo</a>
|
<a href=/people/b/barbara-plank/>Barbara Plank</a><br><a href=/volumes/2020.emnlp-main/ class=text-muted>Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--431><div class="card-body p-3 small">We introduce Biomedical Event Extraction as Sequence Labeling (BeeSL), a joint end-to-end neural information extraction model. BeeSL recasts the task as <a href=https://en.wikipedia.org/wiki/Sequence_labeling>sequence labeling</a>, taking advantage of a multi-label aware encoding strategy and jointly modeling the intermediate tasks via <a href=https://en.wikipedia.org/wiki/Multi-task_learning>multi-task learning</a>. BeeSL is fast, accurate, end-to-end, and unlike current methods does not require any external knowledge base or preprocessing tools. BeeSL outperforms the current best <a href=https://en.wikipedia.org/wiki/System>system</a> (Li et al., 2019) on the Genia 2011 benchmark by 1.57 % absolute <a href=https://en.wikipedia.org/wiki/Free_and_open-source_software>F1 score</a> reaching 60.22 % <a href=https://en.wikipedia.org/wiki/Free_and_open-source_software>F1</a>, establishing a new state of the art for the task. Importantly, we also provide first results on biomedical event extraction without gold entity information. Empirical results show that BeeSL&#8217;s speed and <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> makes it a viable approach for large-scale real-world scenarios.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.peoples-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.peoples-1.0/>Proceedings of the Third Workshop on Computational Modeling of People's Opinions, Personality, and Emotion's in Social Media</a></strong><br><a href=/people/m/malvina-nissim/>Malvina Nissim</a>
|
<a href=/people/v/viviana-patti/>Viviana Patti</a>
|
<a href=/people/b/barbara-plank/>Barbara Plank</a>
|
<a href=/people/e/esin-durmus/>Esin Durmus</a><br><a href=/volumes/2020.peoples-1/ class=text-muted>Proceedings of the Third Workshop on Computational Modeling of People's Opinions, Personality, and Emotion's in Social Media</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.104.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--104 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.104 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.104/>Buhscitu at SemEval-2020 Task 7 : Assessing Humour in Edited News Headlines Using Hand-Crafted Features and Online Knowledge Bases<span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 7: Assessing Humour in Edited News Headlines Using Hand-Crafted Features and Online Knowledge Bases</a></strong><br><a href=/people/k/kristian-norgaard-jensen/>Kristian Nørgaard Jensen</a>
|
<a href=/people/n/nicolaj-filrup-rasmussen/>Nicolaj Filrup Rasmussen</a>
|
<a href=/people/t/thai-wang/>Thai Wang</a>
|
<a href=/people/m/marco-placenti/>Marco Placenti</a>
|
<a href=/people/b/barbara-plank/>Barbara Plank</a><br><a href=/volumes/2020.semeval-1/ class=text-muted>Proceedings of the Fourteenth Workshop on Semantic Evaluation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--104><div class="card-body p-3 small">This paper describes a system that aims at assessing humour intensity in edited news headlines as part of the 7th task of SemEval-2020 on Humor, <a href=https://en.wikipedia.org/wiki/Emphasis_(typography)>Emphasis</a> and <a href=https://en.wikipedia.org/wiki/Sentimentality>Sentiment</a>. Various factors need to be accounted for in order to assess the <a href=https://en.wikipedia.org/wiki/Funniness>funniness</a> of an edited headline. We propose an architecture that uses hand-crafted features, <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge bases</a> and a <a href=https://en.wikipedia.org/wiki/Language_model>language model</a> to understand <a href=https://en.wikipedia.org/wiki/Humour>humour</a>, and combines them in a <a href=https://en.wikipedia.org/wiki/Regression_analysis>regression model</a>. Our <a href=https://en.wikipedia.org/wiki/System>system</a> outperforms two <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a>. In general, automatic humour assessment remains a difficult task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.238.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--238 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.238 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.238/>Team DiSaster at SemEval-2020 Task 11 : Combining BERT and Hand-crafted Features for Identifying Propaganda Techniques in News<span class=acl-fixed-case>D</span>i<span class=acl-fixed-case>S</span>aster at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 11: Combining <span class=acl-fixed-case>BERT</span> and Hand-crafted Features for Identifying Propaganda Techniques in News</a></strong><br><a href=/people/a/anders-kaas/>Anders Kaas</a>
|
<a href=/people/v/viktor-torp-thomsen/>Viktor Torp Thomsen</a>
|
<a href=/people/b/barbara-plank/>Barbara Plank</a><br><a href=/volumes/2020.semeval-1/ class=text-muted>Proceedings of the Fourteenth Workshop on Semantic Evaluation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--238><div class="card-body p-3 small">The identification of communication techniques in <a href=https://en.wikipedia.org/wiki/Article_(publishing)>news articles</a> such as <a href=https://en.wikipedia.org/wiki/Propaganda>propaganda</a> is important, as such techniques can influence the opinions of large numbers of people. Most work so far focused on the <a href=https://en.wikipedia.org/wiki/Identification_(biology)>identification</a> at the news article level. Recently, a new <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> and shared task has been proposed for the identification of propaganda techniques at the finer-grained span level. This paper describes our system submission to the subtask of technique classification (TC) for the SemEval 2020 shared task on detection of propaganda techniques in news articles. We propose a method of combining neural BERT representations with hand-crafted features via stacked generalization. Our model has the added advantage that it combines the power of contextual representations from BERT with simple span-based and article-based global features. We present an ablation study which shows that even though BERT representations are very powerful also for this task, BERT still benefits from being combined with carefully designed task-specific features.</div></div><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-6100.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-6100/>Proceedings of the 22nd Nordic Conference on Computational Linguistics</a></strong><br><a href=/people/m/mareike-hartmann/>Mareike Hartmann</a>
|
<a href=/people/b/barbara-plank/>Barbara Plank</a><br><a href=/volumes/W19-61/ class=text-muted>Proceedings of the 22nd Nordic Conference on Computational Linguistics</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1350.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1350 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1350 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P19-1350.Supplementary.pdf data-toggle=tooltip data-placement=top title=Supplementary><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/384787273 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1350/>Psycholinguistics Meets Continual Learning : Measuring Catastrophic Forgetting in Visual Question Answering</a></strong><br><a href=/people/c/claudio-greco/>Claudio Greco</a>
|
<a href=/people/b/barbara-plank/>Barbara Plank</a>
|
<a href=/people/r/raquel-fernandez/>Raquel Fernández</a>
|
<a href=/people/r/raffaella-bernardi/>Raffaella Bernardi</a><br><a href=/volumes/P19-1/ class=text-muted>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1350><div class="card-body p-3 small">We study the issue of catastrophic forgetting in the context of neural multimodal approaches to Visual Question Answering (VQA). Motivated by evidence from psycholinguistics, we devise a set of linguistically-informed VQA tasks, which differ by the types of questions involved (Wh-questions and polar questions). We test what impact task difficulty has on continual learning, and whether the order in which a child acquires question types facilitates <a href=https://en.wikipedia.org/wiki/Computational_model>computational models</a>. Our results show that dramatic forgetting is at play and that task difficulty and <a href=https://en.wikipedia.org/wiki/Order_and_disorder>order</a> matter. Two well-known current continual learning methods mitigate the problem only to a limiting degree.</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1061.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1061 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1061 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/305211701 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1061" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1061/>Distant Supervision from Disparate Sources for Low-Resource Part-of-Speech Tagging</a></strong><br><a href=/people/b/barbara-plank/>Barbara Plank</a>
|
<a href=/people/z/zeljko-agic/>Željko Agić</a><br><a href=/volumes/D18-1/ class=text-muted>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1061><div class="card-body p-3 small">a cross-lingual neural part-of-speech tagger that learns from disparate sources of distant supervision, and realistically scales to hundreds of low-resource languages. The model exploits annotation projection, <a href=https://en.wikipedia.org/wiki/Instance_selection>instance selection</a>, tag dictionaries, morphological lexicons, and <a href=https://en.wikipedia.org/wiki/Distributed_representation>distributed representations</a>, all in a uniform framework. The approach is simple, yet surprisingly effective, resulting in a new state of the art without access to any gold annotated data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0523.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0523 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0523 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0523/>Grotoco@SLAM : Second Language Acquisition Modeling with Simple Features, Learners and Task-wise Models<span class=acl-fixed-case>SLAM</span>: Second Language Acquisition Modeling with Simple Features, Learners and Task-wise Models</a></strong><br><a href=/people/s/sigrid-klerke/>Sigrid Klerke</a>
|
<a href=/people/h/hector-martinez-alonso/>Héctor Martínez Alonso</a>
|
<a href=/people/b/barbara-plank/>Barbara Plank</a><br><a href=/volumes/W18-05/ class=text-muted>Proceedings of the Thirteenth Workshop on Innovative Use of NLP for Building Educational Applications</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0523><div class="card-body p-3 small">We present our submission to the 2018 Duolingo Shared Task on Second Language Acquisition Modeling (SLAM). We focus on evaluating a range of <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> for the task, including user-derived measures, while examining how far we can get with a simple <a href=https://en.wikipedia.org/wiki/Linear_classifier>linear classifier</a>. Our analysis reveals that errors differ per exercise format, which motivates our final and best-performing system : a task-wise (per exercise-format) model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-1100.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-1100/>Proceedings of the Second Workshop on Computational Modeling of People’s Opinions, Personality, and Emotions in Social Media</a></strong><br><a href=/people/m/malvina-nissim/>Malvina Nissim</a>
|
<a href=/people/v/viviana-patti/>Viviana Patti</a>
|
<a href=/people/b/barbara-plank/>Barbara Plank</a>
|
<a href=/people/c/claudia-wagner/>Claudia Wagner</a><br><a href=/volumes/W18-11/ class=text-muted>Proceedings of the Second Workshop on Computational Modeling of People’s Opinions, Personality, and Emotions in Social Media</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-1113.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-1113 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-1113 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-1113/>Predicting Authorship and Author Traits from Keystroke Dynamics</a></strong><br><a href=/people/b/barbara-plank/>Barbara Plank</a><br><a href=/volumes/W18-11/ class=text-muted>Proceedings of the Second Workshop on Computational Modeling of People’s Opinions, Personality, and Emotions in Social Media</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-1113><div class="card-body p-3 small">Written text transmits a good deal of <a href=https://en.wikipedia.org/wiki/Nonverbal_communication>nonverbal information</a> related to the author&#8217;s identity and social factors, such as <a href=https://en.wikipedia.org/wiki/Ageing>age</a>, <a href=https://en.wikipedia.org/wiki/Gender>gender</a> and <a href=https://en.wikipedia.org/wiki/Personality>personality</a>. However, it is less known to what extent behavioral biometric traces transmit such <a href=https://en.wikipedia.org/wiki/Information>information</a>. We use typist data to study the predictiveness of <a href=https://en.wikipedia.org/wiki/Author>authorship</a>, and present first experiments on predicting both <a href=https://en.wikipedia.org/wiki/Ageing>age</a> and <a href=https://en.wikipedia.org/wiki/Gender>gender</a> from <a href=https://en.wikipedia.org/wiki/Keystroke_dynamics>keystroke dynamics</a>. Our results show that the model based on keystroke features, while being two orders of magnitude smaller, leads to significantly higher accuracies for authorship than the text-based system. For user attribute prediction, the best approach is to combine the two, suggesting that extralinguistic factors are disclosed to a larger degree in written text, while author identity is better transmitted in typing behavior.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3928.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3928 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3928 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3928/>When Simple n-gram Models Outperform Syntactic Approaches : Discriminating between <a href=https://en.wikipedia.org/wiki/Dutch_language>Dutch</a> and Flemish<span class=acl-fixed-case>D</span>utch and <span class=acl-fixed-case>F</span>lemish</a></strong><br><a href=/people/m/martin-kroon/>Martin Kroon</a>
|
<a href=/people/m/masha-medvedeva/>Masha Medvedeva</a>
|
<a href=/people/b/barbara-plank/>Barbara Plank</a><br><a href=/volumes/W18-39/ class=text-muted>Proceedings of the Fifth Workshop on NLP for Similar Languages, Varieties and Dialects (VarDial 2018)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3928><div class="card-body p-3 small">In this paper we present the results of our participation in the Discriminating between <a href=https://en.wikipedia.org/wiki/Dutch_language>Dutch</a> and <a href=https://en.wikipedia.org/wiki/Flemish>Flemish</a> in Subtitles VarDial 2018 shared task. We try techniques proven to work well for discriminating between language varieties as well as explore the potential of using <a href=https://en.wikipedia.org/wiki/Syntax_(linguistics)>syntactic features</a>, i.e. hierarchical syntactic subtrees. We experiment with different combinations of <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a>. Discriminating between these two languages turned out to be a very hard task, not only for a machine : human performance is only around 0.51 <a href=https://en.wikipedia.org/wiki/F-number>F1 score</a> ; our best system is still a simple Naive Bayes model with <a href=https://en.wikipedia.org/wiki/Unigram>word unigrams</a> and <a href=https://en.wikipedia.org/wiki/Bigram>bigrams</a>. The <a href=https://en.wikipedia.org/wiki/System>system</a> achieved an F1 score (macro) of 0.62, which ranked us 4th in the <a href=https://en.wikipedia.org/wiki/Task_(computing)>shared task</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1096.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1096 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1096 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1096.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285802189 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-1096" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-1096/>Strong Baselines for Neural Semi-Supervised Learning under Domain Shift</a></strong><br><a href=/people/s/sebastian-ruder/>Sebastian Ruder</a>
|
<a href=/people/b/barbara-plank/>Barbara Plank</a><br><a href=/volumes/P18-1/ class=text-muted>Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1096><div class="card-body p-3 small">Novel neural models have been proposed in recent years for learning under domain shift. Most <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>, however, only evaluate on a single task, on proprietary datasets, or compare to weak baselines, which makes comparison of <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> difficult. In this paper, we re-evaluate classic <a href=https://en.wikipedia.org/wiki/Bootstrapping_(statistics)>general-purpose bootstrapping approaches</a> in the context of <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a> under <a href=https://en.wikipedia.org/wiki/Bootstrapping_(statistics)>domain shifts</a> vs. recent neural approaches and propose a novel multi-task tri-training method that reduces the time and space complexity of classic tri-training. Extensive experiments on two benchmarks for <a href=https://en.wikipedia.org/wiki/Part-of-speech_tagging>part-of-speech tagging</a> and <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> are negative : while our novel method establishes a new state-of-the-art for <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a>, it does not fare consistently the best. More importantly, we arrive at the somewhat surprising conclusion that classic tri-training, with some additions, outperforms the <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a> for <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a>. Hence classic <a href=https://en.wikipedia.org/wiki/Methodology>approaches</a> constitute an important and strong baseline.</div></div><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-4024.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-4024 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-4024 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-4024/>All-In-1 at IJCNLP-2017 Task 4 : Short Text Classification with One Model for All Languages<span class=acl-fixed-case>IJCNLP</span>-2017 Task 4: Short Text Classification with One Model for All Languages</a></strong><br><a href=/people/b/barbara-plank/>Barbara Plank</a><br><a href=/volumes/I17-4/ class=text-muted>Proceedings of the IJCNLP 2017, Shared Tasks</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-4024><div class="card-body p-3 small">We present All-In-1, a simple <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> for multilingual text classification that does not require any parallel data. It is based on a traditional Support Vector Machine classifier exploiting multilingual word embeddings and character n-grams. Our model is simple, easily extendable yet very effective, overall ranking 1st (out of 12 teams) in the IJCNLP 2017 shared task on customer feedback analysis in four languages : <a href=https://en.wikipedia.org/wiki/English_language>English</a>, <a href=https://en.wikipedia.org/wiki/French_language>French</a>, <a href=https://en.wikipedia.org/wiki/Japanese_language>Japanese</a> and <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1219.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1219 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1219 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1219/>When Sparse Traditional Models Outperform Dense Neural Networks : the Curious Case of Discriminating between Similar Languages</a></strong><br><a href=/people/m/maria-medvedeva/>Maria Medvedeva</a>
|
<a href=/people/m/martin-kroon/>Martin Kroon</a>
|
<a href=/people/b/barbara-plank/>Barbara Plank</a><br><a href=/volumes/W17-12/ class=text-muted>Proceedings of the Fourth Workshop on NLP for Similar Languages, Varieties and Dialects (VarDial)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1219><div class="card-body p-3 small">We present the results of our participation in the VarDial 4 shared task on discriminating closely related languages. Our submission includes simple traditional models using linear support vector machines (SVMs) and a <a href=https://en.wikipedia.org/wiki/Neural_network>neural network (NN)</a>. The main idea was to leverage language group information. We did so with a two-layer approach in the traditional <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> and a multi-task objective in the <a href=https://en.wikipedia.org/wiki/Neural_network>neural network case</a>. Our results confirm earlier findings : simple traditional models outperform <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a> consistently for this task, at least given the amount of systems we could examine in the available time. Our two-layer linear SVM ranked 2nd in the <a href=https://en.wikipedia.org/wiki/Task_(computing)>shared task</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4404.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4404 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4404 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W17-4404" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W17-4404/>To normalize, or not to normalize : The impact of <a href=https://en.wikipedia.org/wiki/Normalization_(image_processing)>normalization</a> on Part-of-Speech tagging</a></strong><br><a href=/people/r/rob-van-der-goot/>Rob van der Goot</a>
|
<a href=/people/b/barbara-plank/>Barbara Plank</a>
|
<a href=/people/m/malvina-nissim/>Malvina Nissim</a><br><a href=/volumes/W17-44/ class=text-muted>Proceedings of the 3rd Workshop on Noisy User-generated Text</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4404><div class="card-body p-3 small">Does <a href=https://en.wikipedia.org/wiki/Normalization_(statistics)>normalization</a> help Part-of-Speech (POS) tagging accuracy on noisy, non-canonical data? To the best of our knowledge, little is known on the actual impact of <a href=https://en.wikipedia.org/wiki/Normalization_(statistics)>normalization</a> in a real-world scenario, where gold error detection is not available. We investigate the effect of automatic normalization on POS tagging of tweets. We also compare <a href=https://en.wikipedia.org/wiki/Normalization_(statistics)>normalization</a> to strategies that leverage large amounts of unlabeled data kept in its raw form. Our results show that <a href=https://en.wikipedia.org/wiki/Normalization_(statistics)>normalization</a> helps, but does not add consistently beyond just word embedding layer initialization. The latter approach yields a tagging model that is competitive with a Twitter state-of-the-art <a href=https://en.wikipedia.org/wiki/Tag_(metadata)>tagger</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5025.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5025 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5025 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5025/>Neural Networks and Spelling Features for Native Language Identification</a></strong><br><a href=/people/j/johannes-bjerva/>Johannes Bjerva</a>
|
<a href=/people/g/gintare-grigonyte/>Gintarė Grigonytė</a>
|
<a href=/people/r/robert-ostling/>Robert Östling</a>
|
<a href=/people/b/barbara-plank/>Barbara Plank</a><br><a href=/volumes/W17-50/ class=text-muted>Proceedings of the 12th Workshop on Innovative Use of NLP for Building Educational Applications</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5025><div class="card-body p-3 small">We present the RUG-SU team&#8217;s submission at the Native Language Identification Shared Task 2017. We combine several approaches into an ensemble, based on spelling error features, a simple <a href=https://en.wikipedia.org/wiki/Neural_network>neural network</a> using word representations, a deep residual network using word and character features, and a system based on a <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent neural network</a>. Our best <a href=https://en.wikipedia.org/wiki/System>system</a> is an ensemble of <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a>, reaching an F1 score of 0.8323. Although our <a href=https://en.wikipedia.org/wiki/System>system</a> is not the highest ranking one, we do outperform the <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline</a> by far.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5043.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5043 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5043 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5043/>The Power of Character N-grams in Native Language Identification</a></strong><br><a href=/people/a/artur-kulmizev/>Artur Kulmizev</a>
|
<a href=/people/b/bo-blankers/>Bo Blankers</a>
|
<a href=/people/j/johannes-bjerva/>Johannes Bjerva</a>
|
<a href=/people/m/malvina-nissim/>Malvina Nissim</a>
|
<a href=/people/g/gertjan-van-noord/>Gertjan van Noord</a>
|
<a href=/people/b/barbara-plank/>Barbara Plank</a>
|
<a href=/people/m/martijn-wieling/>Martijn Wieling</a><br><a href=/volumes/W17-50/ class=text-muted>Proceedings of the 12th Workshop on Innovative Use of NLP for Building Educational Applications</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5043><div class="card-body p-3 small">In this paper, we explore the performance of a linear SVM trained on language independent character features for the NLI Shared Task 2017. Our basic <a href=https://en.wikipedia.org/wiki/System>system</a> (GRONINGEN) achieves the best performance (87.56 F1-score) on the evaluation set using only 1-9 character n-grams as <a href=https://en.wikipedia.org/wiki/Feature_(computer_vision)>features</a>. We compare this against several ensemble and meta-classifiers in order to examine how the <a href=https://en.wikipedia.org/wiki/Linear_system>linear system</a> fares when combined with other, especially non-linear classifiers. Special emphasis is placed on the topic bias that exists by virtue of the assessment essay prompt distribution.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1038.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1038 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1038 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D17-1038" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D17-1038/>Learning to select data for <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a> with Bayesian Optimization<span class=acl-fixed-case>B</span>ayesian Optimization</a></strong><br><a href=/people/s/sebastian-ruder/>Sebastian Ruder</a>
|
<a href=/people/b/barbara-plank/>Barbara Plank</a><br><a href=/volumes/D17-1/ class=text-muted>Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1038><div class="card-body p-3 small">Domain similarity measures can be used to gauge adaptability and select suitable <a href=https://en.wikipedia.org/wiki/Data>data</a> for <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a>, but existing approaches define ad hoc measures that are deemed suitable for respective tasks. Inspired by work on curriculum learning, we propose to learn data selection measures using <a href=https://en.wikipedia.org/wiki/Bayesian_optimization>Bayesian Optimization</a> and evaluate them across models, domains and tasks. Our learned measures outperform existing domain similarity measures significantly on three tasks : <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a>, <a href=https://en.wikipedia.org/wiki/Part-of-speech_tagging>part-of-speech tagging</a>, and <a href=https://en.wikipedia.org/wiki/Parsing>parsing</a>. We show the importance of complementing <a href=https://en.wikipedia.org/wiki/Similarity_measure>similarity</a> with diversity, and that learned measures areto some degreetransferable across models, domains, and even tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1005.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1005 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1005 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1005/>When is <a href=https://en.wikipedia.org/wiki/Multitask_learning>multitask learning</a> effective? Semantic sequence prediction under varying data conditions</a></strong><br><a href=/people/h/hector-martinez-alonso/>Héctor Martínez Alonso</a>
|
<a href=/people/b/barbara-plank/>Barbara Plank</a><br><a href=/volumes/E17-1/ class=text-muted>Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1005><div class="card-body p-3 small">Multitask learning has been applied successfully to a range of <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a>, mostly <a href=https://en.wikipedia.org/wiki/Morphology_(biology)>morphosyntactic</a>. However, little is known on when <a href=https://en.wikipedia.org/wiki/Machine_to_machine>MTL</a> works and whether there are data characteristics that help to determine the success of <a href=https://en.wikipedia.org/wiki/Machine_to_machine>MTL</a>. In this paper we evaluate a range of semantic sequence labeling tasks in a MTL setup. We examine different auxiliary task configurations, amongst which a novel setup, and correlate their impact to data-dependent conditions. Our results show that MTL is not always effective, because significant improvements are obtained only for 1 out of 5 <a href=https://en.wikipedia.org/wiki/Task_(computing)>tasks</a>. When successful, <a href=https://en.wikipedia.org/wiki/Numerical_methods_for_ordinary_differential_equations>auxiliary tasks</a> with compact and more uniform label distributions are preferable.<i>when</i> MTL works and whether there are data characteristics that help to determine the success of MTL. In this paper we evaluate a range of semantic sequence labeling tasks in a MTL setup. We examine different auxiliary task configurations, amongst which a novel setup, and correlate their impact to data-dependent conditions. Our results show that MTL is not always effective, because significant improvements are obtained only for 1 out of 5 tasks. When successful, auxiliary tasks with compact and more uniform label distributions are preferable.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1022.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1022 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1022 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=E17-1022" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/E17-1022/>Parsing Universal Dependencies without training<span class=acl-fixed-case>U</span>niversal <span class=acl-fixed-case>D</span>ependencies without training</a></strong><br><a href=/people/h/hector-martinez-alonso/>Héctor Martínez Alonso</a>
|
<a href=/people/z/zeljko-agic/>Željko Agić</a>
|
<a href=/people/b/barbara-plank/>Barbara Plank</a>
|
<a href=/people/a/anders-sogaard/>Anders Søgaard</a><br><a href=/volumes/E17-1/ class=text-muted>Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1022><div class="card-body p-3 small">We present UDP, the first training-free parser for Universal Dependencies (UD). Our <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> is based on <a href=https://en.wikipedia.org/wiki/PageRank>PageRank</a> and a small set of specific dependency head rules. UDP features two-step decoding to guarantee that <a href=https://en.wikipedia.org/wiki/Function_word>function words</a> are attached as <a href=https://en.wikipedia.org/wiki/Tree_(data_structure)>leaf nodes</a>. The <a href=https://en.wikipedia.org/wiki/Parsing>parser</a> requires no training, and <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> is competitive with a delexicalized transfer system. UDP offers a linguistically sound unsupervised alternative to cross-lingual parsing for UD. The <a href=https://en.wikipedia.org/wiki/Parsing>parser</a> has very few parameters and distinctly robust to domain change across languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2040.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2040 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2040 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2040/>Cross-lingual tagger evaluation without test data</a></strong><br><a href=/people/z/zeljko-agic/>Željko Agić</a>
|
<a href=/people/b/barbara-plank/>Barbara Plank</a>
|
<a href=/people/a/anders-sogaard/>Anders Søgaard</a><br><a href=/volumes/E17-2/ class=text-muted>Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2040><div class="card-body p-3 small">We address the challenge of cross-lingual POS tagger evaluation in absence of manually annotated test data. We put forth and evaluate two dictionary-based metrics. On the tasks of accuracy prediction and system ranking, we reveal that these metrics are reliable enough to approximate test set-based evaluation, and at the same time lean enough to support assessment for truly low-resource languages.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Barbara+Plank" title="Search for 'Barbara Plank' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/r/rob-van-der-goot/ class=align-middle>Rob van der Goot</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/people/m/malvina-nissim/ class=align-middle>Malvina Nissim</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/people/a/alan-ramponi/ class=align-middle>Alan Ramponi</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/z/zeljko-agic/ class=align-middle>Željko Agić</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/h/hector-martinez-alonso/ class=align-middle>Héctor Martínez Alonso</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/k/kristian-norgaard-jensen/ class=align-middle>Kristian Nørgaard Jensen</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/a/ahmet-ustun/ class=align-middle>Ahmet Üstün</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/i/ibrahim-sharaf/ class=align-middle>Ibrahim Sharaf</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/m/martin-kroon/ class=align-middle>Martin Kroon</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/j/johannes-bjerva/ class=align-middle>Johannes Bjerva</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/v/viviana-patti/ class=align-middle>Viviana Patti</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/s/sebastian-ruder/ class=align-middle>Sebastian Ruder</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/a/anders-sogaard/ class=align-middle>Anders Søgaard</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/m/mike-zhang/ class=align-middle>Mike Zhang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/rosario-lombardo/ class=align-middle>Rosario Lombardo</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/maria-medvedeva/ class=align-middle>Maria Medvedeva</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/g/gintare-grigonyte/ class=align-middle>Gintarė Grigonytė</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/robert-ostling/ class=align-middle>Robert Östling</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/artur-kulmizev/ class=align-middle>Artur Kulmizev</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/b/bo-blankers/ class=align-middle>Bo Blankers</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/g/gertjan-van-noord/ class=align-middle>Gertjan van Noord</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/martijn-wieling/ class=align-middle>Martijn Wieling</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/e/esin-durmus/ class=align-middle>Esin Durmus</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/n/nicolaj-filrup-rasmussen/ class=align-middle>Nicolaj Filrup Rasmussen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/thai-wang/ class=align-middle>Thai Wang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/marco-placenti/ class=align-middle>Marco Placenti</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/anders-kaas/ class=align-middle>Anders Kaas</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/v/viktor-torp-thomsen/ class=align-middle>Viktor Torp Thomsen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/aizhan-imankulova/ class=align-middle>Aizhan Imankulova</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/marija-stepanovic/ class=align-middle>Marija Stepanović</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/siti-oryza-khairunnisa/ class=align-middle>Siti Oryza Khairunnisa</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/mamoru-komachi/ class=align-middle>Mamoru Komachi</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/tommaso-fornaciari/ class=align-middle>Tommaso Fornaciari</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/alexandra-uma/ class=align-middle>Alexandra Uma</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/silviu-paun/ class=align-middle>Silviu Paun</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/dirk-hovy/ class=align-middle>Dirk Hovy</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/massimo-poesio/ class=align-middle>Massimo Poesio</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/sigrid-klerke/ class=align-middle>Sigrid Klerke</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/claudia-wagner/ class=align-middle>Claudia Wagner</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/masha-medvedeva/ class=align-middle>Masha Medvedeva</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/mareike-hartmann/ class=align-middle>Mareike Hartmann</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/e/eyal-ben-david/ class=align-middle>Eyal Ben-David</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/shay-b-cohen/ class=align-middle>Shay B. Cohen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/ryan-mcdonald/ class=align-middle>Ryan McDonald</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/roi-reichart/ class=align-middle>Roi Reichart</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/g/guy-rotman/ class=align-middle>Guy Rotman</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yftah-ziser/ class=align-middle>Yftah Ziser</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/claudio-greco/ class=align-middle>Claudio Greco</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/raquel-fernandez/ class=align-middle>Raquel Fernández</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/raffaella-bernardi/ class=align-middle>Raffaella Bernardi</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/ws/ class=align-middle>WS</a><span class="badge badge-secondary align-middle ml-2">9</span></li><li class=list-group-item><a href=/venues/eacl/ class=align-middle>EACL</a><span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/venues/emnlp/ class=align-middle>EMNLP</a><span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/venues/semeval/ class=align-middle>SemEval</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/naacl/ class=align-middle>NAACL</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-venues aria-expanded=false aria-controls=more-venues>show all...</li><div class="collapse border-top" id=more-venues><li class=list-group-item><a href=/venues/acl/ class=align-middle>ACL</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/nodalida/ class=align-middle>NoDaLiDa</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/ijcnlp/ class=align-middle>IJCNLP</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/peoples/ class=align-middle>PEOPLES</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/adaptnlp/ class=align-middle>AdaptNLP</a><span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>