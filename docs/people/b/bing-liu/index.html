<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Bing Liu - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Bing</span> <span class=font-weight-bold>Liu</span></h2><hr><div class=row><div class=col-lg-9><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.sigdial-1.31.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--sigdial-1--31 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.sigdial-1.31 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href="https://www.youtube.com/watch?v=0FxAJvs93WA" data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2021.sigdial-1.31/>Summarizing Behavioral Change Goals from SMS Exchanges to Support Health Coaches<span class=acl-fixed-case>SMS</span> Exchanges to Support Health Coaches</a></strong><br><a href=/people/i/itika-gupta/>Itika Gupta</a>
|
<a href=/people/b/barbara-di-eugenio/>Barbara Di Eugenio</a>
|
<a href=/people/b/brian-d-ziebart/>Brian D. Ziebart</a>
|
<a href=/people/b/bing-liu/>Bing Liu</a>
|
<a href=/people/b/ben-s-gerber/>Ben S. Gerber</a>
|
<a href=/people/l/lisa-k-sharp/>Lisa K. Sharp</a><br><a href=/volumes/2021.sigdial-1/ class=text-muted>Proceedings of the 22nd Annual Meeting of the Special Interest Group on Discourse and Dialogue</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--sigdial-1--31><div class="card-body p-3 small">Regular physical activity is associated with a reduced risk of chronic diseases such as type 2 diabetes and improved <a href=https://en.wikipedia.org/wiki/Mental_health>mental well-being</a>. Yet, more than half of the US population is insufficiently active. Health coaching has been successful in promoting healthy behaviors. In this paper, we present our work towards assisting <a href=https://en.wikipedia.org/wiki/Coaching>health coaches</a> by extracting the physical activity goal the user and coach negotiate via <a href=https://en.wikipedia.org/wiki/Text_messaging>text messages</a>. We show that information captured by dialogue acts can help to improve the goal extraction results. We employ both traditional and transformer-based machine learning models for dialogue acts prediction and find them statistically indistinguishable in performance on our health coaching dataset. Moreover, we discuss the feedback provided by the health coaches when evaluating the correctness of the extracted goal summaries. This work is a step towards building a virtual assistant health coach to promote a healthy lifestyle.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.nlp4convai-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.nlp4convai-1.0/>Proceedings of the 3rd Workshop on Natural Language Processing for Conversational AI</a></strong><br><a href=/people/a/alexandros-papangelis/>Alexandros Papangelis</a>
|
<a href=/people/p/pawel-budzianowski/>Pawe≈Ç Budzianowski</a>
|
<a href=/people/b/bing-liu/>Bing Liu</a>
|
<a href=/people/e/elnaz-nouri/>Elnaz Nouri</a>
|
<a href=/people/a/abhinav-rastogi/>Abhinav Rastogi</a>
|
<a href=/people/y/yun-nung-chen/>Yun-Nung Chen</a><br><a href=/volumes/2021.nlp4convai-1/ class=text-muted>Proceedings of the 3rd Workshop on Natural Language Processing for Conversational AI</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.66.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--66 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.66 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.66" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.66/>Semantic Novelty Detection in Natural Language Descriptions</a></strong><br><a href=/people/n/nianzu-ma/>Nianzu Ma</a>
|
<a href=/people/a/alexander-politowicz/>Alexander Politowicz</a>
|
<a href=/people/s/sahisnu-mazumder/>Sahisnu Mazumder</a>
|
<a href=/people/j/jiahua-chen/>Jiahua Chen</a>
|
<a href=/people/b/bing-liu/>Bing Liu</a>
|
<a href=/people/e/eric-robertson/>Eric Robertson</a>
|
<a href=/people/s/scott-grigsby/>Scott Grigsby</a><br><a href=/volumes/2021.emnlp-main/ class=text-muted>Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--66><div class="card-body p-3 small">This paper proposes to study a fine-grained semantic novelty detection task, which can be illustrated with the following example. It is normal that a person walks a dog in the park, but if someone says A man is walking a chicken in the park, it is novel. Given a set of natural language descriptions of normal scenes, we want to identify descriptions of novel scenes. We are not aware of any existing work that solves the <a href=https://en.wikipedia.org/wiki/Problem_solving>problem</a>. Although existing novelty or anomaly detection algorithms are applicable, since they are usually topic-based, they perform poorly on our fine-grained semantic novelty detection task. This paper proposes an effective <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> (called GAT-MA) to solve the <a href=https://en.wikipedia.org/wiki/Problem_solving>problem</a> and also contributes a new <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>. Experimental evaluation shows that GAT-MA outperforms 11 <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a> by large margins.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.622.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--622 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.622 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.622" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.622/>Zero-Shot Dialogue State Tracking via Cross-Task Transfer</a></strong><br><a href=/people/z/zhaojiang-lin/>Zhaojiang Lin</a>
|
<a href=/people/b/bing-liu/>Bing Liu</a>
|
<a href=/people/a/andrea-madotto/>Andrea Madotto</a>
|
<a href=/people/s/seungwhan-moon/>Seungwhan Moon</a>
|
<a href=/people/z/zhenpeng-zhou/>Zhenpeng Zhou</a>
|
<a href=/people/p/paul-a-crook/>Paul Crook</a>
|
<a href=/people/z/zhiguang-wang/>Zhiguang Wang</a>
|
<a href=/people/z/zhou-yu/>Zhou Yu</a>
|
<a href=/people/e/eunjoon-cho/>Eunjoon Cho</a>
|
<a href=/people/r/rajen-subba/>Rajen Subba</a>
|
<a href=/people/p/pascale-fung/>Pascale Fung</a><br><a href=/volumes/2021.emnlp-main/ class=text-muted>Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--622><div class="card-body p-3 small">Zero-shot transfer learning for dialogue state tracking (DST) enables us to handle a variety of task-oriented dialogue domains without the expense of collecting in-domain data. In this work, we propose to transfer the cross-task knowledge from general question answering (QA) corpora for the zero-shot DST task. Specifically, we propose TransferQA, a transferable generative QA model that seamlessly combines extractive QA and multi-choice QA via a text-to-text transformer framework, and tracks both categorical slots and non-categorical slots in DST. In addition, we introduce two effective ways to construct unanswerable questions, namely, negative question sampling and context truncation, which enable our model to handle none value slots in the zero-shot DST setting. The extensive experiments show that our approaches substantially improve the existing zero-shot and few-shot results on MultiWoz. Moreover, compared to the fully trained <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline</a> on the Schema-Guided Dialogue dataset, our approach shows better generalization ability in unseen domains.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.124.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--124 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.124 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.124/>Adding Chit-Chat to Enhance Task-Oriented Dialogues</a></strong><br><a href=/people/k/kai-sun/>Kai Sun</a>
|
<a href=/people/s/seungwhan-moon/>Seungwhan Moon</a>
|
<a href=/people/p/paul-a-crook/>Paul Crook</a>
|
<a href=/people/s/stephen-roller/>Stephen Roller</a>
|
<a href=/people/b/becka-silvert/>Becka Silvert</a>
|
<a href=/people/b/bing-liu/>Bing Liu</a>
|
<a href=/people/z/zhiguang-wang/>Zhiguang Wang</a>
|
<a href=/people/h/honglei-liu/>Honglei Liu</a>
|
<a href=/people/e/eunjoon-cho/>Eunjoon Cho</a>
|
<a href=/people/c/claire-cardie/>Claire Cardie</a><br><a href=/volumes/2021.naacl-main/ class=text-muted>Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--124><div class="card-body p-3 small">Existing dialogue corpora and <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> are typically designed under two disjoint motives : while task-oriented systems focus on achieving functional goals (e.g., booking hotels), open-domain chatbots aim at making socially engaging conversations. In this work, we propose to integrate both types of systems by Adding <a href=https://en.wikipedia.org/wiki/Chit-chat>Chit-Chat</a> to ENhance Task-ORiented dialogues (ACCENTOR), with the goal of making virtual assistant conversations more engaging and interactive. Specifically, we propose a Human-AI collaborative data collection approach for generating diverse chit-chat responses to augment task-oriented dialogues with minimal annotation effort. We then present our new chit-chat-based annotations to 23.8 K dialogues from two popular <a href=https://en.wikipedia.org/wiki/Task_(computing)>task-oriented datasets</a> (Schema-Guided Dialogue and MultiWOZ 2.1) and demonstrate their advantage over the originals via human evaluation. Lastly, we propose three new models for adding <a href=https://en.wikipedia.org/wiki/Chit-chat>chit-chat</a> to task-oriented dialogues, explicitly trained to predict user goals and to generate contextually relevant chit-chat responses. Automatic and human evaluations show that, compared with the state-of-the-art task-oriented baseline, our models can code-switch between task and chit-chat to be more engaging, interesting, knowledgeable, and humanlike, while maintaining competitive task performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.378.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--378 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.378 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.naacl-main.378" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.378/>Adapting <a href=https://en.wikipedia.org/wiki/BERT>BERT</a> for Continual Learning of a Sequence of Aspect Sentiment Classification Tasks<span class=acl-fixed-case>BERT</span> for Continual Learning of a Sequence of Aspect Sentiment Classification Tasks</a></strong><br><a href=/people/z/zixuan-ke/>Zixuan Ke</a>
|
<a href=/people/h/hu-xu/>Hu Xu</a>
|
<a href=/people/b/bing-liu/>Bing Liu</a><br><a href=/volumes/2021.naacl-main/ class=text-muted>Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--378><div class="card-body p-3 small">This paper studies continual learning (CL) of a sequence of aspect sentiment classification (ASC) tasks. Although some CL techniques have been proposed for document sentiment classification, we are not aware of any CL work on ASC. A CL system that incrementally learns a sequence of ASC tasks should address the following two issues : (1) transfer knowledge learned from previous tasks to the new task to help it learn a better model, and (2) maintain the performance of the models for previous tasks so that they are not forgotten. This paper proposes a novel capsule network based model called B-CL to address these issues. B-CL markedly improves the ASC performance on both the new task and the old tasks via forward and backward knowledge transfer. The effectiveness of B-CL is demonstrated through extensive experiments.</div></div><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.101.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--101 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.101 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.101/>Using the Past Knowledge to Improve Sentiment Classification</a></strong><br><a href=/people/q/qi-qin/>Qi Qin</a>
|
<a href=/people/w/wenpeng-hu/>Wenpeng Hu</a>
|
<a href=/people/b/bing-liu/>Bing Liu</a><br><a href=/volumes/2020.findings-emnlp/ class=text-muted>Findings of the Association for Computational Linguistics: EMNLP 2020</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--101><div class="card-body p-3 small">This paper studies sentiment classification in the lifelong learning setting that incrementally learns a sequence of sentiment classification tasks. It proposes a new lifelong learning model (called L2PG) that can retain and selectively transfer the knowledge learned in the past to help learn the new task. A key innovation of this proposed model is a novel parameter-gate (p-gate) mechanism that regulates the flow or transfer of the previously learned knowledge to the new task. Specifically, it can selectively use the network parameters (which represent the retained knowledge gained from the previous tasks) to assist the learning of the new task t. Knowledge distillation is also employed in the process to preserve the past knowledge by approximating the network output at the state when task t-1 was learned. Experimental results show that L2PG outperforms strong <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a>, including even multiple task learning.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.21.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--21 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.21 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.coling-main.21" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.21/>Understanding Pre-trained BERT for Aspect-based Sentiment Analysis<span class=acl-fixed-case>BERT</span> for Aspect-based Sentiment Analysis</a></strong><br><a href=/people/h/hu-xu/>Hu Xu</a>
|
<a href=/people/l/lei-shu/>Lei Shu</a>
|
<a href=/people/p/philip-s-yu/>Philip Yu</a>
|
<a href=/people/b/bing-liu/>Bing Liu</a><br><a href=/volumes/2020.coling-main/ class=text-muted>Proceedings of the 28th International Conference on Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--21><div class="card-body p-3 small">This paper analyzes the pre-trained hidden representations learned from reviews on BERT for tasks in aspect-based sentiment analysis (ABSA). Our work is motivated by the recent progress in BERT-based language models for ABSA. However, it is not clear how the general proxy task of (masked) language model trained on unlabeled corpus without annotations of aspects or opinions can provide important features for downstream tasks in ABSA. By leveraging the annotated datasets in ABSA, we investigate both the attentions and the learned representations of BERT pre-trained on reviews. We found that BERT uses very few self-attention heads to encode context words (such as prepositions or pronouns that indicating an aspect) and opinion words for an aspect. Most features in the representation of an aspect are dedicated to the fine-grained semantics of the domain (or product category) and the aspect itself, instead of carrying summarized opinions from its context. We hope this investigation can help future research in improving self-supervised learning, <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised learning</a> and <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a> for ABSA. The pre-trained model and code can be found at https://github.com/howardhsu/BERT-for-RRC-ABSA.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.363.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--363 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.363 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.363/>Translation vs. Dialogue : A Comparative Analysis of Sequence-to-Sequence Modeling</a></strong><br><a href=/people/w/wenpeng-hu/>Wenpeng Hu</a>
|
<a href=/people/r/ran-le/>Ran Le</a>
|
<a href=/people/b/bing-liu/>Bing Liu</a>
|
<a href=/people/j/jinwen-ma/>Jinwen Ma</a>
|
<a href=/people/d/dongyan-zhao/>Dongyan Zhao</a>
|
<a href=/people/r/rui-yan/>Rui Yan</a><br><a href=/volumes/2020.coling-main/ class=text-muted>Proceedings of the 28th International Conference on Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--363><div class="card-body p-3 small">Understanding neural models is a major topic of interest in the deep learning community. In this paper, we propose to interpret a general neural model comparatively. Specifically, we study the sequence-to-sequence (Seq2Seq) model in the contexts of two mainstream NLP tasksmachine translation and dialogue response generationas they both use the seq2seq model. We investigate how the two <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> are different and how their task difference results in major differences in the behaviors of the resulting translation and dialogue generation systems. This study allows us to make several interesting observations and gain valuable insights, which can be used to help develop better translation and dialogue generation models. To our knowledge, no such comparative study has been done so far.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.463.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--463 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.463 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.463/>User Memory Reasoning for Conversational Recommendation</a></strong><br><a href=/people/h/hu-xu/>Hu Xu</a>
|
<a href=/people/s/seungwhan-moon/>Seungwhan Moon</a>
|
<a href=/people/h/honglei-liu/>Honglei Liu</a>
|
<a href=/people/b/bing-liu/>Bing Liu</a>
|
<a href=/people/p/pararth-shah/>Pararth Shah</a>
|
<a href=/people/b/bing-liu/>Bing Liu</a>
|
<a href=/people/p/philip-s-yu/>Philip Yu</a><br><a href=/volumes/2020.coling-main/ class=text-muted>Proceedings of the 28th International Conference on Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--463><div class="card-body p-3 small">We study an end-to-end approach for conversational recommendation that dynamically manages and reasons over users&#8217; past (offline) preferences and current (online) requests through a structured and cumulative user memory knowledge graph. This formulation extends existing state tracking beyond the boundary of a single dialog to user state tracking (UST). For this study, we create a new Memory Graph (MG)-Conversational Recommendation parallel corpus called MGConvRex with 7K+ human-to-human role-playing dialogs, grounded on a large-scale user memory bootstrapped from real-world user scenarios. MGConvRex captures human-level reasoning over user memory and has disjoint training / testing sets of users for zero-shot (cold-start) reasoning for recommendation. We propose a simple yet expandable formulation for constructing and updating the MG, and an end-to-end graph-based reasoning model that updates MG from unstructured utterances and predicts optimal dialog policies (eg recommendation) based on updated MG. The prediction of our proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> inherits the <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph structure</a>, providing a natural way to explain policies. Experiments are conducted for both <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>offline metrics</a> and <a href=https://en.wikipedia.org/wiki/Online_simulation>online simulation</a>, showing competitive results.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.463.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--463 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.463 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.463/>User Memory Reasoning for Conversational Recommendation</a></strong><br><a href=/people/h/hu-xu/>Hu Xu</a>
|
<a href=/people/s/seungwhan-moon/>Seungwhan Moon</a>
|
<a href=/people/h/honglei-liu/>Honglei Liu</a>
|
<a href=/people/b/bing-liu/>Bing Liu</a>
|
<a href=/people/p/pararth-shah/>Pararth Shah</a>
|
<a href=/people/b/bing-liu/>Bing Liu</a>
|
<a href=/people/p/philip-s-yu/>Philip Yu</a><br><a href=/volumes/2020.coling-main/ class=text-muted>Proceedings of the 28th International Conference on Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--463><div class="card-body p-3 small">We study an end-to-end approach for conversational recommendation that dynamically manages and reasons over users&#8217; past (offline) preferences and current (online) requests through a structured and cumulative user memory knowledge graph. This formulation extends existing state tracking beyond the boundary of a single dialog to user state tracking (UST). For this study, we create a new Memory Graph (MG)-Conversational Recommendation parallel corpus called MGConvRex with 7K+ human-to-human role-playing dialogs, grounded on a large-scale user memory bootstrapped from real-world user scenarios. MGConvRex captures human-level reasoning over user memory and has disjoint training / testing sets of users for zero-shot (cold-start) reasoning for recommendation. We propose a simple yet expandable formulation for constructing and updating the MG, and an end-to-end graph-based reasoning model that updates MG from unstructured utterances and predicts optimal dialog policies (eg recommendation) based on updated MG. The prediction of our proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> inherits the <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph structure</a>, providing a natural way to explain policies. Experiments are conducted for both <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>offline metrics</a> and <a href=https://en.wikipedia.org/wiki/Online_simulation>online simulation</a>, showing competitive results.</div></div><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1130.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1130 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1130 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1130/>Modeling Multi-Action Policy for Task-Oriented Dialogues</a></strong><br><a href=/people/l/lei-shu/>Lei Shu</a>
|
<a href=/people/h/hu-xu/>Hu Xu</a>
|
<a href=/people/b/bing-liu/>Bing Liu</a>
|
<a href=/people/p/piero-molino/>Piero Molino</a><br><a href=/volumes/D19-1/ class=text-muted>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1130><div class="card-body p-3 small">Dialogue management (DM) plays a key role in the quality of the interaction with the user in a task-oriented dialogue system. In most existing approaches, the agent predicts only one DM policy action per turn. This significantly limits the expressive power of the <a href=https://en.wikipedia.org/wiki/Intelligent_agent>conversational agent</a> and introduces unwanted turns of interactions that may challenge users&#8217; patience. Longer conversations also lead to more errors and the <a href=https://en.wikipedia.org/wiki/System>system</a> needs to be more robust to handle them. In this paper, we compare the performance of several <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> on the task of predicting multiple acts for each turn. A novel policy model is proposed based on a recurrent cell called gated Continue-Act-Slots (gCAS) that overcomes the limitations of the existing models. Experimental results show that <a href=https://en.wikipedia.org/wiki/GCAS>gCAS</a> outperforms other approaches. The datasets and code are available at https://leishu02.github.io/.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1655.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1655 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1655 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1655/>Learning with Noisy Labels for Sentence-level Sentiment Classification</a></strong><br><a href=/people/h/hao-wang/>Hao Wang</a>
|
<a href=/people/b/bing-liu/>Bing Liu</a>
|
<a href=/people/c/chaozhuo-li/>Chaozhuo Li</a>
|
<a href=/people/y/yan-yang/>Yan Yang</a>
|
<a href=/people/t/tianrui-li/>Tianrui Li</a><br><a href=/volumes/D19-1/ class=text-muted>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1655><div class="card-body p-3 small">Deep neural networks (DNNs) can fit (or even over-fit) the training data very well. If a DNN model is trained using data with noisy labels and tested on data with clean labels, the <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> may perform poorly. This paper studies the problem of learning with noisy labels for sentence-level sentiment classification. We propose a novel DNN model called NetAb (as shorthand for <a href=https://en.wikipedia.org/wiki/Convolutional_neural_network>convolutional neural Networks</a> with Ab-networks) to handle noisy labels during training. NetAb consists of two <a href=https://en.wikipedia.org/wiki/Convolutional_neural_network>convolutional neural networks</a>, one with a noise transition layer for dealing with the input noisy labels and the other for predicting &#8216;clean&#8217; labels. We train the two <a href=https://en.wikipedia.org/wiki/Neural_network>networks</a> using their respective <a href=https://en.wikipedia.org/wiki/Loss_function>loss functions</a> in a mutual reinforcement manner. Experimental results demonstrate the effectiveness of the proposed <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5922.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5922 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5922 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-5922" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-5922/>Flexibly-Structured Model for Task-Oriented Dialogues</a></strong><br><a href=/people/l/lei-shu/>Lei Shu</a>
|
<a href=/people/p/piero-molino/>Piero Molino</a>
|
<a href=/people/m/mahdi-namazifar/>Mahdi Namazifar</a>
|
<a href=/people/h/hu-xu/>Hu Xu</a>
|
<a href=/people/b/bing-liu/>Bing Liu</a>
|
<a href=/people/h/huaixiu-zheng/>Huaixiu Zheng</a>
|
<a href=/people/g/gokhan-tur/>Gokhan Tur</a><br><a href=/volumes/W19-59/ class=text-muted>Proceedings of the 20th Annual SIGdial Meeting on Discourse and Dialogue</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5922><div class="card-body p-3 small">This paper proposes a novel end-to-end architecture for task-oriented dialogue systems. It is based on a simple and practical yet very effective sequence-to-sequence approach, where <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>language understanding</a> and state tracking tasks are modeled jointly with a structured copy-augmented sequential decoder and a multi-label decoder for each slot. The policy engine and language generation tasks are modeled jointly following that. The copy-augmented sequential decoder deals with new or unknown values in the conversation, while the multi-label decoder combined with the <a href=https://en.wikipedia.org/wiki/Sequential_decoder>sequential decoder</a> ensures the explicit assignment of values to slots. On the generation part, slot binary classifiers are used to improve performance. This architecture is scalable to real-world scenarios and is shown through an empirical evaluation to achieve state-of-the-art performance on both the Cambridge Restaurant dataset and the Stanford in-car assistant dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1242.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1242 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1242 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-1242/>BERT Post-Training for Review Reading Comprehension and Aspect-based Sentiment Analysis<span class=acl-fixed-case>BERT</span> Post-Training for Review Reading Comprehension and Aspect-based Sentiment Analysis</a></strong><br><a href=/people/h/hu-xu/>Hu Xu</a>
|
<a href=/people/b/bing-liu/>Bing Liu</a>
|
<a href=/people/l/lei-shu/>Lei Shu</a>
|
<a href=/people/p/philip-s-yu/>Philip Yu</a><br><a href=/volumes/N19-1/ class=text-muted>Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1242><div class="card-body p-3 small">Question-answering plays an important role in <a href=https://en.wikipedia.org/wiki/E-commerce>e-commerce</a> as it allows potential customers to actively seek crucial information about products or services to help their purchase decision making. Inspired by the recent success of machine reading comprehension (MRC) on formal documents, this paper explores the potential of turning customer reviews into a large source of knowledge that can be exploited to answer user questions. We call this problem Review Reading Comprehension (RRC). To the best of our knowledge, no existing work has been done on RRC. In this work, we first build an RRC dataset called ReviewRC based on a popular <a href=https://en.wikipedia.org/wiki/Benchmarking>benchmark</a> for aspect-based sentiment analysis. Since ReviewRC has limited training examples for RRC (and also for aspect-based sentiment analysis), we then explore a novel post-training approach on the popular language model BERT to enhance the performance of fine-tuning of BERT for RRC. To show the generality of the approach, the proposed post-training is also applied to some other review-based tasks such as <a href=https://en.wikipedia.org/wiki/Aspect_extraction>aspect extraction</a> and aspect sentiment classification in aspect-based sentiment analysis. Experimental results demonstrate that the proposed post-training is highly effective.</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1160.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1160 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1160 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1160/>An Attribute Enhanced Domain Adaptive Model for Cold-Start Spam Review Detection</a></strong><br><a href=/people/z/zhenni-you/>Zhenni You</a>
|
<a href=/people/t/tieyun-qian/>Tieyun Qian</a>
|
<a href=/people/b/bing-liu/>Bing Liu</a><br><a href=/volumes/C18-1/ class=text-muted>Proceedings of the 27th International Conference on Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1160><div class="card-body p-3 small">Spam detection has long been a research topic in both academic and industry due to its wide applications. Previous studies are mainly focused on extracting linguistic or behavior features to distinguish the <a href=https://en.wikipedia.org/wiki/Spamming>spam and legitimate reviews</a>. Such <a href=https://en.wikipedia.org/wiki/Software_feature>features</a> are either ineffective or take long time to collect and thus are hard to be applied to cold-start spam review detection tasks. Recent advance leveraged the <a href=https://en.wikipedia.org/wiki/Neural_network>neural network</a> to encode the textual and behavior features for the cold-start problem. However, the abundant attribute information are largely neglected by the existing <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a>. In this paper, we propose a novel deep learning architecture for incorporating entities and their inherent attributes from various domains into a unified framework. Specifically, our model not only encodes the entities of reviewer, item, and review, but also their attributes such as location, date, price ranges. Furthermore, we present a domain classifier to adapt the knowledge from one domain to the other. With the abundant attributes in existing entities and knowledge in other domains, we successfully solve the problem of data scarcity in the cold-start settings. Experimental results on two Yelp datasets prove that our proposed <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> significantly outperforms the state-of-the-art methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5041.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5041 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5041 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5041/>Adversarial Learning of Task-Oriented Neural Dialog Models</a></strong><br><a href=/people/b/bing-liu/>Bing Liu</a>
|
<a href=/people/i/ian-lane/>Ian Lane</a><br><a href=/volumes/W18-50/ class=text-muted>Proceedings of the 19th Annual SIGdial Meeting on Discourse and Dialogue</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5041><div class="card-body p-3 small">In this work, we propose an adversarial learning method for reward estimation in reinforcement learning (RL) based task-oriented dialog models. Most of the current RL based task-oriented dialog systems require the access to a reward signal from either user feedback or user ratings. Such user ratings, however, may not always be consistent or available in practice. Furthermore, online dialog policy learning with RL typically requires a large number of queries to users, suffering from sample efficiency problem. To address these challenges, we propose an adversarial learning method to learn dialog rewards directly from dialog samples. Such rewards are further used to optimize the dialog policy with policy gradient based RL. In the evaluation in a restaurant search domain, we show that the proposed adversarial dialog learning method achieves advanced dialog success rate comparing to strong baseline methods. We further discuss the covariate shift problem in online adversarial dialog learning and show how we can address that with partial access to user feedback.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1187.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1187 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1187 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N18-1187" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N18-1187/>Dialogue Learning with Human Teaching and Feedback in End-to-End Trainable Task-Oriented Dialogue Systems</a></strong><br><a href=/people/b/bing-liu/>Bing Liu</a>
|
<a href=/people/g/gokhan-tur/>Gokhan T√ºr</a>
|
<a href=/people/d/dilek-hakkani-tur/>Dilek Hakkani-T√ºr</a>
|
<a href=/people/p/pararth-shah/>Pararth Shah</a>
|
<a href=/people/l/larry-heck/>Larry Heck</a><br><a href=/volumes/N18-1/ class=text-muted>Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1187><div class="card-body p-3 small">In this work, we present a hybrid learning method for training task-oriented dialogue systems through online user interactions. Popular methods for learning task-oriented dialogues include applying <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning</a> with <a href=https://en.wikipedia.org/wiki/User-generated_content>user feedback</a> on supervised pre-training models. Efficiency of such learning method may suffer from the mismatch of dialogue state distribution between offline training and online interactive learning stages. To address this challenge, we propose a hybrid imitation and reinforcement learning method, with which a dialogue agent can effectively learn from its interaction with users by learning from human teaching and feedback. We design a neural network based task-oriented dialogue agent that can be optimized end-to-end with the proposed learning method. Experimental results show that our end-to-end dialogue agent can learn effectively from the mistake it makes via imitation learning from user teaching. Applying <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning</a> with user feedback after the imitation learning stage further improves the <a href=https://en.wikipedia.org/wiki/Intelligent_agent>agent</a>&#8217;s capability in successfully completing a task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-3006.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-3006 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-3006 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://vimeo.com/277631118 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N18-3006/>Bootstrapping a Neural Conversational Agent with Dialogue Self-Play, <a href=https://en.wikipedia.org/wiki/Crowdsourcing>Crowdsourcing</a> and <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>On-Line Reinforcement Learning</a></a></strong><br><a href=/people/p/pararth-shah/>Pararth Shah</a>
|
<a href=/people/d/dilek-hakkani-tur/>Dilek Hakkani-T√ºr</a>
|
<a href=/people/b/bing-liu/>Bing Liu</a>
|
<a href=/people/g/gokhan-tur/>Gokhan T√ºr</a><br><a href=/volumes/N18-3/ class=text-muted>Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 3 (Industry Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-3006><div class="card-body p-3 small">End-to-end neural models show great promise towards building conversational agents that are trained from data and on-line experience using supervised and reinforcement learning. However, these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> require a large corpus of dialogues to learn effectively. For goal-oriented dialogues, such <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> are expensive to collect and annotate, since each task involves a separate schema and <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>database of entities</a>. Further, the Wizard-of-Oz approach commonly used for dialogue collection does not provide sufficient coverage of salient dialogue flows, which is critical for guaranteeing an acceptable task completion rate in consumer-facing conversational agents. In this paper, we study a recently proposed approach for building an <a href=https://en.wikipedia.org/wiki/Intelligent_agent>agent</a> for arbitrary tasks by combining dialogue self-play and <a href=https://en.wikipedia.org/wiki/Crowdsourcing>crowd-sourcing</a> to generate fully-annotated dialogues with diverse and natural utterances. We discuss the advantages of this approach for industry applications of conversational agents, wherein an agent can be rapidly bootstrapped to deploy in front of users and further optimized via interactive learning from actual users of the system.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-4010.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-4010 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-4010 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://vimeo.com/276463184 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N18-4010/>End-to-End Learning of Task-Oriented Dialogs</a></strong><br><a href=/people/b/bing-liu/>Bing Liu</a>
|
<a href=/people/i/ian-lane/>Ian Lane</a><br><a href=/volumes/N18-4/ class=text-muted>Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Student Research Workshop</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-4010><div class="card-body p-3 small">In this thesis proposal, we address the limitations of conventional pipeline design of task-oriented dialog systems and propose end-to-end learning solutions. We design neural network based dialog system that is able to robustly track dialog state, interface with knowledge bases, and incorporate structured query results into system responses to successfully complete task-oriented dialog. In learning such neural network based dialog systems, we propose hybrid offline training and online interactive learning methods. We introduce a multi-task learning method in pre-training the dialog agent in a supervised manner using task-oriented dialog corpora. The supervised training agent can further be improved via interacting with users and learning online from user demonstration and feedback with imitation and <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning</a>. In addressing the sample efficiency issue with online policy learning, we further propose a method by combining the learning-from-user and learning-from-simulation approaches to improve the online interactive learning efficiency.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1088.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1088 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1088 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285801326 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-1088/>Target-Sensitive Memory Networks for Aspect Sentiment Classification</a></strong><br><a href=/people/s/shuai-wang/>Shuai Wang</a>
|
<a href=/people/s/sahisnu-mazumder/>Sahisnu Mazumder</a>
|
<a href=/people/b/bing-liu/>Bing Liu</a>
|
<a href=/people/m/mianwei-zhou/>Mianwei Zhou</a>
|
<a href=/people/y/yi-chang/>Yi Chang</a><br><a href=/volumes/P18-1/ class=text-muted>Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1088><div class="card-body p-3 small">Aspect sentiment classification (ASC) is a fundamental task in <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a>. Given an aspect / target and a sentence, the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> classifies the sentiment polarity expressed on the target in the sentence. Memory networks (MNs) have been used for this task recently and have achieved state-of-the-art results. In MNs, attention mechanism plays a crucial role in detecting the <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment context</a> for the given target. However, we found an important problem with the current MNs in performing the ASC task. Simply improving the attention mechanism will not solve it. The problem is referred to as target-sensitive sentiment, which means that the sentiment polarity of the (detected) context is dependent on the given target and it can not be inferred from the context alone. To tackle this problem, we propose the target-sensitive memory networks (TMNs). Several alternative techniques are designed for the implementation of TMNs and their effectiveness is experimentally evaluated.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2094.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2094 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2094 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2094.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-2094" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P18-2094/>Double Embeddings and CNN-based Sequence Labeling for Aspect Extraction<span class=acl-fixed-case>CNN</span>-based Sequence Labeling for Aspect Extraction</a></strong><br><a href=/people/h/hu-xu/>Hu Xu</a>
|
<a href=/people/b/bing-liu/>Bing Liu</a>
|
<a href=/people/l/lei-shu/>Lei Shu</a>
|
<a href=/people/p/philip-s-yu/>Philip S. Yu</a><br><a href=/volumes/P18-2/ class=text-muted>Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2094><div class="card-body p-3 small">One key task of fine-grained sentiment analysis of product reviews is to extract product aspects or features that users have expressed opinions on. This paper focuses on supervised aspect extraction using <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning</a>. Unlike other highly sophisticated supervised deep learning models, this paper proposes a novel and yet simple CNN model employing two types of pre-trained embeddings for <a href=https://en.wikipedia.org/wiki/Aspect_extraction>aspect extraction</a> : general-purpose embeddings and domain-specific embeddings. Without using any additional <a href=https://en.wikipedia.org/wiki/Supervisor>supervision</a>, this <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieves surprisingly good results, outperforming state-of-the-art sophisticated existing methods. To our knowledge, this paper is the first to report such double embeddings based CNN model for <a href=https://en.wikipedia.org/wiki/Aspect_extraction>aspect extraction</a> and achieve very good results.</div></div><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2023.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2023 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2023 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234956125 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-2023/>Lifelong Learning CRF for Supervised Aspect Extraction<span class=acl-fixed-case>CRF</span> for Supervised Aspect Extraction</a></strong><br><a href=/people/l/lei-shu/>Lei Shu</a>
|
<a href=/people/h/hu-xu/>Hu Xu</a>
|
<a href=/people/b/bing-liu/>Bing Liu</a><br><a href=/volumes/P17-2/ class=text-muted>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2023><div class="card-body p-3 small">This paper makes a focused contribution to supervised aspect extraction. It shows that if the system has performed aspect extraction from many past domains and retained their results as knowledge, Conditional Random Fields (CRF) can leverage this <a href=https://en.wikipedia.org/wiki/Knowledge>knowledge</a> in a lifelong learning manner to extract in a new domain markedly better than the traditional CRF without using this prior knowledge. The key innovation is that even after CRF training, the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> can still improve its extraction with experiences in its applications.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1059.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1059 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1059 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1059/>Sentiment Lexicon Expansion Based on Neural PU Learning, Double Dictionary Lookup, and Polarity Association<span class=acl-fixed-case>PU</span> Learning, Double Dictionary Lookup, and Polarity Association</a></strong><br><a href=/people/y/yasheng-wang/>Yasheng Wang</a>
|
<a href=/people/y/yang-zhang/>Yang Zhang</a>
|
<a href=/people/b/bing-liu/>Bing Liu</a><br><a href=/volumes/D17-1/ class=text-muted>Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1059><div class="card-body p-3 small">Although many sentiment lexicons in different languages exist, most are not comprehensive. In a recent sentiment analysis application, we used a large Chinese sentiment lexicon and found that it missed a large number of sentiment words in <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>. This prompted us to make a new attempt to study sentiment lexicon expansion. This paper first poses the <a href=https://en.wikipedia.org/wiki/Problem_solving>problem</a> as a PU learning problem, which is a new <a href=https://en.wikipedia.org/wiki/Formulation>formulation</a>. It then proposes a new PU learning method suitable for our problem using a <a href=https://en.wikipedia.org/wiki/Neural_network>neural network</a>. The results are enhanced further with a new dictionary-based technique and a novel polarity classification technique. Experimental results show that the proposed approach outperforms baseline methods greatly.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1314.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1314 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1314 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/238232487 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D17-1314/>DOC : Deep Open Classification of Text Documents<span class=acl-fixed-case>DOC</span>: Deep Open Classification of Text Documents</a></strong><br><a href=/people/l/lei-shu/>Lei Shu</a>
|
<a href=/people/h/hu-xu/>Hu Xu</a>
|
<a href=/people/b/bing-liu/>Bing Liu</a><br><a href=/volumes/D17-1/ class=text-muted>Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1314><div class="card-body p-3 small">Traditional <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised learning</a> makes the <a href=https://en.wikipedia.org/wiki/Closed-world_assumption>closed-world assumption</a> that the classes appeared in the test data must have appeared in training. This also applies to text learning or <a href=https://en.wikipedia.org/wiki/Text_classification>text classification</a>. As learning is used increasingly in dynamic open environments where some new / test documents may not belong to any of the training classes, identifying these novel documents during <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a> presents an important problem. This <a href=https://en.wikipedia.org/wiki/Problem_solving>problem</a> is called open-world classification or open classification. This paper proposes a novel deep learning based approach. It outperforms existing state-of-the-art techniques dramatically.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Bing+Liu" title="Search for 'Bing Liu' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/h/hu-xu/ class=align-middle>Hu Xu</a>
<span class="badge badge-secondary align-middle ml-2">10</span></li><li class=list-group-item><a href=/people/l/lei-shu/ class=align-middle>Lei Shu</a>
<span class="badge badge-secondary align-middle ml-2">7</span></li><li class=list-group-item><a href=/people/p/philip-s-yu/ class=align-middle>Philip S. Yu</a>
<span class="badge badge-secondary align-middle ml-2">5</span></li><li class=list-group-item><a href=/people/s/seungwhan-moon/ class=align-middle>Seungwhan Moon</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/people/p/pararth-shah/ class=align-middle>Pararth Shah</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/h/honglei-liu/ class=align-middle>Honglei Liu</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/g/gokhan-tur/ class=align-middle>Gokhan Tur</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/s/sahisnu-mazumder/ class=align-middle>Sahisnu Mazumder</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/p/paul-a-crook/ class=align-middle>Paul A. Crook</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/z/zhiguang-wang/ class=align-middle>Zhiguang Wang</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/e/eunjoon-cho/ class=align-middle>Eunjoon Cho</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/p/piero-molino/ class=align-middle>Piero Molino</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/w/wenpeng-hu/ class=align-middle>Wenpeng Hu</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/i/ian-lane/ class=align-middle>Ian Lane</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/d/dilek-hakkani-tur/ class=align-middle>Dilek Hakkani-Tur</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/z/zhenni-you/ class=align-middle>Zhenni You</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/tieyun-qian/ class=align-middle>Tieyun Qian</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/i/itika-gupta/ class=align-middle>Itika Gupta</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/b/barbara-di-eugenio/ class=align-middle>Barbara Di Eugenio</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/b/brian-d-ziebart/ class=align-middle>Brian D. Ziebart</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/b/ben-s-gerber/ class=align-middle>Ben S. Gerber</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/lisa-k-sharp/ class=align-middle>Lisa K. Sharp</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/alexandros-papangelis/ class=align-middle>Alexandros Papangelis</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/p/pawel-budzianowski/ class=align-middle>Pawe≈Ç Budzianowski</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/e/elnaz-nouri/ class=align-middle>Elnaz Nouri</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/abhinav-rastogi/ class=align-middle>Abhinav Rastogi</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yun-nung-chen/ class=align-middle>Yun-Nung Chen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/n/nianzu-ma/ class=align-middle>Nianzu Ma</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/alexander-politowicz/ class=align-middle>Alexander Politowicz</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jiahua-chen/ class=align-middle>Jiahua Chen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/e/eric-robertson/ class=align-middle>Eric Robertson</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/scott-grigsby/ class=align-middle>Scott Grigsby</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/zhaojiang-lin/ class=align-middle>Zhaojiang Lin</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/andrea-madotto/ class=align-middle>Andrea Madotto</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/zhenpeng-zhou/ class=align-middle>Zhenpeng Zhou</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/zhou-yu/ class=align-middle>Zhou Yu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/rajen-subba/ class=align-middle>Rajen Subba</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/p/pascale-fung/ class=align-middle>Pascale Fung</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/hao-wang/ class=align-middle>Hao Wang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/chaozhuo-li/ class=align-middle>Chaozhuo Li</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yan-yang/ class=align-middle>Yan Yang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/tianrui-li/ class=align-middle>Tianrui Li</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yasheng-wang/ class=align-middle>Yasheng Wang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yang-zhang/ class=align-middle>Yang Zhang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/kai-sun/ class=align-middle>Kai Sun</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/stephen-roller/ class=align-middle>Stephen Roller</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/b/becka-silvert/ class=align-middle>Becka Silvert</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/claire-cardie/ class=align-middle>Claire Cardie</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/zixuan-ke/ class=align-middle>Zixuan Ke</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/q/qi-qin/ class=align-middle>Qi Qin</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/mahdi-namazifar/ class=align-middle>Mahdi Namazifar</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/huaixiu-zheng/ class=align-middle>Huaixiu Zheng</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/ran-le/ class=align-middle>Ran Le</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jinwen-ma/ class=align-middle>Jinwen Ma</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/dongyan-zhao/ class=align-middle>Dongyan Zhao</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/rui-yan/ class=align-middle>Rui Yan</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/larry-heck/ class=align-middle>Larry Heck</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/shuai-wang/ class=align-middle>Shuai Wang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/mianwei-zhou/ class=align-middle>Mianwei Zhou</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yi-chang/ class=align-middle>Yi Chang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/emnlp/ class=align-middle>EMNLP</a><span class="badge badge-secondary align-middle ml-2">6</span></li><li class=list-group-item><a href=/venues/naacl/ class=align-middle>NAACL</a><span class="badge badge-secondary align-middle ml-2">6</span></li><li class=list-group-item><a href=/venues/coling/ class=align-middle>COLING</a><span class="badge badge-secondary align-middle ml-2">5</span></li><li class=list-group-item><a href=/venues/acl/ class=align-middle>ACL</a><span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/venues/ws/ class=align-middle>WS</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-venues aria-expanded=false aria-controls=more-venues>show all...</li><div class="collapse border-top" id=more-venues><li class=list-group-item><a href=/venues/sigdial/ class=align-middle>SIGDIAL</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/nlp4convai/ class=align-middle>NLP4ConvAI</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/findings/ class=align-middle>Findings</a><span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ¬©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>