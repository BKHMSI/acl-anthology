<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Bonnie Webber - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Bonnie</span> <span class=font-weight-bold>Webber</span></h2><hr><div class=row><div class=col-lg-9><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.421.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--421 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.421 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.421/>Refocusing on Relevance : Personalization in NLG<span class=acl-fixed-case>NLG</span></a></strong><br><a href=/people/s/shiran-dudy/>Shiran Dudy</a>
|
<a href=/people/s/steven-bedrick/>Steven Bedrick</a>
|
<a href=/people/b/bonnie-webber/>Bonnie Webber</a><br><a href=/volumes/2021.emnlp-main/ class=text-muted>Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--421><div class="card-body p-3 small">Many NLG tasks such as <a href=https://en.wikipedia.org/wiki/Automatic_summarization>summarization</a>, dialogue response, or <a href=https://en.wikipedia.org/wiki/Question_answering>open domain question answering</a>, focus primarily on a source text in order to generate a target response. This standard approach falls short, however, when a user&#8217;s intent or context of work is not easily recoverable based solely on that source text a scenario that we argue is more of the rule than the exception. In this work, we argue that NLG systems in general should place a much higher level of emphasis on making use of additional context, and suggest that <a href=https://en.wikipedia.org/wiki/Relevance>relevance</a> (as used in Information Retrieval) be thought of as a crucial tool for designing user-oriented text-generating tasks. We further discuss possible harms and hazards around such <a href=https://en.wikipedia.org/wiki/Personalization>personalization</a>, and argue that value-sensitive design represents a crucial path forward through these challenges.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.codi-main.10.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--codi-main--10 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.codi-main.10 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.codi-main.10/>Revisiting Shallow Discourse Parsing in the PDTB-3 : Handling Intra-sentential Implicits<span class=acl-fixed-case>PDTB</span>-3: Handling Intra-sentential Implicits</a></strong><br><a href=/people/z/zheng-zhao/>Zheng Zhao</a>
|
<a href=/people/b/bonnie-webber/>Bonnie Webber</a><br><a href=/volumes/2021.codi-main/ class=text-muted>Proceedings of the 2nd Workshop on Computational Approaches to Discourse</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--codi-main--10><div class="card-body p-3 small">In the PDTB-3, several thousand implicit discourse relations were newly annotated within individual sentences, adding to the over 15,000 implicit relations annotated across adjacent sentences in the PDTB-2. Given that the position of the arguments to these intra-sentential implicits is no longer as well-defined as with inter-sentential implicits, a discourse parser must identify both their location and their sense. That is the focus of the current work. The paper provides a comprehensive analysis of our results, showcasing <a href=https://en.wikipedia.org/wiki/Computer_simulation>model</a> performance under different scenarios, pointing out limitations and noting future directions.</div></div><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.0/>Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</a></strong><br><a href=/people/b/bonnie-webber/>Bonnie Webber</a>
|
<a href=/people/t/trevor-cohn/>Trevor Cohn</a>
|
<a href=/people/y/yulan-he/>Yulan He</a>
|
<a href=/people/y/yang-liu-icsi/>Yang Liu</a><br><a href=/volumes/2020.emnlp-main/ class=text-muted>Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.203.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--203 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.203 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.203/>Reducing Quantity Hallucinations in Abstractive Summarization</a></strong><br><a href=/people/z/zheng-zhao/>Zheng Zhao</a>
|
<a href=/people/s/shay-b-cohen/>Shay B. Cohen</a>
|
<a href=/people/b/bonnie-webber/>Bonnie Webber</a><br><a href=/volumes/2020.findings-emnlp/ class=text-muted>Findings of the Association for Computational Linguistics: EMNLP 2020</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--203><div class="card-body p-3 small">It is well-known that abstractive summaries are subject to hallucinationincluding material that is not supported by the original text. While summaries can be made hallucination-free by limiting them to general phrases, such summaries would fail to be very informative. Alternatively, one can try to avoid <a href=https://en.wikipedia.org/wiki/Hallucination>hallucinations</a> by verifying that any specific entities in the summary appear in the original text in a similar context. This is the approach taken by our <a href=https://en.wikipedia.org/wiki/System>system</a>, Herman. The <a href=https://en.wikipedia.org/wiki/System>system</a> learns to recognize and verify quantity entities (dates, numbers, sums of money, etc.) in a beam-worth of abstractive summaries produced by state-of-the-art models, in order to up-rank those summaries whose quantity terms are supported by the original text. Experimental results demonstrate that the ROUGE scores of such up-ranked summaries have a higher <a href=https://en.wikipedia.org/wiki/Precision_(statistics)>Precision</a> than summaries that have not been up-ranked, without a comparable loss in <a href=https://en.wikipedia.org/wiki/Recall_(memory)>Recall</a>, resulting in higher F1. Preliminary human evaluation of up-ranked vs. original summaries shows people&#8217;s preference for the former.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.law-1.13.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--law-1--13 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.law-1.13 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.law-1.13" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.law-1.13/>Querent Intent in Multi-Sentence Questions</a></strong><br><a href=/people/l/laurie-burchell/>Laurie Burchell</a>
|
<a href=/people/j/jie-chi/>Jie Chi</a>
|
<a href=/people/t/tom-hosking/>Tom Hosking</a>
|
<a href=/people/n/nina-markl/>Nina Markl</a>
|
<a href=/people/b/bonnie-webber/>Bonnie Webber</a><br><a href=/volumes/2020.law-1/ class=text-muted>Proceedings of the 14th Linguistic Annotation Workshop</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--law-1--13><div class="card-body p-3 small">Multi-sentence questions (MSQs) are sequences of questions connected by relations which, unlike sequences of standalone questions, need to be answered as a unit. Following Rhetorical Structure Theory (RST), we recognise that different question discourse relations between the subparts of MSQs reflect different speaker intents, and consequently elicit different answering strategies. Correctly identifying these <a href=https://en.wikipedia.org/wiki/Binary_relation>relations</a> is therefore a crucial step in automatically answering MSQs. We identify five different types of MSQs in <a href=https://en.wikipedia.org/wiki/English_language>English</a>, and define five novel <a href=https://en.wikipedia.org/wiki/Binary_relation>relations</a> to describe them. We extract over 162,000 MSQs from <a href=https://en.wikipedia.org/wiki/Stack_Exchange>Stack Exchange</a> to enable future research. Finally, we implement a high-precision baseline classifier based on <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>surface features</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.codi-1.14.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--codi-1--14 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.codi-1.14 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939702 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.codi-1.14/>Extending Implicit Discourse Relation Recognition to the PDTB-3<span class=acl-fixed-case>PDTB</span>-3</a></strong><br><a href=/people/l/li-liang/>Li Liang</a>
|
<a href=/people/z/zheng-zhao/>Zheng Zhao</a>
|
<a href=/people/b/bonnie-webber/>Bonnie Webber</a><br><a href=/volumes/2020.codi-1/ class=text-muted>Proceedings of the First Workshop on Computational Approaches to Discourse</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--codi-1--14><div class="card-body p-3 small">The PDTB-3 contains many more Implicit discourse relations than the previous PDTB-2. This is in part because implicit relations have now been annotated within sentences as well as between them. In addition, some now co-occur with explicit discourse relations, instead of standing on their own. Here we show that while this can complicate the problem of identifying the location of implicit discourse relations, it can in turn simplify the problem of identifying their senses. We present data to support this claim, as well as methods that can serve as a non-trivial baseline for future state-of-the-art recognizers for implicit discourse relations.</div></div><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1462.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1462 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1462 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1462/>GECOR : An End-to-End Generative Ellipsis and Co-reference Resolution Model for Task-Oriented Dialogue<span class=acl-fixed-case>GECOR</span>: An End-to-End Generative Ellipsis and Co-reference Resolution Model for Task-Oriented Dialogue</a></strong><br><a href=/people/j/jun-quan/>Jun Quan</a>
|
<a href=/people/d/deyi-xiong/>Deyi Xiong</a>
|
<a href=/people/b/bonnie-webber/>Bonnie Webber</a>
|
<a href=/people/c/changjian-hu/>Changjian Hu</a><br><a href=/volumes/D19-1/ class=text-muted>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1462><div class="card-body p-3 small">Ellipsis and co-reference are common and ubiquitous especially in <a href=https://en.wikipedia.org/wiki/Dialogue>multi-turn dialogues</a>. In this paper, we treat the resolution of ellipsis and co-reference in <a href=https://en.wikipedia.org/wiki/Dialogue>dialogue</a> as a problem of generating omitted or referred expressions from the <a href=https://en.wikipedia.org/wiki/Dialogue>dialogue context</a>. We therefore propose a unified end-to-end Generative Ellipsis and CO-reference Resolution model (GECOR) in the context of dialogue. The <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> can generate a new pragmatically complete user utterance by alternating the generation and copy mode for each user utterance. A multi-task learning framework is further proposed to integrate the GECOR into an end-to-end task-oriented dialogue. In order to train both the GECOR and the multi-task learning framework, we manually construct a new dataset on the basis of the public dataset CamRest676 with both ellipsis and co-reference annotation. On this dataset, intrinsic evaluations on the resolution of ellipsis and co-reference show that the GECOR model significantly outperforms the sequence-to-sequence (seq2seq) baseline model in terms of EM, BLEU and F1 while extrinsic evaluations on the downstream dialogue task demonstrate that our multi-task learning framework with GECOR achieves a higher success rate of task completion than TSCP, a state-of-the-art end-to-end task-oriented dialogue model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/R19-1021.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-R19-1021 data-toggle=collapse aria-expanded=false aria-controls=abstract-R19-1021 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/R19-1021/>Classifying Author Intention for Writer Feedback in Related Work</a></strong><br><a href=/people/a/arlene-casey/>Arlene Casey</a>
|
<a href=/people/b/bonnie-webber/>Bonnie Webber</a>
|
<a href=/people/d/dorota-glowacka/>Dorota Glowacka</a><br><a href=/volumes/R19-1/ class=text-muted>Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2019)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-R19-1021><div class="card-body p-3 small">The ability to produce high-quality publishable material is critical to academic success but many Post-Graduate students struggle to learn to do so. While recent years have seen an increase in tools designed to provide feedback on aspects of writing, one aspect that has so far been neglected is the Related Work section of <a href=https://en.wikipedia.org/wiki/Academic_publishing>academic research papers</a>. To address this, we have trained a <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised classifier</a> on a <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> of 94 Related Work sections and evaluated it against a manually annotated gold standard. The <a href=https://en.wikipedia.org/wiki/Classifier_(linguistics)>classifier</a> uses novel features pertaining to citation types and <a href=https://en.wikipedia.org/wiki/Co-reference>co-reference</a>, along with patterns found from studying Related Works. We show that these novel features contribute to <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifier</a> performance with performance being favourable compared to other similar works that classify author intentions and consider <a href=https://en.wikipedia.org/wiki/Feedback>feedback</a> for <a href=https://en.wikipedia.org/wiki/Academic_writing>academic writing</a>.</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1466.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1466 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1466 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/306166016 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1466/>Getting to Hearer-old : Charting Referring Expressions Across Time</a></strong><br><a href=/people/i/ieva-staliunaite/>Ieva Staliūnaitė</a>
|
<a href=/people/h/hannah-rohde/>Hannah Rohde</a>
|
<a href=/people/b/bonnie-webber/>Bonnie Webber</a>
|
<a href=/people/a/annie-louis/>Annie Louis</a><br><a href=/volumes/D18-1/ class=text-muted>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1466><div class="card-body p-3 small">When a reader is first introduced to an entity, its <a href=https://en.wikipedia.org/wiki/Referring_expression>referring expression</a> must describe the entity. For entities that are widely known, a single word or phrase often suffices. This paper presents the first study of how <a href=https://en.wikipedia.org/wiki/Expression_(mathematics)>expressions</a> that refer to the same entity develop over time. We track thousands of person and organization entities over 20 years of New York Times (NYT). As entities move from hearer-new (first introduction to the NYT audience) to hearer-old (common knowledge) status, we show empirically that the referring expressions along this trajectory depend on the type of the entity, and exhibit linguistic properties related to becoming common knowledge (e.g., shorter length, less use of appositives, more definiteness). These properties can also be used to build a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> to predict how long it will take for an entity to reach hearer-old status. Our results reach 10-30 % absolute improvement over a majority-class baseline.</div></div><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1804.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1804 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1804 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1804/>Universal Dependencies to <a href=https://en.wikipedia.org/wiki/Logical_form>Logical Form</a> with Negation Scope<span class=acl-fixed-case>U</span>niversal <span class=acl-fixed-case>D</span>ependencies to Logical Form with Negation Scope</a></strong><br><a href=/people/f/federico-fancellu/>Federico Fancellu</a>
|
<a href=/people/s/siva-reddy/>Siva Reddy</a>
|
<a href=/people/a/adam-lopez/>Adam Lopez</a>
|
<a href=/people/b/bonnie-webber/>Bonnie Webber</a><br><a href=/volumes/W17-18/ class=text-muted>Proceedings of the Workshop Computational Semantics Beyond Events and Roles</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1804><div class="card-body p-3 small">Many language technology applications would benefit from the ability to represent <a href=https://en.wikipedia.org/wiki/Affirmation_and_negation>negation</a> and its scope on top of widely-used linguistic resources. In this paper, we investigate the possibility of obtaining a <a href=https://en.wikipedia.org/wiki/First-order_logic>first-order logic representation</a> with <a href=https://en.wikipedia.org/wiki/Scope_(computer_science)>negation scope</a> marked using Universal Dependencies. To do so, we enhance UDepLambda, a <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> that converts <a href=https://en.wikipedia.org/wiki/Dependency_graph>dependency graphs</a> to <a href=https://en.wikipedia.org/wiki/Logical_form>logical forms</a>. The resulting UDepLambda is able to handle phenomena related to scope by means of an higher-order type theory, relevant not only to <a href=https://en.wikipedia.org/wiki/Negation>negation</a> but also to <a href=https://en.wikipedia.org/wiki/Universal_quantification>universal quantification</a> and other complex semantic phenomena. The initial conversion we did for <a href=https://en.wikipedia.org/wiki/English_language>English</a> is promising, in that one can represent the scope of negation also in the presence of more complex phenomena such as <a href=https://en.wikipedia.org/wiki/Universal_quantification>universal quantifiers</a>.<i>Universal Dependencies</i>. To do so, we enhance <i>UDepLambda</i>, a framework that converts dependency graphs to logical forms. The resulting <i>UDepLambda<tex-math>\\lnot</tex-math>\n </i>\n\nis able to handle phenomena related to scope by means of an higher-order type theory, relevant not only to negation but also to universal quantification and other complex semantic phenomena. The initial conversion we did for English is promising, in that one can represent the scope of negation also in the presence of more complex phenomena such as universal quantifiers.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4800.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4800/>Proceedings of the Third Workshop on Discourse in Machine Translation</a></strong><br><a href=/people/b/bonnie-webber/>Bonnie Webber</a>
|
<a href=/people/a/andrei-popescu-belis/>Andrei Popescu-Belis</a>
|
<a href=/people/j/jorg-tiedemann/>Jörg Tiedemann</a><br><a href=/volumes/W17-48/ class=text-muted>Proceedings of the Third Workshop on Discourse in Machine Translation</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2010.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2010 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2010 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2010/>Detecting negation scope is easy, except when it is n’t</a></strong><br><a href=/people/f/federico-fancellu/>Federico Fancellu</a>
|
<a href=/people/a/adam-lopez/>Adam Lopez</a>
|
<a href=/people/b/bonnie-webber/>Bonnie Webber</a>
|
<a href=/people/h/hangfeng-he/>Hangfeng He</a><br><a href=/volumes/E17-2/ class=text-muted>Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2010><div class="card-body p-3 small">Several <a href=https://en.wikipedia.org/wiki/Text_corpus>corpora</a> have been annotated with negation scopethe set of words whose meaning is negated by a cue like the word notleading to the development of <a href=https://en.wikipedia.org/wiki/Classifier_(linguistics)>classifiers</a> that detect negation scope with high accuracy. We show that for nearly all of these corpora, this high accuracy can be attributed to a single fact : they frequently annotate negation scope as a single span of text delimited by <a href=https://en.wikipedia.org/wiki/Punctuation>punctuation</a>. For negation scopes not of this form, detection accuracy is low and under-sampling the easy training examples does not substantially improve <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>. We demonstrate that this is partly an artifact of annotation guidelines, and we argue that future negation scope annotation efforts should focus on these more difficult cases.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-4004.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-4004 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-4004 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-4004/>Discourse Relations and <a href=https://en.wikipedia.org/wiki/Conjoined_twins>Conjoined VPs</a> : Automated Sense Recognition<span class=acl-fixed-case>VP</span>s: Automated Sense Recognition</a></strong><br><a href=/people/v/valentina-pyatkin/>Valentina Pyatkin</a>
|
<a href=/people/b/bonnie-webber/>Bonnie Webber</a><br><a href=/volumes/E17-4/ class=text-muted>Proceedings of the Student Research Workshop at the 15th Conference of the European Chapter of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-4004><div class="card-body p-3 small">Sense classification of discourse relations is a sub-task of shallow discourse parsing. Discourse relations can occur both across sentences (inter-sentential) and within sentences (intra-sentential), and more than one <a href=https://en.wikipedia.org/wiki/Discourse_relation>discourse relation</a> can hold between the same units. Using a newly available corpus of discourse-annotated intra-sentential conjoined verb phrases, we demonstrate a sequential classification pipeline for their multi-label sense classification. We assess the importance of each <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>feature</a> used in the <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a>, the <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>feature scope</a>, and what is lost in moving from gold standard manual parses to the output of an off-the-shelf <a href=https://en.wikipedia.org/wiki/Parsing>parser</a>.<i>inter-sentential</i>) and within sentences (<i>intra-sentential</i>), and more than one discourse relation can hold between the same units. Using a newly available corpus of discourse-annotated intra-sentential conjoined verb phrases, we demonstrate a sequential classification pipeline for their multi-label sense classification. We assess the importance of each feature used in the classification, the feature scope, and what is lost in moving from gold standard manual parses to the output of an off-the-shelf parser.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Bonnie+Webber" title="Search for 'Bonnie Webber' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/z/zheng-zhao/ class=align-middle>Zheng Zhao</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/f/federico-fancellu/ class=align-middle>Federico Fancellu</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/a/adam-lopez/ class=align-middle>Adam Lopez</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/t/trevor-cohn/ class=align-middle>Trevor Cohn</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yulan-he/ class=align-middle>Yulan He</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/y/yang-liu-icsi/ class=align-middle>Yang Liu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/siva-reddy/ class=align-middle>Siva Reddy</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/andrei-popescu-belis/ class=align-middle>Andrei Popescu-Belis</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jorg-tiedemann/ class=align-middle>Jörg Tiedemann</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/i/ieva-staliunaite/ class=align-middle>Ieva Staliūnaitė</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/hannah-rohde/ class=align-middle>Hannah Rohde</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/annie-louis/ class=align-middle>Annie Louis</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/shiran-dudy/ class=align-middle>Shiran Dudy</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/steven-bedrick/ class=align-middle>Steven Bedrick</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jun-quan/ class=align-middle>Jun Quan</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/deyi-xiong/ class=align-middle>Deyi Xiong</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/changjian-hu/ class=align-middle>Changjian Hu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/shay-b-cohen/ class=align-middle>Shay B. Cohen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/laurie-burchell/ class=align-middle>Laurie Burchell</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jie-chi/ class=align-middle>Jie Chi</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/tom-hosking/ class=align-middle>Tom Hosking</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/n/nina-markl/ class=align-middle>Nina Markl</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/arlene-casey/ class=align-middle>Arlene Casey</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/dorota-glowacka/ class=align-middle>Dorota Glowacka</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/hangfeng-he/ class=align-middle>Hangfeng He</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/v/valentina-pyatkin/ class=align-middle>Valentina Pyatkin</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/li-liang/ class=align-middle>Li Liang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/emnlp/ class=align-middle>EMNLP</a><span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/venues/ws/ class=align-middle>WS</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/eacl/ class=align-middle>EACL</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/codi/ class=align-middle>CODI</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/findings/ class=align-middle>Findings</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-venues aria-expanded=false aria-controls=more-venues>show all...</li><div class="collapse border-top" id=more-venues><li class=list-group-item><a href=/venues/law/ class=align-middle>LAW</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/ranlp/ class=align-middle>RANLP</a><span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>