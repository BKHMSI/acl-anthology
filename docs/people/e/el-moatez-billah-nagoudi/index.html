<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>El Moatez Billah Nagoudi - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>El Moatez Billah</span> <span class=font-weight-bold>Nagoudi</span></h2><hr><div class=row><div class=col-lg-9><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.551.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--551 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.551 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.551/>ARBERT & MARBERT : Deep Bidirectional Transformers for Arabic<span class=acl-fixed-case>ARBERT</span> & <span class=acl-fixed-case>MARBERT</span>: Deep Bidirectional Transformers for <span class=acl-fixed-case>A</span>rabic</a></strong><br><a href=/people/m/muhammad-abdul-mageed/>Muhammad Abdul-Mageed</a>
|
<a href=/people/a/abdelrahim-elmadany/>AbdelRahim Elmadany</a>
|
<a href=/people/e/el-moatez-billah-nagoudi/>El Moatez Billah Nagoudi</a><br><a href=/volumes/2021.acl-long/ class=text-muted>Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--551><div class="card-body p-3 small">Pre-trained language models (LMs) are currently integral to many <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing systems</a>. Although multilingual LMs were also introduced to serve many languages, these have limitations such as being costly at inference time and the size and diversity of non-English data involved in their pre-training. We remedy these issues for a collection of diverse Arabic varieties by introducing two powerful deep bidirectional transformer-based models, ARBERT and MARBERT. To evaluate our models, we also introduce ARLUE, a new <a href=https://en.wikipedia.org/wiki/Benchmarking>benchmark</a> for multi-dialectal Arabic language understanding evaluation. ARLUE is built using 42 <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> targeting six different task clusters, allowing us to offer a series of standardized experiments under rich conditions. When fine-tuned on ARLUE, our models collectively achieve new state-of-the-art results across the majority of tasks (37 out of 48 classification tasks, on the 42 datasets). Our best <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> acquires the highest ARLUE score (77.40) across all six task clusters, outperforming all other <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> including XLM-R Large (3.4x larger size). Our models are publicly available at https://github.com/UBC-NLP/marbert and ARLUE will be released through the same repository.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.calcs-1.6.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--calcs-1--6 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.calcs-1.6 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.calcs-1.6/>Exploring Text-to-Text Transformers for English to Hinglish Machine Translation with Synthetic Code-Mixing<span class=acl-fixed-case>E</span>nglish to <span class=acl-fixed-case>H</span>inglish Machine Translation with Synthetic Code-Mixing</a></strong><br><a href=/people/g/ganesh-jawahar/>Ganesh Jawahar</a>
|
<a href=/people/e/el-moatez-billah-nagoudi/>El Moatez Billah Nagoudi</a>
|
<a href=/people/m/muhammad-abdul-mageed/>Muhammad Abdul-Mageed</a>
|
<a href=/people/l/laks-lakshmanan-v-s/>Laks Lakshmanan, V.S.</a><br><a href=/volumes/2021.calcs-1/ class=text-muted>Proceedings of the Fifth Workshop on Computational Approaches to Linguistic Code-Switching</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--calcs-1--6><div class="card-body p-3 small">We describe models focused at the understudied problem of <a href=https://en.wikipedia.org/wiki/Translation>translating</a> between monolingual and code-mixed language pairs. More specifically, we offer a wide range of models that convert monolingual English text into Hinglish (code-mixed Hindi and English). Given the recent success of pretrained language models, we also test the utility of two recent Transformer-based encoder-decoder models (i.e., mT5 and mBART) on the task finding both to work well. Given the paucity of training data for <a href=https://en.wikipedia.org/wiki/Code_mixing>code-mixing</a>, we also propose a dependency-free method for generating code-mixed texts from bilingual distributed representations that we exploit for improving <a href=https://en.wikipedia.org/wiki/Language_model>language model</a> performance. In particular, armed with this additional data, we adopt a curriculum learning approach where we first finetune the <a href=https://en.wikipedia.org/wiki/Language_model>language models</a> on <a href=https://en.wikipedia.org/wiki/Synthetic_data>synthetic data</a> then on gold code-mixed data. We find that, although simple, our synthetic code-mixing method is competitive with (and in some cases is even superior to) several standard methods (backtranslation, <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>method</a> based on equivalence constraint theory) under a diverse set of conditions. Our work shows that the mT5 model, finetuned following the <a href=https://en.wikipedia.org/wiki/Curriculum>curriculum learning procedure</a>, achieves best translation performance (12.67 BLEU). Our <a href=https://en.wikipedia.org/wiki/Model_(person)>models</a> place first in the overall ranking of the English-Hinglish official shared task.</div></div><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wmt-1.42.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wmt-1--42 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wmt-1.42 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939640 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.wmt-1.42/>Translating Similar Languages : Role of <a href=https://en.wikipedia.org/wiki/Mutual_intelligibility>Mutual Intelligibility</a> in Multilingual Transformers</a></strong><br><a href=/people/i/ife-adebara/>Ife Adebara</a>
|
<a href=/people/e/el-moatez-billah-nagoudi/>El Moatez Billah Nagoudi</a>
|
<a href=/people/m/muhammad-abdul-mageed/>Muhammad Abdul Mageed</a><br><a href=/volumes/2020.wmt-1/ class=text-muted>Proceedings of the Fifth Conference on Machine Translation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wmt-1--42><div class="card-body p-3 small">In this work we investigate different approaches to translate between similar languages despite low resource limitations. This work is done as the participation of the UBC NLP research group in the WMT 2019 Similar Languages Translation Shared Task. We participated in all language pairs and performed various experiments. We used a transformer architecture for all the <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> and used <a href=https://en.wikipedia.org/wiki/Back-translation>back-translation</a> for one of the language pairs. We explore both bilingual and multi-lingual approaches. We describe the pre-processing, training, translation and results for each <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a>. We also investigate the role of <a href=https://en.wikipedia.org/wiki/Mutual_intelligibility>mutual intelligibility</a> in <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> performance.</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S18-1055.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S18-1055 data-toggle=collapse aria-expanded=false aria-controls=abstract-S18-1055 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/S18-1055/>ARB-SEN at SemEval-2018 Task1 : A New Set of Features for Enhancing the Sentiment Intensity Prediction in Arabic Tweets<span class=acl-fixed-case>ARB</span>-<span class=acl-fixed-case>SEN</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2018 Task1: A New Set of Features for Enhancing the Sentiment Intensity Prediction in <span class=acl-fixed-case>A</span>rabic Tweets</a></strong><br><a href=/people/e/el-moatez-billah-nagoudi/>El Moatez Billah Nagoudi</a><br><a href=/volumes/S18-1/ class=text-muted>Proceedings of The 12th International Workshop on Semantic Evaluation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S18-1055><div class="card-body p-3 small">This article describes our proposed Arabic Sentiment Analysis system named ARB-SEN. This system is designed for the International Workshop on Semantic Evaluation 2018 (SemEval-2018), Task1 : Affect in Tweets. ARB-SEN proposes two <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised models</a> to estimate the <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment intensity</a> in <a href=https://en.wikipedia.org/wiki/Twitter>Arabic tweets</a>. Both models use a set of features including sentiment lexicon, <a href=https://en.wikipedia.org/wiki/Affirmation_and_negation>negation</a>, <a href=https://en.wikipedia.org/wiki/Word_embedding>word embedding</a> and emotion symbols features. Our <a href=https://en.wikipedia.org/wiki/System>system</a> combines these <a href=https://en.wikipedia.org/wiki/Software_feature>features</a> to assist the <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis task</a>. ARB-SEN system achieves a <a href=https://en.wikipedia.org/wiki/Correlation_and_dependence>correlation score</a> of 0.720, ranking 6th among all participants in the valence intensity regression (V-reg) for the Arabic sub-task organized within the SemEval 2018 evaluation campaign.</div></div><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1303.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1303 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1303 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1303/>Semantic Similarity of Arabic Sentences with Word Embeddings<span class=acl-fixed-case>A</span>rabic Sentences with Word Embeddings</a></strong><br><a href=/people/e/el-moatez-billah-nagoudi/>El Moatez Billah Nagoudi</a>
|
<a href=/people/d/didier-schwab/>Didier Schwab</a><br><a href=/volumes/W17-13/ class=text-muted>Proceedings of the Third Arabic Natural Language Processing Workshop</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1303><div class="card-body p-3 small">Semantic textual similarity is the basis of countless applications and plays an important role in diverse areas, such as <a href=https://en.wikipedia.org/wiki/Information_retrieval>information retrieval</a>, <a href=https://en.wikipedia.org/wiki/Plagiarism_detection>plagiarism detection</a>, <a href=https://en.wikipedia.org/wiki/Information_extraction>information extraction</a> and <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>. This article proposes an innovative word embedding-based system devoted to calculate the <a href=https://en.wikipedia.org/wiki/Semantic_similarity>semantic similarity</a> in <a href=https://en.wikipedia.org/wiki/Arabic_grammar>Arabic sentences</a>. The main idea is to exploit <a href=https://en.wikipedia.org/wiki/Vector_space>vectors</a> as word representations in a <a href=https://en.wikipedia.org/wiki/Dimension_(vector_space)>multidimensional space</a> in order to capture the semantic and syntactic properties of words. IDF weighting and Part-of-Speech tagging are applied on the examined sentences to support the identification of words that are highly descriptive in each sentence. The performance of our proposed <a href=https://en.wikipedia.org/wiki/System>system</a> is confirmed through the <a href=https://en.wikipedia.org/wiki/Pearson_correlation_coefficient>Pearson correlation</a> between our assigned semantic similarity scores and <a href=https://en.wikipedia.org/wiki/Judgement>human judgments</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S17-2017.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S17-2017 data-toggle=collapse aria-expanded=false aria-controls=abstract-S17-2017 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/S17-2017/>LIM-LIG at SemEval-2017 Task1 : Enhancing the Semantic Similarity for Arabic Sentences with Vectors Weighting<span class=acl-fixed-case>LIM</span>-<span class=acl-fixed-case>LIG</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2017 Task1: Enhancing the Semantic Similarity for <span class=acl-fixed-case>A</span>rabic Sentences with Vectors Weighting</a></strong><br><a href=/people/e/el-moatez-billah-nagoudi/>El Moatez Billah Nagoudi</a>
|
<a href=/people/j/jeremy-ferrero/>Jérémy Ferrero</a>
|
<a href=/people/d/didier-schwab/>Didier Schwab</a><br><a href=/volumes/S17-2/ class=text-muted>Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S17-2017><div class="card-body p-3 small">This article describes our proposed <a href=https://en.wikipedia.org/wiki/System>system</a> named LIM-LIG. This system is designed for SemEval 2017 Task1 : Semantic Textual Similarity (Track1). LIM-LIG proposes an innovative enhancement to word embedding-based model devoted to measure the <a href=https://en.wikipedia.org/wiki/Semantic_similarity>semantic similarity</a> in Arabic sentences. The main idea is to exploit the word representations as vectors in a <a href=https://en.wikipedia.org/wiki/Dimension_(vector_space)>multidimensional space</a> to capture the semantic and syntactic properties of words. IDF weighting and Part-of-Speech tagging are applied on the examined sentences to support the identification of words that are highly descriptive in each sentence. LIM-LIG system achieves a <a href=https://en.wikipedia.org/wiki/Pearson_correlation_coefficient>Pearson&#8217;s correlation</a> of 0.74633, ranking 2nd among all participants in the Arabic monolingual pairs STS task organized within the SemEval 2017 evaluation campaign</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=El+Moatez+Billah+Nagoudi" title="Search for 'El Moatez Billah Nagoudi' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/m/muhammad-abdul-mageed/ class=align-middle>Muhammad Abdul-Mageed</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/d/didier-schwab/ class=align-middle>Didier Schwab</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/a/abdelrahim-elmadany/ class=align-middle>AbdelRahim Elmadany</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/g/ganesh-jawahar/ class=align-middle>Ganesh Jawahar</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/laks-lakshmanan-v-s/ class=align-middle>Laks Lakshmanan, V.S.</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/j/jeremy-ferrero/ class=align-middle>Jérémy Ferrero</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/i/ife-adebara/ class=align-middle>Ife Adebara</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/semeval/ class=align-middle>SemEval</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/acl/ class=align-middle>ACL</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/calcs/ class=align-middle>CALCS</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/ws/ class=align-middle>WS</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/wmt/ class=align-middle>WMT</a><span class="badge badge-secondary align-middle ml-2">1</span></li></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>