<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Philippe Langlais - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Philippe</span> <span class=font-weight-bold>Langlais</span></h2><p class="font-weight-light text-muted"><span class=font-italic>Also published as:</span>
Phillippe <span class=font-weight-normal>Langlais</span></p><hr><div class=row><div class=col-lg-9><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.ranlp-1.139.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--ranlp-1--139 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.ranlp-1.139 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.ranlp-1.139/>Exploiting Domain-Specific Knowledge for Judgment Prediction Is No Panacea</a></strong><br><a href=/people/o/olivier-salaun/>Olivier Sala√ºn</a>
|
<a href=/people/p/philippe-langlais/>Philippe Langlais</a>
|
<a href=/people/k/karim-benyekhlef/>Karim Benyekhlef</a><br><a href=/volumes/2021.ranlp-1/ class=text-muted>Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2021)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--ranlp-1--139><div class="card-body p-3 small">Legal judgment prediction (LJP) usually consists in a text classification task aimed at predicting the verdict on the basis of the fact description. The literature shows that the use of <a href=https://en.wikipedia.org/wiki/Article_(publishing)>articles</a> as input <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> helps improve the <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a> performance. In this work, we designed a verdict prediction task based on landlord-tenant disputes and we applied BERT-based models to which we fed different article-based features. Although the results obtained are consistent with the literature, the improvements with the articles are mostly obtained with the most frequent labels, suggesting that pre-trained and fine-tuned transformer-based models are not scalable as is for legal reasoning in real life scenarios as they would only excel in accurately predicting the most recurrent verdicts to the detriment of other legal outcomes.</div></div><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.527.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--527 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.527 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.527/>Data Selection for Bilingual Lexicon Induction from Specialized Comparable Corpora</a></strong><br><a href=/people/m/martin-laville/>Martin Laville</a>
|
<a href=/people/a/amir-hazem/>Amir Hazem</a>
|
<a href=/people/e/emmanuel-morin/>Emmanuel Morin</a>
|
<a href=/people/p/philippe-langlais/>Phillippe Langlais</a><br><a href=/volumes/2020.coling-main/ class=text-muted>Proceedings of the 28th International Conference on Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--527><div class="card-body p-3 small">Narrow specialized comparable corpora are often small in size. This particularity makes it difficult to build efficient <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> to acquire translation equivalents, especially for less frequent and rare words. One way to overcome this issue is to enrich the specialized corpora with out-of-domain resources. Although some recent studies have shown improvements using <a href=https://en.wikipedia.org/wiki/Data_augmentation>data augmentation</a>, the enrichment method was roughly conducted by adding out-of-domain data with no particular attention given to how to enrich words and how to do it optimally. In this paper, we contrast several data selection techniques to improve bilingual lexicon induction from specialized comparable corpora. We first apply two well-established data selection techniques often used in <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> that is : Tf-Idf and <a href=https://en.wikipedia.org/wiki/Cross_entropy>cross entropy</a>. Then, we propose to exploit BERT for data selection. Overall, all the proposed techniques improve the quality of the extracted bilingual lexicons by a large margin. The best performing <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is the <a href=https://en.wikipedia.org/wiki/Cross_entropy>cross entropy</a>, obtaining a gain of about 4 points in MAP while decreasing <a href=https://en.wikipedia.org/wiki/Time_complexity>computation time</a> by a factor of 10.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.576.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--576 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.576 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.576/>Human or Neural Translation?</a></strong><br><a href=/people/s/shivendra-bhardwaj/>Shivendra Bhardwaj</a>
|
<a href=/people/d/david-alfonso-hermelo/>David Alfonso Hermelo</a>
|
<a href=/people/p/philippe-langlais/>Phillippe Langlais</a>
|
<a href=/people/g/gabriel-bernier-colborne/>Gabriel Bernier-Colborne</a>
|
<a href=/people/c/cyril-goutte/>Cyril Goutte</a>
|
<a href=/people/m/michel-simard/>Michel Simard</a><br><a href=/volumes/2020.coling-main/ class=text-muted>Proceedings of the 28th International Conference on Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--576><div class="card-body p-3 small">Deep neural models tremendously improved <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>. In this context, we investigate whether distinguishing machine from human translations is still feasible. We trained and applied 18 classifiers under two settings : a monolingual task, in which the <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifier</a> only looks at the translation ; and a bilingual task, in which the source text is also taken into consideration. We report on extensive experiments involving 4 neural MT systems (Google Translate, DeepL, as well as two systems we trained) and varying the domain of texts. We show that the bilingual task is the easiest one and that transfer-based deep-learning classifiers perform best, with mean accuracies around 85 % in-domain and 75 % out-of-domain.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.211.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--211 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.211 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.211/>HardEval : Focusing on Challenging Tokens to Assess Robustness of NER<span class=acl-fixed-case>H</span>ard<span class=acl-fixed-case>E</span>val: Focusing on Challenging Tokens to Assess Robustness of <span class=acl-fixed-case>NER</span></a></strong><br><a href=/people/g/gabriel-bernier-colborne/>Gabriel Bernier-Colborne</a>
|
<a href=/people/p/philippe-langlais/>Phillippe Langlais</a><br><a href=/volumes/2020.lrec-1/ class=text-muted>Proceedings of the 12th Language Resources and Evaluation Conference</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--211><div class="card-body p-3 small">To assess the <a href=https://en.wikipedia.org/wiki/Robustness_(computer_science)>robustness</a> of NER systems, we propose an evaluation method that focuses on subsets of tokens that represent specific sources of errors : unknown words and label shift or <a href=https://en.wikipedia.org/wiki/Ambiguity>ambiguity</a>. These <a href=https://en.wikipedia.org/wiki/Subset>subsets</a> provide a system-agnostic basis for evaluating specific sources of NER errors and assessing room for improvement in terms of robustness. We analyze these subsets of challenging tokens in two widely-used NER benchmarks, then exploit them to evaluate NER systems in both in-domain and out-of-domain settings. Results show that these challenging tokens explain the majority of errors made by modern NER systems, although they represent only a small fraction of test tokens. They also indicate that label shift is harder to deal with than unknown words, and that there is much more room for improvement than the standard NER evaluation procedure would suggest. We hope this work will encourage NLP researchers to adopt rigorous and meaningful evaluation methods, and will help them develop more robust models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.442.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--442 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.442 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.lrec-1.442" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.442/>SEDAR : a Large Scale French-English Financial Domain Parallel Corpus<span class=acl-fixed-case>SEDAR</span>: a Large Scale <span class=acl-fixed-case>F</span>rench-<span class=acl-fixed-case>E</span>nglish Financial Domain Parallel Corpus</a></strong><br><a href=/people/a/abbas-ghaddar/>Abbas Ghaddar</a>
|
<a href=/people/p/philippe-langlais/>Phillippe Langlais</a><br><a href=/volumes/2020.lrec-1/ class=text-muted>Proceedings of the 12th Language Resources and Evaluation Conference</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--442><div class="card-body p-3 small">This paper describes the acquisition, preprocessing and characteristics of <a href=https://en.wikipedia.org/wiki/SEDAR>SEDAR</a>, a large scale English-French parallel corpus for the financial domain. Our extensive experiments on <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> show that <a href=https://en.wikipedia.org/wiki/SEDAR>SEDAR</a> is essential to obtain good performance on <a href=https://en.wikipedia.org/wiki/Finance>finance</a>. We observe a large gain in the performance of <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation systems</a> trained on <a href=https://en.wikipedia.org/wiki/SEDAR>SEDAR</a> when tested on finance, which makes <a href=https://en.wikipedia.org/wiki/SEDAR>SEDAR</a> suitable to study domain adaptation for <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a>. The first release of the <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> comprises 8.6 million high quality sentence pairs that are publicly available for research at https://github.com/autorite/sedar-bitext.</div></div><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5513.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5513 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5513 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5513/>Contextualized Word Representations from Distant Supervision with and for NER<span class=acl-fixed-case>NER</span></a></strong><br><a href=/people/a/abbas-ghaddar/>Abbas Ghaddar</a>
|
<a href=/people/p/philippe-langlais/>Phillippe Langlais</a><br><a href=/volumes/D19-55/ class=text-muted>Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT 2019)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5513><div class="card-body p-3 small">We describe a special type of deep contextualized word representation that is learned from distant supervision annotations and dedicated to <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>named entity recognition</a>. Our extensive experiments on 7 <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> show systematic gains across all domains over strong baselines, and demonstrate that our representation is complementary to previously proposed <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a>. We report new state-of-the-art results on CONLL and ONTONOTES datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4002.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4002 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4002 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-4002" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-4002/>WiRe57 : A Fine-Grained Benchmark for Open Information Extraction<span class=acl-fixed-case>W</span>i<span class=acl-fixed-case>R</span>e57 : A Fine-Grained Benchmark for Open Information Extraction</a></strong><br><a href=/people/w/william-lechelle/>William Lechelle</a>
|
<a href=/people/f/fabrizio-gotti/>Fabrizio Gotti</a>
|
<a href=/people/p/philippe-langlais/>Phillippe Langlais</a><br><a href=/volumes/W19-40/ class=text-muted>Proceedings of the 13th Linguistic Annotation Workshop</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4002><div class="card-body p-3 small">We build a <a href=https://en.wikipedia.org/wiki/Reference_work>reference</a> for the task of <a href=https://en.wikipedia.org/wiki/Open_information_extraction>Open Information Extraction</a>, on five documents. We tentatively resolve a number of issues that arise, including <a href=https://en.wikipedia.org/wiki/Coreference>coreference</a> and <a href=https://en.wikipedia.org/wiki/Granularity>granularity</a>, and we take steps toward addressing <a href=https://en.wikipedia.org/wiki/Inference>inference</a>, a significant problem. We seek to better pinpoint the requirements for the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. We produce our annotation guidelines specifying what is correct to extract and what is not. In turn, we use this <a href=https://en.wikipedia.org/wiki/Reference_(computer_science)>reference</a> to score existing Open IE systems. We address the non-trivial problem of evaluating the extractions produced by <a href=https://en.wikipedia.org/wiki/System>systems</a> against the reference tuples, and share our evaluation script. Among seven compared extractors, we find the MinIE system to perform best.</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1122.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1122 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1122 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=C18-1122" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/C18-1122/>Extracting Parallel Sentences with Bidirectional Recurrent Neural Networks to Improve Machine Translation</a></strong><br><a href=/people/f/francis-gregoire/>Francis Gr√©goire</a>
|
<a href=/people/p/philippe-langlais/>Philippe Langlais</a><br><a href=/volumes/C18-1/ class=text-muted>Proceedings of the 27th International Conference on Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1122><div class="card-body p-3 small">Parallel sentence extraction is a task addressing the data sparsity problem found in multilingual natural language processing applications. We propose a bidirectional recurrent neural network based approach to extract parallel sentences from collections of multilingual texts. Our experiments with noisy parallel corpora show that we can achieve promising results against a competitive baseline by removing the need of specific feature engineering or additional external resources. To justify the utility of our approach, we extract <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>sentence pairs</a> from <a href=https://en.wikipedia.org/wiki/Wikipedia>Wikipedia articles</a> to train <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation systems</a> and show significant improvements in <a href=https://en.wikipedia.org/wiki/Translation>translation</a> performance.</div></div><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2501.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2501 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2501 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-2501/>Users and Data : The Two Neglected Children of Bilingual Natural Language Processing Research</a></strong><br><a href=/people/p/philippe-langlais/>Phillippe Langlais</a><br><a href=/volumes/W17-25/ class=text-muted>Proceedings of the 10th Workshop on Building and Using Comparable Corpora</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2501><div class="card-body p-3 small">Despite numerous studies devoted to mining parallel material from bilingual data, we have yet to see the resulting <a href=https://en.wikipedia.org/wiki/Technology>technologies</a> wholeheartedly adopted by professional translators and terminologists alike. I argue that this state of affairs is mainly due to two factors : the emphasis published authors put on <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> (even though data is as important), and the conspicuous lack of concern for actual end-users.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2509.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2509 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2509 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-2509/>BUCC 2017 Shared Task : a First Attempt Toward a Deep Learning Framework for Identifying Parallel Sentences in Comparable Corpora<span class=acl-fixed-case>BUCC</span> 2017 Shared Task: a First Attempt Toward a Deep Learning Framework for Identifying Parallel Sentences in Comparable Corpora</a></strong><br><a href=/people/f/francis-gregoire/>Francis Gr√©goire</a>
|
<a href=/people/p/philippe-langlais/>Philippe Langlais</a><br><a href=/volumes/W17-25/ class=text-muted>Proceedings of the 10th Workshop on Building and Using Comparable Corpora</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2509><div class="card-body p-3 small">This paper describes our participation in BUCC 2017 shared task : identifying parallel sentences in comparable corpora. Our goal is to leverage continuous vector representations and <a href=https://en.wikipedia.org/wiki/Distributional_semantics>distributional semantics</a> with a minimal use of external preprocessing and postprocessing tools. We report experiments that were conducted after transmitting our results.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4812.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4812 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4812 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4812/>Translating Implicit Discourse Connectives Based on Cross-lingual Annotation and Alignment</a></strong><br><a href=/people/h/hongzheng-li/>Hongzheng Li</a>
|
<a href=/people/p/philippe-langlais/>Philippe Langlais</a>
|
<a href=/people/y/yaohong-jin/>Yaohong Jin</a><br><a href=/volumes/W17-48/ class=text-muted>Proceedings of the Third Workshop on Discourse in Machine Translation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4812><div class="card-body p-3 small">Implicit discourse connectives and relations are distributed more widely in Chinese texts, when translating into <a href=https://en.wikipedia.org/wiki/English_language>English</a>, such connectives are usually translated explicitly. Towards Chinese-English MT, in this paper we describe cross-lingual annotation and alignment of dis-course connectives in a parallel corpus, describing related surveys and findings. We then conduct some evaluation experiments to testify the <a href=https://en.wikipedia.org/wiki/Translation>translation of implicit connectives</a> and whether representing implicit connectives explicitly in source language can improve the final <a href=https://en.wikipedia.org/wiki/Translation>translation</a> performance significantly. Preliminary results show it has little improvement by just inserting explicit connectives for implicit relations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2096.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2096 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2096 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2096/>Reranking Translation Candidates Produced by Several Bilingual Word Similarity Sources</a></strong><br><a href=/people/l/laurent-jakubina/>Laurent Jakubina</a>
|
<a href=/people/p/philippe-langlais/>Phillippe Langlais</a><br><a href=/volumes/E17-2/ class=text-muted>Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2096><div class="card-body p-3 small">We investigate the <a href=https://en.wikipedia.org/wiki/Ranking>reranking</a> of the output of several distributional approaches on the Bilingual Lexicon Induction task. We show that reranking an n-best list produced by any of those approaches leads to very substantial improvements. We further demonstrate that combining several n-best lists by <a href=https://en.wikipedia.org/wiki/Ranking>reranking</a> is an effective way of further boosting performance.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Philippe+Langlais" title="Search for 'Philippe Langlais' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/f/francis-gregoire/ class=align-middle>Francis Gr√©goire</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/a/abbas-ghaddar/ class=align-middle>Abbas Ghaddar</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/g/gabriel-bernier-colborne/ class=align-middle>Gabriel Bernier-Colborne</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/o/olivier-salaun/ class=align-middle>Olivier Sala√ºn</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/karim-benyekhlef/ class=align-middle>Karim Benyekhlef</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/h/hongzheng-li/ class=align-middle>Hongzheng Li</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yaohong-jin/ class=align-middle>Yaohong Jin</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/w/william-lechelle/ class=align-middle>William Lechelle</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/f/fabrizio-gotti/ class=align-middle>Fabrizio Gotti</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/martin-laville/ class=align-middle>Martin Laville</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/amir-hazem/ class=align-middle>Amir Hazem</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/e/emmanuel-morin/ class=align-middle>Emmanuel Morin</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/shivendra-bhardwaj/ class=align-middle>Shivendra Bhardwaj</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/david-alfonso-hermelo/ class=align-middle>David Alfonso Hermelo</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/cyril-goutte/ class=align-middle>Cyril Goutte</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/michel-simard/ class=align-middle>Michel Simard</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/laurent-jakubina/ class=align-middle>Laurent Jakubina</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/ws/ class=align-middle>WS</a><span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/venues/coling/ class=align-middle>COLING</a><span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/venues/lrec/ class=align-middle>LREC</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/ranlp/ class=align-middle>RANLP</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/emnlp/ class=align-middle>EMNLP</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-venues aria-expanded=false aria-controls=more-venues>show all...</li><div class="collapse border-top" id=more-venues><li class=list-group-item><a href=/venues/eacl/ class=align-middle>EACL</a><span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ¬©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>