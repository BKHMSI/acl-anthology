<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Percy Liang - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Percy</span> <span class=font-weight-bold>Liang</span></h2><hr><div class=row><div class=col-lg-9><h4>2022</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.acl-long.551.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--acl-long--551 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.acl-long.551 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.acl-long.551" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2022.acl-long.551/>LinkBERT Pretraining Language Models with Document Links<span class=acl-fixed-case>L</span>ink<span class=acl-fixed-case>BERT</span>: Pretraining Language Models with Document Links</a></strong><br><a href=/people/m/michihiro-yasunaga/>Michihiro Yasunaga</a>
|
<a href=/people/j/jure-leskovec/>Jure Leskovec</a>
|
<a href=/people/p/percy-liang/>Percy Liang</a><br><a href=/volumes/2022.acl-long/ class=text-muted>Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--acl-long--551><div class="card-body p-3 small">Language model LM pretraining captures various knowledge from <a href=https://en.wikipedia.org/wiki/Text_corpus>text corpora</a> helping downstream tasks However existing methods such as BERT model a single document and do not capture <a href=https://en.wikipedia.org/wiki/Coupling_(computer_programming)>dependencies</a> or knowledge that span across documents In this work we propose LinkBERT an LM pretraining method that leverages links between documents e.g. <a href=https://en.wikipedia.org/wiki/Hyperlink>hyperlinks</a> Given a <a href=https://en.wikipedia.org/wiki/Text_corpus>text corpus</a> we view it as a graph of documents and create LM inputs by placing linked documents in the same context We then pretrain the LM with two joint self supervised objectives masked language modeling and our new proposal document relation prediction We show that LinkBERT outperforms BERT on various downstream tasks across two domains the general domain pretrained on <a href=https://en.wikipedia.org/wiki/Wikipedia>Wikipedia</a> with <a href=https://en.wikipedia.org/wiki/Hyperlink>hyperlinks</a> and biomedical domain pretrained on <a href=https://en.wikipedia.org/wiki/PubMed>PubMed</a> with citation links LinkBERT is especially effective for multi hop reasoning and few shot QA +5 absolute improvement on HotpotQA and TriviaQA and our biomedical LinkBERT sets new states of the art on various BioNLP tasks +7 on BioASQ and USMLE We release our pretrained models LinkBERT and BioLinkBERT as well as code and data</div></div><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.611.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--611 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.611 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.611" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.611/>LM-Critic : Language Models for Unsupervised Grammatical Error Correction<span class=acl-fixed-case>LM</span>-Critic: Language Models for Unsupervised Grammatical Error Correction</a></strong><br><a href=/people/m/michihiro-yasunaga/>Michihiro Yasunaga</a>
|
<a href=/people/j/jure-leskovec/>Jure Leskovec</a>
|
<a href=/people/p/percy-liang/>Percy Liang</a><br><a href=/volumes/2021.emnlp-main/ class=text-muted>Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--611><div class="card-body p-3 small">Grammatical error correction (GEC) requires a set of labeled ungrammatical / grammatical sentence pairs for training, but obtaining such annotation can be prohibitively expensive. Recently, the Break-It-Fix-It (BIFI) framework has demonstrated strong results on learning to repair a broken program without any labeled examples, but this relies on a perfect critic (e.g., a compiler) that returns whether an example is valid or not, which does not exist for the GEC task. In this work, we show how to leverage a pretrained language model (LM) in defining an LM-Critic, which judges a sentence to be grammatical if the LM assigns it a higher probability than its local perturbations. We apply this LM-Critic and BIFI along with a large set of unlabeled sentences to bootstrap realistic ungrammatical / grammatical pairs for training a corrector. We evaluate our approach on GEC datasets on multiple domains (CoNLL-2014, BEA-2019, GMEG-wiki and GMEG-yahoo) and show that it outperforms existing methods in both the <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised setting</a> (+7.7 F0.5) and the <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised setting</a> (+0.5 F0.5).</div></div><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.acl-main.503.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--acl-main--503 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.acl-main.503 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://slideslive.com/38929409 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.acl-main.503" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.acl-main.503/>Selective Question Answering under Domain Shift</a></strong><br><a href=/people/a/amita-kamath/>Amita Kamath</a>
|
<a href=/people/r/robin-jia/>Robin Jia</a>
|
<a href=/people/p/percy-liang/>Percy Liang</a><br><a href=/volumes/2020.acl-main/ class=text-muted>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--acl-main--503><div class="card-body p-3 small">To avoid giving wrong answers, question answering (QA) models need to know when to abstain from answering. Moreover, users often ask questions that diverge from the <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a>&#8217;s training data, making errors more likely and thus abstention more critical. In this work, we propose the setting of selective question answering under domain shift, in which a QA model is tested on a mixture of in-domain and out-of-domain data, and must answer (i.e., not abstain on) as many questions as possible while maintaining high accuracy. Abstention policies based solely on the model&#8217;s softmax probabilities fare poorly, since models are overconfident on out-of-domain inputs. Instead, we train a <a href=https://en.wikipedia.org/wiki/Calibration>calibrator</a> to identify inputs on which the <a href=https://en.wikipedia.org/wiki/Quantitative_analysis_(finance)>QA model</a> errs, and abstain when it predicts an error is likely. Crucially, the calibrator benefits from observing the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>&#8217;s behavior on out-of-domain data, even if from a different domain than the test data. We combine this method with a SQuAD-trained QA model and evaluate on mixtures of SQuAD and five other QA datasets. Our method answers 56 % of questions while maintaining 80 % <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> ; in contrast, directly using the model&#8217;s probabilities only answers 48 % at 80 % accuracy.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.305.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--305 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.305 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.305" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.305/>On the Importance of Adaptive Data Collection for Extremely Imbalanced Pairwise Tasks<span class=acl-fixed-case>O</span>n the <span class=acl-fixed-case>I</span>mportance of <span class=acl-fixed-case>A</span>daptive <span class=acl-fixed-case>D</span>ata <span class=acl-fixed-case>C</span>ollection for <span class=acl-fixed-case>E</span>xtremely <span class=acl-fixed-case>I</span>mbalanced <span class=acl-fixed-case>P</span>airwise <span class=acl-fixed-case>T</span>asks</a></strong><br><a href=/people/s/stephen-mussmann/>Stephen Mussmann</a>
|
<a href=/people/r/robin-jia/>Robin Jia</a>
|
<a href=/people/p/percy-liang/>Percy Liang</a><br><a href=/volumes/2020.findings-emnlp/ class=text-muted>Findings of the Association for Computational Linguistics: EMNLP 2020</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--305><div class="card-body p-3 small">Many pairwise classification tasks, such as <a href=https://en.wikipedia.org/wiki/Paraphrase_detection>paraphrase detection</a> and <a href=https://en.wikipedia.org/wiki/Open-domain_question_answering>open-domain question answering</a>, naturally have extreme label imbalance (e.g., 99.99 % of examples are negatives). In contrast, many recent <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> heuristically choose examples to ensure label balance. We show that these heuristics lead to trained <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> that generalize poorly : State-of-the art <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> trained on <a href=https://en.wikipedia.org/wiki/QQP>QQP</a> and WikiQA each have only 2.4 % average precision when evaluated on realistically imbalanced test data. We instead collect training data with <a href=https://en.wikipedia.org/wiki/Active_learning_(machine_learning)>active learning</a>, using a BERT-based embedding model to efficiently retrieve uncertain points from a very large pool of unlabeled utterance pairs. By creating balanced training data with more informative negative examples, <a href=https://en.wikipedia.org/wiki/Active_learning>active learning</a> greatly improves <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>average precision</a> to 32.5 % on <a href=https://en.wikipedia.org/wiki/QQP>QQP</a> and 20.1 % on WikiQA.</div></div><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1423.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1423 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1423 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1423.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1423" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1423/>Certified Robustness to Adversarial Word Substitutions</a></strong><br><a href=/people/r/robin-jia/>Robin Jia</a>
|
<a href=/people/a/aditi-raghunathan/>Aditi Raghunathan</a>
|
<a href=/people/k/kerem-goksel/>Kerem Göksel</a>
|
<a href=/people/p/percy-liang/>Percy Liang</a><br><a href=/volumes/D19-1/ class=text-muted>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1423><div class="card-body p-3 small">State-of-the-art <a href=https://en.wikipedia.org/wiki/Neuro-linguistic_programming>NLP models</a> can often be fooled by adversaries that apply seemingly innocuous label-preserving transformations (e.g., <a href=https://en.wikipedia.org/wiki/Paraphrase>paraphrasing</a>) to input text. The number of possible <a href=https://en.wikipedia.org/wiki/Transformation_(function)>transformations</a> scales exponentially with text length, so <a href=https://en.wikipedia.org/wiki/Data_augmentation>data augmentation</a> can not cover all <a href=https://en.wikipedia.org/wiki/Transformation_(function)>transformations</a> of an input. This paper considers one exponentially large family of label-preserving transformations, in which every word in the input can be replaced with a similar word. We train the first <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> that are provably robust to all word substitutions in this family. Our training procedure uses Interval Bound Propagation (IBP) to minimize an upper bound on the worst-case loss that any combination of word substitutions can induce. To evaluate models&#8217; robustness to these transformations, we measure <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> on adversarially chosen word substitutions applied to test examples. Our IBP-trained models attain 75 % adversarial accuracy on both <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> on <a href=https://en.wikipedia.org/wiki/IMDB>IMDB</a> and natural language inference on SNLI ; in comparison, on <a href=https://en.wikipedia.org/wiki/IMDB>IMDB</a>, models trained normally and ones trained with data augmentation achieve adversarial accuracy of only 12 % and 41 %, respectively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1432.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1432 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1432 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1432" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1432/>Distributionally Robust Language Modeling</a></strong><br><a href=/people/y/yonatan-oren/>Yonatan Oren</a>
|
<a href=/people/s/shiori-sagawa/>Shiori Sagawa</a>
|
<a href=/people/t/tatsunori-b-hashimoto/>Tatsunori B. Hashimoto</a>
|
<a href=/people/p/percy-liang/>Percy Liang</a><br><a href=/volumes/D19-1/ class=text-muted>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1432><div class="card-body p-3 small">Language models are generally trained on <a href=https://en.wikipedia.org/wiki/Data>data</a> spanning a wide range of topics (e.g., <a href=https://en.wikipedia.org/wiki/News>news</a>, <a href=https://en.wikipedia.org/wiki/Review>reviews</a>, <a href=https://en.wikipedia.org/wiki/Fiction>fiction</a>), but they might be applied to an a priori unknown target distribution (e.g., restaurant reviews). In this paper, we first show that training on text outside the test distribution can degrade test performance when using standard maximum likelihood (MLE) training. To remedy this without the knowledge of the test distribution, we propose an approach which trains a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> that performs well over a wide range of potential test distributions. In particular, we derive a new distributionally robust optimization (DRO) procedure which minimizes the loss of the model over the worst-case mixture of topics with sufficient overlap with the training distribution. Our approach, called topic conditional value at risk (topic CVaR), obtains a 5.5 point perplexity reduction over <a href=https://en.wikipedia.org/wiki/Model-driven_engineering>MLE</a> when the language models are trained on a mixture of Yelp reviews and news and tested only on <a href=https://en.wikipedia.org/wiki/Review>reviews</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1172.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1172 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1172 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/359670150 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1172" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1172/>Pun Generation with Surprise</a></strong><br><a href=/people/h/he-he/>He He</a>
|
<a href=/people/n/nanyun-peng/>Nanyun Peng</a>
|
<a href=/people/p/percy-liang/>Percy Liang</a><br><a href=/volumes/N19-1/ class=text-muted>Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1172><div class="card-body p-3 small">We tackle the problem of generating a pun sentence given a pair of homophones (e.g., died and dyed). Puns are by their very nature statistically anomalous and not amenable to most text generation methods that are supervised by a <a href=https://en.wikipedia.org/wiki/Text_corpus>large corpus</a>. In this paper, we propose an <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised approach</a> to <a href=https://en.wikipedia.org/wiki/Pun>pun generation</a> based on lots of raw (unhumorous) text and a surprisal principle. Specifically, we posit that in a pun sentence, there is a strong association between the pun word (e.g., dyed) and the <a href=https://en.wikipedia.org/wiki/Context_(language_use)>distant context</a>, but a strong association between the alternative word (e.g., died) and the <a href=https://en.wikipedia.org/wiki/Context_(language_use)>immediate context</a>. We instantiate the surprisal principle in two ways : (i) as a measure based on the ratio of probabilities given by a <a href=https://en.wikipedia.org/wiki/Language_model>language model</a>, and (ii) a retrieve-and-edit approach based on words suggested by a skip-gram model. Based on human evaluation, our retrieve-and-edit approach generates puns successfully 30 % of the time, doubling the success rate of a neural generation baseline.</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1256.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1256 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1256 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D18-1256.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/305940786 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1256" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1256/>Decoupling Strategy and Generation in Negotiation Dialogues</a></strong><br><a href=/people/h/he-he/>He He</a>
|
<a href=/people/d/derek-chen/>Derek Chen</a>
|
<a href=/people/a/anusha-balakrishnan/>Anusha Balakrishnan</a>
|
<a href=/people/p/percy-liang/>Percy Liang</a><br><a href=/volumes/D18-1/ class=text-muted>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1256><div class="card-body p-3 small">We consider negotiation settings in which two agents use <a href=https://en.wikipedia.org/wiki/Natural_language>natural language</a> to bargain on goods. Agents need to decide on both high-level strategy (e.g., proposing $ 50) and the execution of that strategy (e.g., generating The bike is brand new. Selling for just $ 50 !). Recent work on <a href=https://en.wikipedia.org/wiki/Negotiation>negotiation</a> trains neural models, but their end-to-end nature makes it hard to control their <a href=https://en.wikipedia.org/wiki/Strategy>strategy</a>, and <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning</a> tends to lead to degenerate solutions. In this paper, we propose a <a href=https://en.wikipedia.org/wiki/Modular_programming>modular approach</a> based on coarse dialogue acts (e.g., propose(price=50)) that decouples <a href=https://en.wikipedia.org/wiki/Strategy_(game_theory)>strategy</a> and generation. We show that we can flexibly set the strategy using <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised learning</a>, <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning</a>, or domain-specific knowledge without degeneracy, while our retrieval-based generation can maintain context-awareness and produce diverse utterances. We test our approach on the recently proposed DEALORNODEAL game, and we also collect a richer dataset based on real items on <a href=https://en.wikipedia.org/wiki/Craigslist>Craigslist</a>. Human evaluation shows that our <a href=https://en.wikipedia.org/wiki/System>systems</a> achieve higher task success rate and more human-like negotiation behavior than previous approaches.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1540.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1540 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1540 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1540" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D18-1540/>Mapping <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language commands</a> to web elements</a></strong><br><a href=/people/p/panupong-pasupat/>Panupong Pasupat</a>
|
<a href=/people/t/tian-shun-jiang/>Tian-Shun Jiang</a>
|
<a href=/people/e/evan-liu/>Evan Liu</a>
|
<a href=/people/k/kelvin-guu/>Kelvin Guu</a>
|
<a href=/people/p/percy-liang/>Percy Liang</a><br><a href=/volumes/D18-1/ class=text-muted>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1540><div class="card-body p-3 small">The <a href=https://en.wikipedia.org/wiki/World_Wide_Web>web</a> provides a rich, open-domain environment with textual, structural, and spatial properties. We propose a new <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> for grounding language in this environment : given a natural language command (e.g., click on the second article), choose the correct element on the web page (e.g., a hyperlink or text box). We collected a <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> of over 50,000 <a href=https://en.wikipedia.org/wiki/Command_(computing)>commands</a> that capture various phenomena such as functional references (e.g. find who made this site), relational reasoning (e.g. article by john), and <a href=https://en.wikipedia.org/wiki/Visual_reasoning>visual reasoning</a> (e.g. top-most article). We also implemented and analyzed three baseline models that capture different phenomena present in the <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/Q18-1031.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-Q18-1031 data-toggle=collapse aria-expanded=false aria-controls=abstract-Q18-1031 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285801187 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=Q18-1031" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/Q18-1031/>Generating Sentences by Editing Prototypes</a></strong><br><a href=/people/k/kelvin-guu/>Kelvin Guu</a>
|
<a href=/people/t/tatsunori-b-hashimoto/>Tatsunori B. Hashimoto</a>
|
<a href=/people/y/yonatan-oren/>Yonatan Oren</a>
|
<a href=/people/p/percy-liang/>Percy Liang</a><br><a href=/volumes/Q18-1/ class=text-muted>Transactions of the Association for Computational Linguistics, Volume 6</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-Q18-1031><div class="card-body p-3 small">We propose a new generative language model for sentences that first samples a prototype sentence from the training corpus and then edits it into a new sentence. Compared to traditional language models that generate from scratch either left-to-right or by first sampling a latent sentence vector, our prototype-then-edit model improves perplexity on language modeling and generates higher quality outputs according to human evaluation. Furthermore, the model gives rise to a latent edit vector that captures interpretable semantics such as sentence similarity and sentence-level analogies.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/Q18-1037.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-Q18-1037 data-toggle=collapse aria-expanded=false aria-controls=abstract-Q18-1037 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=Q18-1037" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/Q18-1037/>Planning, Inference and Pragmatics in Sequential Language Games</a></strong><br><a href=/people/f/fereshte-khani/>Fereshte Khani</a>
|
<a href=/people/n/noah-goodman/>Noah D. Goodman</a>
|
<a href=/people/p/percy-liang/>Percy Liang</a><br><a href=/volumes/Q18-1/ class=text-muted>Transactions of the Association for Computational Linguistics, Volume 6</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-Q18-1037><div class="card-body p-3 small">We study sequential language games in which two players, each with private information, communicate to achieve a common goal. In such games, a successful player must (i) infer the partner&#8217;s private information from the partner&#8217;s messages, (ii) generate messages that are most likely to help with the goal, and (iii) reason pragmatically about the partner&#8217;s strategy. We propose a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> that captures all three characteristics and demonstrate their importance in capturing <a href=https://en.wikipedia.org/wiki/Human_behavior>human behavior</a> on a new goal-oriented dataset we collected using <a href=https://en.wikipedia.org/wiki/Crowdsourcing>crowdsourcing</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1169.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1169 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1169 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N18-1169.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=http://vimeo.com/277673818 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N18-1169" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/N18-1169/>Delete, Retrieve, Generate : a Simple Approach to Sentiment and Style Transfer</a></strong><br><a href=/people/j/juncen-li/>Juncen Li</a>
|
<a href=/people/r/robin-jia/>Robin Jia</a>
|
<a href=/people/h/he-he/>He He</a>
|
<a href=/people/p/percy-liang/>Percy Liang</a><br><a href=/volumes/N18-1/ class=text-muted>Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1169><div class="card-body p-3 small">We consider the task of text attribute transfer : transforming a sentence to alter a specific attribute (e.g., sentiment) while preserving its <a href=https://en.wikipedia.org/wiki/Content_(media)>attribute-independent content</a> (e.g., screen is just the right size to screen is too small). Our training data includes only sentences labeled with their attribute (e.g., positive and negative), but not pairs of sentences that only differ in the attributes, so we must learn to disentangle <a href=https://en.wikipedia.org/wiki/Attribute_(computing)>attributes</a> from attribute-independent content in an unsupervised way. Previous work using <a href=https://en.wikipedia.org/wiki/Adversarial_system>adversarial methods</a> has struggled to produce high-quality outputs. In this paper, we propose simpler <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> motivated by the observation that <a href=https://en.wikipedia.org/wiki/Attribute_(computing)>text attributes</a> are often marked by distinctive phrases (e.g., too small). Our strongest method extracts content words by deleting phrases associated with the sentence&#8217;s original attribute value, retrieves new phrases associated with the target attribute, and uses a neural model to fluently combine these into a final output. Based on human evaluation, our best method generates grammatical and appropriate responses on 22 % more inputs than the best previous system, averaged over three attribute transfer datasets : altering sentiment of reviews on <a href=https://en.wikipedia.org/wiki/Yelp>Yelp</a>, altering sentiment of reviews on <a href=https://en.wikipedia.org/wiki/Amazon_(company)>Amazon</a>, and altering image captions to be more romantic or humorous.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1175.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1175 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1175 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1175.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285804886 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-1175" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-1175/>Training <a href=https://en.wikipedia.org/wiki/Classifier_(linguistics)>Classifiers</a> with Natural Language Explanations</a></strong><br><a href=/people/b/braden-hancock/>Braden Hancock</a>
|
<a href=/people/p/paroma-varma/>Paroma Varma</a>
|
<a href=/people/s/stephanie-wang/>Stephanie Wang</a>
|
<a href=/people/m/martin-bringmann/>Martin Bringmann</a>
|
<a href=/people/p/percy-liang/>Percy Liang</a>
|
<a href=/people/c/christopher-re/>Christopher Ré</a><br><a href=/volumes/P18-1/ class=text-muted>Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1175><div class="card-body p-3 small">Training accurate <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifiers</a> requires many labels, but each label provides only limited information (one bit for binary classification). In this work, we propose BabbleLabble, a framework for training <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifiers</a> in which an annotator provides a <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language explanation</a> for each labeling decision. A <a href=https://en.wikipedia.org/wiki/Semantic_parser>semantic parser</a> converts these explanations into programmatic labeling functions that generate noisy labels for an arbitrary amount of unlabeled data, which is used to train a <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifier</a>. On three relation extraction tasks, we find that users are able to train <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifiers</a> with comparable F1 scores from 5-100 faster by providing explanations instead of just labels. Furthermore, given the inherent imperfection of labeling functions, we find that a simple rule-based semantic parser suffices.</div></div><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1086.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1086 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1086 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234958455 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P17-1086" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-1086/>Naturalizing a <a href=https://en.wikipedia.org/wiki/Programming_language>Programming Language</a> via Interactive Learning</a></strong><br><a href=/people/s/sida-i-wang/>Sida I. Wang</a>
|
<a href=/people/s/samuel-ginn/>Samuel Ginn</a>
|
<a href=/people/p/percy-liang/>Percy Liang</a>
|
<a href=/people/c/christopher-d-manning/>Christopher D. Manning</a><br><a href=/volumes/P17-1/ class=text-muted>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1086><div class="card-body p-3 small">Our goal is to create a convenient <a href=https://en.wikipedia.org/wiki/Natural-language_user_interface>natural language interface</a> for performing well-specified but complex actions such as <a href=https://en.wikipedia.org/wiki/Data_analysis>analyzing data</a>, manipulating text, and <a href=https://en.wikipedia.org/wiki/Database_query>querying databases</a>. However, existing <a href=https://en.wikipedia.org/wiki/Interface_(computing)>natural language interfaces</a> for such <a href=https://en.wikipedia.org/wiki/Task_(computing)>tasks</a> are quite primitive compared to the power one wields with a <a href=https://en.wikipedia.org/wiki/Programming_language>programming language</a>. To bridge this gap, we start with a core programming language and allow users to naturalize the core language incrementally by defining alternative, more natural syntax and increasingly complex concepts in terms of compositions of simpler ones. In a voxel world, we show that a community of users can simultaneously teach a common system a diverse language and use it to build hundreds of complex voxel structures. Over the course of three days, these users went from using only the core language to using the <a href=https://en.wikipedia.org/wiki/Natural_language>naturalized language</a> in 85.9 % of the last 10 K utterances.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1097.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1097 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1097 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234955545 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P17-1097" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-1097/>From Language to Programs : Bridging <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>Reinforcement Learning</a> and Maximum Marginal Likelihood</a></strong><br><a href=/people/k/kelvin-guu/>Kelvin Guu</a>
|
<a href=/people/p/panupong-pasupat/>Panupong Pasupat</a>
|
<a href=/people/e/evan-liu/>Evan Liu</a>
|
<a href=/people/p/percy-liang/>Percy Liang</a><br><a href=/volumes/P17-1/ class=text-muted>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1097><div class="card-body p-3 small">Our goal is to learn a <a href=https://en.wikipedia.org/wiki/Semantic_parser>semantic parser</a> that maps natural language utterances into executable programs when only indirect supervision is available : examples are labeled with the correct execution result, but not the program itself. Consequently, we must search the space of programs for those that output the correct result, while not being misled by spurious programs : incorrect programs that coincidentally output the correct result. We connect two common learning paradigms, <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning (RL)</a> and maximum marginal likelihood (MML), and then present a new <a href=https://en.wikipedia.org/wiki/Machine_learning>learning algorithm</a> that combines the strengths of both. The new <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> guards against spurious programs by combining the systematic search traditionally employed in MML with the randomized exploration of RL, and by updating parameters such that probability is spread more evenly across consistent programs. We apply our learning algorithm to a new neural semantic parser and show significant gains over existing state-of-the-art results on a recent context-dependent semantic parsing task.<i>spurious programs</i>: incorrect programs that coincidentally output the correct\n result. We connect two common learning paradigms, reinforcement learning\n (RL) and maximum marginal likelihood (MML), and then present a new\n learning algorithm that combines the strengths of both. The new algorithm\n guards against spurious programs by combining the systematic search\n traditionally employed in MML with the randomized exploration of RL, and\n by updating parameters such that probability is spread more evenly across\n consistent programs. We apply our learning algorithm to a new neural\n semantic parser and show significant gains over existing state-of-the-art\n results on a recent context-dependent semantic parsing task.\n</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1162.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1162 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1162 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P17-1162.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P17-1162" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P17-1162/>Learning Symmetric Collaborative Dialogue Agents with Dynamic Knowledge Graph Embeddings</a></strong><br><a href=/people/h/he-he/>He He</a>
|
<a href=/people/a/anusha-balakrishnan/>Anusha Balakrishnan</a>
|
<a href=/people/m/mihail-eric/>Mihail Eric</a>
|
<a href=/people/p/percy-liang/>Percy Liang</a><br><a href=/volumes/P17-1/ class=text-muted>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1162><div class="card-body p-3 small">We study a symmetric collaborative dialogue setting in which two agents, each with private knowledge, must strategically communicate to achieve a common goal. The open-ended dialogue state in this <a href=https://en.wikipedia.org/wiki/Setting_(narrative)>setting</a> poses new challenges for existing <a href=https://en.wikipedia.org/wiki/Dialogue_system>dialogue systems</a>. We collected a dataset of 11 K human-human dialogues, which exhibits interesting lexical, semantic, and strategic elements. To model both structured knowledge and unstructured language, we propose a neural model with dynamic knowledge graph embeddings that evolve as the dialogue progresses. Automatic and human evaluations show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is both more effective at achieving the goal and more human-like than baseline neural and rule-based models.<i>symmetric collaborative dialogue</i> setting in which two agents,\n each with private knowledge, must strategically communicate to achieve a\n common goal. The open-ended dialogue state in this setting poses new\n challenges for existing dialogue systems. We collected a dataset of 11K\n human-human dialogues, which exhibits interesting lexical, semantic, and\n strategic elements. To model both structured knowledge and unstructured\n language, we propose a neural model with dynamic knowledge graph\n embeddings that evolve as the dialogue progresses. Automatic and human\n evaluations show that our model is both more effective at achieving the\n goal and more human-like than baseline neural and rule-based models.\n</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1109.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1109 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1109 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/238233745 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D17-1109/>Importance sampling for unbiased on-demand evaluation of knowledge base population</a></strong><br><a href=/people/a/arun-chaganty/>Arun Chaganty</a>
|
<a href=/people/a/ashwin-paranjape/>Ashwin Paranjape</a>
|
<a href=/people/p/percy-liang/>Percy Liang</a>
|
<a href=/people/c/christopher-d-manning/>Christopher D. Manning</a><br><a href=/volumes/D17-1/ class=text-muted>Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1109><div class="card-body p-3 small">Knowledge base population (KBP) systems take in a large document corpus and extract entities and their relations. Thus far, KBP evaluation has relied on judgements on the pooled predictions of existing systems. We show that this <a href=https://en.wikipedia.org/wiki/Evaluation>evaluation</a> is problematic : when a new system predicts a previously unseen relation, it is penalized even if it is correct. This leads to significant bias against new systems, which counterproductively discourages innovation in the field. Our first contribution is a new importance-sampling based evaluation which corrects for this bias by annotating a new system&#8217;s predictions on-demand via <a href=https://en.wikipedia.org/wiki/Crowdsourcing>crowdsourcing</a>. We show this eliminates bias and reduces variance using data from the 2015 TAC KBP task. Our second contribution is an implementation of our <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>method</a> made publicly available as an online KBP evaluation service. We pilot the service by testing diverse state-of-the-art systems on the TAC KBP 2016 corpus and obtain accurate scores in a cost effective manner.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1125.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1125 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1125 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D17-1125.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D17-1125" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D17-1125/>Macro Grammars and Holistic Triggering for Efficient Semantic Parsing</a></strong><br><a href=/people/y/yuchen-zhang/>Yuchen Zhang</a>
|
<a href=/people/p/panupong-pasupat/>Panupong Pasupat</a>
|
<a href=/people/p/percy-liang/>Percy Liang</a><br><a href=/volumes/D17-1/ class=text-muted>Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1125><div class="card-body p-3 small">To learn a <a href=https://en.wikipedia.org/wiki/Semantic_parser>semantic parser</a> from denotations, a learning algorithm must search over a combinatorially large space of logical forms for ones consistent with the annotated denotations. We propose a new online learning algorithm that searches faster as training progresses. The two key ideas are using macro grammars to cache the abstract patterns of useful logical forms found thus far, and holistic triggering to efficiently retrieve the most relevant patterns based on sentence similarity. On the WikiTableQuestions dataset, we first expand the search space of an existing <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> to improve the state-of-the-art <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> from 38.7 % to 42.7 %, and then use <a href=https://en.wikipedia.org/wiki/Macro_(computer_science)>macro grammars</a> and holistic triggering to achieve an 11x speedup and an <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of 43.7 %.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1215.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1215 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1215 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/238231419 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D17-1215" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/D17-1215/>Adversarial Examples for Evaluating Reading Comprehension Systems</a></strong><br><a href=/people/r/robin-jia/>Robin Jia</a>
|
<a href=/people/p/percy-liang/>Percy Liang</a><br><a href=/volumes/D17-1/ class=text-muted>Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1215><div class="card-body p-3 small">Standard accuracy metrics indicate that <a href=https://en.wikipedia.org/wiki/Reading_comprehension>reading comprehension systems</a> are making rapid progress, but the extent to which these <a href=https://en.wikipedia.org/wiki/System>systems</a> truly understand <a href=https://en.wikipedia.org/wiki/Language>language</a> remains unclear. To reward systems with real language understanding abilities, we propose an adversarial evaluation scheme for the Stanford Question Answering Dataset (SQuAD). Our method tests whether systems can answer questions about paragraphs that contain adversarially inserted sentences, which are automatically generated to distract computer systems without changing the correct answer or misleading humans. In this adversarial setting, the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of sixteen published <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> drops from an average of 75 % F1 score to 36 % ; when the adversary is allowed to add ungrammatical sequences of words, average accuracy on four models decreases further to 7 %. We hope our insights will motivate the development of new <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> that understand language more precisely.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Percy+Liang" title="Search for 'Percy Liang' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/r/robin-jia/ class=align-middle>Robin Jia</a>
<span class="badge badge-secondary align-middle ml-2">5</span></li><li class=list-group-item><a href=/people/h/he-he/ class=align-middle>He He</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/people/k/kelvin-guu/ class=align-middle>Kelvin Guu</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/p/panupong-pasupat/ class=align-middle>Panupong Pasupat</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/c/christopher-d-manning/ class=align-middle>Christopher D. Manning</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/e/evan-liu/ class=align-middle>Evan Liu</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/a/anusha-balakrishnan/ class=align-middle>Anusha Balakrishnan</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/m/michihiro-yasunaga/ class=align-middle>Michihiro Yasunaga</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/j/jure-leskovec/ class=align-middle>Jure Leskovec</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/y/yonatan-oren/ class=align-middle>Yonatan Oren</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/t/tatsunori-b-hashimoto/ class=align-middle>Tatsunori B. Hashimoto</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/a/amita-kamath/ class=align-middle>Amita Kamath</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/sida-i-wang/ class=align-middle>Sida I. Wang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/samuel-ginn/ class=align-middle>Samuel Ginn</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/mihail-eric/ class=align-middle>Mihail Eric</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/derek-chen/ class=align-middle>Derek Chen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/tian-shun-jiang/ class=align-middle>Tian-Shun Jiang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/aditi-raghunathan/ class=align-middle>Aditi Raghunathan</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/kerem-goksel/ class=align-middle>Kerem Göksel</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/shiori-sagawa/ class=align-middle>Shiori Sagawa</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/f/fereshte-khani/ class=align-middle>Fereshte Khani</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/n/noah-goodman/ class=align-middle>Noah Goodman</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/arun-chaganty/ class=align-middle>Arun Chaganty</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/ashwin-paranjape/ class=align-middle>Ashwin Paranjape</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yuchen-zhang/ class=align-middle>Yuchen Zhang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/stephen-mussmann/ class=align-middle>Stephen Mussmann</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/n/nanyun-peng/ class=align-middle>Nanyun Peng</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/juncen-li/ class=align-middle>Juncen Li</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/b/braden-hancock/ class=align-middle>Braden Hancock</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/p/paroma-varma/ class=align-middle>Paroma Varma</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/stephanie-wang/ class=align-middle>Stephanie Wang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/martin-bringmann/ class=align-middle>Martin Bringmann</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/christopher-re/ class=align-middle>Christopher Ré</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/emnlp/ class=align-middle>EMNLP</a><span class="badge badge-secondary align-middle ml-2">8</span></li><li class=list-group-item><a href=/venues/acl/ class=align-middle>ACL</a><span class="badge badge-secondary align-middle ml-2">6</span></li><li class=list-group-item><a href=/venues/tacl/ class=align-middle>TACL</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/naacl/ class=align-middle>NAACL</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/findings/ class=align-middle>Findings</a><span class="badge badge-secondary align-middle ml-2">1</span></li></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>