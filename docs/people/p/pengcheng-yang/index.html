<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Pengcheng Yang - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Pengcheng</span> <span class=font-weight-bold>Yang</span></h2><hr><div class=row><div class=col-lg-9><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1336.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1336 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1336 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1336" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1336/>Pun-GAN : Generative Adversarial Network for Pun Generation<span class=acl-fixed-case>GAN</span>: Generative Adversarial Network for Pun Generation</a></strong><br><a href=/people/f/fuli-luo/>Fuli Luo</a>
|
<a href=/people/s/shunyao-li/>Shunyao Li</a>
|
<a href=/people/p/pengcheng-yang/>Pengcheng Yang</a>
|
<a href=/people/l/lei-li/>Lei Li</a>
|
<a href=/people/b/baobao-chang/>Baobao Chang</a>
|
<a href=/people/z/zhifang-sui/>Zhifang Sui</a>
|
<a href=/people/x/xu-sun/>Xu Sun</a><br><a href=/volumes/D19-1/ class=text-muted>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1336><div class="card-body p-3 small">In this paper, we focus on the task of generating a <a href=https://en.wikipedia.org/wiki/Pun>pun sentence</a> given a pair of <a href=https://en.wikipedia.org/wiki/Word_sense>word senses</a>. A major challenge for pun generation is the lack of large-scale pun corpus to guide <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised learning</a>. To remedy this, we propose an adversarial generative network for pun generation (Pun-GAN). It consists of a <a href=https://en.wikipedia.org/wiki/Generator_(mathematics)>generator</a> to produce <a href=https://en.wikipedia.org/wiki/Pun>pun sentences</a>, and a <a href=https://en.wikipedia.org/wiki/Discriminator>discriminator</a> to distinguish between the generated <a href=https://en.wikipedia.org/wiki/Pun>pun sentences</a> and the real sentences with specific word senses. The output of the <a href=https://en.wikipedia.org/wiki/Discriminator>discriminator</a> is then used as a reward to train the generator via <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning</a>, encouraging it to produce pun sentences which can support two word senses simultaneously. Experiments show that the proposed Pun-GAN can generate sentences that are more ambiguous and diverse in both automatic and human evaluation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1553.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1553 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1553 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1553/>Specificity-Driven Cascading Approach for Unsupervised Sentiment Modification</a></strong><br><a href=/people/p/pengcheng-yang/>Pengcheng Yang</a>
|
<a href=/people/j/junyang-lin/>Junyang Lin</a>
|
<a href=/people/j/jingjing-xu/>Jingjing Xu</a>
|
<a href=/people/j/jun-xie/>Jun Xie</a>
|
<a href=/people/q/qi-su/>Qi Su</a>
|
<a href=/people/x/xu-sun/>Xu Sun</a><br><a href=/volumes/D19-1/ class=text-muted>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1553><div class="card-body p-3 small">The task of unsupervised sentiment modification aims to reverse the sentiment polarity of the input text while preserving its semantic content without any parallel data. Most previous work follows a two-step process. They first separate the content from the original sentiment, and then directly generate text with the target sentiment only based on the content produced by the first step. However, the second step bears both the target sentiment addition and content reconstruction, thus resulting in a lack of specific information like proper nouns in the generated text. To remedy this, we propose a specificity-driven cascading approach in this work, which can effectively increase the specificity of the generated text and further improve content preservation. In addition, we propose a more reasonable <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metric</a> to evaluate <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment modification</a>. The experiments show that our approach outperforms competitive baselines by a large margin, which achieves 11 % and 38 % relative improvements of the overall metric on the Yelp and Amazon datasets, respectively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1197.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1197 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1197 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1197" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-1197/>Key Fact as Pivot : A Two-Stage Model for Low Resource Table-to-Text Generation</a></strong><br><a href=/people/s/shuming-ma/>Shuming Ma</a>
|
<a href=/people/p/pengcheng-yang/>Pengcheng Yang</a>
|
<a href=/people/t/tianyu-liu/>Tianyu Liu</a>
|
<a href=/people/p/peng-li/>Peng Li</a>
|
<a href=/people/j/jie-zhou/>Jie Zhou</a>
|
<a href=/people/x/xu-sun/>Xu Sun</a><br><a href=/volumes/P19-1/ class=text-muted>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1197><div class="card-body p-3 small">Table-to-text generation aims to translate the <a href=https://en.wikipedia.org/wiki/Data_structure>structured data</a> into the <a href=https://en.wikipedia.org/wiki/Unstructured_data>unstructured text</a>. Most existing methods adopt the encoder-decoder framework to learn the <a href=https://en.wikipedia.org/wiki/Transformation_(function)>transformation</a>, which requires large-scale training samples. However, the lack of large parallel data is a major practical problem for many domains. In this work, we consider the scenario of low resource table-to-text generation, where only limited parallel data is available. We propose a novel <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> to separate the generation into two stages : key fact prediction and surface realization. It first predicts the key facts from the tables, and then generates the text with the key facts. The training of key fact prediction needs much fewer annotated data, while surface realization can be trained with pseudo parallel corpus. We evaluate our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> on a biography generation dataset. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> can achieve 27.34 BLEU score with only 1,000 <a href=https://en.wikipedia.org/wiki/Parallel_computing>parallel data</a>, while the baseline model only obtain the performance of 9.71 BLEU score.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1257.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1257 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1257 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/384731731 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1257" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1257/>Cross-Modal Commentator : Automatic Machine Commenting Based on Cross-Modal Information</a></strong><br><a href=/people/p/pengcheng-yang/>Pengcheng Yang</a>
|
<a href=/people/z/zhihan-zhang/>Zhihan Zhang</a>
|
<a href=/people/f/fuli-luo/>Fuli Luo</a>
|
<a href=/people/l/lei-li/>Lei Li</a>
|
<a href=/people/c/chengyang-huang/>Chengyang Huang</a>
|
<a href=/people/x/xu-sun/>Xu Sun</a><br><a href=/volumes/P19-1/ class=text-muted>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1257><div class="card-body p-3 small">Automatic commenting of online articles can provide additional opinions and facts to the reader, which improves user experience and engagement on social media platforms. Previous work focuses on automatic commenting based solely on <a href=https://en.wikipedia.org/wiki/Text_(literary_theory)>textual content</a>. However, in real-scenarios, online articles usually contain multiple modal contents. For instance, graphic news contains plenty of images in addition to text. Contents other than text are also vital because they are not only more attractive to the reader but also may provide critical information. To remedy this, we propose a new task : cross-model automatic commenting (CMAC), which aims to make comments by integrating multiple modal contents. We construct a <a href=https://en.wikipedia.org/wiki/Data_set>large-scale dataset</a> for this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> and explore several representative methods. Going a step further, an effective co-attention model is presented to capture the dependency between textual and visual information. Evaluation results show that our proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> can achieve better performance than competitive <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1518.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1518 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1518 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1518" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-1518/>A Deep Reinforced Sequence-to-Set Model for Multi-Label Classification</a></strong><br><a href=/people/p/pengcheng-yang/>Pengcheng Yang</a>
|
<a href=/people/f/fuli-luo/>Fuli Luo</a>
|
<a href=/people/s/shuming-ma/>Shuming Ma</a>
|
<a href=/people/j/junyang-lin/>Junyang Lin</a>
|
<a href=/people/x/xu-sun/>Xu Sun</a><br><a href=/volumes/P19-1/ class=text-muted>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1518><div class="card-body p-3 small">Multi-label classification (MLC) aims to predict a set of labels for a given instance. Based on a pre-defined label order, the sequence-to-sequence (Seq2Seq) model trained via maximum likelihood estimation method has been successfully applied to the MLC task and shows powerful ability to capture high-order correlations between labels. However, the output labels are essentially an <a href=https://en.wikipedia.org/wiki/Unordered_set>unordered set</a> rather than an <a href=https://en.wikipedia.org/wiki/List_of_order_structures_in_mathematics>ordered sequence</a>. This inconsistency tends to result in some <a href=https://en.wikipedia.org/wiki/Intrinsic_and_extrinsic_properties_(philosophy)>intractable problems</a>, e.g., sensitivity to the label order. To remedy this, we propose a simple but effective sequence-to-set model. The proposed model is trained via <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning</a>, where <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reward feedback</a> is designed to be independent of the label order. In this way, we can reduce the dependence of the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> on the label order, as well as capture high-order correlations between labels. Extensive experiments show that our approach can substantially outperform competitive baselines, as well as effectively reduce the sensitivity to the label order.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1603.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1603 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1603 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-1603/>Learning to Control the Fine-grained Sentiment for Story Ending Generation</a></strong><br><a href=/people/f/fuli-luo/>Fuli Luo</a>
|
<a href=/people/d/damai-dai/>Damai Dai</a>
|
<a href=/people/p/pengcheng-yang/>Pengcheng Yang</a>
|
<a href=/people/t/tianyu-liu/>Tianyu Liu</a>
|
<a href=/people/b/baobao-chang/>Baobao Chang</a>
|
<a href=/people/z/zhifang-sui/>Zhifang Sui</a>
|
<a href=/people/x/xu-sun/>Xu Sun</a><br><a href=/volumes/P19-1/ class=text-muted>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1603><div class="card-body p-3 small">Automatic story ending generation is an interesting and challenging task in <a href=https://en.wikipedia.org/wiki/Natural-language_generation>natural language generation</a>. Previous studies are mainly limited to generate coherent, reasonable and diversified story endings, and few works focus on controlling the sentiment of story endings. This paper focuses on generating a story ending which meets the given fine-grained sentiment intensity. There are two major challenges to this task. First is the lack of story corpus which has fine-grained sentiment labels. Second is the difficulty of explicitly controlling sentiment intensity when generating <a href=https://en.wikipedia.org/wiki/Ending_(linguistics)>endings</a>. Therefore, we propose a generic and novel framework which consists of a <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analyzer</a> and a sentimental generator, respectively addressing the two challenges. The sentiment analyzer adopts a series of methods to acquire <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment intensities</a> of the <a href=https://en.wikipedia.org/wiki/Story_arc>story dataset</a>. The sentimental generator introduces the sentiment intensity into decoder via a Gaussian Kernel Layer to control the sentiment of the output. To the best of our knowledge, this is the first endeavor to control the fine-grained sentiment for story ending generation without manually annotating sentiment labels. Experiments show that our proposed <a href=https://en.wikipedia.org/wiki/Conceptual_framework>framework</a> can generate story endings which are not only more coherent and fluent but also able to meet the given sentiment intensity better.</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1330.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1330 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1330 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=C18-1330" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/C18-1330/>SGM : Sequence Generation Model for Multi-label Classification<span class=acl-fixed-case>SGM</span>: Sequence Generation Model for Multi-label Classification</a></strong><br><a href=/people/p/pengcheng-yang/>Pengcheng Yang</a>
|
<a href=/people/x/xu-sun/>Xu Sun</a>
|
<a href=/people/w/wei-li/>Wei Li</a>
|
<a href=/people/s/shuming-ma/>Shuming Ma</a>
|
<a href=/people/w/wei-wu/>Wei Wu</a>
|
<a href=/people/h/houfeng-wang/>Houfeng Wang</a><br><a href=/volumes/C18-1/ class=text-muted>Proceedings of the 27th International Conference on Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1330><div class="card-body p-3 small">Multi-label classification is an important yet challenging task in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a>. It is more complex than single-label classification in that the labels tend to be correlated. Existing methods tend to ignore the correlations between labels. Besides, different parts of the text can contribute differently for predicting different labels, which is not considered by existing <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>. In this paper, we propose to view the multi-label classification task as a sequence generation problem, and apply a sequence generation model with a novel decoder structure to solve it. Extensive experimental results show that our proposed <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> outperform previous <a href=https://en.wikipedia.org/wiki/Work_(physics)>work</a> by a substantial margin. Further analysis of experimental results demonstrates that the proposed methods not only capture the correlations between labels, but also select the most informative words automatically when predicting different labels.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2079.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2079 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2079 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2079.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-2079" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P18-2079/>Automatic Academic Paper Rating Based on Modularized Hierarchical Convolutional Neural Network</a></strong><br><a href=/people/p/pengcheng-yang/>Pengcheng Yang</a>
|
<a href=/people/x/xu-sun/>Xu Sun</a>
|
<a href=/people/w/wei-li/>Wei Li</a>
|
<a href=/people/s/shuming-ma/>Shuming Ma</a><br><a href=/volumes/P18-2/ class=text-muted>Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2079><div class="card-body p-3 small">As more and more academic papers are being submitted to conferences and journals, evaluating all these papers by professionals is time-consuming and can cause inequality due to the personal factors of the reviewers. In this paper, in order to assist professionals in evaluating academic papers, we propose a novel task : automatic academic paper rating (AAPR), which automatically determine whether to accept academic papers. We build a new <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> for this task and propose a novel modularized hierarchical convolutional neural network to achieve automatic academic paper rating. Evaluation results show that the proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms the <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a> by a large margin. The dataset and code are available at<url>https://github.com/lancopku/AAPR</url>\n</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Pengcheng+Yang" title="Search for 'Pengcheng Yang' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/x/xu-sun/ class=align-middle>Xu Sun</a>
<span class="badge badge-secondary align-middle ml-2">8</span></li><li class=list-group-item><a href=/people/s/shuming-ma/ class=align-middle>Shuming Ma</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/people/f/fuli-luo/ class=align-middle>Fuli Luo</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/people/w/wei-li/ class=align-middle>Wei Li</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/l/lei-li/ class=align-middle>Lei Li</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/b/baobao-chang/ class=align-middle>Baobao Chang</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/z/zhifang-sui/ class=align-middle>Zhifang Sui</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/j/junyang-lin/ class=align-middle>Junyang Lin</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/t/tianyu-liu/ class=align-middle>Tianyu Liu</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/w/wei-wu/ class=align-middle>Wei Wu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/houfeng-wang/ class=align-middle>Houfeng Wang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/shunyao-li/ class=align-middle>Shunyao Li</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jingjing-xu/ class=align-middle>Jingjing Xu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jun-xie/ class=align-middle>Jun Xie</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/q/qi-su/ class=align-middle>Qi Su</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/p/peng-li/ class=align-middle>Peng Li</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jie-zhou/ class=align-middle>Jie Zhou</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/zhihan-zhang/ class=align-middle>Zhihan Zhang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/chengyang-huang/ class=align-middle>Chengyang Huang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/damai-dai/ class=align-middle>Damai Dai</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/acl/ class=align-middle>ACL</a><span class="badge badge-secondary align-middle ml-2">5</span></li><li class=list-group-item><a href=/venues/emnlp/ class=align-middle>EMNLP</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/coling/ class=align-middle>COLING</a><span class="badge badge-secondary align-middle ml-2">1</span></li></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>