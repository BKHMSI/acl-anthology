<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Peng Xu - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Peng</span> <span class=font-weight-bold>Xu</span></h2><hr><div class=row><div class=col-lg-9><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.dialdoc-1.6.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--dialdoc-1--6 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.dialdoc-1.6 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.dialdoc-1.6" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.dialdoc-1.6/>CAiRE in DialDoc21 : Data Augmentation for Information Seeking Dialogue System<span class=acl-fixed-case>CA</span>i<span class=acl-fixed-case>RE</span> in <span class=acl-fixed-case>D</span>ial<span class=acl-fixed-case>D</span>oc21: Data Augmentation for Information Seeking Dialogue System</a></strong><br><a href=/people/y/yan-xu/>Yan Xu</a>
|
<a href=/people/e/etsuko-ishii/>Etsuko Ishii</a>
|
<a href=/people/g/genta-indra-winata/>Genta Indra Winata</a>
|
<a href=/people/z/zhaojiang-lin/>Zhaojiang Lin</a>
|
<a href=/people/a/andrea-madotto/>Andrea Madotto</a>
|
<a href=/people/z/zihan-liu/>Zihan Liu</a>
|
<a href=/people/p/peng-xu/>Peng Xu</a>
|
<a href=/people/p/pascale-fung/>Pascale Fung</a><br><a href=/volumes/2021.dialdoc-1/ class=text-muted>Proceedings of the 1st Workshop on Document-grounded Dialogue and Conversational Question Answering (DialDoc 2021)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--dialdoc-1--6><div class="card-body p-3 small">Information-seeking dialogue systems, including knowledge identification and response generation, aim to respond to users with fluent, coherent, and informative responses based on users&#8217; needs, which. To tackle this challenge, we utilize data augmentation methods and several training techniques with the pre-trained language models to learn a general pattern of the task and thus achieve promising performance. In DialDoc21 competition, our <a href=https://en.wikipedia.org/wiki/System>system</a> achieved 74.95 F1 score and 60.74 Exact Match score in subtask 1, and 37.72 SacreBLEU score in subtask 2. Empirical analysis is provided to explain the effectiveness of our approaches.</div></div><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.226.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--226 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.226 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938958 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.226/>MEGATRON-CNTRL : Controllable Story Generation with External Knowledge Using Large-Scale Language Models<span class=acl-fixed-case>MEGATRON</span>-<span class=acl-fixed-case>CNTRL</span>: Controllable Story Generation with External Knowledge Using Large-Scale Language Models</a></strong><br><a href=/people/p/peng-xu/>Peng Xu</a>
|
<a href=/people/m/mostofa-patwary/>Mostofa Patwary</a>
|
<a href=/people/m/mohammad-shoeybi/>Mohammad Shoeybi</a>
|
<a href=/people/r/raul-puri/>Raul Puri</a>
|
<a href=/people/p/pascale-fung/>Pascale Fung</a>
|
<a href=/people/a/animashree-anandkumar/>Anima Anandkumar</a>
|
<a href=/people/b/bryan-catanzaro/>Bryan Catanzaro</a><br><a href=/volumes/2020.emnlp-main/ class=text-muted>Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--226><div class="card-body p-3 small">Existing pre-trained large language models have shown unparalleled generative capabilities. However, <a href=https://en.wikipedia.org/wiki/Copula_(linguistics)>they</a> are not controllable. In this paper, we propose MEGATRON-CNTRL, a novel framework that uses large-scale language models and adds control to text generation by incorporating an external knowledge base. Our framework consists of a keyword predictor, a <a href=https://en.wikipedia.org/wiki/Knowledge_retrieval>knowledge retriever</a>, a contextual knowledge ranker, and a conditional text generator. As we do not have access to ground-truth supervision for the knowledge ranker, we make use of weak supervision from <a href=https://en.wikipedia.org/wiki/Sentence_embedding>sentence embedding</a>. The empirical results show that our model generates more fluent, consistent, and coherent stories with less <a href=https://en.wikipedia.org/wiki/Repetition_(rhetorical_device)>repetition</a> and higher <a href=https://en.wikipedia.org/wiki/Multiculturalism>diversity</a> compared to prior work on the ROC story dataset. We showcase the controllability of our <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> by replacing the <a href=https://en.wikipedia.org/wiki/Index_term>keywords</a> used to generate stories and re-running the generation process. Human evaluation results show that 77.5 % of these stories are successfully controlled by the new <a href=https://en.wikipedia.org/wiki/Index_term>keywords</a>. Furthermore, by scaling our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> from 124 million to 8.3 billion parameters we demonstrate that larger models improve both the <a href=https://en.wikipedia.org/wiki/Quality_(business)>quality of generation</a> (from 74.5 % to 93.0 % for <a href=https://en.wikipedia.org/wiki/Consistency_(statistics)>consistency</a>) and <a href=https://en.wikipedia.org/wiki/Controllability>controllability</a> (from 77.5 % to 91.5 %).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.acl-main.348.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--acl-main--348 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.acl-main.348 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://slideslive.com/38928739 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.acl-main.348" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.acl-main.348/>Meta-Transfer Learning for Code-Switched Speech Recognition</a></strong><br><a href=/people/g/genta-indra-winata/>Genta Indra Winata</a>
|
<a href=/people/s/samuel-cahyawijaya/>Samuel Cahyawijaya</a>
|
<a href=/people/z/zhaojiang-lin/>Zhaojiang Lin</a>
|
<a href=/people/z/zihan-liu/>Zihan Liu</a>
|
<a href=/people/p/peng-xu/>Peng Xu</a>
|
<a href=/people/p/pascale-fung/>Pascale Fung</a><br><a href=/volumes/2020.acl-main/ class=text-muted>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--acl-main--348><div class="card-body p-3 small">An increasing number of people in the world today speak a <a href=https://en.wikipedia.org/wiki/Mixed_language>mixed-language</a> as a result of being multilingual. However, building a <a href=https://en.wikipedia.org/wiki/Speech_recognition>speech recognition system</a> for <a href=https://en.wikipedia.org/wiki/Code-switching>code-switching</a> remains difficult due to the availability of limited resources and the expense and significant effort required to collect mixed-language data. We therefore propose a new learning method, meta-transfer learning, to transfer learn on a code-switched speech recognition system in a low-resource setting by judiciously extracting information from high-resource monolingual datasets. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> learns to recognize individual languages, and transfer them so as to better recognize mixed-language speech by conditioning the <a href=https://en.wikipedia.org/wiki/Mathematical_optimization>optimization</a> on the code-switching data. Based on experimental results, our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms existing <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a> on <a href=https://en.wikipedia.org/wiki/Speech_recognition>speech recognition and language modeling tasks</a>, and is faster to converge.</div></div><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1012.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1012 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1012 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1012" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1012/>MoEL : Mixture of Empathetic Listeners<span class=acl-fixed-case>M</span>o<span class=acl-fixed-case>EL</span>: Mixture of Empathetic Listeners</a></strong><br><a href=/people/z/zhaojiang-lin/>Zhaojiang Lin</a>
|
<a href=/people/a/andrea-madotto/>Andrea Madotto</a>
|
<a href=/people/j/jamin-shin/>Jamin Shin</a>
|
<a href=/people/p/peng-xu/>Peng Xu</a>
|
<a href=/people/p/pascale-fung/>Pascale Fung</a><br><a href=/volumes/D19-1/ class=text-muted>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1012><div class="card-body p-3 small">Previous research on empathetic dialogue systems has mostly focused on generating responses given certain emotions. However, being empathetic not only requires the ability of generating emotional responses, but more importantly, requires the understanding of user emotions and replying appropriately. In this paper, we propose a novel end-to-end approach for modeling empathy in dialogue systems : Mixture of Empathetic Listeners (MoEL). Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> first captures the <a href=https://en.wikipedia.org/wiki/Emotion>user emotions</a> and outputs an emotion distribution. Based on this, MoEL will softly combine the output states of the appropriate Listener(s), which are each optimized to react to certain emotions, and generate an empathetic response. Human evaluations on EMPATHETIC-DIALOGUES dataset confirm that MoEL outperforms multitask training baseline in terms of <a href=https://en.wikipedia.org/wiki/Empathy>empathy</a>, <a href=https://en.wikipedia.org/wiki/Relevance>relevance</a>, and <a href=https://en.wikipedia.org/wiki/Fluency>fluency</a>. Furthermore, the case study on generated responses of different Listeners shows high interpretability of our <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1303.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1303 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1303 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1303" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1303/>Clickbait? Sensational Headline Generation with Auto-tuned Reinforcement Learning</a></strong><br><a href=/people/p/peng-xu/>Peng Xu</a>
|
<a href=/people/c/chien-sheng-wu/>Chien-Sheng Wu</a>
|
<a href=/people/a/andrea-madotto/>Andrea Madotto</a>
|
<a href=/people/p/pascale-fung/>Pascale Fung</a><br><a href=/volumes/D19-1/ class=text-muted>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1303><div class="card-body p-3 small">Sensational headlines are <a href=https://en.wikipedia.org/wiki/Headline>headlines</a> that capture people&#8217;s attention and generate <a href=https://en.wikipedia.org/wiki/Interest_(emotion)>reader interest</a>. Conventional abstractive headline generation methods, unlike human writers, do not optimize for maximal reader attention. In this paper, we propose a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> that generates <a href=https://en.wikipedia.org/wiki/Sensationalism>sensational headlines</a> without labeled data. We first train a sensationalism scorer by classifying online headlines with many comments (clickbait) against a baseline of headlines generated from a summarization model. The score from the sensationalism scorer is used as the <a href=https://en.wikipedia.org/wiki/Reward_system>reward</a> for a reinforcement learner. However, maximizing the noisy sensationalism reward will generate unnatural phrases instead of sensational headlines. To effectively leverage this noisy reward, we propose a novel <a href=https://en.wikipedia.org/wiki/Loss_function>loss function</a>, Auto-tuned Reinforcement Learning (ARL), to dynamically balance <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning (RL)</a> with <a href=https://en.wikipedia.org/wiki/Maximum_likelihood_estimation>maximum likelihood estimation (MLE)</a>. Human evaluation shows that 60.8 % of samples generated by our <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> are sensational, which is significantly better than the Pointer-Gen baseline and other RL models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5827.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5827 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5827 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5827/>Generalizing Question Answering System with Pre-trained Language Model Fine-tuning</a></strong><br><a href=/people/d/dan-su/>Dan Su</a>
|
<a href=/people/y/yan-xu/>Yan Xu</a>
|
<a href=/people/g/genta-indra-winata/>Genta Indra Winata</a>
|
<a href=/people/p/peng-xu/>Peng Xu</a>
|
<a href=/people/h/hyeondey-kim/>Hyeondey Kim</a>
|
<a href=/people/z/zihan-liu/>Zihan Liu</a>
|
<a href=/people/p/pascale-fung/>Pascale Fung</a><br><a href=/volumes/D19-58/ class=text-muted>Proceedings of the 2nd Workshop on Machine Reading for Question Answering</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5827><div class="card-body p-3 small">With a large number of datasets being released and new techniques being proposed, Question answering (QA) systems have witnessed great breakthroughs in reading comprehension (RC)tasks. However, most existing methods focus on improving in-domain performance, leaving open the research question of how these mod-els and techniques can generalize to out-of-domain and unseen RC tasks. To enhance the generalization ability, we propose a multi-task learning framework that learns the shared representation across different tasks. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is built on top of a large pre-trained language model, such as XLNet, and then fine-tuned on multiple RC datasets. Experimental results show the effectiveness of our <a href=https://en.wikipedia.org/wiki/Methodology>methods</a>, with an average Exact Match score of 56.59 and an <a href=https://en.wikipedia.org/wiki/F-number>average F1 score</a> of 68.98, which significantly improves the BERT-Large baseline by8.39 and 7.22, respectively</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S19-2021.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S19-2021 data-toggle=collapse aria-expanded=false aria-controls=abstract-S19-2021 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/S19-2021/>CAiRE_HKUST at SemEval-2019 Task 3 : Hierarchical Attention for Dialogue Emotion Classification<span class=acl-fixed-case>CA</span>i<span class=acl-fixed-case>RE</span>_<span class=acl-fixed-case>HKUST</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2019 Task 3: Hierarchical Attention for Dialogue Emotion Classification</a></strong><br><a href=/people/g/genta-indra-winata/>Genta Indra Winata</a>
|
<a href=/people/a/andrea-madotto/>Andrea Madotto</a>
|
<a href=/people/z/zhaojiang-lin/>Zhaojiang Lin</a>
|
<a href=/people/j/jamin-shin/>Jamin Shin</a>
|
<a href=/people/y/yan-xu/>Yan Xu</a>
|
<a href=/people/p/peng-xu/>Peng Xu</a>
|
<a href=/people/p/pascale-fung/>Pascale Fung</a><br><a href=/volumes/S19-2/ class=text-muted>Proceedings of the 13th International Workshop on Semantic Evaluation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S19-2021><div class="card-body p-3 small">Detecting emotion from <a href=https://en.wikipedia.org/wiki/Dialogue>dialogue</a> is a challenge that has not yet been extensively surveyed. One could consider the <a href=https://en.wikipedia.org/wiki/Emotion>emotion</a> of each dialogue turn to be independent, but in this paper, we introduce a hierarchical approach to classify <a href=https://en.wikipedia.org/wiki/Emotion>emotion</a>, hypothesizing that the current emotional state depends on previous latent emotions. We benchmark several feature-based classifiers using pre-trained word and emotion embeddings, state-of-the-art end-to-end neural network models, and Gaussian processes for automatic hyper-parameter search. In our experiments, hierarchical architectures consistently give significant improvements, and our best <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieves a 76.77 % <a href=https://en.wikipedia.org/wiki/F-number>F1-score</a> on the test set.</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S18-1039.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S18-1039 data-toggle=collapse aria-expanded=false aria-controls=abstract-S18-1039 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/S18-1039/>PlusEmo2Vec at SemEval-2018 Task 1 : Exploiting emotion knowledge from <a href=https://en.wikipedia.org/wiki/Emoji>emoji</a> and # hashtags<span class=acl-fixed-case>P</span>lus<span class=acl-fixed-case>E</span>mo2<span class=acl-fixed-case>V</span>ec at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2018 Task 1: Exploiting emotion knowledge from emoji and #hashtags</a></strong><br><a href=/people/j/ji-ho-park/>Ji Ho Park</a>
|
<a href=/people/p/peng-xu/>Peng Xu</a>
|
<a href=/people/p/pascale-fung/>Pascale Fung</a><br><a href=/volumes/S18-1/ class=text-muted>Proceedings of The 12th International Workshop on Semantic Evaluation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S18-1039><div class="card-body p-3 small">This paper describes our <a href=https://en.wikipedia.org/wiki/System>system</a> that has been submitted to SemEval-2018 Task 1 : Affect in Tweets (AIT) to solve five subtasks. We focus on modeling both sentence and word level representations of emotion inside texts through large distantly labeled corpora with <a href=https://en.wikipedia.org/wiki/Emoji>emojis</a> and <a href=https://en.wikipedia.org/wiki/Hashtag>hashtags</a>. We transfer the emotional knowledge by exploiting neural network models as <a href=https://en.wikipedia.org/wiki/Feature_extraction>feature extractors</a> and use these representations for traditional machine learning models such as support vector regression (SVR) and <a href=https://en.wikipedia.org/wiki/Logistic_regression>logistic regression</a> to solve the competition tasks. Our <a href=https://en.wikipedia.org/wiki/System>system</a> is placed among the Top3 for all subtasks we participated.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6243.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6243 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6243 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-6243" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-6243/>Emo2Vec : Learning Generalized Emotion Representation by Multi-task Training<span class=acl-fixed-case>E</span>mo2<span class=acl-fixed-case>V</span>ec: Learning Generalized Emotion Representation by Multi-task Training</a></strong><br><a href=/people/p/peng-xu/>Peng Xu</a>
|
<a href=/people/a/andrea-madotto/>Andrea Madotto</a>
|
<a href=/people/c/chien-sheng-wu/>Chien-Sheng Wu</a>
|
<a href=/people/j/ji-ho-park/>Ji Ho Park</a>
|
<a href=/people/p/pascale-fung/>Pascale Fung</a><br><a href=/volumes/W18-62/ class=text-muted>Proceedings of the 9th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6243><div class="card-body p-3 small">In this paper, we propose Emo2Vec which encodes emotional semantics into vectors. We train Emo2Vec by multi-task learning six different emotion-related tasks, including emotion / sentiment analysis, sarcasm classification, stress detection, abusive language classification, insult detection, and personality recognition. Our evaluation of Emo2Vec shows that it outperforms existing affect-related representations, such as Sentiment-Specific Word Embedding and DeepMoji embeddings with much smaller training corpora. When concatenated with <a href=https://en.wikipedia.org/wiki/GloVe_(machine_learning)>GloVe</a>, Emo2Vec achieves competitive performances to state-of-the-art results on several tasks using a simple logistic regression classifier.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1002.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1002 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1002 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://vimeo.com/276389935 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N18-1002" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/N18-1002/>Neural Fine-Grained Entity Type Classification with Hierarchy-Aware Loss</a></strong><br><a href=/people/p/peng-xu/>Peng Xu</a>
|
<a href=/people/d/denilson-barbosa/>Denilson Barbosa</a><br><a href=/volumes/N18-1/ class=text-muted>Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1002><div class="card-body p-3 small">The task of Fine-grained Entity Type Classification (FETC) consists of assigning types from a <a href=https://en.wikipedia.org/wiki/Hierarchy>hierarchy</a> to entity mentions in text. Existing methods rely on distant supervision and are thus susceptible to noisy labels that can be out-of-context or overly-specific for the training sentence. Previous methods that attempt to address these issues do so with heuristics or with the help of hand-crafted features. Instead, we propose an end-to-end solution with a <a href=https://en.wikipedia.org/wiki/Neural_network>neural network model</a> that uses a variant of cross-entropy loss function to handle out-of-context labels, and hierarchical loss normalization to cope with overly-specific ones. Also, previous work solve FETC a <a href=https://en.wikipedia.org/wiki/Multi-label_classification>multi-label classification</a> followed by ad-hoc post-processing. In contrast, our solution is more elegant : we use public word embeddings to train a single-label that jointly learns representations for entity mentions and their context. We show experimentally that our approach is robust against <a href=https://en.wikipedia.org/wiki/Noise_(signal_processing)>noise</a> and consistently outperforms the <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a> on established <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmarks</a> for the task.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Peng+Xu" title="Search for 'Peng Xu' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/p/pascale-fung/ class=align-middle>Pascale Fung</a>
<span class="badge badge-secondary align-middle ml-2">9</span></li><li class=list-group-item><a href=/people/a/andrea-madotto/ class=align-middle>Andrea Madotto</a>
<span class="badge badge-secondary align-middle ml-2">5</span></li><li class=list-group-item><a href=/people/g/genta-indra-winata/ class=align-middle>Genta Indra Winata</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/people/z/zhaojiang-lin/ class=align-middle>Zhaojiang Lin</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/people/z/zihan-liu/ class=align-middle>Zihan Liu</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/y/yan-xu/ class=align-middle>Yan Xu</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/j/jamin-shin/ class=align-middle>Jamin Shin</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/c/chien-sheng-wu/ class=align-middle>Chien-Sheng Wu</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/j/ji-ho-park/ class=align-middle>Ji Ho Park</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/m/mostofa-patwary/ class=align-middle>Mostofa Patwary</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/mohammad-shoeybi/ class=align-middle>Mohammad Shoeybi</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/raul-puri/ class=align-middle>Raul Puri</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/animashree-anandkumar/ class=align-middle>Animashree Anandkumar</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/b/bryan-catanzaro/ class=align-middle>Bryan Catanzaro</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/samuel-cahyawijaya/ class=align-middle>Samuel Cahyawijaya</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/dan-su/ class=align-middle>Dan Su</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/hyeondey-kim/ class=align-middle>Hyeondey Kim</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/e/etsuko-ishii/ class=align-middle>Etsuko Ishii</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/denilson-barbosa/ class=align-middle>Denilson Barbosa</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/emnlp/ class=align-middle>EMNLP</a><span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/venues/semeval/ class=align-middle>SemEval</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/acl/ class=align-middle>ACL</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/dialdoc/ class=align-middle>dialdoc</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/ws/ class=align-middle>WS</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-venues aria-expanded=false aria-controls=more-venues>show all...</li><div class="collapse border-top" id=more-venues><li class=list-group-item><a href=/venues/naacl/ class=align-middle>NAACL</a><span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright Â©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>