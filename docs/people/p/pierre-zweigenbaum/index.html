<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Pierre Zweigenbaum - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Pierre</span> <span class=font-weight-bold>Zweigenbaum</span></h2><hr><div class=row><div class=col-lg-9><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eval4nlp-1.1.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eval4nlp-1--1 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eval4nlp-1.1 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.eval4nlp-1.1/>Differential Evaluation : a Qualitative Analysis of Natural Language Processing System Behavior Based Upon Data Resistance to Processing</a></strong><br><a href=/people/l/lucie-gianola/>Lucie Gianola</a>
|
<a href=/people/h/hicham-el-boukkouri/>Hicham El Boukkouri</a>
|
<a href=/people/c/cyril-grouin/>Cyril Grouin</a>
|
<a href=/people/t/thomas-lavergne/>Thomas Lavergne</a>
|
<a href=/people/p/patrick-paroubek/>Patrick Paroubek</a>
|
<a href=/people/p/pierre-zweigenbaum/>Pierre Zweigenbaum</a><br><a href=/volumes/2021.eval4nlp-1/ class=text-muted>Proceedings of the 2nd Workshop on Evaluation and Comparison of NLP Systems</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eval4nlp-1--1><div class="card-body p-3 small">Most of the time, when dealing with a particular Natural Language Processing task, systems are compared on the basis of global statistics such as <a href=https://en.wikipedia.org/wiki/Recall_(memory)>recall</a>, <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>precision</a>, F1-score, etc. While such scores provide a general idea of the behavior of these <a href=https://en.wikipedia.org/wiki/System>systems</a>, they ignore a key piece of information that can be useful for assessing progress and discerning remaining challenges : the relative difficulty of test instances. To address this shortcoming, we introduce the notion of differential evaluation which effectively defines a pragmatic partition of instances into gradually more difficult bins by leveraging the predictions made by a set of systems. Comparing systems along these difficulty bins enables us to produce a finer-grained analysis of their relative merits, which we illustrate on two use-cases : a comparison of systems participating in a multi-label text classification task (CLEF eHealth 2018 ICD-10 coding), and a comparison of neural models trained for biomedical entity detection (BioCreative V chemical-disease relations dataset).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.bucc-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.bucc-1.0/>Proceedings of the 14th Workshop on Building and Using Comparable Corpora (BUCC 2021)</a></strong><br><a href=/people/r/reinhard-rapp/>Reinhard Rapp</a>
|
<a href=/people/s/serge-sharoff/>Serge Sharoff</a>
|
<a href=/people/p/pierre-zweigenbaum/>Pierre Zweigenbaum</a><br><a href=/volumes/2021.bucc-1/ class=text-muted>Proceedings of the 14th Workshop on Building and Using Comparable Corpora (BUCC 2021)</a></span></p><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.609.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--609 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.609 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.coling-main.609" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.609/>CharacterBERT : Reconciling ELMo and BERT for Word-Level Open-Vocabulary Representations From Characters<span class=acl-fixed-case>C</span>haracter<span class=acl-fixed-case>BERT</span>: Reconciling <span class=acl-fixed-case>ELM</span>o and <span class=acl-fixed-case>BERT</span> for Word-Level Open-Vocabulary Representations From Characters</a></strong><br><a href=/people/h/hicham-el-boukkouri/>Hicham El Boukkouri</a>
|
<a href=/people/o/olivier-ferret/>Olivier Ferret</a>
|
<a href=/people/t/thomas-lavergne/>Thomas Lavergne</a>
|
<a href=/people/h/hiroshi-noji/>Hiroshi Noji</a>
|
<a href=/people/p/pierre-zweigenbaum/>Pierre Zweigenbaum</a>
|
<a href=/people/j/junichi-tsujii/>Jun’ichi Tsujii</a><br><a href=/volumes/2020.coling-main/ class=text-muted>Proceedings of the 28th International Conference on Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--609><div class="card-body p-3 small">Due to the compelling improvements brought by BERT, many recent representation models adopted the Transformer architecture as their main building block, consequently inheriting the wordpiece tokenization system despite it not being intrinsically linked to the notion of <a href=https://en.wikipedia.org/wiki/Transformers_(toy_line)>Transformers</a>. While this system is thought to achieve a good balance between the flexibility of characters and the efficiency of full words, using predefined wordpiece vocabularies from the <a href=https://en.wikipedia.org/wiki/Domain_(software_engineering)>general domain</a> is not always suitable, especially when building <a href=https://en.wikipedia.org/wiki/Conceptual_model_(computer_science)>models</a> for <a href=https://en.wikipedia.org/wiki/Domain_(software_engineering)>specialized domains</a> (e.g., the medical domain). Moreover, adopting a wordpiece tokenization shifts the focus from the word level to the subword level, making the <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> conceptually more complex and arguably less convenient in practice. For these reasons, we propose CharacterBERT, a new variant of BERT that drops the wordpiece system altogether and uses a Character-CNN module instead to represent entire words by consulting their characters. We show that this new model improves the performance of BERT on a variety of medical domain tasks while at the same time producing robust, word-level, and open-vocabulary representations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.bucc-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.bucc-1.0/>Proceedings of the 13th Workshop on Building and Using Comparable Corpora</a></strong><br><a href=/people/r/reinhard-rapp/>Reinhard Rapp</a>
|
<a href=/people/p/pierre-zweigenbaum/>Pierre Zweigenbaum</a>
|
<a href=/people/s/serge-sharoff/>Serge Sharoff</a><br><a href=/volumes/2020.bucc-1/ class=text-muted>Proceedings of the 13th Workshop on Building and Using Comparable Corpora</a></span></p><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-2041.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-2041 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-2041 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-2041/>Embedding Strategies for Specialized Domains : Application to Clinical Entity Recognition</a></strong><br><a href=/people/h/hicham-el-boukkouri/>Hicham El Boukkouri</a>
|
<a href=/people/o/olivier-ferret/>Olivier Ferret</a>
|
<a href=/people/t/thomas-lavergne/>Thomas Lavergne</a>
|
<a href=/people/p/pierre-zweigenbaum/>Pierre Zweigenbaum</a><br><a href=/volumes/P19-2/ class=text-muted>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-2041><div class="card-body p-3 small">Using pre-trained word embeddings in conjunction with <a href=https://en.wikipedia.org/wiki/Deep_learning>Deep Learning models</a> has become the de facto approach in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>Natural Language Processing (NLP)</a>. While this usually yields satisfactory results, off-the-shelf <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> tend to perform poorly on texts from specialized domains such as clinical reports. Moreover, training specialized word representations from scratch is often either impossible or ineffective due to the lack of large enough in-domain data. In this work, we focus on the clinical domain for which we study embedding strategies that rely on general-domain resources only. We show that by combining off-the-shelf contextual embeddings (ELMo) with static word2vec embeddings trained on a small in-domain corpus built from the task data, we manage to reach and sometimes outperform representations learned from a large corpus in the medical domain.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2019.jeptalnrecital-long.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2019.jeptalnrecital-long.0/>Actes de la Conférence sur le Traitement Automatique des Langues Naturelles (TALN) PFIA 2019. Volume I : Articles longs</a></strong><br><a href=/people/e/emmanuel-morin/>Emmanuel Morin</a>
|
<a href=/people/s/sophie-rosset/>Sophie Rosset</a>
|
<a href=/people/p/pierre-zweigenbaum/>Pierre Zweigenbaum</a><br><a href=/volumes/2019.jeptalnrecital-long/ class=text-muted>Actes de la Conférence sur le Traitement Automatique des Langues Naturelles (TALN) PFIA 2019. Volume I : Articles longs</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2019.jeptalnrecital-court.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2019.jeptalnrecital-court.0/>Actes de la Conférence sur le Traitement Automatique des Langues Naturelles (TALN) PFIA 2019. Volume II : Articles courts</a></strong><br><a href=/people/e/emmanuel-morin/>Emmanuel Morin</a>
|
<a href=/people/s/sophie-rosset/>Sophie Rosset</a>
|
<a href=/people/p/pierre-zweigenbaum/>Pierre Zweigenbaum</a><br><a href=/volumes/2019.jeptalnrecital-court/ class=text-muted>Actes de la Conférence sur le Traitement Automatique des Langues Naturelles (TALN) PFIA 2019. Volume II : Articles courts</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2019.jeptalnrecital-recital.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2019.jeptalnrecital-recital.0/>Actes de la Conférence sur le Traitement Automatique des Langues Naturelles (TALN) PFIA 2019. Volume III : RECITAL</a></strong><br><a href=/people/e/emmanuel-morin/>Emmanuel Morin</a>
|
<a href=/people/s/sophie-rosset/>Sophie Rosset</a>
|
<a href=/people/p/pierre-zweigenbaum/>Pierre Zweigenbaum</a><br><a href=/volumes/2019.jeptalnrecital-recital/ class=text-muted>Actes de la Conférence sur le Traitement Automatique des Langues Naturelles (TALN) PFIA 2019. Volume III : RECITAL</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2019.jeptalnrecital-demo.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2019.jeptalnrecital-demo.0/>Actes de la Conférence sur le Traitement Automatique des Langues Naturelles (TALN) PFIA 2019. Volume IV : Démonstrations</a></strong><br><a href=/people/e/emmanuel-morin/>Emmanuel Morin</a>
|
<a href=/people/s/sophie-rosset/>Sophie Rosset</a>
|
<a href=/people/p/pierre-zweigenbaum/>Pierre Zweigenbaum</a><br><a href=/volumes/2019.jeptalnrecital-demo/ class=text-muted>Actes de la Conférence sur le Traitement Automatique des Langues Naturelles (TALN) PFIA 2019. Volume IV : Démonstrations</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2019.jeptalnrecital-deft.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2019.jeptalnrecital-deft.0/>Actes de la Conférence sur le Traitement Automatique des Langues Naturelles (TALN) PFIA 2019. Défi Fouille de Textes (atelier TALN-RECITAL)</a></strong><br><a href=/people/e/emmanuel-morin/>Emmanuel Morin</a>
|
<a href=/people/s/sophie-rosset/>Sophie Rosset</a>
|
<a href=/people/p/pierre-zweigenbaum/>Pierre Zweigenbaum</a><br><a href=/volumes/2019.jeptalnrecital-deft/ class=text-muted>Actes de la Conférence sur le Traitement Automatique des Langues Naturelles (TALN) PFIA 2019. Défi Fouille de Textes (atelier TALN-RECITAL)</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2019.jeptalnrecital-tia.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2019.jeptalnrecital-tia.0/>Actes de la Conférence sur le Traitement Automatique des Langues Naturelles (TALN) PFIA 2019. Terminologie et Intelligence Artificielle (atelier TALN-RECITAL \& IC)</a></strong><br><a href=/people/e/emmanuel-morin/>Emmanuel Morin</a>
|
<a href=/people/s/sophie-rosset/>Sophie Rosset</a>
|
<a href=/people/p/pierre-zweigenbaum/>Pierre Zweigenbaum</a><br><a href=/volumes/2019.jeptalnrecital-tia/ class=text-muted>Actes de la Conférence sur le Traitement Automatique des Langues Naturelles (TALN) PFIA 2019. Terminologie et Intelligence Artificielle (atelier TALN-RECITAL \& IC)</a></span></p><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2090.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2090 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2090 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2090.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-2090/>GNEG : Graph-Based Negative Sampling for word2vec<span class=acl-fixed-case>GNEG</span>: Graph-Based Negative Sampling for word2vec</a></strong><br><a href=/people/z/zheng-zhang/>Zheng Zhang</a>
|
<a href=/people/p/pierre-zweigenbaum/>Pierre Zweigenbaum</a><br><a href=/volumes/P18-2/ class=text-muted>Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2090><div class="card-body p-3 small">Negative sampling is an important component in <a href=https://en.wikipedia.org/wiki/Word2vec>word2vec</a> for distributed word representation learning. We hypothesize that taking into account global, corpus-level information and generating a different noise distribution for each target word better satisfies the requirements of negative examples for each training word than the original frequency-based distribution. In this purpose we pre-compute word co-occurrence statistics from the corpus and apply to it network algorithms such as <a href=https://en.wikipedia.org/wiki/Random_walk>random walk</a>. We test this hypothesis through a set of experiments whose results show that our approach boosts the word analogy task by about 5 % and improves the performance on word similarity tasks by about 1 % compared to the skip-gram negative sampling baseline.</div></div><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2312.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2312 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2312 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-2312/>Representation of complex terms in a <a href=https://en.wikipedia.org/wiki/Vector_space>vector space</a> structured by an <a href=https://en.wikipedia.org/wiki/Ontology_(information_science)>ontology</a> for a normalization task</a></strong><br><a href=/people/a/arnaud-ferre/>Arnaud Ferré</a>
|
<a href=/people/p/pierre-zweigenbaum/>Pierre Zweigenbaum</a>
|
<a href=/people/c/claire-nedellec/>Claire Nédellec</a><br><a href=/volumes/W17-23/ class=text-muted>BioNLP 2017</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2312><div class="card-body p-3 small">We propose in this paper a <a href=https://en.wikipedia.org/wiki/Semi-supervised_learning>semi-supervised method</a> for labeling terms of texts with concepts of a <a href=https://en.wikipedia.org/wiki/Ontology_(information_science)>domain ontology</a>. The method generates continuous vector representations of complex terms in a <a href=https://en.wikipedia.org/wiki/Semantic_space>semantic space</a> structured by the <a href=https://en.wikipedia.org/wiki/Ontology_(information_science)>ontology</a>. The proposed method relies on a distributional semantics approach, which generates initial vectors for each of the extracted terms. Then these <a href=https://en.wikipedia.org/wiki/Vector_(mathematics_and_physics)>vectors</a> are embedded in the <a href=https://en.wikipedia.org/wiki/Vector_space>vector space</a> constructed from the structure of the ontology. This embedding is carried out by training a <a href=https://en.wikipedia.org/wiki/Linear_model>linear model</a>. Finally, we apply a distance calculation to determine the proximity between vectors of terms and vectors of concepts and thus to assign <a href=https://en.wikipedia.org/wiki/Ontology_(information_science)>ontology labels</a> to terms. We have evaluated the quality of these representations for a normalization task by using the concepts of an <a href=https://en.wikipedia.org/wiki/Ontology_(information_science)>ontology</a> as semantic labels. Normalization of terms is an important step to extract a part of the information containing in texts, but the <a href=https://en.wikipedia.org/wiki/Vector_space>vector space</a> generated might find other applications. The performance of this <a href=https://en.wikipedia.org/wiki/Methodology>method</a> is comparable to that of the state of the art for this task of standardization, opening up encouraging prospects.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2343.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2343 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2343 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-2343/>Automatic classification of doctor-patient questions for a virtual patient record query task</a></strong><br><a href=/people/l/leonardo-campillos-llanos/>Leonardo Campillos Llanos</a>
|
<a href=/people/s/sophie-rosset/>Sophie Rosset</a>
|
<a href=/people/p/pierre-zweigenbaum/>Pierre Zweigenbaum</a><br><a href=/volumes/W17-23/ class=text-muted>BioNLP 2017</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2343><div class="card-body p-3 small">We present the work-in-progress of automating the classification of doctor-patient questions in the context of a simulated consultation with a virtual patient. We classify questions according to the computational strategy (rule-based or other) needed for looking up data in the <a href=https://en.wikipedia.org/wiki/Medical_record>clinical record</a>. We compare &#8216;traditional&#8217; machine learning methods (Gaussian and Multinomial Naive Bayes, and Support Vector Machines) and a neural network classifier (FastText). We obtained the best results with the SVM using <a href=https://en.wikipedia.org/wiki/Semantic_annotation>semantic annotations</a>, whereas the neural classifier achieved promising results without it.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2500.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-2500/>Proceedings of the 10th Workshop on Building and Using Comparable Corpora</a></strong><br><a href=/people/s/serge-sharoff/>Serge Sharoff</a>
|
<a href=/people/p/pierre-zweigenbaum/>Pierre Zweigenbaum</a>
|
<a href=/people/r/reinhard-rapp/>Reinhard Rapp</a><br><a href=/volumes/W17-25/ class=text-muted>Proceedings of the 10th Workshop on Building and Using Comparable Corpora</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2510.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2510 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2510 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-2510/>zNLP : Identifying Parallel Sentences in Chinese-English Comparable Corpora<span class=acl-fixed-case>NLP</span>: Identifying Parallel Sentences in <span class=acl-fixed-case>C</span>hinese-<span class=acl-fixed-case>E</span>nglish Comparable Corpora</a></strong><br><a href=/people/z/zheng-zhang/>Zheng Zhang</a>
|
<a href=/people/p/pierre-zweigenbaum/>Pierre Zweigenbaum</a><br><a href=/volumes/W17-25/ class=text-muted>Proceedings of the 10th Workshop on Building and Using Comparable Corpora</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2510><div class="card-body p-3 small">This paper describes the zNLP system for the BUCC 2017 shared task. Our system identifies parallel sentence pairs in Chinese-English comparable corpora by translating word-by-word Chinese sentences into English, using the search engine Solr to select near-parallel sentences and then by using an SVM classifier to identify true parallel sentences from the previous results. It obtains an F1-score of 45 % (resp. 32 %) on the test (training) set.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Pierre+Zweigenbaum" title="Search for 'Pierre Zweigenbaum' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/s/sophie-rosset/ class=align-middle>Sophie Rosset</a>
<span class="badge badge-secondary align-middle ml-2">7</span></li><li class=list-group-item><a href=/people/e/emmanuel-morin/ class=align-middle>Emmanuel Morin</a>
<span class="badge badge-secondary align-middle ml-2">6</span></li><li class=list-group-item><a href=/people/h/hicham-el-boukkouri/ class=align-middle>Hicham El Boukkouri</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/t/thomas-lavergne/ class=align-middle>Thomas Lavergne</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/s/serge-sharoff/ class=align-middle>Serge Sharoff</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/r/reinhard-rapp/ class=align-middle>Reinhard Rapp</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/z/zheng-zhang/ class=align-middle>Zheng Zhang</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/o/olivier-ferret/ class=align-middle>Olivier Ferret</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/l/lucie-gianola/ class=align-middle>Lucie Gianola</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/cyril-grouin/ class=align-middle>Cyril Grouin</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/p/patrick-paroubek/ class=align-middle>Patrick Paroubek</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/arnaud-ferre/ class=align-middle>Arnaud Ferré</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/claire-nedellec/ class=align-middle>Claire Nédellec</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/leonardo-campillos-llanos/ class=align-middle>Leonardo Campillos Llanos</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/hiroshi-noji/ class=align-middle>Hiroshi Noji</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/junichi-tsujii/ class=align-middle>Jun’ichi Tsujii</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/jeptalnrecital/ class=align-middle>JEP/TALN/RECITAL</a><span class="badge badge-secondary align-middle ml-2">6</span></li><li class=list-group-item><a href=/venues/ws/ class=align-middle>WS</a><span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/venues/bucc/ class=align-middle>BUCC</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/acl/ class=align-middle>ACL</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/eval4nlp/ class=align-middle>Eval4NLP</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-venues aria-expanded=false aria-controls=more-venues>show all...</li><div class="collapse border-top" id=more-venues><li class=list-group-item><a href=/venues/coling/ class=align-middle>COLING</a><span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>