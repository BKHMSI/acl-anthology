<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Pushpak Bhattacharyya - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Pushpak</span> <span class=font-weight-bold>Bhattacharyya</span></h2><p class="font-weight-light text-muted"><span class=font-italic>Also published as:</span>
Pushpak <span class=font-weight-normal>Bhattacharya</span></p><hr><div class=row><div class=col-lg-9><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.mtsummit-research.11.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--mtsummit-research--11 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.mtsummit-research.11 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.mtsummit-research.11/>Scrambled Translation Problem : A Problem of Denoising UNMT<span class=acl-fixed-case>UNMT</span></a></strong><br><a href=/people/t/tamali-banerjee/>Tamali Banerjee</a>
|
<a href=/people/r/rudra-v-murthy/>Rudra V Murthy</a>
|
<a href=/people/p/pushpak-bhattacharyya/>Pushpak Bhattacharya</a><br><a href=/volumes/2021.mtsummit-research/ class=text-muted>Proceedings of Machine Translation Summit XVIII: Research Track</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--mtsummit-research--11><div class="card-body p-3 small">In this paper and we identify an interesting kind of error in the output of Unsupervised Neural Machine Translation (UNMT) systems like Undreamt1. We refer to this error type as Scrambled Translation problem. We observe that UNMT models which use word shuffle noise (as in case of Undreamt) can generate correct words and but fail to stitch them together to form phrases. As a result and words of the translated sentence look scrambled and resulting in decreased <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a>. We hypothesise that the reason behind scrambled translation problem is&#8217; shuffling noise&#8217; which is introduced in every input sentence as a <a href=https://en.wikipedia.org/wiki/Noise_reduction>denoising strategy</a>. To test our hypothesis and we experiment by retraining UNMT models with a simple retraining strategy. We stop the training of the Denoising UNMT model after a pre-decided number of iterations and resume the training for the remaining iterations- which number is also pre-decided- using original sentence as input without adding any noise. Our proposed <a href=https://en.wikipedia.org/wiki/Solution>solution</a> achieves significant performance improvement UNMT models that train conventionally. We demonstrate these performance gains on four language pairs and viz. and English-French and English-German and English-Spanish and Hindi-Punjabi. Our qualitative and quantitative analysis shows that the retraining strategy helps achieve better alignment as observed by attention heatmap and better phrasal translation and leading to statistically significant improvement in BLEU scores.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.255.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--255 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.255 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.255/>Modelling Context Emotions using Multi-task Learning for Emotion Controlled Dialog Generation</a></strong><br><a href=/people/d/deeksha-varshney/>Deeksha Varshney</a>
|
<a href=/people/a/asif-ekbal/>Asif Ekbal</a>
|
<a href=/people/p/pushpak-bhattacharyya/>Pushpak Bhattacharyya</a><br><a href=/volumes/2021.eacl-main/ class=text-muted>Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--255><div class="card-body p-3 small">A recent topic of research in <a href=https://en.wikipedia.org/wiki/Natural_language_generation>natural language generation</a> has been the development of automatic response generation modules that can automatically respond to a user&#8217;s utterance in an empathetic manner. Previous research has tackled this task using neural generative methods by augmenting <a href=https://en.wikipedia.org/wiki/Emotion_classification>emotion classes</a> with the input sequences. However, the outputs by these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> may be inconsistent. We employ <a href=https://en.wikipedia.org/wiki/Multi-task_learning>multi-task learning</a> to predict the emotion label and to generate a viable response for a given utterance using a common encoder with multiple decoders. Our proposed encoder-decoder model consists of a self-attention based encoder and a decoder with dot product attention mechanism to generate response with a specified emotion. We use the focal loss to handle imbalanced data distribution, and utilize the consistency loss to allow coherent decoding by the <a href=https://en.wikipedia.org/wiki/Code>decoders</a>. Human evaluation reveals that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> produces more emotionally pertinent responses. In addition, our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms multiple strong baselines on automatic evaluation measures such as F1 and BLEU scores, thus resulting in more fluent and adequate responses.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.288.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--288 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.288 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><span class="align-middle mr-1" data-toggle=tooltip data-placement=bottom title="Honorable Mention for Best Long Paper"><i class="fas fa-award"></i></span><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.eacl-main.288" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2021.eacl-main.288/>Cognition-aware Cognate Detection</a></strong><br><a href=/people/d/diptesh-kanojia/>Diptesh Kanojia</a>
|
<a href=/people/p/prashant-sharma/>Prashant Sharma</a>
|
<a href=/people/s/sayali-ghodekar/>Sayali Ghodekar</a>
|
<a href=/people/p/pushpak-bhattacharyya/>Pushpak Bhattacharyya</a>
|
<a href=/people/g/gholamreza-haffari/>Gholamreza Haffari</a>
|
<a href=/people/m/malhar-kulkarni/>Malhar Kulkarni</a><br><a href=/volumes/2021.eacl-main/ class=text-muted>Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--288><div class="card-body p-3 small">Automatic detection of cognates helps downstream NLP tasks of <a href=https://en.wikipedia.org/wiki/Machine_translation>Machine Translation</a>, <a href=https://en.wikipedia.org/wiki/Cross-lingual_information_retrieval>Cross-lingual Information Retrieval</a>, <a href=https://en.wikipedia.org/wiki/Computational_phylogenetics>Computational Phylogenetics</a> and Cross-lingual Named Entity Recognition. Previous approaches for the task of cognate detection use orthographic, phonetic and semantic similarity based features sets. In this paper, we propose a novel method for enriching the feature sets, with <a href=https://en.wikipedia.org/wiki/Cognition>cognitive features</a> extracted from human readers&#8217; gaze behaviour. We collect gaze behaviour data for a small sample of <a href=https://en.wikipedia.org/wiki/Cognate>cognates</a> and show that extracted cognitive features help the task of cognate detection. However, gaze data collection and annotation is a costly task. We use the collected gaze behaviour data to predict cognitive features for a larger sample and show that predicted cognitive features, also, significantly improve the task performance. We report improvements of 10 % with the collected gaze features, and 12 % using the predicted gaze features, over the previously proposed approaches. Furthermore, we release the collected gaze behaviour data along with our code and cross-lingual models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.299.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--299 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.299 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.299/>Disfluency Correction using <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>Unsupervised and Semi-supervised Learning</a></a></strong><br><a href=/people/n/nikhil-saini/>Nikhil Saini</a>
|
<a href=/people/d/drumil-trivedi/>Drumil Trivedi</a>
|
<a href=/people/s/shreya-khare/>Shreya Khare</a>
|
<a href=/people/t/tejas-dhamecha/>Tejas Dhamecha</a>
|
<a href=/people/p/preethi-jyothi/>Preethi Jyothi</a>
|
<a href=/people/s/samarth-bharadwaj/>Samarth Bharadwaj</a>
|
<a href=/people/p/pushpak-bhattacharyya/>Pushpak Bhattacharyya</a><br><a href=/volumes/2021.eacl-main/ class=text-muted>Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--299><div class="card-body p-3 small">Spoken language is different from the <a href=https://en.wikipedia.org/wiki/Written_language>written language</a> in its style and structure. Disfluencies that appear in transcriptions from <a href=https://en.wikipedia.org/wiki/Speech_recognition>speech recognition systems</a> generally hamper the performance of downstream NLP tasks. Thus, a disfluency correction system that converts disfluent to fluent text is of great value. This paper introduces a disfluency correction model that translates disfluent to fluent text by drawing inspiration from recent encoder-decoder unsupervised style-transfer models for text. We also show considerable benefits in performance when utilizing a small sample of 500 parallel disfluent-fluent sentences in a semi-supervised way. Our <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised approach</a> achieves a BLEU score of 79.39 on the Switchboard corpus test set, with further improvement to a BLEU score of 85.28 with semi-supervision. Both are comparable to two competitive fully-supervised models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wat-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wat-1.0/>Proceedings of the 8th Workshop on Asian Translation (WAT2021)</a></strong><br><a href=/people/t/toshiaki-nakazawa/>Toshiaki Nakazawa</a>
|
<a href=/people/h/hideki-nakayama/>Hideki Nakayama</a>
|
<a href=/people/i/isao-goto/>Isao Goto</a>
|
<a href=/people/h/hideya-mino/>Hideya Mino</a>
|
<a href=/people/c/chenchen-ding/>Chenchen Ding</a>
|
<a href=/people/r/raj-dabre/>Raj Dabre</a>
|
<a href=/people/a/anoop-kunchukuttan/>Anoop Kunchukuttan</a>
|
<a href=/people/s/shohei-higashiyama/>Shohei Higashiyama</a>
|
<a href=/people/h/hiroshi-manabe/>Hiroshi Manabe</a>
|
<a href=/people/w/win-pa-pa/>Win Pa Pa</a>
|
<a href=/people/s/shantipriya-parida/>Shantipriya Parida</a>
|
<a href=/people/o/ondrej-bojar/>Ondřej Bojar</a>
|
<a href=/people/c/chenhui-chu/>Chenhui Chu</a>
|
<a href=/people/a/akiko-eriguchi/>Akiko Eriguchi</a>
|
<a href=/people/k/kaori-abe/>Kaori Abe</a>
|
<a href=/people/y/yusuke-oda/>Yusuke Oda</a>
|
<a href=/people/k/katsuhito-sudoh/>Katsuhito Sudoh</a>
|
<a href=/people/s/sadao-kurohashi/>Sadao Kurohashi</a>
|
<a href=/people/p/pushpak-bhattacharyya/>Pushpak Bhattacharyya</a><br><a href=/volumes/2021.wat-1/ class=text-muted>Proceedings of the 8th Workshop on Asian Translation (WAT2021)</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wat-1.26.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wat-1--26 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wat-1.26 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wat-1.26/>Language Relatedness and Lexical Closeness can help Improve Multilingual NMT : IITBombay@MultiIndicNMT WAT2021<span class=acl-fixed-case>NMT</span>: <span class=acl-fixed-case>IITB</span>ombay@<span class=acl-fixed-case>M</span>ulti<span class=acl-fixed-case>I</span>ndic<span class=acl-fixed-case>NMT</span> <span class=acl-fixed-case>WAT</span>2021</a></strong><br><a href=/people/j/jyotsana-khatri/>Jyotsana Khatri</a>
|
<a href=/people/n/nikhil-saini/>Nikhil Saini</a>
|
<a href=/people/p/pushpak-bhattacharyya/>Pushpak Bhattacharyya</a><br><a href=/volumes/2021.wat-1/ class=text-muted>Proceedings of the 8th Workshop on Asian Translation (WAT2021)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wat-1--26><div class="card-body p-3 small">Multilingual Neural Machine Translation has achieved remarkable performance by training a single translation model for <a href=https://en.wikipedia.org/wiki/Multilingualism>multiple languages</a>. This paper describes our submission (Team ID : CFILT-IITB) for the MultiIndicMT : An Indic Language Multilingual Task at WAT 2021. We train multilingual NMT systems by sharing <a href=https://en.wikipedia.org/wiki/Encoder>encoder and decoder parameters</a> with language embedding associated with each token in both <a href=https://en.wikipedia.org/wiki/Encoder>encoder</a> and decoder. Furthermore, we demonstrate the use of transliteration (script conversion) for <a href=https://en.wikipedia.org/wiki/Indo-Aryan_languages>Indic languages</a> in reducing the lexical gap for training a multilingual NMT system. Further, we show improvement in performance by training a multilingual NMT system using languages of the same family, i.e., <a href=https://en.wikipedia.org/wiki/Lingua_franca>related languages</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wat-1.28.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wat-1--28 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wat-1.28 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wat-1.28/>Multilingual Machine Translation Systems at WAT 2021 : One-to-Many and Many-to-One Transformer based NMT<span class=acl-fixed-case>WAT</span> 2021: One-to-Many and Many-to-One Transformer based <span class=acl-fixed-case>NMT</span></a></strong><br><a href=/people/s/shivam-mhaskar/>Shivam Mhaskar</a>
|
<a href=/people/a/aditya-jain/>Aditya Jain</a>
|
<a href=/people/a/aakash-banerjee/>Aakash Banerjee</a>
|
<a href=/people/p/pushpak-bhattacharyya/>Pushpak Bhattacharyya</a><br><a href=/volumes/2021.wat-1/ class=text-muted>Proceedings of the 8th Workshop on Asian Translation (WAT2021)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wat-1--28><div class="card-body p-3 small">In this paper, we present the details of the systems that we have submitted for the WAT 2021 MultiIndicMT : An Indic Language Multilingual Task. We have submitted two separate multilingual NMT models : one for <a href=https://en.wikipedia.org/wiki/English_language>English</a> to 10 Indic languages and another for 10 Indic languages to English. We discuss the implementation details of two separate multilingual NMT approaches, namely one-to-many and many-to-one, that makes use of a shared decoder and a shared encoder, respectively. From our experiments, we observe that the multilingual NMT systems outperforms the bilingual baseline MT systems for each of the language pairs under consideration.</div></div><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.eamt-1.21.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--eamt-1--21 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.eamt-1.21 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.eamt-1.21/>Modelling Source- and Target- Language Syntactic Information as Conditional Context in Interactive Neural Machine Translation</a></strong><br><a href=/people/k/kamal-kumar-gupta/>Kamal Kumar Gupta</a>
|
<a href=/people/r/rejwanul-haque/>Rejwanul Haque</a>
|
<a href=/people/a/asif-ekbal/>Asif Ekbal</a>
|
<a href=/people/p/pushpak-bhattacharyya/>Pushpak Bhattacharyya</a>
|
<a href=/people/a/andy-way/>Andy Way</a><br><a href=/volumes/2020.eamt-1/ class=text-muted>Proceedings of the 22nd Annual Conference of the European Association for Machine Translation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--eamt-1--21><div class="card-body p-3 small">In interactive machine translation (MT), human translators correct errors in automatic translations in collaboration with the MT systems, which is seen as an effective way to improve the productivity gain in <a href=https://en.wikipedia.org/wiki/Translation>translation</a>. In this study, we model source-language syntactic constituency parse and target-language syntactic descriptions in the form of supertags as conditional context for interactive prediction in neural MT (NMT). We found that the supertags significantly improve productivity gain in <a href=https://en.wikipedia.org/wiki/Translation>translation</a> in interactive-predictive NMT (INMT), while syntactic parsing somewhat found to be effective in reducing human effort in <a href=https://en.wikipedia.org/wiki/Translation>translation</a>. Furthermore, when we model this source- and target-language syntactic information together as the conditional context, both types complement each other and our fully syntax-informed INMT model statistically significantly reduces human efforts in a FrenchtoEnglish translation task, achieving 4.30 points absolute (corresponding to 9.18 % relative) improvement in terms of word prediction accuracy (WPA) and 4.84 points absolute (corresponding to 9.01 % relative) reduction in terms of word stroke ratio (WSR) over the baseline.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.acl-main.402.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--acl-main--402 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.acl-main.402 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://slideslive.com/38929394 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.acl-main.402/>Towards Emotion-aided Multi-modal Dialogue Act Classification</a></strong><br><a href=/people/t/tulika-saha/>Tulika Saha</a>
|
<a href=/people/a/aditya-patra/>Aditya Patra</a>
|
<a href=/people/s/sriparna-saha/>Sriparna Saha</a>
|
<a href=/people/p/pushpak-bhattacharyya/>Pushpak Bhattacharyya</a><br><a href=/volumes/2020.acl-main/ class=text-muted>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--acl-main--402><div class="card-body p-3 small">The task of Dialogue Act Classification (DAC) that purports to capture communicative intent has been studied extensively. But these studies limit themselves to text. Non-verbal features (change of tone, <a href=https://en.wikipedia.org/wiki/Facial_expression>facial expressions</a> etc.) can provide cues to identify DAs, thus stressing the benefit of incorporating multi-modal inputs in the task. Also, the emotional state of the speaker has a substantial effect on the choice of the dialogue act, since conversations are often influenced by <a href=https://en.wikipedia.org/wiki/Emotion>emotions</a>. Hence, the effect of emotion too on automatic identification of DAs needs to be studied. In this work, we address the role of both multi-modality and emotion recognition (ER) in <a href=https://en.wikipedia.org/wiki/Digital-to-analog_converter>DAC</a>. DAC and ER help each other by way of <a href=https://en.wikipedia.org/wiki/Multi-task_learning>multi-task learning</a>. One of the major contributions of this work is a new dataset- multimodal Emotion aware Dialogue Act dataset called EMOTyDA, collected from open-sourced dialogue datasets. To demonstrate the utility of EMOTyDA, we build an attention based (self, inter-modal, inter-task) multi-modal, multi-task Deep Neural Network (DNN) for joint learning of DAs and emotions. We show empirically that multi-modality and multi-tasking achieve better performance of <a href=https://en.wikipedia.org/wiki/Digital-to-analog_converter>DAC</a> compared to uni-modal and single task DAC variants.<i>both</i> multi-modality and emotion recognition (ER) in DAC. DAC and ER help each other by way of multi-task learning. One of the major contributions of this work is a new dataset- multimodal Emotion aware Dialogue Act dataset called EMOTyDA, collected from open-sourced dialogue datasets. To demonstrate the utility of EMOTyDA, we build an attention based (self, inter-modal, inter-task) multi-modal, multi-task Deep Neural Network (DNN) for joint learning of DAs and emotions. We show empirically that multi-modality and multi-tasking achieve better performance of DAC compared to uni-modal and single task DAC variants.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.aacl-main.31.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--aacl-main--31 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.aacl-main.31 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.aacl-main.31/>All-in-One : A Deep Attentive Multi-task Learning Framework for <a href=https://en.wikipedia.org/wiki/Humour>Humour</a>, <a href=https://en.wikipedia.org/wiki/Sarcasm>Sarcasm</a>, Offensive, <a href=https://en.wikipedia.org/wiki/Motivation>Motivation</a>, and Sentiment on Memes</a></strong><br><a href=/people/d/dushyant-singh-chauhan/>Dushyant Singh Chauhan</a>
|
<a href=/people/d/dhanush-s-r/>Dhanush S R</a>
|
<a href=/people/a/asif-ekbal/>Asif Ekbal</a>
|
<a href=/people/p/pushpak-bhattacharyya/>Pushpak Bhattacharyya</a><br><a href=/volumes/2020.aacl-main/ class=text-muted>Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--aacl-main--31><div class="card-body p-3 small">In this paper, we aim at learning the relationships and similarities of a variety of tasks, such as humour detection, sarcasm detection, offensive content detection, motivational content detection and <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> on a somewhat complicated form of information, i.e., <a href=https://en.wikipedia.org/wiki/Meme>memes</a>. We propose a multi-task, multi-modal deep learning framework to solve multiple tasks simultaneously. For <a href=https://en.wikipedia.org/wiki/Computer_multitasking>multi-tasking</a>, we propose two attention-like mechanisms viz., Inter-task Relationship Module (iTRM) and Inter-class Relationship Module (iCRM). The main motivation of iTRM is to learn the relationship between the tasks to realize how they help each other. In contrast, <a href=https://en.wikipedia.org/wiki/ICRM>iCRM</a> develops relations between the different classes of tasks. Finally, <a href=https://en.wikipedia.org/wiki/Representation_(arts)>representations</a> from both the attentions are concatenated and shared across the five <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> (i.e., <a href=https://en.wikipedia.org/wiki/Humour>humour</a>, <a href=https://en.wikipedia.org/wiki/Sarcasm>sarcasm</a>, offensive, motivational, and sentiment) for <a href=https://en.wikipedia.org/wiki/Computer_multitasking>multi-tasking</a>. We use the recently released dataset in the Memotion Analysis task @ SemEval 2020, which consists of <a href=https://en.wikipedia.org/wiki/Meme>memes</a> annotated for the classes as mentioned above. Empirical results on Memotion dataset show the efficacy of our proposed approach over the existing state-of-the-art systems (Baseline and SemEval 2020 winner). The evaluation also indicates that the proposed multi-task framework yields better performance over the single-task learning.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.aacl-main.90.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--aacl-main--90 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.aacl-main.90 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.aacl-main.90/>A Unified Framework for Multilingual and Code-Mixed Visual Question Answering</a></strong><br><a href=/people/d/deepak-gupta/>Deepak Gupta</a>
|
<a href=/people/p/pabitra-lenka/>Pabitra Lenka</a>
|
<a href=/people/a/asif-ekbal/>Asif Ekbal</a>
|
<a href=/people/p/pushpak-bhattacharyya/>Pushpak Bhattacharyya</a><br><a href=/volumes/2020.aacl-main/ class=text-muted>Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--aacl-main--90><div class="card-body p-3 small">In this paper, we propose an effective deep learning framework for multilingual and code- mixed visual question answering. The pro- posed model is capable of predicting answers from the questions in Hindi, English or Code- mixed (Hinglish : Hindi-English) languages. The majority of the existing techniques on Vi- sual Question Answering (VQA) focus on En- glish questions only. However, many applica- tions such as <a href=https://en.wikipedia.org/wiki/Medical_imaging>medical imaging</a>, <a href=https://en.wikipedia.org/wiki/Tourism>tourism</a>, visual assistants require a multilinguality-enabled module for their widespread usages. As there is no available dataset in English-Hindi VQA, we firstly create Hindi and Code-mixed VQA datasets by exploiting the linguistic properties of these languages. We propose a robust tech- nique capable of handling the multilingual and code-mixed question to provide the answer against the visual information (image). To better encode the multilingual and code-mixed questions, we introduce a hierarchy of shared layers. We control the behaviour of these shared layers by an attention-based soft layer sharing mechanism, which learns how shared layers are applied in different ways for the dif- ferent languages of the question. Further, our model uses bi-linear attention with a residual connection to fuse the language and image fea- tures. We perform extensive evaluation and ablation studies for English, Hindi and Code- mixed VQA. The evaluation shows that the proposed multilingual model achieves state-of- the-art performance in all these settings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.214.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--214 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.214 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.214/>EL-BERT at SemEval-2020 Task 10 : A Multi-Embedding Ensemble Based Approach for Emphasis Selection in Visual Media<span class=acl-fixed-case>EL</span>-<span class=acl-fixed-case>BERT</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 10: A Multi-Embedding Ensemble Based Approach for Emphasis Selection in Visual Media</a></strong><br><a href=/people/c/chandresh-kanani/>Chandresh Kanani</a>
|
<a href=/people/s/sriparna-saha/>Sriparna Saha</a>
|
<a href=/people/p/pushpak-bhattacharyya/>Pushpak Bhattacharyya</a><br><a href=/volumes/2020.semeval-1/ class=text-muted>Proceedings of the Fourteenth Workshop on Semantic Evaluation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--214><div class="card-body p-3 small">In <a href=https://en.wikipedia.org/wiki/Media_(communication)>visual media</a>, text emphasis is the strengthening of words in a text to convey the intent of the author. Text emphasis in visual media is generally done by using different colors, backgrounds, or font for the text ; it helps in conveying the actual meaning of the message to the readers. Emphasis selection is the task of choosing candidate words for <a href=https://en.wikipedia.org/wiki/Emphasis_(typography)>emphasis</a>, it helps in automatically designing <a href=https://en.wikipedia.org/wiki/Poster>posters</a> and other media contents with <a href=https://en.wikipedia.org/wiki/Writing>written text</a>. If we consider only the text and do not know the intent, then there can be multiple valid emphasis selections. We propose the use of <a href=https://en.wikipedia.org/wiki/Musical_ensemble>ensembles</a> for emphasis selection to improve over single emphasis selection models. We show that the use of multi-embedding helps in enhancing the results for base models. To show the efficacy of proposed approach we have also done a comparison of our results with state-of-the-art models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.111.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--111 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.111 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.111/>A Retrofitting Model for Incorporating <a href=https://en.wikipedia.org/wiki/Semantic_relation>Semantic Relations</a> into Word Embeddings</a></strong><br><a href=/people/s/sapan-shah/>Sapan Shah</a>
|
<a href=/people/s/sreedhar-reddy/>Sreedhar Reddy</a>
|
<a href=/people/p/pushpak-bhattacharyya/>Pushpak Bhattacharyya</a><br><a href=/volumes/2020.coling-main/ class=text-muted>Proceedings of the 28th International Conference on Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--111><div class="card-body p-3 small">We present a novel retrofitting model that can leverage relational knowledge available in a knowledge resource to improve <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a>. The knowledge is captured in terms of relation inequality constraints that compare similarity of related and unrelated entities in the context of an anchor entity. These <a href=https://en.wikipedia.org/wiki/Constraint_(mathematics)>constraints</a> are used as training data to learn a non-linear transformation function that maps original word vectors to a <a href=https://en.wikipedia.org/wiki/Vector_space>vector space</a> respecting these <a href=https://en.wikipedia.org/wiki/Constraint_(mathematics)>constraints</a>. The transformation function is learned in a similarity metric learning setting using Triplet network architecture. We applied our model to synonymy, antonymy and hypernymy relations in <a href=https://en.wikipedia.org/wiki/WordNet>WordNet</a> and observed large gains in performance over original distributional models as well as other retrofitting approaches on word similarity task and significant overall improvement on lexical entailment detection task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.249.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--249 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.249 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.249/>Reinforced Multi-task Approach for Multi-hop Question Generation</a></strong><br><a href=/people/d/deepak-gupta/>Deepak Gupta</a>
|
<a href=/people/h/hardik-chauhan/>Hardik Chauhan</a>
|
<a href=/people/r/ravi-tej-akella/>Ravi Tej Akella</a>
|
<a href=/people/a/asif-ekbal/>Asif Ekbal</a>
|
<a href=/people/p/pushpak-bhattacharyya/>Pushpak Bhattacharyya</a><br><a href=/volumes/2020.coling-main/ class=text-muted>Proceedings of the 28th International Conference on Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--249><div class="card-body p-3 small">Question generation (QG) attempts to solve the inverse of question answering (QA) problem by generating a natural language question given a document and an answer. While sequence to sequence neural models surpass rule-based systems for QG, they are limited in their capacity to focus on more than one supporting fact. For QG, we often require multiple supporting facts to generate high-quality questions. Inspired by recent works on multi-hop reasoning in <a href=https://en.wikipedia.org/wiki/Quality_assurance>QA</a>, we take up Multi-hop question generation, which aims at generating relevant questions based on supporting facts in the context. We employ <a href=https://en.wikipedia.org/wiki/Multitask_learning>multitask learning</a> with the auxiliary task of answer-aware supporting fact prediction to guide the question generator. In addition, we also proposed a question-aware reward function in a Reinforcement Learning (RL) framework to maximize the utilization of the supporting facts. We demonstrate the effectiveness of our approach through experiments on the multi-hop question answering dataset, HotPotQA. Empirical evaluation shows our model to outperform the single-hop neural question generation models on both automatic evaluation metrics such as <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a>, <a href=https://en.wikipedia.org/wiki/METEOR>METEOR</a>, and ROUGE and human evaluation metrics for quality and coverage of the generated questions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.383.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--383 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.383 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.383/>Filtering Back-Translated Data in Unsupervised Neural Machine Translation</a></strong><br><a href=/people/j/jyotsana-khatri/>Jyotsana Khatri</a>
|
<a href=/people/p/pushpak-bhattacharyya/>Pushpak Bhattacharyya</a><br><a href=/volumes/2020.coling-main/ class=text-muted>Proceedings of the 28th International Conference on Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--383><div class="card-body p-3 small">Unsupervised neural machine translation (NMT) utilizes only <a href=https://en.wikipedia.org/wiki/Monolingualism>monolingual data</a> for training. The quality of back-translated data plays an important role in the performance of <a href=https://en.wikipedia.org/wiki/Network_topology>NMT systems</a>. In <a href=https://en.wikipedia.org/wiki/Back-translation>back-translation</a>, all generated pseudo parallel sentence pairs are not of the same quality. Taking inspiration from domain adaptation where in-domain sentences are given more weight in training, in this paper we propose an approach to filter back-translated data as part of the training process of unsupervised NMT. Our approach gives more weight to good pseudo parallel sentence pairs in the back-translation phase. We calculate the weight of each pseudo parallel sentence pair using sentence-wise round-trip BLEU score which is normalized batch-wise. We compare our approach with the current state of the art approaches for unsupervised NMT.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.534.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--534 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.534 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.534/>Analysing cross-lingual transfer in <a href=https://en.wikipedia.org/wiki/Lemmatisation>lemmatisation</a> for <a href=https://en.wikipedia.org/wiki/Languages_of_India>Indian languages</a><span class=acl-fixed-case>I</span>ndian languages</a></strong><br><a href=/people/k/kumar-saurav/>Kumar Saurav</a>
|
<a href=/people/k/kumar-saunack/>Kumar Saunack</a>
|
<a href=/people/p/pushpak-bhattacharyya/>Pushpak Bhattacharyya</a><br><a href=/volumes/2020.coling-main/ class=text-muted>Proceedings of the 28th International Conference on Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--534><div class="card-body p-3 small">Lemmatization aims to reduce the sparse data problem by relating the <a href=https://en.wikipedia.org/wiki/Inflection>inflected forms</a> of a word to its <a href=https://en.wikipedia.org/wiki/Dictionary>dictionary form</a>. However, most of the prior work on this topic has focused on high resource languages. In this paper, we evaluate cross-lingual approaches for low resource languages, especially in the context of morphologically rich Indian languages. We test our <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> on six languages from two different families and develop linguistic insights into each <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a>&#8217;s performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wat-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.wat-1.0/>Proceedings of the 7th Workshop on Asian Translation</a></strong><br><a href=/people/t/toshiaki-nakazawa/>Toshiaki Nakazawa</a>
|
<a href=/people/h/hideki-nakayama/>Hideki Nakayama</a>
|
<a href=/people/c/chenchen-ding/>Chenchen Ding</a>
|
<a href=/people/r/raj-dabre/>Raj Dabre</a>
|
<a href=/people/a/anoop-kunchukuttan/>Anoop Kunchukuttan</a>
|
<a href=/people/w/win-pa-pa/>Win Pa Pa</a>
|
<a href=/people/o/ondrej-bojar/>Ondřej Bojar</a>
|
<a href=/people/s/shantipriya-parida/>Shantipriya Parida</a>
|
<a href=/people/i/isao-goto/>Isao Goto</a>
|
<a href=/people/h/hidaya-mino/>Hidaya Mino</a>
|
<a href=/people/h/hiroshi-manabe/>Hiroshi Manabe</a>
|
<a href=/people/k/katsuhito-sudoh/>Katsuhito Sudoh</a>
|
<a href=/people/s/sadao-kurohashi/>Sadao Kurohashi</a>
|
<a href=/people/p/pushpak-bhattacharyya/>Pushpak Bhattacharyya</a><br><a href=/volumes/2020.wat-1/ class=text-muted>Proceedings of the 7th Workshop on Asian Translation</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.378.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--378 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.378 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.lrec-1.378" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.378/>Challenge Dataset of Cognates and False Friend Pairs from <a href=https://en.wikipedia.org/wiki/Languages_of_India>Indian Languages</a><span class=acl-fixed-case>I</span>ndian Languages</a></strong><br><a href=/people/d/diptesh-kanojia/>Diptesh Kanojia</a>
|
<a href=/people/m/malhar-kulkarni/>Malhar Kulkarni</a>
|
<a href=/people/p/pushpak-bhattacharyya/>Pushpak Bhattacharyya</a>
|
<a href=/people/g/gholamreza-haffari/>Gholamreza Haffari</a><br><a href=/volumes/2020.lrec-1/ class=text-muted>Proceedings of the 12th Language Resources and Evaluation Conference</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--378><div class="card-body p-3 small">Cognates are present in multiple variants of the same text across different languages (e.g., hund in <a href=https://en.wikipedia.org/wiki/German_language>German</a> and hound in the English language mean dog). They pose a challenge to various Natural Language Processing (NLP) applications such as <a href=https://en.wikipedia.org/wiki/Machine_translation>Machine Translation</a>, Cross-lingual Sense Disambiguation, <a href=https://en.wikipedia.org/wiki/Computational_phylogenetics>Computational Phylogenetics</a>, and <a href=https://en.wikipedia.org/wiki/Information_retrieval>Information Retrieval</a>. A possible solution to address this challenge is to identify <a href=https://en.wikipedia.org/wiki/Cognate>cognates</a> across language pairs. In this paper, we describe the creation of two cognate datasets for twelve Indian languages namely <a href=https://en.wikipedia.org/wiki/Sanskrit>Sanskrit</a>, <a href=https://en.wikipedia.org/wiki/Hindi>Hindi</a>, <a href=https://en.wikipedia.org/wiki/Assamese_language>Assamese</a>, <a href=https://en.wikipedia.org/wiki/Odia_language>Oriya</a>, <a href=https://en.wikipedia.org/wiki/Kannada>Kannada</a>, <a href=https://en.wikipedia.org/wiki/Gujarati_language>Gujarati</a>, <a href=https://en.wikipedia.org/wiki/Tamil_language>Tamil</a>, <a href=https://en.wikipedia.org/wiki/Telugu_language>Telugu</a>, <a href=https://en.wikipedia.org/wiki/Punjabi_language>Punjabi</a>, <a href=https://en.wikipedia.org/wiki/Bengali_language>Bengali</a>, <a href=https://en.wikipedia.org/wiki/Marathi_language>Marathi</a>, and <a href=https://en.wikipedia.org/wiki/Malayalam>Malayalam</a>. We digitize the cognate data from an Indian language cognate dictionary and utilize linked Indian language Wordnets to generate cognate sets. Additionally, we use the Wordnet data to create a False Friends&#8217; dataset for eleven language pairs. We also evaluate the efficacy of our <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> using previously available baseline cognate detection approaches. We also perform a manual evaluation with the help of lexicographers and release the curated gold-standard dataset with this paper.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.bea-1.8.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--bea-1--8 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.bea-1.8 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.bea-1.8/>Can <a href=https://en.wikipedia.org/wiki/Neural_network>Neural Networks</a> Automatically Score Essay Traits?</a></strong><br><a href=/people/s/sandeep-mathias/>Sandeep Mathias</a>
|
<a href=/people/p/pushpak-bhattacharyya/>Pushpak Bhattacharyya</a><br><a href=/volumes/2020.bea-1/ class=text-muted>Proceedings of the Fifteenth Workshop on Innovative Use of NLP for Building Educational Applications</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--bea-1--8><div class="card-body p-3 small">Essay traits are attributes of an essay that can help explain how well written (or badly written) the essay is. Examples of traits include <a href=https://en.wikipedia.org/wiki/Content_(media)>Content</a>, Organization, <a href=https://en.wikipedia.org/wiki/Language>Language</a>, Sentence Fluency, <a href=https://en.wikipedia.org/wiki/Word_choice>Word Choice</a>, etc. A lot of research in the last decade has dealt with automatic holistic essay scoring-where a machine rates an essay and gives a score for the essay. However, writers need feedback, especially if they want to improve their writing-which is why trait-scoring is important. In this paper, we show how a <a href=https://en.wikipedia.org/wiki/Deep_learning>deep-learning based system</a> can outperform feature-based machine learning systems, as well as a string kernel system in scoring <a href=https://en.wikipedia.org/wiki/Essay>essay traits</a>.</div></div><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2019.icon-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2019.icon-1.0/>Proceedings of the 16th International Conference on Natural Language Processing</a></strong><br><a href=/people/d/dipti-misra-sharma/>Dipti Misra Sharma</a>
|
<a href=/people/p/pushpak-bhattacharyya/>Pushpak Bhattacharya</a><br><a href=/volumes/2019.icon-1/ class=text-muted>Proceedings of the 16th International Conference on Natural Language Processing</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2019.icon-1.2.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2019--icon-1--2 data-toggle=collapse aria-expanded=false aria-controls=abstract-2019.icon-1.2 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2019.icon-1.2/>A Deep Ensemble Framework for Fake News Detection and Multi-Class Classification of Short Political Statements</a></strong><br><a href=/people/a/arjun-roy/>Arjun Roy</a>
|
<a href=/people/k/kingshuk-basak/>Kingshuk Basak</a>
|
<a href=/people/a/asif-ekbal/>Asif Ekbal</a>
|
<a href=/people/p/pushpak-bhattacharyya/>Pushpak Bhattacharyya</a><br><a href=/volumes/2019.icon-1/ class=text-muted>Proceedings of the 16th International Conference on Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2019--icon-1--2><div class="card-body p-3 small">Fake news, <a href=https://en.wikipedia.org/wiki/Rumor>rumor</a>, incorrect information, and misinformation detection are nowadays crucial issues as these might have serious consequences for our social fabrics. Such information is increasing rapidly due to the availability of enormous web information sources including <a href=https://en.wikipedia.org/wiki/Social_media>social media feeds</a>, <a href=https://en.wikipedia.org/wiki/Blog>news blogs</a>, <a href=https://en.wikipedia.org/wiki/Online_newspaper>online newspapers</a> etc. In this paper, we develop various deep learning models for detecting <a href=https://en.wikipedia.org/wiki/Fake_news>fake news</a> and classifying them into the pre-defined fine-grained categories. At first, we develop individual models based on Convolutional Neural Network (CNN), and Bi-directional Long Short Term Memory (Bi-LSTM) networks. The <a href=https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning>representations</a> obtained from these two <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> are fed into a Multi-layer Perceptron Model (MLP) for the final <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a>. Our experiments on a <a href=https://en.wikipedia.org/wiki/Benchmarking>benchmark dataset</a> show promising results with an overall <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of 44.87 %, which outperforms the current state of the arts.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2019.icon-1.27.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2019--icon-1--27 data-toggle=collapse aria-expanded=false aria-controls=abstract-2019.icon-1.27 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2019.icon-1.27/>A Deep Learning Approach for Automatic Detection of Fake News</a></strong><br><a href=/people/t/tanik-saikh/>Tanik Saikh</a>
|
<a href=/people/a/arkadipta-de/>Arkadipta De</a>
|
<a href=/people/a/asif-ekbal/>Asif Ekbal</a>
|
<a href=/people/p/pushpak-bhattacharyya/>Pushpak Bhattacharyya</a><br><a href=/volumes/2019.icon-1/ class=text-muted>Proceedings of the 16th International Conference on Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2019--icon-1--27><div class="card-body p-3 small">Fake news detection is a very prominent and essential task in the field of <a href=https://en.wikipedia.org/wiki/Journalism>journalism</a>. This challenging <a href=https://en.wikipedia.org/wiki/Problem_solving>problem</a> is seen so far in the field of <a href=https://en.wikipedia.org/wiki/Politics>politics</a>, but it could be even more challenging when it is to be determined in the multi-domain platform. In this paper, we propose two effective models based on <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning</a> for solving fake news detection problem in online news contents of multiple domains. We evaluate our techniques on the two recently released datasets, namely Fake News AMT and Celebrity for fake news detection. The proposed <a href=https://en.wikipedia.org/wiki/System>systems</a> yield encouraging performance, outperforming the current hand-crafted feature engineering based state-of-the-art system with a significant margin of 3.08 % and 9.3 % by the two <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a>, respectively. In order to exploit the datasets, available for the related tasks, we perform cross-domain analysis (model trained on FakeNews AMT and tested on <a href=https://en.wikipedia.org/wiki/Celebrity>Celebrity</a> and vice versa) to explore the applicability of our systems across the domains.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-1309.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-1309 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-1309 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-1309/>When Numbers Matter ! : Detecting Sarcasm in Numerical Portions of Text</a></strong><br><a href=/people/a/abhijeet-dubey/>Abhijeet Dubey</a>
|
<a href=/people/l/lakshya-kumar/>Lakshya Kumar</a>
|
<a href=/people/a/arpan-somani/>Arpan Somani</a>
|
<a href=/people/a/aditya-joshi/>Aditya Joshi</a>
|
<a href=/people/p/pushpak-bhattacharyya/>Pushpak Bhattacharyya</a><br><a href=/volumes/W19-13/ class=text-muted>Proceedings of the Tenth Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-1309><div class="card-body p-3 small">Research in sarcasm detection spans almost a decade. However a particular form of <a href=https://en.wikipedia.org/wiki/Sarcasm>sarcasm</a> remains unexplored : <a href=https://en.wikipedia.org/wiki/Sarcasm>sarcasm</a> expressed through numbers, which we estimate, forms about 11 % of the sarcastic tweets in our dataset. The sentence &#8216;Love waking up at 3 am&#8217; is sarcastic because of the number. In this paper, we focus on detecting <a href=https://en.wikipedia.org/wiki/Sarcasm>sarcasm</a> in tweets arising out of numbers. Initially, to get an insight into the problem, we implement a rule-based and a statistical machine learning-based (ML) classifier. The rule-based classifier conveys the crux of the numerical sarcasm problem, namely, incongruity arising out of numbers. The statistical ML classifier uncovers the indicators i.e., features of such <a href=https://en.wikipedia.org/wiki/Sarcasm>sarcasm</a>. The actual system in place, however, are two deep learning (DL) models, CNN and attention network that obtains an <a href=https://en.wikipedia.org/wiki/F-score>F-score</a> of 0.93 and 0.91 on our <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> of tweets containing numbers. To the best of our knowledge, this is the first line of research investigating the phenomenon of <a href=https://en.wikipedia.org/wiki/Sarcasm>sarcasm</a> arising out of numbers, culminating in a detector thereof.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2404.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-2404 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-2404 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-2404/>Extraction of Message Sequence Charts from Narrative History Text</a></strong><br><a href=/people/g/girish-palshikar/>Girish Palshikar</a>
|
<a href=/people/s/sachin-pawar/>Sachin Pawar</a>
|
<a href=/people/s/sangameshwar-patil/>Sangameshwar Patil</a>
|
<a href=/people/s/swapnil-hingmire/>Swapnil Hingmire</a>
|
<a href=/people/n/nitin-ramrakhiyani/>Nitin Ramrakhiyani</a>
|
<a href=/people/h/harsimran-bedi/>Harsimran Bedi</a>
|
<a href=/people/p/pushpak-bhattacharyya/>Pushpak Bhattacharyya</a>
|
<a href=/people/v/vasudeva-varma/>Vasudeva Varma</a><br><a href=/volumes/W19-24/ class=text-muted>Proceedings of the First Workshop on Narrative Understanding</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-2404><div class="card-body p-3 small">In this paper, we advocate the use of Message Sequence Chart (MSC) as a <a href=https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning>knowledge representation</a> to capture and visualize multi-actor interactions and their temporal ordering. We propose <a href=https://en.wikipedia.org/wiki/Algorithm>algorithms</a> to automatically extract an <a href=https://en.wikipedia.org/wiki/Most_recent_common_ancestor>MSC</a> from a <a href=https://en.wikipedia.org/wiki/Narrative>history narrative</a>. For a given narrative, we first identify verbs which indicate interactions and then use dependency parsing and Semantic Role Labelling based approaches to identify senders (initiating actors) and receivers (other actors involved) for these interaction verbs. As a final step in MSC extraction, we employ a state-of-the art algorithm to temporally re-order these interactions. Our evaluation on multiple publicly available narratives shows improvements over four <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5426.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5426 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5426 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5426/>Utilizing Monolingual Data in NMT for Similar Languages : Submission to Similar Language Translation Task<span class=acl-fixed-case>NMT</span> for Similar Languages: Submission to Similar Language Translation Task</a></strong><br><a href=/people/j/jyotsana-khatri/>Jyotsana Khatri</a>
|
<a href=/people/p/pushpak-bhattacharyya/>Pushpak Bhattacharyya</a><br><a href=/volumes/W19-54/ class=text-muted>Proceedings of the Fourth Conference on Machine Translation (Volume 3: Shared Task Papers, Day 2)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5426><div class="card-body p-3 small">This paper describes our submission to Shared Task on Similar Language Translation in Fourth Conference on Machine Translation (WMT 2019). We submitted three systems for Hindi-Nepali direction in which we have examined the performance of a RNN based NMT system, a semi-supervised NMT system where monolingual data of both languages is utilized using the architecture by and a system trained with extra synthetic sentences generated using copy of source and target sentences without using any additional monolingual data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1034.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1034 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1034 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-1034/>Multi-task Learning for Multi-modal Emotion Recognition and <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>Sentiment Analysis</a></a></strong><br><a href=/people/m/md-shad-akhtar/>Md Shad Akhtar</a>
|
<a href=/people/d/dushyant-chauhan/>Dushyant Chauhan</a>
|
<a href=/people/d/deepanway-ghosal/>Deepanway Ghosal</a>
|
<a href=/people/s/soujanya-poria/>Soujanya Poria</a>
|
<a href=/people/a/asif-ekbal/>Asif Ekbal</a>
|
<a href=/people/p/pushpak-bhattacharyya/>Pushpak Bhattacharyya</a><br><a href=/volumes/N19-1/ class=text-muted>Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1034><div class="card-body p-3 small">Related <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> often have inter-dependence on each other and perform better when solved in a joint framework. In this paper, we present a deep multi-task learning framework that jointly performs sentiment and emotion analysis both. The <a href=https://en.wikipedia.org/wiki/Multimodal_interaction>multi-modal inputs</a> (i.e. text, acoustic and visual frames) of a video convey diverse and distinctive information, and usually do not have equal contribution in the <a href=https://en.wikipedia.org/wiki/Decision-making>decision making</a>. We propose a context-level inter-modal attention framework for simultaneously predicting the sentiment and expressed emotions of an utterance. We evaluate our proposed approach on CMU-MOSEI dataset for multi-modal sentiment and emotion analysis. Evaluation results suggest that multi-task learning framework offers improvement over the single-task framework. The proposed approach reports new state-of-the-art performance for both <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> and emotion analysis.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-2017.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-2017 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-2017 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-2017/>Extraction of Message Sequence Charts from Software Use-Case Descriptions</a></strong><br><a href=/people/g/girish-palshikar/>Girish Palshikar</a>
|
<a href=/people/n/nitin-ramrakhiyani/>Nitin Ramrakhiyani</a>
|
<a href=/people/s/sangameshwar-patil/>Sangameshwar Patil</a>
|
<a href=/people/s/sachin-pawar/>Sachin Pawar</a>
|
<a href=/people/s/swapnil-hingmire/>Swapnil Hingmire</a>
|
<a href=/people/v/vasudeva-varma/>Vasudeva Varma</a>
|
<a href=/people/p/pushpak-bhattacharyya/>Pushpak Bhattacharyya</a><br><a href=/volumes/N19-2/ class=text-muted>Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Industry Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-2017><div class="card-body p-3 small">Software Requirement Specification documents provide natural language descriptions of the core functional requirements as a set of <a href=https://en.wikipedia.org/wiki/Use_case>use-cases</a>. Essentially, each use-case contains a set of <a href=https://en.wikipedia.org/wiki/Actor_(disambiguation)>actors</a> and sequences of steps describing the interactions among them. Goals of use-case reviews and analyses include their correctness, completeness, detection of ambiguities, <a href=https://en.wikipedia.org/wiki/Software_prototyping>prototyping</a>, <a href=https://en.wikipedia.org/wiki/Software_verification>verification</a>, <a href=https://en.wikipedia.org/wiki/Test_case>test case generation</a> and <a href=https://en.wikipedia.org/wiki/Traceability>traceability</a>. Message Sequence Chart (MSC) have been proposed as a expressive, rigorous yet intuitive visual representation of use-cases. In this paper, we describe a linguistic knowledge-based approach to extract MSCs from <a href=https://en.wikipedia.org/wiki/Use_case>use-cases</a>. Compared to existing techniques, we extract richer constructs of the MSC notation such as <a href=https://en.wikipedia.org/wiki/Timer>timers</a>, <a href=https://en.wikipedia.org/wiki/Conditional_(computer_programming)>conditions</a> and alt-boxes. We apply this <a href=https://en.wikipedia.org/wiki/Tool>tool</a> to extract MSCs from several real-life software use-case descriptions and show that it performs better than the existing techniques. We also discuss the benefits and limitations of the extracted MSCs to meet the above goals.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1297.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1297 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1297 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-1297/>Multilingual Unsupervised NMT using Shared Encoder and Language-Specific Decoders<span class=acl-fixed-case>NMT</span> using Shared Encoder and Language-Specific Decoders</a></strong><br><a href=/people/s/sukanta-sen/>Sukanta Sen</a>
|
<a href=/people/k/kamal-kumar-gupta/>Kamal Kumar Gupta</a>
|
<a href=/people/a/asif-ekbal/>Asif Ekbal</a>
|
<a href=/people/p/pushpak-bhattacharyya/>Pushpak Bhattacharyya</a><br><a href=/volumes/P19-1/ class=text-muted>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1297><div class="card-body p-3 small">In this paper, we propose a multilingual unsupervised NMT scheme which jointly trains multiple languages with a shared encoder and multiple decoders. Our approach is based on denoising autoencoding of each language and back-translating between English and multiple non-English languages. This results in a universal encoder which can encode any language participating in training into an inter-lingual representation, and language-specific decoders. Our experiments using only monolingual corpora show that multilingual unsupervised model performs better than the separately trained bilingual models achieving improvement of up to 1.48 BLEU points on WMT test sets. We also observe that even if we do not train the <a href=https://en.wikipedia.org/wiki/Computer_network>network</a> for all possible translation directions, the <a href=https://en.wikipedia.org/wiki/Computer_network>network</a> is still able to translate in a many-to-many fashion leveraging encoder&#8217;s ability to generate interlingual representation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1540.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1540 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1540 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-1540/>Ordinal and Attribute Aware Response Generation in a Multimodal Dialogue System</a></strong><br><a href=/people/h/hardik-chauhan/>Hardik Chauhan</a>
|
<a href=/people/m/mauajama-firdaus/>Mauajama Firdaus</a>
|
<a href=/people/a/asif-ekbal/>Asif Ekbal</a>
|
<a href=/people/p/pushpak-bhattacharyya/>Pushpak Bhattacharyya</a><br><a href=/volumes/P19-1/ class=text-muted>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1540><div class="card-body p-3 small">Multimodal dialogue systems have opened new frontiers in the traditional goal-oriented dialogue systems. The state-of-the-art dialogue systems are primarily based on unimodal sources, predominantly the text, and hence can not capture the information present in the other sources such as <a href=https://en.wikipedia.org/wiki/Video>videos</a>, <a href=https://en.wikipedia.org/wiki/Videotape>audios</a>, <a href=https://en.wikipedia.org/wiki/Digital_image>images</a> etc. With the availability of large scale multimodal dialogue dataset (MMD) (Saha et al., 2018) on the fashion domain, the visual appearance of the products is essential for understanding the intention of the user. Without capturing the information from both the text and image, the <a href=https://en.wikipedia.org/wiki/System>system</a> will be incapable of generating correct and desirable responses. In this paper, we propose a novel position and attribute aware attention mechanism to learn enhanced image representation conditioned on the user utterance. Our evaluation shows that the proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> can generate appropriate responses while preserving the position and attribute information. Experimental results also prove that our proposed approach attains superior performance compared to the baseline models, and outperforms the state-of-the-art approaches on text similarity based evaluation metrics.</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1042.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1042 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1042 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1042/>Can <a href=https://en.wikipedia.org/wiki/Taxonomy_(general)>Taxonomy</a> Help? Improving Semantic Question Matching using Question Taxonomy</a></strong><br><a href=/people/d/deepak-gupta/>Deepak Gupta</a>
|
<a href=/people/r/rajkumar-pujari/>Rajkumar Pujari</a>
|
<a href=/people/a/asif-ekbal/>Asif Ekbal</a>
|
<a href=/people/p/pushpak-bhattacharyya/>Pushpak Bhattacharyya</a>
|
<a href=/people/a/anutosh-maitra/>Anutosh Maitra</a>
|
<a href=/people/t/tom-jain/>Tom Jain</a>
|
<a href=/people/s/shubhashis-sengupta/>Shubhashis Sengupta</a><br><a href=/volumes/C18-1/ class=text-muted>Proceedings of the 27th International Conference on Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1042><div class="card-body p-3 small">In this paper, we propose a hybrid technique for semantic question matching. It uses a proposed two-layered taxonomy for English questions by augmenting state-of-the-art <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning models</a> with question classes obtained from a deep learning based question classifier. Experiments performed on three open-domain datasets demonstrate the effectiveness of our proposed approach. We achieve state-of-the-art results on partial ordering question ranking (POQR) benchmark dataset. Our empirical analysis shows that coupling standard distributional features (provided by the question encoder) with knowledge from <a href=https://en.wikipedia.org/wiki/Taxonomy_(general)>taxonomy</a> is more effective than either <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning</a> or <a href=https://en.wikipedia.org/wiki/Taxonomy_(general)>taxonomy-based knowledge</a> alone.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1237.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1237 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1237 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=C18-1237" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/C18-1237/>Novelty Goes Deep. A Deep Neural Solution To Document Level Novelty Detection</a></strong><br><a href=/people/t/tirthankar-ghosal/>Tirthankar Ghosal</a>
|
<a href=/people/v/vignesh-edithal/>Vignesh Edithal</a>
|
<a href=/people/a/asif-ekbal/>Asif Ekbal</a>
|
<a href=/people/p/pushpak-bhattacharyya/>Pushpak Bhattacharyya</a>
|
<a href=/people/g/george-tsatsaronis/>George Tsatsaronis</a>
|
<a href=/people/s/srinivasa-satya-sameer-kumar-chivukula/>Srinivasa Satya Sameer Kumar Chivukula</a><br><a href=/volumes/C18-1/ class=text-muted>Proceedings of the 27th International Conference on Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1237><div class="card-body p-3 small">The rapid growth of documents across the web has necessitated finding means of discarding redundant documents and retaining novel ones. Capturing redundancy is challenging as it may involve investigating at a deep semantic level. Techniques for detecting such semantic redundancy at the document level are scarce. In this work we propose a deep Convolutional Neural Networks (CNN) based model to classify a document as novel or redundant with respect to a set of relevant documents already seen by the system. The <a href=https://en.wikipedia.org/wiki/System>system</a> is simple and do not require any manual feature engineering. Our novel scheme encodes relevant and relative information from both source and target texts to generate an <a href=https://en.wikipedia.org/wiki/Intermediate_representation>intermediate representation</a> which we coin as the Relative Document Vector (RDV). The proposed method outperforms the existing <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a> on a document-level novelty detection dataset by a margin of 5 % in terms of <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>. We further demonstrate the effectiveness of our approach on a standard paraphrase detection dataset where paraphrased passages closely resemble to semantically redundant documents.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2018.gwc-1.31.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2018--gwc-1--31 data-toggle=collapse aria-expanded=false aria-controls=abstract-2018.gwc-1.31 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2018.gwc-1.31/>Semi-automatic WordNet Linking using Word Embeddings<span class=acl-fixed-case>W</span>ord<span class=acl-fixed-case>N</span>et Linking using Word Embeddings</a></strong><br><a href=/people/k/kevin-patel/>Kevin Patel</a>
|
<a href=/people/d/diptesh-kanojia/>Diptesh Kanojia</a>
|
<a href=/people/p/pushpak-bhattacharyya/>Pushpak Bhattacharyya</a><br><a href=/volumes/2018.gwc-1/ class=text-muted>Proceedings of the 9th Global Wordnet Conference</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2018--gwc-1--31><div class="card-body p-3 small">Wordnets are rich lexico-semantic resources. Linked wordnets are extensions of <a href=https://en.wikipedia.org/wiki/Wordnet>wordnets</a>, which link similar concepts in <a href=https://en.wikipedia.org/wiki/Wordnet>wordnets</a> of different languages. Such <a href=https://en.wikipedia.org/wiki/Resource_(computer_science)>resources</a> are extremely useful in many Natural Language Processing (NLP) applications, primarily those based on knowledge-based approaches. In such approaches, these <a href=https://en.wikipedia.org/wiki/Natural_resource>resources</a> are considered as gold standard / oracle. Thus, it is crucial that these <a href=https://en.wikipedia.org/wiki/Resource_(computer_science)>resources</a> hold correct information. Thereby, they are created by human experts. However, manual maintenance of such <a href=https://en.wikipedia.org/wiki/Resource_(project_management)>resources</a> is a tedious and costly affair. Thus techniques that can aid the experts are desirable. In this paper, we propose an approach to link <a href=https://en.wikipedia.org/wiki/Wordnet>wordnets</a>. Given a synset of the source language, the approach returns a ranked list of potential candidate synsets in the target language from which the human expert can choose the correct one(s). Our technique is able to retrieve a winner synset in the top 10 ranked list for 60 % of all synsets and 70 % of noun synsets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2018.gwc-1.34.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2018--gwc-1--34 data-toggle=collapse aria-expanded=false aria-controls=abstract-2018.gwc-1.34 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2018.gwc-1.34/>An Iterative Approach for Unsupervised Most Frequent Sense Detection using <a href=https://en.wikipedia.org/wiki/WordNet>WordNet</a> and Word Embeddings<span class=acl-fixed-case>W</span>ord<span class=acl-fixed-case>N</span>et and Word Embeddings</a></strong><br><a href=/people/k/kevin-patel/>Kevin Patel</a>
|
<a href=/people/p/pushpak-bhattacharyya/>Pushpak Bhattacharyya</a><br><a href=/volumes/2018.gwc-1/ class=text-muted>Proceedings of the 9th Global Wordnet Conference</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2018--gwc-1--34><div class="card-body p-3 small">Given a word, what is the most frequent sense in which it occurs in a given corpus? Most Frequent Sense (MFS) is a strong baseline for unsupervised word sense disambiguation. If we have large amounts of sense-annotated corpora, MFS can be trivially created. However, sense-annotated corpora are a rarity. In this paper, we propose a method which can compute MFS from raw corpora. Our approach iteratively exploits the semantic congruity among related words in corpus. Our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> performs better compared to another similar work.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2018.gwc-1.37.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2018--gwc-1--37 data-toggle=collapse aria-expanded=false aria-controls=abstract-2018.gwc-1.37 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2018.gwc-1.37/>Hindi Wordnet for <a href=https://en.wikipedia.org/wiki/Language_pedagogy>Language Teaching</a> : Experiences and Lessons Learnt<span class=acl-fixed-case>H</span>indi <span class=acl-fixed-case>W</span>ordnet for Language Teaching: Experiences and Lessons Learnt</a></strong><br><a href=/people/h/hanumant-redkar/>Hanumant Redkar</a>
|
<a href=/people/r/rajita-shukla/>Rajita Shukla</a>
|
<a href=/people/s/sandhya-singh/>Sandhya Singh</a>
|
<a href=/people/j/jaya-saraswati/>Jaya Saraswati</a>
|
<a href=/people/l/laxmi-kashyap/>Laxmi Kashyap</a>
|
<a href=/people/d/diptesh-kanojia/>Diptesh Kanojia</a>
|
<a href=/people/p/preethi-jyothi/>Preethi Jyothi</a>
|
<a href=/people/m/malhar-kulkarni/>Malhar Kulkarni</a>
|
<a href=/people/p/pushpak-bhattacharyya/>Pushpak Bhattacharyya</a><br><a href=/volumes/2018.gwc-1/ class=text-muted>Proceedings of the 9th Global Wordnet Conference</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2018--gwc-1--37><div class="card-body p-3 small">This paper reports the work related to making Hindi Wordnet1 available as a digital resource for language learning and teaching, and the experiences and lessons that were learnt during the process. The <a href=https://en.wikipedia.org/wiki/Data_(computing)>language data</a> of the Hindi Wordnet has been suitably modified and enhanced to make it into a <a href=https://en.wikipedia.org/wiki/Language_acquisition>language learning aid</a>. This aid is based on modern pedagogical axioms and is aligned to the learning objectives of the syllabi of the school education in India. To make it into a comprehensive language tool, grammatical information has also been encoded, as far as these can be marked on the lexical items. The delivery of information is multi-layered, multi-sensory and is available across multiple <a href=https://en.wikipedia.org/wiki/Computing_platform>digital platforms</a>. The front end has been designed to offer an eye-catching user-friendly interface which is suitable for learners starting from age six onward. Preliminary testing of the <a href=https://en.wikipedia.org/wiki/Tool>tool</a> has been done and it has been modified as per the feedbacks that were received. Above all, the entire exercise has offered gainful insights into learning based on associative networks and how knowledge based on such <a href=https://en.wikipedia.org/wiki/Social_network>networks</a> can be made available to modern learners.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/Q18-1022.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-Q18-1022 data-toggle=collapse aria-expanded=false aria-controls=abstract-Q18-1022 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/Q18-1022/>Leveraging Orthographic Similarity for Multilingual Neural Transliteration</a></strong><br><a href=/people/a/anoop-kunchukuttan/>Anoop Kunchukuttan</a>
|
<a href=/people/m/mitesh-m-khapra/>Mitesh Khapra</a>
|
<a href=/people/g/gurneet-singh/>Gurneet Singh</a>
|
<a href=/people/p/pushpak-bhattacharyya/>Pushpak Bhattacharyya</a><br><a href=/volumes/Q18-1/ class=text-muted>Transactions of the Association for Computational Linguistics, Volume 6</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-Q18-1022><div class="card-body p-3 small">We address the task of joint training of transliteration models for multiple language pairs (multilingual transliteration). This is an instance of <a href=https://en.wikipedia.org/wiki/Multitask_learning>multitask learning</a>, where individual tasks (language pairs) benefit from sharing knowledge with related tasks. We focus on <a href=https://en.wikipedia.org/wiki/Transliteration>transliteration</a> involving related <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> i.e., <a href=https://en.wikipedia.org/wiki/Language_family>languages sharing writing systems</a> and <a href=https://en.wikipedia.org/wiki/Phoneme>phonetic properties</a> (orthographically similar languages). We propose a modified neural encoder-decoder model that maximizes parameter sharing across language pairs in order to effectively leverage orthographic similarity. We show that multilingual transliteration significantly outperforms bilingual transliteration in different scenarios (average increase of 58 % across a variety of languages we experimented with). We also show that multilingual transliteration models can generalize well to languages / language pairs not encountered during training and hence perform well on the zeroshot transliteration task. We show that further improvements can be achieved by using phonetic feature input.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3705.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3705 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3705 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3705/>Thank Goodness ! A Way to Measure Style in Student Essays</a></strong><br><a href=/people/s/sandeep-mathias/>Sandeep Mathias</a>
|
<a href=/people/p/pushpak-bhattacharyya/>Pushpak Bhattacharyya</a><br><a href=/volumes/W18-37/ class=text-muted>Proceedings of the 5th Workshop on Natural Language Processing Techniques for Educational Applications</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3705><div class="card-body p-3 small">Essays have two major components for scoring-content and style. In this paper, we describe a property of the essay, called goodness, and use it to predict the score given for the style of student essays. We compare our approach to solve this problem with baseline approaches, like <a href=https://en.wikipedia.org/wiki/Language_model>language modeling</a> and also a state-of-the-art deep learning system. We show that, despite being quite intuitive, our approach is very powerful in predicting the style of the essays.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1053.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1053 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1053 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-1053/>Solving Data Sparsity for Aspect Based Sentiment Analysis Using Cross-Linguality and Multi-Linguality</a></strong><br><a href=/people/m/md-shad-akhtar/>Md Shad Akhtar</a>
|
<a href=/people/p/palaash-sawant/>Palaash Sawant</a>
|
<a href=/people/s/sukanta-sen/>Sukanta Sen</a>
|
<a href=/people/a/asif-ekbal/>Asif Ekbal</a>
|
<a href=/people/p/pushpak-bhattacharyya/>Pushpak Bhattacharyya</a><br><a href=/volumes/N18-1/ class=text-muted>Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1053><div class="card-body p-3 small">Efficient word representations play an important role in solving various problems related to <a href=https://en.wikipedia.org/wiki/Natural_language_processing>Natural Language Processing (NLP)</a>, <a href=https://en.wikipedia.org/wiki/Data_mining>data mining</a>, <a href=https://en.wikipedia.org/wiki/Text_mining>text mining</a> etc. The issue of data sparsity poses a great challenge in creating efficient word representation model for solving the underlying problem. The <a href=https://en.wikipedia.org/wiki/Problem_solving>problem</a> is more intensified in resource-poor scenario due to the absence of sufficient amount of corpus. In this work we propose to minimize the effect of data sparsity by leveraging bilingual word embeddings learned through a <a href=https://en.wikipedia.org/wiki/Parallel_corpus>parallel corpus</a>. We train and evaluate Long Short Term Memory (LSTM) based architecture for aspect level sentiment classification. The neural network architecture is further assisted by the hand-crafted features for the <a href=https://en.wikipedia.org/wiki/Prediction>prediction</a>. We show the efficacy of the proposed <a href=https://en.wikipedia.org/wiki/Scientific_modelling>model</a> against <a href=https://en.wikipedia.org/wiki/Scientific_method>state-of-the-art methods</a> in two experimental setups i.e. multi-lingual and cross-lingual.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K18-1012.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K18-1012 data-toggle=collapse aria-expanded=false aria-controls=abstract-K18-1012 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/K18-1012/>Uncovering Code-Mixed Challenges : A Framework for Linguistically Driven Question Generation and Neural Based Question Answering</a></strong><br><a href=/people/d/deepak-gupta/>Deepak Gupta</a>
|
<a href=/people/p/pabitra-lenka/>Pabitra Lenka</a>
|
<a href=/people/a/asif-ekbal/>Asif Ekbal</a>
|
<a href=/people/p/pushpak-bhattacharyya/>Pushpak Bhattacharyya</a><br><a href=/volumes/K18-1/ class=text-muted>Proceedings of the 22nd Conference on Computational Natural Language Learning</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K18-1012><div class="card-body p-3 small">Existing research on question answering (QA) and comprehension reading (RC) are mainly focused on the resource-rich language like <a href=https://en.wikipedia.org/wiki/English_language>English</a>. In recent times, the rapid growth of multi-lingual web content has posed several challenges to the existing <a href=https://en.wikipedia.org/wiki/Quality_assurance>QA systems</a>. Code-mixing is one such challenge that makes the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> more complex. In this paper, we propose a linguistically motivated technique for code-mixed question generation (CMQG) and a neural network based architecture for code-mixed question answering (CMQA). For evaluation, we manually create the code-mixed questions for Hindi-English language pair. In order to show the effectiveness of our neural network based CMQA technique, we utilize two benchmark datasets, SQuAD and MMQA. Experiments show that our proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieves encouraging performance on CMQG and CMQA.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1089.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1089 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1089 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1089.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285801356 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-1089/>Identifying Transferable Information Across Domains for Cross-domain Sentiment Classification</a></strong><br><a href=/people/r/raksha-sharma/>Raksha Sharma</a>
|
<a href=/people/p/pushpak-bhattacharyya/>Pushpak Bhattacharyya</a>
|
<a href=/people/s/sandipan-dandapat/>Sandipan Dandapat</a>
|
<a href=/people/h/himanshu-sharad-bhatt/>Himanshu Sharad Bhatt</a><br><a href=/volumes/P18-1/ class=text-muted>Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1089><div class="card-body p-3 small">Getting manually labeled data in each domain is always an expensive and a time consuming task. Cross-domain sentiment analysis has emerged as a demanding concept where a labeled source domain facilitates a <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment classifier</a> for an unlabeled target domain. However, polarity orientation (positive or negative) and the significance of a word to express an opinion often differ from one domain to another domain. Owing to these differences, cross-domain sentiment classification is still a challenging task. In this paper, we propose that words that do not change their polarity and significance represent the transferable (usable) information across domains for cross-domain sentiment classification. We present a novel approach based on 2 test and cosine-similarity between context vector of words to identify polarity preserving significant words across domains. Furthermore, we show that a weighted ensemble of the <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifiers</a> enhances the cross-domain classification performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2011.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2011 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2011 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2011.Software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-2011/>Identification of Alias Links among Participants in Narratives</a></strong><br><a href=/people/s/sangameshwar-patil/>Sangameshwar Patil</a>
|
<a href=/people/s/sachin-pawar/>Sachin Pawar</a>
|
<a href=/people/s/swapnil-hingmire/>Swapnil Hingmire</a>
|
<a href=/people/g/girish-palshikar/>Girish Palshikar</a>
|
<a href=/people/v/vasudeva-varma/>Vasudeva Varma</a>
|
<a href=/people/p/pushpak-bhattacharyya/>Pushpak Bhattacharyya</a><br><a href=/volumes/P18-2/ class=text-muted>Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2011><div class="card-body p-3 small">Identification of distinct and independent participants (entities of interest) in a narrative is an important task for many NLP applications. This <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> becomes challenging because these participants are often referred to using multiple aliases. In this paper, we propose an approach based on linguistic knowledge for identification of aliases mentioned using <a href=https://en.wikipedia.org/wiki/Proper_noun>proper nouns</a>, <a href=https://en.wikipedia.org/wiki/Pronoun>pronouns</a> or <a href=https://en.wikipedia.org/wiki/Noun_phrase>noun phrases</a> with common noun headword. We use Markov Logic Network (MLN) to encode the linguistic knowledge for identification of aliases. We evaluate on four <a href=https://en.wikipedia.org/wiki/Multiculturalism>diverse history narratives</a> of varying complexity. Our approach performs better than the state-of-the-art approach as well as a combination of standard <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>named entity recognition</a> and coreference resolution techniques.</div></div><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-2006.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-2006 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-2006 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/I17-2006.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/I17-2006/>Towards Lower Bounds on Number of Dimensions for Word Embeddings</a></strong><br><a href=/people/k/kevin-patel/>Kevin Patel</a>
|
<a href=/people/p/pushpak-bhattacharyya/>Pushpak Bhattacharyya</a><br><a href=/volumes/I17-2/ class=text-muted>Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-2006><div class="card-body p-3 small">Word embeddings are a relatively new addition to the modern NLP researcher&#8217;s toolkit. However, unlike other <a href=https://en.wikipedia.org/wiki/Tool>tools</a>, <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> are used in a black box manner. There are very few studies regarding various <a href=https://en.wikipedia.org/wiki/Hyperparameter>hyperparameters</a>. One such <a href=https://en.wikipedia.org/wiki/Hyperparameter>hyperparameter</a> is the dimension of word embeddings. They are rather decided based on a rule of thumb : in the range 50 to 300. In this paper, we show that the <a href=https://en.wikipedia.org/wiki/Dimension_(vector_space)>dimension</a> should instead be chosen based on <a href=https://en.wikipedia.org/wiki/Corpus_linguistics>corpus statistics</a>. More specifically, we show that the number of pairwise equidistant words of the corpus vocabulary (as defined by some distance / similarity metric) gives a lower bound on the the number of dimensions, and going below this bound results in degradation of quality of learned word embeddings. Through our evaluations on standard word embedding evaluation tasks, we show that for dimensions higher than or equal to the bound, we get better results as compared to the ones below it.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-2048.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-2048 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-2048 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-2048/>Utilizing Lexical Similarity between Related, Low-resource Languages for Pivot-based SMT<span class=acl-fixed-case>SMT</span></a></strong><br><a href=/people/a/anoop-kunchukuttan/>Anoop Kunchukuttan</a>
|
<a href=/people/m/maulik-shah/>Maulik Shah</a>
|
<a href=/people/p/pradyot-prakash/>Pradyot Prakash</a>
|
<a href=/people/p/pushpak-bhattacharyya/>Pushpak Bhattacharyya</a><br><a href=/volumes/I17-2/ class=text-muted>Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-2048><div class="card-body p-3 small">We investigate pivot-based translation between related languages in a low resource, phrase-based SMT setting. We show that a subword-level pivot-based SMT model using a related pivot language is substantially better than word and morpheme-level pivot models. It is also highly competitive with the best direct translation model, which is encouraging as no direct source-target training corpus is used. We also show that combining multiple related language pivot models can rival a direct translation model. Thus, the use of <a href=https://en.wikipedia.org/wiki/Subword>subwords</a> as translation units coupled with multiple related pivot languages can compensate for the lack of a direct parallel corpus.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-4031.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-4031 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-4031 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-4031/>IITP at IJCNLP-2017 Task 4 : Auto Analysis of Customer Feedback using CNN and GRU Network<span class=acl-fixed-case>IITP</span> at <span class=acl-fixed-case>IJCNLP</span>-2017 Task 4: Auto Analysis of Customer Feedback using <span class=acl-fixed-case>CNN</span> and <span class=acl-fixed-case>GRU</span> Network</a></strong><br><a href=/people/d/deepak-gupta/>Deepak Gupta</a>
|
<a href=/people/p/pabitra-lenka/>Pabitra Lenka</a>
|
<a href=/people/h/harsimran-bedi/>Harsimran Bedi</a>
|
<a href=/people/a/asif-ekbal/>Asif Ekbal</a>
|
<a href=/people/p/pushpak-bhattacharyya/>Pushpak Bhattacharyya</a><br><a href=/volumes/I17-4/ class=text-muted>Proceedings of the IJCNLP 2017, Shared Tasks</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-4031><div class="card-body p-3 small">Analyzing customer feedback is the best way to channelize the data into new <a href=https://en.wikipedia.org/wiki/Marketing_strategy>marketing strategies</a> that benefit entrepreneurs as well as customers. Therefore an automated system which can analyze the <a href=https://en.wikipedia.org/wiki/Consumer_behaviour>customer behavior</a> is in great demand. Users may write feedbacks in any language, and hence mining appropriate information often becomes intractable. Especially in a traditional feature-based supervised model, it is difficult to build a generic system as one has to understand the concerned language for finding the relevant <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a>. In order to overcome this, we propose deep Convolutional Neural Network (CNN) and Recurrent Neural Network (RNN) based approaches that do not require handcrafting of features. We evaluate these techniques for analyzing customer feedback sentences on four languages, namely <a href=https://en.wikipedia.org/wiki/English_language>English</a>, <a href=https://en.wikipedia.org/wiki/French_language>French</a>, <a href=https://en.wikipedia.org/wiki/Japanese_language>Japanese</a> and <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a>. Our empirical analysis shows that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> perform well in all the four languages on the setups of IJCNLP Shared Task on Customer Feedback Analysis. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieved the second rank in <a href=https://en.wikipedia.org/wiki/French_language>French</a>, with an <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of 71.75 % and third ranks for all the other languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1035.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1035 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1035 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234952512 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-1035/>Learning Cognitive Features from Gaze Data for Sentiment and Sarcasm Classification using <a href=https://en.wikipedia.org/wiki/Convolutional_neural_network>Convolutional Neural Network</a></a></strong><br><a href=/people/a/abhijit-mishra/>Abhijit Mishra</a>
|
<a href=/people/k/kuntal-dey/>Kuntal Dey</a>
|
<a href=/people/p/pushpak-bhattacharyya/>Pushpak Bhattacharyya</a><br><a href=/volumes/P17-1/ class=text-muted>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1035><div class="card-body p-3 small">Cognitive NLP systems- i.e., NLP systems that make use of behavioral data-augment traditional text-based features with cognitive features extracted from eye-movement patterns, EEG signals, brain-imaging etc. Such extraction of features is typically manual. We contend that manual extraction of features may not be the best way to tackle text subtleties that characteristically prevail in complex classification tasks like <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>Sentiment Analysis</a> and Sarcasm Detection, and that even the extraction and choice of <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> should be delegated to the learning system. We introduce a framework to automatically extract cognitive features from the eye-movement / gaze data of human readers reading the text and use them as features along with textual features for the tasks of sentiment polarity and sarcasm detection. Our proposed <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> is based on Convolutional Neural Network (CNN). The <a href=https://en.wikipedia.org/wiki/CNN>CNN</a> learns features from both gaze and <a href=https://en.wikipedia.org/wiki/Written_language>text</a> and uses them to classify the input text. We test our technique on published sentiment and sarcasm labeled datasets, enriched with gaze information, to show that using a combination of automatically learned text and gaze features often yields better classification performance over (i) CNN based systems that rely on text input alone and (ii) existing systems that rely on handcrafted gaze and textual features.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2338.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2338 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2338 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-2338/>Adapting Pre-trained Word Embeddings For Use In Medical Coding</a></strong><br><a href=/people/k/kevin-patel/>Kevin Patel</a>
|
<a href=/people/d/divya-patel/>Divya Patel</a>
|
<a href=/people/m/mansi-golakiya/>Mansi Golakiya</a>
|
<a href=/people/p/pushpak-bhattacharyya/>Pushpak Bhattacharyya</a>
|
<a href=/people/n/nilesh-birari/>Nilesh Birari</a><br><a href=/volumes/W17-23/ class=text-muted>BioNLP 2017</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2338><div class="card-body p-3 small">Word embeddings are a crucial component in modern <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a>. Pre-trained embeddings released by different groups have been a major reason for their popularity. However, they are trained on generic corpora, which limits their direct use for domain specific tasks. In this paper, we propose a method to add task specific information to pre-trained word embeddings. Such <a href=https://en.wikipedia.org/wiki/Information>information</a> can improve their utility. We add information from medical coding data, as well as the first level from the hierarchy of ICD-10 medical code set to different pre-trained word embeddings. We adapt CBOW algorithm from the <a href=https://en.wikipedia.org/wiki/Word2vec>word2vec package</a> for our purpose. We evaluated our approach on five different pre-trained word embeddings. Both the original word embeddings, and their modified versions (the ones with added information) were used for automated review of medical coding. The modified <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> give an improvement in <a href=https://en.wikipedia.org/wiki/F-score>f-score</a> by 1 % on the 5-fold evaluation on a private medical claims dataset. Our results show that adding extra information is possible and beneficial for the task at hand.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4102.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4102 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4102 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4102/>Learning variable length units for SMT between related languages via Byte Pair Encoding<span class=acl-fixed-case>SMT</span> between related languages via Byte Pair Encoding</a></strong><br><a href=/people/a/anoop-kunchukuttan/>Anoop Kunchukuttan</a>
|
<a href=/people/p/pushpak-bhattacharyya/>Pushpak Bhattacharyya</a><br><a href=/volumes/W17-41/ class=text-muted>Proceedings of the First Workshop on Subword and Character Level Models in NLP</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4102><div class="card-body p-3 small">We explore the use of <a href=https://en.wikipedia.org/wiki/Segment_(linguistics)>segments</a> learnt using Byte Pair Encoding (referred to as BPE units) as basic units for <a href=https://en.wikipedia.org/wiki/Statistical_machine_translation>statistical machine translation</a> between related languages and compare it with orthographic syllables, which are currently the best performing basic units for this translation task. BPE identifies the most frequent character sequences as basic units, while orthographic syllables are linguistically motivated pseudo-syllables. We show that BPE units modestly outperform orthographic syllables as units of translation, showing up to 11 % increase in BLEU score. While orthographic syllables can be used only for languages whose <a href=https://en.wikipedia.org/wiki/Writing_system>writing systems</a> use vowel representations, BPE is writing system independent and we show that BPE outperforms other units for non-vowel writing systems too. Our results are supported by extensive experimentation spanning multiple language families and <a href=https://en.wikipedia.org/wiki/Writing_system>writing systems</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5717.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5717 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5717 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5717/>Comparing Recurrent and Convolutional Architectures for English-Hindi Neural Machine Translation<span class=acl-fixed-case>E</span>nglish-<span class=acl-fixed-case>H</span>indi Neural Machine Translation</a></strong><br><a href=/people/s/sandhya-singh/>Sandhya Singh</a>
|
<a href=/people/r/ritesh-panjwani/>Ritesh Panjwani</a>
|
<a href=/people/a/anoop-kunchukuttan/>Anoop Kunchukuttan</a>
|
<a href=/people/p/pushpak-bhattacharyya/>Pushpak Bhattacharyya</a><br><a href=/volumes/W17-57/ class=text-muted>Proceedings of the 4th Workshop on Asian Translation (WAT2017)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5717><div class="card-body p-3 small">In this paper, we empirically compare the two encoder-decoder neural machine translation architectures : convolutional sequence to sequence model (ConvS2S) and recurrent sequence to sequence model (RNNS2S) for English-Hindi language pair as part of IIT Bombay&#8217;s submission to WAT2017 shared task. We report the results for both English-Hindi and Hindi-English direction of language pair.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S17-2009.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S17-2009 data-toggle=collapse aria-expanded=false aria-controls=abstract-S17-2009 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=S17-2009" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/S17-2009/>IIT-UHH at SemEval-2017 Task 3 : Exploring Multiple Features for Community Question Answering and Implicit Dialogue Identification<span class=acl-fixed-case>IIT</span>-<span class=acl-fixed-case>UHH</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2017 Task 3: Exploring Multiple Features for Community Question Answering and Implicit Dialogue Identification</a></strong><br><a href=/people/t/titas-nandi/>Titas Nandi</a>
|
<a href=/people/c/chris-biemann/>Chris Biemann</a>
|
<a href=/people/s/seid-muhie-yimam/>Seid Muhie Yimam</a>
|
<a href=/people/d/deepak-gupta/>Deepak Gupta</a>
|
<a href=/people/s/sarah-kohail/>Sarah Kohail</a>
|
<a href=/people/a/asif-ekbal/>Asif Ekbal</a>
|
<a href=/people/p/pushpak-bhattacharyya/>Pushpak Bhattacharyya</a><br><a href=/volumes/S17-2/ class=text-muted>Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S17-2009><div class="card-body p-3 small">In this paper we present the system for Answer Selection and Ranking in Community Question Answering, which we build as part of our participation in SemEval-2017 Task 3. We develop a Support Vector Machine (SVM) based system that makes use of textual, domain-specific, word-embedding and topic-modeling features. In addition, we propose a novel <a href=https://en.wikipedia.org/wiki/Methodology>method</a> for dialogue chain identification in <a href=https://en.wikipedia.org/wiki/Internet_forum>comment threads</a>. Our primary submission won subtask C, outperforming other systems in all the primary evaluation metrics. We performed well in other English subtasks, ranking third in subtask A and eighth in subtask B. We also developed open source toolkits for all the three English subtasks by the name cQARank [ ].<url>https://github.com/TitasNandi/cQARank</url>].\n</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S17-2087.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S17-2087 data-toggle=collapse aria-expanded=false aria-controls=abstract-S17-2087 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/S17-2087/>IITP at SemEval-2017 Task 8 : A Supervised Approach for Rumour Evaluation<span class=acl-fixed-case>IITP</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2017 Task 8 : A Supervised Approach for Rumour Evaluation</a></strong><br><a href=/people/v/vikram-singh/>Vikram Singh</a>
|
<a href=/people/s/sunny-narayan/>Sunny Narayan</a>
|
<a href=/people/m/md-shad-akhtar/>Md Shad Akhtar</a>
|
<a href=/people/a/asif-ekbal/>Asif Ekbal</a>
|
<a href=/people/p/pushpak-bhattacharyya/>Pushpak Bhattacharyya</a><br><a href=/volumes/S17-2/ class=text-muted>Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S17-2087><div class="card-body p-3 small">This paper describes our <a href=https://en.wikipedia.org/wiki/System>system</a> participation in the SemEval-2017 Task 8 &#8216;RumourEval : Determining rumour veracity and support for rumours&#8217;. The objective of this task was to predict the stance and veracity of the underlying <a href=https://en.wikipedia.org/wiki/Rumor>rumour</a>. We propose a supervised classification approach employing several lexical, content and twitter specific features for learning. Evaluation shows promising results for both the <a href=https://en.wikipedia.org/wiki/Problem_solving>problems</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S17-2154.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S17-2154 data-toggle=collapse aria-expanded=false aria-controls=abstract-S17-2154 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/S17-2154/>IITP at SemEval-2017 Task 5 : An Ensemble of Deep Learning and Feature Based Models for Financial Sentiment Analysis<span class=acl-fixed-case>IITP</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2017 Task 5: An Ensemble of Deep Learning and Feature Based Models for Financial Sentiment Analysis</a></strong><br><a href=/people/d/deepanway-ghosal/>Deepanway Ghosal</a>
|
<a href=/people/s/shobhit-bhatnagar/>Shobhit Bhatnagar</a>
|
<a href=/people/m/md-shad-akhtar/>Md Shad Akhtar</a>
|
<a href=/people/a/asif-ekbal/>Asif Ekbal</a>
|
<a href=/people/p/pushpak-bhattacharyya/>Pushpak Bhattacharyya</a><br><a href=/volumes/S17-2/ class=text-muted>Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S17-2154><div class="card-body p-3 small">In this paper we propose an ensemble based model which combines state of the art deep learning sentiment analysis algorithms like Convolution Neural Network (CNN) and Long Short Term Memory (LSTM) along with feature based models to identify optimistic or pessimistic sentiments associated with companies and stocks in financial texts. We build our <a href=https://en.wikipedia.org/wiki/System>system</a> to participate in a competition organized by Semantic Evaluation 2017 International Workshop. We combined predictions from various models using an <a href=https://en.wikipedia.org/wiki/Artificial_neural_network>artificial neural network</a> to determine the opinion towards an entity in (a) Microblog Messages and (b) News Headlines data. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> achieved a cosine similarity score of 0.751 and 0.697 for the above two tracks giving us the rank of 2nd and 7th best team respectively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1058.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1058 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1058 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1058/>Sentiment Intensity Ranking among Adjectives Using Sentiment Bearing Word Embeddings</a></strong><br><a href=/people/r/raksha-sharma/>Raksha Sharma</a>
|
<a href=/people/a/arpan-somani/>Arpan Somani</a>
|
<a href=/people/l/lakshya-kumar/>Lakshya Kumar</a>
|
<a href=/people/p/pushpak-bhattacharyya/>Pushpak Bhattacharyya</a><br><a href=/volumes/D17-1/ class=text-muted>Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1058><div class="card-body p-3 small">Identification of intensity ordering among polar (positive or negative) words which have the same <a href=https://en.wikipedia.org/wiki/Semantics>semantics</a> can lead to a fine-grained sentiment analysis. For example, &#8216;master&#8217;, &#8216;seasoned&#8217; and &#8216;familiar&#8217; point to different intensity levels, though they all convey the same meaning (semantics), i.e., expertise : having a good knowledge of. In this paper, we propose a <a href=https://en.wikipedia.org/wiki/Semi-supervised_learning>semi-supervised technique</a> that uses sentiment bearing word embeddings to produce a continuous ranking among adjectives that share common semantics. Our system demonstrates a strong Spearman&#8217;s rank correlation of 0.83 with the gold standard ranking. We show that sentiment bearing word embeddings facilitate a more accurate intensity ranking system than other standard word embeddings (word2vec and GloVe). Word2vec is the state-of-the-art for intensity ordering task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-3002.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-3002 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-3002 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-3002/>Computational Sarcasm</a></strong><br><a href=/people/p/pushpak-bhattacharyya/>Pushpak Bhattacharyya</a>
|
<a href=/people/a/aditya-joshi/>Aditya Joshi</a><br><a href=/volumes/D17-3/ class=text-muted>Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing: Tutorial Abstracts</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-3002><div class="card-body p-3 small">Sarcasm is a form of <a href=https://en.wikipedia.org/wiki/Irony>verbal irony</a> that is intended to express contempt or ridicule. Motivated by challenges posed by sarcastic text to <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a>, computational approaches to <a href=https://en.wikipedia.org/wiki/Sarcasm>sarcasm</a> have witnessed a growing interest at NLP forums in the past decade. Computational sarcasm refers to automatic approaches pertaining to <a href=https://en.wikipedia.org/wiki/Sarcasm>sarcasm</a>. The tutorial will provide a bird&#8217;s-eye view of the research in computational sarcasm for text, while focusing on significant milestones. The tutorial begins with linguistic theories of sarcasm, with a focus on incongruity : a useful notion that underlies <a href=https://en.wikipedia.org/wiki/Sarcasm>sarcasm</a> and other forms of <a href=https://en.wikipedia.org/wiki/Literal_and_figurative_language>figurative language</a>. Since the most significant work in computational sarcasm is sarcasm detection : predicting whether a given piece of text is sarcastic or not, sarcasm detection forms the focus hereafter. We begin our discussion on sarcasm detection with <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>, touching on strategies, challenges and nature of datasets. Then, we describe algorithms for sarcasm detection : rule-based (where a specific evidence of sarcasm is utilised as a rule), statistical classifier-based (where features are designed for a statistical classifier), a topic model-based technique, and deep learning-based algorithms for sarcasm detection. In case of each of these <a href=https://en.wikipedia.org/wiki/Algorithm>algorithms</a>, we refer to our work on sarcasm detection and share our learnings. Since information beyond the text to be classified, contextual information is useful for sarcasm detection, we then describe approaches that use such information through conversational context or author-specific context. We then follow it by novel areas in computational sarcasm such as sarcasm generation, sarcasm v / s irony classification, etc. We then summarise the tutorial and describe future directions based on errors reported in past work. The tutorial will end with a demonstration of our work on sarcasm detection. This tutorial will be of interest to researchers investigating computational sarcasm and related areas such as <a href=https://en.wikipedia.org/wiki/Computational_humour>computational humour</a>, figurative language understanding, emotion and sentiment sentiment analysis, etc. The tutorial is motivated by our continually evolving survey paper of sarcasm detection, that is available on arXiv at : Joshi, Aditya, Pushpak Bhattacharyya, and Mark James Carman. Automatic Sarcasm Detection : A Survey. arXiv preprint arXiv:1602.03426 (2016).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1077.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1077 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1077 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1077/>End-to-end Relation Extraction using <a href=https://en.wikipedia.org/wiki/Neural_network>Neural Networks</a> and <a href=https://en.wikipedia.org/wiki/Markov_logic_network>Markov Logic Networks</a><span class=acl-fixed-case>M</span>arkov <span class=acl-fixed-case>L</span>ogic <span class=acl-fixed-case>N</span>etworks</a></strong><br><a href=/people/s/sachin-pawar/>Sachin Pawar</a>
|
<a href=/people/p/pushpak-bhattacharyya/>Pushpak Bhattacharyya</a>
|
<a href=/people/g/girish-palshikar/>Girish Palshikar</a><br><a href=/volumes/E17-1/ class=text-muted>Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1077><div class="card-body p-3 small">End-to-end relation extraction refers to identifying boundaries of entity mentions, entity types of these mentions and appropriate <a href=https://en.wikipedia.org/wiki/Semantic_relation>semantic relation</a> for each pair of mentions. Traditionally, separate <a href=https://en.wikipedia.org/wiki/Predictive_modelling>predictive models</a> were trained for each of these tasks and were used in a pipeline fashion where output of one <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is fed as input to another. But it was observed that addressing some of these <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> jointly results in better performance. We propose a single, joint neural network based model to carry out all the three tasks of boundary identification, <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entity type classification</a> and relation type classification. This <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> is referred to as All Word Pairs model (AWP-NN) as it assigns an appropriate label to each word pair in a given sentence for performing end-to-end relation extraction. We also propose to refine output of the AWP-NN model by using inference in Markov Logic Networks (MLN) so that additional <a href=https://en.wikipedia.org/wiki/Domain_knowledge>domain knowledge</a> can be effectively incorporated. We demonstrate effectiveness of our approach by achieving better end-to-end relation extraction performance than all 4 previous joint modelling approaches, on the standard dataset of ACE 2004.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1109.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1109 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1109 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1109/>Entity Extraction in Biomedical Corpora : An Approach to Evaluate Word Embedding Features with PSO based Feature Selection<span class=acl-fixed-case>PSO</span> based Feature Selection</a></strong><br><a href=/people/s/shweta-yadav/>Shweta Yadav</a>
|
<a href=/people/a/asif-ekbal/>Asif Ekbal</a>
|
<a href=/people/s/sriparna-saha/>Sriparna Saha</a>
|
<a href=/people/p/pushpak-bhattacharyya/>Pushpak Bhattacharyya</a><br><a href=/volumes/E17-1/ class=text-muted>Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1109><div class="card-body p-3 small">Text mining has drawn significant attention in recent past due to the rapid growth in biomedical and clinical records. Entity extraction is one of the fundamental components for <a href=https://en.wikipedia.org/wiki/Biomedical_text_mining>biomedical text mining</a>. In this paper, we propose a novel approach of <a href=https://en.wikipedia.org/wiki/Feature_selection>feature selection</a> for <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entity extraction</a> that exploits the concept of <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning</a> and Particle Swarm Optimization (PSO). The system utilizes word embedding features along with several other <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> extracted by studying the properties of the <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>. We obtain an interesting observation that compact word embedding features as determined by PSO are more effective compared to the entire word embedding feature set for entity extraction. The proposed system is evaluated on three benchmark biomedical datasets such as GENIA, GENETAG, and AiMed. The effectiveness of the proposed approach is evident with significant performance gains over the baseline models as well as the other existing systems. We observe improvements of 7.86 %, 5.27 % and 7.25 % <a href=https://en.wikipedia.org/wiki/F-measure>F-measure</a> points over the baseline models for GENIA, GENETAG, and AiMed dataset respectively.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Pushpak+Bhattacharyya" title="Search for 'Pushpak Bhattacharyya' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/a/asif-ekbal/ class=align-middle>Asif Ekbal</a>
<span class="badge badge-secondary align-middle ml-2">19</span></li><li class=list-group-item><a href=/people/d/deepak-gupta/ class=align-middle>Deepak Gupta</a>
<span class="badge badge-secondary align-middle ml-2">6</span></li><li class=list-group-item><a href=/people/a/anoop-kunchukuttan/ class=align-middle>Anoop Kunchukuttan</a>
<span class="badge badge-secondary align-middle ml-2">6</span></li><li class=list-group-item><a href=/people/k/kevin-patel/ class=align-middle>Kevin Patel</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/people/d/diptesh-kanojia/ class=align-middle>Diptesh Kanojia</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/m/md-shad-akhtar/ class=align-middle>Md Shad Akhtar</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/people/g/girish-palshikar/ class=align-middle>Girish Palshikar</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/people/s/sachin-pawar/ class=align-middle>Sachin Pawar</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/people/p/pabitra-lenka/ class=align-middle>Pabitra Lenka</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/s/sriparna-saha/ class=align-middle>Sriparna Saha</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/m/malhar-kulkarni/ class=align-middle>Malhar Kulkarni</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/j/jyotsana-khatri/ class=align-middle>Jyotsana Khatri</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/s/sangameshwar-patil/ class=align-middle>Sangameshwar Patil</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/s/swapnil-hingmire/ class=align-middle>Swapnil Hingmire</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/v/vasudeva-varma/ class=align-middle>Vasudeva Varma</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/k/kamal-kumar-gupta/ class=align-middle>Kamal Kumar Gupta</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/h/harsimran-bedi/ class=align-middle>Harsimran Bedi</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/g/gholamreza-haffari/ class=align-middle>Gholamreza Haffari</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/n/nikhil-saini/ class=align-middle>Nikhil Saini</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/p/preethi-jyothi/ class=align-middle>Preethi Jyothi</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/s/sandhya-singh/ class=align-middle>Sandhya Singh</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/d/deepanway-ghosal/ class=align-middle>Deepanway Ghosal</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/r/raksha-sharma/ class=align-middle>Raksha Sharma</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/a/arpan-somani/ class=align-middle>Arpan Somani</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/l/lakshya-kumar/ class=align-middle>Lakshya Kumar</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/a/aditya-joshi/ class=align-middle>Aditya Joshi</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/t/toshiaki-nakazawa/ class=align-middle>Toshiaki Nakazawa</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/h/hideki-nakayama/ class=align-middle>Hideki Nakayama</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/i/isao-goto/ class=align-middle>Isao Goto</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/c/chenchen-ding/ class=align-middle>Chenchen Ding</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/r/raj-dabre/ class=align-middle>Raj Dabre</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/h/hiroshi-manabe/ class=align-middle>Hiroshi Manabe</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/w/win-pa-pa/ class=align-middle>Win Pa Pa</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/s/shantipriya-parida/ class=align-middle>Shantipriya Parida</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/o/ondrej-bojar/ class=align-middle>Ondřej Bojar</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/k/katsuhito-sudoh/ class=align-middle>Katsuhito Sudoh</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/s/sadao-kurohashi/ class=align-middle>Sadao Kurohashi</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/s/sandeep-mathias/ class=align-middle>Sandeep Mathias</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/n/nitin-ramrakhiyani/ class=align-middle>Nitin Ramrakhiyani</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/h/hardik-chauhan/ class=align-middle>Hardik Chauhan</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/s/sukanta-sen/ class=align-middle>Sukanta Sen</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/r/rejwanul-haque/ class=align-middle>Rejwanul Haque</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/andy-way/ class=align-middle>Andy Way</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/tamali-banerjee/ class=align-middle>Tamali Banerjee</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/rudra-v-murthy/ class=align-middle>Rudra V Murthy</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/rajkumar-pujari/ class=align-middle>Rajkumar Pujari</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/anutosh-maitra/ class=align-middle>Anutosh Maitra</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/tom-jain/ class=align-middle>Tom Jain</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/shubhashis-sengupta/ class=align-middle>Shubhashis Sengupta</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/tirthankar-ghosal/ class=align-middle>Tirthankar Ghosal</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/v/vignesh-edithal/ class=align-middle>Vignesh Edithal</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/g/george-tsatsaronis/ class=align-middle>George Tsatsaronis</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/srinivasa-satya-sameer-kumar-chivukula/ class=align-middle>Srinivasa Satya Sameer Kumar Chivukula</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/maulik-shah/ class=align-middle>Maulik Shah</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/p/pradyot-prakash/ class=align-middle>Pradyot Prakash</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/tulika-saha/ class=align-middle>Tulika Saha</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/aditya-patra/ class=align-middle>Aditya Patra</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/abhijit-mishra/ class=align-middle>Abhijit Mishra</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/kuntal-dey/ class=align-middle>Kuntal Dey</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/deeksha-varshney/ class=align-middle>Deeksha Varshney</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/p/prashant-sharma/ class=align-middle>Prashant Sharma</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/sayali-ghodekar/ class=align-middle>Sayali Ghodekar</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/drumil-trivedi/ class=align-middle>Drumil Trivedi</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/shreya-khare/ class=align-middle>Shreya Khare</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/tejas-dhamecha/ class=align-middle>Tejas Dhamecha</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/samarth-bharadwaj/ class=align-middle>Samarth Bharadwaj</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/dipti-misra-sharma/ class=align-middle>Dipti Misra Sharma</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/arjun-roy/ class=align-middle>Arjun Roy</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/kingshuk-basak/ class=align-middle>Kingshuk Basak</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/tanik-saikh/ class=align-middle>Tanik Saikh</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/arkadipta-de/ class=align-middle>Arkadipta De</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/divya-patel/ class=align-middle>Divya Patel</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/mansi-golakiya/ class=align-middle>Mansi Golakiya</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/n/nilesh-birari/ class=align-middle>Nilesh Birari</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/ritesh-panjwani/ class=align-middle>Ritesh Panjwani</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/hanumant-redkar/ class=align-middle>Hanumant Redkar</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/rajita-shukla/ class=align-middle>Rajita Shukla</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jaya-saraswati/ class=align-middle>Jaya Saraswati</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/laxmi-kashyap/ class=align-middle>Laxmi Kashyap</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/dushyant-singh-chauhan/ class=align-middle>Dushyant Singh Chauhan</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/dhanush-s-r/ class=align-middle>Dhanush S R</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/chandresh-kanani/ class=align-middle>Chandresh Kanani</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/titas-nandi/ class=align-middle>Titas Nandi</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/chris-biemann/ class=align-middle>Chris Biemann</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/seid-muhie-yimam/ class=align-middle>Seid Muhie Yimam</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/sarah-kohail/ class=align-middle>Sarah Kohail</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/v/vikram-singh/ class=align-middle>Vikram Singh</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/sunny-narayan/ class=align-middle>Sunny Narayan</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/shobhit-bhatnagar/ class=align-middle>Shobhit Bhatnagar</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/mitesh-m-khapra/ class=align-middle>Mitesh M. Khapra</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/g/gurneet-singh/ class=align-middle>Gurneet Singh</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/hideya-mino/ class=align-middle>Hideya Mino</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/shohei-higashiyama/ class=align-middle>Shohei Higashiyama</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/chenhui-chu/ class=align-middle>Chenhui Chu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/akiko-eriguchi/ class=align-middle>Akiko Eriguchi</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/kaori-abe/ class=align-middle>Kaori Abe</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yusuke-oda/ class=align-middle>Yusuke Oda</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/shivam-mhaskar/ class=align-middle>Shivam Mhaskar</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/aditya-jain/ class=align-middle>Aditya Jain</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/aakash-banerjee/ class=align-middle>Aakash Banerjee</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/abhijeet-dubey/ class=align-middle>Abhijeet Dubey</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/sapan-shah/ class=align-middle>Sapan Shah</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/sreedhar-reddy/ class=align-middle>Sreedhar Reddy</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/ravi-tej-akella/ class=align-middle>Ravi Tej Akella</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/kumar-saurav/ class=align-middle>Kumar Saurav</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/kumar-saunack/ class=align-middle>Kumar Saunack</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/hidaya-mino/ class=align-middle>Hidaya Mino</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/dushyant-chauhan/ class=align-middle>Dushyant Chauhan</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/soujanya-poria/ class=align-middle>Soujanya Poria</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/p/palaash-sawant/ class=align-middle>Palaash Sawant</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/shweta-yadav/ class=align-middle>Shweta Yadav</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/sandipan-dandapat/ class=align-middle>Sandipan Dandapat</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/himanshu-sharad-bhatt/ class=align-middle>Himanshu Sharad Bhatt</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/mauajama-firdaus/ class=align-middle>Mauajama Firdaus</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/ws/ class=align-middle>WS</a><span class="badge badge-secondary align-middle ml-2">7</span></li><li class=list-group-item><a href=/venues/coling/ class=align-middle>COLING</a><span class="badge badge-secondary align-middle ml-2">6</span></li><li class=list-group-item><a href=/venues/acl/ class=align-middle>ACL</a><span class="badge badge-secondary align-middle ml-2">6</span></li><li class=list-group-item><a href=/venues/eacl/ class=align-middle>EACL</a><span class="badge badge-secondary align-middle ml-2">5</span></li><li class=list-group-item><a href=/venues/semeval/ class=align-middle>SemEval</a><span class="badge badge-secondary align-middle ml-2">4</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-venues aria-expanded=false aria-controls=more-venues>show all...</li><div class="collapse border-top" id=more-venues><li class=list-group-item><a href=/venues/wat/ class=align-middle>WAT</a><span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/venues/ijcnlp/ class=align-middle>IJCNLP</a><span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/venues/icon/ class=align-middle>ICON</a><span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/venues/gwc/ class=align-middle>GWC</a><span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/venues/naacl/ class=align-middle>NAACL</a><span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/venues/aacl/ class=align-middle>AACL</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/emnlp/ class=align-middle>EMNLP</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/eamt/ class=align-middle>EAMT</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/mtsummit/ class=align-middle>MTSummit</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/tacl/ class=align-middle>TACL</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/lrec/ class=align-middle>LREC</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/bea/ class=align-middle>BEA</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/conll/ class=align-middle>CoNLL</a><span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>