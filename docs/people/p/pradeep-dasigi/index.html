<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Pradeep Dasigi - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Pradeep</span> <span class=font-weight-bold>Dasigi</span></h2><hr><div class=row><div class=col-lg-9><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.497.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--497 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.497 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.497" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.497/>Mitigating False-Negative Contexts in Multi-document Question Answering with Retrieval Marginalization</a></strong><br><a href=/people/a/ansong-ni/>Ansong Ni</a>
|
<a href=/people/m/matt-gardner/>Matt Gardner</a>
|
<a href=/people/p/pradeep-dasigi/>Pradeep Dasigi</a><br><a href=/volumes/2021.emnlp-main/ class=text-muted>Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--497><div class="card-body p-3 small">Question Answering (QA) tasks requiring information from multiple documents often rely on a <a href=https://en.wikipedia.org/wiki/Information_retrieval>retrieval model</a> to identify relevant information for <a href=https://en.wikipedia.org/wiki/Reason>reasoning</a>. The retrieval model is typically trained to maximize the likelihood of the labeled supporting evidence. However, when retrieving from large text corpora such as <a href=https://en.wikipedia.org/wiki/Wikipedia>Wikipedia</a>, the correct answer can often be obtained from multiple evidence candidates. Moreover, not all such candidates are labeled as positive during <a href=https://en.wikipedia.org/wiki/Annotation>annotation</a>, rendering the training signal weak and noisy. This problem is exacerbated when the questions are unanswerable or when the answers are Boolean, since the model can not rely on lexical overlap to make a connection between the answer and supporting evidence. We develop a new parameterization of set-valued retrieval that handles unanswerable queries, and we show that marginalizing over this set during training allows a model to mitigate false negatives in supporting evidence annotations. We test our method on two multi-document QA datasets, <a href=https://en.wikipedia.org/wiki/IIRC>IIRC</a> and HotpotQA. On IIRC, we show that joint modeling with marginalization improves <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> performance by 5.5 F1 points and achieves a new state-of-the-art performance of 50.5 F1. We also show that retrieval marginalization results in 4.1 QA F1 improvement over a non-marginalized baseline on HotpotQA in the fullwiki setting.</div></div><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.86.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--86 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.86 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939237 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.86/>IIRC : A Dataset of Incomplete Information Reading Comprehension Questions<span class=acl-fixed-case>IIRC</span>: A Dataset of Incomplete Information Reading Comprehension Questions</a></strong><br><a href=/people/j/james-ferguson/>James Ferguson</a>
|
<a href=/people/m/matt-gardner/>Matt Gardner</a>
|
<a href=/people/h/hannaneh-hajishirzi/>Hannaneh Hajishirzi</a>
|
<a href=/people/t/tushar-khot/>Tushar Khot</a>
|
<a href=/people/p/pradeep-dasigi/>Pradeep Dasigi</a><br><a href=/volumes/2020.emnlp-main/ class=text-muted>Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--86><div class="card-body p-3 small">Humans often have to read multiple documents to address their information needs. However, most existing reading comprehension (RC) tasks only focus on questions for which the contexts provide all the information required to answer them, thus not evaluating a <a href=https://en.wikipedia.org/wiki/System>system</a>&#8217;s performance at identifying a potential lack of sufficient information and locating sources for that information. To fill this gap, we present a dataset, IIRC, with more than 13 K questions over paragraphs from <a href=https://en.wikipedia.org/wiki/English_Wikipedia>English Wikipedia</a> that provide only partial information to answer them, with the missing information occurring in one or more linked documents. The questions were written by crowd workers who did not have access to any of the linked documents, leading to questions that have little lexical overlap with the contexts where the answers appear. This process also gave many questions without answers, and those that require discrete reasoning, increasing the difficulty of the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. We follow recent modeling work on various reading comprehension datasets to construct a <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline model</a> for this dataset, finding that <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> achieves 31.1 % <a href=https://en.wikipedia.org/wiki/F-number>F1</a> on this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>, while estimated <a href=https://en.wikipedia.org/wiki/Human_factors_and_ergonomics>human performance</a> is 88.4 %. The dataset, code for the baseline system, and a leaderboard can be found at https://allennlp.org/iirc.</div></div><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1246.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1246 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1246 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N19-1246.Supplementary.pdf data-toggle=tooltip data-placement=top title=Supplementary><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1246" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N19-1246/>DROP : A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs<span class=acl-fixed-case>DROP</span>: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs</a></strong><br><a href=/people/d/dheeru-dua/>Dheeru Dua</a>
|
<a href=/people/y/yizhong-wang/>Yizhong Wang</a>
|
<a href=/people/p/pradeep-dasigi/>Pradeep Dasigi</a>
|
<a href=/people/g/gabriel-stanovsky/>Gabriel Stanovsky</a>
|
<a href=/people/s/sameer-singh/>Sameer Singh</a>
|
<a href=/people/m/matt-gardner/>Matt Gardner</a><br><a href=/volumes/N19-1/ class=text-muted>Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1246><div class="card-body p-3 small">Reading comprehension has recently seen rapid progress, with <a href=https://en.wikipedia.org/wiki/Computer>systems</a> matching humans on the most popular datasets for the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. However, a large body of work has highlighted the brittleness of these <a href=https://en.wikipedia.org/wiki/System>systems</a>, showing that there is much work left to be done. We introduce a new reading comprehension benchmark, DROP, which requires Discrete Reasoning Over the content of Paragraphs. In this crowdsourced, adversarially-created, 55k-question benchmark, a system must resolve references in a question, perhaps to multiple input positions, and perform discrete operations over them (such as addition, counting, or sorting). These operations require a much more comprehensive understanding of the content of paragraphs, as they remove the paraphrase-and-entity-typing shortcuts available in prior datasets. We apply state-of-the-art methods from both the reading comprehension and semantic parsing literatures on this <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> and show that the best <a href=https://en.wikipedia.org/wiki/System>systems</a> only achieve 38.4 % <a href=https://en.wikipedia.org/wiki/F-number>F1</a> on our generalized accuracy metric, while expert human performance is 96 %. We additionally present a new model that combines reading comprehension methods with simple <a href=https://en.wikipedia.org/wiki/Numerical_analysis>numerical reasoning</a> to achieve 51 % <a href=https://en.wikipedia.org/wiki/F-number>F1</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1273.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1273 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1273 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N19-1273.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/361691015 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1273/>Iterative Search for Weakly Supervised Semantic Parsing</a></strong><br><a href=/people/p/pradeep-dasigi/>Pradeep Dasigi</a>
|
<a href=/people/m/matt-gardner/>Matt Gardner</a>
|
<a href=/people/s/shikhar-murty/>Shikhar Murty</a>
|
<a href=/people/l/luke-zettlemoyer/>Luke Zettlemoyer</a>
|
<a href=/people/e/eduard-hovy/>Eduard Hovy</a><br><a href=/volumes/N19-1/ class=text-muted>Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1273><div class="card-body p-3 small">Training semantic parsers from question-answer pairs typically involves searching over an exponentially large space of <a href=https://en.wikipedia.org/wiki/Logical_form>logical forms</a>, and an unguided search can easily be misled by spurious logical forms that coincidentally evaluate to the correct answer. We propose a novel iterative training algorithm that alternates between searching for consistent logical forms and maximizing the marginal likelihood of the retrieved ones. This training scheme lets us iteratively train <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> that provide guidance to subsequent ones to search for logical forms of increasing <a href=https://en.wikipedia.org/wiki/Computational_complexity_theory>complexity</a>, thus dealing with the problem of spuriousness. We evaluate these techniques on two hard datasets : WikiTableQuestions (WTQ) and Cornell Natural Language Visual Reasoning (NLVR), and show that our training algorithm outperforms the previous best systems, on WTQ in a comparable setting, and on NLVR with significantly less supervision.</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-2501.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-2501 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-2501 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-2501" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-2501/>AllenNLP : A Deep Semantic Natural Language Processing Platform<span class=acl-fixed-case>A</span>llen<span class=acl-fixed-case>NLP</span>: A Deep Semantic Natural Language Processing Platform</a></strong><br><a href=/people/m/matt-gardner/>Matt Gardner</a>
|
<a href=/people/j/joel-grus/>Joel Grus</a>
|
<a href=/people/m/mark-neumann/>Mark Neumann</a>
|
<a href=/people/o/oyvind-tafjord/>Oyvind Tafjord</a>
|
<a href=/people/p/pradeep-dasigi/>Pradeep Dasigi</a>
|
<a href=/people/n/nelson-f-liu/>Nelson F. Liu</a>
|
<a href=/people/m/matthew-e-peters/>Matthew Peters</a>
|
<a href=/people/m/michael-schmitz/>Michael Schmitz</a>
|
<a href=/people/l/luke-zettlemoyer/>Luke Zettlemoyer</a><br><a href=/volumes/W18-25/ class=text-muted>Proceedings of Workshop for NLP Open Source Software (NLP-OSS)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-2501><div class="card-body p-3 small">Modern natural language processing (NLP) research requires writing code. Ideally this <a href=https://en.wikipedia.org/wiki/Source_code>code</a> would provide a precise definition of the approach, easy repeatability of results, and a basis for extending the <a href=https://en.wikipedia.org/wiki/Research>research</a>. However, many research codebases bury high-level parameters under implementation details, are challenging to run and debug, and are difficult enough to extend that they are more likely to be rewritten. This paper describes AllenNLP, a library for applying deep learning methods to NLP research that addresses these issues with easy-to-use command-line tools, declarative configuration-driven experiments, and modular NLP abstractions. AllenNLP has already increased the rate of research experimentation and the sharing of NLP components at the Allen Institute for Artificial Intelligence, and we are working to have the same impact across the field.</div></div><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1191.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1191 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1191 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P17-1191" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P17-1191/>Ontology-Aware Token Embeddings for Prepositional Phrase Attachment</a></strong><br><a href=/people/p/pradeep-dasigi/>Pradeep Dasigi</a>
|
<a href=/people/w/waleed-ammar/>Waleed Ammar</a>
|
<a href=/people/c/chris-dyer/>Chris Dyer</a>
|
<a href=/people/e/eduard-hovy/>Eduard Hovy</a><br><a href=/volumes/P17-1/ class=text-muted>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1191><div class="card-body p-3 small">Type-level word embeddings use the same set of parameters to represent all instances of a word regardless of its context, ignoring the inherent lexical ambiguity in language. Instead, we embed <a href=https://en.wikipedia.org/wiki/Semantics>semantic concepts</a> (or synsets) as defined in <a href=https://en.wikipedia.org/wiki/WordNet>WordNet</a> and represent a word token in a particular context by estimating a distribution over relevant semantic concepts. We use the new, context-sensitive embeddings in a <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> for predicting prepositional phrase (PP) attachments and jointly learn the concept embeddings and <a href=https://en.wikipedia.org/wiki/Conceptual_model>model parameters</a>. We show that using context-sensitive embeddings improves the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of the PP attachment model by 5.4 % absolute points, which amounts to a 34.4 % relative reduction in errors.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Pradeep+Dasigi" title="Search for 'Pradeep Dasigi' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/m/matt-gardner/ class=align-middle>Matt Gardner</a>
<span class="badge badge-secondary align-middle ml-2">5</span></li><li class=list-group-item><a href=/people/e/eduard-hovy/ class=align-middle>Eduard Hovy</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/l/luke-zettlemoyer/ class=align-middle>Luke Zettlemoyer</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/j/james-ferguson/ class=align-middle>James Ferguson</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/hannaneh-hajishirzi/ class=align-middle>Hannaneh Hajishirzi</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/t/tushar-khot/ class=align-middle>Tushar Khot</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/w/waleed-ammar/ class=align-middle>Waleed Ammar</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/chris-dyer/ class=align-middle>Chris Dyer</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/ansong-ni/ class=align-middle>Ansong Ni</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/joel-grus/ class=align-middle>Joel Grus</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/mark-neumann/ class=align-middle>Mark Neumann</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/o/oyvind-tafjord/ class=align-middle>Oyvind Tafjord</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/n/nelson-f-liu/ class=align-middle>Nelson F. Liu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/matthew-e-peters/ class=align-middle>Matthew E. Peters</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/michael-schmitz/ class=align-middle>Michael Schmitz</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/dheeru-dua/ class=align-middle>Dheeru Dua</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yizhong-wang/ class=align-middle>Yizhong Wang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/g/gabriel-stanovsky/ class=align-middle>Gabriel Stanovsky</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/sameer-singh/ class=align-middle>Sameer Singh</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/shikhar-murty/ class=align-middle>Shikhar Murty</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/emnlp/ class=align-middle>EMNLP</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/naacl/ class=align-middle>NAACL</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/acl/ class=align-middle>ACL</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/ws/ class=align-middle>WS</a><span class="badge badge-secondary align-middle ml-2">1</span></li></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright Â©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>