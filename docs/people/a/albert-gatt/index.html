<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Albert Gatt - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Albert</span> <span class=font-weight-bold>Gatt</span></h2><hr><div class=row><div class=col-lg-9><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.blackboxnlp-1.15.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--blackboxnlp-1--15 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.blackboxnlp-1.15 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.blackboxnlp-1.15" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.blackboxnlp-1.15/>On the Language-specificity of Multilingual BERT and the Impact of <a href=https://en.wikipedia.org/wiki/Fine-tuning>Fine-tuning</a><span class=acl-fixed-case>BERT</span> and the Impact of Fine-tuning</a></strong><br><a href=/people/m/marc-tanti/>Marc Tanti</a>
|
<a href=/people/l/lonneke-van-der-plas/>Lonneke van der Plas</a>
|
<a href=/people/c/claudia-borg/>Claudia Borg</a>
|
<a href=/people/a/albert-gatt/>Albert Gatt</a><br><a href=/volumes/2021.blackboxnlp-1/ class=text-muted>Proceedings of the Fourth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--blackboxnlp-1--15><div class="card-body p-3 small">Recent work has shown evidence that the <a href=https://en.wikipedia.org/wiki/Knowledge>knowledge</a> acquired by multilingual BERT (mBERT) has two components : a language-specific and a language-neutral one. This paper analyses the relationship between them, in the context of fine-tuning on two tasks POS tagging and natural language inference which require the model to bring to bear different degrees of language-specific knowledge. Visualisations reveal that mBERT loses the ability to cluster representations by language after <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a>, a result that is supported by evidence from <a href=https://en.wikipedia.org/wiki/Language_identification>language identification</a> experiments. However, further experiments on &#8216;unlearning&#8217; language-specific representations using gradient reversal and iterative adversarial learning are shown not to add further improvement to the language-independent component over and above the effect of <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a>. The results presented here suggest that the process of <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a> causes a reorganisation of the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a>&#8217;s limited representational capacity, enhancing language-independent representations at the expense of language-specific ones.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.mmsr-1.4.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--mmsr-1--4 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.mmsr-1.4 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.mmsr-1.4/>Seeing past words : Testing the cross-modal capabilities of pretrained V&L models on counting tasks<span class=acl-fixed-case>V</span>&<span class=acl-fixed-case>L</span> models on counting tasks</a></strong><br><a href=/people/l/letitia-parcalabescu/>Letitia Parcalabescu</a>
|
<a href=/people/a/albert-gatt/>Albert Gatt</a>
|
<a href=/people/a/anette-frank/>Anette Frank</a>
|
<a href=/people/i/iacer-calixto/>Iacer Calixto</a><br><a href=/volumes/2021.mmsr-1/ class=text-muted>Proceedings of the 1st Workshop on Multimodal Semantic Representations (MMSR)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--mmsr-1--4><div class="card-body p-3 small">We investigate the reasoning ability of pretrained vision and language (V&L) models in two tasks that require multimodal integration : (1) discriminating a correct image-sentence pair from an incorrect one, and (2) counting entities in an image. We evaluate three pretrained V&L models on these tasks : ViLBERT, ViLBERT 12-in-1 and LXMERT, in zero-shot and finetuned settings. Our results show that <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> solve task (1) very well, as expected, since all <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> are pretrained on task (1). However, none of the pretrained V&L models is able to adequately solve task (2), our counting probe, and they can not generalise to out-of-distribution quantities. We propose a number of explanations for these findings : LXMERT (and to some extent ViLBERT 12-in-1) show some evidence of catastrophic forgetting on task (1). Concerning our results on the counting probe, we find evidence that all models are impacted by dataset bias, and also fail to individuate entities in the visual input. While a selling point of pretrained V&L models is their ability to solve complex tasks, our findings suggest that understanding their reasoning and grounding capabilities requires more targeted investigations on specific phenomena.</div></div><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.nl4xai-1.5.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--nl4xai-1--5 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.nl4xai-1.5 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.nl4xai-1.5/>The Natural Language Pipeline, Neural Text Generation and Explainability</a></strong><br><a href=/people/j/juliette-faille/>Juliette Faille</a>
|
<a href=/people/a/albert-gatt/>Albert Gatt</a>
|
<a href=/people/c/claire-gardent/>Claire Gardent</a><br><a href=/volumes/2020.nl4xai-1/ class=text-muted>2nd Workshop on Interactive Natural Language Technology for Explainable Artificial Intelligence</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--nl4xai-1--5><div class="card-body p-3 small">End-to-end encoder-decoder approaches to data-to-text generation are often black boxes whose predictions are difficult to explain. Breaking up the end-to-end model into sub-modules is a natural way to address this problem. The traditional pre-neural Natural Language Generation (NLG) pipeline provides a framework for breaking up the end-to-end encoder-decoder. We survey recent papers that integrate traditional NLG submodules in neural approaches and analyse their explainability. Our survey is a first step towards building explainable neural NLG models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.onion-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.onion-1.0/>Proceedings of LREC2020 Workshop "People in language, vision and the mind" (ONION2020)</a></strong><br><a href=/people/p/patrizia-paggio/>Patrizia Paggio</a>
|
<a href=/people/a/albert-gatt/>Albert Gatt</a>
|
<a href=/people/r/roman-klinger/>Roman Klinger</a><br><a href=/volumes/2020.onion-1/ class=text-muted>Proceedings of LREC2020 Workshop "People in language, vision and the mind" (ONION2020)</a></span></p><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1246.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1246 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1246 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-1246/>You Write like You Eat : Stylistic Variation as a Predictor of <a href=https://en.wikipedia.org/wiki/Social_stratification>Social Stratification</a></a></strong><br><a href=/people/a/angelo-basile/>Angelo Basile</a>
|
<a href=/people/a/albert-gatt/>Albert Gatt</a>
|
<a href=/people/m/malvina-nissim/>Malvina Nissim</a><br><a href=/volumes/P19-1/ class=text-muted>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1246><div class="card-body p-3 small">Inspired by Labov&#8217;s seminal work on stylisticvariation as a function of <a href=https://en.wikipedia.org/wiki/Social_stratification>social stratification</a>, we develop and compare neural models thatpredict a person&#8217;s presumed socio-economicstatus, obtained through distant supervision, from their writing style on <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>. Thefocus of our work is on identifying the mostimportant stylistic parameters to predict <a href=https://en.wikipedia.org/wiki/Socioeconomic_status>socio-economic group</a>. In particular, we show theeffectiveness of morpho-syntactic features aspredictors of style, in contrast to lexical fea-tures, which are good predictors of topic</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1199.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1199 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1199 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=C18-1199" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/C18-1199/>Grounded Textual Entailment</a></strong><br><a href=/people/h/hoa-trong-vu/>Hoa Trong Vu</a>
|
<a href=/people/c/claudio-greco/>Claudio Greco</a>
|
<a href=/people/a/aliia-erofeeva/>Aliia Erofeeva</a>
|
<a href=/people/s/somayeh-jafaritazehjani/>Somayeh Jafaritazehjan</a>
|
<a href=/people/g/guido-linders/>Guido Linders</a>
|
<a href=/people/m/marc-tanti/>Marc Tanti</a>
|
<a href=/people/a/alberto-testoni/>Alberto Testoni</a>
|
<a href=/people/r/raffaella-bernardi/>Raffaella Bernardi</a>
|
<a href=/people/a/albert-gatt/>Albert Gatt</a><br><a href=/volumes/C18-1/ class=text-muted>Proceedings of the 27th International Conference on Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1199><div class="card-body p-3 small">Capturing semantic relations between sentences, such as <a href=https://en.wikipedia.org/wiki/Logical_consequence>entailment</a>, is a long-standing challenge for <a href=https://en.wikipedia.org/wiki/Computational_semantics>computational semantics</a>. Logic-based models analyse <a href=https://en.wikipedia.org/wiki/Logical_consequence>entailment</a> in terms of possible worlds (interpretations, or situations) where a premise P entails a hypothesis H iff in all worlds where P is true, H is also true. Statistical models view this relationship probabilistically, addressing it in terms of whether a human would likely infer H from P. In this paper, we wish to bridge these two perspectives, by arguing for a visually-grounded version of the Textual Entailment task. Specifically, we ask whether <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> can perform better if, in addition to <a href=https://en.wikipedia.org/wiki/P_(complexity)>P</a> and H, there is also an <a href=https://en.wikipedia.org/wiki/Image>image</a> (corresponding to the relevant world or situation). We use a multimodal version of the SNLI dataset (Bowman et al., 2015) and we compare blind and visually-augmented models of textual entailment. We show that visual information is beneficial, but we also conduct an in-depth error analysis that reveals that current multimodal models are not performing grounding in an optimal fashion.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6500.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6500/>Proceedings of the 11th International Conference on Natural Language Generation</a></strong><br><a href=/people/e/emiel-krahmer/>Emiel Krahmer</a>
|
<a href=/people/a/albert-gatt/>Albert Gatt</a>
|
<a href=/people/m/martijn-goudbeek/>Martijn Goudbeek</a><br><a href=/volumes/W18-65/ class=text-muted>Proceedings of the 11th International Conference on Natural Language Generation</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6551.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6551 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6551 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6551/>Meteorologists and Students : A resource for language grounding of geographical descriptors</a></strong><br><a href=/people/a/alejandro-ramos-soto/>Alejandro Ramos-Soto</a>
|
<a href=/people/e/ehud-reiter/>Ehud Reiter</a>
|
<a href=/people/k/kees-van-deemter/>Kees van Deemter</a>
|
<a href=/people/j/jose-m-alonso/>Jose Alonso</a>
|
<a href=/people/a/albert-gatt/>Albert Gatt</a><br><a href=/volumes/W18-65/ class=text-muted>Proceedings of the 11th International Conference on Natural Language Generation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6551><div class="card-body p-3 small">We present a <a href=https://en.wikipedia.org/wiki/Data_(computing)>data resource</a> which can be useful for research purposes on language grounding tasks in the context of geographical referring expression generation. The resource is composed of two <a href=https://en.wikipedia.org/wiki/Data_set>data sets</a> that encompass 25 different geographical descriptors and a set of associated graphical representations, drawn as <a href=https://en.wikipedia.org/wiki/Polygon_(computer_graphics)>polygons</a> on a map by two groups of human subjects : teenage students and expert meteorologists.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6562.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6562 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6562 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6562/>Specificity measures and reference</a></strong><br><a href=/people/a/albert-gatt/>Albert Gatt</a>
|
<a href=/people/n/nicolas-marin/>Nicolás Marín</a>
|
<a href=/people/g/gustavo-rivas-gervilla/>Gustavo Rivas-Gervilla</a>
|
<a href=/people/d/daniel-sanchez-cisneros/>Daniel Sánchez</a><br><a href=/volumes/W18-65/ class=text-muted>Proceedings of the 11th International Conference on Natural Language Generation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6562><div class="card-body p-3 small">In this paper we study empirically the validity of measures of referential success for referring expressions involving <a href=https://en.wikipedia.org/wiki/Gradualism>gradual properties</a>. More specifically, we study the ability of several measures of referential success to predict the success of a user in choosing the right object, given a referring expression. Experimental results indicate that certain fuzzy measures of success are able to predict <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>human accuracy</a> in reference resolution. Such measures are therefore suitable for the estimation of the success or otherwise of a <a href=https://en.wikipedia.org/wiki/Referring_expression>referring expression</a> produced by a generation algorithm, especially in case the properties in a domain can not be assumed to have crisp denotations.</div></div><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1304.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1304 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1304 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1304/>Morphological Analysis for the <a href=https://en.wikipedia.org/wiki/Maltese_language>Maltese Language</a> : The challenges of a <a href=https://en.wikipedia.org/wiki/Hybrid_system>hybrid system</a><span class=acl-fixed-case>M</span>altese Language: The challenges of a hybrid system</a></strong><br><a href=/people/c/claudia-borg/>Claudia Borg</a>
|
<a href=/people/a/albert-gatt/>Albert Gatt</a><br><a href=/volumes/W17-13/ class=text-muted>Proceedings of the Third Arabic Natural Language Processing Workshop</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1304><div class="card-body p-3 small">Maltese is a morphologically rich language with a hybrid morphological system which features both concatenative and non-concatenative processes. This paper analyses the impact of this <a href=https://en.wikipedia.org/wiki/Hybridity>hybridity</a> on the performance of machine learning techniques for <a href=https://en.wikipedia.org/wiki/Morphology_(biology)>morphological labelling</a> and <a href=https://en.wikipedia.org/wiki/Cluster_analysis>clustering</a>. In particular, we analyse a dataset of morphologically related word clusters to evaluate the difference in results for concatenative and non-concatenative clusters. We also describe research carried out in <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphological labelling</a>, with a particular focus on the verb category. Two evaluations were carried out, one using an unseen dataset, and another one using a gold standard dataset which was manually labelled. The <a href=https://en.wikipedia.org/wiki/Gold_standard_(test)>gold standard dataset</a> was split into concatenative and non-concatenative to analyse the difference in results between the two <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphological systems</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-3506.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-3506 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-3506 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W17-3506" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W17-3506/>What is the Role of Recurrent Neural Networks (RNNs) in an Image Caption Generator?<span class=acl-fixed-case>RNN</span>s) in an Image Caption Generator?</a></strong><br><a href=/people/m/marc-tanti/>Marc Tanti</a>
|
<a href=/people/a/albert-gatt/>Albert Gatt</a>
|
<a href=/people/k/kenneth-camilleri/>Kenneth Camilleri</a><br><a href=/volumes/W17-35/ class=text-muted>Proceedings of the 10th International Conference on Natural Language Generation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-3506><div class="card-body p-3 small">Image captioning has evolved into a core task for Natural Language Generation and has also proved to be an important testbed for deep learning approaches to handling multimodal representations. Most contemporary approaches rely on a combination of a convolutional network to handle image features, and a <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent network</a> to encode linguistic information. The latter is typically viewed as the primary generation component. Beyond this high-level characterisation, a CNN+RNN model supports a variety of architectural designs. The dominant model in the literature is one in which visual features encoded by a CNN are injected as part of the linguistic encoding process, driving the RNN&#8217;s linguistic choices. By contrast, it is possible to envisage an <a href=https://en.wikipedia.org/wiki/Architecture>architecture</a> in which visual and linguistic features are encoded separately, and merged at a subsequent stage. In this paper, we address two related questions : (1) Is direct injection the best way of combining multimodal information, or is a late merging alternative better for the image captioning task? (2) To what extent should a <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent network</a> be viewed as actually generating, rather than simply encoding, linguistic information?</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Albert+Gatt" title="Search for 'Albert Gatt' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/m/marc-tanti/ class=align-middle>Marc Tanti</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/c/claudia-borg/ class=align-middle>Claudia Borg</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/h/hoa-trong-vu/ class=align-middle>Hoa Trong Vu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/claudio-greco/ class=align-middle>Claudio Greco</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/aliia-erofeeva/ class=align-middle>Aliia Erofeeva</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/s/somayeh-jafaritazehjani/ class=align-middle>Somayeh Jafaritazehjani</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/g/guido-linders/ class=align-middle>Guido Linders</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/alberto-testoni/ class=align-middle>Alberto Testoni</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/raffaella-bernardi/ class=align-middle>Raffaella Bernardi</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/juliette-faille/ class=align-middle>Juliette Faille</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/claire-gardent/ class=align-middle>Claire Gardent</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/kenneth-camilleri/ class=align-middle>Kenneth Camilleri</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/p/patrizia-paggio/ class=align-middle>Patrizia Paggio</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/roman-klinger/ class=align-middle>Roman Klinger</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/lonneke-van-der-plas/ class=align-middle>Lonneke van der Plas</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/e/emiel-krahmer/ class=align-middle>Emiel Krahmer</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/martijn-goudbeek/ class=align-middle>Martijn Goudbeek</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/alejandro-ramos-soto/ class=align-middle>Alejandro Ramos-Soto</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/e/ehud-reiter/ class=align-middle>Ehud Reiter</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/kees-van-deemter/ class=align-middle>Kees van Deemter</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jose-m-alonso/ class=align-middle>Jose M. Alonso</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/n/nicolas-marin/ class=align-middle>Nicolas Marin</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/g/gustavo-rivas-gervilla/ class=align-middle>Gustavo Rivas-Gervilla</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/daniel-sanchez-cisneros/ class=align-middle>Daniel Sanchez-Cisneros</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/letitia-parcalabescu/ class=align-middle>Letitia Parcalabescu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/anette-frank/ class=align-middle>Anette Frank</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/i/iacer-calixto/ class=align-middle>Iacer Calixto</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/angelo-basile/ class=align-middle>Angelo Basile</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/malvina-nissim/ class=align-middle>Malvina Nissim</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/ws/ class=align-middle>WS</a><span class="badge badge-secondary align-middle ml-2">5</span></li><li class=list-group-item><a href=/venues/coling/ class=align-middle>COLING</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/nl4xai/ class=align-middle>NL4XAI</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/onion/ class=align-middle>ONION</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/blackboxnlp/ class=align-middle>BlackboxNLP</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-venues aria-expanded=false aria-controls=more-venues>show all...</li><div class="collapse border-top" id=more-venues><li class=list-group-item><a href=/venues/mmsr/ class=align-middle>MMSR</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/acl/ class=align-middle>ACL</a><span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>