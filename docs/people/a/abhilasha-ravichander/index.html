<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Abhilasha Ravichander - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Abhilasha</span> <span class=font-weight-bold>Ravichander</span></h2><hr><div class=row><div class=col-lg-9><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.295.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--295 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.295 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.295/>Probing the Probing Paradigm : Does Probing Accuracy Entail Task Relevance?</a></strong><br><a href=/people/a/abhilasha-ravichander/>Abhilasha Ravichander</a>
|
<a href=/people/y/yonatan-belinkov/>Yonatan Belinkov</a>
|
<a href=/people/e/eduard-hovy/>Eduard Hovy</a><br><a href=/volumes/2021.eacl-main/ class=text-muted>Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--295><div class="card-body p-3 small">Although neural models have achieved impressive results on several NLP benchmarks, little is understood about the mechanisms they use to perform language tasks. Thus, much recent attention has been devoted to analyzing the <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>sentence representations</a> learned by <a href=https://en.wikipedia.org/wiki/Neural_coding>neural encoders</a>, through the lens of &#8216;probing&#8217; tasks. However, to what extent was the information encoded in sentence representations, as discovered through a probe, actually used by the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> to perform its task? In this work, we examine this probing paradigm through a case study in Natural Language Inference, showing that models can learn to encode linguistic properties even if they are not needed for the task on which the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> was trained. We further identify that pretrained word embeddings play a considerable role in encoding these properties rather than the training task itself, highlighting the importance of careful controls when designing probing experiments. Finally, through a set of controlled synthetic tasks, we demonstrate models can encode these properties considerably above chance-level, even when distributed in the data as random noise, calling into question the interpretation of absolute claims on probing tasks.</div></div><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.sdp-1.24.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--sdp-1--24 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.sdp-1.24 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.sdp-1.24/>Overview and Insights from the Shared Tasks at Scholarly Document Processing 2020 : CL-SciSumm, LaySumm and LongSumm<span class=acl-fixed-case>CL</span>-<span class=acl-fixed-case>S</span>ci<span class=acl-fixed-case>S</span>umm, <span class=acl-fixed-case>L</span>ay<span class=acl-fixed-case>S</span>umm and <span class=acl-fixed-case>L</span>ong<span class=acl-fixed-case>S</span>umm</a></strong><br><a href=/people/m/muthu-kumar-chandrasekaran/>Muthu Kumar Chandrasekaran</a>
|
<a href=/people/g/guy-feigenblat/>Guy Feigenblat</a>
|
<a href=/people/e/eduard-hovy/>Eduard Hovy</a>
|
<a href=/people/a/abhilasha-ravichander/>Abhilasha Ravichander</a>
|
<a href=/people/m/michal-shmueli-scheuer/>Michal Shmueli-Scheuer</a>
|
<a href=/people/a/anita-de-waard/>Anita de Waard</a><br><a href=/volumes/2020.sdp-1/ class=text-muted>Proceedings of the First Workshop on Scholarly Document Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--sdp-1--24><div class="card-body p-3 small">We present the results of three Shared Tasks held at the Scholarly Document Processing Workshop at EMNLP2020 : CL-SciSumm, LaySumm and LongSumm. We report on each of the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a>, which received 18 submissions in total, with some submissions addressing two or three of the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a>. In summary, the quality and quantity of the submissions show that there is ample interest in scholarly document summarization, and the state of the art in this domain is at a midway point between being an impossible task and one that is fully resolved.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.starsem-1.10.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--starsem-1--10 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.starsem-1.10 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.starsem-1.10" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.starsem-1.10/>On the Systematicity of Probing Contextualized Word Representations : The Case of <a href=https://en.wikipedia.org/wiki/Hypernymy>Hypernymy</a> in BERT<span class=acl-fixed-case>BERT</span></a></strong><br><a href=/people/a/abhilasha-ravichander/>Abhilasha Ravichander</a>
|
<a href=/people/e/eduard-hovy/>Eduard Hovy</a>
|
<a href=/people/k/kaheer-suleman/>Kaheer Suleman</a>
|
<a href=/people/a/adam-trischler/>Adam Trischler</a>
|
<a href=/people/j/jackie-chi-kit-cheung/>Jackie Chi Kit Cheung</a><br><a href=/volumes/2020.starsem-1/ class=text-muted>Proceedings of the Ninth Joint Conference on Lexical and Computational Semantics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--starsem-1--10><div class="card-body p-3 small">Contextualized word representations have become a driving force in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a>, motivating widespread interest in understanding their capabilities and the mechanisms by which they operate. Particularly intriguing is their ability to identify and encode conceptual abstractions. Past work has probed BERT representations for this competence, finding that BERT can correctly retrieve noun hypernyms in cloze tasks. In this work, we ask the question : do probing studies shed light on systematic knowledge in BERT representations? As a case study, we examine <a href=https://en.wikipedia.org/wiki/Hypernymy>hypernymy knowledge</a> encoded in BERT representations. In particular, we demonstrate through a simple consistency probe that the ability to correctly retrieve hypernyms in cloze tasks, as used in prior work, does not correspond to systematic knowledge in BERT. Our main conclusion is cautionary : even if BERT demonstrates high probing accuracy for a particular competence, it does not necessarily follow that BERT &#8216;understands&#8217; a concept, and it can not be expected to systematically generalize across applicable contexts.<i>do probing studies shed light on systematic knowledge in BERT representations?</i> As a case study, we examine hypernymy knowledge encoded in BERT representations. In particular, we demonstrate through a simple consistency probe that the ability to correctly retrieve hypernyms in cloze tasks, as used in prior work, does not correspond to systematic knowledge in BERT. Our main conclusion is cautionary: even if BERT demonstrates high probing accuracy for a particular competence, it does not necessarily follow that BERT &#8216;understands&#8217; a concept, and it cannot be expected to systematically generalize across applicable contexts.</div></div><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1500.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1500 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1500 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1500" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1500/>Question Answering for Privacy Policies : Combining Computational and Legal Perspectives</a></strong><br><a href=/people/a/abhilasha-ravichander/>Abhilasha Ravichander</a>
|
<a href=/people/a/alan-w-black/>Alan W Black</a>
|
<a href=/people/s/shomir-wilson/>Shomir Wilson</a>
|
<a href=/people/t/thomas-norton/>Thomas Norton</a>
|
<a href=/people/n/norman-sadeh/>Norman Sadeh</a><br><a href=/volumes/D19-1/ class=text-muted>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1500><div class="card-body p-3 small">Privacy policies are long and complex documents that are difficult for users to read and understand. Yet, they have legal effects on how <a href=https://en.wikipedia.org/wiki/User_data>user data</a> can be collected, managed and used. Ideally, we would like to empower users to inform themselves about the issues that matter to them, and enable them to selectively explore these issues. We present PrivacyQA, a <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> consisting of 1750 questions about the <a href=https://en.wikipedia.org/wiki/Privacy_policy>privacy policies</a> of <a href=https://en.wikipedia.org/wiki/Mobile_app>mobile applications</a>, and over 3500 expert annotations of relevant answers. We observe that a strong neural baseline underperforms <a href=https://en.wikipedia.org/wiki/Human_performance>human performance</a> by almost 0.3 <a href=https://en.wikipedia.org/wiki/F-number>F1</a> on PrivacyQA, suggesting considerable room for improvement for future systems. Further, we use this <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> to categorically identify challenges to <a href=https://en.wikipedia.org/wiki/Question_answering>question answerability</a>, with domain-general implications for any <a href=https://en.wikipedia.org/wiki/Question_answering>question answering system</a>. The PrivacyQA corpus offers a challenging corpus for <a href=https://en.wikipedia.org/wiki/Question_answering>question answering</a>, with genuine real world utility.</div></div><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5545.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5545 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5545 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5545/>How Would You Say It? Eliciting Lexically Diverse Dialogue for Supervised Semantic Parsing</a></strong><br><a href=/people/a/abhilasha-ravichander/>Abhilasha Ravichander</a>
|
<a href=/people/t/thomas-manzini/>Thomas Manzini</a>
|
<a href=/people/m/matthias-grabmair/>Matthias Grabmair</a>
|
<a href=/people/g/graham-neubig/>Graham Neubig</a>
|
<a href=/people/j/jonathan-francis/>Jonathan Francis</a>
|
<a href=/people/e/eric-nyberg/>Eric Nyberg</a><br><a href=/volumes/W17-55/ class=text-muted>Proceedings of the 18th Annual SIGdial Meeting on Discourse and Dialogue</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5545><div class="card-body p-3 small">Building dialogue interfaces for real-world scenarios often entails training semantic parsers starting from zero examples. How can we build <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> that better capture the variety of ways users might phrase their queries, and what queries are actually realistic? Wang et al. (2015) proposed a method to build semantic parsing datasets by generating canonical utterances using a <a href=https://en.wikipedia.org/wiki/Grammar>grammar</a> and having crowdworkers paraphrase them into natural wording. A limitation of this approach is that it induces bias towards using similar language as the canonical utterances. In this work, we present a <a href=https://en.wikipedia.org/wiki/Methodology>methodology</a> that elicits meaningful and lexically diverse queries from users for semantic parsing tasks. Starting from a seed lexicon and a <a href=https://en.wikipedia.org/wiki/Generative_grammar>generative grammar</a>, we pair logical forms with mixed text-image representations and ask crowdworkers to paraphrase and confirm the plausibility of the queries that they generated. We use this method to build a semantic parsing dataset from scratch for a dialog agent in a smart-home simulation. We find evidence that this dataset, which we have named SmartHome, is demonstrably more lexically diverse and difficult to parse than existing domain-specific semantic parsing datasets.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Abhilasha+Ravichander" title="Search for 'Abhilasha Ravichander' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/e/eduard-hovy/ class=align-middle>Eduard Hovy</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/m/muthu-kumar-chandrasekaran/ class=align-middle>Muthu Kumar Chandrasekaran</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/g/guy-feigenblat/ class=align-middle>Guy Feigenblat</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/michal-shmueli-scheuer/ class=align-middle>Michal Shmueli-Scheuer</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/anita-de-waard/ class=align-middle>Anita de Waard</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/k/kaheer-suleman/ class=align-middle>Kaheer Suleman</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/adam-trischler/ class=align-middle>Adam Trischler</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jackie-chi-kit-cheung/ class=align-middle>Jackie Chi Kit Cheung</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yonatan-belinkov/ class=align-middle>Yonatan Belinkov</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/thomas-manzini/ class=align-middle>Thomas Manzini</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/matthias-grabmair/ class=align-middle>Matthias Grabmair</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/g/graham-neubig/ class=align-middle>Graham Neubig</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jonathan-francis/ class=align-middle>Jonathan Francis</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/e/eric-nyberg/ class=align-middle>Eric Nyberg</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/alan-w-black/ class=align-middle>Alan W. Black</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/shomir-wilson/ class=align-middle>Shomir Wilson</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/thomas-norton/ class=align-middle>Thomas Norton</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/n/norman-sadeh/ class=align-middle>Norman Sadeh</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/sdp/ class=align-middle>sdp</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/starsem/ class=align-middle>*SEM</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/eacl/ class=align-middle>EACL</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/ws/ class=align-middle>WS</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/emnlp/ class=align-middle>EMNLP</a><span class="badge badge-secondary align-middle ml-2">1</span></li></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright Â©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>