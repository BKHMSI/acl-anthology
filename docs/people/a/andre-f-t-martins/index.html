<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>André F. T. Martins - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>André F. T.</span> <span class=font-weight-bold>Martins</span></h2><p class="font-weight-light text-muted"><span class=font-italic>Also published as:</span>
Andre <span class=font-weight-normal>Martins</span>,
André <span class=font-weight-normal>Martins</span></p><hr><div class=row><div class=col-lg-9><h4>2022</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.acl-long.375.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--acl-long--375 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.acl-long.375 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.acl-long.375/><span class=tex-math>∞</span>-former: Infinite Memory Transformer</a></strong><br><a href=/people/p/pedro-henrique-martins/>Pedro Henrique Martins</a>
|
<a href=/people/z/zita-marinho/>Zita Marinho</a>
|
<a href=/people/a/andre-f-t-martins/>Andre Martins</a><br><a href=/volumes/2022.acl-long/ class=text-muted>Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--acl-long--375><div class="card-body p-3 small">Transformers are unable to model long-term memories effectively, since the amount of computation they need to perform grows with the context length. While variations of efficient transformers have been proposed, they all have a finite memory capacity and are forced to drop old information. In this paper, we propose the <tex-math>\\infty</tex-math>-former, which extends the vanilla transformer with an unbounded long-term memory. By making use of a continuous-space attention mechanism to attend over the long-term memory, the <tex-math>\\infty</tex-math>-former&#8217;s attention complexity becomes independent of the context length, trading off memory length with precision.In order to control where precision is more important, <tex-math>\\infty</tex-math>-former maintains &#8220;sticky memories,&#8221; being able to model arbitrarily long contexts while keeping the computation budget fixed.Experiments on a synthetic sorting task, language modeling, and document grounded dialogue generation demonstrate the <tex-math>\\infty</tex-math>-former&#8217;s ability to retain information from long sequences.</div></div><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eval4nlp-1.14.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eval4nlp-1--14 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eval4nlp-1.14 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.eval4nlp-1.14" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.eval4nlp-1.14/>IST-Unbabel 2021 Submission for the Explainable Quality Estimation Shared Task<span class=acl-fixed-case>IST</span>-Unbabel 2021 Submission for the Explainable Quality Estimation Shared Task</a></strong><br><a href=/people/m/marcos-treviso/>Marcos Treviso</a>
|
<a href=/people/n/nuno-m-guerreiro/>Nuno M. Guerreiro</a>
|
<a href=/people/r/ricardo-rei/>Ricardo Rei</a>
|
<a href=/people/a/andre-f-t-martins/>André F. T. Martins</a><br><a href=/volumes/2021.eval4nlp-1/ class=text-muted>Proceedings of the 2nd Workshop on Evaluation and Comparison of NLP Systems</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eval4nlp-1--14><div class="card-body p-3 small">We present the joint contribution of Instituto Superior Tcnico (IST) and Unbabel to the Explainable Quality Estimation (QE) shared task, where systems were submitted to two tracks : constrained (without word-level supervision) and unconstrained (with word-level supervision). For the constrained track, we experimented with several explainability methods to extract the relevance of input tokens from sentence-level QE models built on top of multilingual pre-trained transformers. Among the different tested methods, composing explanations in the form of attention weights scaled by the <a href=https://en.wikipedia.org/wiki/Norm_(mathematics)>norm of value vectors</a> yielded the best results. When word-level labels are used during training, our best results were obtained by using word-level predicted probabilities. We further improve the performance of our methods on the two tracks by ensembling explanation scores extracted from models trained with different pre-trained transformers, achieving strong results for in-domain and zero-shot language pairs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.65.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--65 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.65 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.65" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.65/>Do Context-Aware Translation Models Pay the Right Attention?</a></strong><br><a href=/people/k/kayo-yin/>Kayo Yin</a>
|
<a href=/people/p/patrick-fernandes/>Patrick Fernandes</a>
|
<a href=/people/d/danish-pruthi/>Danish Pruthi</a>
|
<a href=/people/a/aditi-chaudhary/>Aditi Chaudhary</a>
|
<a href=/people/a/andre-f-t-martins/>André F. T. Martins</a>
|
<a href=/people/g/graham-neubig/>Graham Neubig</a><br><a href=/volumes/2021.acl-long/ class=text-muted>Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--65><div class="card-body p-3 small">Context-aware machine translation models are designed to leverage <a href=https://en.wikipedia.org/wiki/Context_(language_use)>contextual information</a>, but often fail to do so. As a result, they inaccurately disambiguate <a href=https://en.wikipedia.org/wiki/Pronoun>pronouns</a> and <a href=https://en.wikipedia.org/wiki/Polysemy>polysemous words</a> that require <a href=https://en.wikipedia.org/wiki/Context_(language_use)>context</a> for resolution. In this paper, we ask several questions : What contexts do human translators use to resolve ambiguous words? Are <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> paying large amounts of attention to the same context? What if we explicitly train them to do so? To answer these questions, we introduce SCAT (Supporting Context for Ambiguous Translations), a new English-French dataset comprising supporting context words for 14 K translations that professional translators found useful for pronoun disambiguation. Using SCAT, we perform an in-depth analysis of the context used to disambiguate, examining positional and lexical characteristics of the supporting words. Furthermore, we measure the degree of alignment between the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model&#8217;s attention scores</a> and the supporting context from SCAT, and apply a guided attention strategy to encourage agreement between the two.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.505.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--505 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.505 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.505" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.505/>Measuring and Increasing Context Usage in Context-Aware Machine Translation</a></strong><br><a href=/people/p/patrick-fernandes/>Patrick Fernandes</a>
|
<a href=/people/k/kayo-yin/>Kayo Yin</a>
|
<a href=/people/g/graham-neubig/>Graham Neubig</a>
|
<a href=/people/a/andre-f-t-martins/>André F. T. Martins</a><br><a href=/volumes/2021.acl-long/ class=text-muted>Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--505><div class="card-body p-3 small">Recent work in <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a> has demonstrated both the necessity and feasibility of using inter-sentential context, context from sentences other than those currently being translated. However, while many current methods present model architectures that theoretically can use this extra context, it is often not clear how much they do actually utilize <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> at translation time. In this paper, we introduce a new <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metric</a>, conditional cross-mutual information, to quantify usage of context by these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>. Using this <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metric</a>, we measure how much document-level machine translation systems use particular varieties of context. We find that target context is referenced more than source context, and that including more context has a diminishing affect on results. We then introduce a new, simple training method, context-aware word dropout, to increase the usage of context by context-aware models. Experiments show that our method not only increases context usage, but also improves the translation quality according to metrics such as <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a> and COMET, as well as performance on anaphoric pronoun resolution and lexical cohesion contrastive datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wmt-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wmt-1.0/>Proceedings of the Sixth Conference on Machine Translation</a></strong><br><a href=/people/l/loic-barrault/>Loic Barrault</a>
|
<a href=/people/o/ondrej-bojar/>Ondrej Bojar</a>
|
<a href=/people/f/fethi-bougares/>Fethi Bougares</a>
|
<a href=/people/r/rajen-chatterjee/>Rajen Chatterjee</a>
|
<a href=/people/m/marta-r-costa-jussa/>Marta R. Costa-jussa</a>
|
<a href=/people/c/christian-federmann/>Christian Federmann</a>
|
<a href=/people/m/mark-fishel/>Mark Fishel</a>
|
<a href=/people/a/alexander-fraser/>Alexander Fraser</a>
|
<a href=/people/m/markus-freitag/>Markus Freitag</a>
|
<a href=/people/y/yvette-graham/>Yvette Graham</a>
|
<a href=/people/r/roman-grundkiewicz/>Roman Grundkiewicz</a>
|
<a href=/people/p/paco-guzman/>Paco Guzman</a>
|
<a href=/people/b/barry-haddow/>Barry Haddow</a>
|
<a href=/people/m/matthias-huck/>Matthias Huck</a>
|
<a href=/people/a/antonio-jimeno-yepes/>Antonio Jimeno Yepes</a>
|
<a href=/people/p/philipp-koehn/>Philipp Koehn</a>
|
<a href=/people/t/tom-kocmi/>Tom Kocmi</a>
|
<a href=/people/a/andre-f-t-martins/>Andre Martins</a>
|
<a href=/people/m/makoto-morishita/>Makoto Morishita</a>
|
<a href=/people/c/christof-monz/>Christof Monz</a><br><a href=/volumes/2021.wmt-1/ class=text-muted>Proceedings of the Sixth Conference on Machine Translation</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wmt-1.71.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wmt-1--71 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wmt-1.71 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wmt-1.71/>Findings of the WMT 2021 Shared Task on Quality Estimation<span class=acl-fixed-case>WMT</span> 2021 Shared Task on Quality Estimation</a></strong><br><a href=/people/l/lucia-specia/>Lucia Specia</a>
|
<a href=/people/f/frederic-blain/>Frédéric Blain</a>
|
<a href=/people/m/marina-fomicheva/>Marina Fomicheva</a>
|
<a href=/people/c/chrysoula-zerva/>Chrysoula Zerva</a>
|
<a href=/people/z/zhenhao-li/>Zhenhao Li</a>
|
<a href=/people/v/vishrav-chaudhary/>Vishrav Chaudhary</a>
|
<a href=/people/a/andre-f-t-martins/>André F. T. Martins</a><br><a href=/volumes/2021.wmt-1/ class=text-muted>Proceedings of the Sixth Conference on Machine Translation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wmt-1--71><div class="card-body p-3 small">We report the results of the WMT 2021 shared task on Quality Estimation, where the challenge is to predict the quality of the output of neural machine translation systems at the word and sentence levels. This edition focused on two main novel additions : (i) <a href=https://en.wikipedia.org/wiki/Prediction>prediction</a> for unseen languages, i.e. zero-shot settings, and (ii) prediction of sentences with catastrophic errors. In addition, new <a href=https://en.wikipedia.org/wiki/Data_(computing)>data</a> was released for a number of languages, especially post-edited data. Participating teams from 19 institutions submitted altogether 1263 <a href=https://en.wikipedia.org/wiki/System>systems</a> to different task variants and language pairs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wmt-1.102.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wmt-1--102 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wmt-1.102 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wmt-1.102/>IST-Unbabel 2021 Submission for the Quality Estimation Shared Task<span class=acl-fixed-case>IST</span>-Unbabel 2021 Submission for the Quality Estimation Shared Task</a></strong><br><a href=/people/c/chrysoula-zerva/>Chrysoula Zerva</a>
|
<a href=/people/d/daan-van-stigt/>Daan van Stigt</a>
|
<a href=/people/r/ricardo-rei/>Ricardo Rei</a>
|
<a href=/people/a/ana-c-farinha/>Ana C Farinha</a>
|
<a href=/people/p/pedro-ramos/>Pedro Ramos</a>
|
<a href=/people/j/jose-g-c-de-souza/>José G. C. de Souza</a>
|
<a href=/people/t/taisiya-glushkova/>Taisiya Glushkova</a>
|
<a href=/people/m/miguel-vera/>Miguel Vera</a>
|
<a href=/people/f/fabio-kepler/>Fabio Kepler</a>
|
<a href=/people/a/andre-f-t-martins/>André F. T. Martins</a><br><a href=/volumes/2021.wmt-1/ class=text-muted>Proceedings of the Sixth Conference on Machine Translation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wmt-1--102><div class="card-body p-3 small">We present the joint contribution of IST and Unbabel to the WMT 2021 Shared Task on Quality Estimation. Our team participated on two tasks : Direct Assessment and Post-Editing Effort, encompassing a total of 35 submissions. For all submissions, our efforts focused on training multilingual models on top of OpenKiwi predictor-estimator architecture, using pre-trained multilingual encoders combined with adapters. We further experiment with and uncertainty-related objectives and features as well as training on out-of-domain direct assessment data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wmt-1.111.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wmt-1--111 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wmt-1.111 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.wmt-1.111" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.wmt-1.111/>Are References Really Needed? Unbabel-IST 2021 Submission for the Metrics Shared Task<span class=acl-fixed-case>IST</span> 2021 Submission for the Metrics Shared Task</a></strong><br><a href=/people/r/ricardo-rei/>Ricardo Rei</a>
|
<a href=/people/a/ana-c-farinha/>Ana C Farinha</a>
|
<a href=/people/c/chrysoula-zerva/>Chrysoula Zerva</a>
|
<a href=/people/d/daan-van-stigt/>Daan van Stigt</a>
|
<a href=/people/c/craig-stewart/>Craig Stewart</a>
|
<a href=/people/p/pedro-ramos/>Pedro Ramos</a>
|
<a href=/people/t/taisiya-glushkova/>Taisiya Glushkova</a>
|
<a href=/people/a/andre-f-t-martins/>André F. T. Martins</a>
|
<a href=/people/a/alon-lavie/>Alon Lavie</a><br><a href=/volumes/2021.wmt-1/ class=text-muted>Proceedings of the Sixth Conference on Machine Translation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wmt-1--111><div class="card-body p-3 small">In this paper, we present the joint contribution of Unbabel and IST to the WMT 2021 Metrics Shared Task. With this year&#8217;s focus on Multidimensional Quality Metric (MQM) as the ground-truth human assessment, our aim was to steer COMET towards higher correlations with MQM. We do so by first pre-training on Direct Assessments and then fine-tuning on z-normalized MQM scores. In our experiments we also show that reference-free COMET models are becoming competitive with reference-based models, even outperforming the best COMET model from 2020 on this year&#8217;s development data. Additionally, we present COMETinho, a lightweight COMET model that is 19x faster on <a href=https://en.wikipedia.org/wiki/Central_processing_unit>CPU</a> than the original model, while also achieving state-of-the-art correlations with <a href=https://en.wikipedia.org/wiki/MQM>MQM</a>. Finally, in the <a href=https://en.wikipedia.org/wiki/Quantum_electrodynamics>QE</a> as a metric track, we also participated with a <a href=https://en.wikipedia.org/wiki/Quantum_electrodynamics>QE model</a> trained using the OpenKiwi framework leveraging MQM scores and word-level annotations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.210.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--210 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.210 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.naacl-main.210" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.210/>Smoothing and Shrinking the Sparse Seq2Seq Search Space<span class=acl-fixed-case>S</span>eq2<span class=acl-fixed-case>S</span>eq Search Space</a></strong><br><a href=/people/b/ben-peters/>Ben Peters</a>
|
<a href=/people/a/andre-f-t-martins/>André F. T. Martins</a><br><a href=/volumes/2021.naacl-main/ class=text-muted>Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--210><div class="card-body p-3 small">Current sequence-to-sequence models are trained to minimize <a href=https://en.wikipedia.org/wiki/Cross-entropy>cross-entropy</a> and use <a href=https://en.wikipedia.org/wiki/Softmax>softmax</a> to compute the locally normalized probabilities over target sequences. While this setup has led to strong results in a variety of tasks, one unsatisfying aspect is its length bias : models give high scores to short, inadequate hypotheses and often make the empty string the argmaxthe so-called cat got your tongue problem. Recently proposed entmax-based sparse sequence-to-sequence models present a possible solution, since they can shrink the search space by assigning zero probability to bad hypotheses, but their ability to handle word-level tasks with transformers has never been tested. In this work, we show that entmax-based models effectively solve the cat got your tongue problem, removing a major source of model error for <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a>. In addition, we generalize label smoothing, a critical regularization technique, to the broader family of Fenchel-Young losses, which includes both <a href=https://en.wikipedia.org/wiki/Cross-entropy>cross-entropy</a> and the entmax losses. Our resulting label-smoothed entmax loss models set a new state of the art on multilingual grapheme-to-phoneme conversion and deliver improvements and better calibration properties on cross-lingual morphological inflection and <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> for 7 language pairs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.spnlp-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.spnlp-1.0/>Proceedings of the 5th Workshop on Structured Prediction for NLP (SPNLP 2021)</a></strong><br><a href=/people/z/zornitsa-kozareva/>Zornitsa Kozareva</a>
|
<a href=/people/s/sujith-ravi/>Sujith Ravi</a>
|
<a href=/people/a/andreas-vlachos/>Andreas Vlachos</a>
|
<a href=/people/p/priyanka-agrawal/>Priyanka Agrawal</a>
|
<a href=/people/a/andre-f-t-martins/>André Martins</a><br><a href=/volumes/2021.spnlp-1/ class=text-muted>Proceedings of the 5th Workshop on Structured Prediction for NLP (SPNLP 2021)</a></span></p><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.eamt-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.eamt-1.0/>Proceedings of the 22nd Annual Conference of the European Association for Machine Translation</a></strong><br><a href=/people/a/andre-f-t-martins/>André Martins</a>
|
<a href=/people/h/helena-moniz/>Helena Moniz</a>
|
<a href=/people/s/sara-fumega/>Sara Fumega</a>
|
<a href=/people/b/bruno-martins/>Bruno Martins</a>
|
<a href=/people/f/fernando-batista/>Fernando Batista</a>
|
<a href=/people/l/luisa-coheur/>Luisa Coheur</a>
|
<a href=/people/c/carla-parra-escartin/>Carla Parra</a>
|
<a href=/people/i/isabel-trancoso/>Isabel Trancoso</a>
|
<a href=/people/m/marco-turchi/>Marco Turchi</a>
|
<a href=/people/a/arianna-bisazza/>Arianna Bisazza</a>
|
<a href=/people/j/joss-moorkens/>Joss Moorkens</a>
|
<a href=/people/a/ana-guerberof/>Ana Guerberof</a>
|
<a href=/people/m/mary-nurminen/>Mary Nurminen</a>
|
<a href=/people/l/lena-marg/>Lena Marg</a>
|
<a href=/people/m/mikel-l-forcada/>Mikel L. Forcada</a><br><a href=/volumes/2020.eamt-1/ class=text-muted>Proceedings of the 22nd Annual Conference of the European Association for Machine Translation</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.171.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--171 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.171 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939140 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.171" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.171/>Understanding the Mechanics of SPIGOT : Surrogate Gradients for Latent Structure Learning<span class=acl-fixed-case>SPIGOT</span>: Surrogate Gradients for Latent Structure Learning</a></strong><br><a href=/people/t/tsvetomila-mihaylova/>Tsvetomila Mihaylova</a>
|
<a href=/people/v/vlad-niculae/>Vlad Niculae</a>
|
<a href=/people/a/andre-f-t-martins/>André F. T. Martins</a><br><a href=/volumes/2020.emnlp-main/ class=text-muted>Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--171><div class="card-body p-3 small">Latent structure models are a powerful tool for modeling language data : they can mitigate the error propagation and annotation bottleneck in pipeline systems, while simultaneously uncovering linguistic insights about the data. One challenge with end-to-end training of these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> is the argmax operation, which has null gradient. In this paper, we focus on surrogate gradients, a popular strategy to deal with this problem. We explore latent structure learning through the angle of pulling back the downstream learning objective. In this paradigm, we discover a principled motivation for both the straight-through estimator (STE) as well as the recently-proposed SPIGOT a variant of STE for structured models. Our perspective leads to new <a href=https://en.wikipedia.org/wiki/Algorithm>algorithms</a> in the same family. We empirically compare the known and the novel pulled-back estimators against the popular alternatives, yielding new insight for practitioners and revealing intriguing failure cases.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.acl-main.776.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--acl-main--776 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.acl-main.776 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://slideslive.com/38929446 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.acl-main.776/>Revisiting Higher-Order Dependency Parsers</a></strong><br><a href=/people/e/erick-fonseca/>Erick Fonseca</a>
|
<a href=/people/a/andre-f-t-martins/>André F. T. Martins</a><br><a href=/volumes/2020.acl-main/ class=text-muted>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--acl-main--776><div class="card-body p-3 small">Neural encoders have allowed dependency parsers to shift from higher-order structured models to simpler first-order ones, making decoding faster and still achieving better accuracy than non-neural parsers. This has led to a belief that neural encoders can implicitly encode structural constraints, such as <a href=https://en.wikipedia.org/wiki/Sibling>siblings</a> and grandparents in a <a href=https://en.wikipedia.org/wiki/Tree_(graph_theory)>tree</a>. We tested this hypothesis and found that <a href=https://en.wikipedia.org/wiki/Parsing>neural parsers</a> may benefit from higher-order features, even when employing a powerful pre-trained encoder, such as BERT. While the gains of higher-order features are small in the presence of a powerful <a href=https://en.wikipedia.org/wiki/Encoder>encoder</a>, they are consistent for long-range dependencies and long sentences. In particular, higher-order models are more accurate on full sentence parses and on the exact match of modifier lists, indicating that they deal better with larger, more complex structures.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.spnlp-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.spnlp-1.0/>Proceedings of the Fourth Workshop on Structured Prediction for NLP</a></strong><br><a href=/people/p/priyanka-agrawal/>Priyanka Agrawal</a>
|
<a href=/people/z/zornitsa-kozareva/>Zornitsa Kozareva</a>
|
<a href=/people/j/julia-kreutzer/>Julia Kreutzer</a>
|
<a href=/people/g/gerasimos-lampouras/>Gerasimos Lampouras</a>
|
<a href=/people/a/andre-f-t-martins/>André Martins</a>
|
<a href=/people/s/sujith-ravi/>Sujith Ravi</a>
|
<a href=/people/a/andreas-vlachos/>Andreas Vlachos</a><br><a href=/volumes/2020.spnlp-1/ class=text-muted>Proceedings of the Fourth Workshop on Structured Prediction for NLP</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wmt-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.wmt-1.0/>Proceedings of the Fifth Conference on Machine Translation</a></strong><br><a href=/people/l/loic-barrault/>Loïc Barrault</a>
|
<a href=/people/o/ondrej-bojar/>Ondřej Bojar</a>
|
<a href=/people/f/fethi-bougares/>Fethi Bougares</a>
|
<a href=/people/r/rajen-chatterjee/>Rajen Chatterjee</a>
|
<a href=/people/m/marta-r-costa-jussa/>Marta R. Costa-jussà</a>
|
<a href=/people/c/christian-federmann/>Christian Federmann</a>
|
<a href=/people/m/mark-fishel/>Mark Fishel</a>
|
<a href=/people/a/alexander-fraser/>Alexander Fraser</a>
|
<a href=/people/y/yvette-graham/>Yvette Graham</a>
|
<a href=/people/p/paco-guzman/>Paco Guzman</a>
|
<a href=/people/b/barry-haddow/>Barry Haddow</a>
|
<a href=/people/m/matthias-huck/>Matthias Huck</a>
|
<a href=/people/a/antonio-jimeno-yepes/>Antonio Jimeno Yepes</a>
|
<a href=/people/p/philipp-koehn/>Philipp Koehn</a>
|
<a href=/people/a/andre-f-t-martins/>André Martins</a>
|
<a href=/people/m/makoto-morishita/>Makoto Morishita</a>
|
<a href=/people/c/christof-monz/>Christof Monz</a>
|
<a href=/people/m/masaaki-nagata/>Masaaki Nagata</a>
|
<a href=/people/t/toshiaki-nakazawa/>Toshiaki Nakazawa</a>
|
<a href=/people/m/matteo-negri/>Matteo Negri</a><br><a href=/volumes/2020.wmt-1/ class=text-muted>Proceedings of the Fifth Conference on Machine Translation</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wmt-1.119.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wmt-1--119 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wmt-1.119 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939643 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.wmt-1.119/>IST-Unbabel Participation in the WMT20 Quality Estimation Shared Task<span class=acl-fixed-case>IST</span>-Unbabel Participation in the <span class=acl-fixed-case>WMT</span>20 Quality Estimation Shared Task</a></strong><br><a href=/people/j/joao-moura/>João Moura</a>
|
<a href=/people/m/miguel-vera/>Miguel Vera</a>
|
<a href=/people/d/daan-van-stigt/>Daan van Stigt</a>
|
<a href=/people/f/fabio-kepler/>Fabio Kepler</a>
|
<a href=/people/a/andre-f-t-martins/>André F. T. Martins</a><br><a href=/volumes/2020.wmt-1/ class=text-muted>Proceedings of the Fifth Conference on Machine Translation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wmt-1--119><div class="card-body p-3 small">We present the joint contribution of IST and Unbabel to the WMT 2020 Shared Task on Quality Estimation. Our team participated on all tracks (Direct Assessment, Post-Editing Effort, Document-Level), encompassing a total of 14 submissions. Our submitted systems were developed by extending the OpenKiwi framework to a transformer-based predictor-estimator architecture, and to cope with glass-box, uncertainty-based features coming from neural machine translation systems.</div></div><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-1500.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-1500/>Proceedings of the Third Workshop on Structured Prediction for <span class=acl-fixed-case>NLP</span></a></strong><br><a href=/people/a/andre-f-t-martins/>Andre Martins</a>
|
<a href=/people/a/andreas-vlachos/>Andreas Vlachos</a>
|
<a href=/people/z/zornitsa-kozareva/>Zornitsa Kozareva</a>
|
<a href=/people/s/sujith-ravi/>Sujith Ravi</a>
|
<a href=/people/g/gerasimos-lampouras/>Gerasimos Lampouras</a>
|
<a href=/people/v/vlad-niculae/>Vlad Niculae</a>
|
<a href=/people/j/julia-kreutzer/>Julia Kreutzer</a><br><a href=/volumes/W19-15/ class=text-muted>Proceedings of the Third Workshop on Structured Prediction for NLP</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4207.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4207 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4207 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4207/>ITIST at the SIGMORPHON 2019 Shared Task : Sparse Two-headed Models for Inflection<span class=acl-fixed-case>IT</span>–<span class=acl-fixed-case>IST</span> at the <span class=acl-fixed-case>SIGMORPHON</span> 2019 Shared Task: Sparse Two-headed Models for Inflection</a></strong><br><a href=/people/b/ben-peters/>Ben Peters</a>
|
<a href=/people/a/andre-f-t-martins/>André F. T. Martins</a><br><a href=/volumes/W19-42/ class=text-muted>Proceedings of the 16th Workshop on Computational Research in Phonetics, Phonology, and Morphology</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4207><div class="card-body p-3 small">This paper presents the Instituto de TelecomunicaesInstituto Superior Tcnico submission to Task 1 of the SIGMORPHON 2019 Shared Task. Our models combine sparse sequence-to-sequence models with a two-headed attention mechanism that learns separate attention distributions for the <a href=https://en.wikipedia.org/wiki/Lemma_(morphology)>lemma</a> and inflectional tags. Among submissions to Task 1, our <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> rank second and third. Despite the low data setting of the task (only 100 in-language training examples), they learn plausible inflection patterns and often concentrate all probability mass into a small set of hypotheses, making <a href=https://en.wikipedia.org/wiki/Beam_search>beam search</a> exact.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5200.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5200/>Proceedings of the Fourth Conference on Machine Translation (Volume 1: Research Papers)</a></strong><br><a href=/people/o/ondrej-bojar/>Ondřej Bojar</a>
|
<a href=/people/r/rajen-chatterjee/>Rajen Chatterjee</a>
|
<a href=/people/c/christian-federmann/>Christian Federmann</a>
|
<a href=/people/m/mark-fishel/>Mark Fishel</a>
|
<a href=/people/y/yvette-graham/>Yvette Graham</a>
|
<a href=/people/b/barry-haddow/>Barry Haddow</a>
|
<a href=/people/m/matthias-huck/>Matthias Huck</a>
|
<a href=/people/a/antonio-jimeno-yepes/>Antonio Jimeno Yepes</a>
|
<a href=/people/p/philipp-koehn/>Philipp Koehn</a>
|
<a href=/people/a/andre-f-t-martins/>André Martins</a>
|
<a href=/people/c/christof-monz/>Christof Monz</a>
|
<a href=/people/m/matteo-negri/>Matteo Negri</a>
|
<a href=/people/a/aurelie-neveol/>Aurélie Névéol</a>
|
<a href=/people/m/mariana-neves/>Mariana Neves</a>
|
<a href=/people/m/matt-post/>Matt Post</a>
|
<a href=/people/m/marco-turchi/>Marco Turchi</a>
|
<a href=/people/k/karin-verspoor/>Karin Verspoor</a><br><a href=/volumes/W19-52/ class=text-muted>Proceedings of the Fourth Conference on Machine Translation (Volume 1: Research Papers)</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5300.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5300/>Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)</a></strong><br><a href=/people/o/ondrej-bojar/>Ondřej Bojar</a>
|
<a href=/people/r/rajen-chatterjee/>Rajen Chatterjee</a>
|
<a href=/people/c/christian-federmann/>Christian Federmann</a>
|
<a href=/people/m/mark-fishel/>Mark Fishel</a>
|
<a href=/people/y/yvette-graham/>Yvette Graham</a>
|
<a href=/people/b/barry-haddow/>Barry Haddow</a>
|
<a href=/people/m/matthias-huck/>Matthias Huck</a>
|
<a href=/people/a/antonio-jimeno-yepes/>Antonio Jimeno Yepes</a>
|
<a href=/people/p/philipp-koehn/>Philipp Koehn</a>
|
<a href=/people/a/andre-f-t-martins/>André Martins</a>
|
<a href=/people/c/christof-monz/>Christof Monz</a>
|
<a href=/people/m/matteo-negri/>Matteo Negri</a>
|
<a href=/people/a/aurelie-neveol/>Aurélie Névéol</a>
|
<a href=/people/m/mariana-neves/>Mariana Neves</a>
|
<a href=/people/m/matt-post/>Matt Post</a>
|
<a href=/people/m/marco-turchi/>Marco Turchi</a>
|
<a href=/people/k/karin-verspoor/>Karin Verspoor</a><br><a href=/volumes/W19-53/ class=text-muted>Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5400.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5400/>Proceedings of the Fourth Conference on Machine Translation (Volume 3: Shared Task Papers, Day 2)</a></strong><br><a href=/people/o/ondrej-bojar/>Ondřej Bojar</a>
|
<a href=/people/r/rajen-chatterjee/>Rajen Chatterjee</a>
|
<a href=/people/c/christian-federmann/>Christian Federmann</a>
|
<a href=/people/m/mark-fishel/>Mark Fishel</a>
|
<a href=/people/y/yvette-graham/>Yvette Graham</a>
|
<a href=/people/b/barry-haddow/>Barry Haddow</a>
|
<a href=/people/m/matthias-huck/>Matthias Huck</a>
|
<a href=/people/a/antonio-jimeno-yepes/>Antonio Jimeno Yepes</a>
|
<a href=/people/p/philipp-koehn/>Philipp Koehn</a>
|
<a href=/people/a/andre-f-t-martins/>André Martins</a>
|
<a href=/people/c/christof-monz/>Christof Monz</a>
|
<a href=/people/m/matteo-negri/>Matteo Negri</a>
|
<a href=/people/a/aurelie-neveol/>Aurélie Névéol</a>
|
<a href=/people/m/mariana-neves/>Mariana Neves</a>
|
<a href=/people/m/matt-post/>Matt Post</a>
|
<a href=/people/m/marco-turchi/>Marco Turchi</a>
|
<a href=/people/k/karin-verspoor/>Karin Verspoor</a><br><a href=/volumes/W19-54/ class=text-muted>Proceedings of the Fourth Conference on Machine Translation (Volume 3: Shared Task Papers, Day 2)</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1313.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1313 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1313 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N19-1313.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/361725345 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1313" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1313/>Selective Attention for Context-aware Neural Machine Translation</a></strong><br><a href=/people/s/sameen-maruf/>Sameen Maruf</a>
|
<a href=/people/a/andre-f-t-martins/>André F. T. Martins</a>
|
<a href=/people/g/gholamreza-haffari/>Gholamreza Haffari</a><br><a href=/volumes/N19-1/ class=text-muted>Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1313><div class="card-body p-3 small">Despite the progress made in sentence-level NMT, current systems still fall short at achieving fluent, good quality translation for a full document. Recent works in context-aware NMT consider only a few previous sentences as context and may not scale to entire documents. To this end, we propose a novel and scalable top-down approach to hierarchical attention for context-aware NMT which uses sparse attention to selectively focus on relevant sentences in the document context and then attends to key words in those sentences. We also propose single-level attention approaches based on sentence or word-level information in the context. The document-level context representation, produced from these attention modules, is integrated into the encoder or decoder of the Transformer model depending on whether we use monolingual or bilingual context. Our experiments and evaluation on English-German datasets in different document MT settings show that our selective attention approach not only significantly outperforms context-agnostic baselines but also surpasses context-aware baselines in most cases.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1292.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1292 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1292 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1292" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-1292/>A Simple and Effective Approach to Automatic Post-Editing with <a href=https://en.wikipedia.org/wiki/Transfer_learning>Transfer Learning</a></a></strong><br><a href=/people/g/goncalo-m-correia/>Gonçalo M. Correia</a>
|
<a href=/people/a/andre-f-t-martins/>André F. T. Martins</a><br><a href=/volumes/P19-1/ class=text-muted>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1292><div class="card-body p-3 small">Automatic post-editing (APE) seeks to automatically refine the output of a black-box machine translation (MT) system through human post-edits. APE systems are usually trained by complementing human post-edited data with large, artificial data generated through back-translations, a time-consuming process often no easier than training a MT system from scratch. in this paper, we propose an alternative where we fine-tune pre-trained BERT models on both the encoder and decoder of an APE system, exploring several parameter sharing strategies. By only training on a <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> of 23 K sentences for 3 hours on a single GPU we obtain results that are competitive with <a href=https://en.wikipedia.org/wiki/Artificial_intelligence>systems</a> that were trained on 5 M artificial sentences. When we add this <a href=https://en.wikipedia.org/wiki/Data_(computing)>artificial data</a> our <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>method</a> obtains state-of-the-art results.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-2026.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-2026 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-2026 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-2026/>Joint Learning of Named Entity Recognition and Entity Linking</a></strong><br><a href=/people/p/pedro-henrique-martins/>Pedro Henrique Martins</a>
|
<a href=/people/z/zita-marinho/>Zita Marinho</a>
|
<a href=/people/a/andre-f-t-martins/>André F. T. Martins</a><br><a href=/volumes/P19-2/ class=text-muted>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-2026><div class="card-body p-3 small">Named entity recognition (NER) and entity linking (EL) are two fundamentally related tasks, since in order to perform EL, first the mentions to entities have to be detected. However, most entity linking approaches disregard the mention detection part, assuming that the correct mentions have been previously detected. In this paper, we perform joint learning of NER and EL to leverage their relatedness and obtain a more robust and generalisable system. For that, we introduce a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> inspired by the Stack-LSTM approach. We observe that, in fact, doing <a href=https://en.wikipedia.org/wiki/Multi-task_learning>multi-task learning</a> of NER and EL improves the performance in both tasks when comparing with <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> trained with individual objectives. Furthermore, we achieve results competitive with the <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a> in both NER and EL.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-2049.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-2049 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-2049 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-2049" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-2049/>Scheduled Sampling for Transformers</a></strong><br><a href=/people/t/tsvetomila-mihaylova/>Tsvetomila Mihaylova</a>
|
<a href=/people/a/andre-f-t-martins/>André F. T. Martins</a><br><a href=/volumes/P19-2/ class=text-muted>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-2049><div class="card-body p-3 small">Scheduled sampling is a technique for avoiding one of the known problems in sequence-to-sequence generation : <a href=https://en.wikipedia.org/wiki/Exposure_bias>exposure bias</a>. It consists of feeding the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> a mix of the teacher forced embeddings and the model predictions from the previous step in training time. The <a href=https://en.wikipedia.org/wiki/Scientific_technique>technique</a> has been used for improving <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> performance with <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent neural networks (RNN)</a>. In the Transformer model, unlike the RNN, the generation of a new word attends to the full sentence generated so far, not only to the last word, and it is not straightforward to apply the scheduled sampling technique. We propose some structural changes to allow scheduled sampling to be applied to Transformer architectures, via a two-pass decoding strategy. Experiments on two language pairs achieve performance close to a teacher-forcing baseline and show that this technique is promising for further exploration.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-3020.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-3020 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-3020 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><span class="align-middle mr-1" data-toggle=tooltip data-placement=bottom title="Best Demo Paper"><i class="fas fa-award"></i></span><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-3020" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-3020/>OpenKiwi : An Open Source Framework for <a href=https://en.wikipedia.org/wiki/Quality_assurance>Quality Estimation</a><span class=acl-fixed-case>O</span>pen<span class=acl-fixed-case>K</span>iwi: An Open Source Framework for Quality Estimation</a></strong><br><a href=/people/f/fabio-kepler/>Fabio Kepler</a>
|
<a href=/people/j/jonay-trenous/>Jonay Trénous</a>
|
<a href=/people/m/marcos-treviso/>Marcos Treviso</a>
|
<a href=/people/m/miguel-vera/>Miguel Vera</a>
|
<a href=/people/a/andre-f-t-martins/>André F. T. Martins</a><br><a href=/volumes/P19-3/ class=text-muted>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: System Demonstrations</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-3020><div class="card-body p-3 small">We introduce OpenKiwi, a Pytorch-based open source framework for translation quality estimation. OpenKiwi supports training and testing of word-level and sentence-level quality estimation systems, implementing the winning systems of the WMT 201518 quality estimation campaigns. We benchmark OpenKiwi on two datasets from WMT 2018 (English-German SMT and NMT), yielding state-of-the-art performance on the word-level tasks and near state-of-the-art in the sentence-level tasks.</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1108.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1108 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1108 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D18-1108.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/305198410 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1108" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1108/>Towards Dynamic Computation Graphs via Sparse Latent Structure</a></strong><br><a href=/people/v/vlad-niculae/>Vlad Niculae</a>
|
<a href=/people/a/andre-f-t-martins/>André F. T. Martins</a>
|
<a href=/people/c/claire-cardie/>Claire Cardie</a><br><a href=/volumes/D18-1/ class=text-muted>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1108><div class="card-body p-3 small">Deep NLP models benefit from underlying structures in the datae.g., parse treestypically extracted using off-the-shelf <a href=https://en.wikipedia.org/wiki/Parsing>parsers</a>. Recent attempts to jointly learn the latent structure encounter a tradeoff : either make factorization assumptions that limit expressiveness, or sacrifice end-to-end differentiability. Using the recently proposed SparseMAP inference, which retrieves a sparse distribution over latent structures, we propose a novel approach for end-to-end learning of latent structure predictors jointly with a downstream predictor. To the best of our knowledge, our method is the first to enable unrestricted dynamic computation graph construction from the global latent structure, while maintaining <a href=https://en.wikipedia.org/wiki/Derivative>differentiability</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-2100.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-2100/>Proceedings of the <span class=acl-fixed-case>AMTA</span> 2018 Workshop on Translation Quality Estimation and Automatic Post-Editing</a></strong><br><a href=/people/r/ramon-fernandez-astudillo/>Ramón Astudillo</a>
|
<a href=/people/j/joao-graca/>João Graça</a>
|
<a href=/people/a/andre-f-t-martins/>André Martins</a><br><a href=/volumes/W18-21/ class=text-muted>Proceedings of the AMTA 2018 Workshop on Translation Quality Estimation and Automatic Post-Editing</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5450.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5450 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5450 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5450/>Interpretable Structure Induction via Sparse Attention</a></strong><br><a href=/people/b/ben-peters/>Ben Peters</a>
|
<a href=/people/v/vlad-niculae/>Vlad Niculae</a>
|
<a href=/people/a/andre-f-t-martins/>André F. T. Martins</a><br><a href=/volumes/W18-54/ class=text-muted>Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5450><div class="card-body p-3 small">Neural network methods are experiencing wide adoption in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a>, thanks to their empirical performance on many tasks. Modern neural architectures go way beyond simple feedforward and recurrent models : they are complex pipelines that perform soft, differentiable computation instead of <a href=https://en.wikipedia.org/wiki/Discrete_mathematics>discrete logic</a>. The price of such <a href=https://en.wikipedia.org/wiki/Soft_computing>soft computing</a> is the introduction of dense dependencies, which make it hard to disentangle the patterns that trigger a prediction. Our recent work on sparse and structured latent computation presents a promising avenue for enhancing interpretability of such neural pipelines. Through this extended abstract, we aim to discuss and explore the potential and impact of our <a href=https://en.wikipedia.org/wiki/Methodology>methods</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6311.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6311 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6311 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-6311" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-6311/>Contextual Neural Model for Translating Bilingual Multi-Speaker Conversations</a></strong><br><a href=/people/s/sameen-maruf/>Sameen Maruf</a>
|
<a href=/people/a/andre-f-t-martins/>André F. T. Martins</a>
|
<a href=/people/g/gholamreza-haffari/>Gholamreza Haffari</a><br><a href=/volumes/W18-63/ class=text-muted>Proceedings of the Third Conference on Machine Translation: Research Papers</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6311><div class="card-body p-3 small">Recent works in <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a> have begun to explore document translation. However, translating online multi-speaker conversations is still an open problem. In this work, we propose the task of translating Bilingual Multi-Speaker Conversations, and explore neural architectures which exploit both source and target-side conversation histories for this task. To initiate an evaluation for this task, we introduce <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> extracted from <a href=https://en.wikipedia.org/wiki/Europarl>Europarl v7</a> and OpenSubtitles2016. Our experiments on four language-pairs confirm the significance of leveraging conversation history, both in terms of <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a> and manual evaluation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2059.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2059 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2059 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2059.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2059.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285803960 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-2059/>Sparse and Constrained Attention for Neural Machine Translation</a></strong><br><a href=/people/c/chaitanya-malaviya/>Chaitanya Malaviya</a>
|
<a href=/people/p/pedro-ferreira/>Pedro Ferreira</a>
|
<a href=/people/a/andre-f-t-martins/>André F. T. Martins</a><br><a href=/volumes/P18-2/ class=text-muted>Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2059><div class="card-body p-3 small">In <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a>, words are sometimes dropped from the source or generated repeatedly in the <a href=https://en.wikipedia.org/wiki/Translation>translation</a>. We explore novel <a href=https://en.wikipedia.org/wiki/Strategy>strategies</a> to address the coverage problem that change only the attention transformation. Our approach allocates fertilities to source words, used to bound the attention each word can receive. We experiment with various sparse and constrained attention transformations and propose a new one, constrained sparsemax, shown to be differentiable and sparse. Empirical evaluation is provided in three languages pairs.</div></div><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/Q17-1015.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-Q17-1015 data-toggle=collapse aria-expanded=false aria-controls=abstract-Q17-1015 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234955039 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/Q17-1015/>Pushing the Limits of Translation Quality Estimation</a></strong><br><a href=/people/a/andre-f-t-martins/>André F. T. Martins</a>
|
<a href=/people/m/marcin-junczys-dowmunt/>Marcin Junczys-Dowmunt</a>
|
<a href=/people/f/fabio-kepler/>Fabio N. Kepler</a>
|
<a href=/people/r/ramon-fernandez-astudillo/>Ramón Astudillo</a>
|
<a href=/people/c/chris-hokamp/>Chris Hokamp</a>
|
<a href=/people/r/roman-grundkiewicz/>Roman Grundkiewicz</a><br><a href=/volumes/Q17-1/ class=text-muted>Transactions of the Association for Computational Linguistics, Volume 5</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-Q17-1015><div class="card-body p-3 small">Translation quality estimation is a task of growing importance in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a>, due to its potential to reduce post-editing human effort in disruptive ways. However, this potential is currently limited by the relatively low <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of existing <a href=https://en.wikipedia.org/wiki/System>systems</a>. In this paper, we achieve remarkable improvements by exploiting synergies between the related tasks of word-level quality estimation and automatic post-editing. First, we stack a new, carefully engineered, neural model into a rich feature-based word-level quality estimation system. Then, we use the output of an automatic post-editing system as an extra feature, obtaining striking results on WMT16 : a word-level FMULT1 score of 57.47 % (an absolute gain of +7.95 % over the current state of the art), and a <a href=https://en.wikipedia.org/wiki/Pearson_correlation_coefficient>Pearson correlation score</a> of 65.56 % for sentence-level HTER prediction (an absolute gain of +13.36 %).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1036.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1036 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1036 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D17-1036.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D17-1036/>Learning What’s Easy : Fully Differentiable Neural Easy-First Taggers</a></strong><br><a href=/people/a/andre-f-t-martins/>André F. T. Martins</a>
|
<a href=/people/j/julia-kreutzer/>Julia Kreutzer</a><br><a href=/volumes/D17-1/ class=text-muted>Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1036><div class="card-body p-3 small">We introduce a novel neural easy-first decoder that learns to solve sequence tagging tasks in a flexible order. In contrast to previous easy-first decoders, our <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> are end-to-end differentiable. The <a href=https://en.wikipedia.org/wiki/Codec>decoder</a> iteratively updates a sketch of the predictions over the sequence. At its core is an <a href=https://en.wikipedia.org/wiki/Attentional_control>attention mechanism</a> that controls which parts of the input are strategically the best to process next. We present a new constrained softmax transformation that ensures the same cumulative attention to every word, and show how to efficiently evaluate and backpropagate over it. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> compare favourably to BILSTM taggers on three sequence tagging tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-3000.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-3000/>Proceedings of the Software Demonstrations of the 15th Conference of the <span class=acl-fixed-case>E</span>uropean Chapter of the Association for Computational Linguistics</a></strong><br><a href=/people/a/andre-f-t-martins/>André Martins</a>
|
<a href=/people/a/anselmo-penas/>Anselmo Peñas</a><br><a href=/volumes/E17-3/ class=text-muted>Proceedings of the Software Demonstrations of the 15th Conference of the European Chapter of the Association for Computational Linguistics</a></span></p></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Andr%C3%A9+F.+T.+Martins" title="Search for 'André F. T. Martins' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/o/ondrej-bojar/ class=align-middle>Ondřej Bojar</a>
<span class="badge badge-secondary align-middle ml-2">5</span></li><li class=list-group-item><a href=/people/r/rajen-chatterjee/ class=align-middle>Rajen Chatterjee</a>
<span class="badge badge-secondary align-middle ml-2">5</span></li><li class=list-group-item><a href=/people/c/christian-federmann/ class=align-middle>Christian Federmann</a>
<span class="badge badge-secondary align-middle ml-2">5</span></li><li class=list-group-item><a href=/people/m/mark-fishel/ class=align-middle>Mark Fishel</a>
<span class="badge badge-secondary align-middle ml-2">5</span></li><li class=list-group-item><a href=/people/y/yvette-graham/ class=align-middle>Yvette Graham</a>
<span class="badge badge-secondary align-middle ml-2">5</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/b/barry-haddow/ class=align-middle>Barry Haddow</a>
<span class="badge badge-secondary align-middle ml-2">5</span></li><li class=list-group-item><a href=/people/m/matthias-huck/ class=align-middle>Matthias Huck</a>
<span class="badge badge-secondary align-middle ml-2">5</span></li><li class=list-group-item><a href=/people/a/antonio-jimeno-yepes/ class=align-middle>Antonio Jimeno Yepes</a>
<span class="badge badge-secondary align-middle ml-2">5</span></li><li class=list-group-item><a href=/people/p/philipp-koehn/ class=align-middle>Philipp Koehn</a>
<span class="badge badge-secondary align-middle ml-2">5</span></li><li class=list-group-item><a href=/people/c/christof-monz/ class=align-middle>Christof Monz</a>
<span class="badge badge-secondary align-middle ml-2">5</span></li><li class=list-group-item><a href=/people/m/marco-turchi/ class=align-middle>Marco Turchi</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/people/v/vlad-niculae/ class=align-middle>Vlad Niculae</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/people/f/fabio-kepler/ class=align-middle>Fabio Kepler</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/people/m/matteo-negri/ class=align-middle>Matteo Negri</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/people/r/ricardo-rei/ class=align-middle>Ricardo Rei</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/j/julia-kreutzer/ class=align-middle>Julia Kreutzer</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/z/zornitsa-kozareva/ class=align-middle>Zornitsa Kozareva</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/s/sujith-ravi/ class=align-middle>Sujith Ravi</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/a/andreas-vlachos/ class=align-middle>Andreas Vlachos</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/c/chrysoula-zerva/ class=align-middle>Chrysoula Zerva</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/d/daan-van-stigt/ class=align-middle>Daan van Stigt</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/m/miguel-vera/ class=align-middle>Miguel Vera</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/b/ben-peters/ class=align-middle>Ben Peters</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/a/aurelie-neveol/ class=align-middle>Aurelie Neveol</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/m/mariana-neves/ class=align-middle>Mariana Neves</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/m/matt-post/ class=align-middle>Matt Post</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/k/karin-verspoor/ class=align-middle>Karin Verspoor</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/m/marcos-treviso/ class=align-middle>Marcos Treviso</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/k/kayo-yin/ class=align-middle>Kayo Yin</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/p/patrick-fernandes/ class=align-middle>Patrick Fernandes</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/g/graham-neubig/ class=align-middle>Graham Neubig</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/t/tsvetomila-mihaylova/ class=align-middle>Tsvetomila Mihaylova</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/p/pedro-henrique-martins/ class=align-middle>Pedro Henrique Martins</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/z/zita-marinho/ class=align-middle>Zita Marinho</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/r/ramon-fernandez-astudillo/ class=align-middle>Ramón Fernandez Astudillo</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/r/roman-grundkiewicz/ class=align-middle>Roman Grundkiewicz</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/p/priyanka-agrawal/ class=align-middle>Priyanka Agrawal</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/g/gerasimos-lampouras/ class=align-middle>Gerasimos Lampouras</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/l/loic-barrault/ class=align-middle>Loïc Barrault</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/f/fethi-bougares/ class=align-middle>Fethi Bougares</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/m/marta-r-costa-jussa/ class=align-middle>Marta R. Costa-jussà</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/a/alexander-fraser/ class=align-middle>Alexander Fraser</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/p/paco-guzman/ class=align-middle>Paco Guzman</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/m/makoto-morishita/ class=align-middle>Makoto Morishita</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/a/ana-c-farinha/ class=align-middle>Ana C Farinha</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/p/pedro-ramos/ class=align-middle>Pedro Ramos</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/t/taisiya-glushkova/ class=align-middle>Taisiya Glushkova</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/s/sameen-maruf/ class=align-middle>Sameen Maruf</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/g/gholamreza-haffari/ class=align-middle>Gholamreza Haffari</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/h/helena-moniz/ class=align-middle>Helena Moniz</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/sara-fumega/ class=align-middle>Sara Fumega</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/b/bruno-martins/ class=align-middle>Bruno Martins</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/f/fernando-batista/ class=align-middle>Fernando Batista</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/luisa-coheur/ class=align-middle>Luísa Coheur</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/carla-parra-escartin/ class=align-middle>Carla Parra Escartín</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/i/isabel-trancoso/ class=align-middle>Isabel Trancoso</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/arianna-bisazza/ class=align-middle>Arianna Bisazza</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/joss-moorkens/ class=align-middle>Joss Moorkens</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/ana-guerberof/ class=align-middle>Ana Guerberof</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/mary-nurminen/ class=align-middle>Mary Nurminen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/lena-marg/ class=align-middle>Lena Marg</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/mikel-l-forcada/ class=align-middle>Mikel L. Forcada</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/n/nuno-m-guerreiro/ class=align-middle>Nuno M. Guerreiro</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/danish-pruthi/ class=align-middle>Danish Pruthi</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/aditi-chaudhary/ class=align-middle>Aditi Chaudhary</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/e/erick-fonseca/ class=align-middle>Erick Fonseca</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/marcin-junczys-dowmunt/ class=align-middle>Marcin Junczys-Dowmunt</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/chris-hokamp/ class=align-middle>Chris Hokamp</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/claire-cardie/ class=align-middle>Claire Cardie</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/markus-freitag/ class=align-middle>Markus Freitag</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/tom-kocmi/ class=align-middle>Tom Kocmi</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/lucia-specia/ class=align-middle>Lucia Specia</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/f/frederic-blain/ class=align-middle>Frédéric Blain</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/marina-fomicheva/ class=align-middle>Marina Fomicheva</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/zhenhao-li/ class=align-middle>Zhenhao Li</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/v/vishrav-chaudhary/ class=align-middle>Vishrav Chaudhary</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jose-g-c-de-souza/ class=align-middle>José G. C. de Souza</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/craig-stewart/ class=align-middle>Craig Stewart</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/alon-lavie/ class=align-middle>Alon Lavie</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/joao-graca/ class=align-middle>João Graça</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/masaaki-nagata/ class=align-middle>Masaaki Nagata</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/toshiaki-nakazawa/ class=align-middle>Toshiaki Nakazawa</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/joao-moura/ class=align-middle>João Moura</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/anselmo-penas/ class=align-middle>Anselmo Peñas</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/chaitanya-malaviya/ class=align-middle>Chaitanya Malaviya</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/p/pedro-ferreira/ class=align-middle>Pedro Ferreira</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/g/goncalo-m-correia/ class=align-middle>Gonçalo M. Correia</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jonay-trenous/ class=align-middle>Jonay Trénous</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/acl/ class=align-middle>ACL</a><span class="badge badge-secondary align-middle ml-2">9</span></li><li class=list-group-item><a href=/venues/ws/ class=align-middle>WS</a><span class="badge badge-secondary align-middle ml-2">8</span></li><li class=list-group-item><a href=/venues/wmt/ class=align-middle>WMT</a><span class="badge badge-secondary align-middle ml-2">6</span></li><li class=list-group-item><a href=/venues/emnlp/ class=align-middle>EMNLP</a><span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/venues/spnlp/ class=align-middle>spnlp</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-venues aria-expanded=false aria-controls=more-venues>show all...</li><div class="collapse border-top" id=more-venues><li class=list-group-item><a href=/venues/naacl/ class=align-middle>NAACL</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/eamt/ class=align-middle>EAMT</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/eval4nlp/ class=align-middle>Eval4NLP</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/tacl/ class=align-middle>TACL</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/eacl/ class=align-middle>EACL</a><span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>