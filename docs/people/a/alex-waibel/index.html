<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Alex Waibel - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Alex</span> <span class=font-weight-bold>Waibel</span></h2><p class="font-weight-light text-muted"><span class=font-italic>Also published as:</span>
Alexander <span class=font-weight-normal>Waibel</span></p><hr><div class=row><div class=col-lg-9><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.iwslt-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.iwslt-1.0/>Proceedings of the 18th International Conference on Spoken Language Translation (IWSLT 2021)</a></strong><br><a href=/people/m/marcello-federico/>Marcello Federico</a>
|
<a href=/people/a/alex-waibel/>Alex Waibel</a>
|
<a href=/people/m/marta-r-costa-jussa/>Marta R. Costa-jussà</a>
|
<a href=/people/j/jan-niehues/>Jan Niehues</a>
|
<a href=/people/s/sebastian-stuker/>Sebastian Stuker</a>
|
<a href=/people/e/elizabeth-salesky/>Elizabeth Salesky</a><br><a href=/volumes/2021.iwslt-1/ class=text-muted>Proceedings of the 18th International Conference on Spoken Language Translation (IWSLT 2021)</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.iwslt-1.1.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--iwslt-1--1 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.iwslt-1.1 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.iwslt-1.1/>FINDINGS OF THE IWSLT 2021 EVALUATION CAMPAIGN<span class=acl-fixed-case>FINDINGS</span> <span class=acl-fixed-case>OF</span> <span class=acl-fixed-case>THE</span> <span class=acl-fixed-case>IWSLT</span> 2021 <span class=acl-fixed-case>EVALUATION</span> <span class=acl-fixed-case>CAMPAIGN</span></a></strong><br><a href=/people/a/antonios-anastasopoulos/>Antonios Anastasopoulos</a>
|
<a href=/people/o/ondrej-bojar/>Ondřej Bojar</a>
|
<a href=/people/j/jacob-bremerman/>Jacob Bremerman</a>
|
<a href=/people/r/roldano-cattoni/>Roldano Cattoni</a>
|
<a href=/people/m/maha-elbayad/>Maha Elbayad</a>
|
<a href=/people/m/marcello-federico/>Marcello Federico</a>
|
<a href=/people/x/xutai-ma/>Xutai Ma</a>
|
<a href=/people/s/satoshi-nakamura/>Satoshi Nakamura</a>
|
<a href=/people/m/matteo-negri/>Matteo Negri</a>
|
<a href=/people/j/jan-niehues/>Jan Niehues</a>
|
<a href=/people/j/juan-pino/>Juan Pino</a>
|
<a href=/people/e/elizabeth-salesky/>Elizabeth Salesky</a>
|
<a href=/people/s/sebastian-stuker/>Sebastian Stüker</a>
|
<a href=/people/k/katsuhito-sudoh/>Katsuhito Sudoh</a>
|
<a href=/people/m/marco-turchi/>Marco Turchi</a>
|
<a href=/people/a/alex-waibel/>Alexander Waibel</a>
|
<a href=/people/c/changhan-wang/>Changhan Wang</a>
|
<a href=/people/m/matthew-wiesner/>Matthew Wiesner</a><br><a href=/volumes/2021.iwslt-1/ class=text-muted>Proceedings of the 18th International Conference on Spoken Language Translation (IWSLT 2021)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--iwslt-1--1><div class="card-body p-3 small">The evaluation campaign of the International Conference on Spoken Language Translation (IWSLT 2021) featured this year four shared tasks : (i) Simultaneous speech translation, (ii) Offline speech translation, (iii) Multilingual speech translation, (iv) Low-resource speech translation. A total of 22 teams participated in at least one of the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a>. This paper describes each shared task, data and evaluation metrics, and reports results of the received submissions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.iwslt-1.18.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--iwslt-1--18 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.iwslt-1.18 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.iwslt-1.18/>Multilingual Speech Translation KIT @ IWSLT2021<span class=acl-fixed-case>KIT</span> @ <span class=acl-fixed-case>IWSLT</span>2021</a></strong><br><a href=/people/n/ngoc-quan-pham/>Ngoc-Quan Pham</a>
|
<a href=/people/t/tuan-nam-nguyen/>Tuan Nam Nguyen</a>
|
<a href=/people/t/thanh-le-ha/>Thanh-Le Ha</a>
|
<a href=/people/s/sebastian-stuker/>Sebastian Stüker</a>
|
<a href=/people/a/alex-waibel/>Alexander Waibel</a>
|
<a href=/people/d/dan-he/>Dan He</a><br><a href=/volumes/2021.iwslt-1/ class=text-muted>Proceedings of the 18th International Conference on Spoken Language Translation (IWSLT 2021)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--iwslt-1--18><div class="card-body p-3 small">This paper contains the description for the submission of Karlsruhe Institute of Technology (KIT) for the multilingual TEDx translation task in the IWSLT 2021 evaluation campaign. Our main approach is to develop both cascade and end-to-end systems and eventually combine them together to achieve the best possible results for this extremely low-resource setting. The report also confirms certain consistent architectural improvement added to the Transformer architecture, for all tasks : <a href=https://en.wikipedia.org/wiki/Translation>translation</a>, <a href=https://en.wikipedia.org/wiki/Transcription_(linguistics)>transcription</a> and speech translation.</div></div><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.eamt-1.6.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--eamt-1--6 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.eamt-1.6 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.eamt-1.6/>Incorporating External Annotation to improve Named Entity Translation in NMT<span class=acl-fixed-case>NMT</span></a></strong><br><a href=/people/m/maciej-modrzejewski/>Maciej Modrzejewski</a>
|
<a href=/people/m/miriam-exel/>Miriam Exel</a>
|
<a href=/people/b/bianka-buschbeck/>Bianka Buschbeck</a>
|
<a href=/people/t/thanh-le-ha/>Thanh-Le Ha</a>
|
<a href=/people/a/alex-waibel/>Alexander Waibel</a><br><a href=/volumes/2020.eamt-1/ class=text-muted>Proceedings of the 22nd Annual Conference of the European Association for Machine Translation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--eamt-1--6><div class="card-body p-3 small">The correct translation of named entities (NEs) still poses a challenge for conventional neural machine translation (NMT) systems. This study explores methods incorporating <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>named entity recognition (NER)</a> into <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>NMT</a> with the aim to improve <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>named entity translation</a>. It proposes an annotation method that integrates <a href=https://en.wikipedia.org/wiki/Named_entity>named entities</a> and insideoutsidebeginning (IOB) tagging into the neural network input with the use of source factors. Our experiments on EnglishGerman and English Chinese show that just by including different NE classes and IOB tagging, we can increase the BLEU score by around 1 point using the standard test set from WMT2019 and achieve up to 12 % increase in NE translation rates over a strong baseline.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lifelongnlp-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lifelongnlp-1.0/>Proceedings of the 2nd Workshop on Life-long Learning for Spoken Language Systems</a></strong><br><a href=/people/w/william-m-campbell/>William M. Campbell</a>
|
<a href=/people/a/alex-waibel/>Alex Waibel</a>
|
<a href=/people/d/dilek-hakkani-tur/>Dilek Hakkani-Tur</a>
|
<a href=/people/t/timothy-j-hazen/>Timothy J. Hazen</a>
|
<a href=/people/k/kevin-kilgour/>Kevin Kilgour</a>
|
<a href=/people/e/eunah-cho/>Eunah Cho</a>
|
<a href=/people/v/varun-kumar/>Varun Kumar</a>
|
<a href=/people/h/hadrien-glaude/>Hadrien Glaude</a><br><a href=/volumes/2020.lifelongnlp-1/ class=text-muted>Proceedings of the 2nd Workshop on Life-long Learning for Spoken Language Systems</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lifelongnlp-1.2.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lifelongnlp-1--2 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lifelongnlp-1.2 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lifelongnlp-1.2/>Supervised Adaptation of Sequence-to-Sequence Speech Recognition Systems using Batch-Weighting</a></strong><br><a href=/people/c/christian-huber/>Christian Huber</a>
|
<a href=/people/j/juan-hussain/>Juan Hussain</a>
|
<a href=/people/t/tuan-nam-nguyen/>Tuan-Nam Nguyen</a>
|
<a href=/people/k/kaihang-song/>Kaihang Song</a>
|
<a href=/people/s/sebastian-stuker/>Sebastian Stüker</a>
|
<a href=/people/a/alex-waibel/>Alexander Waibel</a><br><a href=/volumes/2020.lifelongnlp-1/ class=text-muted>Proceedings of the 2nd Workshop on Life-long Learning for Spoken Language Systems</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lifelongnlp-1--2><div class="card-body p-3 small">When training <a href=https://en.wikipedia.org/wiki/Speech_recognition>speech recognition systems</a>, one often faces the situation that sufficient amounts of training data for the language in question are available but only small amounts of data for the domain in question. This problem is even bigger for end-to-end speech recognition systems that only accept transcribed speech as training data, which is harder and more expensive to obtain than text data. In this paper we present experiments in adapting end-to-end speech recognition systems by a method which is called batch-weighting and which we contrast against <a href=https://en.wikipedia.org/wiki/Fine-tuning>regular fine-tuning</a>, i.e., to continue to train existing neural speech recognition models on <a href=https://en.wikipedia.org/wiki/Adaptation_data>adaptation data</a>. We perform experiments using these s techniques in adapting to topic, accent and <a href=https://en.wikipedia.org/wiki/Vocabulary>vocabulary</a>, showing that batch-weighting consistently outperforms <a href=https://en.wikipedia.org/wiki/Musical_tuning>fine-tuning</a>. In order to show the generalization capabilities of batch-weighting we perform experiments in several languages, i.e., <a href=https://en.wikipedia.org/wiki/Arabic>Arabic</a>, <a href=https://en.wikipedia.org/wiki/English_language>English</a> and <a href=https://en.wikipedia.org/wiki/German_language>German</a>. Due to its relatively small computational requirements <a href=https://en.wikipedia.org/wiki/Batch_processing>batch-weighting</a> is a suitable technique for <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised life-long learning</a> during the life-time of a <a href=https://en.wikipedia.org/wiki/Speech_recognition>speech recognition system</a>, e.g., from user corrections.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.iwltp-1.7.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--iwltp-1--7 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.iwltp-1.7 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.iwltp-1.7/>Removing European Language Barriers with Innovative Machine Translation Technology<span class=acl-fixed-case>E</span>uropean Language Barriers with Innovative Machine Translation Technology</a></strong><br><a href=/people/d/dario-franceschini/>Dario Franceschini</a>
|
<a href=/people/c/chiara-canton/>Chiara Canton</a>
|
<a href=/people/i/ivan-simonini/>Ivan Simonini</a>
|
<a href=/people/a/armin-schweinfurth/>Armin Schweinfurth</a>
|
<a href=/people/a/adelheid-glott/>Adelheid Glott</a>
|
<a href=/people/s/sebastian-stuker/>Sebastian Stüker</a>
|
<a href=/people/t/thai-son-nguyen/>Thai-Son Nguyen</a>
|
<a href=/people/f/felix-schneider/>Felix Schneider</a>
|
<a href=/people/t/thanh-le-ha/>Thanh-Le Ha</a>
|
<a href=/people/a/alex-waibel/>Alex Waibel</a>
|
<a href=/people/b/barry-haddow/>Barry Haddow</a>
|
<a href=/people/p/philip-williams/>Philip Williams</a>
|
<a href=/people/r/rico-sennrich/>Rico Sennrich</a>
|
<a href=/people/o/ondrej-bojar/>Ondřej Bojar</a>
|
<a href=/people/s/sangeet-sagar/>Sangeet Sagar</a>
|
<a href=/people/d/dominik-machacek/>Dominik Macháček</a>
|
<a href=/people/o/otakar-smrz/>Otakar Smrž</a><br><a href=/volumes/2020.iwltp-1/ class=text-muted>Proceedings of the 1st International Workshop on Language Technology Platforms</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--iwltp-1--7><div class="card-body p-3 small">This paper presents our progress towards deploying a versatile communication platform in the task of highly multilingual live speech translation for <a href=https://en.wikipedia.org/wiki/Convention_(meeting)>conferences</a> and remote meetings live subtitling. The <a href=https://en.wikipedia.org/wiki/Computing_platform>platform</a> has been designed with a focus on very low latency and high flexibility while allowing research prototypes of speech and text processing tools to be easily connected, regardless of where they physically run. We outline our architecture solution and also briefly compare it with the ELG platform. Technical details are provided on the most important components and we summarize the test deployment events we ran so far.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.iwslt-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.iwslt-1.0/>Proceedings of the 17th International Conference on Spoken Language Translation</a></strong><br><a href=/people/m/marcello-federico/>Marcello Federico</a>
|
<a href=/people/a/alex-waibel/>Alex Waibel</a>
|
<a href=/people/k/kevin-knight/>Kevin Knight</a>
|
<a href=/people/s/satoshi-nakamura/>Satoshi Nakamura</a>
|
<a href=/people/h/hermann-ney/>Hermann Ney</a>
|
<a href=/people/j/jan-niehues/>Jan Niehues</a>
|
<a href=/people/s/sebastian-stuker/>Sebastian Stüker</a>
|
<a href=/people/d/dekai-wu/>Dekai Wu</a>
|
<a href=/people/j/joseph-mariani/>Joseph Mariani</a>
|
<a href=/people/f/francois-yvon/>Francois Yvon</a><br><a href=/volumes/2020.iwslt-1/ class=text-muted>Proceedings of the 17th International Conference on Spoken Language Translation</a></span></p><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2019.iwslt-1.13.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2019--iwslt-1--13 data-toggle=collapse aria-expanded=false aria-controls=abstract-2019.iwslt-1.13 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2019.iwslt-1.13/>KIT’s Submission to the IWSLT 2019 Shared Task on Text Translation<span class=acl-fixed-case>KIT</span>’s Submission to the <span class=acl-fixed-case>IWSLT</span> 2019 Shared Task on Text Translation</a></strong><br><a href=/people/f/felix-schneider/>Felix Schneider</a>
|
<a href=/people/a/alex-waibel/>Alex Waibel</a><br><a href=/volumes/2019.iwslt-1/ class=text-muted>Proceedings of the 16th International Conference on Spoken Language Translation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2019--iwslt-1--13><div class="card-body p-3 small">In this paper, we describe KIT&#8217;s submission for the IWSLT 2019 shared task on text translation. Our <a href=https://en.wikipedia.org/wiki/System>system</a> is based on the transformer model [ 1 ] using our in-house implementation. We augment the available training data using <a href=https://en.wikipedia.org/wiki/Back-translation>back-translation</a> and employ <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a> for the final <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a>. For our best results, we used a 12-layer transformer-big config- uration, achieving state-of-the-art results on the WMT2018 test set. We also experiment with student-teacher models to improve performance of smaller <a href=https://en.wikipedia.org/wiki/Computer_simulation>models</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1115.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1115 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1115 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-1115/>Self-Attentional Models for <a href=https://en.wikipedia.org/wiki/Lattice_model_(physics)>Lattice Inputs</a></a></strong><br><a href=/people/m/matthias-sperber/>Matthias Sperber</a>
|
<a href=/people/g/graham-neubig/>Graham Neubig</a>
|
<a href=/people/n/ngoc-quan-pham/>Ngoc-Quan Pham</a>
|
<a href=/people/a/alex-waibel/>Alex Waibel</a><br><a href=/volumes/P19-1/ class=text-muted>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1115><div class="card-body p-3 small">Lattices are an efficient and effective method to encode ambiguity of upstream systems in natural language processing tasks, for example to compactly capture multiple speech recognition hypotheses, or to represent multiple linguistic analyses. Previous work has extended <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent neural networks</a> to model lattice inputs and achieved improvements in various tasks, but these models suffer from very slow computation speeds. This paper extends the recently proposed paradigm of self-attention to handle lattice inputs. Self-attention is a sequence modeling technique that relates inputs to one another by computing pairwise similarities and has gained popularity for both its strong results and its computational efficiency. To extend such models to handle <a href=https://en.wikipedia.org/wiki/Lattice_(group)>lattices</a>, we introduce probabilistic reachability masks that incorporate lattice structure into the model and support lattice scores if available. We also propose a method for adapting positional embeddings to <a href=https://en.wikipedia.org/wiki/Lattice_model_(physics)>lattice structures</a>. We apply the proposed model to a speech translation task and find that it outperforms all examined baselines while being much faster to compute than previous neural lattice models during both <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training</a> and <a href=https://en.wikipedia.org/wiki/Statistical_inference>inference</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-2015.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-2015 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-2015 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-2015/>Paraphrases as Foreign Languages in Multilingual Neural Machine Translation</a></strong><br><a href=/people/z/zhong-zhou/>Zhong Zhou</a>
|
<a href=/people/m/matthias-sperber/>Matthias Sperber</a>
|
<a href=/people/a/alex-waibel/>Alexander Waibel</a><br><a href=/volumes/P19-2/ class=text-muted>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-2015><div class="card-body p-3 small">Paraphrases, rewordings of the same semantic meaning, are useful for improving <a href=https://en.wikipedia.org/wiki/Generalization>generalization</a> and <a href=https://en.wikipedia.org/wiki/Translation>translation</a>. Unlike previous works that only explore <a href=https://en.wikipedia.org/wiki/Paraphrase>paraphrases</a> at the word or phrase level, we use different translations of the whole training data that are consistent in structure as <a href=https://en.wikipedia.org/wiki/Paraphrase>paraphrases</a> at the corpus level. We treat <a href=https://en.wikipedia.org/wiki/Paraphrase>paraphrases</a> as foreign languages, tag source sentences with paraphrase labels, and train on parallel paraphrases in the style of multilingual Neural Machine Translation (NMT). Our multi-paraphrase NMT that trains only on two languages outperforms the multilingual baselines. Adding <a href=https://en.wikipedia.org/wiki/Paraphrase>paraphrases</a> improves the rare word translation and increases <a href=https://en.wikipedia.org/wiki/Entropy>entropy</a> and diversity in <a href=https://en.wikipedia.org/wiki/Lexical_choice>lexical choice</a>. Adding the source paraphrases boosts performance better than adding the target ones, while adding both lifts performance further. We achieve a BLEU score of 57.2 for <a href=https://en.wikipedia.org/wiki/Bible_translations_into_French>French-to-English translation</a> using 24 corpus-level paraphrases of the <a href=https://en.wikipedia.org/wiki/Bible>Bible</a>, which outperforms the multilingual baselines and is +34.7 above the single-source single-target NMT baseline.</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-2020.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-2020 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-2020 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-2020/>KIT Lecture Translator : Multilingual Speech Translation with One-Shot Learning<span class=acl-fixed-case>KIT</span> Lecture Translator: Multilingual Speech Translation with One-Shot Learning</a></strong><br><a href=/people/f/florian-dessloch/>Florian Dessloch</a>
|
<a href=/people/t/thanh-le-ha/>Thanh-Le Ha</a>
|
<a href=/people/m/markus-muller/>Markus Müller</a>
|
<a href=/people/j/jan-niehues/>Jan Niehues</a>
|
<a href=/people/t/thai-son-nguyen/>Thai-Son Nguyen</a>
|
<a href=/people/n/ngoc-quan-pham/>Ngoc-Quan Pham</a>
|
<a href=/people/e/elizabeth-salesky/>Elizabeth Salesky</a>
|
<a href=/people/m/matthias-sperber/>Matthias Sperber</a>
|
<a href=/people/s/sebastian-stuker/>Sebastian Stüker</a>
|
<a href=/people/t/thomas-zenkel/>Thomas Zenkel</a>
|
<a href=/people/a/alex-waibel/>Alexander Waibel</a><br><a href=/volumes/C18-2/ class=text-muted>Proceedings of the 27th International Conference on Computational Linguistics: System Demonstrations</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-2020><div class="card-body p-3 small">In today&#8217;s globalized world we have the ability to communicate with people across the world. However, in many situations the <a href=https://en.wikipedia.org/wiki/Language_barrier>language barrier</a> still presents a major issue. For example, many foreign students coming to KIT to study are initially unable to follow a lecture in <a href=https://en.wikipedia.org/wiki/German_language>German</a>. Therefore, we offer an automatic simultaneous interpretation service for students. To fulfill this task, we have developed a low-latency translation system that is adapted to lectures and covers several language pairs. While the switch from traditional <a href=https://en.wikipedia.org/wiki/Statistical_machine_translation>Statistical Machine Translation</a> to Neural Machine Translation (NMT) significantly improved performance, to integrate NMT into the speech translation framework required several adjustments. We have addressed the <a href=https://en.wikipedia.org/wiki/Run_time_(program_lifecycle_phase)>run-time constraints</a> and different types of input. Furthermore, we utilized <a href=https://en.wikipedia.org/wiki/One-shot_learning>one-shot learning</a> to easily add new topic-specific terms to the <a href=https://en.wikipedia.org/wiki/System>system</a>. Besides better performance, <a href=https://en.wikipedia.org/wiki/Non-verbal_communication>NMT</a> also enabled us increase our covered languages through multilingual NMT. % Combining these techniques, we are able to provide an adapted speech translation system for several <a href=https://en.wikipedia.org/wiki/Languages_of_Europe>European languages</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-2606.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-2606 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-2606 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-2606" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-2606/>Robust and Scalable Differentiable Neural Computer for <a href=https://en.wikipedia.org/wiki/Question_answering>Question Answering</a></a></strong><br><a href=/people/j/jorg-franke/>Jörg Franke</a>
|
<a href=/people/j/jan-niehues/>Jan Niehues</a>
|
<a href=/people/a/alex-waibel/>Alex Waibel</a><br><a href=/volumes/W18-26/ class=text-muted>Proceedings of the Workshop on Machine Reading for Question Answering</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-2606><div class="card-body p-3 small">Deep learning models are often not easily adaptable to new <a href=https://en.wikipedia.org/wiki/Computational_complexity_theory>tasks</a> and require task-specific adjustments. The differentiable neural computer (DNC), a memory-augmented neural network, is designed as a general problem solver which can be used in a wide range of tasks. But in reality, it is hard to apply this <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> to new <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a>. We analyze the DNC and identify possible improvements within the application of <a href=https://en.wikipedia.org/wiki/Question_answering>question answering</a>. This motivates a more robust and scalable DNC (rsDNC). The objective precondition is to keep the general character of this <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> intact while making its application more reliable and speeding up its required training time. The rsDNC is distinguished by a more robust training, a slim memory unit and a bidirectional architecture. We not only achieve new state-of-the-art performance on the bAbI task, but also minimize the performance variance between different initializations. Furthermore, we demonstrate the simplified applicability of the rsDNC to new <a href=https://en.wikipedia.org/wiki/Task_(computing)>tasks</a> with passable results on the CNN RC task without adaptions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6324.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6324 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6324 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6324/>Massively Parallel Cross-Lingual Learning in Low-Resource Target Language Translation</a></strong><br><a href=/people/z/zhong-zhou/>Zhong Zhou</a>
|
<a href=/people/m/matthias-sperber/>Matthias Sperber</a>
|
<a href=/people/a/alex-waibel/>Alexander Waibel</a><br><a href=/volumes/W18-63/ class=text-muted>Proceedings of the Third Conference on Machine Translation: Research Papers</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6324><div class="card-body p-3 small">We work on <a href=https://en.wikipedia.org/wiki/Translation>translation</a> from rich-resource languages to low-resource languages. The main challenges we identify are the lack of low-resource language data, effective methods for cross-lingual transfer, and the variable-binding problem that is common in neural systems. We build a <a href=https://en.wikipedia.org/wiki/Machine_translation>translation system</a> that addresses these challenges using eight <a href=https://en.wikipedia.org/wiki/Languages_of_Europe>European language families</a> as our test ground. Firstly, we add the source and the target family labels and study intra-family and inter-family influences for effective cross-lingual transfer. We achieve an improvement of +9.9 in BLEU score for English-Swedish translation using eight families compared to the single-family multi-source multi-target baseline. Moreover, we find that training on two neighboring families closest to the low-resource language is often enough. Secondly, we construct an ablation study and find that reasonably good results can be achieved even with considerably less target data. Thirdly, we address the variable-binding problem by building an order-preserving named entity translation model. We obtain 60.6 % <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> in qualitative evaluation where our translations are akin to human translations in a preliminary study.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6422.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6422 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6422 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6422/>The Karlsruhe Institute of Technology Systems for the News Translation Task in WMT 2018<span class=acl-fixed-case>WMT</span> 2018</a></strong><br><a href=/people/n/ngoc-quan-pham/>Ngoc-Quan Pham</a>
|
<a href=/people/j/jan-niehues/>Jan Niehues</a>
|
<a href=/people/a/alex-waibel/>Alexander Waibel</a><br><a href=/volumes/W18-64/ class=text-muted>Proceedings of the Third Conference on Machine Translation: Shared Task Papers</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6422><div class="card-body p-3 small">We present our experiments in the scope of the news translation task in WMT 2018, in directions : EnglishGerman. The core of our systems is the encoder-decoder based neural machine translation models using the transformer architecture. We enhanced the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> with a deeper architecture. By using techniques to limit the memory consumption, we were able to train <a href=https://en.wikipedia.org/wiki/Computer_simulation>models</a> that are 4 times larger on one <a href=https://en.wikipedia.org/wiki/Graphics_processing_unit>GPU</a> and improve the performance by 1.2 BLEU points. Furthermore, we performed sentence selection for the newly available ParaCrawl corpus. Thereby, we could improve the effectiveness of the <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> by 0.5 BLEU points.</div></div><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-3202.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-3202 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-3202 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-3202/>Analyzing Neural MT Search and Model Performance<span class=acl-fixed-case>MT</span> Search and Model Performance</a></strong><br><a href=/people/j/jan-niehues/>Jan Niehues</a>
|
<a href=/people/e/eunah-cho/>Eunah Cho</a>
|
<a href=/people/t/thanh-le-ha/>Thanh-Le Ha</a>
|
<a href=/people/a/alex-waibel/>Alex Waibel</a><br><a href=/volumes/W17-32/ class=text-muted>Proceedings of the First Workshop on Neural Machine Translation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-3202><div class="card-body p-3 small">In this paper, we offer an in-depth analysis about the modeling and search performance. We address the question if a more complex <a href=https://en.wikipedia.org/wiki/Search_algorithm>search algorithm</a> is necessary. Furthermore, we investigate the question if more complex <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> which might only be applicable during rescoring are promising. By separating the <a href=https://en.wikipedia.org/wiki/Feasible_region>search space</a> and the <a href=https://en.wikipedia.org/wiki/Mathematical_model>modeling</a> using n-best list reranking, we analyze the influence of both parts of an NMT system independently. By comparing differently performing NMT systems, we show that the better translation is already in the <a href=https://en.wikipedia.org/wiki/Feasible_region>search space</a> of the translation systems with less performance. This results indicate that the current <a href=https://en.wikipedia.org/wiki/Search_algorithm>search algorithms</a> are sufficient for the <a href=https://en.wikipedia.org/wiki/Network_topology>NMT systems</a>. Furthermore, we could show that even a relatively small n-best list of 50 hypotheses already contain notably better translations.<tex-math>n</tex-math>-best list of 50 hypotheses already contain notably better translations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2017.iwslt-1.6.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2017--iwslt-1--6 data-toggle=collapse aria-expanded=false aria-controls=abstract-2017.iwslt-1.6 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2017.iwslt-1.6/>KIT’s Multilingual Neural Machine Translation systems for IWSLT 2017<span class=acl-fixed-case>KIT</span>’s Multilingual Neural Machine Translation systems for <span class=acl-fixed-case>IWSLT</span> 2017</a></strong><br><a href=/people/n/ngoc-quan-pham/>Ngoc-Quan Pham</a>
|
<a href=/people/m/matthias-sperber/>Matthias Sperber</a>
|
<a href=/people/e/elizabeth-salesky/>Elizabeth Salesky</a>
|
<a href=/people/t/thanh-le-ha/>Thanh-Le Ha</a>
|
<a href=/people/j/jan-niehues/>Jan Niehues</a>
|
<a href=/people/a/alex-waibel/>Alexander Waibel</a><br><a href=/volumes/2017.iwslt-1/ class=text-muted>Proceedings of the 14th International Conference on Spoken Language Translation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2017--iwslt-1--6><div class="card-body p-3 small">In this paper, we present KIT&#8217;s multilingual neural machine translation (NMT) systems for the IWSLT 2017 evaluation campaign machine translation (MT) and spoken language translation (SLT) tasks. For our MT task submissions, we used our multi-task system, modified from a standard attentional neural machine translation framework, instead of building 20 individual NMT systems. We investigated different <a href=https://en.wikipedia.org/wiki/Software_architecture>architectures</a> as well as different <a href=https://en.wikipedia.org/wiki/Data_corpus>data corpora</a> in training such a multilingual system. We also suggested an effective adaptation scheme for <a href=https://en.wikipedia.org/wiki/Multilingualism>multilingual systems</a> which brings great improvements compared to <a href=https://en.wikipedia.org/wiki/Monolingualism>monolingual systems</a>. For the SLT track, in addition to a monolingual neural translation system used to generate correct punctuations and true cases of the data prior to training our multilingual system, we introduced a noise model in order to make our system more robust. Results show that our novel modifications improved our <a href=https://en.wikipedia.org/wiki/System>systems</a> considerably on all <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a>.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Alex+Waibel" title="Search for 'Alex Waibel' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/j/jan-niehues/ class=align-middle>Jan Niehues</a>
<span class="badge badge-secondary align-middle ml-2">8</span></li><li class=list-group-item><a href=/people/s/sebastian-stuker/ class=align-middle>Sebastian Stüker</a>
<span class="badge badge-secondary align-middle ml-2">7</span></li><li class=list-group-item><a href=/people/t/thanh-le-ha/ class=align-middle>Thanh-Le Ha</a>
<span class="badge badge-secondary align-middle ml-2">6</span></li><li class=list-group-item><a href=/people/n/ngoc-quan-pham/ class=align-middle>Ngoc-Quan Pham</a>
<span class="badge badge-secondary align-middle ml-2">5</span></li><li class=list-group-item><a href=/people/m/matthias-sperber/ class=align-middle>Matthias Sperber</a>
<span class="badge badge-secondary align-middle ml-2">5</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/e/elizabeth-salesky/ class=align-middle>Elizabeth Salesky</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/people/m/marcello-federico/ class=align-middle>Marcello Federico</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/t/thai-son-nguyen/ class=align-middle>Thai-Son Nguyen</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/e/eunah-cho/ class=align-middle>Eunah Cho</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/t/tuan-nam-nguyen/ class=align-middle>Tuan-Nam Nguyen</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/o/ondrej-bojar/ class=align-middle>Ondřej Bojar</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/s/satoshi-nakamura/ class=align-middle>Satoshi Nakamura</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/z/zhong-zhou/ class=align-middle>Zhong Zhou</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/f/felix-schneider/ class=align-middle>Felix Schneider</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/m/maciej-modrzejewski/ class=align-middle>Maciej Modrzejewski</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/miriam-exel/ class=align-middle>Miriam Exel</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/b/bianka-buschbeck/ class=align-middle>Bianka Buschbeck</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/f/florian-dessloch/ class=align-middle>Florian Dessloch</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/markus-muller/ class=align-middle>Markus Müller</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/thomas-zenkel/ class=align-middle>Thomas Zenkel</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/w/william-m-campbell/ class=align-middle>William M. Campbell</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/dilek-hakkani-tur/ class=align-middle>Dilek Hakkani-Tur</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/timothy-j-hazen/ class=align-middle>Timothy J. Hazen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/kevin-kilgour/ class=align-middle>Kevin Kilgour</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/v/varun-kumar/ class=align-middle>Varun Kumar</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/hadrien-glaude/ class=align-middle>Hadrien Glaude</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/christian-huber/ class=align-middle>Christian Huber</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/juan-hussain/ class=align-middle>Juan Hussain</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/kaihang-song/ class=align-middle>Kaihang Song</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/marta-r-costa-jussa/ class=align-middle>Marta R. Costa-jussà</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/antonios-anastasopoulos/ class=align-middle>Antonios Anastasopoulos</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jacob-bremerman/ class=align-middle>Jacob Bremerman</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/roldano-cattoni/ class=align-middle>Roldano Cattoni</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/maha-elbayad/ class=align-middle>Maha Elbayad</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/x/xutai-ma/ class=align-middle>Xutai Ma</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/matteo-negri/ class=align-middle>Matteo Negri</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/juan-pino/ class=align-middle>Juan Pino</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/katsuhito-sudoh/ class=align-middle>Katsuhito Sudoh</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/marco-turchi/ class=align-middle>Marco Turchi</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/changhan-wang/ class=align-middle>Changhan Wang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/matthew-wiesner/ class=align-middle>Matthew Wiesner</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/dan-he/ class=align-middle>Dan He</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jorg-franke/ class=align-middle>Jörg Franke</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/dario-franceschini/ class=align-middle>Dario Franceschini</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/chiara-canton/ class=align-middle>Chiara Canton</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/i/ivan-simonini/ class=align-middle>Ivan Simonini</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/armin-schweinfurth/ class=align-middle>Armin Schweinfurth</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/adelheid-glott/ class=align-middle>Adelheid Glott</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/b/barry-haddow/ class=align-middle>Barry Haddow</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/p/philip-williams/ class=align-middle>Philip Williams</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/rico-sennrich/ class=align-middle>Rico Sennrich</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/sangeet-sagar/ class=align-middle>Sangeet Sagar</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/dominik-machacek/ class=align-middle>Dominik Macháček</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/o/otakar-smrz/ class=align-middle>Otakar Smrz</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/g/graham-neubig/ class=align-middle>Graham Neubig</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/kevin-knight/ class=align-middle>Kevin Knight</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/hermann-ney/ class=align-middle>Hermann Ney</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/dekai-wu/ class=align-middle>Dekai Wu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/joseph-mariani/ class=align-middle>Joseph Mariani</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/f/francois-yvon/ class=align-middle>François Yvon</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/iwslt/ class=align-middle>IWSLT</a><span class="badge badge-secondary align-middle ml-2">6</span></li><li class=list-group-item><a href=/venues/ws/ class=align-middle>WS</a><span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/venues/lifelongnlp/ class=align-middle>lifelongnlp</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/acl/ class=align-middle>ACL</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/eamt/ class=align-middle>EAMT</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-venues aria-expanded=false aria-controls=more-venues>show all...</li><div class="collapse border-top" id=more-venues><li class=list-group-item><a href=/venues/coling/ class=align-middle>COLING</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/iwltp/ class=align-middle>IWLTP</a><span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>