<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Alexander Panchenko - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Alexander</span> <span class=font-weight-bold>Panchenko</span></h2><hr><div class=row><div class=col-lg-9><h4>2022</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.acl-long.469.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--acl-long--469 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.acl-long.469 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.acl-long.469" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2022.acl-long.469/>ParaDetox Detoxification with Parallel Data<span class=acl-fixed-case>P</span>ara<span class=acl-fixed-case>D</span>etox: Detoxification with Parallel Data</a></strong><br><a href=/people/v/varvara-logacheva/>Varvara Logacheva</a>
|
<a href=/people/d/daryna-dementieva/>Daryna Dementieva</a>
|
<a href=/people/s/sergey-ustyantsev/>Sergey Ustyantsev</a>
|
<a href=/people/d/daniil-moskovskiy/>Daniil Moskovskiy</a>
|
<a href=/people/d/david-dale/>David Dale</a>
|
<a href=/people/i/irina-krotova/>Irina Krotova</a>
|
<a href=/people/n/nikita-semenov/>Nikita Semenov</a>
|
<a href=/people/a/alexander-panchenko/>Alexander Panchenko</a><br><a href=/volumes/2022.acl-long/ class=text-muted>Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--acl-long--469><div class="card-body p-3 small">We present a novel <a href=https://en.wikipedia.org/wiki/Pipeline_(computing)>pipeline</a> for the collection of <a href=https://en.wikipedia.org/wiki/Parallel_computing>parallel data</a> for the detoxification task We collect non toxic paraphrases for over 10,000 English toxic sentences We also show that this <a href=https://en.wikipedia.org/wiki/Pipeline_transport>pipeline</a> can be used to distill a large existing corpus of paraphrases to get toxic neutral sentence pairs We release two parallel corpora which can be used for the training of <a href=https://en.wikipedia.org/wiki/Detoxification_(alternative_medicine)>detoxification models</a> To the best of our knowledge these are the first parallel datasets for this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> We describe our <a href=https://en.wikipedia.org/wiki/Pipeline_(software)>pipeline</a> in detail to make it fast to set up for a new language or domain thus contributing to faster and easier development of new parallel resources We train several detoxification models on the collected data and compare them with several baselines and state of the art unsupervised approaches We conduct both automatic and manual evaluations All <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> trained on <a href=https://en.wikipedia.org/wiki/Parallel_computing>parallel data</a> outperform the state of the art <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised models</a> by a large margin This suggests that our novel datasets can boost the performance of <a href=https://en.wikipedia.org/wiki/Detoxification_(alternative_medicine)>detoxification systems</a></div></div><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.145.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--145 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.145 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.145/>Active Learning for Sequence Tagging with Deep Pre-trained Models and Bayesian Uncertainty Estimates<span class=acl-fixed-case>B</span>ayesian Uncertainty Estimates</a></strong><br><a href=/people/a/artem-shelmanov/>Artem Shelmanov</a>
|
<a href=/people/d/dmitri-puzyrev/>Dmitri Puzyrev</a>
|
<a href=/people/l/lyubov-kupriyanova/>Lyubov Kupriyanova</a>
|
<a href=/people/d/denis-belyakov/>Denis Belyakov</a>
|
<a href=/people/d/daniil-larionov/>Daniil Larionov</a>
|
<a href=/people/n/nikita-khromov/>Nikita Khromov</a>
|
<a href=/people/o/olga-kozlova/>Olga Kozlova</a>
|
<a href=/people/e/ekaterina-artemova/>Ekaterina Artemova</a>
|
<a href=/people/d/dmitry-v-dylov/>Dmitry V. Dylov</a>
|
<a href=/people/a/alexander-panchenko/>Alexander Panchenko</a><br><a href=/volumes/2021.eacl-main/ class=text-muted>Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--145><div class="card-body p-3 small">Annotating training data for sequence tagging of texts is usually very time-consuming. Recent advances in <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a> for <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a> in conjunction with active learning open the possibility to significantly reduce the necessary annotation budget. We are the first to thoroughly investigate this powerful combination for the sequence tagging task. We conduct an extensive empirical study of various Bayesian uncertainty estimation methods and Monte Carlo dropout options for deep pre-trained models in the active learning framework and find the best combinations for different types of <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a>. Besides, we also demonstrate that to acquire instances during <a href=https://en.wikipedia.org/wiki/Active_learning>active learning</a>, a full-size Transformer can be substituted with a distilled version, which yields better computational performance and reduces obstacles for applying deep active learning in practice.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-demos.36.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-demos--36 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-demos.36 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-demos.36/>Which is Better for <a href=https://en.wikipedia.org/wiki/Deep_learning>Deep Learning</a> : <a href=https://en.wikipedia.org/wiki/Python_(programming_language)>Python</a> or <a href=https://en.wikipedia.org/wiki/MATLAB>MATLAB</a>? Answering Comparative Questions in Natural Language<span class=acl-fixed-case>MATLAB</span>? Answering Comparative Questions in Natural Language</a></strong><br><a href=/people/v/viktoriia-chekalina/>Viktoriia Chekalina</a>
|
<a href=/people/a/alexander-bondarenko/>Alexander Bondarenko</a>
|
<a href=/people/c/chris-biemann/>Chris Biemann</a>
|
<a href=/people/m/meriem-beloucif/>Meriem Beloucif</a>
|
<a href=/people/v/varvara-logacheva/>Varvara Logacheva</a>
|
<a href=/people/a/alexander-panchenko/>Alexander Panchenko</a><br><a href=/volumes/2021.eacl-demos/ class=text-muted>Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: System Demonstrations</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-demos--36><div class="card-body p-3 small">We present a <a href=https://en.wikipedia.org/wiki/System>system</a> for answering comparative questions (Is X better than Y with respect to Z?) in <a href=https://en.wikipedia.org/wiki/Natural_language>natural language</a>. Answering such questions is important for assisting humans in making informed decisions. The key component of our system is a <a href=https://en.wikipedia.org/wiki/Natural-language_user_interface>natural language interface</a> for comparative QA that can be used in <a href=https://en.wikipedia.org/wiki/Personal_assistant>personal assistants</a>, <a href=https://en.wikipedia.org/wiki/Chatbot>chatbots</a>, and similar <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP devices</a>. Comparative QA is a challenging NLP task, since it requires collecting support evidence from many different sources, and direct comparisons of rare objects may be not available even on the entire Web. We take the first step towards a solution for such a task offering a testbed for comparative QA in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language</a> by probing several methods, making the three best ones available as an online demo.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.629.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--629 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.629 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.629" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.629/>Text Detoxification using Large Pre-trained Neural Models</a></strong><br><a href=/people/d/david-dale/>David Dale</a>
|
<a href=/people/a/anton-voronov/>Anton Voronov</a>
|
<a href=/people/d/daryna-dementieva/>Daryna Dementieva</a>
|
<a href=/people/v/varvara-logacheva/>Varvara Logacheva</a>
|
<a href=/people/o/olga-kozlova/>Olga Kozlova</a>
|
<a href=/people/n/nikita-semenov/>Nikita Semenov</a>
|
<a href=/people/a/alexander-panchenko/>Alexander Panchenko</a><br><a href=/volumes/2021.emnlp-main/ class=text-muted>Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--629><div class="card-body p-3 small">We present two novel <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised methods</a> for eliminating toxicity in text. Our first method combines two recent ideas : (1) guidance of the generation process with small style-conditional language models and (2) use of paraphrasing models to perform style transfer. We use a well-performing <a href=https://en.wikipedia.org/wiki/Paraphrase>paraphraser</a> guided by style-trained language models to keep the text content and remove <a href=https://en.wikipedia.org/wiki/Toxicity>toxicity</a>. Our second method uses BERT to replace toxic words with their non-offensive synonyms. We make the <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>method</a> more flexible by enabling BERT to replace mask tokens with a variable number of words. Finally, we present the first large-scale comparative study of style transfer models on the task of toxicity removal. We compare our <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> with a number of <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> for style transfer. The <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> are evaluated in a reference-free way using a combination of unsupervised style transfer metrics. Both <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> we suggest yield new SOTA results.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.bsnlp-1.4.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--bsnlp-1--4 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.bsnlp-1.4 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.bsnlp-1.4/>Detecting Inappropriate Messages on Sensitive Topics that Could Harm a Company’s Reputation</a></strong><br><a href=/people/n/nikolay-babakov/>Nikolay Babakov</a>
|
<a href=/people/v/varvara-logacheva/>Varvara Logacheva</a>
|
<a href=/people/o/olga-kozlova/>Olga Kozlova</a>
|
<a href=/people/n/nikita-semenov/>Nikita Semenov</a>
|
<a href=/people/a/alexander-panchenko/>Alexander Panchenko</a><br><a href=/volumes/2021.bsnlp-1/ class=text-muted>Proceedings of the 8th Workshop on Balto-Slavic Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--bsnlp-1--4><div class="card-body p-3 small">Not all topics are equally flammable in terms of <a href=https://en.wikipedia.org/wiki/Toxicity>toxicity</a> : a calm discussion of turtles or fishing less often fuels inappropriate toxic dialogues than a discussion of politics or <a href=https://en.wikipedia.org/wiki/Sexual_minority>sexual minorities</a>. We define a set of sensitive topics that can yield inappropriate and toxic messages and describe the <a href=https://en.wikipedia.org/wiki/Methodology>methodology</a> of collecting and labelling a <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> for appropriateness. While <a href=https://en.wikipedia.org/wiki/Toxicity>toxicity</a> in user-generated data is well-studied, we aim at defining a more fine-grained notion of <a href=https://en.wikipedia.org/wiki/Inappropriateness>inappropriateness</a>. The core of <a href=https://en.wikipedia.org/wiki/Inappropriateness>inappropriateness</a> is that it can harm the reputation of a speaker. This is different from <a href=https://en.wikipedia.org/wiki/Toxicity>toxicity</a> in two respects : (i) <a href=https://en.wikipedia.org/wiki/Inappropriateness>inappropriateness</a> is topic-related, and (ii) inappropriate message is not toxic but still unacceptable. We collect and release two <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> for <a href=https://en.wikipedia.org/wiki/Russian_language>Russian</a> : a topic-labelled dataset and an appropriateness-labelled dataset. We also release pre-trained classification models trained on this <a href=https://en.wikipedia.org/wiki/Data>data</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.semeval-1.16.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--semeval-1--16 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.semeval-1.16 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.semeval-1.16/>SkoltechNLP at SemEval-2021 Task 2 : Generating Cross-Lingual Training Data for the Word-in-Context Task<span class=acl-fixed-case>S</span>koltech<span class=acl-fixed-case>NLP</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2021 Task 2: Generating Cross-Lingual Training Data for the Word-in-Context Task</a></strong><br><a href=/people/a/anton-razzhigaev/>Anton Razzhigaev</a>
|
<a href=/people/n/nikolay-arefyev/>Nikolay Arefyev</a>
|
<a href=/people/a/alexander-panchenko/>Alexander Panchenko</a><br><a href=/volumes/2021.semeval-1/ class=text-muted>Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--semeval-1--16><div class="card-body p-3 small">In this paper, we present a <a href=https://en.wikipedia.org/wiki/System>system</a> for the solution of the cross-lingual and multilingual word-in-context disambiguation task. Task organizers provided monolingual data in several languages, but no cross-lingual training data were available. To address the lack of the officially provided cross-lingual training data, we decided to generate such <a href=https://en.wikipedia.org/wiki/Data>data</a> ourselves. We describe a simple yet effective approach based on <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> and back translation of the lexical units to the original language used in the context of this shared task. In our experiments, we used a neural system based on the XLM-R, a pre-trained transformer-based masked language model, as a baseline. We show the effectiveness of the proposed approach as it allows to substantially improve the performance of this strong neural baseline model. In addition, in this study, we present multiple types of the XLM-R based classifier, experimenting with various ways of mixing information from the first and second occurrences of the target word in two samples.</div></div><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.textgraphs-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.textgraphs-1.0/>Proceedings of the Graph-based Methods for Natural Language Processing (TextGraphs)</a></strong><br><a href=/people/d/dmitry-ustalov/>Dmitry Ustalov</a>
|
<a href=/people/s/swapna-somasundaran/>Swapna Somasundaran</a>
|
<a href=/people/a/alexander-panchenko/>Alexander Panchenko</a>
|
<a href=/people/f/fragkiskos-d-malliaros/>Fragkiskos D. Malliaros</a>
|
<a href=/people/i/ioana-hulpus/>Ioana Hulpuș</a>
|
<a href=/people/p/peter-jansen/>Peter Jansen</a>
|
<a href=/people/a/abhik-jana/>Abhik Jana</a><br><a href=/volumes/2020.textgraphs-1/ class=text-muted>Proceedings of the Graph-based Methods for Natural Language Processing (TextGraphs)</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.pam-1.13.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--pam-1--13 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.pam-1.13 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.pam-1.13/>Generating Lexical Representations of Frames using Lexical Substitution</a></strong><br><a href=/people/s/saba-anwar/>Saba Anwar</a>
|
<a href=/people/a/artem-shelmanov/>Artem Shelmanov</a>
|
<a href=/people/a/alexander-panchenko/>Alexander Panchenko</a>
|
<a href=/people/c/chris-biemann/>Chris Biemann</a><br><a href=/volumes/2020.pam-1/ class=text-muted>Proceedings of the Probability and Meaning Conference (PaM 2020)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--pam-1--13><div class="card-body p-3 small">Semantic frames are <a href=https://en.wikipedia.org/wiki/Formal_language>formal linguistic structures</a> describing situations / actions / events, e.g. Commercial transfer of goods. Each frame provides a set of <a href=https://en.wikipedia.org/wiki/Character_(arts)>roles</a> corresponding to the situation participants, e.g. Buyer and Goods, and lexical units (LUs) words and phrases that can evoke this particular <a href=https://en.wikipedia.org/wiki/Frame_story>frame</a> in texts, e.g. Sell. The scarcity of annotated resources hinders wider adoption of <a href=https://en.wikipedia.org/wiki/Frame_semantics_(linguistics)>frame semantics</a> across languages and domains. We investigate a simple yet effective method, <a href=https://en.wikipedia.org/wiki/Lexical_substitution>lexical substitution</a> with word representation models, to automatically expand a small set of frame-annotated sentences with new words for their respective roles and LUs. We evaluate the expansion quality using <a href=https://en.wikipedia.org/wiki/FrameNet>FrameNet</a>. Contextualized models demonstrate overall superior performance compared to the non-contextualized ones on roles. However, the latter show comparable performance on the task of LU expansion.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.234.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--234 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.234 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.234/>SkoltechNLP at SemEval-2020 Task 11 : Exploring Unsupervised Text Augmentation for Propaganda Detection<span class=acl-fixed-case>S</span>koltech<span class=acl-fixed-case>NLP</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 11: Exploring Unsupervised Text Augmentation for Propaganda Detection</a></strong><br><a href=/people/d/daryna-dementieva/>Daryna Dementieva</a>
|
<a href=/people/i/igor-markov/>Igor Markov</a>
|
<a href=/people/a/alexander-panchenko/>Alexander Panchenko</a><br><a href=/volumes/2020.semeval-1/ class=text-muted>Proceedings of the Fourteenth Workshop on Semantic Evaluation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--234><div class="card-body p-3 small">This paper presents a solution for the Span Identification (SI) task in the Detection of Propaganda Techniques in News Articles competition at SemEval-2020. The goal of the SI task is to identify specific fragments of each article which contain the use of at least one propaganda technique. This is a binary sequence tagging task. We tested several approaches finally selecting a fine-tuned BERT model as our <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline model</a>. Our main contribution is an investigation of several unsupervised data augmentation techniques based on distributional semantics expanding the original small training dataset as applied to this BERT-based sequence tagger. We explore various expansion strategies and show that they can substantially shift the balance between <a href=https://en.wikipedia.org/wiki/Precision_(computer_science)>precision</a> and <a href=https://en.wikipedia.org/wiki/Recall_(memory)>recall</a>, while maintaining comparable levels of the F1 score.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.knlp-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.knlp-1.0/>Proceedings of Knowledgeable NLP: the First Workshop on Integrating Structured Knowledge and Neural Networks for NLP</a></strong><br><a href=/people/o/oren-sar-shalom/>Oren Sar Shalom</a>
|
<a href=/people/a/alexander-panchenko/>Alexander Panchenko</a>
|
<a href=/people/c/cicero-dos-santos/>Cicero dos Santos</a>
|
<a href=/people/v/varvara-logacheva/>Varvara Logacheva</a>
|
<a href=/people/a/alessandro-moschitti/>Alessandro Moschitti</a>
|
<a href=/people/i/ido-dagan/>Ido Dagan</a><br><a href=/volumes/2020.knlp-1/ class=text-muted>Proceedings of Knowledgeable NLP: the First Workshop on Integrating Structured Knowledge and Neural Networks for NLP</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.728.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--728 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.728 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.728/>Word Sense Disambiguation for 158 Languages using Word Embeddings Only</a></strong><br><a href=/people/v/varvara-logacheva/>Varvara Logacheva</a>
|
<a href=/people/d/denis-teslenko/>Denis Teslenko</a>
|
<a href=/people/a/artem-shelmanov/>Artem Shelmanov</a>
|
<a href=/people/s/steffen-remus/>Steffen Remus</a>
|
<a href=/people/d/dmitry-ustalov/>Dmitry Ustalov</a>
|
<a href=/people/a/andrey-kutuzov/>Andrey Kutuzov</a>
|
<a href=/people/e/ekaterina-artemova/>Ekaterina Artemova</a>
|
<a href=/people/c/chris-biemann/>Chris Biemann</a>
|
<a href=/people/s/simone-paolo-ponzetto/>Simone Paolo Ponzetto</a>
|
<a href=/people/a/alexander-panchenko/>Alexander Panchenko</a><br><a href=/volumes/2020.lrec-1/ class=text-muted>Proceedings of the 12th Language Resources and Evaluation Conference</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--728><div class="card-body p-3 small">Disambiguation of word senses in context is easy for humans, but is a major challenge for automatic approaches. Sophisticated supervised and knowledge-based models were developed to solve this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. However, (i) the inherent <a href=https://en.wikipedia.org/wiki/Zipfian_distribution>Zipfian distribution</a> of supervised training instances for a given word and/or (ii) the quality of linguistic knowledge representations motivate the development of completely unsupervised and knowledge-free approaches to word sense disambiguation (WSD). They are particularly useful for under-resourced languages which do not have any resources for building either supervised and/or knowledge-based models. In this paper, we present a method that takes as input a standard pre-trained word embedding model and induces a fully-fledged word sense inventory, which can be used for disambiguation in context. We use this method to induce a collection of sense inventories for 158 languages on the basis of the original pre-trained fastText word embeddings by Grave et al., (2018), enabling WSD in these <a href=https://en.wikipedia.org/wiki/Language>languages</a>. Models and system are available online.</div></div><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3708.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3708 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3708 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-3708" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-3708/>A Dataset for Noun Compositionality Detection for a <a href=https://en.wikipedia.org/wiki/Slavic_languages>Slavic Language</a><span class=acl-fixed-case>S</span>lavic Language</a></strong><br><a href=/people/d/dmitry-puzyrev/>Dmitry Puzyrev</a>
|
<a href=/people/a/artem-shelmanov/>Artem Shelmanov</a>
|
<a href=/people/a/alexander-panchenko/>Alexander Panchenko</a>
|
<a href=/people/e/ekaterina-artemova/>Ekaterina Artemova</a><br><a href=/volumes/W19-37/ class=text-muted>Proceedings of the 7th Workshop on Balto-Slavic Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3708><div class="card-body p-3 small">This paper presents the first gold-standard resource for <a href=https://en.wikipedia.org/wiki/Russian_language>Russian</a> annotated with compositionality information of noun compounds. The compound phrases are collected from the Universal Dependency treebanks according to part of speech patterns, such as ADJ+NOUN or NOUN+NOUN, using the gold-standard annotations. Each <a href=https://en.wikipedia.org/wiki/Compound_(linguistics)>compound phrase</a> is annotated by two experts and a moderator according to the following schema : the phrase can be either compositional, non-compositional, or ambiguous (i.e., depending on the context it can be interpreted both as compositional or non-compositional). We conduct an experimental evaluation of <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> and <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> for predicting compositionality of noun compounds in unsupervised and supervised setups. We show that <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> from previous work evaluated on the proposed Russian-language resource achieve the performance comparable with results on <a href=https://en.wikipedia.org/wiki/English_language>English corpora</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4516.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4516 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4516 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-4516" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-4516/>Categorizing Comparative Sentences</a></strong><br><a href=/people/a/alexander-panchenko/>Alexander Panchenko</a>
|
<a href=/people/a/alexander-bondarenko/>Alexander Bondarenko</a>
|
<a href=/people/m/mirco-franzek/>Mirco Franzek</a>
|
<a href=/people/m/matthias-hagen/>Matthias Hagen</a>
|
<a href=/people/c/chris-biemann/>Chris Biemann</a><br><a href=/volumes/W19-45/ class=text-muted>Proceedings of the 6th Workshop on Argument Mining</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4516><div class="card-body p-3 small">We tackle the tasks of automatically identifying comparative sentences and categorizing the intended preference (e.g., <a href=https://en.wikipedia.org/wiki/Python_(programming_language)>Python</a> has better NLP libraries than <a href=https://en.wikipedia.org/wiki/MATLAB>MATLAB Python</a>, better, <a href=https://en.wikipedia.org/wiki/MATLAB>MATLAB</a>). To this end, we manually annotate 7,199 sentences for 217 distinct target item pairs from several domains (27 % of the sentences contain an oriented comparison in the sense of better or worse). A gradient boosting model based on pre-trained sentence embeddings reaches an F1 score of 85 % in our experimental evaluation. The model can be used to extract comparative sentences for pro / con argumentation in comparative / argument search engines or debating technologies.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1316.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1316 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1316 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-1316/>On the Compositionality Prediction of Noun Phrases using Poincar Embeddings</a></strong><br><a href=/people/a/abhik-jana/>Abhik Jana</a>
|
<a href=/people/d/dima-puzyrev/>Dima Puzyrev</a>
|
<a href=/people/a/alexander-panchenko/>Alexander Panchenko</a>
|
<a href=/people/p/pawan-goyal/>Pawan Goyal</a>
|
<a href=/people/c/chris-biemann/>Chris Biemann</a>
|
<a href=/people/a/animesh-mukherjee/>Animesh Mukherjee</a><br><a href=/volumes/P19-1/ class=text-muted>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1316><div class="card-body p-3 small">The compositionality degree of multiword expressions indicates to what extent the meaning of a phrase can be derived from the meaning of its constituents and their <a href=https://en.wikipedia.org/wiki/Grammatical_relation>grammatical relations</a>. Prediction of (non)-compositionality is a task that has been frequently addressed with distributional semantic models. We introduce a novel technique to blend hierarchical information with <a href=https://en.wikipedia.org/wiki/Distribution_(mathematics)>distributional information</a> for predicting compositionality. In particular, we use hypernymy information of the multiword and its constituents encoded in the form of the recently introduced Poincar embeddings in addition to the distributional information to detect compositionality for <a href=https://en.wikipedia.org/wiki/Noun_phrase>noun phrases</a>. Using a <a href=https://en.wikipedia.org/wiki/Weighted_arithmetic_mean>weighted average</a> of the distributional similarity and a Poincar similarity function, we obtain consistent and substantial, statistically significant improvement across three gold standard datasets over state-of-the-art models based on distributional information only. Unlike traditional approaches that solely use an <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised setting</a>, we have also framed the <a href=https://en.wikipedia.org/wiki/Problem_solving>problem</a> as a <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised task</a>, obtaining comparable improvements. Further, we publicly release our Poincar embeddings, which are trained on the output of handcrafted lexical-syntactic patterns on a large corpus.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1325.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1325 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1325 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1325" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-1325/>Making Fast Graph-based Algorithms with Graph Metric Embeddings</a></strong><br><a href=/people/a/andrey-kutuzov/>Andrey Kutuzov</a>
|
<a href=/people/m/mohammad-dorgham/>Mohammad Dorgham</a>
|
<a href=/people/o/oleksiy-oliynyk/>Oleksiy Oliynyk</a>
|
<a href=/people/c/chris-biemann/>Chris Biemann</a>
|
<a href=/people/a/alexander-panchenko/>Alexander Panchenko</a><br><a href=/volumes/P19-1/ class=text-muted>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1325><div class="card-body p-3 small">Graph measures, such as node distances, are inefficient to compute. We explore dense vector representations as an effective way to approximate the same information. We introduce a simple yet efficient and effective approach for learning <a href=https://en.wikipedia.org/wiki/Graph_embedding>graph embeddings</a>. Instead of directly operating on the <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph structure</a>, our method takes structural measures of pairwise node similarities into account and learns dense node representations reflecting user-defined graph distance measures, such as e.g. the shortest path distance or distance measures that take information beyond the <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph structure</a> into account. We demonstrate a speed-up of several orders of magnitude when predicting word similarity by vector operations on our embeddings as opposed to directly computing the respective path-based measures, while outperforming various other graph embeddings on semantic similarity and word sense disambiguation tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-2044.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-2044 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-2044 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-2044/>Improving Neural Entity Disambiguation with Graph Embeddings</a></strong><br><a href=/people/o/ozge-sevgili/>Özge Sevgili</a>
|
<a href=/people/a/alexander-panchenko/>Alexander Panchenko</a>
|
<a href=/people/c/chris-biemann/>Chris Biemann</a><br><a href=/volumes/P19-2/ class=text-muted>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-2044><div class="card-body p-3 small">Entity Disambiguation (ED) is the task of linking an ambiguous entity mention to a corresponding entry in a <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge base</a>. Current methods have mostly focused on unstructured text data to learn representations of entities, however, there is structured information in the knowledge base itself that should be useful to disambiguate entities. In this work, we propose a method that uses <a href=https://en.wikipedia.org/wiki/Graph_embedding>graph embeddings</a> for integrating structured information from the <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge base</a> with <a href=https://en.wikipedia.org/wiki/Unstructured_data>unstructured information</a> from text-based representations. Our experiments confirm that graph embeddings trained on a graph of hyperlinks between Wikipedia articles improve the performances of simple feed-forward neural ED model and a state-of-the-art neural ED system.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-3031.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-3031 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-3031 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-3031" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-3031/>TARGER : Neural Argument Mining at Your Fingertips<span class=acl-fixed-case>TARGER</span>: Neural Argument Mining at Your Fingertips</a></strong><br><a href=/people/a/artem-chernodub/>Artem Chernodub</a>
|
<a href=/people/o/oleksiy-oliynyk/>Oleksiy Oliynyk</a>
|
<a href=/people/p/philipp-heidenreich/>Philipp Heidenreich</a>
|
<a href=/people/a/alexander-bondarenko/>Alexander Bondarenko</a>
|
<a href=/people/m/matthias-hagen/>Matthias Hagen</a>
|
<a href=/people/c/chris-biemann/>Chris Biemann</a>
|
<a href=/people/a/alexander-panchenko/>Alexander Panchenko</a><br><a href=/volumes/P19-3/ class=text-muted>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: System Demonstrations</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-3031><div class="card-body p-3 small">We present TARGER, an open source neural argument mining framework for tagging arguments in free input texts and for keyword-based retrieval of arguments from an argument-tagged web-scale corpus. The currently available models are pre-trained on three recent argument mining datasets and enable the use of neural argument mining without any reproducibility effort on the user&#8217;s side. The open source code ensures portability to other domains and use cases.</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2010.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2010 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2010 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2010.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2010.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-2010" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P18-2010/>Unsupervised Semantic Frame Induction using Triclustering</a></strong><br><a href=/people/d/dmitry-ustalov/>Dmitry Ustalov</a>
|
<a href=/people/a/alexander-panchenko/>Alexander Panchenko</a>
|
<a href=/people/a/andrey-kutuzov/>Andrey Kutuzov</a>
|
<a href=/people/c/chris-biemann/>Chris Biemann</a>
|
<a href=/people/s/simone-paolo-ponzetto/>Simone Paolo Ponzetto</a><br><a href=/volumes/P18-2/ class=text-muted>Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2010><div class="card-body p-3 small">We use dependency triples automatically extracted from a Web-scale corpus to perform unsupervised semantic frame induction. We cast the frame induction problem as a triclustering problem that is a generalization of <a href=https://en.wikipedia.org/wiki/Cluster_analysis>clustering</a> for triadic data. Our replicable benchmarks demonstrate that the proposed graph-based approach, Triframes, shows state-of-the art results on this task on a FrameNet-derived dataset and performing on par with competitive methods on a verb class clustering task.</div></div><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1145.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1145 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1145 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P17-1145" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P17-1145/>Watset : Automatic Induction of Synsets from a Graph of Synonyms<span class=acl-fixed-case>W</span>atset: Automatic Induction of Synsets from a Graph of Synonyms</a></strong><br><a href=/people/d/dmitry-ustalov/>Dmitry Ustalov</a>
|
<a href=/people/a/alexander-panchenko/>Alexander Panchenko</a>
|
<a href=/people/c/chris-biemann/>Chris Biemann</a><br><a href=/volumes/P17-1/ class=text-muted>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1145><div class="card-body p-3 small">This paper presents a new graph-based approach that induces synsets using <a href=https://en.wikipedia.org/wiki/Synonym>synonymy dictionaries</a> and <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a>. First, we build a weighted graph of synonyms extracted from commonly available resources, such as <a href=https://en.wikipedia.org/wiki/Wiktionary>Wiktionary</a>. Second, we apply <a href=https://en.wikipedia.org/wiki/Word-sense_induction>word sense induction</a> to deal with ambiguous words. Finally, we cluster the disambiguated version of the ambiguous input graph into <a href=https://en.wikipedia.org/wiki/Synset>synsets</a>. Our meta-clustering approach lets us use an efficient hard clustering algorithm to perform a fuzzy clustering of the graph. Despite its simplicity, our approach shows excellent results, outperforming five competitive state-of-the-art methods in terms of <a href=https://en.wikipedia.org/wiki/F-score>F-score</a> on three gold standard datasets for <a href=https://en.wikipedia.org/wiki/English_language>English</a> and <a href=https://en.wikipedia.org/wiki/Russian_language>Russian</a> derived from large-scale manually constructed lexical resources.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-2016.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-2016 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-2016 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D17-2016" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D17-2016/>Unsupervised, Knowledge-Free, and Interpretable Word Sense Disambiguation</a></strong><br><a href=/people/a/alexander-panchenko/>Alexander Panchenko</a>
|
<a href=/people/f/fide-marten/>Fide Marten</a>
|
<a href=/people/e/eugen-ruppert/>Eugen Ruppert</a>
|
<a href=/people/s/stefano-faralli/>Stefano Faralli</a>
|
<a href=/people/d/dmitry-ustalov/>Dmitry Ustalov</a>
|
<a href=/people/s/simone-paolo-ponzetto/>Simone Paolo Ponzetto</a>
|
<a href=/people/c/chris-biemann/>Chris Biemann</a><br><a href=/volumes/D17-2/ class=text-muted>Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-2016><div class="card-body p-3 small">Interpretability of a <a href=https://en.wikipedia.org/wiki/Predictive_modelling>predictive model</a> is a powerful feature that gains the trust of users in the correctness of the predictions. In word sense disambiguation (WSD), knowledge-based systems tend to be much more interpretable than knowledge-free counterparts as they rely on the wealth of manually-encoded elements representing word senses, such as hypernyms, usage examples, and images. We present a WSD system that bridges the gap between these two so far disconnected groups of methods. Namely, our <a href=https://en.wikipedia.org/wiki/System>system</a>, providing access to several state-of-the-art WSD models, aims to be interpretable as a knowledge-based system while it remains completely unsupervised and knowledge-free. The presented tool features a Web interface for all-word disambiguation of texts that makes the sense predictions human readable by providing interpretable word sense inventories, sense representations, and disambiguation results. We provide a public API, enabling seamless integration.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1009.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1009 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1009 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1009/>Unsupervised Does Not Mean Uninterpretable : The Case for Word Sense Induction and Disambiguation</a></strong><br><a href=/people/a/alexander-panchenko/>Alexander Panchenko</a>
|
<a href=/people/e/eugen-ruppert/>Eugen Ruppert</a>
|
<a href=/people/s/stefano-faralli/>Stefano Faralli</a>
|
<a href=/people/s/simone-paolo-ponzetto/>Simone Paolo Ponzetto</a>
|
<a href=/people/c/chris-biemann/>Chris Biemann</a><br><a href=/volumes/E17-1/ class=text-muted>Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1009><div class="card-body p-3 small">The current trend in <a href=https://en.wikipedia.org/wiki/Neuro-linguistic_programming>NLP</a> is the use of highly opaque models, e.g. neural networks and <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a>. While these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> yield state-of-the-art results on a range of <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a>, their drawback is poor interpretability. On the example of word sense induction and disambiguation (WSID), we show that it is possible to develop an interpretable <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> that matches the state-of-the-art models in <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>. Namely, we present an unsupervised, knowledge-free WSID approach, which is interpretable at three levels : word sense inventory, sense feature representations, and disambiguation procedure. Experiments show that our model performs on par with state-of-the-art word sense embeddings and other <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised systems</a> while offering the possibility to justify its decisions in <a href=https://en.wikipedia.org/wiki/Human-readable_medium>human-readable form</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1056.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1056 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1056 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1056/>The ContrastMedium Algorithm : Taxonomy Induction From Noisy Knowledge Graphs With Just A Few Links<span class=acl-fixed-case>C</span>ontrast<span class=acl-fixed-case>M</span>edium Algorithm: Taxonomy Induction From Noisy Knowledge Graphs With Just A Few Links</a></strong><br><a href=/people/s/stefano-faralli/>Stefano Faralli</a>
|
<a href=/people/a/alexander-panchenko/>Alexander Panchenko</a>
|
<a href=/people/c/chris-biemann/>Chris Biemann</a>
|
<a href=/people/s/simone-paolo-ponzetto/>Simone Paolo Ponzetto</a><br><a href=/volumes/E17-1/ class=text-muted>Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1056><div class="card-body p-3 small">In this paper, we present ContrastMedium, an <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> that transforms noisy semantic networks into full-fledged, clean taxonomies. ContrastMedium is able to identify the embedded taxonomy structure from a noisy knowledge graph without explicit human supervision such as, for instance, a set of manually selected input root and leaf concepts. This is achieved by leveraging structural information from a companion reference taxonomy, to which the input <a href=https://en.wikipedia.org/wiki/Knowledge_graph>knowledge graph</a> is linked (either automatically or manually). When used in conjunction with methods for hypernym acquisition and knowledge base linking, our methodology provides a complete solution for end-to-end taxonomy induction. We conduct experiments using automatically acquired knowledge graphs, as well as a SemEval benchmark, and show that our method is able to achieve high performance on the task of taxonomy induction.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2087.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2087 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2087 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=E17-2087" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/E17-2087/>Negative Sampling Improves Hypernymy Extraction Based on Projection Learning</a></strong><br><a href=/people/d/dmitry-ustalov/>Dmitry Ustalov</a>
|
<a href=/people/n/nikolay-arefyev/>Nikolay Arefyev</a>
|
<a href=/people/c/chris-biemann/>Chris Biemann</a>
|
<a href=/people/a/alexander-panchenko/>Alexander Panchenko</a><br><a href=/volumes/E17-2/ class=text-muted>Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2087><div class="card-body p-3 small">We present a new approach to extraction of hypernyms based on projection learning and <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a>. In contrast to <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification-based approaches</a>, <a href=https://en.wikipedia.org/wiki/Projection_(linear_algebra)>projection-based methods</a> require no candidate <a href=https://en.wikipedia.org/wiki/Hyponymy_and_hypernymy>hyponym-hypernym pairs</a>. While it is natural to use both positive and negative training examples in supervised relation extraction, the impact of positive examples on hypernym prediction was not studied so far. In this paper, we show that explicit negative examples used for <a href=https://en.wikipedia.org/wiki/Regularization_(mathematics)>regularization</a> of the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> significantly improve performance compared to the state-of-the-art approach of Fu et al. (2014) on three <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> from different languages.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Alexander+Panchenko" title="Search for 'Alexander Panchenko' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/c/chris-biemann/ class=align-middle>Chris Biemann</a>
<span class="badge badge-secondary align-middle ml-2">14</span></li><li class=list-group-item><a href=/people/d/dmitry-ustalov/ class=align-middle>Dmitry Ustalov</a>
<span class="badge badge-secondary align-middle ml-2">6</span></li><li class=list-group-item><a href=/people/v/varvara-logacheva/ class=align-middle>Varvara Logacheva</a>
<span class="badge badge-secondary align-middle ml-2">6</span></li><li class=list-group-item><a href=/people/s/simone-paolo-ponzetto/ class=align-middle>Simone Paolo Ponzetto</a>
<span class="badge badge-secondary align-middle ml-2">5</span></li><li class=list-group-item><a href=/people/a/artem-shelmanov/ class=align-middle>Artem Shelmanov</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/o/olga-kozlova/ class=align-middle>Olga Kozlova</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/e/ekaterina-artemova/ class=align-middle>Ekaterina Artemova</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/a/alexander-bondarenko/ class=align-middle>Alexander Bondarenko</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/d/daryna-dementieva/ class=align-middle>Daryna Dementieva</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/n/nikita-semenov/ class=align-middle>Nikita Semenov</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/s/stefano-faralli/ class=align-middle>Stefano Faralli</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/a/andrey-kutuzov/ class=align-middle>Andrey Kutuzov</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/a/abhik-jana/ class=align-middle>Abhik Jana</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/d/david-dale/ class=align-middle>David Dale</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/e/eugen-ruppert/ class=align-middle>Eugen Ruppert</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/m/matthias-hagen/ class=align-middle>Matthias Hagen</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/n/nikolay-arefyev/ class=align-middle>Nikolay Arefyev</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/o/oleksiy-oliynyk/ class=align-middle>Oleksiy Oliynyk</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/s/swapna-somasundaran/ class=align-middle>Swapna Somasundaran</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/f/fragkiskos-d-malliaros/ class=align-middle>Fragkiskos D. Malliaros</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/i/ioana-hulpus/ class=align-middle>Ioana Hulpuș</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/p/peter-jansen/ class=align-middle>Peter Jansen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/saba-anwar/ class=align-middle>Saba Anwar</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/dmitri-puzyrev/ class=align-middle>Dmitri Puzyrev</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/lyubov-kupriyanova/ class=align-middle>Lyubov Kupriyanova</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/denis-belyakov/ class=align-middle>Denis Belyakov</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/daniil-larionov/ class=align-middle>Daniil Larionov</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/n/nikita-khromov/ class=align-middle>Nikita Khromov</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/dmitry-v-dylov/ class=align-middle>Dmitry V. Dylov</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/v/viktoriia-chekalina/ class=align-middle>Viktoriia Chekalina</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/meriem-beloucif/ class=align-middle>Meriem Beloucif</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/sergey-ustyantsev/ class=align-middle>Sergey Ustyantsev</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/daniil-moskovskiy/ class=align-middle>Daniil Moskovskiy</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/i/irina-krotova/ class=align-middle>Irina Krotova</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/anton-voronov/ class=align-middle>Anton Voronov</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/i/igor-markov/ class=align-middle>Igor Markov</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/o/oren-sar-shalom/ class=align-middle>Oren Sar Shalom</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/cicero-dos-santos/ class=align-middle>Cicero dos Santos</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/alessandro-moschitti/ class=align-middle>Alessandro Moschitti</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/i/ido-dagan/ class=align-middle>Ido Dagan</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/f/fide-marten/ class=align-middle>Fide Marten</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/n/nikolay-babakov/ class=align-middle>Nikolay Babakov</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/dmitry-puzyrev/ class=align-middle>Dmitry Puzyrev</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/mirco-franzek/ class=align-middle>Mirco Franzek</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/denis-teslenko/ class=align-middle>Denis Teslenko</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/steffen-remus/ class=align-middle>Steffen Remus</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/anton-razzhigaev/ class=align-middle>Anton Razzhigaev</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/dima-puzyrev/ class=align-middle>Dima Puzyrev</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/p/pawan-goyal/ class=align-middle>Pawan Goyal</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/animesh-mukherjee/ class=align-middle>Animesh Mukherjee</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/mohammad-dorgham/ class=align-middle>Mohammad Dorgham</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/o/ozge-sevgili/ class=align-middle>Özge Sevgili</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/artem-chernodub/ class=align-middle>Artem Chernodub</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/p/philipp-heidenreich/ class=align-middle>Philipp Heidenreich</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/acl/ class=align-middle>ACL</a><span class="badge badge-secondary align-middle ml-2">7</span></li><li class=list-group-item><a href=/venues/eacl/ class=align-middle>EACL</a><span class="badge badge-secondary align-middle ml-2">5</span></li><li class=list-group-item><a href=/venues/emnlp/ class=align-middle>EMNLP</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/semeval/ class=align-middle>SemEval</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/ws/ class=align-middle>WS</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-venues aria-expanded=false aria-controls=more-venues>show all...</li><div class="collapse border-top" id=more-venues><li class=list-group-item><a href=/venues/textgraphs/ class=align-middle>TextGraphs</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/pam/ class=align-middle>PaM</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/knlp/ class=align-middle>knlp</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/bsnlp/ class=align-middle>BSNLP</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/lrec/ class=align-middle>LREC</a><span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>