<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Andrew McCallum - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Andrew</span> <span class=font-weight-bold>McCallum</span></h2><hr><div class=row><div class=col-lg-9><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.160.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--160 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.160 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.160" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.160/>Modeling Fine-Grained Entity Types with Box Embeddings</a></strong><br><a href=/people/y/yasumasa-onoe/>Yasumasa Onoe</a>
|
<a href=/people/m/michael-boratko/>Michael Boratko</a>
|
<a href=/people/a/andrew-mccallum/>Andrew McCallum</a>
|
<a href=/people/g/greg-durrett/>Greg Durrett</a><br><a href=/volumes/2021.acl-long/ class=text-muted>Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--160><div class="card-body p-3 small">Neural entity typing models typically represent fine-grained entity types as vectors in a high-dimensional space, but such spaces are not well-suited to modeling these types&#8217; complex interdependencies. We study the ability of box embeddings, which embed concepts as d-dimensional hyperrectangles, to capture hierarchies of types even when these relationships are not defined explicitly in the <a href=https://en.wikipedia.org/wiki/Ontology_(information_science)>ontology</a>. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> represents both types and entity mentions as boxes. Each mention and its context are fed into a BERT-based model to embed that mention in our box space ; essentially, this <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> leverages typological clues present in the surface text to hypothesize a type representation for the mention. Box containment can then be used to derive both the <a href=https://en.wikipedia.org/wiki/Posterior_probability>posterior probability</a> of a mention exhibiting a given type and the conditional probability relations between types themselves. We compare our approach with a vector-based typing model and observe state-of-the-art performance on several entity typing benchmarks. In addition to competitive typing performance, our box-based model shows better performance in prediction consistency (predicting a supertype and a subtype together) and <a href=https://en.wikipedia.org/wiki/Confidence>confidence</a> (i.e., calibration), demonstrating that the box-based model captures the latent type hierarchies better than the vector-based model does.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.349.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--349 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.349 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.349" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.349/>Energy-Based Reranking : Improving Neural Machine Translation Using Energy-Based Models</a></strong><br><a href=/people/s/sumanta-bhattacharyya/>Sumanta Bhattacharyya</a>
|
<a href=/people/a/amirmohammad-rooshenas/>Amirmohammad Rooshenas</a>
|
<a href=/people/s/subhajit-naskar/>Subhajit Naskar</a>
|
<a href=/people/s/simeng-sun/>Simeng Sun</a>
|
<a href=/people/m/mohit-iyyer/>Mohit Iyyer</a>
|
<a href=/people/a/andrew-mccallum/>Andrew McCallum</a><br><a href=/volumes/2021.acl-long/ class=text-muted>Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--349><div class="card-body p-3 small">The discrepancy between <a href=https://en.wikipedia.org/wiki/Maximum_likelihood_estimation>maximum likelihood estimation (MLE)</a> and task measures such as BLEU score has been studied before for autoregressive neural machine translation (NMT) and resulted in alternative training algorithms (Ranzato et al., 2016 ; Norouzi et al., 2016 ; Shen et al., 2016 ; Wu et al., 2018). However, MLE training remains the de facto approach for autoregressive NMT because of its computational efficiency and <a href=https://en.wikipedia.org/wiki/Stability_theory>stability</a>. Despite this mismatch between the training objective and task measure, we notice that the samples drawn from an MLE-based trained NMT support the desired distribution there are samples with much higher BLEU score comparing to the beam decoding output. To benefit from this observation, we train an energy-based model to mimic the behavior of the task measure (i.e., the energy-based model assigns lower energy to samples with higher BLEU score), which is resulted in a <a href=https://en.wikipedia.org/wiki/Ranking_(statistics)>re-ranking algorithm</a> based on the samples drawn from NMT : energy-based re-ranking (EBR). We use both marginal energy models (over target sentence) and joint energy models (over both source and target sentences). Our EBR with the joint energy model consistently improves the performance of the Transformer-based NMT : +3.7 BLEU points on IWSLT&#8217;14 German-English, +3.37 BELU points on Sinhala-English, +1.4 BLEU points on WMT&#8217;16 English-German tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.364.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--364 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.364 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.364" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.364/>Benchmarking Scalable Methods for Streaming Cross Document Entity Coreference</a></strong><br><a href=/people/r/robert-l-logan-iv/>Robert L Logan IV</a>
|
<a href=/people/a/andrew-mccallum/>Andrew McCallum</a>
|
<a href=/people/s/sameer-singh/>Sameer Singh</a>
|
<a href=/people/d/dan-bikel/>Dan Bikel</a><br><a href=/volumes/2021.acl-long/ class=text-muted>Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--364><div class="card-body p-3 small">Streaming cross document entity coreference (CDC) systems disambiguate mentions of named entities in a scalable manner via incremental clustering. Unlike other approaches for named entity disambiguation (e.g., entity linking), streaming CDC allows for the disambiguation of entities that are unknown at inference time. Thus, <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> is well-suited for processing <a href=https://en.wikipedia.org/wiki/Stream_(computing)>streams of data</a> where new entities are frequently introduced. Despite these benefits, this task is currently difficult to study, as existing approaches are either evaluated on datasets that are no longer available, or omit other crucial details needed to ensure fair comparison. In this work, we address this issue by compiling a large benchmark adapted from existing free datasets, and performing a comprehensive evaluation of a number of novel and existing baseline models. We investigate : how to best encode <a href=https://en.wikipedia.org/wiki/Note_(typography)>mentions</a>, which clustering algorithms are most effective for grouping mentions, how models transfer to different domains, and how bounding the number of mentions tracked during <a href=https://en.wikipedia.org/wiki/Statistical_inference>inference</a> impacts performance. Our results show that the relative performance of neural and feature-based mention encoders varies across different domains, and in most cases the best performance is achieved using a combination of both approaches. We also find that performance is minimally impacted by limiting the number of tracked mentions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-srw.7.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-srw--7 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-srw.7 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.acl-srw.7.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2021.acl-srw.7/>Long Document Summarization in a Low Resource Setting using Pretrained Language Models</a></strong><br><a href=/people/a/ahsaas-bajaj/>Ahsaas Bajaj</a>
|
<a href=/people/p/pavitra-dangati/>Pavitra Dangati</a>
|
<a href=/people/k/kalpesh-krishna/>Kalpesh Krishna</a>
|
<a href=/people/p/pradhiksha-ashok-kumar/>Pradhiksha Ashok Kumar</a>
|
<a href=/people/r/rheeya-uppaal/>Rheeya Uppaal</a>
|
<a href=/people/b/bradford-windsor/>Bradford Windsor</a>
|
<a href=/people/e/eliot-brenner/>Eliot Brenner</a>
|
<a href=/people/d/dominic-dotterrer/>Dominic Dotterrer</a>
|
<a href=/people/r/rajarshi-das/>Rajarshi Das</a>
|
<a href=/people/a/andrew-mccallum/>Andrew McCallum</a><br><a href=/volumes/2021.acl-srw/ class=text-muted>Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: Student Research Workshop</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-srw--7><div class="card-body p-3 small">Abstractive summarization is the task of compressing a long document into a coherent short document while retaining <a href=https://en.wikipedia.org/wiki/Salience_(neuroscience)>salient information</a>. Modern abstractive summarization methods are based on <a href=https://en.wikipedia.org/wiki/Deep_learning>deep neural networks</a> which often require large training datasets. Since collecting summarization datasets is an expensive and time-consuming task, practical industrial settings are usually low-resource. In this paper, we study a challenging low-resource setting of summarizing long legal briefs with an average source document length of 4268 words and only 120 available (document, summary) pairs. To account for data scarcity, we used a modern pre-trained abstractive summarizer BART, which only achieves 17.9 ROUGE-L as it struggles with long documents. We thus attempt to compress these long documents by identifying salient sentences in the source which best ground the summary, using a novel <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> based on GPT-2 language model perplexity scores, that operates within the low resource regime. On feeding the compressed documents to <a href=https://en.wikipedia.org/wiki/Bay_Area_Rapid_Transit>BART</a>, we observe a 6.0 ROUGE-L improvement. Our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> also beats several competitive salience detection baselines. Furthermore, the identified salient sentences tend to agree with independent human labeling by domain experts.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.395.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--395 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.395 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.395" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.395/>Improved Latent Tree Induction with Distant Supervision via Span Constraints</a></strong><br><a href=/people/z/zhiyang-xu/>Zhiyang Xu</a>
|
<a href=/people/a/andrew-drozdov/>Andrew Drozdov</a>
|
<a href=/people/j/jay-yoon-lee/>Jay Yoon Lee</a>
|
<a href=/people/t/tim-ogorman/>Tim O’Gorman</a>
|
<a href=/people/s/subendhu-rongali/>Subendhu Rongali</a>
|
<a href=/people/d/dylan-finkbeiner/>Dylan Finkbeiner</a>
|
<a href=/people/s/shilpa-suresh/>Shilpa Suresh</a>
|
<a href=/people/m/mohit-iyyer/>Mohit Iyyer</a>
|
<a href=/people/a/andrew-mccallum/>Andrew McCallum</a><br><a href=/volumes/2021.emnlp-main/ class=text-muted>Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--395><div class="card-body p-3 small">For over thirty years, researchers have developed and analyzed <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> for latent tree induction as an approach for unsupervised syntactic parsing. Nonetheless, modern <a href=https://en.wikipedia.org/wiki/System>systems</a> still do not perform well enough compared to their supervised counterparts to have any practical use as structural annotation of text. In this work, we present a technique that uses distant supervision in the form of <a href=https://en.wikipedia.org/wiki/Constraint_(mathematics)>span constraints</a> (i.e. phrase bracketing) to improve performance in unsupervised constituency parsing. Using a relatively small number of span constraints we can substantially improve the output from DIORA, an already competitive unsupervised parsing system. Compared with full parse tree annotation, span constraints can be acquired with minimal effort, such as with a lexicon derived from <a href=https://en.wikipedia.org/wiki/Wikipedia>Wikipedia</a>, to find exact text matches. Our experiments show span constraints based on entities improves constituency parsing on English WSJ Penn Treebank by more than 5 F1. Furthermore, our method extends to any domain where span constraints are easily attainable, and as a case study we demonstrate its effectiveness by parsing biomedical text from the CRAFT dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.755.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--755 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.755 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.755/>Case-based Reasoning for Natural Language Queries over Knowledge Bases</a></strong><br><a href=/people/r/rajarshi-das/>Rajarshi Das</a>
|
<a href=/people/m/manzil-zaheer/>Manzil Zaheer</a>
|
<a href=/people/d/dung-thai/>Dung Thai</a>
|
<a href=/people/a/ameya-godbole/>Ameya Godbole</a>
|
<a href=/people/e/ethan-perez/>Ethan Perez</a>
|
<a href=/people/j/jay-yoon-lee/>Jay Yoon Lee</a>
|
<a href=/people/l/lizhen-tan/>Lizhen Tan</a>
|
<a href=/people/l/lazaros-polymenakos/>Lazaros Polymenakos</a>
|
<a href=/people/a/andrew-mccallum/>Andrew McCallum</a><br><a href=/volumes/2021.emnlp-main/ class=text-muted>Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--755><div class="card-body p-3 small">It is often challenging to solve a complex problem from scratch, but much easier if we can access other similar problems with their solutions a <a href=https://en.wikipedia.org/wiki/Paradigm>paradigm</a> known as case-based reasoning (CBR). We propose a neuro-symbolic CBR approach (CBR-KBQA) for <a href=https://en.wikipedia.org/wiki/Question_answering>question answering</a> over large knowledge bases. CBR-KBQA consists of a nonparametric memory that stores cases (question and logical forms) and a <a href=https://en.wikipedia.org/wiki/Parametric_model>parametric model</a> that can generate a <a href=https://en.wikipedia.org/wiki/Logical_form>logical form</a> for a new question by retrieving cases that are relevant to it. On several KBQA datasets that contain complex questions, CBR-KBQA achieves competitive performance. For example, on the CWQ dataset, CBR-KBQA outperforms the current state of the art by 11 % on <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>. Furthermore, we show that CBR-KBQA is capable of using new cases without any further training : by incorporating a few human-labeled examples in the case memory, CBR-KBQA is able to successfully generate logical forms containing unseen KB entities as well as relations.<i>without</i> any further training: by incorporating a few human-labeled examples in the case memory, CBR-KBQA is able to successfully generate logical forms containing unseen KB entities as well as relations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.68.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--68 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.68 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.naacl-main.68" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.68/>Probabilistic Box Embeddings for Uncertain Knowledge Graph Reasoning</a></strong><br><a href=/people/x/xuelu-chen/>Xuelu Chen</a>
|
<a href=/people/m/michael-boratko/>Michael Boratko</a>
|
<a href=/people/m/muhao-chen/>Muhao Chen</a>
|
<a href=/people/s/shib-sankar-dasgupta/>Shib Sankar Dasgupta</a>
|
<a href=/people/x/xiang-lorraine-li/>Xiang Lorraine Li</a>
|
<a href=/people/a/andrew-mccallum/>Andrew McCallum</a><br><a href=/volumes/2021.naacl-main/ class=text-muted>Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--68><div class="card-body p-3 small">Knowledge bases often consist of facts which are harvested from a variety of sources, many of which are noisy and some of which conflict, resulting in a level of uncertainty for each triple. Knowledge bases are also often incomplete, prompting the use of <a href=https://en.wikipedia.org/wiki/Embedding>embedding methods</a> to generalize from known facts, however, existing <a href=https://en.wikipedia.org/wiki/Embedding>embedding methods</a> only model triple-level uncertainty, and reasoning results lack <a href=https://en.wikipedia.org/wiki/Global_consistency>global consistency</a>. To address these shortcomings, we propose BEUrRE, a novel uncertain knowledge graph embedding method with calibrated probabilistic semantics. BEUrRE models each entity as a box (i.e. axis-aligned hyperrectangle) and relations between two entities as affine transforms on the head and tail entity boxes. The geometry of the boxes allows for efficient calculation of intersections and volumes, endowing the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> with calibrated probabilistic semantics and facilitating the incorporation of relational constraints. Extensive experiments on two benchmark datasets show that BEUrRE consistently outperforms baselines on <a href=https://en.wikipedia.org/wiki/Confidence_interval>confidence prediction</a> and fact ranking due to its probabilistic calibration and ability to capture high-order dependencies among facts.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.205.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--205 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.205 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.205/>Clustering-based Inference for Biomedical Entity Linking</a></strong><br><a href=/people/r/rico-angell/>Rico Angell</a>
|
<a href=/people/n/nicholas-monath/>Nicholas Monath</a>
|
<a href=/people/s/sunil-mohan/>Sunil Mohan</a>
|
<a href=/people/n/nishant-yadav/>Nishant Yadav</a>
|
<a href=/people/a/andrew-mccallum/>Andrew McCallum</a><br><a href=/volumes/2021.naacl-main/ class=text-muted>Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--205><div class="card-body p-3 small">Due to large number of entities in biomedical knowledge bases, only a small fraction of entities have corresponding labelled training data. This necessitates entity linking models which are able to link mentions of unseen entities using learned representations of entities. Previous approaches link each mention independently, ignoring the relationships within and across documents between the entity mentions. These relations can be very useful for linking mentions in biomedical text where linking decisions are often difficult due mentions having a generic or a highly specialized form. In this paper, we introduce a model in which linking decisions can be made not merely by linking to a knowledge base entity but also by grouping multiple mentions together via <a href=https://en.wikipedia.org/wiki/Cluster_analysis>clustering</a> and jointly making linking predictions. In experiments on the largest publicly available biomedical dataset, we improve the best <a href=https://en.wikipedia.org/wiki/Independence_(probability_theory)>independent prediction</a> for <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entity linking</a> by 3.0 points of <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>, and our clustering-based inference model further improves <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entity linking</a> by 2.3 points.</div></div><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.38.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--38 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.38 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939198 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.38" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.38/>Self-Supervised Meta-Learning for Few-Shot Natural Language Classification Tasks</a></strong><br><a href=/people/t/trapit-bansal/>Trapit Bansal</a>
|
<a href=/people/r/rishikesh-jha/>Rishikesh Jha</a>
|
<a href=/people/t/tsendsuren-munkhdalai/>Tsendsuren Munkhdalai</a>
|
<a href=/people/a/andrew-mccallum/>Andrew McCallum</a><br><a href=/volumes/2020.emnlp-main/ class=text-muted>Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--38><div class="card-body p-3 small">Self-supervised pre-training of transformer models has revolutionized NLP applications. Such pre-training with language modeling objectives provides a useful initial point for parameters that generalize well to new tasks with <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a>. However, <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a> is still data inefficient when there are few labeled examples, <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> can be low. Data efficiency can be improved by optimizing pre-training directly for future <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a> with few examples ; this can be treated as a meta-learning problem. However, standard meta-learning techniques require many training tasks in order to generalize ; unfortunately, finding a diverse set of such supervised tasks is usually difficult. This paper proposes a self-supervised approach to generate a large, rich, meta-learning task distribution from unlabeled text. This is achieved using a cloze-style objective, but creating separate multi-class classification tasks by gathering tokens-to-be blanked from among only a handful of vocabulary terms. This yields as many unique meta-training tasks as the number of subsets of vocabulary terms. We meta-train a transformer model on this distribution of tasks using a recent meta-learning framework. On 17 NLP tasks, we show that this meta-training leads to better few-shot generalization than language-model pre-training followed by <a href=https://en.wikipedia.org/wiki/Finetuning>finetuning</a>. Furthermore, we show how the self-supervised tasks can be combined with <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised tasks</a> for <a href=https://en.wikipedia.org/wiki/Meta-learning>meta-learning</a>, providing substantial accuracy gains over previous supervised meta-learning.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.392.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--392 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.392 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939296 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.392/>Unsupervised Parsing with S-DIORA : Single Tree Encoding for Deep Inside-Outside Recursive Autoencoders<span class=acl-fixed-case>S</span>-<span class=acl-fixed-case>DIORA</span>: Single Tree Encoding for Deep Inside-Outside Recursive Autoencoders</a></strong><br><a href=/people/a/andrew-drozdov/>Andrew Drozdov</a>
|
<a href=/people/s/subendhu-rongali/>Subendhu Rongali</a>
|
<a href=/people/y/yi-pei-chen/>Yi-Pei Chen</a>
|
<a href=/people/t/tim-ogorman/>Tim O’Gorman</a>
|
<a href=/people/m/mohit-iyyer/>Mohit Iyyer</a>
|
<a href=/people/a/andrew-mccallum/>Andrew McCallum</a><br><a href=/volumes/2020.emnlp-main/ class=text-muted>Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--392><div class="card-body p-3 small">The deep inside-outside recursive autoencoder (DIORA ; Drozdov et al. 2019) is a self-supervised neural model that learns to induce syntactic tree structures for input sentences * without access to labeled training data *. In this paper, we discover that while DIORA exhaustively encodes all possible binary trees of a sentence with a soft dynamic program, its vector averaging approach is locally greedy and can not recover from errors when computing the highest scoring parse tree in bottom-up chart parsing. To fix this issue, we introduce S-DIORA, an improved variant of DIORA that encodes a single tree rather than a softly-weighted mixture of trees by employing a hard argmax operation and a beam at each cell in the chart. Our experiments show that through * fine-tuning * a pre-trained DIORA with our new <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a>, we improve the state of the art in * unsupervised * constituency parsing on the English WSJ Penn Treebank by 2.2-6 % F1, depending on the data used for <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.270.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--270 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.270 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38940651 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.270/>An Instance Level Approach for Shallow Semantic Parsing in Scientific Procedural Text<span class=acl-fixed-case>A</span>n <span class=acl-fixed-case>I</span>nstance <span class=acl-fixed-case>L</span>evel <span class=acl-fixed-case>A</span>pproach for <span class=acl-fixed-case>S</span>hallow <span class=acl-fixed-case>S</span>emantic <span class=acl-fixed-case>P</span>arsing in <span class=acl-fixed-case>S</span>cientific <span class=acl-fixed-case>P</span>rocedural <span class=acl-fixed-case>T</span>ext</a></strong><br><a href=/people/d/daivik-swarup/>Daivik Swarup</a>
|
<a href=/people/a/ahsaas-bajaj/>Ahsaas Bajaj</a>
|
<a href=/people/s/sheshera-mysore/>Sheshera Mysore</a>
|
<a href=/people/t/tim-ogorman/>Tim O’Gorman</a>
|
<a href=/people/r/rajarshi-das/>Rajarshi Das</a>
|
<a href=/people/a/andrew-mccallum/>Andrew McCallum</a><br><a href=/volumes/2020.findings-emnlp/ class=text-muted>Findings of the Association for Computational Linguistics: EMNLP 2020</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--270><div class="card-body p-3 small">In specific domains, such as procedural scientific text, human labeled data for <a href=https://en.wikipedia.org/wiki/Shallow_semantic_parsing>shallow semantic parsing</a> is especially limited and expensive to create. Fortunately, such specific domains often use rather formulaic writing, such that the different ways of expressing relations in a small number of grammatically similar labeled sentences may provide high coverage of semantic structures in the corpus, through an appropriately rich similarity metric. In light of this opportunity, this paper explores an instance-based approach to the relation prediction sub-task within shallow semantic parsing, in which semantic labels from structurally similar sentences in the training set are copied to test sentences. Candidate similar sentences are retrieved using SciBERT embeddings. For labels where it is possible to copy from a similar sentence we employ an instance level copy network, when this is not possible, a globally shared parametric model is employed. Experiments show our approach outperforms both baseline and prior methods by 0.75 to 3 F1 absolute in the Wet Lab Protocol Corpus and 1 F1 absolute in the Materials Science Procedural Text Corpus.</div></div><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1161.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1161 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1161 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1161.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D19-1161/>Unsupervised Labeled Parsing with Deep Inside-Outside Recursive Autoencoders</a></strong><br><a href=/people/a/andrew-drozdov/>Andrew Drozdov</a>
|
<a href=/people/p/patrick-verga/>Patrick Verga</a>
|
<a href=/people/y/yi-pei-chen/>Yi-Pei Chen</a>
|
<a href=/people/m/mohit-iyyer/>Mohit Iyyer</a>
|
<a href=/people/a/andrew-mccallum/>Andrew McCallum</a><br><a href=/volumes/D19-1/ class=text-muted>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1161><div class="card-body p-3 small">Understanding text often requires identifying meaningful <a href=https://en.wikipedia.org/wiki/Constituent_(linguistics)>constituent spans</a> such as <a href=https://en.wikipedia.org/wiki/Noun_phrase>noun phrases</a> and <a href=https://en.wikipedia.org/wiki/Verb_phrase>verb phrases</a>. In this work, we show that we can effectively recover these types of labels using the learned phrase vectors from deep inside-outside recursive autoencoders (DIORA). Specifically, we cluster span representations to induce span labels. Additionally, we improve the <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a>&#8217;s labeling accuracy by integrating latent code learning into the training procedure. We evaluate this approach empirically through unsupervised labeled constituency parsing. Our method outperforms ELMo and BERT on two versions of the Wall Street Journal (WSJ) dataset and is competitive to prior work that requires additional human annotations, improving over a previous state-of-the-art system that depends on ground-truth part-of-speech tags by 5 absolute F1 points (19 % relative error reduction).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5313.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5313 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5313 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5313/>Chains-of-Reasoning at TextGraphs 2019 Shared Task : Reasoning over Chains of Facts for Explainable Multi-hop Inference<span class=acl-fixed-case>T</span>ext<span class=acl-fixed-case>G</span>raphs 2019 Shared Task: Reasoning over Chains of Facts for Explainable Multi-hop Inference</a></strong><br><a href=/people/r/rajarshi-das/>Rajarshi Das</a>
|
<a href=/people/a/ameya-godbole/>Ameya Godbole</a>
|
<a href=/people/m/manzil-zaheer/>Manzil Zaheer</a>
|
<a href=/people/s/shehzaad-dhuliawala/>Shehzaad Dhuliawala</a>
|
<a href=/people/a/andrew-mccallum/>Andrew McCallum</a><br><a href=/volumes/D19-53/ class=text-muted>Proceedings of the Thirteenth Workshop on Graph-Based Methods for Natural Language Processing (TextGraphs-13)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5313><div class="card-body p-3 small">This paper describes our submission to the shared task on Multi-hop Inference Explanation Regeneration in TextGraphs workshop at EMNLP 2019 (Jansen and Ustalov, 2019). Our <a href=https://en.wikipedia.org/wiki/System>system</a> identifies chains of facts relevant to explain an answer to an elementary science examination question. To counter the problem of &#8216;spurious chains&#8217; leading to &#8216;semantic drifts&#8217;, we train a <a href=https://en.wikipedia.org/wiki/Ranker>ranker</a> that uses contextualized representation of facts to score its relevance for explaining an answer to a question. Our <a href=https://en.wikipedia.org/wiki/System>system</a> was ranked first w.r.t the mean average precision (MAP) metric outperforming the second best <a href=https://en.wikipedia.org/wiki/System>system</a> by 14.95 points.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2600.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-2600/>Proceedings of the Workshop on Extracting Structured Knowledge from Scientific Publications</a></strong><br><a href=/people/v/vivi-nastase/>Vivi Nastase</a>
|
<a href=/people/b/benjamin-roth/>Benjamin Roth</a>
|
<a href=/people/l/laura-dietz/>Laura Dietz</a>
|
<a href=/people/a/andrew-mccallum/>Andrew McCallum</a><br><a href=/volumes/W19-26/ class=text-muted>Proceedings of the Workshop on Extracting Structured Knowledge from Scientific Publications</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1116.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1116 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1116 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-1116/>Unsupervised Latent Tree Induction with Deep Inside-Outside Recursive Auto-Encoders</a></strong><br><a href=/people/a/andrew-drozdov/>Andrew Drozdov</a>
|
<a href=/people/p/patrick-verga/>Patrick Verga</a>
|
<a href=/people/m/mohit-yadav/>Mohit Yadav</a>
|
<a href=/people/m/mohit-iyyer/>Mohit Iyyer</a>
|
<a href=/people/a/andrew-mccallum/>Andrew McCallum</a><br><a href=/volumes/N19-1/ class=text-muted>Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1116><div class="card-body p-3 small">We introduce the deep inside-outside recursive autoencoder (DIORA), a fully-unsupervised method for discovering <a href=https://en.wikipedia.org/wiki/Syntax>syntax</a> that simultaneously learns representations for constituents within the induced tree. Our approach predicts each word in an input sentence conditioned on the rest of the sentence. During training we use <a href=https://en.wikipedia.org/wiki/Dynamic_programming>dynamic programming</a> to consider all possible binary trees over the sentence, and for <a href=https://en.wikipedia.org/wiki/Statistical_inference>inference</a> we use the <a href=https://en.wikipedia.org/wiki/CKY_algorithm>CKY algorithm</a> to extract the highest scoring parse. DIORA outperforms previously reported results for unsupervised binary constituency parsing on the benchmark WSJ dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1355.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1355 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1355 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/384787604 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1355" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1355/>Energy and Policy Considerations for <a href=https://en.wikipedia.org/wiki/Deep_learning>Deep Learning</a> in NLP<span class=acl-fixed-case>NLP</span></a></strong><br><a href=/people/e/emma-strubell/>Emma Strubell</a>
|
<a href=/people/a/ananya-ganesh/>Ananya Ganesh</a>
|
<a href=/people/a/andrew-mccallum/>Andrew McCallum</a><br><a href=/volumes/P19-1/ class=text-muted>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1355><div class="card-body p-3 small">Recent progress in hardware and methodology for training <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a> has ushered in a new generation of large networks trained on abundant data. These <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> have obtained notable gains in <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> across many <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP tasks</a>. However, these <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> improvements depend on the availability of exceptionally large computational resources that necessitate similarly substantial <a href=https://en.wikipedia.org/wiki/Energy_consumption>energy consumption</a>. As a result these models are costly to train and develop, both financially, due to the cost of <a href=https://en.wikipedia.org/wiki/Computer_hardware>hardware</a> and electricity or cloud compute time, and environmentally, due to the <a href=https://en.wikipedia.org/wiki/Carbon_footprint>carbon footprint</a> required to fuel modern tensor processing hardware. In this paper we bring this issue to the attention of NLP researchers by quantifying the approximate financial and environmental costs of training a variety of recently successful neural network models for <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a>. Based on these findings, we propose actionable recommendations to reduce costs and improve equity in NLP research and practice.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1431.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1431 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1431 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/385264668 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1431/>A2N : Attending to Neighbors for Knowledge Graph Inference<span class=acl-fixed-case>A</span>2<span class=acl-fixed-case>N</span>: Attending to Neighbors for Knowledge Graph Inference</a></strong><br><a href=/people/t/trapit-bansal/>Trapit Bansal</a>
|
<a href=/people/d/da-cheng-juan/>Da-Cheng Juan</a>
|
<a href=/people/s/sujith-ravi/>Sujith Ravi</a>
|
<a href=/people/a/andrew-mccallum/>Andrew McCallum</a><br><a href=/volumes/P19-1/ class=text-muted>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1431><div class="card-body p-3 small">State-of-the-art models for knowledge graph completion aim at learning a fixed embedding representation of entities in a multi-relational graph which can generalize to infer unseen entity relationships at test time. This can be sub-optimal as it requires memorizing and generalizing to all possible entity relationships using these fixed representations. We thus propose a novel attention-based method to learn query-dependent representation of entities which adaptively combines the relevant graph neighborhood of an entity leading to more accurate KG completion. The proposed method is evaluated on two benchmark datasets for knowledge graph completion, and experimental results show that the proposed model performs competitively or better than existing <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a>, including recent methods for explicit multi-hop reasoning. Qualitative probing offers insight into how the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> can reason about facts involving multiple hops in the <a href=https://en.wikipedia.org/wiki/Knowledge_graph>knowledge graph</a>, through the use of neighborhood attention.</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1306.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1306 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1306 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D18-1306.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/306054703 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1306/>Marginal Likelihood Training of BiLSTM-CRF for Biomedical Named Entity Recognition from Disjoint Label Sets<span class=acl-fixed-case>B</span>i<span class=acl-fixed-case>LSTM</span>-<span class=acl-fixed-case>CRF</span> for Biomedical Named Entity Recognition from Disjoint Label Sets</a></strong><br><a href=/people/n/nathan-greenberg/>Nathan Greenberg</a>
|
<a href=/people/t/trapit-bansal/>Trapit Bansal</a>
|
<a href=/people/p/patrick-verga/>Patrick Verga</a>
|
<a href=/people/a/andrew-mccallum/>Andrew McCallum</a><br><a href=/volumes/D18-1/ class=text-muted>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1306><div class="card-body p-3 small">Extracting typed entity mentions from <a href=https://en.wikipedia.org/wiki/Text_(literary_theory)>text</a> is a fundamental component to <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>language understanding and reasoning</a>. While there exist substantial labeled text datasets for multiple subsets of biomedical entity typessuch as genes and proteins, or chemicals and diseasesit is rare to find large labeled datasets containing labels for all desired entity types together. This paper presents a method for training a single CRF extractor from multiple <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> with disjoint or partially overlapping sets of entity types. Our approach employs marginal likelihood training to insist on labels that are present in the data, while filling in missing labels. This allows us to leverage all the available data within a single <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a>. In experimental results on the Biocreative V CDR (chemicals / diseases), Biocreative VI ChemProt (chemicals / proteins) and MedMentions (19 entity types) datasets, we show that joint training on multiple datasets improves NER F1 over training in isolation, and our methods achieve state-of-the-art results.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1548.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1548 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1548 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D18-1548.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/306141078 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1548" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1548/>Linguistically-Informed Self-Attention for Semantic Role Labeling</a></strong><br><a href=/people/e/emma-strubell/>Emma Strubell</a>
|
<a href=/people/p/patrick-verga/>Patrick Verga</a>
|
<a href=/people/d/daniel-andor/>Daniel Andor</a>
|
<a href=/people/d/david-weiss/>David Weiss</a>
|
<a href=/people/a/andrew-mccallum/>Andrew McCallum</a><br><a href=/volumes/D18-1/ class=text-muted>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1548><div class="card-body p-3 small">Current state-of-the-art semantic role labeling (SRL) uses a <a href=https://en.wikipedia.org/wiki/Deep_learning>deep neural network</a> with no explicit linguistic features. However, prior work has shown that gold syntax trees can dramatically improve SRL decoding, suggesting the possibility of increased accuracy from explicit modeling of syntax. In this work, we present linguistically-informed self-attention (LISA): a neural network model that combines multi-head self-attention with multi-task learning across dependency parsing, <a href=https://en.wikipedia.org/wiki/Part-of-speech_tagging>part-of-speech tagging</a>, predicate detection and <a href=https://en.wikipedia.org/wiki/Speech_recognition>SRL</a>. Unlike previous models which require significant pre-processing to prepare linguistic features, LISA can incorporate <a href=https://en.wikipedia.org/wiki/Syntax>syntax</a> using merely raw tokens as input, encoding the sequence only once to simultaneously perform <a href=https://en.wikipedia.org/wiki/Parsing>parsing</a>, predicate detection and role labeling for all predicates. Syntax is incorporated by training one <a href=https://en.wikipedia.org/wiki/Head_(linguistics)>attention head</a> to attend to syntactic parents for each token. Moreover, if a high-quality syntactic parse is already available, it can be beneficially injected at test time without re-training our SRL model. In experiments on CoNLL-2005 SRL, LISA achieves new state-of-the-art performance for a model using predicted predicates and standard <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a>, attaining 2.5 F1 absolute higher than the previous state-of-the-art on <a href=https://en.wikipedia.org/wiki/News_agency>newswire</a> and more than 3.5 F1 on out-of-domain data, nearly 10 % reduction in error. On ConLL-2012 English SRL we also show an improvement of more than 2.5 F1.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-2018.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-2018 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-2018 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-2018/>An Interface for Annotating Science Questions</a></strong><br><a href=/people/m/michael-boratko/>Michael Boratko</a>
|
<a href=/people/h/harshit-padigela/>Harshit Padigela</a>
|
<a href=/people/d/divyendra-mikkilineni/>Divyendra Mikkilineni</a>
|
<a href=/people/p/pritish-yuvraj/>Pritish Yuvraj</a>
|
<a href=/people/r/rajarshi-das/>Rajarshi Das</a>
|
<a href=/people/a/andrew-mccallum/>Andrew McCallum</a>
|
<a href=/people/m/maria-chang/>Maria Chang</a>
|
<a href=/people/a/achille-fokoue-nkoutche/>Achille Fokoue</a>
|
<a href=/people/p/pavan-kapanipathi/>Pavan Kapanipathi</a>
|
<a href=/people/n/nicholas-mattei/>Nicholas Mattei</a>
|
<a href=/people/r/ryan-musa/>Ryan Musa</a>
|
<a href=/people/k/kartik-talamadupula/>Kartik Talamadupula</a>
|
<a href=/people/m/michael-j-witbrock/>Michael Witbrock</a><br><a href=/volumes/D18-2/ class=text-muted>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-2018><div class="card-body p-3 small">Recent work introduces the AI2 Reasoning Challenge (ARC) and the associated ARC dataset that partitions open domain, complex science questions into an Easy Set and a Challenge Set. That work includes an analysis of 100 questions with respect to the types of knowledge and reasoning required to answer them. However, it does not include clear definitions of these types, nor does it offer information about the quality of the labels or the annotation process used. In this paper, we introduce a novel interface for human annotation of science question-answer pairs with their respective knowledge and reasoning types, in order that the classification of new questions may be improved. We build on the classification schema proposed by prior work on the ARC dataset, and evaluate the effectiveness of our interface with a preliminary study involving 10 participants.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-1706.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-1706 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-1706 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-1706/>Efficient Graph-based Word Sense Induction by Distributional Inclusion Vector Embeddings</a></strong><br><a href=/people/h/haw-shiuan-chang/>Haw-Shiuan Chang</a>
|
<a href=/people/a/amol-agrawal/>Amol Agrawal</a>
|
<a href=/people/a/ananya-ganesh/>Ananya Ganesh</a>
|
<a href=/people/a/anirudha-desai/>Anirudha Desai</a>
|
<a href=/people/v/vinayak-mathur/>Vinayak Mathur</a>
|
<a href=/people/a/alfred-hough/>Alfred Hough</a>
|
<a href=/people/a/andrew-mccallum/>Andrew McCallum</a><br><a href=/volumes/W18-17/ class=text-muted>Proceedings of the Twelfth Workshop on Graph-Based Methods for Natural Language Processing (TextGraphs-12)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-1706><div class="card-body p-3 small">Word sense induction (WSI), which addresses <a href=https://en.wikipedia.org/wiki/Polysemy>polysemy</a> by unsupervised discovery of multiple word senses, resolves ambiguities for downstream NLP tasks and also makes word representations more interpretable. This paper proposes an accurate and efficient graph-based method for WSI that builds a global non-negative vector embedding basis (which are interpretable like topics) and clusters the basis indexes in the ego network of each polysemous word. By adopting distributional inclusion vector embeddings as our basis formation model, we avoid the expensive step of <a href=https://en.wikipedia.org/wiki/Nearest_neighbor_search>nearest neighbor search</a> that plagues other graph-based methods without sacrificing the quality of sense clusters. Experiments on three datasets show that our proposed method produces similar or better sense clusters and embeddings compared with previous state-of-the-art methods while being significantly more efficient.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1045.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1045 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1045 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N18-1045.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N18-1045/>Distributional Inclusion Vector Embedding for Unsupervised Hypernymy Detection</a></strong><br><a href=/people/h/haw-shiuan-chang/>Haw-Shiuan Chang</a>
|
<a href=/people/z/ziyun-wang/>Ziyun Wang</a>
|
<a href=/people/l/luke-vilnis/>Luke Vilnis</a>
|
<a href=/people/a/andrew-mccallum/>Andrew McCallum</a><br><a href=/volumes/N18-1/ class=text-muted>Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1045><div class="card-body p-3 small">Modeling hypernymy, such as poodle is-a dog, is an important generalization aid to many NLP tasks, such as <a href=https://en.wikipedia.org/wiki/Logical_consequence>entailment</a>, relation extraction, and <a href=https://en.wikipedia.org/wiki/Question_answering>question answering</a>. Supervised learning from labeled hypernym sources, such as <a href=https://en.wikipedia.org/wiki/WordNet>WordNet</a>, limits the coverage of these models, which can be addressed by learning <a href=https://en.wikipedia.org/wiki/Hypernym>hypernyms</a> from unlabeled text. Existing <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised methods</a> either do not scale to large vocabularies or yield unacceptably poor accuracy. This paper introduces distributional inclusion vector embedding (DIVE), a simple-to-implement unsupervised method of hypernym discovery via per-word non-negative vector embeddings which preserve the inclusion property of word contexts. In experimental evaluations more comprehensive than any previous literature of which we are awareevaluating on 11 datasets using multiple existing as well as newly proposed scoring functionswe find that our method provides up to double the precision of previous <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised methods</a>, and the highest average performance, using a much more compact word representation, and yielding many new state-of-the-art results.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K18-1001.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K18-1001 data-toggle=collapse aria-expanded=false aria-controls=abstract-K18-1001 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/K18-1001/>Embedded-State Latent Conditional Random Fields for Sequence Labeling</a></strong><br><a href=/people/d/dung-thai/>Dung Thai</a>
|
<a href=/people/s/sree-harsha-ramesh/>Sree Harsha Ramesh</a>
|
<a href=/people/s/shikhar-murty/>Shikhar Murty</a>
|
<a href=/people/l/luke-vilnis/>Luke Vilnis</a>
|
<a href=/people/a/andrew-mccallum/>Andrew McCallum</a><br><a href=/volumes/K18-1/ class=text-muted>Proceedings of the 22nd Conference on Computational Natural Language Learning</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K18-1001><div class="card-body p-3 small">Complex textual information extraction tasks are often posed as <a href=https://en.wikipedia.org/wiki/Sequence_labeling>sequence labeling</a> or <a href=https://en.wikipedia.org/wiki/Shallow_parsing>shallow parsing</a>, where fields are extracted using local labels made consistent through <a href=https://en.wikipedia.org/wiki/Statistical_inference>probabilistic inference</a> in a <a href=https://en.wikipedia.org/wiki/Graphical_model>graphical model</a> with constrained transitions. Recently, it has become common to locally parametrize these models using rich features extracted by <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent neural networks</a> (such as LSTM), while enforcing consistent outputs through a simple linear-chain model, representing <a href=https://en.wikipedia.org/wiki/Markov_chain>Markovian dependencies</a> between successive labels. However, the simple <a href=https://en.wikipedia.org/wiki/Graphical_model>graphical model structure</a> belies the often complex <a href=https://en.wikipedia.org/wiki/Constraint_(mathematics)>non-local constraints</a> between output labels. For example, many fields, such as a first name, can only occur a fixed number of times, or in the presence of other fields. While RNNs have provided increasingly powerful context-aware local features for sequence tagging, they have yet to be integrated with a global graphical model of similar expressivity in the output distribution. Our model goes beyond the linear chain CRF to incorporate multiple hidden states per output label, but parametrizes them parsimoniously with low-rank log-potential scoring matrices, effectively learning an embedding space for hidden states. This augmented latent space of inference variables complements the rich feature representation of the RNN, and allows exact global inference obeying complex, learned non-local output constraints. We experiment with several datasets and show that the model outperforms baseline CRF+RNN models when global output constraints are necessary at inference-time, and explore the interpretable latent structure.<i>shallow parsing</i>, where fields are extracted using local labels made consistent through probabilistic inference in a graphical model with constrained transitions. Recently, it has become common to locally parametrize these models using rich features extracted by recurrent neural networks (such as LSTM), while enforcing consistent outputs through a simple linear-chain model, representing Markovian dependencies between successive labels. However, the simple graphical model structure belies the often complex non-local constraints between output labels. For example, many fields, such as a first name, can only occur a fixed number of times, or in the presence of other fields. While RNNs have provided increasingly powerful context-aware local features for sequence tagging, they have yet to be integrated with a global graphical model of similar expressivity in the output distribution. Our model goes beyond the linear chain CRF to incorporate multiple hidden states per output label, but parametrizes them parsimoniously with low-rank log-potential scoring matrices, effectively learning an embedding space for hidden states. This augmented latent space of inference variables complements the rich feature representation of the RNN, and allows exact global inference obeying complex, learned non-local output constraints. We experiment with several datasets and show that the model outperforms baseline CRF+RNN models when global output constraints are necessary at inference-time, and explore the interpretable latent structure.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1025.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1025 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1025 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1025.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-1025/>Probabilistic Embedding of Knowledge Graphs with Box Lattice Measures</a></strong><br><a href=/people/l/luke-vilnis/>Luke Vilnis</a>
|
<a href=/people/x/xiang-li/>Xiang Li</a>
|
<a href=/people/s/shikhar-murty/>Shikhar Murty</a>
|
<a href=/people/a/andrew-mccallum/>Andrew McCallum</a><br><a href=/volumes/P18-1/ class=text-muted>Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1025><div class="card-body p-3 small">Embedding methods which enforce a partial order or lattice structure over the concept space, such as Order Embeddings (OE), are a natural way to model transitive relational data (e.g. entailment graphs). However, OE learns a deterministic knowledge base, limiting expressiveness of queries and the ability to use <a href=https://en.wikipedia.org/wiki/Uncertainty>uncertainty</a> for both <a href=https://en.wikipedia.org/wiki/Prediction>prediction</a> and <a href=https://en.wikipedia.org/wiki/Machine_learning>learning</a> (e.g. learning from expectations). Probabilistic extensions of OE have provided the ability to somewhat calibrate these denotational probabilities while retaining the consistency and inductive bias of ordered models, but lack the ability to model the negative correlations found in real-world knowledge. In this work we show that a broad class of models that assign probability measures to OE can never capture negative correlation, which motivates our construction of a novel box lattice and accompanying <a href=https://en.wikipedia.org/wiki/Probability_measure>probability measure</a> to capture anti-correlation and even disjoint concepts, while still providing the benefits of probabilistic modeling, such as the ability to perform rich joint and conditional queries over arbitrary sets of concepts, and both learning from and predicting calibrated uncertainty. We show improvements over previous approaches in modeling the Flickr and WordNet entailment graphs, and investigate the power of the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>.</div></div><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2057.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2057 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2057 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-2057/>Question Answering on Knowledge Bases and Text using Universal Schema and Memory Networks</a></strong><br><a href=/people/r/rajarshi-das/>Rajarshi Das</a>
|
<a href=/people/m/manzil-zaheer/>Manzil Zaheer</a>
|
<a href=/people/s/siva-reddy/>Siva Reddy</a>
|
<a href=/people/a/andrew-mccallum/>Andrew McCallum</a><br><a href=/volumes/P17-2/ class=text-muted>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2057><div class="card-body p-3 small">Existing <a href=https://en.wikipedia.org/wiki/Question_answering>question answering methods</a> infer answers either from a <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge base</a> or from <a href=https://en.wikipedia.org/wiki/Text_corpus>raw text</a>. While knowledge base (KB) methods are good at answering compositional questions, their performance is often affected by the incompleteness of the KB. Au contraire, <a href=https://en.wikipedia.org/wiki/Web_page>web text</a> contains millions of facts that are absent in the KB, however in an <a href=https://en.wikipedia.org/wiki/Unstructured_data>unstructured form</a>. Universal schema can support reasoning on the union of both structured KBs and <a href=https://en.wikipedia.org/wiki/Unstructured_data>unstructured text</a> by aligning them in a common embedded space. In this paper we extend universal schema to <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language question answering</a>, employing Memory networks to attend to the large body of facts in the combination of text and KB. Our <a href=https://en.wikipedia.org/wiki/Model_(person)>models</a> can be trained in an end-to-end fashion on question-answer pairs. Evaluation results on Spades fill-in-the-blank question answering dataset show that exploiting universal schema for <a href=https://en.wikipedia.org/wiki/Question_answering>question answering</a> is better than using either a KB or text alone. This <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> also outperforms the current <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a> by 8.5 F1 points.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4301.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4301 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4301 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4301/>Dependency Parsing with Dilated Iterated Graph CNNs<span class=acl-fixed-case>CNN</span>s</a></strong><br><a href=/people/e/emma-strubell/>Emma Strubell</a>
|
<a href=/people/a/andrew-mccallum/>Andrew McCallum</a><br><a href=/volumes/W17-43/ class=text-muted>Proceedings of the 2nd Workshop on Structured Prediction for Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4301><div class="card-body p-3 small">Dependency parses are an effective way to inject linguistic knowledge into many downstream tasks, and many practitioners wish to efficiently parse sentences at scale. Recent advances in <a href=https://en.wikipedia.org/wiki/Graphics_processing_unit>GPU hardware</a> have enabled <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a> to achieve significant gains over the previous best models, these models still fail to leverage GPUs&#8217; capability for <a href=https://en.wikipedia.org/wiki/Massively_parallel>massive parallelism</a> due to their requirement of sequential processing of the sentence. In response, we propose Dilated Iterated Graph Convolutional Neural Networks (DIG-CNNs) for graph-based dependency parsing, a graph convolutional architecture that allows for efficient end-to-end GPU parsing. In experiments on the English Penn TreeBank benchmark, we show that DIG-CNNs perform on par with some of the best <a href=https://en.wikipedia.org/wiki/Parsing>neural network parsers</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S17-2091.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S17-2091 data-toggle=collapse aria-expanded=false aria-controls=abstract-S17-2091 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/S17-2091/>SemEval 2017 Task 10 : ScienceIE-Extracting Keyphrases and Relations from Scientific Publications<span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val 2017 Task 10: <span class=acl-fixed-case>S</span>cience<span class=acl-fixed-case>IE</span> - Extracting Keyphrases and Relations from Scientific Publications</a></strong><br><a href=/people/i/isabelle-augenstein/>Isabelle Augenstein</a>
|
<a href=/people/m/mrinal-das/>Mrinal Das</a>
|
<a href=/people/s/sebastian-riedel/>Sebastian Riedel</a>
|
<a href=/people/l/lakshmi-vikraman/>Lakshmi Vikraman</a>
|
<a href=/people/a/andrew-mccallum/>Andrew McCallum</a><br><a href=/volumes/S17-2/ class=text-muted>Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S17-2091><div class="card-body p-3 small">We describe the SemEval task of extracting keyphrases and relations between them from scientific documents, which is crucial for understanding which publications describe which processes, tasks and materials. Although this was a new <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>, we had a total of 26 submissions across 3 evaluation scenarios. We expect the task and the findings reported in this paper to be relevant for researchers working on understanding scientific content, as well as the broader knowledge base population and information extraction communities.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1283.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1283 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1283 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D17-1283" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D17-1283/>Fast and Accurate Entity Recognition with Iterated Dilated Convolutions</a></strong><br><a href=/people/e/emma-strubell/>Emma Strubell</a>
|
<a href=/people/p/patrick-verga/>Patrick Verga</a>
|
<a href=/people/d/david-belanger/>David Belanger</a>
|
<a href=/people/a/andrew-mccallum/>Andrew McCallum</a><br><a href=/volumes/D17-1/ class=text-muted>Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1283><div class="card-body p-3 small">Today when many practitioners run basic NLP on the entire <a href=https://en.wikipedia.org/wiki/World_Wide_Web>web</a> and large-volume traffic, faster methods are paramount to saving time and energy costs. Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining per-token vector representations serving as input to labeling tasks such as NER (often followed by prediction in a linear-chain CRF). Though expressive and accurate, these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> fail to fully exploit <a href=https://en.wikipedia.org/wiki/Graphics_processing_unit>GPU parallelism</a>, limiting their computational efficiency. This paper proposes a faster alternative to Bi-LSTMs for NER : Iterated Dilated Convolutional Neural Networks (ID-CNNs), which have better capacity than traditional CNNs for large context and structured prediction. Unlike LSTMs whose sequential processing on sentences of length N requires O(N) time even in the face of <a href=https://en.wikipedia.org/wiki/Parallel_computing>parallelism</a>, ID-CNNs permit fixed-depth convolutions to run in parallel across entire documents. We describe a distinct combination of network structure, parameter sharing and training procedures that enable dramatic 14-20x test-time speedups while retaining <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> comparable to the Bi-LSTM-CRF. Moreover, ID-CNNs trained to aggregate context from the entire document are more accurate than Bi-LSTM-CRFs while attaining 8x faster test time speeds.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1013.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1013 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1013 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=E17-1013" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/E17-1013/>Chains of Reasoning over Entities, Relations, and Text using <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>Recurrent Neural Networks</a></a></strong><br><a href=/people/r/rajarshi-das/>Rajarshi Das</a>
|
<a href=/people/a/arvind-neelakantan/>Arvind Neelakantan</a>
|
<a href=/people/d/david-belanger/>David Belanger</a>
|
<a href=/people/a/andrew-mccallum/>Andrew McCallum</a><br><a href=/volumes/E17-1/ class=text-muted>Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1013><div class="card-body p-3 small">Our goal is to combine the rich multi-step inference of symbolic logical reasoning with the generalization capabilities of <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a>. We are particularly interested in complex reasoning about entities and relations in <a href=https://en.wikipedia.org/wiki/Text_(literary_theory)>text</a> and large-scale knowledge bases (KBs). Neelakantan et al. (2015) use RNNs to compose the distributed semantics of multi-hop paths in KBs ; however for multiple reasons, the approach lacks accuracy and practicality. This paper proposes three significant modeling advances : (1) we learn to jointly reason about <a href=https://en.wikipedia.org/wiki/Binary_relation>relations</a>, entities, and entity-types ; (2) we use neural attention modeling to incorporate multiple paths ; (3) we learn to share strength in a single RNN that represents logical composition across all relations. On a large-scale Freebase+ClueWeb prediction task, we achieve 25 % <a href=https://en.wikipedia.org/wiki/Error_detection_and_correction>error reduction</a>, and a 53 % <a href=https://en.wikipedia.org/wiki/Error_detection_and_correction>error reduction</a> on sparse relations due to shared strength. On chains of reasoning in <a href=https://en.wikipedia.org/wiki/WordNet>WordNet</a> we reduce error in mean quantile by 84 % versus previous <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a>.<i>entities, and entity-types</i>; (2) we use neural attention modeling to incorporate <i>multiple paths</i>; (3) we learn to <i>share strength in a single RNN</i> that represents logical composition across all relations. On a large-scale Freebase+ClueWeb prediction task, we achieve 25% error reduction, and a 53% error reduction on sparse relations due to shared strength. On chains of reasoning in WordNet we reduce error in mean quantile by 84% versus previous state-of-the-art.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1058.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1058 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1058 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=E17-1058" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/E17-1058/>Generalizing to Unseen Entities and Entity Pairs with Row-less Universal Schema</a></strong><br><a href=/people/p/patrick-verga/>Patrick Verga</a>
|
<a href=/people/a/arvind-neelakantan/>Arvind Neelakantan</a>
|
<a href=/people/a/andrew-mccallum/>Andrew McCallum</a><br><a href=/volumes/E17-1/ class=text-muted>Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1058><div class="card-body p-3 small">Universal schema predicts the types of entities and relations in a knowledge base (KB) by jointly embedding the union of all available schema typesnot only types from multiple structured databases (such as <a href=https://en.wikipedia.org/wiki/Freebase>Freebase</a> or Wikipedia infoboxes), but also types expressed as textual patterns from raw text. This prediction is typically modeled as a matrix completion problem, with one type per column, and either one or two entities per row (in the case of <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entity types</a> or <a href=https://en.wikipedia.org/wiki/Binary_relation>binary relation types</a>, respectively). Factorizing this sparsely observed matrix yields a learned vector embedding for each row and each column. In this paper we explore the problem of making predictions for entities or entity-pairs unseen at training time (and hence without a pre-learned row embedding). We propose an approach having no per-row parameters at all ; rather we produce a row vector on the fly using a learned aggregation function of the vectors of the observed columns for that row. We experiment with various aggregation functions, including neural network attention models. Our approach can be understood as a natural language database, in that questions about KB entities are answered by attending to textual or database evidence. In experiments predicting both relations and entity types, we demonstrate that despite having an order of magnitude fewer parameters than traditional universal schema, we can match the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of the traditional model, and more importantly, we can now make predictions about unseen rows with nearly the same <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> as rows available at training time.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Andrew+McCallum" title="Search for 'Andrew McCallum' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/r/rajarshi-das/ class=align-middle>Rajarshi Das</a>
<span class="badge badge-secondary align-middle ml-2">7</span></li><li class=list-group-item><a href=/people/p/patrick-verga/ class=align-middle>Patrick Verga</a>
<span class="badge badge-secondary align-middle ml-2">6</span></li><li class=list-group-item><a href=/people/m/mohit-iyyer/ class=align-middle>Mohit Iyyer</a>
<span class="badge badge-secondary align-middle ml-2">5</span></li><li class=list-group-item><a href=/people/a/andrew-drozdov/ class=align-middle>Andrew Drozdov</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/people/e/emma-strubell/ class=align-middle>Emma Strubell</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/m/michael-boratko/ class=align-middle>Michael Boratko</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/t/trapit-bansal/ class=align-middle>Trapit Bansal</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/t/tim-ogorman/ class=align-middle>Tim O’Gorman</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/m/manzil-zaheer/ class=align-middle>Manzil Zaheer</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/l/luke-vilnis/ class=align-middle>Luke Vilnis</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/a/ahsaas-bajaj/ class=align-middle>Ahsaas Bajaj</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/s/subendhu-rongali/ class=align-middle>Subendhu Rongali</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/y/yi-pei-chen/ class=align-middle>Yi-Pei Chen</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/j/jay-yoon-lee/ class=align-middle>Jay Yoon Lee</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/d/dung-thai/ class=align-middle>Dung Thai</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/a/ameya-godbole/ class=align-middle>Ameya Godbole</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/d/david-belanger/ class=align-middle>David Belanger</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/h/haw-shiuan-chang/ class=align-middle>Haw-Shiuan Chang</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/a/ananya-ganesh/ class=align-middle>Ananya Ganesh</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/s/shikhar-murty/ class=align-middle>Shikhar Murty</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/a/arvind-neelakantan/ class=align-middle>Arvind Neelakantan</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/y/yasumasa-onoe/ class=align-middle>Yasumasa Onoe</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/g/greg-durrett/ class=align-middle>Greg Durrett</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/sumanta-bhattacharyya/ class=align-middle>Sumanta Bhattacharyya</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/amirmohammad-rooshenas/ class=align-middle>Amirmohammad Rooshenas</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/subhajit-naskar/ class=align-middle>Subhajit Naskar</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/simeng-sun/ class=align-middle>Simeng Sun</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/robert-l-logan-iv/ class=align-middle>Robert L. Logan IV</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/sameer-singh/ class=align-middle>Sameer Singh</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/dan-bikel/ class=align-middle>Dan Bikel</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/p/pavitra-dangati/ class=align-middle>Pavitra Dangati</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/kalpesh-krishna/ class=align-middle>Kalpesh Krishna</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/p/pradhiksha-ashok-kumar/ class=align-middle>Pradhiksha Ashok Kumar</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/rheeya-uppaal/ class=align-middle>Rheeya Uppaal</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/b/bradford-windsor/ class=align-middle>Bradford Windsor</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/e/eliot-brenner/ class=align-middle>Eliot Brenner</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/dominic-dotterrer/ class=align-middle>Dominic Dotterrer</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/rishikesh-jha/ class=align-middle>Rishikesh Jha</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/tsendsuren-munkhdalai/ class=align-middle>Tsendsuren Munkhdalai</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/siva-reddy/ class=align-middle>Siva Reddy</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/n/nathan-greenberg/ class=align-middle>Nathan Greenberg</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/daniel-andor/ class=align-middle>Daniel Andor</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/david-weiss/ class=align-middle>David Weiss</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/harshit-padigela/ class=align-middle>Harshit Padigela</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/divyendra-mikkilineni/ class=align-middle>Divyendra Mikkilineni</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/p/pritish-yuvraj/ class=align-middle>Pritish Yuvraj</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/maria-chang/ class=align-middle>Maria Chang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/achille-fokoue-nkoutche/ class=align-middle>Achille Fokoue-Nkoutche</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/p/pavan-kapanipathi/ class=align-middle>Pavan Kapanipathi</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/n/nicholas-mattei/ class=align-middle>Nicholas Mattei</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/ryan-musa/ class=align-middle>Ryan Musa</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/kartik-talamadupula/ class=align-middle>Kartik Talamadupula</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/michael-j-witbrock/ class=align-middle>Michael J. Witbrock</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/zhiyang-xu/ class=align-middle>Zhiyang Xu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/dylan-finkbeiner/ class=align-middle>Dylan Finkbeiner</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/shilpa-suresh/ class=align-middle>Shilpa Suresh</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/e/ethan-perez/ class=align-middle>Ethan Perez</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/lizhen-tan/ class=align-middle>Lizhen Tan</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/lazaros-polymenakos/ class=align-middle>Lazaros Polymenakos</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/shehzaad-dhuliawala/ class=align-middle>Shehzaad Dhuliawala</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/i/isabelle-augenstein/ class=align-middle>Isabelle Augenstein</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/mrinal-das/ class=align-middle>Mrinal Das</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/sebastian-riedel/ class=align-middle>Sebastian Riedel</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/lakshmi-vikraman/ class=align-middle>Lakshmi Vikraman</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/x/xuelu-chen/ class=align-middle>Xuelu Chen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/muhao-chen/ class=align-middle>Muhao Chen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/shib-sankar-dasgupta/ class=align-middle>Shib Sankar Dasgupta</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/x/xiang-lorraine-li/ class=align-middle>Xiang Lorraine Li</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/rico-angell/ class=align-middle>Rico Angell</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/n/nicholas-monath/ class=align-middle>Nicholas Monath</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/sunil-mohan/ class=align-middle>Sunil Mohan</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/n/nishant-yadav/ class=align-middle>Nishant Yadav</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/daivik-swarup/ class=align-middle>Daivik Swarup</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/sheshera-mysore/ class=align-middle>Sheshera Mysore</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/amol-agrawal/ class=align-middle>Amol Agrawal</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/anirudha-desai/ class=align-middle>Anirudha Desai</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/v/vinayak-mathur/ class=align-middle>Vinayak Mathur</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/alfred-hough/ class=align-middle>Alfred Hough</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/v/vivi-nastase/ class=align-middle>Vivi Nastase</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/b/benjamin-roth/ class=align-middle>Benjamin Roth</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/laura-dietz/ class=align-middle>Laura Dietz</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/mohit-yadav/ class=align-middle>Mohit Yadav</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/ziyun-wang/ class=align-middle>Ziyun Wang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/sree-harsha-ramesh/ class=align-middle>Sree Harsha Ramesh</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/x/xiang-li/ class=align-middle>Xiang Li</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/da-cheng-juan/ class=align-middle>Da-Cheng Juan</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/sujith-ravi/ class=align-middle>Sujith Ravi</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/emnlp/ class=align-middle>EMNLP</a><span class="badge badge-secondary align-middle ml-2">10</span></li><li class=list-group-item><a href=/venues/acl/ class=align-middle>ACL</a><span class="badge badge-secondary align-middle ml-2">8</span></li><li class=list-group-item><a href=/venues/naacl/ class=align-middle>NAACL</a><span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/venues/ws/ class=align-middle>WS</a><span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/venues/eacl/ class=align-middle>EACL</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-venues aria-expanded=false aria-controls=more-venues>show all...</li><div class="collapse border-top" id=more-venues><li class=list-group-item><a href=/venues/semeval/ class=align-middle>SemEval</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/findings/ class=align-middle>Findings</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/conll/ class=align-middle>CoNLL</a><span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>