<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Atsushi Fujita - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Atsushi</span> <span class=font-weight-bold>Fujita</span></h2><hr><div class=row><div class=col-lg-9><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.mtsummit-research.10.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--mtsummit-research--10 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.mtsummit-research.10 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.mtsummit-research.10/>Investigating Softmax Tempering for Training Neural Machine Translation Models</a></strong><br><a href=/people/r/raj-dabre/>Raj Dabre</a>
|
<a href=/people/a/atsushi-fujita/>Atsushi Fujita</a><br><a href=/volumes/2021.mtsummit-research/ class=text-muted>Proceedings of Machine Translation Summit XVIII: Research Track</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--mtsummit-research--10><div class="card-body p-3 small">Neural machine translation (NMT) models are typically trained using a softmax cross-entropy loss where the softmax distribution is compared against the gold labels. In low-resource scenarios and NMT models tend to perform poorly because the model training quickly converges to a point where the softmax distribution computed using <a href=https://en.wikipedia.org/wiki/Logit>logits</a> approaches the gold label distribution. Although label smoothing is a well-known solution to address this issue and we further propose to divide the logits by a <a href=https://en.wikipedia.org/wiki/Temperature_coefficient>temperature coefficient</a> greater than one and forcing the softmax distribution to be smoother during training. This makes it harder for the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> to quickly over-fit. In our experiments on 11 language pairs in the low-resource Asian Language Treebank dataset and we observed significant improvements in translation quality. Our analysis focuses on finding the right balance of label smoothing and softmax tempering which indicates that they are orthogonal methods. Finally and a study of softmax entropies and gradients reveal the impact of our method on the internal behavior of our NMT models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.mtsummit-research.18.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--mtsummit-research--18 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.mtsummit-research.18 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.mtsummit-research.18" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.mtsummit-research.18/>Attainable Text-to-Text Machine Translation vs. <a href=https://en.wikipedia.org/wiki/Translation>Translation</a> : Issues Beyond Linguistic Processing</a></strong><br><a href=/people/a/atsushi-fujita/>Atsushi Fujita</a><br><a href=/volumes/2021.mtsummit-research/ class=text-muted>Proceedings of Machine Translation Summit XVIII: Research Track</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--mtsummit-research--18><div class="card-body p-3 small">Existing approaches for machine translation (MT) mostly translate given text in the source language into the target language and without explicitly referring to information indispensable for producing proper <a href=https://en.wikipedia.org/wiki/Translation>translation</a>. This includes not only information in other textual elements and modalities than texts in the same document and but also extra-document and non-linguistic information and such as <a href=https://en.wikipedia.org/wiki/Social_norm>norms</a> and <a href=https://en.wikipedia.org/wiki/Skopos>skopos</a>. To design better translation production work-flows and we need to distinguish translation issues that could be resolved by the existing text-to-text approaches and those beyond them. To this end and we conducted an analytic assessment of MT outputs and taking an English-to-Japanese news translation task as a case study. First and examples of <a href=https://en.wikipedia.org/wiki/Translation>translation issues</a> and their revisions were collected by a two-stage post-editing (PE) method : performing minimal PE to obtain <a href=https://en.wikipedia.org/wiki/Translation>translation</a> attainable based on the given textual information and further performing full PE to obtain truly acceptable <a href=https://en.wikipedia.org/wiki/Translation>translation</a> referring to any information if necessary. Then and the collected revision examples were manually analyzed. We revealed dominant issues and information indispensable for resolving them and such as fine-grained style specifications and terminology and domain-specific knowledge and and reference documents and delineating a clear distinction between <a href=https://en.wikipedia.org/wiki/Translation>translation</a> and what text-to-text MT can ultimately attain.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.566.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--566 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.566 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><span class="align-middle mr-1" data-toggle=tooltip data-placement=bottom title="Outstanding Paper"><i class="fas fa-award"></i></span><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.acl-long.566.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material">
<i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.566" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.566/>Scientific Credibility of Machine Translation Research : A Meta-Evaluation of 769 Papers</a></strong><br><a href=/people/b/benjamin-marie/>Benjamin Marie</a>
|
<a href=/people/a/atsushi-fujita/>Atsushi Fujita</a>
|
<a href=/people/r/raphael-rubino/>Raphael Rubino</a><br><a href=/volumes/2021.acl-long/ class=text-muted>Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--566><div class="card-body p-3 small">This paper presents the first large-scale meta-evaluation of <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation (MT)</a>. We annotated MT evaluations conducted in 769 research papers published from 2010 to 2020. Our study shows that practices for automatic MT evaluation have dramatically changed during the past decade and follow concerning trends. An increasing number of MT evaluations exclusively rely on differences between <a href=https://en.wikipedia.org/wiki/BLEU>BLEU scores</a> to draw conclusions, without performing any kind of statistical significance testing nor human evaluation, while at least 108 metrics claiming to be better than <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a> have been proposed. MT evaluations in recent papers tend to copy and compare automatic metric scores from previous work to claim the superiority of a method or an <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> without confirming neither exactly the same training, validating, and testing data have been used nor the metric scores are comparable. Furthermore, tools for reporting standardized metric scores are still far from being widely adopted by the MT community. After showing how the accumulation of these pitfalls leads to dubious evaluation, we propose a guideline to encourage better automatic MT evaluation along with a simple meta-evaluation scoring method to assess its credibility.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.132.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--132 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.132 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.132/>Understanding Pre-Editing for Black-Box Neural Machine Translation</a></strong><br><a href=/people/r/rei-miyata/>Rei Miyata</a>
|
<a href=/people/a/atsushi-fujita/>Atsushi Fujita</a><br><a href=/volumes/2021.eacl-main/ class=text-muted>Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--132><div class="card-body p-3 small">Pre-editing is the process of modifying the source text (ST) so that it can be translated by machine translation (MT) in a better quality. Despite the unpredictability of black-box neural MT (NMT), <a href=https://en.wikipedia.org/wiki/Pre-editing>pre-editing</a> has been deployed in various practical MT use cases. Although many studies have demonstrated the effectiveness of <a href=https://en.wikipedia.org/wiki/Pre-editing>pre-editing methods</a> for particular settings, thus far, a deep understanding of what <a href=https://en.wikipedia.org/wiki/Pre-editing>pre-editing</a> is and how it works for black-box NMT is lacking. To elicit such understanding, we extensively investigated human pre-editing practices. We first implemented a <a href=https://en.wikipedia.org/wiki/Communication_protocol>protocol</a> to incrementally record the minimum edits for each <a href=https://en.wikipedia.org/wiki/Translation>ST</a> and collected 6,652 instances of <a href=https://en.wikipedia.org/wiki/Pre-editing>pre-editing</a> across three translation directions, two MT systems, and four text domains. We then analysed the instances from three perspectives : the characteristics of the pre-edited ST, the diversity of pre-editing operations, and the impact of the pre-editing operations on NMT outputs. Our findings include the following : (1) enhancing the explicitness of the meaning of an ST and its syntactic structure is more important for obtaining better translations than making the ST shorter and simpler, and (2) although the impact of pre-editing on NMT is generally unpredictable, there are some tendencies of changes in the NMT outputs depending on the editing operation types.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wmt-1.99.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wmt-1--99 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wmt-1.99 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wmt-1.99/>NICT Kyoto Submission for the WMT’21 Quality Estimation Task : Multimetric Multilingual Pretraining for Critical Error Detection<span class=acl-fixed-case>NICT</span> <span class=acl-fixed-case>K</span>yoto Submission for the <span class=acl-fixed-case>WMT</span>’21 Quality Estimation Task: Multimetric Multilingual Pretraining for Critical Error Detection</a></strong><br><a href=/people/r/raphael-rubino/>Raphael Rubino</a>
|
<a href=/people/a/atsushi-fujita/>Atsushi Fujita</a>
|
<a href=/people/b/benjamin-marie/>Benjamin Marie</a><br><a href=/volumes/2021.wmt-1/ class=text-muted>Proceedings of the Sixth Conference on Machine Translation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wmt-1--99><div class="card-body p-3 small">This paper presents the NICT Kyoto submission for the WMT&#8217;21 Quality Estimation (QE) Critical Error Detection shared task (Task 3). Our approach relies mainly on QE model pretraining for which we used 11 language pairs, three sentence-level and three word-level translation quality metrics. Starting from an XLM-R checkpoint, we perform continued training by modifying the learning objective, switching from masked language modeling to QE oriented signals, before finetuning and ensembling the models. Results obtained on the test set in terms of <a href=https://en.wikipedia.org/wiki/Correlation_coefficient>correlation coefficient</a> and <a href=https://en.wikipedia.org/wiki/F-score>F-score</a> show that automatic metrics and synthetic data perform well for pretraining, with our submissions ranked first for two out of four language pairs. A deeper look at the impact of each metric on the downstream task indicates higher performance for token oriented metrics, while an ablation study emphasizes the usefulness of conducting both self-supervised and QE pretraining.</div></div><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.449.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--449 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.449 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.lrec-1.449" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.449/>Coursera Corpus Mining and Multistage Fine-Tuning for Improving Lectures Translation<span class=acl-fixed-case>C</span>oursera Corpus Mining and Multistage Fine-Tuning for Improving Lectures Translation</a></strong><br><a href=/people/h/haiyue-song/>Haiyue Song</a>
|
<a href=/people/r/raj-dabre/>Raj Dabre</a>
|
<a href=/people/a/atsushi-fujita/>Atsushi Fujita</a>
|
<a href=/people/s/sadao-kurohashi/>Sadao Kurohashi</a><br><a href=/volumes/2020.lrec-1/ class=text-muted>Proceedings of the 12th Language Resources and Evaluation Conference</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--449><div class="card-body p-3 small">Lectures translation is a case of <a href=https://en.wikipedia.org/wiki/Translation>spoken language translation</a> and there is a lack of publicly available <a href=https://en.wikipedia.org/wiki/Parallel_text>parallel corpora</a> for this purpose. To address this, we examine a framework for parallel corpus mining which is a quick and effective way to mine a parallel corpus from publicly available lectures at Coursera. Our approach determines sentence alignments, relying on <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> and <a href=https://en.wikipedia.org/wiki/Cosine_similarity>cosine similarity</a> over continuous-space sentence representations. We also show how to use the resulting <a href=https://en.wikipedia.org/wiki/Text_corpus>corpora</a> in a multistage fine-tuning based domain adaptation for high-quality lectures translation. For JapaneseEnglish lectures translation, we extracted parallel data of approximately 40,000 lines and created development and test sets through manual filtering for benchmarking translation performance. We demonstrate that the mined corpus greatly enhances the quality of translation when used in conjunction with out-of-domain parallel corpora via multistage training. This paper also suggests some guidelines to gather and clean corpora, mine parallel sentences, address noise in the mined data, and create high-quality evaluation splits. For the sake of reproducibility, we have released our code for parallel data creation.</div></div><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1146.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1146 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1146 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1146/>Exploiting <a href=https://en.wikipedia.org/wiki/Multilingualism>Multilingualism</a> through Multistage Fine-Tuning for Low-Resource Neural Machine Translation</a></strong><br><a href=/people/r/raj-dabre/>Raj Dabre</a>
|
<a href=/people/a/atsushi-fujita/>Atsushi Fujita</a>
|
<a href=/people/c/chenhui-chu/>Chenhui Chu</a><br><a href=/volumes/D19-1/ class=text-muted>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1146><div class="card-body p-3 small">This paper highlights the impressive utility of multi-parallel corpora for <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a> in a one-to-many low-resource neural machine translation (NMT) setting. We report on a systematic comparison of multistage fine-tuning configurations, consisting of (1) pre-training on an external large (209k440k) parallel corpus for <a href=https://en.wikipedia.org/wiki/English_language>English</a> and a helping target language, (2) mixed pre-training or fine-tuning on a mixture of the external and low-resource (18k) target parallel corpora, and (3) pure fine-tuning on the target parallel corpora. Our experiments confirm that multi-parallel corpora are extremely useful despite their scarcity and content-wise redundancy thus exhibiting the true power of <a href=https://en.wikipedia.org/wiki/Multilingualism>multilingualism</a>. Even when the helping target language is not one of the target languages of our concern, our multistage fine-tuning can give 39 BLEU score gains over a simple one-to-one model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5330.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5330 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5330 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5330/>NICT’s Unsupervised Neural and Statistical Machine Translation Systems for the WMT19 News Translation Task<span class=acl-fixed-case>NICT</span>’s Unsupervised Neural and Statistical Machine Translation Systems for the <span class=acl-fixed-case>WMT</span>19 News Translation Task</a></strong><br><a href=/people/b/benjamin-marie/>Benjamin Marie</a>
|
<a href=/people/h/haipeng-sun/>Haipeng Sun</a>
|
<a href=/people/r/rui-wang/>Rui Wang</a>
|
<a href=/people/k/kehai-chen/>Kehai Chen</a>
|
<a href=/people/a/atsushi-fujita/>Atsushi Fujita</a>
|
<a href=/people/m/masao-utiyama/>Masao Utiyama</a>
|
<a href=/people/e/eiichiro-sumita/>Eiichiro Sumita</a><br><a href=/volumes/W19-53/ class=text-muted>Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5330><div class="card-body p-3 small">This paper presents the NICT&#8217;s participation in the WMT19 unsupervised news translation task. We participated in the unsupervised translation direction : <a href=https://en.wikipedia.org/wiki/German_language>German-Czech</a>. Our primary submission to the task is the result of a simple combination of our unsupervised neural and statistical machine translation systems. Our system is ranked first for the German-to-Czech translation task, using only the data provided by the organizers (constraint&#8217;), according to both BLEU-cased and human evaluation. We also performed contrastive experiments with other language pairs, namely, English-Gujarati and English-Kazakh, to better assess the effectiveness of unsupervised machine translation in for distant language pairs and in truly low-resource conditions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1384.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1384 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1384 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-1384/>Unsupervised Extraction of Partial Translations for <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a></a></strong><br><a href=/people/b/benjamin-marie/>Benjamin Marie</a>
|
<a href=/people/a/atsushi-fujita/>Atsushi Fujita</a><br><a href=/volumes/N19-1/ class=text-muted>Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1384><div class="card-body p-3 small">In neural machine translation (NMT), monolingual data are usually exploited through a so-called <a href=https://en.wikipedia.org/wiki/Back-translation>back-translation</a> : sentences in the target language are translated into the source language to synthesize new parallel data. While this method provides more training data to better model the target language, on the source side, it only exploits translations that the NMT system is already able to generate using a <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> trained on existing parallel data. In this work, we assume that new translation knowledge can be extracted from <a href=https://en.wikipedia.org/wiki/Monolingualism>monolingual data</a>, without relying at all on existing parallel data. We propose a new <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> for extracting from monolingual data what we call partial translations : pairs of source and target sentences that contain sequences of tokens that are translations of each other. Our <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> is fully unsupervised and takes only source and target monolingual data as input. Our empirical evaluation points out that our partial translations can be used in combination with <a href=https://en.wikipedia.org/wiki/Back-translation>back-translation</a> to further improve NMT models. Furthermore, while partial translations are particularly useful for low-resource language pairs, they can also be successfully exploited in resource-rich scenarios to improve translation quality.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1312.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1312 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1312 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-1312/>Unsupervised Joint Training of Bilingual Word Embeddings</a></strong><br><a href=/people/b/benjamin-marie/>Benjamin Marie</a>
|
<a href=/people/a/atsushi-fujita/>Atsushi Fujita</a><br><a href=/volumes/P19-1/ class=text-muted>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1312><div class="card-body p-3 small">State-of-the-art methods for unsupervised bilingual word embeddings (BWE) train a <a href=https://en.wikipedia.org/wiki/Function_(mathematics)>mapping function</a> that maps pre-trained monolingual word embeddings into a bilingual space. Despite its remarkable results, unsupervised mapping is also well-known to be limited by the original dissimilarity between the word embedding spaces to be mapped. In this work, we propose a new approach that trains unsupervised BWE jointly on synthetic parallel data generated through unsupervised machine translation. We demonstrate that existing algorithms that jointly train BWE are very robust to noisy training data and show that unsupervised BWE jointly trained significantly outperform unsupervised mapped BWE in several cross-lingual NLP tasks.</div></div><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-2019.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-2019 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-2019 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-2019/>Semantic Features Based on Word Alignments for Estimating Quality of Text Simplification</a></strong><br><a href=/people/t/tomoyuki-kajiwara/>Tomoyuki Kajiwara</a>
|
<a href=/people/a/atsushi-fujita/>Atsushi Fujita</a><br><a href=/volumes/I17-2/ class=text-muted>Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-2019><div class="card-body p-3 small">This paper examines the usefulness of <a href=https://en.wikipedia.org/wiki/Semantic_feature>semantic features</a> based on <a href=https://en.wikipedia.org/wiki/Word_alignment>word alignments</a> for estimating the quality of text simplification. Specifically, we introduce seven types of alignment-based features computed on the basis of <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> and paraphrase lexicons. Through an empirical experiment using the QATS dataset, we confirm that we can achieve the state-of-the-art performance only with these <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2062.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2062 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2062 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-2062/>Efficient Extraction of Pseudo-Parallel Sentences from Raw Monolingual Data Using Word Embeddings</a></strong><br><a href=/people/b/benjamin-marie/>Benjamin Marie</a>
|
<a href=/people/a/atsushi-fujita/>Atsushi Fujita</a><br><a href=/volumes/P17-2/ class=text-muted>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2062><div class="card-body p-3 small">We propose a new method for extracting pseudo-parallel sentences from a pair of large monolingual corpora, without relying on any document-level information. Our method first exploits <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> in order to efficiently evaluate trillions of candidate sentence pairs and then a <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifier</a> to find the most reliable ones. We report significant improvements in <a href=https://en.wikipedia.org/wiki/Domain_adaptation>domain adaptation</a> for <a href=https://en.wikipedia.org/wiki/Statistical_machine_translation>statistical machine translation</a> when using a <a href=https://en.wikipedia.org/wiki/Machine_translation>translation model</a> trained on the sentence pairs extracted from in-domain monolingual corpora.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Atsushi+Fujita" title="Search for 'Atsushi Fujita' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/b/benjamin-marie/ class=align-middle>Benjamin Marie</a>
<span class="badge badge-secondary align-middle ml-2">6</span></li><li class=list-group-item><a href=/people/r/raj-dabre/ class=align-middle>Raj Dabre</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/r/raphael-rubino/ class=align-middle>Raphael Rubino</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/t/tomoyuki-kajiwara/ class=align-middle>Tomoyuki Kajiwara</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/rei-miyata/ class=align-middle>Rei Miyata</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/c/chenhui-chu/ class=align-middle>Chenhui Chu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/haipeng-sun/ class=align-middle>Haipeng Sun</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/rui-wang/ class=align-middle>Rui Wang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/kehai-chen/ class=align-middle>Kehai Chen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/masao-utiyama/ class=align-middle>Masao Utiyama</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/e/eiichiro-sumita/ class=align-middle>Eiichiro Sumita</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/haiyue-song/ class=align-middle>Haiyue Song</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/sadao-kurohashi/ class=align-middle>Sadao Kurohashi</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/acl/ class=align-middle>ACL</a><span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/venues/mtsummit/ class=align-middle>MTSummit</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/ijcnlp/ class=align-middle>IJCNLP</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/eacl/ class=align-middle>EACL</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/emnlp/ class=align-middle>EMNLP</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-venues aria-expanded=false aria-controls=more-venues>show all...</li><div class="collapse border-top" id=more-venues><li class=list-group-item><a href=/venues/wmt/ class=align-middle>WMT</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/ws/ class=align-middle>WS</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/naacl/ class=align-middle>NAACL</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/lrec/ class=align-middle>LREC</a><span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>