<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Anil Kumar Singh - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Anil Kumar</span> <span class=font-weight-bold>Singh</span></h2><p class="font-weight-light text-muted"><span class=font-italic>Also published as:</span>
Anil <span class=font-weight-normal>Kumar Singh</span>,
Anil kumar <span class=font-weight-normal>Singh</span></p><hr><div class=row><div class=col-lg-9><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wmt-1.126.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wmt-1--126 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wmt-1.126 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939625 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.wmt-1.126/>NLPRL System for Very Low Resource Supervised Machine Translation<span class=acl-fixed-case>NLPRL</span> System for Very Low Resource Supervised Machine Translation</a></strong><br><a href=/people/r/rupjyoti-baruah/>Rupjyoti Baruah</a>
|
<a href=/people/r/rajesh-kumar-mundotiya/>Rajesh Kumar Mundotiya</a>
|
<a href=/people/a/amit-kumar/>Amit Kumar</a>
|
<a href=/people/a/anil-kumar-singh/>Anil kumar Singh</a><br><a href=/volumes/2020.wmt-1/ class=text-muted>Proceedings of the Fifth Conference on Machine Translation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wmt-1--126><div class="card-body p-3 small">This paper describes the results of the system that we used for the WMT20 very low resource (VLR) supervised MT shared task. For our experiments, we use a byte-level version of BPE, which requires a base vocabulary of size 256 only. BPE based models are a kind of sub-word models. Such models try to address the Out of Vocabulary (OOV) word problem by performing <a href=https://en.wikipedia.org/wiki/Word_segmentation>word segmentation</a> so that segments correspond to morphological units. They are also reported to work across different languages, especially similar languages due to their sub-word nature. Based on BLEU cased score, our NLPRL systems ranked ninth for HSB to GER and tenth in GER to HSB translation scenario.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wnut-1.60.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wnut-1--60 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wnut-1.60 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.wnut-1.60/>NLPRL at WNUT-2020 Task 2 : ELMo-based System for Identification of COVID-19 Tweets<span class=acl-fixed-case>NLPRL</span> at <span class=acl-fixed-case>WNUT</span>-2020 Task 2: <span class=acl-fixed-case>ELM</span>o-based System for Identification of <span class=acl-fixed-case>COVID</span>-19 Tweets</a></strong><br><a href=/people/r/rajesh-kumar-mundotiya/>Rajesh Kumar Mundotiya</a>
|
<a href=/people/r/rupjyoti-baruah/>Rupjyoti Baruah</a>
|
<a href=/people/b/bhavana-srivastava/>Bhavana Srivastava</a>
|
<a href=/people/a/anil-kumar-singh/>Anil Kumar Singh</a><br><a href=/volumes/2020.wnut-1/ class=text-muted>Proceedings of the Sixth Workshop on Noisy User-generated Text (W-NUT 2020)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wnut-1--60><div class="card-body p-3 small">The Coronavirus pandemic has been a dominating news on <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> for the last many months. Efforts are being made to reduce its spread and reduce the casualties as well as new infections. For this purpose, the information about the infected people and their related symptoms, as available on <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>, such as <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a>, can help in <a href=https://en.wikipedia.org/wiki/Preventive_healthcare>prevention</a> and taking precautions. This is an example of using noisy text processing for <a href=https://en.wikipedia.org/wiki/Emergency_management>disaster management</a>. This paper discusses the NLPRL results in Shared Task-2 of WNUT-2020 workshop. We have considered this problem as a binary classification problem and have used a pre-trained ELMo embedding with GRU units. This approach helps classify the tweets with <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> as 80.85 % and 78.54 % as <a href=https://en.wikipedia.org/wiki/F-number>F1-score</a> on the provided test dataset. The experimental code is available online.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.loresmt-1.6.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--loresmt-1--6 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.loresmt-1.6 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.loresmt-1.6/>Unsupervised Approach for Zero-Shot Experiments : BhojpuriHindi and MagahiHindi@LoResMT 2020<span class=acl-fixed-case>B</span>hojpuri–<span class=acl-fixed-case>H</span>indi and <span class=acl-fixed-case>M</span>agahi–<span class=acl-fixed-case>H</span>indi@<span class=acl-fixed-case>L</span>o<span class=acl-fixed-case>R</span>es<span class=acl-fixed-case>MT</span> 2020</a></strong><br><a href=/people/a/amit-kumar/>Amit Kumar</a>
|
<a href=/people/r/rajesh-kumar-mundotiya/>Rajesh Kumar Mundotiya</a>
|
<a href=/people/a/anil-kumar-singh/>Anil Kumar Singh</a><br><a href=/volumes/2020.loresmt-1/ class=text-muted>Proceedings of the 3rd Workshop on Technologies for MT of Low Resource Languages</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--loresmt-1--6><div class="card-body p-3 small">This paper reports a Machine Translation (MT) system submitted by the NLPRL team for the BhojpuriHindi and MagahiHindi language pairs at LoResMT 2020 shared task. We used an unsupervised domain adaptation approach that gives promising results for zero or extremely low resource languages. Task organizers provide the development and the test sets for evaluation and the monolingual data for training. Our <a href=https://en.wikipedia.org/wiki/Software_development_process>approach</a> is a hybrid approach of <a href=https://en.wikipedia.org/wiki/Domain_adaptation>domain adaptation</a> and <a href=https://en.wikipedia.org/wiki/Translation_(biology)>back-translation</a>. Metrics used to evaluate the trained model are BLEU, RIBES, <a href=https://en.wikipedia.org/wiki/Precision_(statistics)>Precision</a>, Recall and <a href=https://en.wikipedia.org/wiki/F-measure>F-measure</a>. Our approach gives relatively promising results, with a wide range, of 19.5, 13.71, 2.54, and 3.16 BLEU points for <a href=https://en.wikipedia.org/wiki/Bhojpuri_language>Bhojpuri</a> to <a href=https://en.wikipedia.org/wiki/Hindi>Hindi</a>, Magahi to Hindi, <a href=https://en.wikipedia.org/wiki/Hindi>Hindi</a> to Bhojpuri and <a href=https://en.wikipedia.org/wiki/Hindi>Hindi to Magahi language pairs</a>, respectively.</div></div><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5222.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5222 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5222 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5222/>NLPRL at WAT2019 : Transformer-based Tamil English Indic Task Neural Machine Translation System<span class=acl-fixed-case>NLPRL</span> at <span class=acl-fixed-case>WAT</span>2019: Transformer-based <span class=acl-fixed-case>T</span>amil – <span class=acl-fixed-case>E</span>nglish Indic Task Neural Machine Translation System</a></strong><br><a href=/people/a/amit-kumar/>Amit Kumar</a>
|
<a href=/people/a/anil-kumar-singh/>Anil Kumar Singh</a><br><a href=/volumes/D19-52/ class=text-muted>Proceedings of the 6th Workshop on Asian Translation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5222><div class="card-body p-3 small">This paper describes the Machine Translation system for Tamil-English Indic Task organized at WAT 2019. We use Transformer- based architecture for <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a>.</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1247.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1247 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1247 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=C18-1247" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/C18-1247/>How emotional are you? Neural Architectures for Emotion Intensity Prediction in Microblogs</a></strong><br><a href=/people/d/devang-kulshreshtha/>Devang Kulshreshtha</a>
|
<a href=/people/p/pranav-goel/>Pranav Goel</a>
|
<a href=/people/a/anil-kumar-singh/>Anil Kumar Singh</a><br><a href=/volumes/C18-1/ class=text-muted>Proceedings of the 27th International Conference on Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1247><div class="card-body p-3 small">Social media based micro-blogging sites like <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a> have become a common source of real-time information (impacting organizations and their strategies, and are used for expressing emotions and opinions. Automated analysis of such <a href=https://en.wikipedia.org/wiki/Content_(media)>content</a> therefore rises in importance. To this end, we explore the viability of using <a href=https://en.wikipedia.org/wiki/Deep_learning>deep neural networks</a> on the specific task of emotion intensity prediction in <a href=https://en.wikipedia.org/wiki/Twitter>tweets</a>. We propose a neural architecture combining convolutional and fully connected layers in a non-sequential manner-done for the first time in context of natural language based tasks. Combined with lexicon-based features along with <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a>, our model achieves state-of-the-art performance, outperforming the previous <a href=https://en.wikipedia.org/wiki/System>system</a> by 0.044 or 4.4 % <a href=https://en.wikipedia.org/wiki/Pearson_correlation_coefficient>Pearson correlation</a> on the WASSA&#8217;17 EmoInt shared task dataset. We investigate the performance of deep multi-task learning models trained for all <a href=https://en.wikipedia.org/wiki/Emotion>emotions</a> at once in a unified architecture and get encouraging results. Experiments performed on evaluating correlation between emotion pairs offer interesting insights into the relationship between them.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0914.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0914 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0914 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0914/>Di-LSTM Contrast : A Deep Neural Network for Metaphor Detection<span class=acl-fixed-case>LSTM</span> Contrast : A Deep Neural Network for Metaphor Detection</a></strong><br><a href=/people/k/krishnkant-swarnkar/>Krishnkant Swarnkar</a>
|
<a href=/people/a/anil-kumar-singh/>Anil Kumar Singh</a><br><a href=/volumes/W18-09/ class=text-muted>Proceedings of the Workshop on Figurative Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0914><div class="card-body p-3 small">The contrast between the contextual and general meaning of a word serves as an important clue for detecting its <a href=https://en.wikipedia.org/wiki/Metaphor>metaphoricity</a>. In this paper, we present a <a href=https://en.wikipedia.org/wiki/Deep_learning>deep neural architecture</a> for metaphor detection which exploits this contrast. Additionally, we also use cost-sensitive learning by re-weighting examples, and baseline features like concreteness ratings, POS and WordNet-based features. The best performing <a href=https://en.wikipedia.org/wiki/System>system</a> of ours achieves an overall F1 score of 0.570 on All POS category and 0.605 on the Verbs category at the Metaphor Shared Task 2018.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3220.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3220 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3220 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3220/>IIT (BHU) Submission for the ACL Shared Task on <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>Named Entity Recognition</a> on Code-switched Data<span class=acl-fixed-case>IIT</span> (<span class=acl-fixed-case>BHU</span>) Submission for the <span class=acl-fixed-case>ACL</span> Shared Task on Named Entity Recognition on Code-switched Data</a></strong><br><a href=/people/s/shashwat-trivedi/>Shashwat Trivedi</a>
|
<a href=/people/h/harsh-rangwani/>Harsh Rangwani</a>
|
<a href=/people/a/anil-kumar-singh/>Anil Kumar Singh</a><br><a href=/volumes/W18-32/ class=text-muted>Proceedings of the Third Workshop on Computational Approaches to Linguistic Code-Switching</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3220><div class="card-body p-3 small">This paper describes the best performing system for the shared task on Named Entity Recognition (NER) on code-switched data for the language pair Spanish-English (ENG-SPA). We introduce a gated neural architecture for the NER task. Our final <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieves an <a href=https://en.wikipedia.org/wiki/F-number>F1 score</a> of 63.76 %, outperforming the <a href=https://en.wikipedia.org/wiki/Baseline_(medicine)>baseline</a> by 10 %.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6116.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6116 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6116 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6116/>Language Identification in Code-Mixed Data using Multichannel Neural Networks and Context Capture</a></strong><br><a href=/people/s/soumil-mandal/>Soumil Mandal</a>
|
<a href=/people/a/anil-kumar-singh/>Anil Kumar Singh</a><br><a href=/volumes/W18-61/ class=text-muted>Proceedings of the 2018 EMNLP Workshop W-NUT: The 4th Workshop on Noisy User-generated Text</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6116><div class="card-body p-3 small">An accurate language identification tool is an absolute necessity for building complex NLP systems to be used on code-mixed data. Lot of work has been recently done on the same, but there&#8217;s still room for improvement. Inspired from the recent advancements in neural network architectures for computer vision tasks, we have implemented multichannel neural networks combining CNN and LSTM for word level language identification of code-mixed data. Combining this with a Bi-LSTM-CRF context capture module, accuracies of 93.28 % and 93.32 % is achieved on our two testing sets.</div></div><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-4003.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-4003 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-4003 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-4003/>IJCNLP-2017 Task 3 : Review Opinion Diversification (RevOpiD-2017)<span class=acl-fixed-case>IJCNLP</span>-2017 Task 3: Review Opinion Diversification (<span class=acl-fixed-case>R</span>ev<span class=acl-fixed-case>O</span>pi<span class=acl-fixed-case>D</span>-2017)</a></strong><br><a href=/people/a/anil-kumar-singh/>Anil Kumar Singh</a>
|
<a href=/people/a/avijit-thawani/>Avijit Thawani</a>
|
<a href=/people/m/mayank-panchal/>Mayank Panchal</a>
|
<a href=/people/a/anubhav-gupta/>Anubhav Gupta</a>
|
<a href=/people/j/julian-mcauley/>Julian McAuley</a><br><a href=/volumes/I17-4/ class=text-muted>Proceedings of the IJCNLP 2017, Shared Tasks</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-4003><div class="card-body p-3 small">Unlike Entity Disambiguation in <a href=https://en.wikipedia.org/wiki/Web_search_engine>web search results</a>, Opinion Disambiguation is a relatively unexplored topic. RevOpiD shared task at IJCNLP-2107 aimed to attract attention towards this research problem. In this paper, we summarize the first run of this task and introduce a new dataset that we have annotated for the purpose of evaluating <a href=https://en.wikipedia.org/wiki/Opinion_mining>Opinion Mining</a>, Summarization and Disambiguation methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-0912.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-0912 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-0912 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-0912/>IIT (BHU): System Description for LSDSem’17 Shared Task<span class=acl-fixed-case>IIT</span> (<span class=acl-fixed-case>BHU</span>): System Description for <span class=acl-fixed-case>LSDS</span>em’17 Shared Task</a></strong><br><a href=/people/p/pranav-goel/>Pranav Goel</a>
|
<a href=/people/a/anil-kumar-singh/>Anil Kumar Singh</a><br><a href=/volumes/W17-09/ class=text-muted>Proceedings of the 2nd Workshop on Linking Models of Lexical, Sentential and Discourse-level Semantics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-0912><div class="card-body p-3 small">This paper describes an ensemble system submitted as part of the LSDSem Shared Task 2017-the Story Cloze Test. The main conclusion from our results is that an approach based on <a href=https://en.wikipedia.org/wiki/Semantic_similarity>semantic similarity</a> alone may not be enough for this task. We test various approaches and compare them with two ensemble systems. One is based on <a href=https://en.wikipedia.org/wiki/Voting>voting</a> and the other on <a href=https://en.wikipedia.org/wiki/Logistic_regression>logistic regression based classifier</a>. Our final <a href=https://en.wikipedia.org/wiki/System>system</a> is able to outperform the previous <a href=https://en.wikipedia.org/wiki/State_(computer_science)>state</a> of the art for the Story Cloze test. Another very interesting observation is the performance of sentiment based approach which works almost as well on its own as our final ensemble system.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Anil+Kumar+Singh" title="Search for 'Anil Kumar Singh' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/a/amit-kumar/ class=align-middle>Amit Kumar</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/r/rajesh-kumar-mundotiya/ class=align-middle>Rajesh Kumar Mundotiya</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/p/pranav-goel/ class=align-middle>Pranav Goel</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/r/rupjyoti-baruah/ class=align-middle>Rupjyoti Baruah</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/d/devang-kulshreshtha/ class=align-middle>Devang Kulshreshtha</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/a/avijit-thawani/ class=align-middle>Avijit Thawani</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/mayank-panchal/ class=align-middle>Mayank Panchal</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/anubhav-gupta/ class=align-middle>Anubhav Gupta</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/julian-mcauley/ class=align-middle>Julian McAuley</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/krishnkant-swarnkar/ class=align-middle>Krishnkant Swarnkar</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/shashwat-trivedi/ class=align-middle>Shashwat Trivedi</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/harsh-rangwani/ class=align-middle>Harsh Rangwani</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/soumil-mandal/ class=align-middle>Soumil Mandal</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/b/bhavana-srivastava/ class=align-middle>Bhavana Srivastava</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/ws/ class=align-middle>WS</a><span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/venues/coling/ class=align-middle>COLING</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/ijcnlp/ class=align-middle>IJCNLP</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/emnlp/ class=align-middle>EMNLP</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/wmt/ class=align-middle>WMT</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-venues aria-expanded=false aria-controls=more-venues>show all...</li><div class="collapse border-top" id=more-venues><li class=list-group-item><a href=/venues/wnut/ class=align-middle>WNUT</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/loresmt/ class=align-middle>loresmt</a><span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>