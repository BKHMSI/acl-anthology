<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Alessandro Raganato - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Alessandro</span> <span class=font-weight-bold>Raganato</span></h2><hr><div class=row><div class=col-lg-9><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4305.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4305 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4305 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4305/>Multilingual NMT with a Language-Independent Attention Bridge<span class=acl-fixed-case>NMT</span> with a Language-Independent Attention Bridge</a></strong><br><a href=/people/r/raul-vazquez/>Raúl Vázquez</a>
|
<a href=/people/a/alessandro-raganato/>Alessandro Raganato</a>
|
<a href=/people/j/jorg-tiedemann/>Jörg Tiedemann</a>
|
<a href=/people/m/mathias-creutz/>Mathias Creutz</a><br><a href=/volumes/W19-43/ class=text-muted>Proceedings of the 4th Workshop on Representation Learning for NLP (RepL4NLP-2019)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4305><div class="card-body p-3 small">In this paper, we propose an architecture for <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation (MT)</a> capable of obtaining multilingual sentence representations by incorporating an intermediate attention bridge that is shared across all languages. We train the model with language-specific encoders and decoders that are connected through an inner-attention layer on the encoder side. The attention bridge exploits the <a href=https://en.wikipedia.org/wiki/Semantics>semantics</a> from each language for <a href=https://en.wikipedia.org/wiki/Translation>translation</a> and develops into a language-agnostic meaning representation that can efficiently be used for <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a>. We present a new <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> for the efficient development of multilingual neural machine translation (NMT) using this model and scheduled training. We have tested the approach in a systematic way with a multi-parallel data set. The <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieves substantial improvements over strong bilingual models and performs well for zero-shot translation, which demonstrates its ability of abstraction and <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5347.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5347 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5347 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5347/>The University of Helsinki Submissions to the WMT19 News Translation Task<span class=acl-fixed-case>U</span>niversity of <span class=acl-fixed-case>H</span>elsinki Submissions to the <span class=acl-fixed-case>WMT</span>19 News Translation Task</a></strong><br><a href=/people/a/aarne-talman/>Aarne Talman</a>
|
<a href=/people/u/umut-sulubacak/>Umut Sulubacak</a>
|
<a href=/people/r/raul-vazquez/>Raúl Vázquez</a>
|
<a href=/people/y/yves-scherrer/>Yves Scherrer</a>
|
<a href=/people/s/sami-virpioja/>Sami Virpioja</a>
|
<a href=/people/a/alessandro-raganato/>Alessandro Raganato</a>
|
<a href=/people/a/arvi-hurskainen/>Arvi Hurskainen</a>
|
<a href=/people/j/jorg-tiedemann/>Jörg Tiedemann</a><br><a href=/volumes/W19-53/ class=text-muted>Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5347><div class="card-body p-3 small">In this paper we present the University of Helsinki submissions to the WMT 2019 shared news translation task in three language pairs : <a href=https://en.wikipedia.org/wiki/German_language>English-German</a>, <a href=https://en.wikipedia.org/wiki/Finnish_language>English-Finnish</a> and <a href=https://en.wikipedia.org/wiki/Finnish_language>Finnish-English</a>. This year we focused first on cleaning and filtering the training data using multiple data-filtering approaches, resulting in much smaller and cleaner training sets. For <a href=https://en.wikipedia.org/wiki/German_language>English-German</a> we trained both sentence-level transformer models as well as compared different document-level translation approaches. For Finnish-English and English-Finnish we focused on different segmentation approaches and we also included a rule-based system for English-Finnish.</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5431.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5431 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5431 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5431/>An Analysis of Encoder Representations in Transformer-Based Machine Translation</a></strong><br><a href=/people/a/alessandro-raganato/>Alessandro Raganato</a>
|
<a href=/people/j/jorg-tiedemann/>Jörg Tiedemann</a><br><a href=/volumes/W18-54/ class=text-muted>Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5431><div class="card-body p-3 small">The attention mechanism is a successful technique in modern <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a>, especially in tasks like <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>. The recently proposed <a href=https://en.wikipedia.org/wiki/Network_architecture>network architecture</a> of the Transformer is based entirely on <a href=https://en.wikipedia.org/wiki/Attentional_control>attention mechanisms</a> and achieves new state of the art results in <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a>, outperforming other sequence-to-sequence models. However, so far not much is known about the internal properties of the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> and the representations it learns to achieve that performance. To study this question, we investigate the information that is learned by the <a href=https://en.wikipedia.org/wiki/Attentional_control>attention mechanism</a> in Transformer models with different translation quality. We assess the representations of the encoder by extracting dependency relations based on self-attention weights, we perform four probing tasks to study the amount of syntactic and semantic captured information and we also test attention in a transfer learning scenario. Our analysis sheds light on the relative strengths and weaknesses of the various encoder representations. We observe that specific attention heads mark syntactic dependency relations and we can also confirm that lower layers tend to learn more about <a href=https://en.wikipedia.org/wiki/Syntax>syntax</a> while higher layers tend to encode more <a href=https://en.wikipedia.org/wiki/Semantics>semantics</a>.<i>Transformer</i> is based entirely on attention mechanisms and achieves new state of the art results in neural machine translation, outperforming other sequence-to-sequence models. However, so far not much is known about the internal properties of the model and the representations it learns to achieve that performance. To study this question, we investigate the information that is learned by the attention mechanism in Transformer models with different translation quality. We assess the representations of the encoder by extracting dependency relations based on self-attention weights, we perform four probing tasks to study the amount of syntactic and semantic captured information and we also test attention in a transfer learning scenario. Our analysis sheds light on the relative strengths and weaknesses of the various encoder representations. We observe that specific attention heads mark syntactic dependency relations and we can also confirm that lower layers tend to learn more about syntax while higher layers tend to encode more semantics.</div></div><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2094.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2094 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2094 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-2094/>EuroSense : Automatic Harvesting of Multilingual Sense Annotations from Parallel Text<span class=acl-fixed-case>E</span>uro<span class=acl-fixed-case>S</span>ense: Automatic Harvesting of Multilingual Sense Annotations from Parallel Text</a></strong><br><a href=/people/c/claudio-delli-bovi/>Claudio Delli Bovi</a>
|
<a href=/people/j/jose-camacho-collados/>Jose Camacho-Collados</a>
|
<a href=/people/a/alessandro-raganato/>Alessandro Raganato</a>
|
<a href=/people/r/roberto-navigli/>Roberto Navigli</a><br><a href=/volumes/P17-2/ class=text-muted>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2094><div class="card-body p-3 small">Parallel corpora are widely used in a variety of Natural Language Processing tasks, from <a href=https://en.wikipedia.org/wiki/Machine_translation>Machine Translation</a> to cross-lingual Word Sense Disambiguation, where parallel sentences can be exploited to automatically generate high-quality sense annotations on a large scale. In this paper we present EuroSense, a multilingual sense-annotated resource based on the joint disambiguation of the Europarl parallel corpus, with almost 123 million sense annotations for over 155 thousand distinct concepts and entities from a language-independent unified sense inventory. We evaluate the quality of our sense annotations intrinsically and extrinsically, showing their effectiveness as training data for <a href=https://en.wikipedia.org/wiki/Word-sense_disambiguation>Word Sense Disambiguation</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S17-2041.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S17-2041 data-toggle=collapse aria-expanded=false aria-controls=abstract-S17-2041 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/S17-2041/>Sew-Embed at SemEval-2017 Task 2 : Language-Independent Concept Representations from a Semantically Enriched Wikipedia<span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2017 Task 2: Language-Independent Concept Representations from a Semantically Enriched <span class=acl-fixed-case>W</span>ikipedia</a></strong><br><a href=/people/c/claudio-delli-bovi/>Claudio Delli Bovi</a>
|
<a href=/people/a/alessandro-raganato/>Alessandro Raganato</a><br><a href=/volumes/S17-2/ class=text-muted>Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S17-2041><div class="card-body p-3 small">This paper describes Sew-Embed, our language-independent approach to multilingual and cross-lingual semantic word similarity as part of the SemEval-2017 Task 2. We leverage the Wikipedia-based concept representations developed by Raganato et al. (2016), and propose an embedded augmentation of their explicit high-dimensional vectors, which we obtain by plugging in an arbitrary word (or sense) embedding representation, and computing a <a href=https://en.wikipedia.org/wiki/Weighted_arithmetic_mean>weighted average</a> in the continuous vector space. We evaluate Sew-Embed with two different off-the-shelf embedding representations, and report their performances across all monolingual and cross-lingual benchmarks available for the task. Despite its simplicity, especially compared with supervised or overly tuned approaches, Sew-Embed achieves competitive results in the cross-lingual setting (3rd best result in the global ranking of subtask 2, score 0.56).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1120.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1120 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1120 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1120/>Neural Sequence Learning Models for Word Sense Disambiguation</a></strong><br><a href=/people/a/alessandro-raganato/>Alessandro Raganato</a>
|
<a href=/people/c/claudio-delli-bovi/>Claudio Delli Bovi</a>
|
<a href=/people/r/roberto-navigli/>Roberto Navigli</a><br><a href=/volumes/D17-1/ class=text-muted>Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1120><div class="card-body p-3 small">Word Sense Disambiguation models exist in many flavors. Even though supervised ones tend to perform best in terms of <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>, they often lose ground to more flexible knowledge-based solutions, which do not require training by a word expert for every <a href=https://en.wikipedia.org/wiki/Word-sense_disambiguation>disambiguation target</a>. To bridge this gap we adopt a different perspective and rely on <a href=https://en.wikipedia.org/wiki/Sequence_learning>sequence learning</a> to frame the disambiguation problem : we propose and study in depth a series of end-to-end neural architectures directly tailored to the task, from bidirectional Long Short-Term Memory to encoder-decoder models. Our extensive evaluation over standard benchmarks and in multiple languages shows that <a href=https://en.wikipedia.org/wiki/Sequence_learning>sequence learning</a> enables more versatile all-words models that consistently lead to state-of-the-art results, even against word experts with engineered features.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-2018.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-2018 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-2018 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D17-2018" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D17-2018/>SupWSD : A Flexible Toolkit for Supervised Word Sense Disambiguation<span class=acl-fixed-case>S</span>up<span class=acl-fixed-case>WSD</span>: A Flexible Toolkit for Supervised Word Sense Disambiguation</a></strong><br><a href=/people/s/simone-papandrea/>Simone Papandrea</a>
|
<a href=/people/a/alessandro-raganato/>Alessandro Raganato</a>
|
<a href=/people/c/claudio-delli-bovi/>Claudio Delli Bovi</a><br><a href=/volumes/D17-2/ class=text-muted>Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-2018><div class="card-body p-3 small">In this demonstration we present SupWSD, a <a href=https://en.wikipedia.org/wiki/Java_API>Java API</a> for supervised Word Sense Disambiguation (WSD). This toolkit includes the implementation of a state-of-the-art supervised WSD system, together with a Natural Language Processing pipeline for <a href=https://en.wikipedia.org/wiki/Data_preprocessing>preprocessing</a> and <a href=https://en.wikipedia.org/wiki/Feature_extraction>feature extraction</a>. Our aim is to provide an easy-to-use tool for the research community, designed to be modular, fast and scalable for training and testing on large datasets. The source code of SupWSD is available at.<url>http://github.com/SI3P/SupWSD</url>.\n</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1010.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1010 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1010 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1010/>Word Sense Disambiguation : A Unified Evaluation Framework and Empirical Comparison</a></strong><br><a href=/people/a/alessandro-raganato/>Alessandro Raganato</a>
|
<a href=/people/j/jose-camacho-collados/>Jose Camacho-Collados</a>
|
<a href=/people/r/roberto-navigli/>Roberto Navigli</a><br><a href=/volumes/E17-1/ class=text-muted>Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1010><div class="card-body p-3 small">Word Sense Disambiguation is a long-standing task in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>Natural Language Processing</a>, lying at the core of <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>human language understanding</a>. However, the evaluation of <a href=https://en.wikipedia.org/wiki/Automation>automatic systems</a> has been problematic, mainly due to the lack of a reliable evaluation framework. In this paper we develop a unified evaluation framework and analyze the performance of various Word Sense Disambiguation systems in a fair setup. The results show that <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised systems</a> clearly outperform knowledge-based models. Among the <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised systems</a>, a <a href=https://en.wikipedia.org/wiki/Linear_classifier>linear classifier</a> trained on conventional local features still proves to be a hard baseline to beat. Nonetheless, recent approaches exploiting <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a> on unlabeled corpora achieve promising results, surpassing this hard baseline in most test sets.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Alessandro+Raganato" title="Search for 'Alessandro Raganato' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/c/claudio-delli-bovi/ class=align-middle>Claudio Delli Bovi</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/people/r/roberto-navigli/ class=align-middle>Roberto Navigli</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/j/jorg-tiedemann/ class=align-middle>Jörg Tiedemann</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/j/jose-camacho-collados/ class=align-middle>Jose Camacho-Collados</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/r/raul-vazquez/ class=align-middle>Raúl Vázquez</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/s/simone-papandrea/ class=align-middle>Simone Papandrea</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/mathias-creutz/ class=align-middle>Mathias Creutz</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/aarne-talman/ class=align-middle>Aarne Talman</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/u/umut-sulubacak/ class=align-middle>Umut Sulubacak</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yves-scherrer/ class=align-middle>Yves Scherrer</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/sami-virpioja/ class=align-middle>Sami Virpioja</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/arvi-hurskainen/ class=align-middle>Arvi Hurskainen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/ws/ class=align-middle>WS</a><span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/venues/emnlp/ class=align-middle>EMNLP</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/acl/ class=align-middle>ACL</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/semeval/ class=align-middle>SemEval</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/eacl/ class=align-middle>EACL</a><span class="badge badge-secondary align-middle ml-2">1</span></li></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>