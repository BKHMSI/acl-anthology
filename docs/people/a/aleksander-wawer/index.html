<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Aleksander Wawer - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Aleksander</span> <span class=font-weight-bold>Wawer</span></h2><hr><div class=row><div class=col-lg-9><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.ranlp-1.169.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--ranlp-1--169 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.ranlp-1.169 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.ranlp-1.169/>ComboNER : A Lightweight All-In-One POS Tagger, Dependency Parser and NER<span class=acl-fixed-case>C</span>ombo<span class=acl-fixed-case>NER</span>: A Lightweight All-In-One <span class=acl-fixed-case>POS</span> Tagger, Dependency Parser and <span class=acl-fixed-case>NER</span></a></strong><br><a href=/people/a/aleksander-wawer/>Aleksander Wawer</a><br><a href=/volumes/2021.ranlp-1/ class=text-muted>Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2021)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--ranlp-1--169><div class="card-body p-3 small">The current <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a> is strongly focused on raising <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>. The progress comes at a cost of super-heavy models with hundreds of millions or even billions of parameters. However, simple syntactic tasks such as part-of-speech (POS) tagging, dependency parsing or named entity recognition (NER) do not require the largest models to achieve acceptable results. In line with this assumption we try to minimize the size of the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> that jointly performs all three <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a>. We introduce ComboNER : a lightweight tool, orders of magnitude smaller than state-of-the-art <a href=https://en.wikipedia.org/wiki/Transformer>transformers</a>. It is based on pre-trained subword embeddings and <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent neural network architecture</a>. ComboNER operates on <a href=https://en.wikipedia.org/wiki/Polish_language>Polish language data</a>. The <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> has outputs for <a href=https://en.wikipedia.org/wiki/Tag_(metadata)>POS tagging</a>, dependency parsing and <a href=https://en.wikipedia.org/wiki/Non-return-to-zero>NER</a>. Our paper contains some insights from fine-tuning of the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> and reports its overall results.</div></div><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5409.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5409 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5409 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5409/>SAMSum Corpus : A Human-annotated Dialogue Dataset for Abstractive Summarization<span class=acl-fixed-case>SAMS</span>um Corpus: A Human-annotated Dialogue Dataset for Abstractive Summarization</a></strong><br><a href=/people/b/bogdan-gliwa/>Bogdan Gliwa</a>
|
<a href=/people/i/iwona-mochol/>Iwona Mochol</a>
|
<a href=/people/m/maciej-biesek/>Maciej Biesek</a>
|
<a href=/people/a/aleksander-wawer/>Aleksander Wawer</a><br><a href=/volumes/D19-54/ class=text-muted>Proceedings of the 2nd Workshop on New Frontiers in Summarization</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5409><div class="card-body p-3 small">This paper introduces the SAMSum Corpus, a new <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> with abstractive dialogue summaries. We investigate the challenges it poses for <a href=https://en.wikipedia.org/wiki/Automatic_summarization>automated summarization</a> by testing several <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> and comparing their results with those obtained on a <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus of news articles</a>. We show that model-generated summaries of dialogues achieve higher ROUGE scores than the model-generated summaries of news in contrast with human evaluators&#8217; judgement. This suggests that a challenging task of abstractive dialogue summarization requires dedicated <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> and non-standard <a href=https://en.wikipedia.org/wiki/Quality_(business)>quality measures</a>. To our knowledge, our study is the first attempt to introduce a high-quality chat-dialogues corpus, manually annotated with abstractive summarizations, which can be used by the research community for further studies.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/R19-1151.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-R19-1151 data-toggle=collapse aria-expanded=false aria-controls=abstract-R19-1151 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/R19-1151/>Predicting Sentiment of Polish Language Short Texts<span class=acl-fixed-case>P</span>olish Language Short Texts</a></strong><br><a href=/people/a/aleksander-wawer/>Aleksander Wawer</a>
|
<a href=/people/j/julita-sobiczewska/>Julita Sobiczewska</a><br><a href=/volumes/R19-1/ class=text-muted>Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2019)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-R19-1151><div class="card-body p-3 small">The goal of this paper is to use all available Polish language data sets to seek the best possible performance in supervised sentiment analysis of short texts. We use text collections with labelled sentiment such as <a href=https://en.wikipedia.org/wiki/Twitter>tweets</a>, <a href=https://en.wikipedia.org/wiki/Film_criticism>movie reviews</a> and a sentiment treebank, in three comparison modes. In the first, we examine the performance of <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> trained and tested on the same <a href=https://en.wikipedia.org/wiki/Text_corpus>text collection</a> using standard cross-validation (in-domain). In the second we train <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> on all available <a href=https://en.wikipedia.org/wiki/Data>data</a> except the given test collection, which we use for testing (one vs rest cross-domain). In the third, we train a <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> on one <a href=https://en.wikipedia.org/wiki/Data_set>data set</a> and apply it to another one (one vs one cross-domain). We compare wide range of methods including <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning</a> on bag-of-words representation, bidirectional recurrent neural networks as well as the most recent pre-trained architectures <a href=https://en.wikipedia.org/wiki/ELMO>ELMO</a> and BERT. We formulate conclusions as to cross-domain and in-domain performance of each <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>method</a>. Unsurprisingly, BERT turned out to be a strong performer, especially in the cross-domain setting. What is surprising however, is solid performance of the relatively simple multinomial Naive Bayes classifier, which performed equally well as BERT on several data sets.</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0916.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0916 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0916 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0916/>Detecting Figurative Word Occurrences Using <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>Recurrent Neural Networks</a></a></strong><br><a href=/people/a/agnieszka-mykowiecka/>Agnieszka Mykowiecka</a>
|
<a href=/people/a/aleksander-wawer/>Aleksander Wawer</a>
|
<a href=/people/m/malgorzata-marciniak/>Malgorzata Marciniak</a><br><a href=/volumes/W18-09/ class=text-muted>Proceedings of the Workshop on Figurative Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0916><div class="card-body p-3 small">The paper addresses detection of figurative usage of words in <a href=https://en.wikipedia.org/wiki/English_language>English text</a>. The chosen method was to use <a href=https://en.wikipedia.org/wiki/Artificial_neural_network>neural nets</a> fed by pretrained word embeddings. The obtained results show that simple <a href=https://en.wikipedia.org/wiki/Solution_concept>solutions</a>, based on <a href=https://en.wikipedia.org/wiki/Word_embedding>words embeddings</a> only, are comparable to complex solutions, using many sources of information which are not available for languages less-studied than <a href=https://en.wikipedia.org/wiki/English_language>English</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0917.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0917 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0917 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0917/>Multi-Module Recurrent Neural Networks with Transfer Learning</a></strong><br><a href=/people/f/filip-skurniak/>Filip Skurniak</a>
|
<a href=/people/m/maria-janicka/>Maria Janicka</a>
|
<a href=/people/a/aleksander-wawer/>Aleksander Wawer</a><br><a href=/volumes/W18-09/ class=text-muted>Proceedings of the Workshop on Figurative Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0917><div class="card-body p-3 small">This paper describes multiple solutions designed and tested for the problem of word-level metaphor detection. The proposed systems are all based on variants of recurrent neural network architectures. Specifically, we explore multiple sources of information : pre-trained word embeddings (Glove), a dictionary of language concreteness and a transfer learning scenario based on the states of an encoder network from neural network machine translation system. One of the architectures is based on combining all three systems : (1) Neural CRF (Conditional Random Fields), trained directly on the metaphor data set ; (2) Neural Machine Translation encoder of a transfer learning scenario ; (3) a neural network used to predict final labels, trained directly on the metaphor data set. Our results vary between test sets : Neural CRF standalone is the best one on submission data, while combined system scores the highest on a test subset randomly selected from training data.</div></div><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1915.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1915 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1915 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1915/>Supervised and Unsupervised Word Sense Disambiguation on Word Embedding Vectors of Unambigous Synonyms</a></strong><br><a href=/people/a/aleksander-wawer/>Aleksander Wawer</a>
|
<a href=/people/a/agnieszka-mykowiecka/>Agnieszka Mykowiecka</a><br><a href=/volumes/W17-19/ class=text-muted>Proceedings of the 1st Workshop on Sense, Concept and Entity Representations and their Applications</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1915><div class="card-body p-3 small">This paper compares two approaches to <a href=https://en.wikipedia.org/wiki/Word-sense_disambiguation>word sense disambiguation</a> using <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> trained on unambiguous synonyms. The first is <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised method</a> based on computing log probability from sequences of word embedding vectors, taking into account ambiguous word senses and guessing correct sense from context. The second method is supervised. We use a multilayer neural network model to learn a context-sensitive transformation that maps an input vector of ambiguous word into an output vector representing its sense. We evaluate both methods on corpora with manual annotations of word senses from the Polish wordnet (plWordnet).</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Aleksander+Wawer" title="Search for 'Aleksander Wawer' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/a/agnieszka-mykowiecka/ class=align-middle>Agnieszka Mykowiecka</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/b/bogdan-gliwa/ class=align-middle>Bogdan Gliwa</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/i/iwona-mochol/ class=align-middle>Iwona Mochol</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/maciej-biesek/ class=align-middle>Maciej Biesek</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/malgorzata-marciniak/ class=align-middle>Malgorzata Marciniak</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/f/filip-skurniak/ class=align-middle>Filip Skurniak</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/maria-janicka/ class=align-middle>Maria Janicka</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/julita-sobiczewska/ class=align-middle>Julita Sobiczewska</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/ws/ class=align-middle>WS</a><span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/venues/ranlp/ class=align-middle>RANLP</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/emnlp/ class=align-middle>EMNLP</a><span class="badge badge-secondary align-middle ml-2">1</span></li></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>