<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Ajay Nagesh - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Ajay</span> <span class=font-weight-bold>Nagesh</span></h2><hr><div class=row><div class=col-lg-9><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-demo.23.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-demo--23 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-demo.23 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-demo.23/>MeetDot : <a href=https://en.wikipedia.org/wiki/Videotelephony>Videoconferencing</a> with Live Translation Captions<span class=acl-fixed-case>M</span>eet<span class=acl-fixed-case>D</span>ot: Videoconferencing with Live Translation Captions</a></strong><br><a href=/people/a/arkady-arkhangorodsky/>Arkady Arkhangorodsky</a>
|
<a href=/people/c/christopher-chu/>Christopher Chu</a>
|
<a href=/people/s/scot-fang/>Scot Fang</a>
|
<a href=/people/y/yiqi-huang/>Yiqi Huang</a>
|
<a href=/people/d/denglin-jiang/>Denglin Jiang</a>
|
<a href=/people/a/ajay-nagesh/>Ajay Nagesh</a>
|
<a href=/people/b/boliang-zhang/>Boliang Zhang</a>
|
<a href=/people/k/kevin-knight/>Kevin Knight</a><br><a href=/volumes/2021.emnlp-demo/ class=text-muted>Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-demo--23><div class="card-body p-3 small">We present MeetDot, a videoconferencing system with live translation captions overlaid on screen. The <a href=https://en.wikipedia.org/wiki/System>system</a> aims to facilitate conversation between people who speak different languages, thereby reducing <a href=https://en.wikipedia.org/wiki/Language_barrier>communication barriers</a> between multilingual participants. Currently, our system supports <a href=https://en.wikipedia.org/wiki/Speech>speech</a> and captions in 4 languages and combines <a href=https://en.wikipedia.org/wiki/Speech_recognition>automatic speech recognition (ASR)</a> and <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation (MT)</a> in a cascade. We use the re-translation strategy to translate the streamed speech, resulting in caption flicker. Additionally, our <a href=https://en.wikipedia.org/wiki/System>system</a> has very strict latency requirements to have acceptable call quality. We implement several features to enhance <a href=https://en.wikipedia.org/wiki/User_experience>user experience</a> and reduce their <a href=https://en.wikipedia.org/wiki/Cognitive_load>cognitive load</a>, such as smooth scrolling captions and reducing caption flicker. The modular architecture allows us to integrate different ASR and MT services in our backend. Our system provides an integrated evaluation suite to optimize key intrinsic evaluation metrics such as <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>, <a href=https://en.wikipedia.org/wiki/Latency_(engineering)>latency</a> and erasure. Finally, we present an innovative cross-lingual word-guessing game as an extrinsic evaluation metric to measure end-to-end system performance. We plan to make our system open-source for research purposes.</div></div><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-1504.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-1504 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-1504 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-1504/>Lightly-supervised Representation Learning with Global Interpretability</a></strong><br><a href=/people/a/andrew-zupon/>Andrew Zupon</a>
|
<a href=/people/m/maria-alexeeva/>Maria Alexeeva</a>
|
<a href=/people/m/marco-valenzuela-escarcega/>Marco Valenzuela-Esc√°rcega</a>
|
<a href=/people/a/ajay-nagesh/>Ajay Nagesh</a>
|
<a href=/people/m/mihai-surdeanu/>Mihai Surdeanu</a><br><a href=/volumes/W19-15/ class=text-muted>Proceedings of the Third Workshop on Structured Prediction for NLP</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-1504><div class="card-body p-3 small">We propose a lightly-supervised approach for <a href=https://en.wikipedia.org/wiki/Information_extraction>information extraction</a>, in particular named entity classification, which combines the benefits of traditional <a href=https://en.wikipedia.org/wiki/Bootstrapping_(statistics)>bootstrapping</a>, i.e., use of limited annotations and interpretability of extraction patterns, with the robust learning approaches proposed in <a href=https://en.wikipedia.org/wiki/Representation_learning>representation learning</a>. Our <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> iteratively learns custom embeddings for both the multi-word entities to be extracted and the patterns that match them from a few example entities per category. We demonstrate that this representation-based approach outperforms three other state-of-the-art bootstrapping approaches on two datasets : CoNLL-2003 and OntoNotes. Additionally, using these embeddings, our approach outputs a globally-interpretable model consisting of a <a href=https://en.wikipedia.org/wiki/Decision_list>decision list</a>, by ranking patterns based on their proximity to the average entity embedding in a given class. We show that this interpretable model performs close to our complete bootstrapping model, proving that <a href=https://en.wikipedia.org/wiki/Representation_learning>representation learning</a> can be used to produce interpretable models with small loss in performance. This <a href=https://en.wikipedia.org/wiki/Decision_list>decision list</a> can be edited by human experts to mitigate some of that loss and in some cases outperform the original <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-1505.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-1505 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-1505 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-1505/>Semi-Supervised Teacher-Student Architecture for Relation Extraction</a></strong><br><a href=/people/f/fan-luo/>Fan Luo</a>
|
<a href=/people/a/ajay-nagesh/>Ajay Nagesh</a>
|
<a href=/people/r/rebecca-sharp/>Rebecca Sharp</a>
|
<a href=/people/m/mihai-surdeanu/>Mihai Surdeanu</a><br><a href=/volumes/W19-15/ class=text-muted>Proceedings of the Third Workshop on Structured Prediction for NLP</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-1505><div class="card-body p-3 small">Generating a large amount of training data for information extraction (IE) is either costly (if annotations are created manually), or runs the risk of introducing noisy instances (if distant supervision is used). On the other hand, semi-supervised learning (SSL) is a cost-efficient solution to combat lack of training data. In this paper, we adapt <a href=https://en.wikipedia.org/wiki/Mean_Teacher>Mean Teacher</a> (Tarvainen and Valpola, 2017), a denoising SSL framework to extract semantic relations between pairs of entities. We explore the sweet spot of amount of supervision required for good performance on this binary relation extraction task. Additionally, different syntax representations are incorporated into our <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> to enhance the learned representation of sentences. We evaluate our approach on the Google-IISc Distant Supervision (GDS) dataset, which removes test data noise present in all previous distance supervision datasets, which makes it a reliable evaluation benchmark (Jat et al., 2017). Our results show that the SSL Mean Teacher approach nears the performance of fully-supervised approaches even with only 10 % of the labeled corpus. Further, the syntax-aware model outperforms other syntax-free approaches across all levels of <a href=https://en.wikipedia.org/wiki/Supervisor>supervision</a>.</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1196.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1196 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1196 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1196/>An Exploration of Three Lightly-supervised Representation Learning Approaches for Named Entity Classification</a></strong><br><a href=/people/a/ajay-nagesh/>Ajay Nagesh</a>
|
<a href=/people/m/mihai-surdeanu/>Mihai Surdeanu</a><br><a href=/volumes/C18-1/ class=text-muted>Proceedings of the 27th International Conference on Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1196><div class="card-body p-3 small">Several semi-supervised representation learning methods have been proposed recently that mitigate the drawbacks of traditional <a href=https://en.wikipedia.org/wiki/Bootstrapping_(statistics)>bootstrapping</a> : they reduce the amount of <a href=https://en.wikipedia.org/wiki/Semantic_drift>semantic drift</a> introduced by iterative approaches through <a href=https://en.wikipedia.org/wiki/One-shot_learning>one-shot learning</a> ; others address the sparsity of data through the learning of custom, dense representation for the information modeled. In this work, we are the first to adapt three of these methods, most of which have been originally proposed for <a href=https://en.wikipedia.org/wiki/Digital_image_processing>image processing</a>, to an information extraction task, specifically, named entity classification. Further, we perform a rigorous comparative analysis on two distinct datasets. Our analysis yields several important observations. First, all <a href=https://en.wikipedia.org/wiki/Representation_learning>representation learning methods</a> outperform state-of-the-art <a href=https://en.wikipedia.org/wiki/Semi-supervised_learning>semi-supervised methods</a> that do not rely on <a href=https://en.wikipedia.org/wiki/Representation_learning>representation learning</a>. To the best of our knowledge, we report the latest state-of-the-art results on the semi-supervised named entity classification task. Second, one-shot learning methods clearly outperform iterative representation learning approaches. Lastly, one of the best performers relies on the mean teacher framework (Tarvainen and Valpola, 2017), a simple teacher / student approach that is independent of the underlying task-specific model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2057.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2057 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2057 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-2057/>Keep Your Bearings : Lightly-Supervised Information Extraction with <a href=https://en.wikipedia.org/wiki/Ladder_network>Ladder Networks</a> That Avoids <a href=https://en.wikipedia.org/wiki/Semantic_drift>Semantic Drift</a></a></strong><br><a href=/people/a/ajay-nagesh/>Ajay Nagesh</a>
|
<a href=/people/m/mihai-surdeanu/>Mihai Surdeanu</a><br><a href=/volumes/N18-2/ class=text-muted>Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2057><div class="card-body p-3 small">We propose a novel approach to <a href=https://en.wikipedia.org/wiki/Semi-supervised_learning>semi-supervised learning</a> for <a href=https://en.wikipedia.org/wiki/Information_extraction>information extraction</a> that uses <a href=https://en.wikipedia.org/wiki/Ladder_network>ladder networks</a> (Rasmus et al., 2015). In particular, we focus on the task of named entity classification, defined as identifying the correct label (e.g., person or organization name) of an entity mention in a given context. Our approach is simple, efficient and has the benefit of being robust to <a href=https://en.wikipedia.org/wiki/Semantic_drift>semantic drift</a>, a dominant problem in most <a href=https://en.wikipedia.org/wiki/Semi-supervised_learning>semi-supervised learning systems</a>. We empirically demonstrate the superior performance of our <a href=https://en.wikipedia.org/wiki/System>system</a> compared to the <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a> on two standard <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> for named entity classification. We obtain between 62 % and 200 % improvement over the state-of-art baseline on these two <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Ajay+Nagesh" title="Search for 'Ajay Nagesh' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/m/mihai-surdeanu/ class=align-middle>Mihai Surdeanu</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/people/a/arkady-arkhangorodsky/ class=align-middle>Arkady Arkhangorodsky</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/christopher-chu/ class=align-middle>Christopher Chu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/scot-fang/ class=align-middle>Scot Fang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yiqi-huang/ class=align-middle>Yiqi Huang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/d/denglin-jiang/ class=align-middle>Denglin Jiang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/b/boliang-zhang/ class=align-middle>Boliang Zhang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/kevin-knight/ class=align-middle>Kevin Knight</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/andrew-zupon/ class=align-middle>Andrew Zupon</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/maria-alexeeva/ class=align-middle>Maria Alexeeva</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/marco-valenzuela-escarcega/ class=align-middle>Marco Valenzuela-Esc√°rcega</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/f/fan-luo/ class=align-middle>Fan Luo</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/rebecca-sharp/ class=align-middle>Rebecca Sharp</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/ws/ class=align-middle>WS</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/coling/ class=align-middle>COLING</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/emnlp/ class=align-middle>EMNLP</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/naacl/ class=align-middle>NAACL</a><span class="badge badge-secondary align-middle ml-2">1</span></li></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ¬©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>