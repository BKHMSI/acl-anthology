<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Akiko Aizawa - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Akiko</span> <span class=font-weight-bold>Aizawa</span></h2><hr><div class=row><div class=col-lg-9><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.mrqa-1.6.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--mrqa-1--6 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.mrqa-1.6 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.mrqa-1.6/>Can Question Generation Debias <a href=https://en.wikipedia.org/wiki/Question_answering>Question Answering Models</a>? A Case Study on QuestionContext Lexical Overlap</a></strong><br><a href=/people/k/kazutoshi-shinoda/>Kazutoshi Shinoda</a>
|
<a href=/people/s/saku-sugawara/>Saku Sugawara</a>
|
<a href=/people/a/akiko-aizawa/>Akiko Aizawa</a><br><a href=/volumes/2021.mrqa-1/ class=text-muted>Proceedings of the 3rd Workshop on Machine Reading for Question Answering</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--mrqa-1--6><div class="card-body p-3 small">Question answering (QA) models for <a href=https://en.wikipedia.org/wiki/Reading_comprehension>reading comprehension</a> have been demonstrated to exploit unintended dataset biases such as questioncontext lexical overlap. This hinders QA models from generalizing to <a href=https://en.wikipedia.org/wiki/Underrepresented_group>under-represented samples</a> such as questions with low lexical overlap. Question generation (QG), a method for augmenting QA datasets, can be a solution for such performance degradation if QG can properly debias QA datasets. However, we discover that recent neural QG models are biased towards generating questions with high lexical overlap, which can amplify the dataset bias. Moreover, our analysis reveals that data augmentation with these QG models frequently impairs the performance on questions with low <a href=https://en.wikipedia.org/wiki/Lexical_overlap>lexical overlap</a>, while improving that on questions with high <a href=https://en.wikipedia.org/wiki/Lexical_overlap>lexical overlap</a>. To address this problem, we use a synonym replacement-based approach to augment questions with low lexical overlap. We demonstrate that the proposed data augmentation approach is simple yet effective to mitigate the degradation problem with only 70k synthetic examples.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-srw.21.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-srw--21 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-srw.21 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-srw.21" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-srw.21/>Improving the Robustness of QA Models to Challenge Sets with Variational Question-Answer Pair Generation<span class=acl-fixed-case>QA</span> Models to Challenge Sets with Variational Question-Answer Pair Generation</a></strong><br><a href=/people/k/kazutoshi-shinoda/>Kazutoshi Shinoda</a>
|
<a href=/people/s/saku-sugawara/>Saku Sugawara</a>
|
<a href=/people/a/akiko-aizawa/>Akiko Aizawa</a><br><a href=/volumes/2021.acl-srw/ class=text-muted>Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: Student Research Workshop</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-srw--21><div class="card-body p-3 small">Question answering (QA) models for <a href=https://en.wikipedia.org/wiki/Reading_comprehension>reading comprehension</a> have achieved human-level accuracy on in-distribution test sets. However, they have been demonstrated to lack <a href=https://en.wikipedia.org/wiki/Robust_statistics>robustness</a> to challenge sets, whose <a href=https://en.wikipedia.org/wiki/Probability_distribution>distribution</a> is different from that of training sets. Existing data augmentation methods mitigate this problem by simply augmenting training sets with synthetic examples sampled from the same distribution as the challenge sets. However, these methods assume that the <a href=https://en.wikipedia.org/wiki/Probability_distribution>distribution</a> of a challenge set is known a priori, making them less applicable to unseen challenge sets. In this study, we focus on question-answer pair generation (QAG) to mitigate this problem. While most existing QAG methods aim to improve the quality of synthetic examples, we conjecture that diversity-promoting QAG can mitigate the sparsity of training sets and lead to better <a href=https://en.wikipedia.org/wiki/Robust_statistics>robustness</a>. We present a variational QAG model that generates multiple diverse QA pairs from a paragraph. Our experiments show that our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> can improve the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of 12 challenge sets, as well as the in-distribution accuracy.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.170.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--170 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.170 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.eacl-main.170" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.170/>Attention-based Relational Graph Convolutional Network for Target-Oriented Opinion Words Extraction</a></strong><br><a href=/people/j/junfeng-jiang/>Junfeng Jiang</a>
|
<a href=/people/a/an-wang/>An Wang</a>
|
<a href=/people/a/akiko-aizawa/>Akiko Aizawa</a><br><a href=/volumes/2021.eacl-main/ class=text-muted>Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--170><div class="card-body p-3 small">Target-oriented opinion words extraction (TOWE) is a subtask of aspect-based sentiment analysis (ABSA). It aims to extract the corresponding opinion words for a given opinion target in a review sentence. Intuitively, the relation between an opinion target and an opinion word mostly relies on <a href=https://en.wikipedia.org/wiki/Syntax>syntactics</a>. In this study, we design a directed syntactic dependency graph based on a dependency tree to establish a path from the target to candidate opinions. Subsequently, we propose a novel attention-based relational graph convolutional neural network (ARGCN) to exploit syntactic information over dependency graphs. Moreover, to explicitly extract the corresponding opinion words toward the given opinion target, we effectively encode target information in our model with the target-aware representation. Empirical results demonstrate that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> significantly outperforms all of the existing <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> on four benchmark datasets. Extensive analysis also demonstrates the effectiveness of each component of our <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>. Our code is available at https://github.com/wcwowwwww/towe-eacl.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.304.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--304 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.304 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.304/>Communicative-Function-Based Sentence Classification for Construction of an Academic Formulaic Expression Database</a></strong><br><a href=/people/k/kenichi-iwatsuki/>Kenichi Iwatsuki</a>
|
<a href=/people/a/akiko-aizawa/>Akiko Aizawa</a><br><a href=/volumes/2021.eacl-main/ class=text-muted>Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--304><div class="card-body p-3 small">Formulaic expressions (FEs), such as &#8216;in this paper, we propose&#8217; are frequently used in <a href=https://en.wikipedia.org/wiki/Scientific_literature>scientific papers</a>. FEs convey a communicative function (CF), i.e. &#8216;showing the aim of the paper&#8217; in the above-mentioned example. Although CF-labelled FEs are helpful in assisting <a href=https://en.wikipedia.org/wiki/Academic_writing>academic writing</a>, the construction of FE databases requires manual labour for assigning CF labels. In this study, we considered a fully automated construction of a CF-labelled FE database using the topdown approach, in which the CF labels are first assigned to sentences, and then the FEs are extracted. For the CF-label assignment, we created a CF-labelled sentence dataset, on which we trained a SciBERT classifier. We show that the <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifier</a> and <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> can be used to construct FE databases of disciplines that are different from the training data. The <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of <a href=https://en.wikipedia.org/wiki/Medical_classification>in-disciplinary classification</a> was more than 80 %, while <a href=https://en.wikipedia.org/wiki/Medical_classification>cross-disciplinary classification</a> also worked well. We also propose an FE extraction method, which was applied to the CF-labelled sentences. Finally, we constructed and published a new, large CF-labelled FE database. The evaluation of the final CF-labelled FE database showed that approximately 65 % of the FEs are correct and useful, which is sufficiently high considering practical use.</div></div><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.420.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--420 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.420 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38940091 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.420" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.420/>Language-Conditioned Feature Pyramids for Visual Selection Tasks<span class=acl-fixed-case>C</span>onditioned <span class=acl-fixed-case>F</span>eature <span class=acl-fixed-case>P</span>yramids for <span class=acl-fixed-case>V</span>isual <span class=acl-fixed-case>S</span>election <span class=acl-fixed-case>T</span>asks</a></strong><br><a href=/people/t/taichi-iki/>Taichi Iki</a>
|
<a href=/people/a/akiko-aizawa/>Akiko Aizawa</a><br><a href=/volumes/2020.findings-emnlp/ class=text-muted>Findings of the Association for Computational Linguistics: EMNLP 2020</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--420><div class="card-body p-3 small">Referring expression comprehension, which is the ability to locate language to an object in an image, plays an important role in creating common ground. Many <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> that fuse <a href=https://en.wikipedia.org/wiki/Visual_system>visual and linguistic features</a> have been proposed. However, few models consider the fusion of linguistic features with multiple <a href=https://en.wikipedia.org/wiki/Visual_system>visual features</a> with different sizes of <a href=https://en.wikipedia.org/wiki/Receptive_field>receptive fields</a>, though the proper size of the receptive field of <a href=https://en.wikipedia.org/wiki/Visual_system>visual features</a> intuitively varies depending on expressions. In this paper, we introduce a <a href=https://en.wikipedia.org/wiki/Neural_network>neural network architecture</a> that modulates visual features with varying sizes of <a href=https://en.wikipedia.org/wiki/Receptive_field>receptive field</a> by <a href=https://en.wikipedia.org/wiki/Feature_(linguistics)>linguistic features</a>. We evaluate our <a href=https://en.wikipedia.org/wiki/Software_architecture>architecture</a> on tasks related to referring expression comprehension in two visual dialogue games. The results show the advantages and broad applicability of our <a href=https://en.wikipedia.org/wiki/Architecture>architecture</a>. Source code is available at https://github.com/Alab-NII/lcfp.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.368.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--368 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.368 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.coling-main.368" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.368/>Deconstruct to Reconstruct a Configurable Evaluation Metric for Open-Domain Dialogue Systems</a></strong><br><a href=/people/v/vitou-phy/>Vitou Phy</a>
|
<a href=/people/y/yang-zhao/>Yang Zhao</a>
|
<a href=/people/a/akiko-aizawa/>Akiko Aizawa</a><br><a href=/volumes/2020.coling-main/ class=text-muted>Proceedings of the 28th International Conference on Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--368><div class="card-body p-3 small">Many automatic evaluation metrics have been proposed to score the overall quality of a response in open-domain dialogue. Generally, the overall quality is comprised of various aspects, such as relevancy, <a href=https://en.wikipedia.org/wiki/Sensitivity_and_specificity>specificity</a>, and <a href=https://en.wikipedia.org/wiki/Empathy>empathy</a>, and the importance of each aspect differs according to the task. For instance, <a href=https://en.wikipedia.org/wiki/Sensitivity_and_specificity>specificity</a> is mandatory in a food-ordering dialogue task, whereas <a href=https://en.wikipedia.org/wiki/Fluency>fluency</a> is preferred in a language-teaching dialogue system. However, existing <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> are not designed to cope with such flexibility. For example, BLEU score fundamentally relies only on word overlapping, whereas BERTScore relies on <a href=https://en.wikipedia.org/wiki/Semantic_similarity>semantic similarity</a> between reference and candidate response. Thus, <a href=https://en.wikipedia.org/wiki/Copula_(linguistics)>they</a> are not guaranteed to capture the required <a href=https://en.wikipedia.org/wiki/Complex_system>aspects</a>, i.e., <a href=https://en.wikipedia.org/wiki/Sensitivity_and_specificity>specificity</a>. To design a metric that is flexible to a task, we first propose making these qualities manageable by grouping them into three groups : understandability, sensibleness, and likability, where likability is a combination of qualities that are essential for a task. We also propose a simple method to composite metrics of each aspect to obtain a single <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metric</a> called USL-H, which stands for Understandability, Sensibleness, and Likability in Hierarchy. We demonstrated that USL-H score achieves good correlations with human judgment and maintains its configurability towards different aspects and metrics.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.580.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--580 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.580 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.coling-main.580" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.580/>Constructing A Multi-hop QA Dataset for Comprehensive Evaluation of Reasoning Steps<span class=acl-fixed-case>QA</span> Dataset for Comprehensive Evaluation of Reasoning Steps</a></strong><br><a href=/people/x/xanh-ho/>Xanh Ho</a>
|
<a href=/people/a/anh-khoa-duong-nguyen/>Anh-Khoa Duong Nguyen</a>
|
<a href=/people/s/saku-sugawara/>Saku Sugawara</a>
|
<a href=/people/a/akiko-aizawa/>Akiko Aizawa</a><br><a href=/volumes/2020.coling-main/ class=text-muted>Proceedings of the 28th International Conference on Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--580><div class="card-body p-3 small">A multi-hop question answering (QA) dataset aims to test reasoning and inference skills by requiring a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> to read multiple paragraphs to answer a given question. However, current <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> do not provide a complete explanation for the <a href=https://en.wikipedia.org/wiki/Reason>reasoning process</a> from the question to the answer. Further, previous studies revealed that many examples in existing multi-hop datasets do not require multi-hop reasoning to answer a question. In this study, we present a new multi-hop QA dataset, called 2WikiMultiHopQA, which uses structured and unstructured data. In our <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>, we introduce the <a href=https://en.wikipedia.org/wiki/Evidence-based_practice>evidence information</a> containing a reasoning path for multi-hop questions. The evidence information has two benefits : (i) providing a comprehensive explanation for predictions and (ii) evaluating the reasoning skills of a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>. We carefully design a <a href=https://en.wikipedia.org/wiki/Pipeline_(software)>pipeline</a> and a set of <a href=https://en.wikipedia.org/wiki/Template_(word_processing)>templates</a> when generating a question-answer pair that guarantees the multi-hop steps and the quality of the questions. We also exploit the structured format in <a href=https://en.wikipedia.org/wiki/Wikidata>Wikidata</a> and use logical rules to create questions that are natural but still require multi-hop reasoning. Through experiments, we demonstrate that our <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> is challenging for multi-hop models and it ensures that multi-hop reasoning is required.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wosp-1.1.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wosp-1--1 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wosp-1.1 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.wosp-1.1/>Virtual Citation Proximity (VCP): Empowering Document Recommender Systems by Learning a Hypothetical In-Text Citation-Proximity Metric for Uncited Documents<span class=acl-fixed-case>VCP</span>): Empowering Document Recommender Systems by Learning a Hypothetical In-Text Citation-Proximity Metric for Uncited Documents</a></strong><br><a href=/people/p/paul-molloy/>Paul Molloy</a>
|
<a href=/people/j/joeran-beel/>Joeran Beel</a>
|
<a href=/people/a/akiko-aizawa/>Akiko Aizawa</a><br><a href=/volumes/2020.wosp-1/ class=text-muted>Proceedings of the 8th International Workshop on Mining Scientific Publications</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wosp-1--1><div class="card-body p-3 small">The relatedness of <a href=https://en.wikipedia.org/wiki/Article_(publishing)>research articles</a>, <a href=https://en.wikipedia.org/wiki/Patent>patents</a>, court rulings, web pages, and other document types is often calculated with citation or hyperlink-based approaches like co-citation (proximity) analysis. The main limitation of citation-based approaches is that they can not be used for documents that receive little or no <a href=https://en.wikipedia.org/wiki/Citation>citations</a>. We propose Virtual Citation Proximity (VCP), a Siamese Neural Network architecture, which combines the advantages of co-citation proximity analysis (diverse notions of relatedness / high recommendation performance), with the advantage of content-based filtering (high coverage). VCP is trained on a <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus of documents</a> with textual features, and with real citation proximity as ground truth. VCP then predicts for any two documents, based on their title and abstract, in what proximity the two documents would be co-cited, if they were indeed co-cited. The prediction can be used in the same way as real citation proximity to calculate document relatedness, even for uncited documents. In our evaluation with 2 million <a href=https://en.wikipedia.org/wiki/Citation>co-citations</a> from Wikipedia articles, VCP achieves an <a href=https://en.wikipedia.org/wiki/Greatest_common_divisor>MAE</a> of 0.0055, i.e. an improvement of 20 % over the <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline</a>, though the <a href=https://en.wikipedia.org/wiki/Learning_curve>learning curve</a> suggests that more work is needed.</div></div><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1216.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1216 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1216 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P19-1216.Supplementary.pdf data-toggle=tooltip data-placement=top title=Supplementary><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1216/>Unsupervised Rewriter for Multi-Sentence Compression</a></strong><br><a href=/people/y/yang-zhao/>Yang Zhao</a>
|
<a href=/people/x/xiaoyu-shen/>Xiaoyu Shen</a>
|
<a href=/people/w/wei-bi/>Wei Bi</a>
|
<a href=/people/a/akiko-aizawa/>Akiko Aizawa</a><br><a href=/volumes/P19-1/ class=text-muted>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1216><div class="card-body p-3 small">Multi-sentence compression (MSC) aims to generate a grammatical but reduced compression from multiple input sentences while retaining their key information. Previous dominating approach for <a href=https://en.wikipedia.org/wiki/Microsoft_SQL_Server>MSC</a> is the extraction-based word graph approach. A few variants further leveraged <a href=https://en.wikipedia.org/wiki/Lexical_substitution>lexical substitution</a> to yield more abstractive compression. However, two limitations exist. First, the word graph approach that simply concatenates fragments from multiple sentences may yield non-fluent or ungrammatical compression. Second, <a href=https://en.wikipedia.org/wiki/Lexical_substitution>lexical substitution</a> is often inappropriate without the consideration of <a href=https://en.wikipedia.org/wiki/Context_(language_use)>context information</a>. To tackle the above-mentioned issues, we present a neural rewriter for multi-sentence compression that does not need any <a href=https://en.wikipedia.org/wiki/Parallel_text>parallel corpus</a>. Empirical studies have shown that our approach achieves comparable results upon automatic evaluation and improves the <a href=https://en.wikipedia.org/wiki/Grammaticality>grammaticality</a> of compression based on human evaluation. A <a href=https://en.wikipedia.org/wiki/Parallel_corpus>parallel corpus</a> with more than 140,000 (sentence group, compression) pairs is also constructed as a by-product for future research.</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1227.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1227 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1227 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=C18-1227" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/C18-1227/>Using Formulaic Expressions in Writing Assistance Systems</a></strong><br><a href=/people/k/kenichi-iwatsuki/>Kenichi Iwatsuki</a>
|
<a href=/people/a/akiko-aizawa/>Akiko Aizawa</a><br><a href=/volumes/C18-1/ class=text-muted>Proceedings of the 27th International Conference on Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1227><div class="card-body p-3 small">Formulaic expressions (FEs) used in scholarly papers, such as &#8216;there has been little discussion about&#8217;, are helpful for non-native English speakers. However, it is time-consuming for users to manually search for an appropriate expression every time they want to consult FE dictionaries. For this reason, we tackle the task of semantic searches of FE dictionaries. At the start of our research, we identified two salient difficulties in this task. First, the paucity of example sentences in existing FE dictionaries results in a shortage of context information, which is necessary for acquiring semantic representation of FEs. Second, while a semantic category label is assigned to each FE in many FE dictionaries, it is difficult to predict the labels from user input, forcing users to manually designate the semantic category when searching. To address these difficulties, we propose a new framework for semantic searches of FEs and propose a new method to leverage both existing <a href=https://en.wikipedia.org/wiki/Dictionary>dictionaries</a> and domain sentence corpora. Further, we expand an existing FE dictionary to consider building a more comprehensive and domain-specific FE dictionary and to verify the effectiveness of our method.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1453.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1453 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1453 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/306150555 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1453" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1453/>What Makes Reading Comprehension Questions Easier?</a></strong><br><a href=/people/s/saku-sugawara/>Saku Sugawara</a>
|
<a href=/people/k/kentaro-inui/>Kentaro Inui</a>
|
<a href=/people/s/satoshi-sekine/>Satoshi Sekine</a>
|
<a href=/people/a/akiko-aizawa/>Akiko Aizawa</a><br><a href=/volumes/D18-1/ class=text-muted>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1453><div class="card-body p-3 small">A challenge in creating a <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> for machine reading comprehension (MRC) is to collect questions that require a sophisticated understanding of language to answer beyond using superficial cues. In this work, we investigate what makes questions easier across recent 12 MRC datasets with three question styles (answer extraction, description, and multiple choice). We propose to employ simple <a href=https://en.wikipedia.org/wiki/Heuristic>heuristics</a> to split each <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> into easy and hard subsets and examine the performance of two baseline models for each of the <a href=https://en.wikipedia.org/wiki/Subset>subsets</a>. We then manually annotate questions sampled from each <a href=https://en.wikipedia.org/wiki/Subset>subset</a> with both validity and requisite reasoning skills to investigate which skills explain the difference between easy and hard questions. From this study, we observed that (i) the baseline performances for the hard subsets remarkably degrade compared to those of entire datasets, (ii) hard questions require knowledge inference and multiple-sentence reasoning in comparison with easy questions, and (iii) multiple-choice questions tend to require a broader range of reasoning skills than answer extraction and description questions. These results suggest that one might overestimate recent advances in <a href=https://en.wikipedia.org/wiki/Microscopic_scale>MRC</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S18-1126.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S18-1126 data-toggle=collapse aria-expanded=false aria-controls=abstract-S18-1126 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/S18-1126/>UC3M-NII Team at SemEval-2018 Task 7 : Semantic Relation Classification in Scientific Papers via Convolutional Neural Network<span class=acl-fixed-case>UC</span>3<span class=acl-fixed-case>M</span>-<span class=acl-fixed-case>NII</span> Team at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2018 Task 7: Semantic Relation Classification in Scientific Papers via Convolutional Neural Network</a></strong><br><a href=/people/v/victor-suarez-paniagua/>Víctor Suárez-Paniagua</a>
|
<a href=/people/i/isabel-segura-bedmar/>Isabel Segura-Bedmar</a>
|
<a href=/people/a/akiko-aizawa/>Akiko Aizawa</a><br><a href=/volumes/S18-1/ class=text-muted>Proceedings of The 12th International Workshop on Semantic Evaluation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S18-1126><div class="card-body p-3 small">This paper reports our participation for SemEval-2018 Task 7 on extraction and classification of relationships between entities in <a href=https://en.wikipedia.org/wiki/Scientific_literature>scientific papers</a>. Our approach is based on the use of a Convolutional Neural Network (CNN) trained on350 abstract with manually annotated entities and relations. Our hypothesis is that this deep learning model can be applied to extract and classify relations between entities for <a href=https://en.wikipedia.org/wiki/Scientific_literature>scientific papers</a> at the same time. We use the Part-of-Speech and the distances to the target entities as part of the embedding for each word and we blind all the entities by marker names. In addition, we use <a href=https://en.wikipedia.org/wiki/Sampling_(statistics)>sampling techniques</a> to overcome the imbalance issues of this <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>. Our <a href=https://en.wikipedia.org/wiki/Computer_architecture>architecture</a> obtained an F1-score of 35.4 % for the relation extraction task and 18.5 % for the relation classification task with a basic configuration of the one step CNN.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2028.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2028 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2028 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2028.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2028.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-2028/>A Language Model based Evaluator for Sentence Compression</a></strong><br><a href=/people/y/yang-zhao/>Yang Zhao</a>
|
<a href=/people/z/zhiyuan-luo/>Zhiyuan Luo</a>
|
<a href=/people/a/akiko-aizawa/>Akiko Aizawa</a><br><a href=/volumes/P18-2/ class=text-muted>Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2028><div class="card-body p-3 small">We herein present a language-model-based evaluator for deletion-based sentence compression and view this task as a series of deletion-and-evaluation operations using the evaluator. More specifically, the evaluator is a syntactic neural language model that is first built by learning the syntactic and structural collocation among words. Subsequently, a series of trial-and-error deletion operations are conducted on the source sentences via a reinforcement learning framework to obtain the best target compression. An empirical study shows that the proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> can effectively generate more readable compression, comparable or superior to several strong <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a>. Furthermore, we introduce a 200-sentence test set for a large-scale dataset, setting a new baseline for the future research.</div></div><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1075.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1075 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1075 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234958313 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-1075/>Evaluation Metrics for Machine Reading Comprehension : Prerequisite Skills and Readability</a></strong><br><a href=/people/s/saku-sugawara/>Saku Sugawara</a>
|
<a href=/people/y/yusuke-kido/>Yusuke Kido</a>
|
<a href=/people/h/hikaru-yokono/>Hikaru Yokono</a>
|
<a href=/people/a/akiko-aizawa/>Akiko Aizawa</a><br><a href=/volumes/P17-1/ class=text-muted>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1075><div class="card-body p-3 small">Knowing the quality of reading comprehension (RC) datasets is important for the development of <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural-language understanding systems</a>. In this study, two classes of <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> were adopted for evaluating RC datasets : prerequisite skills and <a href=https://en.wikipedia.org/wiki/Readability>readability</a>. We applied these classes to six existing <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>, including MCTest and SQuAD, and highlighted the characteristics of the <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> according to each metric and the correlation between the two classes. Our dataset analysis suggests that the <a href=https://en.wikipedia.org/wiki/Readability>readability</a> of RC datasets does not directly affect the question difficulty and that it is possible to create an RC dataset that is easy to read but difficult to answer.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2080.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2080 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2080 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-2080/>A Conditional Variational Framework for Dialog Generation</a></strong><br><a href=/people/x/xiaoyu-shen/>Xiaoyu Shen</a>
|
<a href=/people/h/hui-su/>Hui Su</a>
|
<a href=/people/y/yanran-li/>Yanran Li</a>
|
<a href=/people/w/wenjie-li/>Wenjie Li</a>
|
<a href=/people/s/shuzi-niu/>Shuzi Niu</a>
|
<a href=/people/y/yang-zhao/>Yang Zhao</a>
|
<a href=/people/a/akiko-aizawa/>Akiko Aizawa</a>
|
<a href=/people/g/guoping-long/>Guoping Long</a><br><a href=/volumes/P17-2/ class=text-muted>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2080><div class="card-body p-3 small">Deep latent variable models have been shown to facilitate the response generation for open-domain dialog systems. However, these latent variables are highly randomized, leading to uncontrollable generated responses. In this paper, we propose a <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> allowing conditional response generation based on specific attributes. These <a href=https://en.wikipedia.org/wiki/Attribute_(computing)>attributes</a> can be either manually assigned or automatically detected. Moreover, the dialog states for both speakers are modeled separately in order to reflect personal features. We validate this <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> on two different scenarios, where the attribute refers to <a href=https://en.wikipedia.org/wiki/Generic_property>genericness</a> and <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment states</a> respectively. The experiment result testified the potential of our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>, where meaningful responses can be generated in accordance with the specified attributes.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Akiko+Aizawa" title="Search for 'Akiko Aizawa' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/s/saku-sugawara/ class=align-middle>Saku Sugawara</a>
<span class="badge badge-secondary align-middle ml-2">5</span></li><li class=list-group-item><a href=/people/y/yang-zhao/ class=align-middle>Yang Zhao</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/people/k/kazutoshi-shinoda/ class=align-middle>Kazutoshi Shinoda</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/k/kenichi-iwatsuki/ class=align-middle>Kenichi Iwatsuki</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/x/xiaoyu-shen/ class=align-middle>Xiaoyu Shen</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/y/yusuke-kido/ class=align-middle>Yusuke Kido</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/hikaru-yokono/ class=align-middle>Hikaru Yokono</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/hui-su/ class=align-middle>Hui Su</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yanran-li/ class=align-middle>Yanran Li</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/w/wenjie-li/ class=align-middle>Wenjie Li</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/shuzi-niu/ class=align-middle>Shuzi Niu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/g/guoping-long/ class=align-middle>Guoping Long</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/junfeng-jiang/ class=align-middle>Junfeng Jiang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/an-wang/ class=align-middle>An Wang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/kentaro-inui/ class=align-middle>Kentaro Inui</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/satoshi-sekine/ class=align-middle>Satoshi Sekine</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/taichi-iki/ class=align-middle>Taichi Iki</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/v/victor-suarez-paniagua/ class=align-middle>Víctor Suárez-Paniagua</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/i/isabel-segura-bedmar/ class=align-middle>Isabel Segura-Bedmar</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/v/vitou-phy/ class=align-middle>Vitou Phy</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/x/xanh-ho/ class=align-middle>Xanh Ho</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/anh-khoa-duong-nguyen/ class=align-middle>Anh-Khoa Duong Nguyen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/zhiyuan-luo/ class=align-middle>Zhiyuan Luo</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/w/wei-bi/ class=align-middle>Wei Bi</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/p/paul-molloy/ class=align-middle>Paul Molloy</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/joeran-beel/ class=align-middle>Joeran Beel</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/acl/ class=align-middle>ACL</a><span class="badge badge-secondary align-middle ml-2">5</span></li><li class=list-group-item><a href=/venues/coling/ class=align-middle>COLING</a><span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/venues/eacl/ class=align-middle>EACL</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/mrqa/ class=align-middle>MRQA</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/emnlp/ class=align-middle>EMNLP</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-venues aria-expanded=false aria-controls=more-venues>show all...</li><div class="collapse border-top" id=more-venues><li class=list-group-item><a href=/venues/findings/ class=align-middle>Findings</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/semeval/ class=align-middle>SemEval</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/wosp/ class=align-middle>WOSP</a><span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>