<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Asad Sayeed - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Asad</span> <span class=font-weight-bold>Sayeed</span></h2><hr><div class=row><div class=col-lg-9><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.globalex-1.8.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--globalex-1--8 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.globalex-1.8 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.globalex-1.8/>Building Sense Representations in <a href=https://en.wikipedia.org/wiki/Danish_language>Danish</a> by Combining Word Embeddings with Lexical Resources<span class=acl-fixed-case>D</span>anish by Combining Word Embeddings with Lexical Resources</a></strong><br><a href=/people/i/ida-rormann-olsen/>Ida Rørmann Olsen</a>
|
<a href=/people/b/bolette-sandford-pedersen/>Bolette Pedersen</a>
|
<a href=/people/a/asad-sayeed/>Asad Sayeed</a><br><a href=/volumes/2020.globalex-1/ class=text-muted>Proceedings of the 2020 Globalex Workshop on Linked Lexicography</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--globalex-1--8><div class="card-body p-3 small">Our aim is to identify suitable sense representations for <a href=https://en.wikipedia.org/wiki/Neuro-linguistic_programming>NLP</a> in <a href=https://en.wikipedia.org/wiki/Danish_language>Danish</a>. We investigate sense inventories that correlate with human interpretations of word meaning and <a href=https://en.wikipedia.org/wiki/Ambiguity>ambiguity</a> as typically described in dictionaries and wordnets and that are well reflected distributionally as expressed in <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a>. To this end, we study a number of highly ambiguous Danish nouns and examine the effectiveness of sense representations constructed by combining <a href=https://en.wikipedia.org/wiki/Vector_space>vectors</a> from a <a href=https://en.wikipedia.org/wiki/Distribution_(mathematics)>distributional model</a> with the information from a <a href=https://en.wikipedia.org/wiki/Wordnet>wordnet</a>. We establish <a href=https://en.wikipedia.org/wiki/Representation_(arts)>representations</a> based on centroids obtained from wordnet synests and example sentences as well as <a href=https://en.wikipedia.org/wiki/Representation_(arts)>representations</a> established via are tested in a word sense disambiguation task. We conclude that the more information extracted from the wordnet entries (example sentence, definition, semantic relations) the more successful the sense representation vector.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.95.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--95 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.95 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.95/>An Annotation Approach for Social and Referential Gaze in Dialogue</a></strong><br><a href=/people/v/vidya-somashekarappa/>Vidya Somashekarappa</a>
|
<a href=/people/c/christine-howes/>Christine Howes</a>
|
<a href=/people/a/asad-sayeed/>Asad Sayeed</a><br><a href=/volumes/2020.lrec-1/ class=text-muted>Proceedings of the 12th Language Resources and Evaluation Conference</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--95><div class="card-body p-3 small">This paper introduces an approach for annotating <a href=https://en.wikipedia.org/wiki/Eye_gaze>eye gaze</a> considering both its social and the referential functions in multi-modal human-human dialogue. Detecting and interpreting the temporal patterns of gaze behavior cues is natural for humans and also mostly an unconscious process. However, these <a href=https://en.wikipedia.org/wiki/Sensory_cue>cues</a> are difficult for conversational agents such as <a href=https://en.wikipedia.org/wiki/Robot>robots</a> or <a href=https://en.wikipedia.org/wiki/Avatar_(computing)>avatars</a> to process or generate. The key factor is to recognize these variants and carry out a successful conversation, as misinterpretation can lead to total failure of the given interaction. This paper introduces an annotation scheme for <a href=https://en.wikipedia.org/wiki/Eye_contact>eye-gaze</a> in human-human dyadic interactions that is intended to facilitate the learning of eye-gaze patterns in multi-modal natural dialogue.</div></div><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2915.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-2915 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-2915 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-2915/>Verb-Second Effect on Quantifier Scope Interpretation</a></strong><br><a href=/people/a/asad-sayeed/>Asad Sayeed</a>
|
<a href=/people/m/matthias-lindemann/>Matthias Lindemann</a>
|
<a href=/people/v/vera-demberg/>Vera Demberg</a><br><a href=/volumes/W19-29/ class=text-muted>Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-2915><div class="card-body p-3 small">Sentences like Every child climbed a tree have at least two interpretations depending on the precedence order of the <a href=https://en.wikipedia.org/wiki/Universal_quantifier>universal quantifier</a> and the indefinite. Previous experimental work explores the role that different <a href=https://en.wikipedia.org/wiki/Mechanism_(sociology)>mechanisms</a> such as semantic reanalysis and <a href=https://en.wikipedia.org/wiki/World_knowledge>world knowledge</a> may have in enabling each interpretation. This paper discusses a web-based task that uses the verb-second characteristic of German main clauses to estimate the influence of word order variation over <a href=https://en.wikipedia.org/wiki/World_knowledge>world knowledge</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3404.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3404 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3404 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3404/>A Hybrid Model for Globally Coherent Story Generation</a></strong><br><a href=/people/f/fangzhou-zhai/>Fangzhou Zhai</a>
|
<a href=/people/v/vera-demberg/>Vera Demberg</a>
|
<a href=/people/p/pavel-shkadzko/>Pavel Shkadzko</a>
|
<a href=/people/w/wei-shi/>Wei Shi</a>
|
<a href=/people/a/asad-sayeed/>Asad Sayeed</a><br><a href=/volumes/W19-34/ class=text-muted>Proceedings of the Second Workshop on Storytelling</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3404><div class="card-body p-3 small">Automatically generating globally coherent stories is a challenging problem. Neural text generation models have been shown to perform well at generating fluent sentences from data, but they usually fail to keep track of the overall coherence of the story after a couple of sentences. Existing work that incorporates a text planning module succeeded in generating <a href=https://en.wikipedia.org/wiki/Recipe>recipes</a> and <a href=https://en.wikipedia.org/wiki/Dialogue>dialogues</a>, but appears quite data-demanding. We propose a novel story generation approach that generates globally coherent stories from a fairly small corpus. The model exploits a symbolic text planning module to produce text plans, thus reducing the demand of data ; a neural surface realization module then generates fluent text conditioned on the text plan. Human evaluation showed that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms various baselines by a wide margin and generates stories which are fluent as well as globally coherent.</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S18-2002.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S18-2002 data-toggle=collapse aria-expanded=false aria-controls=abstract-S18-2002 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/S18-2002/>Learning distributed event representations with a multi-task approach</a></strong><br><a href=/people/x/xudong-hong/>Xudong Hong</a>
|
<a href=/people/a/asad-sayeed/>Asad Sayeed</a>
|
<a href=/people/v/vera-demberg/>Vera Demberg</a><br><a href=/volumes/S18-2/ class=text-muted>Proceedings of the Seventh Joint Conference on Lexical and Computational Semantics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S18-2002><div class="card-body p-3 small">Human world knowledge contains information about prototypical events and their participants and locations. In this paper, we train the first models using <a href=https://en.wikipedia.org/wiki/Multi-task_learning>multi-task learning</a> that can both predict missing event participants and also perform semantic role classification based on semantic plausibility. Our best-performing <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is an improvement over the previous <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a> on thematic fit modelling tasks. The event embeddings learned by the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> can additionally be used effectively in an event similarity task, also outperforming the <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0100.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0100/>Proceedings of the 8th Workshop on Cognitive Modeling and Computational Linguistics (<span class=acl-fixed-case>CMCL</span> 2018)</a></strong><br><a href=/people/a/asad-sayeed/>Asad Sayeed</a>
|
<a href=/people/c/cassandra-l-jacobs/>Cassandra Jacobs</a>
|
<a href=/people/t/tal-linzen/>Tal Linzen</a>
|
<a href=/people/m/marten-van-schijndel/>Marten van Schijndel</a><br><a href=/volumes/W18-01/ class=text-muted>Proceedings of the 8th Workshop on Cognitive Modeling and Computational Linguistics (CMCL 2018)</a></span></p><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-0700.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-0700/>Proceedings of the 7th Workshop on Cognitive Modeling and Computational Linguistics (<span class=acl-fixed-case>CMCL</span> 2017)</a></strong><br><a href=/people/t/ted-gibson/>Ted Gibson</a>
|
<a href=/people/t/tal-linzen/>Tal Linzen</a>
|
<a href=/people/a/asad-sayeed/>Asad Sayeed</a>
|
<a href=/people/m/marten-van-schijndel/>Martin van Schijndel</a>
|
<a href=/people/w/william-schuler/>William Schuler</a><br><a href=/volumes/W17-07/ class=text-muted>Proceedings of the 7th Workshop on Cognitive Modeling and Computational Linguistics (CMCL 2017)</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/Q17-1003.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-Q17-1003 data-toggle=collapse aria-expanded=false aria-controls=abstract-Q17-1003 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234958123 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/Q17-1003/>Modeling Semantic Expectation : Using Script Knowledge for Referent Prediction</a></strong><br><a href=/people/a/ashutosh-modi/>Ashutosh Modi</a>
|
<a href=/people/i/ivan-titov/>Ivan Titov</a>
|
<a href=/people/v/vera-demberg/>Vera Demberg</a>
|
<a href=/people/a/asad-sayeed/>Asad Sayeed</a>
|
<a href=/people/m/manfred-pinkal/>Manfred Pinkal</a><br><a href=/volumes/Q17-1/ class=text-muted>Transactions of the Association for Computational Linguistics, Volume 5</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-Q17-1003><div class="card-body p-3 small">Recent research in <a href=https://en.wikipedia.org/wiki/Psycholinguistics>psycholinguistics</a> has provided increasing evidence that humans predict upcoming content. Prediction also affects <a href=https://en.wikipedia.org/wiki/Perception>perception</a> and might be a key to robustness in <a href=https://en.wikipedia.org/wiki/Language_processing_in_the_brain>human language processing</a>. In this paper, we investigate the factors that affect human prediction by building a <a href=https://en.wikipedia.org/wiki/Computational_model>computational model</a> that can predict upcoming discourse referents based on linguistic knowledge alone vs. linguistic knowledge jointly with common-sense knowledge in the form of scripts. We find that script knowledge significantly improves model estimates of human predictions. In a second study, we test the highly controversial hypothesis that <a href=https://en.wikipedia.org/wiki/Predictability>predictability</a> influences referring expression type but do not find evidence for such an effect.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Asad+Sayeed" title="Search for 'Asad Sayeed' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/v/vera-demberg/ class=align-middle>Vera Demberg</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/people/t/tal-linzen/ class=align-middle>Tal Linzen</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/m/marten-van-schijndel/ class=align-middle>Marten van Schijndel</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/i/ida-rormann-olsen/ class=align-middle>Ida Rørmann Olsen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/b/bolette-sandford-pedersen/ class=align-middle>Bolette Sandford Pedersen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/t/ted-gibson/ class=align-middle>Ted Gibson</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/w/william-schuler/ class=align-middle>William Schuler</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/ashutosh-modi/ class=align-middle>Ashutosh Modi</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/i/ivan-titov/ class=align-middle>Ivan Titov</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/manfred-pinkal/ class=align-middle>Manfred Pinkal</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/x/xudong-hong/ class=align-middle>Xudong Hong</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/cassandra-l-jacobs/ class=align-middle>Cassandra L. Jacobs</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/matthias-lindemann/ class=align-middle>Matthias Lindemann</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/f/fangzhou-zhai/ class=align-middle>Fangzhou Zhai</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/p/pavel-shkadzko/ class=align-middle>Pavel Shkadzko</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/w/wei-shi/ class=align-middle>Wei Shi</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/v/vidya-somashekarappa/ class=align-middle>Vidya Somashekarappa</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/christine-howes/ class=align-middle>Christine Howes</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/ws/ class=align-middle>WS</a><span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/venues/globalex/ class=align-middle>GLOBALEX</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/tacl/ class=align-middle>TACL</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/semeval/ class=align-middle>SemEval</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/lrec/ class=align-middle>LREC</a><span class="badge badge-secondary align-middle ml-2">1</span></li></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>