<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Antoine Bosselut - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Antoine</span> <span class=font-weight-bold>Bosselut</span></h2><hr><div class=row><div class=col-lg-9><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.158.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--158 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.158 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.158/>Edited Media Understanding Frames : Reasoning About the Intent and Implications of Visual Misinformation</a></strong><br><a href=/people/j/jeff-da/>Jeff Da</a>
|
<a href=/people/m/maxwell-forbes/>Maxwell Forbes</a>
|
<a href=/people/r/rowan-zellers/>Rowan Zellers</a>
|
<a href=/people/a/anthony-zheng/>Anthony Zheng</a>
|
<a href=/people/j/jena-d-hwang/>Jena D. Hwang</a>
|
<a href=/people/a/antoine-bosselut/>Antoine Bosselut</a>
|
<a href=/people/y/yejin-choi/>Yejin Choi</a><br><a href=/volumes/2021.acl-long/ class=text-muted>Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--158><div class="card-body p-3 small">Understanding manipulated media, from automatically generated &#8216;deepfakes&#8217; to manually edited ones, raises novel research challenges. Because the vast majority of edited or manipulated images are benign, such as <a href=https://en.wikipedia.org/wiki/Photo_manipulation>photoshopped images</a> for visual enhancements, the key challenge is to understand the complex layers of underlying intents of media edits and their implications with respect to <a href=https://en.wikipedia.org/wiki/Disinformation>disinformation</a>. In this paper, we study Edited Media Frames, a new formalism to understand visual media manipulation as structured annotations with respect to the intents, emotional reactions, attacks on individuals, and the overall implications of <a href=https://en.wikipedia.org/wiki/Disinformation>disinformation</a>. We introduce a <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> for our task, EMU, with 56k question-answer pairs written in rich natural language. We evaluate a wide variety of vision-and-language models for our task, and introduce a new model PELICAN, which builds upon recent progress in pretrained multimodal representations. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> obtains promising results on our <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>, with humans rating its answers as accurate 48.2 % of the time. At the same time, there is still much work to be done and we provide analysis that highlights areas for further progress.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.gem-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.gem-1.0/>Proceedings of the 1st Workshop on Natural Language Generation, Evaluation, and Metrics (GEM 2021)</a></strong><br><a href=/people/a/antoine-bosselut/>Antoine Bosselut</a>
|
<a href=/people/e/esin-durmus/>Esin Durmus</a>
|
<a href=/people/v/varun-prashant-gangal/>Varun Prashant Gangal</a>
|
<a href=/people/s/sebastian-gehrmann/>Sebastian Gehrmann</a>
|
<a href=/people/y/yacine-jernite/>Yacine Jernite</a>
|
<a href=/people/l/laura-perez-beltrachini/>Laura Perez-Beltrachini</a>
|
<a href=/people/s/samira-shaikh/>Samira Shaikh</a>
|
<a href=/people/w/wei-xu/>Wei Xu</a><br><a href=/volumes/2021.gem-1/ class=text-muted>Proceedings of the 1st Workshop on Natural Language Generation, Evaluation, and Metrics (GEM 2021)</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.gem-1.10.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--gem-1--10 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.gem-1.10 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.gem-1.10/>The GEM Benchmark : <a href=https://en.wikipedia.org/wiki/Natural-language_generation>Natural Language Generation</a>, its Evaluation and Metrics<span class=acl-fixed-case>GEM</span> Benchmark: Natural Language Generation, its Evaluation and Metrics</a></strong><br><a href=/people/s/sebastian-gehrmann/>Sebastian Gehrmann</a>
|
<a href=/people/t/tosin-adewumi/>Tosin Adewumi</a>
|
<a href=/people/k/karmanya-aggarwal/>Karmanya Aggarwal</a>
|
<a href=/people/p/pawan-sasanka-ammanamanchi/>Pawan Sasanka Ammanamanchi</a>
|
<a href=/people/a/anuoluwapo-aremu/>Anuoluwapo Aremu</a>
|
<a href=/people/a/antoine-bosselut/>Antoine Bosselut</a>
|
<a href=/people/k/khyathi-raghavi-chandu/>Khyathi Raghavi Chandu</a>
|
<a href=/people/m/miruna-clinciu/>Miruna-Adriana Clinciu</a>
|
<a href=/people/d/dipanjan-das/>Dipanjan Das</a>
|
<a href=/people/k/kaustubh-dhole/>Kaustubh Dhole</a>
|
<a href=/people/w/wanyu-du/>Wanyu Du</a>
|
<a href=/people/e/esin-durmus/>Esin Durmus</a>
|
<a href=/people/o/ondrej-dusek/>Ondřej Dušek</a>
|
<a href=/people/c/chris-chinenye-emezue/>Chris Chinenye Emezue</a>
|
<a href=/people/v/varun-gangal/>Varun Gangal</a>
|
<a href=/people/c/cristina-garbacea/>Cristina Garbacea</a>
|
<a href=/people/t/tatsunori-b-hashimoto/>Tatsunori Hashimoto</a>
|
<a href=/people/y/yufang-hou/>Yufang Hou</a>
|
<a href=/people/y/yacine-jernite/>Yacine Jernite</a>
|
<a href=/people/h/harsh-jhamtani/>Harsh Jhamtani</a>
|
<a href=/people/y/yangfeng-ji/>Yangfeng Ji</a>
|
<a href=/people/s/shailza-jolly/>Shailza Jolly</a>
|
<a href=/people/m/mihir-kale/>Mihir Kale</a>
|
<a href=/people/d/dhruv-kumar/>Dhruv Kumar</a>
|
<a href=/people/f/faisal-ladhak/>Faisal Ladhak</a>
|
<a href=/people/a/aman-madaan/>Aman Madaan</a>
|
<a href=/people/m/mounica-maddela/>Mounica Maddela</a>
|
<a href=/people/k/khyati-mahajan/>Khyati Mahajan</a>
|
<a href=/people/s/saad-mahamood/>Saad Mahamood</a>
|
<a href=/people/b/bodhisattwa-prasad-majumder/>Bodhisattwa Prasad Majumder</a>
|
<a href=/people/p/pedro-henrique-martins/>Pedro Henrique Martins</a>
|
<a href=/people/a/angelina-mcmillan-major/>Angelina McMillan-Major</a>
|
<a href=/people/s/simon-mille/>Simon Mille</a>
|
<a href=/people/e/emiel-van-miltenburg/>Emiel van Miltenburg</a>
|
<a href=/people/m/moin-nadeem/>Moin Nadeem</a>
|
<a href=/people/s/shashi-narayan/>Shashi Narayan</a>
|
<a href=/people/v/vitaly-nikolaev/>Vitaly Nikolaev</a>
|
<a href=/people/a/andre-niyongabo-rubungo/>Andre Niyongabo Rubungo</a>
|
<a href=/people/s/salomey-osei/>Salomey Osei</a>
|
<a href=/people/a/ankur-parikh/>Ankur Parikh</a>
|
<a href=/people/l/laura-perez-beltrachini/>Laura Perez-Beltrachini</a>
|
<a href=/people/n/niranjan-ramesh-rao/>Niranjan Ramesh Rao</a>
|
<a href=/people/v/vikas-raunak/>Vikas Raunak</a>
|
<a href=/people/j/juan-diego-rodriguez/>Juan Diego Rodriguez</a>
|
<a href=/people/s/sashank-santhanam/>Sashank Santhanam</a>
|
<a href=/people/j/joao-sedoc/>João Sedoc</a>
|
<a href=/people/t/thibault-sellam/>Thibault Sellam</a>
|
<a href=/people/s/samira-shaikh/>Samira Shaikh</a>
|
<a href=/people/a/anastasia-shimorina/>Anastasia Shimorina</a>
|
<a href=/people/m/marco-antonio-sobrevilla-cabezudo/>Marco Antonio Sobrevilla Cabezudo</a>
|
<a href=/people/h/hendrik-strobelt/>Hendrik Strobelt</a>
|
<a href=/people/n/nishant-subramani/>Nishant Subramani</a>
|
<a href=/people/w/wei-xu/>Wei Xu</a>
|
<a href=/people/d/diyi-yang/>Diyi Yang</a>
|
<a href=/people/a/akhila-yerukola/>Akhila Yerukola</a>
|
<a href=/people/j/jiawei-zhou/>Jiawei Zhou</a><br><a href=/volumes/2021.gem-1/ class=text-muted>Proceedings of the 1st Workshop on Natural Language Generation, Evaluation, and Metrics (GEM 2021)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--gem-1--10><div class="card-body p-3 small">We introduce GEM, a living benchmark for natural language Generation (NLG), its Evaluation, and Metrics. Measuring progress in NLG relies on a constantly evolving ecosystem of automated metrics, <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>, and human evaluation standards. Due to this moving target, new <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> often still evaluate on divergent anglo-centric corpora with well-established, but flawed, <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a>. This disconnect makes it challenging to identify the limitations of current <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> and opportunities for progress. Addressing this limitation, <a href=https://en.wikipedia.org/wiki/Graphics_Environment_Manager>GEM</a> provides an environment in which <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> can easily be applied to a wide set of <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> and in which <a href=https://en.wikipedia.org/wiki/Evaluation_strategy>evaluation strategies</a> can be tested. Regular updates to the <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmark</a> will help NLG research become more multilingual and evolve the challenge alongside <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>. This paper serves as the description of the data for the 2021 shared task at the associated GEM Workshop.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.346.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--346 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.346 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.346/>I’m Not Mad : Commonsense Implications of Negation and Contradiction<span class=acl-fixed-case>I</span>’m Not Mad”: Commonsense Implications of Negation and Contradiction</a></strong><br><a href=/people/l/liwei-jiang/>Liwei Jiang</a>
|
<a href=/people/a/antoine-bosselut/>Antoine Bosselut</a>
|
<a href=/people/c/chandra-bhagavatula/>Chandra Bhagavatula</a>
|
<a href=/people/y/yejin-choi/>Yejin Choi</a><br><a href=/volumes/2021.naacl-main/ class=text-muted>Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--346><div class="card-body p-3 small">Natural language inference requires reasoning about <a href=https://en.wikipedia.org/wiki/Contradiction>contradictions</a>, <a href=https://en.wikipedia.org/wiki/Affirmation_and_negation>negations</a>, and their commonsense implications. Given a simple premise (e.g., I&#8217;m mad at you), humans can reason about the varying shades of contradictory statements ranging from straightforward negations (I&#8217;m not mad at you) to commonsense contradictions (I&#8217;m happy). Moreover, these negated or contradictory statements shift the commonsense implications of the original premise in interesting and nontrivial ways. For example, while I&#8217;m mad implies I&#8217;m unhappy about something, negating the premise does not necessarily negate the corresponding commonsense implications. In this paper, we present the first comprehensive study focusing on commonsense implications of <a href=https://en.wikipedia.org/wiki/Negation>negated statements</a> and <a href=https://en.wikipedia.org/wiki/Contradiction>contradictions</a>. We introduce ANION, a new commonsense knowledge graph with 624 K if-then rules focusing on negated and contradictory events. We then present joint generative and discriminative inference models for this new resource, providing novel empirical insights on how logical negations and commonsense contradictions reshape the commonsense implications of their original premises.</div></div><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1629.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1629 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1629 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1629.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D19-1629/>WIQA : A dataset for What if... reasoning over procedural text<span class=acl-fixed-case>WIQA</span>: A dataset for “What if...” reasoning over procedural text</a></strong><br><a href=/people/n/niket-tandon/>Niket Tandon</a>
|
<a href=/people/b/bhavana-dalvi/>Bhavana Dalvi</a>
|
<a href=/people/k/keisuke-sakaguchi/>Keisuke Sakaguchi</a>
|
<a href=/people/p/peter-clark/>Peter Clark</a>
|
<a href=/people/a/antoine-bosselut/>Antoine Bosselut</a><br><a href=/volumes/D19-1/ class=text-muted>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1629><div class="card-body p-3 small">We introduce WIQA, the first large-scale dataset of What if... questions over procedural text. WIQA contains a collection of paragraphs, each annotated with multiple influence graphs describing how one change affects another, and a large (40k) collection of What if...? multiple-choice questions derived from these. For example, given a paragraph about beach erosion, would stormy weather hasten or decelerate erosion? WIQA contains three kinds of questions : perturbations to steps mentioned in the paragraph ; external (out-of-paragraph) perturbations requiring commonsense knowledge ; and irrelevant (no effect) perturbations. We find that state-of-the-art <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> achieve 73.8 % <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>, well below the <a href=https://en.wikipedia.org/wiki/Human_factors_and_ergonomics>human performance</a> of 96.3 %. We analyze the challenges, in particular tracking chains of influences, and present the <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> as an open challenge to the community.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2300.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-2300/>Proceedings of the Workshop on Methods for Optimizing and Evaluating Neural Language Generation</a></strong><br><a href=/people/a/antoine-bosselut/>Antoine Bosselut</a>
|
<a href=/people/a/asli-celikyilmaz/>Asli Celikyilmaz</a>
|
<a href=/people/m/marjan-ghazvininejad/>Marjan Ghazvininejad</a>
|
<a href=/people/s/srinivasan-iyer/>Srinivasan Iyer</a>
|
<a href=/people/u/urvashi-khandelwal/>Urvashi Khandelwal</a>
|
<a href=/people/h/hannah-rashkin/>Hannah Rashkin</a>
|
<a href=/people/t/thomas-wolf/>Thomas Wolf</a><br><a href=/volumes/W19-23/ class=text-muted>Proceedings of the Workshop on Methods for Optimizing and Evaluating Neural Language Generation</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1244.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1244 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1244 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1244" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N19-1244/>Be Consistent ! Improving Procedural Text Comprehension using Label Consistency</a></strong><br><a href=/people/x/xinya-du/>Xinya Du</a>
|
<a href=/people/b/bhavana-dalvi/>Bhavana Dalvi</a>
|
<a href=/people/n/niket-tandon/>Niket Tandon</a>
|
<a href=/people/a/antoine-bosselut/>Antoine Bosselut</a>
|
<a href=/people/w/wen-tau-yih/>Wen-tau Yih</a>
|
<a href=/people/p/peter-clark/>Peter Clark</a>
|
<a href=/people/c/claire-cardie/>Claire Cardie</a><br><a href=/volumes/N19-1/ class=text-muted>Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1244><div class="card-body p-3 small">Our goal is procedural text comprehension, namely tracking how the properties of entities (e.g., their location) change with time given a procedural text (e.g., a paragraph about photosynthesis, a recipe). This <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> is challenging as the world is changing throughout the text, and despite recent advances, current <a href=https://en.wikipedia.org/wiki/System>systems</a> still struggle with this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. Our approach is to leverage the fact that, for many procedural texts, multiple independent descriptions are readily available, and that predictions from them should be consistent (label consistency). We present a new learning framework that leverages label consistency during training, allowing consistency bias to be built into the <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a>. Evaluation on a standard benchmark dataset for procedural text, ProPara (Dalvi et al., 2018), shows that our approach significantly improves prediction performance (F1) over prior state-of-the-art systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1470.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1470 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1470 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1470" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-1470/>COMET : Commonsense Transformers for Automatic Knowledge Graph Construction<span class=acl-fixed-case>COMET</span>: Commonsense Transformers for Automatic Knowledge Graph Construction</a></strong><br><a href=/people/a/antoine-bosselut/>Antoine Bosselut</a>
|
<a href=/people/h/hannah-rashkin/>Hannah Rashkin</a>
|
<a href=/people/m/maarten-sap/>Maarten Sap</a>
|
<a href=/people/c/chaitanya-malaviya/>Chaitanya Malaviya</a>
|
<a href=/people/a/asli-celikyilmaz/>Asli Celikyilmaz</a>
|
<a href=/people/y/yejin-choi/>Yejin Choi</a><br><a href=/volumes/P19-1/ class=text-muted>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1470><div class="card-body p-3 small">We present the first comprehensive study on automatic knowledge base construction for two prevalent commonsense knowledge graphs : <a href=https://en.wikipedia.org/wiki/ATOMIC>ATOMIC</a> (Sap et al., 2019) and <a href=https://en.wikipedia.org/wiki/ConceptNet>ConceptNet</a> (Speer et al., 2017). Contrary to many conventional <a href=https://en.wikipedia.org/wiki/Knowledge_base>KBs</a> that store knowledge with canonical templates, commonsense KBs only store loosely structured open-text descriptions of knowledge. We posit that an important step toward automatic commonsense completion is the development of generative models of commonsense knowledge, and propose COMmonsEnse Transformers (COMET) that learn to generate rich and diverse commonsense descriptions in natural language. Despite the challenges of commonsense modeling, our investigation reveals promising results when <a href=https://en.wikipedia.org/wiki/Implicit_knowledge>implicit knowledge</a> from deep pre-trained language models is transferred to generate <a href=https://en.wikipedia.org/wiki/Explicit_knowledge>explicit knowledge</a> in commonsense knowledge graphs. Empirical results demonstrate that COMET is able to generate novel knowledge that humans rate as high quality, with up to 77.5 % (ATOMIC) and 91.7 % (ConceptNet) precision at top 1, which approaches human performance for these resources. Our findings suggest that using generative commonsense models for automatic commonsense KB completion could soon be a plausible alternative to extractive methods.</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1006.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1006 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1006 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/305193585 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1006" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1006/>Reasoning about Actions and State Changes by Injecting Commonsense Knowledge</a></strong><br><a href=/people/n/niket-tandon/>Niket Tandon</a>
|
<a href=/people/b/bhavana-dalvi/>Bhavana Dalvi</a>
|
<a href=/people/j/joel-grus/>Joel Grus</a>
|
<a href=/people/w/wen-tau-yih/>Wen-tau Yih</a>
|
<a href=/people/a/antoine-bosselut/>Antoine Bosselut</a>
|
<a href=/people/p/peter-clark/>Peter Clark</a><br><a href=/volumes/D18-1/ class=text-muted>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1006><div class="card-body p-3 small">Comprehending procedural text, e.g., a paragraph describing photosynthesis, requires modeling actions and the state changes they produce, so that questions about entities at different timepoints can be answered. Although several recent systems have shown impressive progress in this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>, their predictions can be globally inconsistent or highly improbable. In this paper, we show how the predicted effects of actions in the context of a paragraph can be improved in two ways : (1) by incorporating global, commonsense constraints (e.g., a non-existent entity can not be destroyed), and (2) by biasing reading with preferences from large-scale corpora (e.g., trees rarely move). Unlike earlier methods, we treat the problem as a neural structured prediction task, allowing <a href=https://en.wikipedia.org/wiki/Constraint_(mathematics)>hard and soft constraints</a> to steer the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> away from unlikely predictions. We show that the new model significantly outperforms earlier <a href=https://en.wikipedia.org/wiki/System>systems</a> on a benchmark dataset for procedural text comprehension (+8 % relative gain), and that it also avoids some of the nonsensical predictions that earlier systems make.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1150.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1150 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1150 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-1150/>Deep Communicating Agents for Abstractive Summarization</a></strong><br><a href=/people/a/asli-celikyilmaz/>Asli Celikyilmaz</a>
|
<a href=/people/a/antoine-bosselut/>Antoine Bosselut</a>
|
<a href=/people/x/xiaodong-he/>Xiaodong He</a>
|
<a href=/people/y/yejin-choi/>Yejin Choi</a><br><a href=/volumes/N18-1/ class=text-muted>Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1150><div class="card-body p-3 small">We present deep communicating agents in an encoder-decoder architecture to address the challenges of representing a long document for abstractive summarization. With deep communicating agents, the task of encoding a long text is divided across multiple collaborating agents, each in charge of a subsection of the input text. These encoders are connected to a single <a href=https://en.wikipedia.org/wiki/Encoder>decoder</a>, trained end-to-end using <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning</a> to generate a focused and coherent summary. Empirical results demonstrate that multiple communicating encoders lead to a higher quality summary compared to several strong baselines, including those based on a single <a href=https://en.wikipedia.org/wiki/Encoder>encoder</a> or multiple non-communicating encoders.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1152.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1152 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1152 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1152.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1152.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-1152" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P18-1152/>Learning to Write with Cooperative Discriminators</a></strong><br><a href=/people/a/ari-holtzman/>Ari Holtzman</a>
|
<a href=/people/j/jan-buys/>Jan Buys</a>
|
<a href=/people/m/maxwell-forbes/>Maxwell Forbes</a>
|
<a href=/people/a/antoine-bosselut/>Antoine Bosselut</a>
|
<a href=/people/d/david-golub/>David Golub</a>
|
<a href=/people/y/yejin-choi/>Yejin Choi</a><br><a href=/volumes/P18-1/ class=text-muted>Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1152><div class="card-body p-3 small">Despite their local fluency, long-form text generated from RNNs is often generic, repetitive, and even self-contradictory. We propose a unified learning framework that collectively addresses all the above issues by composing a committee of discriminators that can guide a base RNN generator towards more globally coherent generations. More concretely, discriminators each specialize in a different principle of communication, such as <a href=https://en.wikipedia.org/wiki/Grice&#8217;s_maxims>Grice&#8217;s maxims</a>, and are collectively combined with the base RNN generator through a composite decoding objective. Human evaluation demonstrates that text generated by our model is preferred over that of baselines by a large margin, significantly enhancing the overall coherence, style, and information of the generations.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Antoine+Bosselut" title="Search for 'Antoine Bosselut' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/y/yejin-choi/ class=align-middle>Yejin Choi</a>
<span class="badge badge-secondary align-middle ml-2">5</span></li><li class=list-group-item><a href=/people/n/niket-tandon/ class=align-middle>Niket Tandon</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/b/bhavana-dalvi/ class=align-middle>Bhavana Dalvi</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/p/peter-clark/ class=align-middle>Peter Clark</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/a/asli-celikyilmaz/ class=align-middle>Asli Celikyilmaz</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/m/maxwell-forbes/ class=align-middle>Maxwell Forbes</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/w/wen-tau-yih/ class=align-middle>Wen-tau Yih</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/e/esin-durmus/ class=align-middle>Esin Durmus</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/s/sebastian-gehrmann/ class=align-middle>Sebastian Gehrmann</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/y/yacine-jernite/ class=align-middle>Yacine Jernite</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/l/laura-perez-beltrachini/ class=align-middle>Laura Perez-Beltrachini</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/s/samira-shaikh/ class=align-middle>Samira Shaikh</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/w/wei-xu/ class=align-middle>Wei Xu</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/h/hannah-rashkin/ class=align-middle>Hannah Rashkin</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/j/jeff-da/ class=align-middle>Jeff Da</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/rowan-zellers/ class=align-middle>Rowan Zellers</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/anthony-zheng/ class=align-middle>Anthony Zheng</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jena-d-hwang/ class=align-middle>Jena D. Hwang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/joel-grus/ class=align-middle>Joel Grus</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/v/varun-prashant-gangal/ class=align-middle>Varun Prashant Gangal</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/tosin-adewumi/ class=align-middle>Tosin Adewumi</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/karmanya-aggarwal/ class=align-middle>Karmanya Aggarwal</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/p/pawan-sasanka-ammanamanchi/ class=align-middle>Pawan Sasanka Ammanamanchi</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/anuoluwapo-aremu/ class=align-middle>Anuoluwapo Aremu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/khyathi-raghavi-chandu/ class=align-middle>Khyathi Raghavi Chandu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/miruna-clinciu/ class=align-middle>Miruna Clinciu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/dipanjan-das/ class=align-middle>Dipanjan Das</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/kaustubh-dhole/ class=align-middle>Kaustubh Dhole</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/w/wanyu-du/ class=align-middle>Wanyu Du</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/o/ondrej-dusek/ class=align-middle>Ondřej Dušek</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/chris-chinenye-emezue/ class=align-middle>Chris Chinenye Emezue</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/v/varun-gangal/ class=align-middle>Varun Gangal</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/cristina-garbacea/ class=align-middle>Cristina Garbacea</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/tatsunori-b-hashimoto/ class=align-middle>Tatsunori B. Hashimoto</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yufang-hou/ class=align-middle>Yufang Hou</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/harsh-jhamtani/ class=align-middle>Harsh Jhamtani</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yangfeng-ji/ class=align-middle>Yangfeng Ji</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/shailza-jolly/ class=align-middle>Shailza Jolly</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/mihir-kale/ class=align-middle>Mihir Kale</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/dhruv-kumar/ class=align-middle>Dhruv Kumar</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/f/faisal-ladhak/ class=align-middle>Faisal Ladhak</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/aman-madaan/ class=align-middle>Aman Madaan</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/mounica-maddela/ class=align-middle>Mounica Maddela</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/khyati-mahajan/ class=align-middle>Khyati Mahajan</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/saad-mahamood/ class=align-middle>Saad Mahamood</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/b/bodhisattwa-prasad-majumder/ class=align-middle>Bodhisattwa Prasad Majumder</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/p/pedro-henrique-martins/ class=align-middle>Pedro Henrique Martins</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/angelina-mcmillan-major/ class=align-middle>Angelina McMillan-Major</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/simon-mille/ class=align-middle>Simon Mille</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/e/emiel-van-miltenburg/ class=align-middle>Emiel Van Miltenburg</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/moin-nadeem/ class=align-middle>Moin Nadeem</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/shashi-narayan/ class=align-middle>Shashi Narayan</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/v/vitaly-nikolaev/ class=align-middle>Vitaly Nikolaev</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/andre-niyongabo-rubungo/ class=align-middle>Andre Niyongabo Rubungo</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/salomey-osei/ class=align-middle>Salomey Osei</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/ankur-parikh/ class=align-middle>Ankur Parikh</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/n/niranjan-ramesh-rao/ class=align-middle>Niranjan Ramesh Rao</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/v/vikas-raunak/ class=align-middle>Vikas Raunak</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/juan-diego-rodriguez/ class=align-middle>Juan Diego Rodriguez</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/sashank-santhanam/ class=align-middle>Sashank Santhanam</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/joao-sedoc/ class=align-middle>João Sedoc</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/thibault-sellam/ class=align-middle>Thibault Sellam</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/anastasia-shimorina/ class=align-middle>Anastasia Shimorina</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/marco-antonio-sobrevilla-cabezudo/ class=align-middle>Marco Antonio Sobrevilla Cabezudo</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/hendrik-strobelt/ class=align-middle>Hendrik Strobelt</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/n/nishant-subramani/ class=align-middle>Nishant Subramani</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/diyi-yang/ class=align-middle>Diyi Yang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/akhila-yerukola/ class=align-middle>Akhila Yerukola</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jiawei-zhou/ class=align-middle>Jiawei Zhou</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/keisuke-sakaguchi/ class=align-middle>Keisuke Sakaguchi</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/liwei-jiang/ class=align-middle>Liwei Jiang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/chandra-bhagavatula/ class=align-middle>Chandra Bhagavatula</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/marjan-ghazvininejad/ class=align-middle>Marjan Ghazvininejad</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/srinivasan-iyer/ class=align-middle>Srinivasan Iyer</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/u/urvashi-khandelwal/ class=align-middle>Urvashi Khandelwal</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/thomas-wolf/ class=align-middle>Thomas Wolf</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/x/xinya-du/ class=align-middle>Xinya Du</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/claire-cardie/ class=align-middle>Claire Cardie</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/x/xiaodong-he/ class=align-middle>Xiaodong He</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/ari-holtzman/ class=align-middle>Ari Holtzman</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jan-buys/ class=align-middle>Jan Buys</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/david-golub/ class=align-middle>David Golub</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/maarten-sap/ class=align-middle>Maarten Sap</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/chaitanya-malaviya/ class=align-middle>Chaitanya Malaviya</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/acl/ class=align-middle>ACL</a><span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/venues/naacl/ class=align-middle>NAACL</a><span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/venues/emnlp/ class=align-middle>EMNLP</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/gem/ class=align-middle>GEM</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/ws/ class=align-middle>WS</a><span class="badge badge-secondary align-middle ml-2">1</span></li></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>