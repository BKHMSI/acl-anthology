<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Alessandro Lenci - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Alessandro</span> <span class=font-weight-bold>Lenci</span></h2><hr><div class=row><div class=col-lg-9><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.cl-3.20.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--cl-3--20 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.cl-3.20 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.cl-3.20/>Decoding Word Embeddings with Brain-Based Semantic Features</a></strong><br><a href=/people/e/emmanuele-chersoni/>Emmanuele Chersoni</a>
|
<a href=/people/e/enrico-santus/>Enrico Santus</a>
|
<a href=/people/c/chu-ren-huang/>Chu-Ren Huang</a>
|
<a href=/people/a/alessandro-lenci/>Alessandro Lenci</a><br><a href=/volumes/2021.cl-3/ class=text-muted>Computational Linguistics, Volume 47, Issue 3 - November 2021</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--cl-3--20><div class="card-body p-3 small">Word embeddings are vectorial semantic representations built with either counting or predicting techniques aimed at capturing shades of meaning from word co-occurrences. Since their introduction, these <a href=https://en.wikipedia.org/wiki/Representation_(arts)>representations</a> have been criticized for lacking interpretable dimensions. This property of <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> limits our understanding of the <a href=https://en.wikipedia.org/wiki/Semantic_feature>semantic features</a> they actually encode. Moreover, it contributes to the black box nature of the tasks in which they are used, since the reasons for <a href=https://en.wikipedia.org/wiki/Word_embedding>word embedding</a> performance often remain opaque to humans. In this contribution, we explore the semantic properties encoded in word embeddings by mapping them onto interpretable vectors, consisting of explicit and neurobiologically motivated semantic features (Binder et al. Our exploration takes into account different types of embeddings, including factorized count vectors and predict models (Skip-Gram, GloVe, etc.), as well as the most recent contextualized representations (i.e., ELMo and BERT). In our analysis, we first evaluate the quality of the <a href=https://en.wikipedia.org/wiki/Map_(mathematics)>mapping</a> in a retrieval task, then we shed light on the semantic features that are better encoded in each embedding type. A large number of probing tasks is finally set to assess how the original and the mapped embeddings perform in discriminating semantic categories. For each probing task, we identify the most relevant semantic features and we show that there is a correlation between the <a href=https://en.wikipedia.org/wiki/Embedding>embedding</a> performance and how they encode those <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a>. This study sets itself as a step forward in understanding which aspects of meaning are captured by <a href=https://en.wikipedia.org/wiki/Vector_space>vector spaces</a>, by proposing a new and simple method to carve human-interpretable semantic representations from <a href=https://en.wikipedia.org/wiki/Distribution_(mathematics)>distributional vectors</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.cmcl-1.12.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--cmcl-1--12 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.cmcl-1.12 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.cmcl-1.12/>PIHKers at CMCL 2021 Shared Task : Cosine Similarity and Surprisal to Predict Human Reading Patterns.<span class=acl-fixed-case>PIHK</span>ers at <span class=acl-fixed-case>CMCL</span> 2021 Shared Task: Cosine Similarity and Surprisal to Predict Human Reading Patterns.</a></strong><br><a href=/people/l/lavinia-salicchi/>Lavinia Salicchi</a>
|
<a href=/people/a/alessandro-lenci/>Alessandro Lenci</a><br><a href=/volumes/2021.cmcl-1/ class=text-muted>Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--cmcl-1--12><div class="card-body p-3 small">Eye-tracking psycholinguistic studies have revealed that context-word semantic coherence and <a href=https://en.wikipedia.org/wiki/Predictability>predictability</a> influence <a href=https://en.wikipedia.org/wiki/Language_processing_in_the_brain>language processing</a>. In this paper we show our approach to predict eye-tracking features from the ZuCo dataset for the shared task of the Cognitive Modeling and Computational Linguistics (CMCL2021) workshop. Using both cosine similarity and surprisal within a <a href=https://en.wikipedia.org/wiki/Regression_analysis>regression model</a>, we significantly improved the baseline Mean Absolute Error computed among five eye-tracking features.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.blackboxnlp-1.13.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--blackboxnlp-1--13 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.blackboxnlp-1.13 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.blackboxnlp-1.13/>A howling success or a working sea? Testing what BERT knows about metaphors<span class=acl-fixed-case>BERT</span> knows about metaphors</a></strong><br><a href=/people/p/paolo-pedinotti/>Paolo Pedinotti</a>
|
<a href=/people/e/eliana-di-palma/>Eliana Di Palma</a>
|
<a href=/people/l/ludovica-cerini/>Ludovica Cerini</a>
|
<a href=/people/a/alessandro-lenci/>Alessandro Lenci</a><br><a href=/volumes/2021.blackboxnlp-1/ class=text-muted>Proceedings of the Fourth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--blackboxnlp-1--13><div class="card-body p-3 small">Metaphor is a widespread linguistic and cognitive phenomenon that is ruled by mechanisms which have received attention in the literature. Transformer Language Models such as BERT have brought improvements in metaphor-related tasks. However, they have been used only in application contexts, while their knowledge of the phenomenon has not been analyzed. To test what BERT knows about <a href=https://en.wikipedia.org/wiki/Metaphor>metaphors</a>, we challenge it on a new dataset that we designed to test various aspects of this phenomenon such as variations in linguistic structure, variations in <a href=https://en.wikipedia.org/wiki/Convention_(norm)>conventionality</a>, the boundaries of the plausibility of a metaphor and the interpretations that we attribute to metaphoric expressions. Results bring out some tendencies that suggest that the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> can reproduce some human intuitions about <a href=https://en.wikipedia.org/wiki/Metaphor>metaphors</a>.</div></div><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.starsem-1.14.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--starsem-1--14 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.starsem-1.14 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.starsem-1.14" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.starsem-1.14/>PISA : A measure of Preference In Selection of Arguments to model verb argument recoverability<span class=acl-fixed-case>PISA</span>: A measure of Preference In Selection of Arguments to model verb argument recoverability</a></strong><br><a href=/people/g/giulia-cappelli/>Giulia Cappelli</a>
|
<a href=/people/a/alessandro-lenci/>Alessandro Lenci</a><br><a href=/volumes/2020.starsem-1/ class=text-muted>Proceedings of the Ninth Joint Conference on Lexical and Computational Semantics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--starsem-1--14><div class="card-body p-3 small">Our paper offers a computational model of the semantic recoverability of verb arguments, tested in particular on direct objects and Instruments. Our fully distributional model is intended to improve on older taxonomy-based models, which require a lexicon in addition to the training corpus. We computed the selectional preferences of 99 transitive verbs and 173 Instrument verbs as the mean value of the pairwise cosines between their arguments (a weighted mean between all the arguments, or an unweighted mean with the topmost k arguments). Results show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> can predict the recoverability of objects and Instruments, providing a similar result to that of taxonomy-based models but at a much cheaper computational cost.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.cogalex-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.cogalex-1.0/>Proceedings of the Workshop on the Cognitive Aspects of the Lexicon</a></strong><br><a href=/people/m/michael-zock/>Michael Zock</a>
|
<a href=/people/e/emmanuele-chersoni/>Emmanuele Chersoni</a>
|
<a href=/people/a/alessandro-lenci/>Alessandro Lenci</a>
|
<a href=/people/e/enrico-santus/>Enrico Santus</a><br><a href=/volumes/2020.cogalex-1/ class=text-muted>Proceedings of the Workshop on the Cognitive Aspects of the Lexicon</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.602.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--602 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.602 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.602/>Do n’t Invite BERT to Drink a Bottle : Modeling the Interpretation of Metonymies Using BERT and Distributional Representations<span class=acl-fixed-case>BERT</span> to Drink a Bottle: Modeling the Interpretation of Metonymies Using <span class=acl-fixed-case>BERT</span> and Distributional Representations</a></strong><br><a href=/people/p/paolo-pedinotti/>Paolo Pedinotti</a>
|
<a href=/people/a/alessandro-lenci/>Alessandro Lenci</a><br><a href=/volumes/2020.coling-main/ class=text-muted>Proceedings of the 28th International Conference on Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--602><div class="card-body p-3 small">In this work, we carry out two experiments in order to assess the ability of BERT to capture the meaning shift associated with <a href=https://en.wikipedia.org/wiki/Metonymy>metonymic expressions</a>. We test the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> on a new <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> that is representative of the most common types of <a href=https://en.wikipedia.org/wiki/Metonymy>metonymy</a>. We compare BERT with the Structured Distributional Model (SDM), a model for the representation of words in context which is based on the notion of Generalized Event Knowledge. The results reveal that, while BERT ability to deal with <a href=https://en.wikipedia.org/wiki/Metonymy>metonymy</a> is quite limited, SDM is good at predicting the meaning of metonymic expressions, providing support for an account of <a href=https://en.wikipedia.org/wiki/Metonymy>metonymy</a> based on event knowledge.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.700.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--700 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.700 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.700/>Are Word Embeddings Really a Bad Fit for the Estimation of Thematic Fit?</a></strong><br><a href=/people/e/emmanuele-chersoni/>Emmanuele Chersoni</a>
|
<a href=/people/l/ludovica-pannitto/>Ludovica Pannitto</a>
|
<a href=/people/e/enrico-santus/>Enrico Santus</a>
|
<a href=/people/a/alessandro-lenci/>Alessandro Lenci</a>
|
<a href=/people/c/chu-ren-huang/>Chu-Ren Huang</a><br><a href=/volumes/2020.lrec-1/ class=text-muted>Proceedings of the 12th Language Resources and Evaluation Conference</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--700><div class="card-body p-3 small">While neural embeddings represent a popular choice for word representation in a wide variety of NLP tasks, their usage for thematic fit modeling has been limited, as they have been reported to lag behind syntax-based count models. In this paper, we propose a complete evaluation of count models and <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> on thematic fit estimation, by taking into account a larger number of parameters and verb roles and introducing also dependency-based embeddings in the comparison. Our results show a complex scenario, where a determinant factor for the performance seems to be the availability to the model of reliable syntactic information for building the distributional representations of the roles.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.718.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--718 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.718 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.718/>Representing Verbs with Visual Argument Vectors</a></strong><br><a href=/people/i/irene-sucameli/>Irene Sucameli</a>
|
<a href=/people/a/alessandro-lenci/>Alessandro Lenci</a><br><a href=/volumes/2020.lrec-1/ class=text-muted>Proceedings of the 12th Language Resources and Evaluation Conference</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--718><div class="card-body p-3 small">Is it possible to use <a href=https://en.wikipedia.org/wiki/Digital_image>images</a> to model verb semantic similarities? Starting from this core question, we developed two textual distributional semantic models and a visual one. We found particularly interesting and challenging to investigate this Part of Speech since verbs are not often analysed in researches focused on multimodal distributional semantics. After the creation of the visual and textual distributional space, the three models were evaluated in relation to SimLex-999, a gold standard resource. Through this evaluation, we demonstrate that, using visual distributional models, it is possible to extract meaningful information and to effectively capture the <a href=https://en.wikipedia.org/wiki/Semantic_similarity>semantic similarity</a> between verbs.</div></div><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2900.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-2900/>Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics</a></strong><br><a href=/people/e/emmanuele-chersoni/>Emmanuele Chersoni</a>
|
<a href=/people/c/cassandra-l-jacobs/>Cassandra Jacobs</a>
|
<a href=/people/a/alessandro-lenci/>Alessandro Lenci</a>
|
<a href=/people/t/tal-linzen/>Tal Linzen</a>
|
<a href=/people/l/laurent-prevot/>Laurent Prévot</a>
|
<a href=/people/e/enrico-santus/>Enrico Santus</a><br><a href=/volumes/W19-29/ class=text-muted>Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics</a></span></p><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S18-1117.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S18-1117 data-toggle=collapse aria-expanded=false aria-controls=abstract-S18-1117 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/S18-1117/>SemEval-2018 Task 10 : Capturing Discriminative Attributes<span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2018 Task 10: Capturing Discriminative Attributes</a></strong><br><a href=/people/a/alicia-krebs/>Alicia Krebs</a>
|
<a href=/people/a/alessandro-lenci/>Alessandro Lenci</a>
|
<a href=/people/d/denis-paperno/>Denis Paperno</a><br><a href=/volumes/S18-1/ class=text-muted>Proceedings of The 12th International Workshop on Semantic Evaluation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S18-1117><div class="card-body p-3 small">This paper describes the SemEval 2018 Task 10 on Capturing Discriminative Attributes. Participants were asked to identify whether an attribute could help discriminate between two concepts. For example, a successful <a href=https://en.wikipedia.org/wiki/System>system</a> should determine that &#8216;urine&#8217; is a discriminating feature in the word pair &#8216;kidney&#8217;, &#8216;bone&#8217;. The aim of the task is to better evaluate the capabilities of state of the art semantic models, beyond pure semantic similarity. The <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> attracted submissions from 21 teams, and the best <a href=https://en.wikipedia.org/wiki/System>system</a> achieved a 0.75 F1 score.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S18-2000.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/S18-2000/>Proceedings of the Seventh Joint Conference on Lexical and Computational Semantics</a></strong><br><a href=/people/m/malvina-nissim/>Malvina Nissim</a>
|
<a href=/people/j/jonathan-berant/>Jonathan Berant</a>
|
<a href=/people/a/alessandro-lenci/>Alessandro Lenci</a><br><a href=/volumes/S18-2/ class=text-muted>Proceedings of the Seventh Joint Conference on Lexical and Computational Semantics</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-2800.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-2800/>Proceedings of the Eight Workshop on Cognitive Aspects of Computational Language Learning and Processing</a></strong><br><a href=/people/m/marco-idiart/>Marco Idiart</a>
|
<a href=/people/a/alessandro-lenci/>Alessandro Lenci</a>
|
<a href=/people/t/thierry-poibeau/>Thierry Poibeau</a>
|
<a href=/people/a/aline-villavicencio/>Aline Villavicencio</a><br><a href=/volumes/W18-28/ class=text-muted>Proceedings of the Eight Workshop on Cognitive Aspects of Computational Language Learning and Processing</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4603.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4603 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4603 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4603/>Modeling Violations of Selectional Restrictions with Distributional Semantics</a></strong><br><a href=/people/e/emmanuele-chersoni/>Emmanuele Chersoni</a>
|
<a href=/people/a/adria-torrens-urrutia/>Adrià Torrens Urrutia</a>
|
<a href=/people/p/philippe-blache/>Philippe Blache</a>
|
<a href=/people/a/alessandro-lenci/>Alessandro Lenci</a><br><a href=/volumes/W18-46/ class=text-muted>Proceedings of the Workshop on Linguistic Complexity and Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4603><div class="card-body p-3 small">Distributional Semantic Models have been successfully used for modeling selectional preferences in a variety of scenarios, since distributional similarity naturally provides an estimate of the degree to which an argument satisfies the requirement of a given predicate. However, we argue that the performance of such models on rare verb-argument combinations has received relatively little attention : it is not clear whether they are able to distinguish the combinations that are simply atypical, or implausible, from the semantically anomalous ones, and in particular, they have never been tested on the task of modeling their differences in processing complexity. In this paper, we compare two different models of thematic fit by testing their ability of identifying violations of selectional restrictions in two datasets from the experimental studies.</div></div><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S17-1021.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S17-1021 data-toggle=collapse aria-expanded=false aria-controls=abstract-S17-1021 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/S17-1021/>Logical Metonymy in a Distributional Model of Sentence Comprehension</a></strong><br><a href=/people/e/emmanuele-chersoni/>Emmanuele Chersoni</a>
|
<a href=/people/a/alessandro-lenci/>Alessandro Lenci</a>
|
<a href=/people/p/philippe-blache/>Philippe Blache</a><br><a href=/volumes/S17-1/ class=text-muted>Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (*SEM 2017)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S17-1021><div class="card-body p-3 small">In <a href=https://en.wikipedia.org/wiki/Theoretical_linguistics>theoretical linguistics</a>, logical metonymy is defined as the combination of an event-subcategorizing verb with an <a href=https://en.wikipedia.org/wiki/Object_(grammar)>entity-denoting direct object</a> (e.g., The author began the book), so that the interpretation of the VP requires the retrieval of a covert event (e.g., writing). Psycholinguistic studies have revealed extra processing costs for logical metonymy, a phenomenon generally explained with the introduction of new semantic structure. In this paper, we present a general <a href=https://en.wikipedia.org/wiki/Distribution_(mathematics)>distributional model</a> for <a href=https://en.wikipedia.org/wiki/Sentence_processing>sentence comprehension</a> inspired by the Memory, Unification and Control model by Hagoort (2013,2016). We show that our distributional framework can account for the extra processing costs of logical metonymy and can identify the covert event in a <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification task</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1068.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1068 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1068 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/238234243 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D17-1068" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/D17-1068/>Measuring Thematic Fit with Distributional Feature Overlap</a></strong><br><a href=/people/e/enrico-santus/>Enrico Santus</a>
|
<a href=/people/e/emmanuele-chersoni/>Emmanuele Chersoni</a>
|
<a href=/people/a/alessandro-lenci/>Alessandro Lenci</a>
|
<a href=/people/p/philippe-blache/>Philippe Blache</a><br><a href=/volumes/D17-1/ class=text-muted>Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1068><div class="card-body p-3 small">In this paper, we introduce a new <a href=https://en.wikipedia.org/wiki/Distribution_(mathematics)>distributional method</a> for modeling predicate-argument thematic fit judgments. We use a syntax-based DSM to build a prototypical representation of verb-specific roles : for every verb, we extract the most salient second order contexts for each of its roles (i.e. the most salient dimensions of typical role fillers), and then we compute thematic fit as a weighted overlap between the top features of candidate fillers and role prototypes. Our experiments show that our method consistently outperforms a baseline re-implementing a state-of-the-art <a href=https://en.wikipedia.org/wiki/System>system</a>, and achieves better or comparable results to those reported in the literature for the other <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised systems</a>. Moreover, it provides an explicit representation of the <a href=https://en.wikipedia.org/wiki/Feature_(linguistics)>features</a> characterizing verb-specific semantic roles.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Alessandro+Lenci" title="Search for 'Alessandro Lenci' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/e/emmanuele-chersoni/ class=align-middle>Emmanuele Chersoni</a>
<span class="badge badge-secondary align-middle ml-2">7</span></li><li class=list-group-item><a href=/people/e/enrico-santus/ class=align-middle>Enrico Santus</a>
<span class="badge badge-secondary align-middle ml-2">5</span></li><li class=list-group-item><a href=/people/p/philippe-blache/ class=align-middle>Philippe Blache</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/c/chu-ren-huang/ class=align-middle>Chu-Ren Huang</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/p/paolo-pedinotti/ class=align-middle>Paolo Pedinotti</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/g/giulia-cappelli/ class=align-middle>Giulia Cappelli</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/michael-zock/ class=align-middle>Michael Zock</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/alicia-krebs/ class=align-middle>Alicia Krebs</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/denis-paperno/ class=align-middle>Denis Paperno</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/malvina-nissim/ class=align-middle>Malvina Nissim</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jonathan-berant/ class=align-middle>Jonathan Berant</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/lavinia-salicchi/ class=align-middle>Lavinia Salicchi</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/e/eliana-di-palma/ class=align-middle>Eliana Di Palma</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/ludovica-cerini/ class=align-middle>Ludovica Cerini</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/marco-idiart/ class=align-middle>Marco Idiart</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/thierry-poibeau/ class=align-middle>Thierry Poibeau</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/aline-villavicencio/ class=align-middle>Aline Villavicencio</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/adria-torrens-urrutia/ class=align-middle>Adrià Torrens Urrutia</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/cassandra-l-jacobs/ class=align-middle>Cassandra L. Jacobs</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/tal-linzen/ class=align-middle>Tal Linzen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/laurent-prevot/ class=align-middle>Laurent Prévot</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/ludovica-pannitto/ class=align-middle>Ludovica Pannitto</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/i/irene-sucameli/ class=align-middle>Irene Sucameli</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/semeval/ class=align-middle>SemEval</a><span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/venues/ws/ class=align-middle>WS</a><span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/venues/lrec/ class=align-middle>LREC</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/starsem/ class=align-middle>*SEM</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/cl/ class=align-middle>CL</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-venues aria-expanded=false aria-controls=more-venues>show all...</li><div class="collapse border-top" id=more-venues><li class=list-group-item><a href=/venues/cogalex/ class=align-middle>CogALex</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/emnlp/ class=align-middle>EMNLP</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/cmcl/ class=align-middle>CMCL</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/blackboxnlp/ class=align-middle>BlackboxNLP</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/coling/ class=align-middle>COLING</a><span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>