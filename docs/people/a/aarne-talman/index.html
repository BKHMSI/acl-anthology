<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Aarne Talman - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Aarne</span> <span class=font-weight-bold>Talman</span></h2><hr><div class=row><div class=col-lg-9><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.nodalida-main.28.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--nodalida-main--28 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.nodalida-main.28 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.nodalida-main.28" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.nodalida-main.28/>NLI Data Sanity Check : Assessing the Effect of <a href=https://en.wikipedia.org/wiki/Data_corruption>Data Corruption</a> on Model Performance<span class=acl-fixed-case>NLI</span> Data Sanity Check: Assessing the Effect of Data Corruption on Model Performance</a></strong><br><a href=/people/a/aarne-talman/>Aarne Talman</a>
|
<a href=/people/m/marianna-apidianaki/>Marianna Apidianaki</a>
|
<a href=/people/s/stergios-chatzikyriakidis/>Stergios Chatzikyriakidis</a>
|
<a href=/people/j/jorg-tiedemann/>Jörg Tiedemann</a><br><a href=/volumes/2021.nodalida-main/ class=text-muted>Proceedings of the 23rd Nordic Conference on Computational Linguistics (NoDaLiDa)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--nodalida-main--28><div class="card-body p-3 small">Pre-trained neural language models give high performance on natural language inference (NLI) tasks. But whether they actually understand the meaning of the processed sequences is still unclear. We propose a new diagnostics test suite which allows to assess whether a <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> constitutes a good testbed for evaluating the models&#8217; meaning understanding capabilities. We specifically apply controlled corruption transformations to widely used <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmarks</a> (MNLI and ANLI), which involve removing entire word classes and often lead to non-sensical sentence pairs. If model accuracy on the corrupted data remains high, then the <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> is likely to contain <a href=https://en.wikipedia.org/wiki/Bias_(statistics)>statistical biases</a> and artefacts that guide <a href=https://en.wikipedia.org/wiki/Prediction>prediction</a>. Inversely, a large decrease in <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>model accuracy</a> indicates that the original <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> provides a proper challenge to the models&#8217; reasoning capabilities. Hence, our proposed controls can serve as a crash test for developing high quality data for NLI tasks.</div></div><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4810.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4810 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4810 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4810/>Testing the Generalization Power of Neural Network Models across NLI Benchmarks<span class=acl-fixed-case>NLI</span> Benchmarks</a></strong><br><a href=/people/a/aarne-talman/>Aarne Talman</a>
|
<a href=/people/s/stergios-chatzikyriakidis/>Stergios Chatzikyriakidis</a><br><a href=/volumes/W19-48/ class=text-muted>Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4810><div class="card-body p-3 small">Neural network models have been very successful in <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language inference</a>, with the best <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> reaching 90 % <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> in some benchmarks. However, the success of these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> turns out to be largely benchmark specific. We show that <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> trained on a natural language inference dataset drawn from one benchmark fail to perform well in others, even if the notion of <a href=https://en.wikipedia.org/wiki/Statistical_inference>inference</a> assumed in these <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmarks</a> is the same or similar. We train six high performing neural network models on different datasets and show that each one of these has problems of generalizing when we replace the original test set with a test set taken from another corpus designed for the same task. In light of these results, we argue that most of the current neural network models are not able to generalize well in the task of <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language inference</a>. We find that using large pre-trained language models helps with <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a> when the datasets are similar enough. Our results also highlight that the current NLI datasets do not cover the different nuances of <a href=https://en.wikipedia.org/wiki/Statistical_inference>inference</a> extensively enough.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5347.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5347 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5347 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5347/>The University of Helsinki Submissions to the WMT19 News Translation Task<span class=acl-fixed-case>U</span>niversity of <span class=acl-fixed-case>H</span>elsinki Submissions to the <span class=acl-fixed-case>WMT</span>19 News Translation Task</a></strong><br><a href=/people/a/aarne-talman/>Aarne Talman</a>
|
<a href=/people/u/umut-sulubacak/>Umut Sulubacak</a>
|
<a href=/people/r/raul-vazquez/>Raúl Vázquez</a>
|
<a href=/people/y/yves-scherrer/>Yves Scherrer</a>
|
<a href=/people/s/sami-virpioja/>Sami Virpioja</a>
|
<a href=/people/a/alessandro-raganato/>Alessandro Raganato</a>
|
<a href=/people/a/arvi-hurskainen/>Arvi Hurskainen</a>
|
<a href=/people/j/jorg-tiedemann/>Jörg Tiedemann</a><br><a href=/volumes/W19-53/ class=text-muted>Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5347><div class="card-body p-3 small">In this paper we present the University of Helsinki submissions to the WMT 2019 shared news translation task in three language pairs : <a href=https://en.wikipedia.org/wiki/German_language>English-German</a>, <a href=https://en.wikipedia.org/wiki/Finnish_language>English-Finnish</a> and <a href=https://en.wikipedia.org/wiki/Finnish_language>Finnish-English</a>. This year we focused first on cleaning and filtering the training data using multiple data-filtering approaches, resulting in much smaller and cleaner training sets. For <a href=https://en.wikipedia.org/wiki/German_language>English-German</a> we trained both sentence-level transformer models as well as compared different document-level translation approaches. For Finnish-English and English-Finnish we focused on different segmentation approaches and we also included a rule-based system for English-Finnish.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-6129.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-6129 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-6129 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-6129" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-6129/>Predicting Prosodic Prominence from Text with Pre-trained Contextualized Word Representations</a></strong><br><a href=/people/a/aarne-talman/>Aarne Talman</a>
|
<a href=/people/a/antti-suni/>Antti Suni</a>
|
<a href=/people/h/hande-celikkanat/>Hande Celikkanat</a>
|
<a href=/people/s/sofoklis-kakouros/>Sofoklis Kakouros</a>
|
<a href=/people/j/jorg-tiedemann/>Jörg Tiedemann</a>
|
<a href=/people/m/martti-vainio/>Martti Vainio</a><br><a href=/volumes/W19-61/ class=text-muted>Proceedings of the 22nd Nordic Conference on Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-6129><div class="card-body p-3 small">In this paper we introduce a new natural language processing dataset and <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmark</a> for predicting prosodic prominence from written text. To our knowledge this will be the largest publicly available dataset with <a href=https://en.wikipedia.org/wiki/Prosody_(linguistics)>prosodic labels</a>. We describe the dataset construction and the resulting benchmark dataset in detail and train a number of different models ranging from feature-based classifiers to <a href=https://en.wikipedia.org/wiki/Artificial_neural_network>neural network systems</a> for the prediction of discretized prosodic prominence. We show that pre-trained contextualized word representations from BERT outperform the other models even with less than 10 % of the training data. Finally we discuss the <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> in light of the results and point to future research and plans for further improving both the <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> and methods of predicting prosodic prominence from <a href=https://en.wikipedia.org/wiki/Text_(literary_theory)>text</a>. The <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> and the code for the <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> will be made publicly available.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Aarne+Talman" title="Search for 'Aarne Talman' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/j/jorg-tiedemann/ class=align-middle>Jörg Tiedemann</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/s/stergios-chatzikyriakidis/ class=align-middle>Stergios Chatzikyriakidis</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/m/marianna-apidianaki/ class=align-middle>Marianna Apidianaki</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/u/umut-sulubacak/ class=align-middle>Umut Sulubacak</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/raul-vazquez/ class=align-middle>Raúl Vázquez</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/y/yves-scherrer/ class=align-middle>Yves Scherrer</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/sami-virpioja/ class=align-middle>Sami Virpioja</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/alessandro-raganato/ class=align-middle>Alessandro Raganato</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/arvi-hurskainen/ class=align-middle>Arvi Hurskainen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/antti-suni/ class=align-middle>Antti Suni</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/hande-celikkanat/ class=align-middle>Hande Celikkanat</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/sofoklis-kakouros/ class=align-middle>Sofoklis Kakouros</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/martti-vainio/ class=align-middle>Martti Vainio</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/ws/ class=align-middle>WS</a><span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/venues/nodalida/ class=align-middle>NoDaLiDa</a><span class="badge badge-secondary align-middle ml-2">1</span></li></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>