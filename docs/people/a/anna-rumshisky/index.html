<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Anna Rumshisky - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Anna</span> <span class=font-weight-bold>Rumshisky</span></h2><hr><div class=row><div class=col-lg-9><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.0/>Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</a></strong><br><a href=/people/k/kristina-toutanova/>Kristina Toutanova</a>
|
<a href=/people/a/anna-rumshisky/>Anna Rumshisky</a>
|
<a href=/people/l/luke-zettlemoyer/>Luke Zettlemoyer</a>
|
<a href=/people/d/dilek-hakkani-tur/>Dilek Hakkani-Tur</a>
|
<a href=/people/i/iz-beltagy/>Iz Beltagy</a>
|
<a href=/people/s/steven-bethard/>Steven Bethard</a>
|
<a href=/people/r/ryan-cotterell/>Ryan Cotterell</a>
|
<a href=/people/t/tanmoy-chakraborty/>Tanmoy Chakraborty</a>
|
<a href=/people/y/yichao-zhou/>Yichao Zhou</a><br><a href=/volumes/2021.naacl-main/ class=text-muted>Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.insights-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.insights-1.0/>Proceedings of the Second Workshop on Insights from Negative Results in NLP</a></strong><br><a href=/people/j/joao-sedoc/>João Sedoc</a>
|
<a href=/people/a/anna-rogers/>Anna Rogers</a>
|
<a href=/people/a/anna-rumshisky/>Anna Rumshisky</a>
|
<a href=/people/s/shabnam-tafreshi/>Shabnam Tafreshi</a><br><a href=/volumes/2021.insights-1/ class=text-muted>Proceedings of the Second Workshop on Insights from Negative Results in NLP</a></span></p><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.clinicalnlp-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.clinicalnlp-1.0/>Proceedings of the 3rd Clinical Natural Language Processing Workshop</a></strong><br><a href=/people/a/anna-rumshisky/>Anna Rumshisky</a>
|
<a href=/people/k/kirk-roberts/>Kirk Roberts</a>
|
<a href=/people/s/steven-bethard/>Steven Bethard</a>
|
<a href=/people/t/tristan-naumann/>Tristan Naumann</a><br><a href=/volumes/2020.clinicalnlp-1/ class=text-muted>Proceedings of the 3rd Clinical Natural Language Processing Workshop</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.insights-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.insights-1.0/>Proceedings of the First Workshop on Insights from Negative Results in NLP</a></strong><br><a href=/people/a/anna-rogers/>Anna Rogers</a>
|
<a href=/people/j/joao-sedoc/>João Sedoc</a>
|
<a href=/people/a/anna-rumshisky/>Anna Rumshisky</a><br><a href=/volumes/2020.insights-1/ class=text-muted>Proceedings of the First Workshop on Insights from Negative Results in NLP</a></span></p><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1445.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1445 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1445 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1445.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D19-1445/>Revealing the Dark Secrets of BERT<span class=acl-fixed-case>BERT</span></a></strong><br><a href=/people/o/olga-kovaleva/>Olga Kovaleva</a>
|
<a href=/people/a/alexey-romanov/>Alexey Romanov</a>
|
<a href=/people/a/anna-rogers/>Anna Rogers</a>
|
<a href=/people/a/anna-rumshisky/>Anna Rumshisky</a><br><a href=/volumes/D19-1/ class=text-muted>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1445><div class="card-body p-3 small">BERT-based architectures currently give state-of-the-art performance on many NLP tasks, but little is known about the exact mechanisms that contribute to its success. In the current work, we focus on the interpretation of self-attention, which is one of the fundamental underlying components of BERT. Using a subset of GLUE tasks and a set of handcrafted features-of-interest, we propose the methodology and carry out a qualitative and quantitative analysis of the information encoded by the individual BERT&#8217;s heads. Our findings suggest that there is a limited set of attention patterns that are repeated across different heads, indicating the overall model overparametrization. While different heads consistently use the same attention patterns, they have varying impact on performance across different tasks. We show that manually disabling attention in certain heads leads to a performance improvement over the regular fine-tuned BERT models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-1900.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-1900/>Proceedings of the 2nd Clinical Natural Language Processing Workshop</a></strong><br><a href=/people/a/anna-rumshisky/>Anna Rumshisky</a>
|
<a href=/people/k/kirk-roberts/>Kirk Roberts</a>
|
<a href=/people/s/steven-bethard/>Steven Bethard</a>
|
<a href=/people/t/tristan-naumann/>Tristan Naumann</a><br><a href=/volumes/W19-19/ class=text-muted>Proceedings of the 2nd Clinical Natural Language Processing Workshop</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2000.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-2000/>Proceedings of the 3rd Workshop on Evaluating Vector Space Representations for <span class=acl-fixed-case>NLP</span></a></strong><br><a href=/people/a/anna-rogers/>Anna Rogers</a>
|
<a href=/people/a/aleksandr-drozd/>Aleksandr Drozd</a>
|
<a href=/people/a/anna-rumshisky/>Anna Rumshisky</a>
|
<a href=/people/y/yoav-goldberg/>Yoav Goldberg</a><br><a href=/volumes/W19-20/ class=text-muted>Proceedings of the 3rd Workshop on Evaluating Vector Space Representations for NLP</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1088.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1088 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1088 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1088" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N19-1088/>Adversarial Decomposition of Text Representation</a></strong><br><a href=/people/a/alexey-romanov/>Alexey Romanov</a>
|
<a href=/people/a/anna-rumshisky/>Anna Rumshisky</a>
|
<a href=/people/a/anna-rogers/>Anna Rogers</a>
|
<a href=/people/d/david-donahue/>David Donahue</a><br><a href=/volumes/N19-1/ class=text-muted>Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1088><div class="card-body p-3 small">In this paper, we present a <a href=https://en.wikipedia.org/wiki/Methodology>method</a> for adversarial decomposition of text representation. This method can be used to decompose a representation of an input sentence into several independent vectors, each of them responsible for a specific aspect of the input sentence. We evaluate the proposed method on two <a href=https://en.wikipedia.org/wiki/Case_study>case studies</a> : the conversion between different <a href=https://en.wikipedia.org/wiki/Register_(sociolinguistics)>social registers</a> and <a href=https://en.wikipedia.org/wiki/Language_change>diachronic language change</a>. We show that the proposed method is capable of fine-grained controlled change of these aspects of the input sentence. It is also learning a continuous (rather than categorical) representation of the style of the sentence, which is more linguistically realistic. The model uses adversarial-motivational training and includes a special motivational loss, which acts opposite to the discriminator and encourages a better decomposition. Furthermore, we evaluate the obtained meaning embeddings on a downstream task of <a href=https://en.wikipedia.org/wiki/Paraphrase_detection>paraphrase detection</a> and show that they significantly outperform the <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a> of a regular autoencoder.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1424.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1424 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1424 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><span class="align-middle mr-1" data-toggle=tooltip data-placement=bottom title="Best Thematic Paper"><i class="fas fa-award"></i></span><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/365132300 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1424/>What’s in a Name? Reducing Bias in Bios without Access to Protected Attributes<span class=acl-fixed-case>R</span>educing Bias in Bios without Access to Protected Attributes</a></strong><br><a href=/people/a/alexey-romanov/>Alexey Romanov</a>
|
<a href=/people/m/maria-de-arteaga/>Maria De-Arteaga</a>
|
<a href=/people/h/hanna-wallach/>Hanna Wallach</a>
|
<a href=/people/j/jennifer-chayes/>Jennifer Chayes</a>
|
<a href=/people/c/christian-borgs/>Christian Borgs</a>
|
<a href=/people/a/alexandra-chouldechova/>Alexandra Chouldechova</a>
|
<a href=/people/s/sahin-geyik/>Sahin Geyik</a>
|
<a href=/people/k/krishnaram-kenthapadi/>Krishnaram Kenthapadi</a>
|
<a href=/people/a/anna-rumshisky/>Anna Rumshisky</a>
|
<a href=/people/a/adam-kalai/>Adam Kalai</a><br><a href=/volumes/N19-1/ class=text-muted>Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1424><div class="card-body p-3 small">There is a growing body of work that proposes methods for mitigating bias in <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning systems</a>. These methods typically rely on access to protected attributes such as <a href=https://en.wikipedia.org/wiki/Race_(human_categorization)>race</a>, <a href=https://en.wikipedia.org/wiki/Gender>gender</a>, or age. However, this raises two significant challenges : (1) protected attributes may not be available or it may not be legal to use them, and (2) it is often desirable to simultaneously consider multiple protected attributes, as well as their intersections. In the context of mitigating bias in occupation classification, we propose a method for discouraging correlation between the predicted probability of an individual&#8217;s true occupation and a <a href=https://en.wikipedia.org/wiki/Word_embedding>word embedding</a> of their name. This method leverages the societal biases that are encoded in <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a>, eliminating the need for access to <a href=https://en.wikipedia.org/wiki/Attribute_(computing)>protected attributes</a>. Crucially, <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> only requires access to individuals&#8217; names at training time and not at deployment time. We evaluate two variations of our proposed <a href=https://en.wikipedia.org/wiki/Methodology>method</a> using a large-scale dataset of online biographies. We find that both variations simultaneously reduce race and gender biases, with almost no reduction in the classifier&#8217;s overall true positive rate.</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1228.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1228 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1228 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1228/>What’s in Your Embedding, And How It Predicts Task Performance</a></strong><br><a href=/people/a/anna-rogers/>Anna Rogers</a>
|
<a href=/people/s/shashwath-hosur-ananthakrishna/>Shashwath Hosur Ananthakrishna</a>
|
<a href=/people/a/anna-rumshisky/>Anna Rumshisky</a><br><a href=/volumes/C18-1/ class=text-muted>Proceedings of the 27th International Conference on Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1228><div class="card-body p-3 small">Attempts to find a single technique for general-purpose intrinsic evaluation of word embeddings have so far not been successful. We present a new approach based on scaled-up qualitative analysis of word vector neighborhoods that quantifies interpretable characteristics of a given <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> (e.g. its preference for <a href=https://en.wikipedia.org/wiki/Synonym>synonyms</a> or shared morphological forms as nearest neighbors). We analyze 21 such factors and show how they correlate with performance on 14 extrinsic and intrinsic task datasets (and also explain the lack of correlation between some of them). Our approach enables multi-faceted evaluation, parameter search, and generally a more principled, hypothesis-driven approach to development of distributional semantic representations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1525.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1525 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1525 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-1525/>Similarity-Based Reconstruction Loss for Meaning Representation</a></strong><br><a href=/people/o/olga-kovaleva/>Olga Kovaleva</a>
|
<a href=/people/a/anna-rumshisky/>Anna Rumshisky</a>
|
<a href=/people/a/alexey-romanov/>Alexey Romanov</a><br><a href=/volumes/D18-1/ class=text-muted>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1525><div class="card-body p-3 small">This paper addresses the problem of <a href=https://en.wikipedia.org/wiki/Representation_learning>representation learning</a>. Using an autoencoder framework, we propose and evaluate several <a href=https://en.wikipedia.org/wiki/Loss_function>loss functions</a> that can be used as an alternative to the commonly used cross-entropy reconstruction loss. The proposed <a href=https://en.wikipedia.org/wiki/Loss_function>loss functions</a> use similarities between words in the <a href=https://en.wikipedia.org/wiki/Glossary_of_computer_graphics>embedding space</a>, and can be used to train any neural model for <a href=https://en.wikipedia.org/wiki/Text_generator>text generation</a>. We show that the introduced <a href=https://en.wikipedia.org/wiki/Loss_function>loss functions</a> amplify semantic diversity of reconstructed sentences, while preserving the original meaning of the input. We test the derived autoencoder-generated representations on paraphrase detection and language inference tasks and demonstrate performance improvement compared to the traditional cross-entropy loss.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-1604.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-1604 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-1604 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-1604/>Evaluating Creative Language Generation : The Case of Rap Lyric Ghostwriting</a></strong><br><a href=/people/p/peter-potash/>Peter Potash</a>
|
<a href=/people/a/alexey-romanov/>Alexey Romanov</a>
|
<a href=/people/a/anna-rumshisky/>Anna Rumshisky</a><br><a href=/volumes/W18-16/ class=text-muted>Proceedings of the Second Workshop on Stylistic Variation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-1604><div class="card-body p-3 small">Language generation tasks that seek to mimic human ability to use language creatively are difficult to evaluate, since one must consider <a href=https://en.wikipedia.org/wiki/Creativity>creativity</a>, <a href=https://en.wikipedia.org/wiki/Style_(visual_arts)>style</a>, and other non-trivial aspects of the generated text. The goal of this paper is to develop evaluations methods for one such <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>, ghostwriting of rap lyrics, and to provide an explicit, quantifiable foundation for the goals and future directions for this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. Ghostwriting must produce text that is similar in style to the emulated artist, yet distinct in content. We develop a novel <a href=https://en.wikipedia.org/wiki/Evaluation>evaluation methodology</a> that addresses several complementary aspects of this task, and illustrate how such <a href=https://en.wikipedia.org/wiki/Evaluation>evaluation</a> can be used to meaning fully analyze system performance. We provide a corpus of lyrics for 13 rap artists, annotated for stylistic similarity, which allows us to assess the feasibility of manual evaluation for generated verse.</div></div><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1035.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1035 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1035 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-1035/>Length, <a href=https://en.wikipedia.org/wiki/Interchangeability>Interchangeability</a>, and External Knowledge : Observations from Predicting Argument Convincingness</a></strong><br><a href=/people/p/peter-potash/>Peter Potash</a>
|
<a href=/people/r/robin-bhattacharya/>Robin Bhattacharya</a>
|
<a href=/people/a/anna-rumshisky/>Anna Rumshisky</a><br><a href=/volumes/I17-1/ class=text-muted>Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1035><div class="card-body p-3 small">In this work, we provide insight into three key aspects related to predicting argument convincingness. First, we explicitly display the power that text length possesses for predicting convincingness in an unsupervised setting. Second, we show that a bag-of-words embedding model posts state-of-the-art on a dataset of arguments annotated for convincingness, outperforming an SVM with numerous hand-crafted features as well as recurrent neural network models that attempt to capture semantic composition. Finally, we assess the feasibility of integrating external knowledge when predicting convincingness, as arguments are often more convincing when they contain abundant information and facts. We finish by analyzing the correlations between the various <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> we propose.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S17-2004.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S17-2004 data-toggle=collapse aria-expanded=false aria-controls=abstract-S17-2004 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/S17-2004/>SemEval-2017 Task 6 : # HashtagWars : Learning a Sense of Humor<span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2017 Task 6: #<span class=acl-fixed-case>H</span>ashtag<span class=acl-fixed-case>W</span>ars: Learning a Sense of Humor</a></strong><br><a href=/people/p/peter-potash/>Peter Potash</a>
|
<a href=/people/a/alexey-romanov/>Alexey Romanov</a>
|
<a href=/people/a/anna-rumshisky/>Anna Rumshisky</a><br><a href=/volumes/S17-2/ class=text-muted>Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S17-2004><div class="card-body p-3 small">This paper describes a new shared task for humor understanding that attempts to eschew the ubiquitous binary approach to humor detection and focus on comparative humor ranking instead. The task is based on a new dataset of funny tweets posted in response to shared hashtags, collected from the &#8216;Hashtag Wars&#8217; segment of the TV show @midnight. The results are evaluated in two subtasks that require the participants to generate either the correct pairwise comparisons of tweets (subtask A), or the correct ranking of the tweets (subtask B) in terms of how funny they are. 7 teams participated in subtask A, and 5 teams participated in subtask B. The best <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> in subtask A was 0.675. The best (lowest) rank edit distance for subtask B was 0.872.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S17-2010.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S17-2010 data-toggle=collapse aria-expanded=false aria-controls=abstract-S17-2010 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/S17-2010/>HumorHawk at SemEval-2017 Task 6 : Mixing Meaning and Sound for Humor Recognition<span class=acl-fixed-case>H</span>umor<span class=acl-fixed-case>H</span>awk at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2017 Task 6: Mixing Meaning and Sound for Humor Recognition</a></strong><br><a href=/people/d/david-donahue/>David Donahue</a>
|
<a href=/people/a/alexey-romanov/>Alexey Romanov</a>
|
<a href=/people/a/anna-rumshisky/>Anna Rumshisky</a><br><a href=/volumes/S17-2/ class=text-muted>Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S17-2010><div class="card-body p-3 small">This paper describes the winning system for SemEval-2017 Task 6 : # HashtagWars : Learning a Sense of Humor. Humor detection has up until now been predominantly addressed using feature-based approaches. Our system utilizes recurrent deep learning methods with dense embeddings to predict humorous tweets from the @midnight show # HashtagWars. In order to include both meaning and sound in the analysis, GloVe embeddings are combined with a novel <a href=https://en.wikipedia.org/wiki/Phonetic_transcription>phonetic representation</a> to serve as input to an LSTM component. The output is combined with a character-based CNN model, and an XGBoost component in an <a href=https://en.wikipedia.org/wiki/Statistical_ensemble_(mathematical_physics)>ensemble model</a> which achieves 0.675 <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> on the evaluation data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1092.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1092 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1092 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1092/>Temporal Information Extraction for Question Answering Using Syntactic Dependencies in an LSTM-based Architecture<span class=acl-fixed-case>LSTM</span>-based Architecture</a></strong><br><a href=/people/y/yuanliang-meng/>Yuanliang Meng</a>
|
<a href=/people/a/anna-rumshisky/>Anna Rumshisky</a>
|
<a href=/people/a/alexey-romanov/>Alexey Romanov</a><br><a href=/volumes/D17-1/ class=text-muted>Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1092><div class="card-body p-3 small">In this paper, we propose to use a set of simple, uniform in architecture LSTM-based models to recover different kinds of temporal relations from text. Using the shortest dependency path between entities as input, the same architecture is used to extract intra-sentence, cross-sentence, and document creation time relations. A double-checking technique reverses entity pairs in <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a>, boosting the <a href=https://en.wikipedia.org/wiki/Recall_(memory)>recall</a> of positive cases and reducing misclassifications between opposite classes. An efficient pruning algorithm resolves conflicts globally. Evaluated on QA-TempEval (SemEval2015 Task 5), our proposed technique outperforms state-of-the-art methods by a large margin. We also conduct intrinsic evaluation and post state-of-the-art results on Timebank-Dense.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1143.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1143 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1143 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1143/>Here’s My Point : Joint Pointer Architecture for Argument Mining</a></strong><br><a href=/people/p/peter-potash/>Peter Potash</a>
|
<a href=/people/a/alexey-romanov/>Alexey Romanov</a>
|
<a href=/people/a/anna-rumshisky/>Anna Rumshisky</a><br><a href=/volumes/D17-1/ class=text-muted>Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1143><div class="card-body p-3 small">In order to determine argument structure in text, one must understand how individual components of the overall argument are linked. This work presents the first neural network-based approach to link extraction in <a href=https://en.wikipedia.org/wiki/Argument_mining>argument mining</a>. Specifically, we propose a novel architecture that applies Pointer Network sequence-to-sequence attention modeling to structural prediction in discourse parsing tasks. We then develop a joint model that extends this <a href=https://en.wikipedia.org/wiki/Computer_architecture>architecture</a> to simultaneously address the link extraction task and the classification of argument components. The proposed joint model achieves state-of-the-art results on two separate evaluation corpora, showing far superior performance than the previously proposed corpus-specific and heavily feature-engineered models. Furthermore, our results demonstrate that jointly optimizing for both <a href=https://en.wikipedia.org/wiki/Task_(computing)>tasks</a> is crucial for high performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1261.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1261 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1261 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/238236302 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D17-1261/>Towards Debate Automation : a Recurrent Model for Predicting Debate Winners</a></strong><br><a href=/people/p/peter-potash/>Peter Potash</a>
|
<a href=/people/a/anna-rumshisky/>Anna Rumshisky</a><br><a href=/volumes/D17-1/ class=text-muted>Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1261><div class="card-body p-3 small">In this paper we introduce a practical first step towards the creation of an automated debate agent : a state-of-the-art recurrent predictive model for predicting debate winners. By having an accurate <a href=https://en.wikipedia.org/wiki/Predictive_modelling>predictive model</a>, we are able to objectively rate the quality of a statement made at a specific turn in a debate. The <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is based on a <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent neural network architecture</a> with <a href=https://en.wikipedia.org/wiki/Attention>attention</a>, which allows the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> to effectively account for the entire debate when making its prediction. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieves state-of-the-art <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> on a dataset of debate transcripts annotated with <a href=https://en.wikipedia.org/wiki/Audience_measurement>audience favorability</a> of the debate teams. Finally, we discuss how future work can leverage our proposed model for the creation of an automated debate agent. We accomplish this by determining the model input that will maximize audience favorability toward a given side of a debate at an arbitrary turn.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Anna+Rumshisky" title="Search for 'Anna Rumshisky' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/a/alexey-romanov/ class=align-middle>Alexey Romanov</a>
<span class="badge badge-secondary align-middle ml-2">9</span></li><li class=list-group-item><a href=/people/a/anna-rogers/ class=align-middle>Anna Rogers</a>
<span class="badge badge-secondary align-middle ml-2">6</span></li><li class=list-group-item><a href=/people/p/peter-potash/ class=align-middle>Peter Potash</a>
<span class="badge badge-secondary align-middle ml-2">5</span></li><li class=list-group-item><a href=/people/s/steven-bethard/ class=align-middle>Steven Bethard</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/o/olga-kovaleva/ class=align-middle>Olga Kovaleva</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/d/david-donahue/ class=align-middle>David Donahue</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/k/kirk-roberts/ class=align-middle>Kirk Roberts</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/t/tristan-naumann/ class=align-middle>Tristan Naumann</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/j/joao-sedoc/ class=align-middle>João Sedoc</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/s/shashwath-hosur-ananthakrishna/ class=align-middle>Shashwath Hosur Ananthakrishna</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/robin-bhattacharya/ class=align-middle>Robin Bhattacharya</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yuanliang-meng/ class=align-middle>Yuanliang Meng</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/kristina-toutanova/ class=align-middle>Kristina Toutanova</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/luke-zettlemoyer/ class=align-middle>Luke Zettlemoyer</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/dilek-hakkani-tur/ class=align-middle>Dilek Hakkani-Tur</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/i/iz-beltagy/ class=align-middle>Iz Beltagy</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/ryan-cotterell/ class=align-middle>Ryan Cotterell</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/tanmoy-chakraborty/ class=align-middle>Tanmoy Chakraborty</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yichao-zhou/ class=align-middle>Yichao Zhou</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/shabnam-tafreshi/ class=align-middle>Shabnam Tafreshi</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/aleksandr-drozd/ class=align-middle>Aleksandr Drozd</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yoav-goldberg/ class=align-middle>Yoav Goldberg</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/maria-de-arteaga/ class=align-middle>Maria De-Arteaga</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/hanna-wallach/ class=align-middle>Hanna Wallach</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jennifer-chayes/ class=align-middle>Jennifer Chayes</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/christian-borgs/ class=align-middle>Christian Borgs</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/alexandra-chouldechova/ class=align-middle>Alexandra Chouldechova</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/sahin-geyik/ class=align-middle>Sahin Geyik</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/krishnaram-kenthapadi/ class=align-middle>Krishnaram Kenthapadi</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/adam-kalai/ class=align-middle>Adam Kalai</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/emnlp/ class=align-middle>EMNLP</a><span class="badge badge-secondary align-middle ml-2">5</span></li><li class=list-group-item><a href=/venues/naacl/ class=align-middle>NAACL</a><span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/venues/ws/ class=align-middle>WS</a><span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/venues/semeval/ class=align-middle>SemEval</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/insights/ class=align-middle>insights</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-venues aria-expanded=false aria-controls=more-venues>show all...</li><div class="collapse border-top" id=more-venues><li class=list-group-item><a href=/venues/coling/ class=align-middle>COLING</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/ijcnlp/ class=align-middle>IJCNLP</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/clinicalnlp/ class=align-middle>ClinicalNLP</a><span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>