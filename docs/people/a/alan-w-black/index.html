<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Alan W. Black - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Alan W.</span> <span class=font-weight-bold>Black</span></h2><p class="font-weight-light text-muted"><span class=font-italic>Also published as:</span>
Alan <span class=font-weight-normal>Black</span>,
Alan W <span class=font-weight-normal>Black</span></p><hr><div class=row><div class=col-lg-9><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.calcs-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.calcs-1.0/>Proceedings of the Fifth Workshop on Computational Approaches to Linguistic Code-Switching</a></strong><br><a href=/people/t/thamar-solorio/>Thamar Solorio</a>
|
<a href=/people/s/shuguang-chen/>Shuguang Chen</a>
|
<a href=/people/a/alan-w-black/>Alan W. Black</a>
|
<a href=/people/m/mona-diab/>Mona Diab</a>
|
<a href=/people/s/sunayana-sitaram/>Sunayana Sitaram</a>
|
<a href=/people/v/victor-soto/>Victor Soto</a>
|
<a href=/people/e/emre-yilmaz/>Emre Yilmaz</a>
|
<a href=/people/a/anirudh-srinivasan/>Anirudh Srinivasan</a><br><a href=/volumes/2021.calcs-1/ class=text-muted>Proceedings of the Fifth Workshop on Computational Approaches to Linguistic Code-Switching</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.calcs-1.14.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--calcs-1--14 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.calcs-1.14 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.calcs-1.14" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.calcs-1.14/>CodemixedNLP : An Extensible and Open NLP Toolkit for Code-Mixing<span class=acl-fixed-case>C</span>odemixed<span class=acl-fixed-case>NLP</span>: An Extensible and Open <span class=acl-fixed-case>NLP</span> Toolkit for Code-Mixing</a></strong><br><a href=/people/s/sai-muralidhar-jayanthi/>Sai Muralidhar Jayanthi</a>
|
<a href=/people/k/kavya-nerella/>Kavya Nerella</a>
|
<a href=/people/k/khyathi-raghavi-chandu/>Khyathi Raghavi Chandu</a>
|
<a href=/people/a/alan-w-black/>Alan W Black</a><br><a href=/volumes/2021.calcs-1/ class=text-muted>Proceedings of the Fifth Workshop on Computational Approaches to Linguistic Code-Switching</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--calcs-1--14><div class="card-body p-3 small">The NLP community has witnessed steep progress in a variety of <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> across the realms of monolingual and multilingual language processing recently. These successes, in conjunction with the proliferating mixed language interactions on <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>, have boosted interest in modeling code-mixed texts. In this work, we present CodemixedNLP, an open-source library with the goals of bringing together the advances in code-mixed NLP and opening it up to a wider machine learning community. The <a href=https://en.wikipedia.org/wiki/Library_(computing)>library</a> consists of tools to develop and benchmark versatile model architectures that are tailored for mixed texts, methods to expand training sets, techniques to quantify mixing styles, and fine-tuned state-of-the-art models for 7 tasks in <a href=https://en.wikipedia.org/wiki/Hinglish>Hinglish</a>. We believe this work has the potential to foster a distributed yet collaborative and sustainable ecosystem in an otherwise dispersed space of code-mixing research. The <a href=https://en.wikipedia.org/wiki/List_of_toolkits>toolkit</a> is designed to be simple, easily extensible, and resourceful to both researchers as well as practitioners. Demo : http://k-ikkees.pc.cs.cmu.edu:5000 and Library : https://github.com/murali1996/CodemixedNLP</div></div><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.93.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--93 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.93 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939186 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.93/>Reading Between the Lines : Exploring Infilling in Visual Narratives</a></strong><br><a href=/people/k/khyathi-raghavi-chandu/>Khyathi Raghavi Chandu</a>
|
<a href=/people/r/ruo-ping-dong/>Ruo-Ping Dong</a>
|
<a href=/people/a/alan-w-black/>Alan W Black</a><br><a href=/volumes/2020.emnlp-main/ class=text-muted>Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--93><div class="card-body p-3 small">Generating long form narratives such as <a href=https://en.wikipedia.org/wiki/Narrative>stories</a> and procedures from multiple modalities has been a long standing dream for <a href=https://en.wikipedia.org/wiki/Artificial_intelligence>artificial intelligence</a>. In this regard, there is often crucial subtext that is derived from the surrounding contexts. The general seq2seq training methods render the models shorthanded while attempting to bridge the gap between these neighbouring contexts. In this paper, we tackle this problem by using infilling techniques involving prediction of missing steps in a narrative while generating textual descriptions from a sequence of images. We also present a new large scale visual procedure telling (ViPT) dataset with a total of 46,200 procedures and around 340k pairwise images and textual descriptions that is rich in such contextual dependencies. Generating steps using infilling technique demonstrates the effectiveness in visual procedures with more coherent texts. We conclusively show a METEOR score of 27.51 on <a href=https://en.wikipedia.org/wiki/Procedure_code>procedures</a> which is higher than the state-of-the-art on <a href=https://en.wikipedia.org/wiki/Visual_storytelling>visual storytelling</a>. We also demonstrate the effects of interposing new text with missing images during <a href=https://en.wikipedia.org/wiki/Inference>inference</a>. The code and the <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> will be publicly available at https://visual-narratives.github.io/Visual-Narratives/.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.acl-main.169.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--acl-main--169 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.acl-main.169 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://slideslive.com/38929267 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.acl-main.169/>Politeness Transfer : A Tag and Generate Approach</a></strong><br><a href=/people/a/aman-madaan/>Aman Madaan</a>
|
<a href=/people/a/amrith-setlur/>Amrith Setlur</a>
|
<a href=/people/t/tanmay-parekh/>Tanmay Parekh</a>
|
<a href=/people/b/barnabas-poczos/>Barnabas Poczos</a>
|
<a href=/people/g/graham-neubig/>Graham Neubig</a>
|
<a href=/people/y/yiming-yang/>Yiming Yang</a>
|
<a href=/people/r/ruslan-salakhutdinov/>Ruslan Salakhutdinov</a>
|
<a href=/people/a/alan-w-black/>Alan W Black</a>
|
<a href=/people/s/shrimai-prabhumoye/>Shrimai Prabhumoye</a><br><a href=/volumes/2020.acl-main/ class=text-muted>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--acl-main--169><div class="card-body p-3 small">This paper introduces a new task of politeness transfer which involves converting non-polite sentences to polite sentences while preserving the <a href=https://en.wikipedia.org/wiki/Meaning_(linguistics)>meaning</a>. We also provide a <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> of more than 1.39 instances automatically labeled for <a href=https://en.wikipedia.org/wiki/Politeness>politeness</a> to encourage benchmark evaluations on this new <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. We design a tag and generate pipeline that identifies stylistic attributes and subsequently generates a sentence in the target style while preserving most of the source content. For <a href=https://en.wikipedia.org/wiki/Politeness>politeness</a> as well as five other transfer tasks, our model outperforms the state-of-the-art methods on automatic metrics for content preservation, with a comparable or better performance on style transfer accuracy. Additionally, our model surpasses existing methods on human evaluations for <a href=https://en.wikipedia.org/wiki/Grammaticality>grammaticality</a>, meaning preservation and transfer accuracy across all the six style transfer tasks. The data and code is located at https://github.com/tag-and-generate.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.656.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--656 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.656 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.656/>AlloVera : A Multilingual Allophone Database<span class=acl-fixed-case>A</span>llo<span class=acl-fixed-case>V</span>era: A Multilingual Allophone Database</a></strong><br><a href=/people/d/david-r-mortensen/>David R. Mortensen</a>
|
<a href=/people/x/xinjian-li/>Xinjian Li</a>
|
<a href=/people/p/patrick-littell/>Patrick Littell</a>
|
<a href=/people/a/alexis-michaud/>Alexis Michaud</a>
|
<a href=/people/s/shruti-rijhwani/>Shruti Rijhwani</a>
|
<a href=/people/a/antonios-anastasopoulos/>Antonios Anastasopoulos</a>
|
<a href=/people/a/alan-w-black/>Alan W Black</a>
|
<a href=/people/f/florian-metze/>Florian Metze</a>
|
<a href=/people/g/graham-neubig/>Graham Neubig</a><br><a href=/volumes/2020.lrec-1/ class=text-muted>Proceedings of the 12th Language Resources and Evaluation Conference</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--656><div class="card-body p-3 small">We introduce a new resource, AlloVera, which provides mappings from 218 <a href=https://en.wikipedia.org/wiki/Allophone>allophones</a> to <a href=https://en.wikipedia.org/wiki/Phoneme>phonemes</a> for 14 languages. Phonemes are contrastive phonological units, and allophones are their various concrete realizations, which are predictable from phonological context. While <a href=https://en.wikipedia.org/wiki/Phoneme>phonemic representations</a> are language specific, <a href=https://en.wikipedia.org/wiki/Phoneme>phonetic representations</a> (stated in terms of (allo)phones) are much closer to a universal (language-independent) transcription. AlloVera allows the training of <a href=https://en.wikipedia.org/wiki/Speech_recognition>speech recognition models</a> that output <a href=https://en.wikipedia.org/wiki/Transcription_(linguistics)>phonetic transcriptions</a> in the <a href=https://en.wikipedia.org/wiki/International_Phonetic_Alphabet>International Phonetic Alphabet (IPA)</a>, regardless of the input language. We show that a universal allophone model, <a href=https://en.wikipedia.org/wiki/Allosaurus>Allosaurus</a>, built with AlloVera, outperforms universal phonemic models and language-specific models on a speech-transcription task. We explore the implications of this <a href=https://en.wikipedia.org/wiki/Technology>technology</a> (and related technologies) for the documentation of endangered and minority languages. We further explore other <a href=https://en.wikipedia.org/wiki/Application_software>applications</a> for which AlloVera will be suitable as <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> grows, including phonological typology.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wnut-1.22.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wnut-1--22 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wnut-1.22 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.wnut-1.22" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.wnut-1.22/>Detecting Entailment in Code-Mixed Hindi-English Conversations<span class=acl-fixed-case>H</span>indi-<span class=acl-fixed-case>E</span>nglish Conversations</a></strong><br><a href=/people/s/sharanya-chakravarthy/>Sharanya Chakravarthy</a>
|
<a href=/people/a/anjana-umapathy/>Anjana Umapathy</a>
|
<a href=/people/a/alan-w-black/>Alan W Black</a><br><a href=/volumes/2020.wnut-1/ class=text-muted>Proceedings of the Sixth Workshop on Noisy User-generated Text (W-NUT 2020)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wnut-1--22><div class="card-body p-3 small">The presence of large-scale corpora for Natural Language Inference (NLI) has spurred deep learning research in this area, though much of this research has focused solely on monolingual data. Code-mixing is the intertwined usage of multiple languages, and is commonly seen in informal conversations among <a href=https://en.wikipedia.org/wiki/Multilingualism>polyglots</a>. Given the rising importance of dialogue agents, it is imperative that they understand <a href=https://en.wikipedia.org/wiki/Code-mixing>code-mixing</a>, but the scarcity of code-mixed Natural Language Understanding (NLU) datasets has precluded research in this area. The <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> by Khanuja et. al. for detecting conversational entailment in code-mixed Hindi-English text is the first of its kind. We investigate the effectiveness of <a href=https://en.wikipedia.org/wiki/Language_model>language modeling</a>, <a href=https://en.wikipedia.org/wiki/Data_augmentation>data augmentation</a>, <a href=https://en.wikipedia.org/wiki/Translation>translation</a>, and architectural approaches to address the code-mixed, conversational, and low-resource aspects of this <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>. We obtain an 8.09 % increase in test set accuracy over the current state of the art.</div></div><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1500.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1500 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1500 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1500" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1500/>Question Answering for Privacy Policies : Combining Computational and Legal Perspectives</a></strong><br><a href=/people/a/abhilasha-ravichander/>Abhilasha Ravichander</a>
|
<a href=/people/a/alan-w-black/>Alan W Black</a>
|
<a href=/people/s/shomir-wilson/>Shomir Wilson</a>
|
<a href=/people/t/thomas-norton/>Thomas Norton</a>
|
<a href=/people/n/norman-sadeh/>Norman Sadeh</a><br><a href=/volumes/D19-1/ class=text-muted>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1500><div class="card-body p-3 small">Privacy policies are long and complex documents that are difficult for users to read and understand. Yet, they have legal effects on how <a href=https://en.wikipedia.org/wiki/User_data>user data</a> can be collected, managed and used. Ideally, we would like to empower users to inform themselves about the issues that matter to them, and enable them to selectively explore these issues. We present PrivacyQA, a <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> consisting of 1750 questions about the <a href=https://en.wikipedia.org/wiki/Privacy_policy>privacy policies</a> of <a href=https://en.wikipedia.org/wiki/Mobile_app>mobile applications</a>, and over 3500 expert annotations of relevant answers. We observe that a strong neural baseline underperforms <a href=https://en.wikipedia.org/wiki/Human_performance>human performance</a> by almost 0.3 <a href=https://en.wikipedia.org/wiki/F-number>F1</a> on PrivacyQA, suggesting considerable room for improvement for future systems. Further, we use this <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> to categorically identify challenges to <a href=https://en.wikipedia.org/wiki/Question_answering>question answerability</a>, with domain-general implications for any <a href=https://en.wikipedia.org/wiki/Question_answering>question answering system</a>. The PrivacyQA corpus offers a challenging corpus for <a href=https://en.wikipedia.org/wiki/Question_answering>question answering</a>, with genuine real world utility.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5502.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5502 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5502 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-5502.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D19-5502/>Formality Style Transfer for Noisy, User-generated Conversations : Extracting Labeled, Parallel Data from Unlabeled Corpora</a></strong><br><a href=/people/i/isak-czeresnia-etinger/>Isak Czeresnia Etinger</a>
|
<a href=/people/a/alan-w-black/>Alan W Black</a><br><a href=/volumes/D19-55/ class=text-muted>Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT 2019)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5502><div class="card-body p-3 small">Typical <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> used for style transfer in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> contain aligned pairs of two opposite extremes of a style. As each existing <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> is sourced from a specific domain and context, most use cases will have a sizable mismatch from the vocabulary and sentence structures of any <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> available. This reduces the performance of the style transfer, and is particularly significant for noisy, user-generated text. To solve this problem, we show a technique to derive a dataset of aligned pairs (style-agnostic vs stylistic sentences) from an unlabeled corpus by using an auxiliary dataset, allowing for in-domain training. We test the technique with the Yahoo Formality Dataset and 6 novel datasets we produced, which consist of scripts from 5 popular TV-shows (Friends, Futurama, Seinfeld, Southpark, Stargate SG-1) and the Slate Star Codex online forum. We gather 1080 human evaluations, which show that our method produces a sizable change in formality while maintaining fluency and context ; and that it considerably outperforms OpenNMT&#8217;s Seq2Seq model directly trained on the Yahoo Formality Dataset. Additionally, we publish the full pipeline code and our novel <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5527.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5527 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5527 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5527/>What A Sunny Day : Toward Emoji-Sensitive Irony Detection</a></strong><br><a href=/people/s/shirley-anugrah-hayati/>Shirley Anugrah Hayati</a>
|
<a href=/people/a/aditi-chaudhary/>Aditi Chaudhary</a>
|
<a href=/people/n/naoki-otani/>Naoki Otani</a>
|
<a href=/people/a/alan-w-black/>Alan W Black</a><br><a href=/volumes/D19-55/ class=text-muted>Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT 2019)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5527><div class="card-body p-3 small">Irony detection is an important task with applications in identification of online abuse and <a href=https://en.wikipedia.org/wiki/Harassment>harassment</a>. With the ubiquitous use of <a href=https://en.wikipedia.org/wiki/Nonverbal_communication>non-verbal cues</a> such as <a href=https://en.wikipedia.org/wiki/Emoji>emojis</a> in <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>, in this work we aim to study the role of these structures in irony detection. Since the existing irony detection datasets have 10 % ironic tweets with <a href=https://en.wikipedia.org/wiki/Emoji>emoji</a>, <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifiers</a> trained on them are insensitive to <a href=https://en.wikipedia.org/wiki/Emoji>emojis</a>. We propose an <a href=https://en.wikipedia.org/wiki/Pipeline_(computing)>automated pipeline</a> for creating a more balanced dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2108.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-2108 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-2108 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-2108/>Stance Classification, Outcome Prediction, and <a href=https://en.wikipedia.org/wiki/Impact_assessment>Impact Assessment</a> : NLP Tasks for Studying Group Decision-Making<span class=acl-fixed-case>NLP</span> Tasks for Studying Group Decision-Making</a></strong><br><a href=/people/e/elijah-mayfield/>Elijah Mayfield</a>
|
<a href=/people/a/alan-w-black/>Alan Black</a><br><a href=/volumes/W19-21/ class=text-muted>Proceedings of the Third Workshop on Natural Language Processing and Computational Social Science</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-2108><div class="card-body p-3 small">In <a href=https://en.wikipedia.org/wiki/Group_decision-making>group decision-making</a>, the nuanced process of conflict and resolution that leads to <a href=https://en.wikipedia.org/wiki/Consensus_decision-making>consensus formation</a> is closely tied to the quality of decisions made. Behavioral scientists rarely have rich access to <a href=https://en.wikipedia.org/wiki/Dependent_and_independent_variables>process variables</a>, though, as unstructured discussion transcripts are difficult to analyze. Here, we define ways for NLP researchers to contribute to the study of groups and teams. We introduce three <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> alongside a large new corpus of over 400,000 <a href=https://en.wikipedia.org/wiki/Internet_forum>group debates</a> on <a href=https://en.wikipedia.org/wiki/Wikipedia>Wikipedia</a>. We describe the tasks and their importance, then provide baselines showing that BERT contextualized word embeddings consistently outperform other language representations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3413.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3413 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3413 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-3413" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-3413/>WriterForcing : Generating more interesting story endings<span class=acl-fixed-case>W</span>riter<span class=acl-fixed-case>F</span>orcing: Generating more interesting story endings</a></strong><br><a href=/people/p/prakhar-gupta/>Prakhar Gupta</a>
|
<a href=/people/v/vinayshekhar-bannihatti-kumar/>Vinayshekhar Bannihatti Kumar</a>
|
<a href=/people/m/mukul-bhutani/>Mukul Bhutani</a>
|
<a href=/people/a/alan-w-black/>Alan W Black</a><br><a href=/volumes/W19-34/ class=text-muted>Proceedings of the Second Workshop on Storytelling</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3413><div class="card-body p-3 small">We study the problem of <a href=https://en.wikipedia.org/wiki/Plot_(narrative)>generating interesting endings</a> for stories. Neural generative models have shown promising results for various text generation problems. Sequence to Sequence (Seq2Seq) models are typically trained to generate a single output sequence for a given input sequence. However, in the context of a story, multiple endings are possible. Seq2Seq models tend to ignore the context and generate generic and dull responses. Very few works have studied generating diverse and interesting story endings for the same <a href=https://en.wikipedia.org/wiki/Context_(language_use)>story context</a>. In this paper, we propose <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> which generate more diverse and interesting outputs by 1) training <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> to focus attention on important keyphrases of the story, and 2) promoting generating nongeneric words. We show that the combination of the two leads to more interesting endings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3637 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3637 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3637/>Principled Frameworks for Evaluating Ethics in <span class=acl-fixed-case>NLP</span> Systems</a></strong><br><a href=/people/s/shrimai-prabhumoye/>Shrimai Prabhumoye</a>
|
<a href=/people/e/elijah-mayfield/>Elijah Mayfield</a>
|
<a href=/people/a/alan-w-black/>Alan W Black</a><br><a href=/volumes/W19-36/ class=text-muted>Proceedings of the 2019 Workshop on Widening NLP</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3637><div class="card-body p-3 small">We critique recent work on ethics in natural language processing. Those discussions have focused on data collection, experimental design, and interventions in modeling. But we argue that we ought to first understand the frameworks of ethics that are being used to evaluate the fairness and justice of algorithmic systems. Here, we begin that discussion by outlining deontological and consequentialist ethics, and make predictions on the research agenda prioritized by each.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4446.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4446 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4446 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4446/>Equity Beyond Bias in Language Technologies for Education</a></strong><br><a href=/people/e/elijah-mayfield/>Elijah Mayfield</a>
|
<a href=/people/m/michael-madaio/>Michael Madaio</a>
|
<a href=/people/s/shrimai-prabhumoye/>Shrimai Prabhumoye</a>
|
<a href=/people/d/david-gerritsen/>David Gerritsen</a>
|
<a href=/people/b/brittany-mclaughlin/>Brittany McLaughlin</a>
|
<a href=/people/e/ezekiel-dixon-roman/>Ezekiel Dixon-Román</a>
|
<a href=/people/a/alan-w-black/>Alan W Black</a><br><a href=/volumes/W19-44/ class=text-muted>Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4446><div class="card-body p-3 small">There is a long record of research on equity in schools. As machine learning researchers begin to study fairness and bias in earnest, language technologies in education have an unusually strong theoretical and applied foundation to build on. Here, we introduce concepts from culturally relevant pedagogy and other frameworks for teaching and learning, identifying future work on equity in <a href=https://en.wikipedia.org/wiki/Neuro-linguistic_programming>NLP</a>. We present case studies in a range of topics like <a href=https://en.wikipedia.org/wiki/Intelligent_tutoring_system>intelligent tutoring systems</a>, <a href=https://en.wikipedia.org/wiki/Computer-assisted_language_learning>computer-assisted language learning</a>, automated essay scoring, and <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> in classrooms, and provide an actionable agenda for research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1005.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1005 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1005 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/383953490 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1005/>Boosting Dialog Response Generation</a></strong><br><a href=/people/w/wenchao-du/>Wenchao Du</a>
|
<a href=/people/a/alan-w-black/>Alan W Black</a><br><a href=/volumes/P19-1/ class=text-muted>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1005><div class="card-body p-3 small">Neural models have become one of the most important approaches to dialog response generation. However, they still tend to generate the most common and generic responses in the corpus all the time. To address this problem, we designed an iterative training process and <a href=https://en.wikipedia.org/wiki/Ensemble_learning>ensemble method</a> based on <a href=https://en.wikipedia.org/wiki/Boosting_(machine_learning)>boosting</a>. We combined our method with different training and decoding paradigms as the base model, including mutual-information-based decoding and reward-augmented maximum likelihood learning. Empirical results show that our approach can significantly improve the diversity and relevance of the responses generated by all base models, backed by objective measurements and human evaluation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1179.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1179 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1179 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/384515395 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1179/>Exploring Phoneme-Level Speech Representations for End-to-End Speech Translation</a></strong><br><a href=/people/e/elizabeth-salesky/>Elizabeth Salesky</a>
|
<a href=/people/m/matthias-sperber/>Matthias Sperber</a>
|
<a href=/people/a/alan-w-black/>Alan W Black</a><br><a href=/volumes/P19-1/ class=text-muted>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1179><div class="card-body p-3 small">Previous work on end-to-end translation from <a href=https://en.wikipedia.org/wiki/Speech>speech</a> has primarily used frame-level features as <a href=https://en.wikipedia.org/wiki/Speech>speech representations</a>, which creates longer, sparser sequences than <a href=https://en.wikipedia.org/wiki/Written_language>text</a>. We show that a naive method to create compressed phoneme-like speech representations is far more effective and efficient for <a href=https://en.wikipedia.org/wiki/Translation>translation</a> than traditional frame-level speech features. Specifically, we generate phoneme labels for speech frames and average consecutive frames with the same label to create shorter, higher-level source sequences for <a href=https://en.wikipedia.org/wiki/Translation>translation</a>. We see improvements of up to 5 <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a> on both our high and low resource language pairs, with a reduction in training time of 60 %. Our improvements hold across multiple data sizes and two language pairs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1606.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1606 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1606 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-1606/>Storyboarding of Recipes : Grounded Contextual Generation</a></strong><br><a href=/people/k/khyathi-chandu/>Khyathi Chandu</a>
|
<a href=/people/e/eric-nyberg/>Eric Nyberg</a>
|
<a href=/people/a/alan-w-black/>Alan W Black</a><br><a href=/volumes/P19-1/ class=text-muted>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1606><div class="card-body p-3 small">Information need of humans is essentially multimodal in nature, enabling maximum exploitation of situated context. We introduce a <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> for sequential procedural (how-to) text generation from images in cooking domain. The <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> consists of 16,441 cooking recipes with 160,479 photos associated with different steps. We setup a baseline motivated by the best performing model in terms of human evaluation for the Visual Story Telling (ViST) task. In addition, we introduce two models to incorporate high level structure learnt by a Finite State Machine (FSM) in neural sequential generation process by : (1) Scaffolding Structure in Decoder (SSiD) (2) Scaffolding Structure in Loss (SSiL). Our best performing <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> (SSiL) achieves a <a href=https://en.wikipedia.org/wiki/METEOR>METEOR score</a> of 0.31, which is an improvement of 0.6 over the baseline model. We also conducted human evaluation of the generated grounded recipes, which reveal that 61 % found that our proposed (SSiL) model is better than the baseline model in terms of overall recipes. We also discuss analysis of the output highlighting key important NLP issues for prospective directions.</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1076.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1076 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1076 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D18-1076.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1076" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D18-1076/>A Dataset for Document Grounded Conversations</a></strong><br><a href=/people/k/kangyan-zhou/>Kangyan Zhou</a>
|
<a href=/people/s/shrimai-prabhumoye/>Shrimai Prabhumoye</a>
|
<a href=/people/a/alan-w-black/>Alan W Black</a><br><a href=/volumes/D18-1/ class=text-muted>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1076><div class="card-body p-3 small">This paper introduces a document grounded dataset for <a href=https://en.wikipedia.org/wiki/Conversation>conversations</a>. We define Document Grounded Conversations as conversations that are about the contents of a specified document. In this <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> the specified documents were Wikipedia articles about popular movies. The <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> contains 4112 conversations with an average of 21.43 turns per conversation. This positions this <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> to not only provide a relevant chat history while generating responses but also provide a source of information that the <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> could use. We describe two neural architectures that provide benchmark performance on the task of generating the next response. We also evaluate our models for <a href=https://en.wikipedia.org/wiki/Engagement>engagement</a> and <a href=https://en.wikipedia.org/wiki/Fluency>fluency</a>, and find that the information from the document helps in generating more engaging and fluent responses.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3204.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3204 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3204 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3204/>Code-Mixed Question Answering Challenge : Crowd-sourcing Data and Techniques</a></strong><br><a href=/people/k/khyathi-chandu/>Khyathi Chandu</a>
|
<a href=/people/e/ekaterina-loginova/>Ekaterina Loginova</a>
|
<a href=/people/v/vishal-gupta/>Vishal Gupta</a>
|
<a href=/people/j/josef-van-genabith/>Josef van Genabith</a>
|
<a href=/people/g/gunter-neumann/>Günter Neumann</a>
|
<a href=/people/m/manoj-chinnakotla/>Manoj Chinnakotla</a>
|
<a href=/people/e/eric-nyberg/>Eric Nyberg</a>
|
<a href=/people/a/alan-w-black/>Alan W. Black</a><br><a href=/volumes/W18-32/ class=text-muted>Proceedings of the Third Workshop on Computational Approaches to Linguistic Code-Switching</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3204><div class="card-body p-3 small">Code-Mixing (CM) is the phenomenon of alternating between two or more languages which is prevalent in bi- and multi-lingual communities. Most NLP applications today are still designed with the assumption of a single interaction language and are most likely to break given a CM utterance with multiple languages mixed at a morphological, phrase or sentence level. For example, popular commercial search engines do not yet fully understand the intents expressed in CM queries. As a first step towards fostering research which supports CM in NLP applications, we systematically crowd-sourced and curated an evaluation dataset for factoid question answering in three CM languages-Hinglish (Hindi+English), Tenglish (Telugu+English) and Tamlish (Tamil+English) which belong to two language families (Indo-Aryan and Dravidian). We share the details of our data collection process, techniques which were used to avoid inducing lexical bias amongst the crowd workers and other CM specific linguistic properties of the dataset. Our final dataset, which is available freely for research purposes, has 1,694 <a href=https://en.wikipedia.org/wiki/Hinglish>Hinglish</a>, 2,848 Tamlish and 1,391 Tenglish factoid questions and their answers. We discuss the <a href=https://en.wikipedia.org/wiki/List_of_art_media>techniques</a> used by the participants for the first edition of this ongoing challenge.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3211.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3211 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3211 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3211/>Language Informed Modeling of Code-Switched Text</a></strong><br><a href=/people/k/khyathi-chandu/>Khyathi Chandu</a>
|
<a href=/people/t/thomas-manzini/>Thomas Manzini</a>
|
<a href=/people/s/sumeet-singh/>Sumeet Singh</a>
|
<a href=/people/a/alan-w-black/>Alan W. Black</a><br><a href=/volumes/W18-32/ class=text-muted>Proceedings of the Third Workshop on Computational Approaches to Linguistic Code-Switching</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3211><div class="card-body p-3 small">Code-switching (CS), the practice of alternating between two or more languages in conversations, is pervasive in most <a href=https://en.wikipedia.org/wiki/Multilingualism>multi-lingual communities</a>. CS texts have a complex interplay between languages and occur in informal contexts that make them harder to collect and construct NLP tools for. We approach this problem through Language Modeling (LM) on a new Hindi-English mixed corpus containing 59,189 unique sentences collected from <a href=https://en.wikipedia.org/wiki/Blog>blogging websites</a>. We implement and discuss different <a href=https://en.wikipedia.org/wiki/Language_model>Language Models</a> derived from a multi-layered LSTM architecture. We hypothesize that <a href=https://en.wikipedia.org/wiki/Code-switching>encoding language information</a> strengthens a <a href=https://en.wikipedia.org/wiki/Language_model>language model</a> by helping to learn code-switching points. We show that our highest performing <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> achieves a test perplexity of 19.52 on the <a href=https://en.wikipedia.org/wiki/Corpus_linguistics>CS corpus</a> that we collected and processed. On this data we demonstrate that our performance is an improvement over AWD-LSTM LM (a recent state of the art on monolingual English).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3217.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3217 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3217 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3217/>Tackling Code-Switched NER : Participation of CMU<span class=acl-fixed-case>NER</span>: Participation of <span class=acl-fixed-case>CMU</span></a></strong><br><a href=/people/p/parvathy-geetha/>Parvathy Geetha</a>
|
<a href=/people/k/khyathi-chandu/>Khyathi Chandu</a>
|
<a href=/people/a/alan-w-black/>Alan W Black</a><br><a href=/volumes/W18-32/ class=text-muted>Proceedings of the Third Workshop on Computational Approaches to Linguistic Code-Switching</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3217><div class="card-body p-3 small">Named Entity Recognition plays a major role in several downstream applications in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a>. Though this task has been heavily studied in formal monolingual texts and also <a href=https://en.wikipedia.org/wiki/Noisy_text>noisy texts</a> like Twitter data, it is still an emerging task in code-switched (CS) content on <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>. This paper describes our participation in the shared task of NER on code-switched data for Spanglish (Spanish + English) and Arabish (Arabic + English). In this paper we describe <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> that intuitively developed from the <a href=https://en.wikipedia.org/wiki/Data>data</a> for the shared task Named Entity Recognition on Code-switched Data. Owing to the sparse and non-linear relationships between words in Twitter data, we explored neural architectures that are capable of non-linearities fairly well. In specific, we trained character level models and word level models based on Bidirectional LSTMs (Bi-LSTMs) to perform sequential tagging. We train multiple models to identify nominal mentions and subsequently use this information to predict the labels of named entity in a sequence. Our best <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> is a character level model along with word level pre-trained multilingual embeddings that gave an <a href=https://en.wikipedia.org/wiki/F-score>F-score</a> of 56.72 in <a href=https://en.wikipedia.org/wiki/Spanglish>Spanglish</a> and a word level model that gave an <a href=https://en.wikipedia.org/wiki/F-score>F-score</a> of 65.02 in <a href=https://en.wikipedia.org/wiki/Arabic>Arabish</a> on the test data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5708.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5708 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5708 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5708/>Data Augmentation for Neural Online Chats Response Selection</a></strong><br><a href=/people/w/wenchao-du/>Wenchao Du</a>
|
<a href=/people/a/alan-w-black/>Alan Black</a><br><a href=/volumes/W18-57/ class=text-muted>Proceedings of the 2018 EMNLP Workshop SCAI: The 2nd International Workshop on Search-Oriented Conversational AI</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5708><div class="card-body p-3 small">Data augmentation seeks to manipulate the available data for training to improve the generalization ability of models. We investigate two data augmentation proxies, permutation and flipping, for neural dialog response selection task on various models over multiple datasets, including both Chinese and English languages. Different from standard data augmentation techniques, our method combines the original and synthesized data for <a href=https://en.wikipedia.org/wiki/Prediction>prediction</a>. Empirical results show that our approach can gain 1 to 3 recall-at-1 points over baseline models in both full-scale and small-scale settings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1080.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1080 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1080 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1080.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285801126 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-1080" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-1080/>Style Transfer Through Back-Translation</a></strong><br><a href=/people/s/shrimai-prabhumoye/>Shrimai Prabhumoye</a>
|
<a href=/people/y/yulia-tsvetkov/>Yulia Tsvetkov</a>
|
<a href=/people/r/ruslan-salakhutdinov/>Ruslan Salakhutdinov</a>
|
<a href=/people/a/alan-w-black/>Alan W Black</a><br><a href=/volumes/P18-1/ class=text-muted>Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1080><div class="card-body p-3 small">Style transfer is the task of rephrasing the text to contain specific stylistic properties without changing the intent or affect within the context. This paper introduces a new <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>method</a> for automatic style transfer. We first learn a latent representation of the input sentence which is grounded in a language translation model in order to better preserve the meaning of the sentence while reducing stylistic properties. Then adversarial generation techniques are used to make the output match the desired style. We evaluate this technique on three different style transformations : <a href=https://en.wikipedia.org/wiki/Sentimentality>sentiment</a>, <a href=https://en.wikipedia.org/wiki/Gender>gender</a> and political slant. Compared to two state-of-the-art style transfer modeling techniques we show improvements both in automatic evaluation of style transfer and in manual evaluation of meaning preservation and fluency.</div></div><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2908.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2908 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2908 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-2908/>Linguistic Markers of Influence in Informal Interactions</a></strong><br><a href=/people/s/shrimai-prabhumoye/>Shrimai Prabhumoye</a>
|
<a href=/people/s/samridhi-choudhary/>Samridhi Choudhary</a>
|
<a href=/people/e/evangelia-spiliopoulou/>Evangelia Spiliopoulou</a>
|
<a href=/people/c/christopher-bogart/>Christopher Bogart</a>
|
<a href=/people/c/carolyn-rose/>Carolyn Rose</a>
|
<a href=/people/a/alan-w-black/>Alan W Black</a><br><a href=/volumes/W17-29/ class=text-muted>Proceedings of the Second Workshop on NLP and Computational Social Science</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2908><div class="card-body p-3 small">There has been a long standing interest in understanding &#8216;<a href=https://en.wikipedia.org/wiki/Social_influence>Social Influence</a>&#8217; both in <a href=https://en.wikipedia.org/wiki/Social_science>Social Sciences</a> and in <a href=https://en.wikipedia.org/wiki/Computational_linguistics>Computational Linguistics</a>. In this paper, we present a novel approach to study and measure <a href=https://en.wikipedia.org/wiki/Interpersonal_influence>interpersonal influence</a> in <a href=https://en.wikipedia.org/wiki/Interpersonal_relationship>daily interactions</a>. Motivated by the basic principles of <a href=https://en.wikipedia.org/wiki/Social_influence>influence</a>, we attempt to identify indicative linguistic features of the posts in an online knitting community. We present the <a href=https://en.wikipedia.org/wiki/Scheme_(mathematics)>scheme</a> used to operationalize and label the posts as influential or non-influential. Experiments with the identified <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> show an improvement in the <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification accuracy</a> of <a href=https://en.wikipedia.org/wiki/Social_influence>influence</a> by 3.15 %. Our results illustrate the important correlation between the structure of the language and its potential to influence others.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Alan+W.+Black" title="Search for 'Alan W. Black' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/s/shrimai-prabhumoye/ class=align-middle>Shrimai Prabhumoye</a>
<span class="badge badge-secondary align-middle ml-2">6</span></li><li class=list-group-item><a href=/people/k/khyathi-chandu/ class=align-middle>Khyathi Chandu</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/people/e/elijah-mayfield/ class=align-middle>Elijah Mayfield</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/k/khyathi-raghavi-chandu/ class=align-middle>Khyathi Raghavi Chandu</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/g/graham-neubig/ class=align-middle>Graham Neubig</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/r/ruslan-salakhutdinov/ class=align-middle>Ruslan Salakhutdinov</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/e/eric-nyberg/ class=align-middle>Eric Nyberg</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/w/wenchao-du/ class=align-middle>Wenchao Du</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/r/ruo-ping-dong/ class=align-middle>Ruo-Ping Dong</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/thamar-solorio/ class=align-middle>Thamar Solorio</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/shuguang-chen/ class=align-middle>Shuguang Chen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/mona-diab/ class=align-middle>Mona Diab</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/sunayana-sitaram/ class=align-middle>Sunayana Sitaram</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/v/victor-soto/ class=align-middle>Victor Soto</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/e/emre-yilmaz/ class=align-middle>Emre Yilmaz</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/anirudh-srinivasan/ class=align-middle>Anirudh Srinivasan</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/sai-muralidhar-jayanthi/ class=align-middle>Sai Muralidhar Jayanthi</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/kavya-nerella/ class=align-middle>Kavya Nerella</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/aman-madaan/ class=align-middle>Aman Madaan</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/amrith-setlur/ class=align-middle>Amrith Setlur</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/tanmay-parekh/ class=align-middle>Tanmay Parekh</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/b/barnabas-poczos/ class=align-middle>Barnabás Poczós</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yiming-yang/ class=align-middle>Yiming Yang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/samridhi-choudhary/ class=align-middle>Samridhi Choudhary</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/e/evangelia-spiliopoulou/ class=align-middle>Evangelia Spiliopoulou</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/christopher-bogart/ class=align-middle>Christopher Bogart</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/carolyn-rose/ class=align-middle>Carolyn Rose</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/kangyan-zhou/ class=align-middle>Kangyan Zhou</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/abhilasha-ravichander/ class=align-middle>Abhilasha Ravichander</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/shomir-wilson/ class=align-middle>Shomir Wilson</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/thomas-norton/ class=align-middle>Thomas Norton</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/n/norman-sadeh/ class=align-middle>Norman Sadeh</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/i/isak-czeresnia-etinger/ class=align-middle>Isak Czeresnia Etinger</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/shirley-anugrah-hayati/ class=align-middle>Shirley Anugrah Hayati</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/aditi-chaudhary/ class=align-middle>Aditi Chaudhary</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/n/naoki-otani/ class=align-middle>Naoki Otani</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/e/ekaterina-loginova/ class=align-middle>Ekaterina Loginova</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/v/vishal-gupta/ class=align-middle>Vishal Gupta</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/josef-van-genabith/ class=align-middle>Josef van Genabith</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/g/gunter-neumann/ class=align-middle>Günter Neumann</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/manoj-chinnakotla/ class=align-middle>Manoj Chinnakotla</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/thomas-manzini/ class=align-middle>Thomas Manzini</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/sumeet-singh/ class=align-middle>Sumeet Singh</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/p/parvathy-geetha/ class=align-middle>Parvathy Geetha</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/p/prakhar-gupta/ class=align-middle>Prakhar Gupta</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/v/vinayshekhar-bannihatti-kumar/ class=align-middle>Vinayshekhar Bannihatti Kumar</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/mukul-bhutani/ class=align-middle>Mukul Bhutani</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/michael-madaio/ class=align-middle>Michael Madaio</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/david-gerritsen/ class=align-middle>David Gerritsen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/b/brittany-mclaughlin/ class=align-middle>Brittany McLaughlin</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/e/ezekiel-dixon-roman/ class=align-middle>Ezekiel Dixon-Román</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/david-r-mortensen/ class=align-middle>David R. Mortensen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/x/xinjian-li/ class=align-middle>Xinjian Li</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/p/patrick-littell/ class=align-middle>Patrick Littell</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/alexis-michaud/ class=align-middle>Alexis Michaud</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/shruti-rijhwani/ class=align-middle>Shruti Rijhwani</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/antonios-anastasopoulos/ class=align-middle>Antonios Anastasopoulos</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/f/florian-metze/ class=align-middle>Florian Metze</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/sharanya-chakravarthy/ class=align-middle>Sharanya Chakravarthy</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/anjana-umapathy/ class=align-middle>Anjana Umapathy</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yulia-tsvetkov/ class=align-middle>Yulia Tsvetkov</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/e/elizabeth-salesky/ class=align-middle>Elizabeth Salesky</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/matthias-sperber/ class=align-middle>Matthias Sperber</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/ws/ class=align-middle>WS</a><span class="badge badge-secondary align-middle ml-2">9</span></li><li class=list-group-item><a href=/venues/emnlp/ class=align-middle>EMNLP</a><span class="badge badge-secondary align-middle ml-2">5</span></li><li class=list-group-item><a href=/venues/acl/ class=align-middle>ACL</a><span class="badge badge-secondary align-middle ml-2">5</span></li><li class=list-group-item><a href=/venues/calcs/ class=align-middle>CALCS</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/lrec/ class=align-middle>LREC</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-venues aria-expanded=false aria-controls=more-venues>show all...</li><div class="collapse border-top" id=more-venues><li class=list-group-item><a href=/venues/wnut/ class=align-middle>WNUT</a><span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>