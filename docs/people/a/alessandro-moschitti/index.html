<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Alessandro Moschitti - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Alessandro</span> <span class=font-weight-bold>Moschitti</span></h2><hr><div class=row><div class=col-lg-9><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.252.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--252 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.252 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.252/>Joint Models for Answer Verification in Question Answering Systems</a></strong><br><a href=/people/z/zeyu-zhang/>Zeyu Zhang</a>
|
<a href=/people/t/thuy-vu/>Thuy Vu</a>
|
<a href=/people/a/alessandro-moschitti/>Alessandro Moschitti</a><br><a href=/volumes/2021.acl-long/ class=text-muted>Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--252><div class="card-body p-3 small">This paper studies joint models for selecting correct answer sentences among the top k provided by answer sentence selection (AS2) modules, which are core components of retrieval-based Question Answering (QA) systems. Our work shows that a critical step to effectively exploiting an <a href=https://en.wikipedia.org/wiki/Answer_set>answer set</a> regards modeling the interrelated information between pair of answers. For this purpose, we build a three-way multi-classifier, which decides if an answer supports, refutes, or is neutral with respect to another one. More specifically, our neural architecture integrates a state-of-the-art AS2 module with the multi-classifier, and a joint layer connecting all components. We tested our models on WikiQA, TREC-QA, and a real-world dataset. The results show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> obtain the new state of the art in AS2.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.261.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--261 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.261 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.261/>Modeling Context in Answer Sentence Selection Systems on a Latency Budget</a></strong><br><a href=/people/r/rujun-han/>Rujun Han</a>
|
<a href=/people/l/luca-soldaini/>Luca Soldaini</a>
|
<a href=/people/a/alessandro-moschitti/>Alessandro Moschitti</a><br><a href=/volumes/2021.eacl-main/ class=text-muted>Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--261><div class="card-body p-3 small">Answer Sentence Selection (AS2) is an efficient approach for the design of open-domain Question Answering (QA) systems. In order to achieve low <a href=https://en.wikipedia.org/wiki/Latency_(engineering)>latency</a>, traditional AS2 models score question-answer pairs individually, ignoring any information from the document each potential answer was extracted from. In contrast, more computationally expensive <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> designed for machine reading comprehension tasks typically receive one or more passages as input, which often results in better <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>. In this work, we present an approach to efficiently incorporate <a href=https://en.wikipedia.org/wiki/Context_(language_use)>contextual information</a> in AS2 models. For each answer candidate, we first use unsupervised similarity techniques to extract relevant sentences from its source document, which we then feed into an efficient transformer architecture fine-tuned for AS2. Our best approach, which leverages a multi-way attention architecture to efficiently encode <a href=https://en.wikipedia.org/wiki/Context_(computing)>context</a>, improves 6 % to 11 % over non-contextual state of the art in <a href=https://en.wikipedia.org/wiki/IBM_System_i>AS2</a> with minimal impact on <a href=https://en.wikipedia.org/wiki/Latency_(engineering)>system latency</a>. All experiments in this work were conducted in English.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.266.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--266 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.266 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.266/>CDA : a Cost Efficient Content-based Multilingual Web Document Aligner<span class=acl-fixed-case>CDA</span>: a Cost Efficient Content-based Multilingual Web Document Aligner</a></strong><br><a href=/people/t/thuy-vu/>Thuy Vu</a>
|
<a href=/people/a/alessandro-moschitti/>Alessandro Moschitti</a><br><a href=/volumes/2021.eacl-main/ class=text-muted>Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--266><div class="card-body p-3 small">We introduce a Content-based Document Alignment approach (CDA), an efficient method to align multilingual web documents based on content in creating parallel training data for machine translation (MT) systems operating at the industrial level. CDA works in two steps : (i) projecting documents of a <a href=https://en.wikipedia.org/wiki/Web_domain>web domain</a> to a shared multilingual space ; then (ii) aligning them based on the similarity of their representations in such space. We leverage lexical translation models to build vector representations using TFIDF. CDA achieves performance comparable with state-of-the-art systems in the WMT-16 Bilingual Document Alignment Shared Task benchmark while operating in multilingual space. Besides, we created two web-scale datasets to examine the <a href=https://en.wikipedia.org/wiki/Robustness_(computer_science)>robustness</a> of <a href=https://en.wikipedia.org/wiki/Computer-aided_design>CDA</a> in an industrial setting involving up to 28 languages and millions of documents. The experiments show that CDA is robust, cost-effective, and is significantly superior in (i) processing large and noisy web data and (ii) scaling to new and low-resourced languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.583.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--583 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.583 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.583/>Will this Question be Answered? Question Filtering via Answer Model Distillation for Efficient Question Answering</a></strong><br><a href=/people/s/siddhant-garg/>Siddhant Garg</a>
|
<a href=/people/a/alessandro-moschitti/>Alessandro Moschitti</a><br><a href=/volumes/2021.emnlp-main/ class=text-muted>Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--583><div class="card-body p-3 small">In this paper we propose a novel approach towards improving the efficiency of Question Answering (QA) systems by filtering out questions that will not be answered by them. This is based on an interesting new finding : the answer confidence scores of state-of-the-art QA systems can be approximated well by models solely using the input question text. This enables preemptive filtering of questions that are not answered by the <a href=https://en.wikipedia.org/wiki/System>system</a> due to their answer confidence scores being lower than the <a href=https://en.wikipedia.org/wiki/System>system threshold</a>. Specifically, we learn Transformer-based question models by distilling Transformer-based answering models. Our experiments on three popular QA datasets and one industrial QA benchmark demonstrate the ability of our question models to approximate the Precision / Recall curves of the target QA system well. These question models, when used as filters, can effectively trade off lower computation cost of QA systems for lower <a href=https://en.wikipedia.org/wiki/Recall_(memory)>Recall</a>, e.g., reducing computation by ~60 %, while only losing ~3-4 % of <a href=https://en.wikipedia.org/wiki/Recall_(memory)>Recall</a>.</div></div><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.acl-main.504.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--acl-main--504 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.acl-main.504 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://slideslive.com/38929221 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.acl-main.504" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.acl-main.504/>The Cascade Transformer : an Application for Efficient Answer Sentence Selection</a></strong><br><a href=/people/l/luca-soldaini/>Luca Soldaini</a>
|
<a href=/people/a/alessandro-moschitti/>Alessandro Moschitti</a><br><a href=/volumes/2020.acl-main/ class=text-muted>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--acl-main--504><div class="card-body p-3 small">Large transformer-based language models have been shown to be very effective in many <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification tasks</a>. However, their <a href=https://en.wikipedia.org/wiki/Computational_complexity_theory>computational complexity</a> prevents their use in applications requiring the <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a> of a large set of candidates. While previous works have investigated approaches to reduce model size, relatively little attention has been paid to techniques to improve <a href=https://en.wikipedia.org/wiki/Batch_processing>batch throughput</a> during <a href=https://en.wikipedia.org/wiki/Statistical_inference>inference</a>. In this paper, we introduce the Cascade Transformer, a simple yet effective technique to adapt transformer-based models into a cascade of rankers. Each <a href=https://en.wikipedia.org/wiki/Ranker>ranker</a> is used to prune a subset of candidates in a batch, thus dramatically increasing <a href=https://en.wikipedia.org/wiki/Throughput>throughput</a> at <a href=https://en.wikipedia.org/wiki/Time_complexity>inference time</a>. Partial encodings from the transformer model are shared among <a href=https://en.wikipedia.org/wiki/Rank_(linear_algebra)>rerankers</a>, providing further speed-up. When compared to a state-of-the-art transformer model, our approach reduces <a href=https://en.wikipedia.org/wiki/Computation>computation</a> by 37 % with almost no impact on <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>, as measured on two English Question Answering datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.knlp-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.knlp-1.0/>Proceedings of Knowledgeable NLP: the First Workshop on Integrating Structured Knowledge and Neural Networks for NLP</a></strong><br><a href=/people/o/oren-sar-shalom/>Oren Sar Shalom</a>
|
<a href=/people/a/alexander-panchenko/>Alexander Panchenko</a>
|
<a href=/people/c/cicero-dos-santos/>Cicero dos Santos</a>
|
<a href=/people/v/varvara-logacheva/>Varvara Logacheva</a>
|
<a href=/people/a/alessandro-moschitti/>Alessandro Moschitti</a>
|
<a href=/people/i/ido-dagan/>Ido Dagan</a><br><a href=/volumes/2020.knlp-1/ class=text-muted>Proceedings of Knowledgeable NLP: the First Workshop on Integrating Structured Knowledge and Neural Networks for NLP</a></span></p><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1183.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1183 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1183 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-1183/>A Study of Latent Structured Prediction Approaches to Passage Reranking</a></strong><br><a href=/people/i/iryna-haponchyk/>Iryna Haponchyk</a>
|
<a href=/people/a/alessandro-moschitti/>Alessandro Moschitti</a><br><a href=/volumes/N19-1/ class=text-muted>Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1183><div class="card-body p-3 small">The structured output framework provides a helpful tool for learning to rank problems. In this paper, we propose a structured output approach which regards <a href=https://en.wikipedia.org/wiki/Ranking>rankings</a> as <a href=https://en.wikipedia.org/wiki/Latent_variable>latent variables</a>. Our approach addresses the complex optimization of Mean Average Precision (MAP) ranking metric. We provide an <a href=https://en.wikipedia.org/wiki/Statistical_inference>inference procedure</a> to find the max-violating ranking based on the decomposition of the corresponding loss. The results of our experiments on WikiQA and TREC13 datasets show that our reranking based on <a href=https://en.wikipedia.org/wiki/Structured_prediction>structured prediction</a> is a promising research direction.</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1185.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1185 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1185 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1185/>Learning to Progressively Recognize New Named Entities with Sequence to Sequence Models</a></strong><br><a href=/people/l/lingzhen-chen/>Lingzhen Chen</a>
|
<a href=/people/a/alessandro-moschitti/>Alessandro Moschitti</a><br><a href=/volumes/C18-1/ class=text-muted>Proceedings of the 27th International Conference on Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1185><div class="card-body p-3 small">In this paper, we propose to use a sequence to sequence model for Named Entity Recognition (NER) and we explore the effectiveness of such <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> in a progressive NER setting a Transfer Learning (TL) setting. We train an initial <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> on source data and transfer it to a <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> that can recognize new NE categories in the target data during a subsequent step, when the source data is no longer available. Our solution consists in : (i) to reshape and re-parametrize the output layer of the first learned model to enable the recognition of new NEs ; (ii) to leave the rest of the architecture unchanged, such that it is initialized with parameters transferred from the initial model ; and (iii) to fine tune the network on the target data. Most importantly, we design a new NER approach based on sequence to sequence (Seq2Seq) models, which can intuitively work better in our progressive setting. We compare our approach with a Bidirectional LSTM, which is a strong neural NER model. Our experiments show that the Seq2Seq model performs very well on the standard NER setting and it is more robust in the progressive setting. Our approach can recognize previously unseen NE categories while preserving the knowledge of the seen data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1133.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1133 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1133 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D18-1133.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1133/>Semantic Linking in <a href=https://en.wikipedia.org/wiki/Convolutional_neural_network>Convolutional Neural Networks</a> for Answer Sentence Selection</a></strong><br><a href=/people/m/massimo-nicosia/>Massimo Nicosia</a>
|
<a href=/people/a/alessandro-moschitti/>Alessandro Moschitti</a><br><a href=/volumes/D18-1/ class=text-muted>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1133><div class="card-body p-3 small">State-of-the-art <a href=https://en.wikipedia.org/wiki/Social_network>networks</a> that model relations between two pieces of text often use complex architectures and <a href=https://en.wikipedia.org/wiki/Attention>attention</a>. In this paper, instead of focusing on architecture engineering, we take advantage of small amounts of labelled data that model semantic phenomena in text to encode matching features directly in the word representations. This greatly boosts the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of our reference network, while keeping the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> simple and fast to train. Our approach also beats a tree kernel model that uses similar input encodings, and neural models which use advanced attention and compare-aggregate mechanisms.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1240.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1240 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1240 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1240" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D18-1240/>Cross-Pair Text Representations for Answer Sentence Selection</a></strong><br><a href=/people/k/kateryna-tymoshenko/>Kateryna Tymoshenko</a>
|
<a href=/people/a/alessandro-moschitti/>Alessandro Moschitti</a><br><a href=/volumes/D18-1/ class=text-muted>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1240><div class="card-body p-3 small">High-level semantics tasks, e.g., paraphrasing, textual entailment or question answering, involve modeling of text pairs. Before the emergence of <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a>, this has been mostly performed using intra-pair features, which incorporate similarity scores or rewrite rules computed between the members within the same pair. In this paper, we compute scalar products between vectors representing similarity between members of different pairs, in place of simply using a single vector for each pair. This allows us to obtain a representation specific to any pair of pairs, which delivers the state of the art in answer sentence selection. Most importantly, our approach can outperform much more complex <a href=https://en.wikipedia.org/wiki/Algorithm>algorithms</a> based on <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1254.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1254 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1254 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/305938531 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1254/>Supervised Clustering of Questions into Intents for Dialog System Applications</a></strong><br><a href=/people/i/iryna-haponchyk/>Iryna Haponchyk</a>
|
<a href=/people/a/antonio-uva/>Antonio Uva</a>
|
<a href=/people/s/seunghak-yu/>Seunghak Yu</a>
|
<a href=/people/o/olga-uryupina/>Olga Uryupina</a>
|
<a href=/people/a/alessandro-moschitti/>Alessandro Moschitti</a><br><a href=/volumes/D18-1/ class=text-muted>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1254><div class="card-body p-3 small">Modern automated dialog systems require complex dialog managers able to deal with <a href=https://en.wikipedia.org/wiki/User_intent>user intent</a> triggered by high-level semantic questions. In this paper, we propose a <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> for automatically clustering questions into <a href=https://en.wikipedia.org/wiki/User_intent>user intents</a> to help the design tasks. Since questions are short texts, uncovering their <a href=https://en.wikipedia.org/wiki/Semantics>semantics</a> to group them together can be very challenging. We approach the problem by using powerful semantic classifiers from question duplicate / matching research along with a novel idea of supervised clustering methods based on structured output. We test our approach on two intent clustering corpora, showing an impressive improvement over previous methods for two languages / domains.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1070.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1070 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1070 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-1070/>Automatic Stance Detection Using End-to-End Memory Networks</a></strong><br><a href=/people/m/mitra-mohtarami/>Mitra Mohtarami</a>
|
<a href=/people/r/ramy-baly/>Ramy Baly</a>
|
<a href=/people/j/james-glass/>James Glass</a>
|
<a href=/people/p/preslav-nakov/>Preslav Nakov</a>
|
<a href=/people/l/lluis-marquez/>Lluís Màrquez</a>
|
<a href=/people/a/alessandro-moschitti/>Alessandro Moschitti</a><br><a href=/volumes/N18-1/ class=text-muted>Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1070><div class="card-body p-3 small">We present an effective end-to-end memory network model that jointly (i) predicts whether a given document can be considered as relevant evidence for a given claim, and (ii) extracts snippets of evidence that can be used to reason about the factuality of the target claim. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> combines the advantages of convolutional and recurrent neural networks as part of a memory network. We further introduce a <a href=https://en.wikipedia.org/wiki/Similarity_matrix>similarity matrix</a> at the inference level of the memory network in order to extract snippets of evidence for input claims more accurately. Our experiments on a public benchmark dataset, FakeNewsChallenge, demonstrate the effectiveness of our approach.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2046.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2046 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2046 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2046.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-2046" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P18-2046/>Injecting <a href=https://en.wikipedia.org/wiki/Relational_structure>Relational Structural Representation</a> in <a href=https://en.wikipedia.org/wiki/Neural_network>Neural Networks</a> for Question Similarity</a></strong><br><a href=/people/a/antonio-uva/>Antonio Uva</a>
|
<a href=/people/d/daniele-bonadiman/>Daniele Bonadiman</a>
|
<a href=/people/a/alessandro-moschitti/>Alessandro Moschitti</a><br><a href=/volumes/P18-2/ class=text-muted>Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2046><div class="card-body p-3 small">Effectively using full syntactic parsing information in Neural Networks (NNs) for solving <a href=https://en.wikipedia.org/wiki/Relational_model>relational tasks</a>, e.g., question similarity, is still an open problem. In this paper, we propose to inject structural representations in NNs by (i) learning a model with Tree Kernels (TKs) on relatively few pairs of questions (few thousands) as gold standard (GS) training data is typically scarce, (ii) predicting labels on a very large corpus of question pairs, and (iii) pre-training NNs on such large corpus. The results on Quora and SemEval question similarity datasets show that NNs using our approach can learn more accurate models, especially after fine tuning on GS.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-4023.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-4023 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-4023 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P18-4023/>A Flexible, Efficient and Accurate Framework for Community Question Answering Pipelines</a></strong><br><a href=/people/s/salvatore-romeo/>Salvatore Romeo</a>
|
<a href=/people/g/giovanni-da-san-martino/>Giovanni Da San Martino</a>
|
<a href=/people/a/alberto-barron-cedeno/>Alberto Barrón-Cedeño</a>
|
<a href=/people/a/alessandro-moschitti/>Alessandro Moschitti</a><br><a href=/volumes/P18-4/ class=text-muted>Proceedings of ACL 2018, System Demonstrations</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-4023><div class="card-body p-3 small">Although <a href=https://en.wikipedia.org/wiki/Deep_learning>deep neural networks</a> have been proving to be excellent tools to deliver state-of-the-art results, when data is scarce and the tackled tasks involve complex semantic inference, <a href=https://en.wikipedia.org/wiki/Deep_learning>deep linguistic processing</a> and traditional structure-based approaches, such as tree kernel methods, are an alternative solution. Community Question Answering is a research area that benefits from deep linguistic analysis to improve the experience of the community of forum users. In this paper, we present a UIMA framework to distribute the computation of cQA tasks over computer clusters such that traditional systems can scale to large datasets and deliver fast processing.</div></div><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K17-1007.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K17-1007 data-toggle=collapse aria-expanded=false aria-controls=abstract-K17-1007 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/K17-1007/>Collaborative Partitioning for Coreference Resolution</a></strong><br><a href=/people/o/olga-uryupina/>Olga Uryupina</a>
|
<a href=/people/a/alessandro-moschitti/>Alessandro Moschitti</a><br><a href=/volumes/K17-1/ class=text-muted>Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K17-1007><div class="card-body p-3 small">This paper presents a collaborative partitioning algorithma novel ensemble-based approach to <a href=https://en.wikipedia.org/wiki/Coreference_resolution>coreference resolution</a>. Starting from the all-singleton partition, we search for a solution close to the <a href=https://en.wikipedia.org/wiki/Statistical_ensemble_(mathematical_physics)>ensemble</a>&#8217;s outputs in terms of a task-specific similarity measure. Our approach assumes a loose integration of individual components of the <a href=https://en.wikipedia.org/wiki/Statistical_ensemble_(mathematical_physics)>ensemble</a> and can therefore combine arbitrary coreference resolvers, regardless of their models. Our experiments on the CoNLL dataset show that collaborative partitioning yields results superior to those attained by the individual components, for ensembles of both strong and weak systems. Moreover, by applying the collaborative partitioning algorithm on top of three state-of-the-art resolvers, we obtain the best coreference performance reported so far in the literature (MELA v08 score of 64.47).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K17-1027.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K17-1027 data-toggle=collapse aria-expanded=false aria-controls=abstract-K17-1027 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/K17-1027/>Learning Contextual Embeddings for Structural Semantic Similarity using Categorical Information</a></strong><br><a href=/people/m/massimo-nicosia/>Massimo Nicosia</a>
|
<a href=/people/a/alessandro-moschitti/>Alessandro Moschitti</a><br><a href=/volumes/K17-1/ class=text-muted>Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K17-1027><div class="card-body p-3 small">Tree kernels (TKs) and <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a> are two effective approaches for automatic feature engineering. In this paper, we combine them by modeling context word similarity in semantic TKs. This way, the latter can operate subtree matching by applying neural-based similarity on <a href=https://en.wikipedia.org/wiki/Tree_(data_structure)>tree lexical nodes</a>. We study how to learn <a href=https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning>representations</a> for the words in context such that <a href=https://en.wikipedia.org/wiki/Theory_of_Knowledge>TKs</a> can exploit more <a href=https://en.wikipedia.org/wiki/Focus_(linguistics)>focused information</a>. We found that neural embeddings produced by current methods do not provide a suitable contextual similarity. Thus, we define a new approach based on a <a href=https://en.wikipedia.org/wiki/Siamese_network>Siamese Network</a>, which produces word representations while learning a binary text similarity. We set the <a href=https://en.wikipedia.org/wiki/Logical_disjunction>latter</a> considering examples in the same category as similar. The experiments on question and sentiment classification show that our semantic TK highly improves previous results.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2082.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2082 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2082 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-2082/>Self-Crowdsourcing Training for <a href=https://en.wikipedia.org/wiki/Relation_extraction>Relation Extraction</a></a></strong><br><a href=/people/a/azad-abad/>Azad Abad</a>
|
<a href=/people/m/moin-nabi/>Moin Nabi</a>
|
<a href=/people/a/alessandro-moschitti/>Alessandro Moschitti</a><br><a href=/volumes/P17-2/ class=text-muted>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2082><div class="card-body p-3 small">In this paper we introduce a self-training strategy for <a href=https://en.wikipedia.org/wiki/Crowdsourcing>crowdsourcing</a>. The training examples are automatically selected to train the <a href=https://en.wikipedia.org/wiki/Crowdsourcing>crowd workers</a>. Our experimental results show an impact of 5 % Improvement in terms of <a href=https://en.wikipedia.org/wiki/F-number>F1</a> for relation extraction task, compared to the method based on distant supervision.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S17-2003.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S17-2003 data-toggle=collapse aria-expanded=false aria-controls=abstract-S17-2003 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=S17-2003" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/S17-2003/>SemEval-2017 Task 3 : Community Question Answering<span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2017 Task 3: Community Question Answering</a></strong><br><a href=/people/p/preslav-nakov/>Preslav Nakov</a>
|
<a href=/people/d/doris-hoogeveen/>Doris Hoogeveen</a>
|
<a href=/people/l/lluis-marquez/>Lluís Màrquez</a>
|
<a href=/people/a/alessandro-moschitti/>Alessandro Moschitti</a>
|
<a href=/people/h/hamdy-mubarak/>Hamdy Mubarak</a>
|
<a href=/people/t/timothy-baldwin/>Timothy Baldwin</a>
|
<a href=/people/k/karin-verspoor/>Karin Verspoor</a><br><a href=/volumes/S17-2/ class=text-muted>Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S17-2003><div class="card-body p-3 small">We describe SemEval2017 Task 3 on Community Question Answering. This year, we reran the four subtasks from SemEval-2016 : (A) QuestionComment Similarity, (B) QuestionQuestion Similarity, (C) QuestionExternal Comment Similarity, and (D) Rerank the correct answers for a new question in Arabic, providing all the data from 2015 and 2016 for training, and fresh data for testing. Additionally, we added a new subtask E in order to enable experimentation with Multi-domain Question Duplicate Detection in a larger-scale scenario, using StackExchange subforums. A total of 23 teams participated in the task, and submitted a total of 85 runs (36 primary and 49 contrastive) for subtasks AD. Unfortunately, no teams participated in subtask E. A variety of <a href=https://en.wikipedia.org/wiki/Software_development_process>approaches</a> and <a href=https://en.wikipedia.org/wiki/Software_feature>features</a> were used by the participating <a href=https://en.wikipedia.org/wiki/System>systems</a> to address the different subtasks. The best systems achieved an official score (MAP) of 88.43, 47.22, 15.46, and 61.16 in subtasks A, B, C, and D, respectively. These scores are better than the <a href=https://en.wikipedia.org/wiki/Baseline_(surveying)>baselines</a>, especially for subtasks AC.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S17-2053.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S17-2053 data-toggle=collapse aria-expanded=false aria-controls=abstract-S17-2053 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/S17-2053/>KeLP at SemEval-2017 Task 3 : Learning Pairwise Patterns in Community Question Answering<span class=acl-fixed-case>K</span>e<span class=acl-fixed-case>LP</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2017 Task 3: Learning Pairwise Patterns in Community Question Answering</a></strong><br><a href=/people/s/simone-filice/>Simone Filice</a>
|
<a href=/people/g/giovanni-da-san-martino/>Giovanni Da San Martino</a>
|
<a href=/people/a/alessandro-moschitti/>Alessandro Moschitti</a><br><a href=/volumes/S17-2/ class=text-muted>Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S17-2053><div class="card-body p-3 small">This paper describes the KeLP system participating in the SemEval-2017 community Question Answering (cQA) task. The system is a refinement of the kernel-based sentence pair modeling we proposed for the previous year challenge. It is implemented within the Kernel-based Learning Platform called KeLP, from which we inherit the team&#8217;s name. Our primary submission ranked first in subtask A, and third in subtasks B and C, being the only systems appearing in the top-3 ranking for all the English subtasks. This shows that the proposed <a href=https://en.wikipedia.org/wiki/Conceptual_framework>framework</a>, which has minor variations among the three subtasks, is extremely flexible and effective in tackling learning tasks defined on sentence pairs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1093.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1093 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1093 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1093/>Ranking Kernels for Structures and Embeddings : A Hybrid Preference and Classification Model</a></strong><br><a href=/people/k/kateryna-tymoshenko/>Kateryna Tymoshenko</a>
|
<a href=/people/d/daniele-bonadiman/>Daniele Bonadiman</a>
|
<a href=/people/a/alessandro-moschitti/>Alessandro Moschitti</a><br><a href=/volumes/D17-1/ class=text-muted>Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1093><div class="card-body p-3 small">Recent work has shown that Tree Kernels (TKs) and Convolutional Neural Networks (CNNs) obtain the state of the art in answer sentence reranking. Additionally, their combination used in Support Vector Machines (SVMs) is promising as it can exploit both the syntactic patterns captured by TKs and the embeddings learned by CNNs. However, the <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a> are constructed according to a <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification function</a>, which is not directly exploitable in the preference ranking algorithm of SVMs. In this work, we propose a new hybrid approach combining preference ranking applied to TKs and pointwise ranking applied to CNNs. We show that our approach produces better results on two well-known and rather different datasets : WikiQA for answer sentence selection and SemEval cQA for comment selection in Community Question Answering.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2023.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2023 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2023 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2023/>A Practical Perspective on Latent Structured Prediction for <a href=https://en.wikipedia.org/wiki/Coreference_resolution>Coreference Resolution</a></a></strong><br><a href=/people/i/iryna-haponchyk/>Iryna Haponchyk</a>
|
<a href=/people/a/alessandro-moschitti/>Alessandro Moschitti</a><br><a href=/volumes/E17-2/ class=text-muted>Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2023><div class="card-body p-3 small">Latent structured prediction theory proposes powerful methods such as Latent Structural SVM (LSSVM), which can potentially be very appealing for coreference resolution (CR). In contrast, only small work is available, mainly targeting the latent structured perceptron (LSP). In this paper, we carried out a practical study comparing for the first time <a href=https://en.wikipedia.org/wiki/Educational_technology>online learning</a> with LSSVM. We analyze the intricacies that may have made initial attempts to use LSSVM fail, i.e., a huge <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training time</a> and much lower accuracy produced by Kruskal&#8217;s spanning tree algorithm. In this respect, we also propose a new effective feature selection approach for improving system efficiency. The results show that LSP, if correctly parameterized, produces the same performance as LSSVM, being much more efficient.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2115.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2115 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2115 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2115/>Effective shared representations with <a href=https://en.wikipedia.org/wiki/Multitask_learning>Multitask Learning</a> for Community Question Answering</a></strong><br><a href=/people/d/daniele-bonadiman/>Daniele Bonadiman</a>
|
<a href=/people/a/antonio-uva/>Antonio Uva</a>
|
<a href=/people/a/alessandro-moschitti/>Alessandro Moschitti</a><br><a href=/volumes/E17-2/ class=text-muted>Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2115><div class="card-body p-3 small">An important asset of using Deep Neural Networks (DNNs) for text applications is their ability to automatically engineering features. Unfortunately, DNNs usually require a lot of training data, especially for highly semantic tasks such as community Question Answering (cQA). In this paper, we tackle the problem of data scarcity by learning the target <a href=https://en.wikipedia.org/wiki/Deep_learning>DNN</a> together with two auxiliary tasks in a multitask learning setting. We exploit the strong semantic connection between selection of comments relevant to (i) new questions and (ii) forum questions. This enables a global representation for comments, new and previous questions. The experiments of our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> on a SemEval challenge dataset for cQA show a 20 % of relative improvement over standard DNNs.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Alessandro+Moschitti" title="Search for 'Alessandro Moschitti' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/i/iryna-haponchyk/ class=align-middle>Iryna Haponchyk</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/a/antonio-uva/ class=align-middle>Antonio Uva</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/d/daniele-bonadiman/ class=align-middle>Daniele Bonadiman</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/t/thuy-vu/ class=align-middle>Thuy Vu</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/l/luca-soldaini/ class=align-middle>Luca Soldaini</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/o/olga-uryupina/ class=align-middle>Olga Uryupina</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/m/massimo-nicosia/ class=align-middle>Massimo Nicosia</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/k/kateryna-tymoshenko/ class=align-middle>Kateryna Tymoshenko</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/p/preslav-nakov/ class=align-middle>Preslav Nakov</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/l/lluis-marquez/ class=align-middle>Lluís Màrquez</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/g/giovanni-da-san-martino/ class=align-middle>Giovanni Da San Martino</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/l/lingzhen-chen/ class=align-middle>Lingzhen Chen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/zeyu-zhang/ class=align-middle>Zeyu Zhang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/azad-abad/ class=align-middle>Azad Abad</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/moin-nabi/ class=align-middle>Moin Nabi</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/rujun-han/ class=align-middle>Rujun Han</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/seunghak-yu/ class=align-middle>Seunghak Yu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/siddhant-garg/ class=align-middle>Siddhant Garg</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/doris-hoogeveen/ class=align-middle>Doris Hoogeveen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/hamdy-mubarak/ class=align-middle>Hamdy Mubarak</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/timothy-baldwin/ class=align-middle>Timothy Baldwin</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/karin-verspoor/ class=align-middle>Karin Verspoor</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/simone-filice/ class=align-middle>Simone Filice</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/o/oren-sar-shalom/ class=align-middle>Oren Sar Shalom</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/alexander-panchenko/ class=align-middle>Alexander Panchenko</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/cicero-dos-santos/ class=align-middle>Cicero dos Santos</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/v/varvara-logacheva/ class=align-middle>Varvara Logacheva</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/i/ido-dagan/ class=align-middle>Ido Dagan</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/mitra-mohtarami/ class=align-middle>Mitra Mohtarami</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/ramy-baly/ class=align-middle>Ramy Baly</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/james-glass/ class=align-middle>James Glass</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/salvatore-romeo/ class=align-middle>Salvatore Romeo</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/alberto-barron-cedeno/ class=align-middle>Alberto Barrón-Cedeño</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/acl/ class=align-middle>ACL</a><span class="badge badge-secondary align-middle ml-2">5</span></li><li class=list-group-item><a href=/venues/emnlp/ class=align-middle>EMNLP</a><span class="badge badge-secondary align-middle ml-2">5</span></li><li class=list-group-item><a href=/venues/eacl/ class=align-middle>EACL</a><span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/venues/conll/ class=align-middle>CoNLL</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/semeval/ class=align-middle>SemEval</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-venues aria-expanded=false aria-controls=more-venues>show all...</li><div class="collapse border-top" id=more-venues><li class=list-group-item><a href=/venues/naacl/ class=align-middle>NAACL</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/coling/ class=align-middle>COLING</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/knlp/ class=align-middle>knlp</a><span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>