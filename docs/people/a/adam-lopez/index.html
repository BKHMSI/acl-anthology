<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Adam Lopez - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Adam</span> <span class=font-weight-bold>Lopez</span></h2><hr><div class=row><div class=col-lg-9><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.150.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--150 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.150 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.150/>Intrinsic Bias Metrics Do Not Correlate with Application Bias</a></strong><br><a href=/people/s/seraphina-goldfarb-tarrant/>Seraphina Goldfarb-Tarrant</a>
|
<a href=/people/r/rebecca-marchant/>Rebecca Marchant</a>
|
<a href=/people/r/ricardo-munoz-sanchez/>Ricardo Muñoz Sánchez</a>
|
<a href=/people/m/mugdha-pandya/>Mugdha Pandya</a>
|
<a href=/people/a/adam-lopez/>Adam Lopez</a><br><a href=/volumes/2021.acl-long/ class=text-muted>Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--150><div class="card-body p-3 small">Natural Language Processing (NLP) systems learn harmful societal biases that cause them to amplify inequality as they are deployed in more and more situations. To guide efforts at debiasing these systems, the NLP community relies on a variety of <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> that quantify bias in models. Some of these <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> are intrinsic, measuring bias in word embedding spaces, and some are extrinsic, measuring bias in downstream tasks that the word embeddings enable. Do these intrinsic and extrinsic metrics correlate with each other? We compare intrinsic and extrinsic metrics across hundreds of trained models covering different tasks and experimental conditions. Our results show no reliable correlation between these <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> that holds in all scenarios across tasks and languages. We urge researchers working on <a href=https://en.wikipedia.org/wiki/Debiasing>debiasing</a> to focus on extrinsic measures of bias, and to make using these measures more feasible via creation of new challenge sets and annotated test data. To aid this effort, we release <a href=https://en.wikipedia.org/wiki/Source_code>code</a>, a new intrinsic metric, and an annotated test set focused on gender bias in <a href=https://en.wikipedia.org/wiki/Hate_speech>hate speech</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.sigmorphon-1.9.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--sigmorphon-1--9 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.sigmorphon-1.9 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.sigmorphon-1.9/>Adaptor Grammars for Unsupervised Paradigm Clustering<span class=acl-fixed-case>A</span>daptor <span class=acl-fixed-case>G</span>rammars for Unsupervised Paradigm Clustering</a></strong><br><a href=/people/k/kate-mccurdy/>Kate McCurdy</a>
|
<a href=/people/s/sharon-goldwater/>Sharon Goldwater</a>
|
<a href=/people/a/adam-lopez/>Adam Lopez</a><br><a href=/volumes/2021.sigmorphon-1/ class=text-muted>Proceedings of the 18th SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--sigmorphon-1--9><div class="card-body p-3 small">This work describes the Edinburgh submission to the SIGMORPHON 2021 Shared Task 2 on unsupervised morphological paradigm clustering. Given raw text input, the task was to assign each token to a cluster with other tokens from the same paradigm. We use Adaptor Grammar segmentations combined with frequency-based heuristics to predict paradigm clusters. Our system achieved the highest average F1 score across 9 test languages, placing first out of 15 submissions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.semeval-1.9.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--semeval-1--9 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.semeval-1.9 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.semeval-1.9/>SemEval 2021 Task 7 : HaHackathon, Detecting and Rating Humor and Offense<span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val 2021 Task 7: <span class=acl-fixed-case>H</span>a<span class=acl-fixed-case>H</span>ackathon, Detecting and Rating Humor and Offense</a></strong><br><a href=/people/j/j-a-meaney/>J. A. Meaney</a>
|
<a href=/people/s/steven-wilson/>Steven Wilson</a>
|
<a href=/people/l/luis-chiruzzo/>Luis Chiruzzo</a>
|
<a href=/people/a/adam-lopez/>Adam Lopez</a>
|
<a href=/people/w/walid-magdy/>Walid Magdy</a><br><a href=/volumes/2021.semeval-1/ class=text-muted>Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--semeval-1--9><div class="card-body p-3 small">SemEval 2021 Task 7, HaHackathon, was the first shared task to combine the previously separate domains of humor detection and offense detection. We collected 10,000 texts from <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a> and the Kaggle Short Jokes dataset, and had each annotated for humor and offense by 20 annotators aged 18-70. Our subtasks were binary humor detection, prediction of humor and offense ratings, and a novel controversy task : to predict if the variance in the humor ratings was higher than a specific threshold. The subtasks attracted 36-58 submissions, with most of the participants choosing to use pre-trained language models. Many of the highest performing teams also implemented additional optimization techniques, including task-adaptive training and adversarial training. The results suggest that the participating <a href=https://en.wikipedia.org/wiki/System>systems</a> are well suited to humor detection, but that humor controversy is a more challenging task. We discuss which <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> excel in this task, which auxiliary techniques boost their performance, and analyze the errors which were not captured by the best <a href=https://en.wikipedia.org/wiki/System>systems</a>.</div></div><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.acl-main.159.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--acl-main--159 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.acl-main.159 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.acl-main.159.Dataset.zip data-toggle=tooltip data-placement=top title=Dataset><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=http://slideslive.com/38929385 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.acl-main.159/>Inflecting When There’s No Majority : Limitations of Encoder-Decoder Neural Networks as Cognitive Models for German Plurals<span class=acl-fixed-case>G</span>erman Plurals</a></strong><br><a href=/people/k/kate-mccurdy/>Kate McCurdy</a>
|
<a href=/people/s/sharon-goldwater/>Sharon Goldwater</a>
|
<a href=/people/a/adam-lopez/>Adam Lopez</a><br><a href=/volumes/2020.acl-main/ class=text-muted>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--acl-main--159><div class="card-body p-3 small">Can <a href=https://en.wikipedia.org/wiki/Artificial_neural_network>artificial neural networks</a> learn to represent <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>inflectional morphology</a> and generalize to new words as <a href=https://en.wikipedia.org/wiki/Speech>human speakers</a> do? Kirov and Cotterell (2018) argue that the answer is yes : modern Encoder-Decoder (ED) architectures learn human-like behavior when inflecting English verbs, such as extending the regular past tense form /-(e)d/ to novel words. However, their work does not address the criticism raised by Marcus et al. (1995): that neural models may learn to extend not the regular, but the most frequent class and thus fail on tasks like German number inflection, where infrequent suffixes like /-s/ can still be productively generalized. To investigate this question, we first collect a new <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> from <a href=https://en.wikipedia.org/wiki/German_language>German speakers</a> (production and ratings of plural forms for novel nouns) that is designed to avoid sources of information unavailable to the ED model. The speaker data show high variability, and two <a href=https://en.wikipedia.org/wiki/Suffix>suffixes</a> evince &#8216;regular&#8217; behavior, appearing more often with phonologically atypical inputs. Encoder-decoder models do generalize the most frequently produced plural class, but do not show human-like variability or &#8216;regular&#8217; extension of these other plural markers. We conclude that modern neural models may still struggle with minority-class generalization.</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1278.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1278 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1278 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D18-1278.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1278/>What do character-level models learn about <a href=https://en.wikipedia.org/wiki/Morphology_(biology)>morphology</a>? The case of dependency parsing</a></strong><br><a href=/people/c/clara-vania/>Clara Vania</a>
|
<a href=/people/a/andreas-grivas/>Andreas Grivas</a>
|
<a href=/people/a/adam-lopez/>Adam Lopez</a><br><a href=/volumes/D18-1/ class=text-muted>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1278><div class="card-body p-3 small">When parsing morphologically-rich languages with neural models, it is beneficial to model input at the character level, and it has been claimed that this is because character-level models learn <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphology</a>. We test these claims by comparing character-level models to an oracle with access to explicit morphological analysis on twelve languages with varying <a href=https://en.wikipedia.org/wiki/Morphological_typology>morphological typologies</a>. Our results highlight many strengths of character-level models, but also show that they are poor at disambiguating some words, particularly in the face of case syncretism. We then demonstrate that explicitly modeling morphological case improves our best model, showing that character-level models can benefit from targeted forms of explicit morphological modeling.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-info align-middle mr-1" href=#abstract-D18-3002 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-3002 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-3002/>Graph Formalisms for Meaning Representations</a></strong><br><a href=/people/a/adam-lopez/>Adam Lopez</a>
|
<a href=/people/s/sorcha-gilroy/>Sorcha Gilroy</a><br><a href=/volumes/D18-3/ class=text-muted>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: Tutorial Abstracts</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-3002><div class="card-body p-3 small">In this tutorial we will focus on Hyperedge Replacement Languages (HRL; Drewes et al. 1997), a context-free graph rewriting system. HRL are one of the most popular graph formalisms to be studied in NLP (Chiang et al., 2013; Peng et al., 2015; Bauer and Rambow, 2016). We will discuss HRL by formally defining them, studying several examples, discussing their properties, and providing exercises for the tutorial. While HRL have been used in NLP in the past, there is some speculation that they are more expressive than is necessary for graphs representing natural language (Drewes, 2017). Part of our own research has been exploring what restrictions of HRL could yield languages that are more useful for NLP and also those that have desirable properties for NLP models, such as being closed under intersection. With that in mind, we also plan to discuss Regular Graph Languages (RGL; Courcelle 1991), a subfamily of HRL which are closed under intersection. The definition of RGL is relatively simple after being introduced to HRL. We do not plan on discussing any proofs of why RGL are also a subfamily of MSOL, as described in Gilroy et al. (2017b). We will briefly mention the other formalisms shown in Figure 1 such as MSOL and DAGAL but this will focus on their properties rather than any formal definitions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/J18-1005.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-J18-1005 data-toggle=collapse aria-expanded=false aria-controls=abstract-J18-1005 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/J18-1005/>Weighted DAG Automata for Semantic Graphs<span class=acl-fixed-case>DAG</span> Automata for Semantic Graphs</a></strong><br><a href=/people/d/david-chiang/>David Chiang</a>
|
<a href=/people/f/frank-drewes/>Frank Drewes</a>
|
<a href=/people/d/daniel-gildea/>Daniel Gildea</a>
|
<a href=/people/a/adam-lopez/>Adam Lopez</a>
|
<a href=/people/g/giorgio-satta/>Giorgio Satta</a><br><a href=/volumes/J18-1/ class=text-muted>Computational Linguistics, Volume 44, Issue 1 - April 2018</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-J18-1005><div class="card-body p-3 small">Graphs have a variety of uses in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a>, particularly as representations of linguistic meaning. A deficit in this area of research is a formal framework for creating, combining, and using models involving graphs that parallels the frameworks of <a href=https://en.wikipedia.org/wiki/Finite-state_machine>finite automata</a> for <a href=https://en.wikipedia.org/wiki/String_(computer_science)>strings</a> and <a href=https://en.wikipedia.org/wiki/Finite-state_machine>finite tree automata</a> for <a href=https://en.wikipedia.org/wiki/Tree_(graph_theory)>trees</a>. A possible starting point for such a framework is the formalism of directed acyclic graph (DAG) automata, defined by Kamimura and Slutzki and extended by Quernheim and Knight. In this article, we study the latter in depth, demonstrating several new results, including a practical recognition algorithm that can be used for <a href=https://en.wikipedia.org/wiki/Inference>inference</a> and <a href=https://en.wikipedia.org/wiki/Machine_learning>learning</a> with <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> defined on DAG automata. We also propose an extension to <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graphs</a> with unbounded node degree and show that our results carry over to the extended <a href=https://en.wikipedia.org/wiki/Formalism_(philosophy_of_mathematics)>formalism</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5438.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5438 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5438 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5438/>Language Models Learn POS First<span class=acl-fixed-case>POS</span> First</a></strong><br><a href=/people/n/naomi-saphra/>Naomi Saphra</a>
|
<a href=/people/a/adam-lopez/>Adam Lopez</a><br><a href=/volumes/W18-54/ class=text-muted>Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5438><div class="card-body p-3 small">A glut of recent research shows that <a href=https://en.wikipedia.org/wiki/Language_model>language models</a> capture linguistic structure. Such work answers the question of whether a <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> represents linguistic structure. But how and when are these <a href=https://en.wikipedia.org/wiki/List_of_nonbuilding_structure_types>structures</a> acquired? Rather than treating the training process itself as a black box, we investigate how representations of linguistic structure are learned over time. In particular, we demonstrate that different aspects of linguistic structure are learned at different rates, with <a href=https://en.wikipedia.org/wiki/Part_of_speech_tagging>part of speech tagging</a> acquired early and global topic information learned continuously.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5447.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5447 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5447 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5447/>Explicitly modeling case improves neural dependency parsing</a></strong><br><a href=/people/c/clara-vania/>Clara Vania</a>
|
<a href=/people/a/adam-lopez/>Adam Lopez</a><br><a href=/volumes/W18-54/ class=text-muted>Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5447><div class="card-body p-3 small">Neural dependency parsing models that compose word representations from <a href=https://en.wikipedia.org/wiki/Character_(computing)>characters</a> can presumably exploit <a href=https://en.wikipedia.org/wiki/Morphosyntax>morphosyntax</a> when making attachment decisions. How much do they know about <a href=https://en.wikipedia.org/wiki/Morphology_(biology)>morphology</a>? We investigate how well they handle <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphological case</a>, which is important for <a href=https://en.wikipedia.org/wiki/Parsing>parsing</a>. Our experiments on <a href=https://en.wikipedia.org/wiki/Czech_language>Czech</a>, <a href=https://en.wikipedia.org/wiki/German_language>German</a> and <a href=https://en.wikipedia.org/wiki/Russian_language>Russian</a> suggest that adding explicit morphological caseeither oracle or predictedimproves neural dependency parsing, indicating that the learned representations in these models do not fully encode the morphological knowledge that they need, and can still benefit from targeted forms of explicit linguistic modeling.</div></div><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K17-3010.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K17-3010 data-toggle=collapse aria-expanded=false aria-controls=abstract-K17-3010 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/K17-3010/>UParse : the Edinburgh system for the CoNLL 2017 UD shared task<span class=acl-fixed-case>UP</span>arse: the <span class=acl-fixed-case>E</span>dinburgh system for the <span class=acl-fixed-case>C</span>o<span class=acl-fixed-case>NLL</span> 2017 <span class=acl-fixed-case>UD</span> shared task</a></strong><br><a href=/people/c/clara-vania/>Clara Vania</a>
|
<a href=/people/x/xingxing-zhang/>Xingxing Zhang</a>
|
<a href=/people/a/adam-lopez/>Adam Lopez</a><br><a href=/volumes/K17-3/ class=text-muted>Proceedings of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K17-3010><div class="card-body p-3 small">This paper presents our submissions for the CoNLL 2017 UD Shared Task. Our <a href=https://en.wikipedia.org/wiki/Parsing>parser</a>, called UParse, is based on a neural network graph-based dependency parser. The <a href=https://en.wikipedia.org/wiki/Parsing>parser</a> uses <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> from a bidirectional LSTM to to produce a distribution over possible heads for each word in the sentence. To allow <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a> for low-resource treebanks and surprise languages, we train several multilingual models for related languages, grouped by their genus and language families. Out of 33 participants, our system achieves rank 9th in the main results, with 75.49 UAS and 68.87 LAS F-1 scores (average across 81 treebanks).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1184.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1184 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1184 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P17-1184.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-1184/>From Characters to Words to in Between : Do We Capture <a href=https://en.wikipedia.org/wiki/Morphology_(biology)>Morphology</a>?</a></strong><br><a href=/people/c/clara-vania/>Clara Vania</a>
|
<a href=/people/a/adam-lopez/>Adam Lopez</a><br><a href=/volumes/P17-1/ class=text-muted>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1184><div class="card-body p-3 small">Words can be represented by composing the representations of subword units such as word segments, <a href=https://en.wikipedia.org/wiki/Character_(symbol)>characters</a>, and/or character n-grams. While such <a href=https://en.wikipedia.org/wiki/Depiction>representations</a> are effective and may capture the morphological regularities of words, they have not been systematically compared, and it is not understood how they interact with different <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphological typologies</a>. On a language modeling task, we present experiments that systematically vary (1) the basic unit of representation, (2) the composition of these representations, and (3) the <a href=https://en.wikipedia.org/wiki/Morphological_typology>morphological typology</a> of the language modeled. Our results extend previous findings that character representations are effective across typologies, and we find that a previously unstudied combination of character trigram representations composed with bi-LSTMs outperforms most others. But we also find room for improvement : none of the character-level models match the predictive accuracy of a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> with access to true morphological analyses, even when learned from an order of magnitude more data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2019.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2019 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2019 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234957682 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P17-2019" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-2019/>A Generative Parser with a Discriminative Recognition Algorithm</a></strong><br><a href=/people/j/jianpeng-cheng/>Jianpeng Cheng</a>
|
<a href=/people/a/adam-lopez/>Adam Lopez</a>
|
<a href=/people/m/mirella-lapata/>Mirella Lapata</a><br><a href=/volumes/P17-2/ class=text-muted>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2019><div class="card-body p-3 small">Generative models defining joint distributions over parse trees and sentences are useful for parsing and language modeling, but impose restrictions on the scope of <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> and are often outperformed by discriminative models. We propose a framework for parsing and language modeling which marries a <a href=https://en.wikipedia.org/wiki/Generative_model>generative model</a> with a discriminative recognition model in an encoder-decoder setting. We provide interpretations of the framework based on <a href=https://en.wikipedia.org/wiki/Expectation_maximization>expectation maximization</a> and <a href=https://en.wikipedia.org/wiki/Variational_inference>variational inference</a>, and show that it enables parsing and language modeling within a single implementation. On the English Penn Treen-bank, our framework obtains competitive performance on constituency parsing while matching the state-of-the-art single-model language modeling score.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1804.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1804 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1804 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1804/>Universal Dependencies to <a href=https://en.wikipedia.org/wiki/Logical_form>Logical Form</a> with Negation Scope<span class=acl-fixed-case>U</span>niversal <span class=acl-fixed-case>D</span>ependencies to Logical Form with Negation Scope</a></strong><br><a href=/people/f/federico-fancellu/>Federico Fancellu</a>
|
<a href=/people/s/siva-reddy/>Siva Reddy</a>
|
<a href=/people/a/adam-lopez/>Adam Lopez</a>
|
<a href=/people/b/bonnie-webber/>Bonnie Webber</a><br><a href=/volumes/W17-18/ class=text-muted>Proceedings of the Workshop Computational Semantics Beyond Events and Roles</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1804><div class="card-body p-3 small">Many language technology applications would benefit from the ability to represent <a href=https://en.wikipedia.org/wiki/Affirmation_and_negation>negation</a> and its scope on top of widely-used linguistic resources. In this paper, we investigate the possibility of obtaining a <a href=https://en.wikipedia.org/wiki/First-order_logic>first-order logic representation</a> with <a href=https://en.wikipedia.org/wiki/Scope_(computer_science)>negation scope</a> marked using Universal Dependencies. To do so, we enhance UDepLambda, a <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> that converts <a href=https://en.wikipedia.org/wiki/Dependency_graph>dependency graphs</a> to <a href=https://en.wikipedia.org/wiki/Logical_form>logical forms</a>. The resulting UDepLambda is able to handle phenomena related to scope by means of an higher-order type theory, relevant not only to <a href=https://en.wikipedia.org/wiki/Negation>negation</a> but also to <a href=https://en.wikipedia.org/wiki/Universal_quantification>universal quantification</a> and other complex semantic phenomena. The initial conversion we did for <a href=https://en.wikipedia.org/wiki/English_language>English</a> is promising, in that one can represent the scope of negation also in the presence of more complex phenomena such as <a href=https://en.wikipedia.org/wiki/Universal_quantification>universal quantifiers</a>.<i>Universal Dependencies</i>. To do so, we enhance <i>UDepLambda</i>, a framework that converts dependency graphs to logical forms. The resulting <i>UDepLambda<tex-math>\\lnot</tex-math>\n </i>\n\nis able to handle phenomena related to scope by means of an higher-order type theory, relevant not only to negation but also to universal quantification and other complex semantic phenomena. The initial conversion we did for English is promising, in that one can represent the scope of negation also in the presence of more complex phenomena such as universal quantifiers.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4607.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4607 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4607 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4607/>Spoken Term Discovery for <a href=https://en.wikipedia.org/wiki/Language_documentation>Language Documentation</a> using Translations</a></strong><br><a href=/people/a/antonios-anastasopoulos/>Antonios Anastasopoulos</a>
|
<a href=/people/s/sameer-bansal/>Sameer Bansal</a>
|
<a href=/people/d/david-chiang/>David Chiang</a>
|
<a href=/people/s/sharon-goldwater/>Sharon Goldwater</a>
|
<a href=/people/a/adam-lopez/>Adam Lopez</a><br><a href=/volumes/W17-46/ class=text-muted>Proceedings of the Workshop on Speech-Centric Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4607><div class="card-body p-3 small">Vast amounts of speech data collected for <a href=https://en.wikipedia.org/wiki/Language_documentation>language documentation</a> and research remain untranscribed and unsearchable, but often a small amount of speech may have text translations available. We present a <a href=https://en.wikipedia.org/wiki/Methodology>method</a> for partially labeling additional speech with translations in this <a href=https://en.wikipedia.org/wiki/Scenario>scenario</a>. We modify an unsupervised speech-to-translation alignment model and obtain prototype speech segments that match the translation words, which are in turn used to discover terms in the unlabelled data. We evaluate our method on a Spanish-English speech translation corpus and on two corpora of endangered languages, <a href=https://en.wikipedia.org/wiki/Arapaho_language>Arapaho</a> and Ainu, demonstrating its appropriateness and applicability in an actual very-low-resource scenario.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S17-1024.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S17-1024 data-toggle=collapse aria-expanded=false aria-controls=abstract-S17-1024 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/S17-1024/>Parsing Graphs with Regular Graph Grammars</a></strong><br><a href=/people/s/sorcha-gilroy/>Sorcha Gilroy</a>
|
<a href=/people/a/adam-lopez/>Adam Lopez</a>
|
<a href=/people/s/sebastian-maneth/>Sebastian Maneth</a><br><a href=/volumes/S17-1/ class=text-muted>Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (*SEM 2017)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S17-1024><div class="card-body p-3 small">Recently, several <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> have become available which represent <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language phenomena</a> as <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graphs</a>. Hyperedge Replacement Languages (HRL) have been the focus of much attention as a formalism to represent the <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graphs</a> in these datasets. Chiang et al. (2013) prove that HRL graphs can be parsed in <a href=https://en.wikipedia.org/wiki/Time_complexity>polynomial time</a> with respect to the size of the input graph. We believe that HRL are more expressive than is necessary to represent semantic graphs and we propose the use of Regular Graph Languages (RGL ; Courcelle 1991), which is a subfamily of HRL, as a possible alternative. We provide a top-down parsing algorithm for RGL that runs in time linear in the size of the input graph.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2010.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2010 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2010 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2010/>Detecting negation scope is easy, except when it is n’t</a></strong><br><a href=/people/f/federico-fancellu/>Federico Fancellu</a>
|
<a href=/people/a/adam-lopez/>Adam Lopez</a>
|
<a href=/people/b/bonnie-webber/>Bonnie Webber</a>
|
<a href=/people/h/hangfeng-he/>Hangfeng He</a><br><a href=/volumes/E17-2/ class=text-muted>Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2010><div class="card-body p-3 small">Several <a href=https://en.wikipedia.org/wiki/Text_corpus>corpora</a> have been annotated with negation scopethe set of words whose meaning is negated by a cue like the word notleading to the development of <a href=https://en.wikipedia.org/wiki/Classifier_(linguistics)>classifiers</a> that detect negation scope with high accuracy. We show that for nearly all of these corpora, this high accuracy can be attributed to a single fact : they frequently annotate negation scope as a single span of text delimited by <a href=https://en.wikipedia.org/wiki/Punctuation>punctuation</a>. For negation scopes not of this form, detection accuracy is low and under-sampling the easy training examples does not substantially improve <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>. We demonstrate that this is partly an artifact of annotation guidelines, and we argue that future negation scope annotation efforts should focus on these more difficult cases.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Adam+Lopez" title="Search for 'Adam Lopez' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/c/clara-vania/ class=align-middle>Clara Vania</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/people/s/sharon-goldwater/ class=align-middle>Sharon Goldwater</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/k/kate-mccurdy/ class=align-middle>Kate McCurdy</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/f/federico-fancellu/ class=align-middle>Federico Fancellu</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/b/bonnie-webber/ class=align-middle>Bonnie Webber</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/d/david-chiang/ class=align-middle>David Chiang</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/s/sorcha-gilroy/ class=align-middle>Sorcha Gilroy</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/s/seraphina-goldfarb-tarrant/ class=align-middle>Seraphina Goldfarb-Tarrant</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/rebecca-marchant/ class=align-middle>Rebecca Marchant</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/ricardo-munoz-sanchez/ class=align-middle>Ricardo Muñoz Sánchez</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/mugdha-pandya/ class=align-middle>Mugdha Pandya</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/x/xingxing-zhang/ class=align-middle>Xingxing Zhang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jianpeng-cheng/ class=align-middle>Jianpeng Cheng</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/mirella-lapata/ class=align-middle>Mirella Lapata</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/siva-reddy/ class=align-middle>Siva Reddy</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/antonios-anastasopoulos/ class=align-middle>Antonios Anastasopoulos</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/sameer-bansal/ class=align-middle>Sameer Bansal</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/andreas-grivas/ class=align-middle>Andreas Grivas</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/sebastian-maneth/ class=align-middle>Sebastian Maneth</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/f/frank-drewes/ class=align-middle>Frank Drewes</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/daniel-gildea/ class=align-middle>Daniel Gildea</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/g/giorgio-satta/ class=align-middle>Giorgio Satta</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/n/naomi-saphra/ class=align-middle>Naomi Saphra</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/j-a-meaney/ class=align-middle>J. A. Meaney</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/steven-wilson/ class=align-middle>Steven Wilson</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/luis-chiruzzo/ class=align-middle>Luis Chiruzzo</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/w/walid-magdy/ class=align-middle>Walid Magdy</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/hangfeng-he/ class=align-middle>Hangfeng He</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/acl/ class=align-middle>ACL</a><span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/venues/ws/ class=align-middle>WS</a><span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/venues/emnlp/ class=align-middle>EMNLP</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/semeval/ class=align-middle>SemEval</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/conll/ class=align-middle>CoNLL</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-venues aria-expanded=false aria-controls=more-venues>show all...</li><div class="collapse border-top" id=more-venues><li class=list-group-item><a href=/venues/sigmorphon/ class=align-middle>SIGMORPHON</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/cl/ class=align-middle>CL</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/eacl/ class=align-middle>EACL</a><span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>