<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Abbas Ghaddar - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Abbas</span> <span class=font-weight-bold>Ghaddar</span></h2><hr><div class=row><div class=col-lg-9><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.526.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--526 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.526 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.526/>Towards Zero-Shot Knowledge Distillation for <a href=https://en.wikipedia.org/wiki/Natural_language_processing>Natural Language Processing</a></a></strong><br><a href=/people/a/ahmad-rashid/>Ahmad Rashid</a>
|
<a href=/people/v/vasileios-lioutas/>Vasileios Lioutas</a>
|
<a href=/people/a/abbas-ghaddar/>Abbas Ghaddar</a>
|
<a href=/people/m/mehdi-rezagholizadeh/>Mehdi Rezagholizadeh</a><br><a href=/volumes/2021.emnlp-main/ class=text-muted>Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--526><div class="card-body p-3 small">Knowledge distillation (KD) is a common <a href=https://en.wikipedia.org/wiki/Knowledge_transfer>knowledge transfer algorithm</a> used for <a href=https://en.wikipedia.org/wiki/Data_compression>model compression</a> across a variety of deep learning based natural language processing (NLP) solutions. In its regular manifestations, KD requires access to the teacher&#8217;s training data for <a href=https://en.wikipedia.org/wiki/Knowledge_transfer>knowledge transfer</a> to the student network. However, privacy concerns, data regulations and proprietary reasons may prevent access to such <a href=https://en.wikipedia.org/wiki/Data_(computing)>data</a>. We present, to the best of our knowledge, the first work on Zero-shot Knowledge Distillation for <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a>, where the student learns from the much larger teacher without any task specific data. Our solution combines out-of-domain data and <a href=https://en.wikipedia.org/wiki/Adversarial_system>adversarial training</a> to learn the teacher&#8217;s output distribution. We investigate six tasks from the GLUE benchmark and demonstrate that we can achieve between 75 % and 92 % of the teacher&#8217;s classification score (accuracy or F1) while compressing the model 30 times.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.603.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--603 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.603 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.603/>Universal-KD : Attention-based Output-Grounded Intermediate Layer Knowledge Distillation<span class=acl-fixed-case>KD</span>: Attention-based Output-Grounded Intermediate Layer Knowledge Distillation</a></strong><br><a href=/people/y/yimeng-wu/>Yimeng Wu</a>
|
<a href=/people/m/mehdi-rezagholizadeh/>Mehdi Rezagholizadeh</a>
|
<a href=/people/a/abbas-ghaddar/>Abbas Ghaddar</a>
|
<a href=/people/m/md-akmal-haidar/>Md Akmal Haidar</a>
|
<a href=/people/a/ali-ghodsi/>Ali Ghodsi</a><br><a href=/volumes/2021.emnlp-main/ class=text-muted>Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--603><div class="card-body p-3 small">Intermediate layer matching is shown as an effective approach for improving knowledge distillation (KD). However, this technique applies <a href=https://en.wikipedia.org/wiki/Matching_(graph_theory)>matching</a> in the hidden spaces of two different <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>networks</a> (i.e. student and teacher), which lacks clear interpretability. Moreover, intermediate layer KD can not easily deal with other problems such as layer mapping search and architecture mismatch (i.e. it requires the teacher and student to be of the same model type). To tackle the aforementioned problems all together, we propose Universal-KD to match intermediate layers of the teacher and the student in the output space (by adding pseudo classifiers on intermediate layers) via the attention-based layer projection. By doing this, our unified approach has three merits : (i) it can be flexibly combined with current intermediate layer distillation techniques to improve their results (ii) the pseudo classifiers of the teacher can be deployed instead of extra expensive teacher assistant networks to address the capacity gap problem in KD which is a common issue when the gap between the size of the teacher and student networks becomes too large ; (iii) it can be used in cross-architecture intermediate layer KD. We did comprehensive experiments in distilling BERT-base into BERT-4, RoBERTa-large into DistilRoBERTa and BERT-base into CNN and LSTM-based models. Results on the GLUE tasks show that our approach is able to outperform other KD techniques.</div></div><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.442.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--442 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.442 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.lrec-1.442" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.442/>SEDAR : a Large Scale French-English Financial Domain Parallel Corpus<span class=acl-fixed-case>SEDAR</span>: a Large Scale <span class=acl-fixed-case>F</span>rench-<span class=acl-fixed-case>E</span>nglish Financial Domain Parallel Corpus</a></strong><br><a href=/people/a/abbas-ghaddar/>Abbas Ghaddar</a>
|
<a href=/people/p/philippe-langlais/>Phillippe Langlais</a><br><a href=/volumes/2020.lrec-1/ class=text-muted>Proceedings of the 12th Language Resources and Evaluation Conference</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--442><div class="card-body p-3 small">This paper describes the acquisition, preprocessing and characteristics of <a href=https://en.wikipedia.org/wiki/SEDAR>SEDAR</a>, a large scale English-French parallel corpus for the financial domain. Our extensive experiments on <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> show that <a href=https://en.wikipedia.org/wiki/SEDAR>SEDAR</a> is essential to obtain good performance on <a href=https://en.wikipedia.org/wiki/Finance>finance</a>. We observe a large gain in the performance of <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation systems</a> trained on <a href=https://en.wikipedia.org/wiki/SEDAR>SEDAR</a> when tested on finance, which makes <a href=https://en.wikipedia.org/wiki/SEDAR>SEDAR</a> suitable to study domain adaptation for <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a>. The first release of the <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> comprises 8.6 million high quality sentence pairs that are publicly available for research at https://github.com/autorite/sedar-bitext.</div></div><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5513.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5513 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5513 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5513/>Contextualized Word Representations from Distant Supervision with and for NER<span class=acl-fixed-case>NER</span></a></strong><br><a href=/people/a/abbas-ghaddar/>Abbas Ghaddar</a>
|
<a href=/people/p/philippe-langlais/>Phillippe Langlais</a><br><a href=/volumes/D19-55/ class=text-muted>Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT 2019)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5513><div class="card-body p-3 small">We describe a special type of deep contextualized word representation that is learned from distant supervision annotations and dedicated to <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>named entity recognition</a>. Our extensive experiments on 7 <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> show systematic gains across all domains over strong baselines, and demonstrate that our representation is complementary to previously proposed <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a>. We report new state-of-the-art results on CONLL and ONTONOTES datasets.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Abbas+Ghaddar" title="Search for 'Abbas Ghaddar' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/m/mehdi-rezagholizadeh/ class=align-middle>Mehdi Rezagholizadeh</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/p/philippe-langlais/ class=align-middle>Philippe Langlais</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/a/ahmad-rashid/ class=align-middle>Ahmad Rashid</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/v/vasileios-lioutas/ class=align-middle>Vasileios Lioutas</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yimeng-wu/ class=align-middle>Yimeng Wu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/m/md-akmal-haidar/ class=align-middle>Md. Akmal Haidar</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/ali-ghodsi/ class=align-middle>Ali Ghodsi</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/emnlp/ class=align-middle>EMNLP</a><span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/venues/lrec/ class=align-middle>LREC</a><span class="badge badge-secondary align-middle ml-2">1</span></li></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>