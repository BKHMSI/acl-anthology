<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Rahul Aralikatte - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Rahul</span> <span class=font-weight-bold>Aralikatte</span></h2><hr><div class=row><div class=col-lg-9><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wat-1.22.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wat-1--22 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wat-1.22 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wat-1.22/>Itihasa : A <a href=https://en.wikipedia.org/wiki/Text_corpus>large-scale corpus</a> for Sanskrit to English translation<span class=acl-fixed-case>S</span>anskrit to <span class=acl-fixed-case>E</span>nglish translation</a></strong><br><a href=/people/r/rahul-aralikatte/>Rahul Aralikatte</a>
|
<a href=/people/m/miryam-de-lhoneux/>Miryam de Lhoneux</a>
|
<a href=/people/a/anoop-kunchukuttan/>Anoop Kunchukuttan</a>
|
<a href=/people/a/anders-sogaard/>Anders Søgaard</a><br><a href=/volumes/2021.wat-1/ class=text-muted>Proceedings of the 8th Workshop on Asian Translation (WAT2021)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wat-1--22><div class="card-body p-3 small">This work introduces <a href=https://en.wikipedia.org/wiki/Itihasa>Itihasa</a>, a large-scale translation dataset containing 93,000 pairs of Sanskrit shlokas and their English translations. The <a href=https://en.wikipedia.org/wiki/Shloka>shlokas</a> are extracted from two <a href=https://en.wikipedia.org/wiki/Indian_epic_poetry>Indian epics</a> viz., The <a href=https://en.wikipedia.org/wiki/Ramayana>Ramayana</a> and The <a href=https://en.wikipedia.org/wiki/Mahabharata>Mahabharata</a>. We first describe the motivation behind the curation of such a <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> and follow up with empirical analysis to bring out its nuances. We then benchmark the performance of standard translation models on this <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> and show that even state-of-the-art transformer architectures perform poorly, emphasizing the complexity of the dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wat-1.24.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wat-1--24 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wat-1.24 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wat-1.24/>How far can we get with one GPU in 100 hours? CoAStaL at MultiIndicMT Shared Task<span class=acl-fixed-case>GPU</span> in 100 hours? <span class=acl-fixed-case>C</span>o<span class=acl-fixed-case>AS</span>ta<span class=acl-fixed-case>L</span> at <span class=acl-fixed-case>M</span>ulti<span class=acl-fixed-case>I</span>ndic<span class=acl-fixed-case>MT</span> Shared Task</a></strong><br><a href=/people/r/rahul-aralikatte/>Rahul Aralikatte</a>
|
<a href=/people/h/hector-ricardo-murrieta-bello/>Héctor Ricardo Murrieta Bello</a>
|
<a href=/people/m/miryam-de-lhoneux/>Miryam de Lhoneux</a>
|
<a href=/people/d/daniel-hershcovich/>Daniel Hershcovich</a>
|
<a href=/people/m/marcel-bollmann/>Marcel Bollmann</a>
|
<a href=/people/a/anders-sogaard/>Anders Søgaard</a><br><a href=/volumes/2021.wat-1/ class=text-muted>Proceedings of the 8th Workshop on Asian Translation (WAT2021)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wat-1--24><div class="card-body p-3 small">This work shows that competitive <a href=https://en.wikipedia.org/wiki/Translation_(geometry)>translation</a> results can be obtained in a constrained setting by incorporating the latest advances in memory and compute optimization. We train and evaluate large multilingual translation models using a single <a href=https://en.wikipedia.org/wiki/Graphics_processing_unit>GPU</a> for a maximum of 100 hours and get within 4-5 BLEU points of the top submission on the leaderboard. We also benchmark standard baselines on the PMI corpus and re-discover well-known shortcomings of translation systems and <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a>.</div></div><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.9.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--9 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.9 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.lrec-1.9" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.9/>Model-based Annotation of Coreference</a></strong><br><a href=/people/r/rahul-aralikatte/>Rahul Aralikatte</a>
|
<a href=/people/a/anders-sogaard/>Anders Søgaard</a><br><a href=/volumes/2020.lrec-1/ class=text-muted>Proceedings of the 12th Language Resources and Evaluation Conference</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--9><div class="card-body p-3 small">Humans do not make inferences over texts, but over models of what texts are about. When annotators are asked to annotate coreferent spans of text, it is therefore a somewhat unnatural task. This paper presents an alternative in which we preprocess documents, linking entities to a <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge base</a>, and turn the coreference annotation task in our case limited to <a href=https://en.wikipedia.org/wiki/Pronoun>pronouns</a> into an annotation task where annotators are asked to assign <a href=https://en.wikipedia.org/wiki/Pronoun>pronouns</a> to entities. Model-based annotation is shown to lead to faster <a href=https://en.wikipedia.org/wiki/Annotation>annotation</a> and higher <a href=https://en.wikipedia.org/wiki/Inter-annotator_agreement>inter-annotator agreement</a>, and we argue that it also opens up an alternative approach to <a href=https://en.wikipedia.org/wiki/Coreference_resolution>coreference resolution</a>. We present two new coreference benchmark datasets, for English Wikipedia and English teacher-student dialogues, and evaluate state-of-the-art coreference resolvers on them.</div></div><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1118.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1118 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1118 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1118" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1118/>Rewarding Coreference Resolvers for Being Consistent with World Knowledge</a></strong><br><a href=/people/r/rahul-aralikatte/>Rahul Aralikatte</a>
|
<a href=/people/h/heather-lent/>Heather Lent</a>
|
<a href=/people/a/ana-valeria-gonzalez/>Ana Valeria Gonzalez</a>
|
<a href=/people/d/daniel-herschcovich/>Daniel Herschcovich</a>
|
<a href=/people/c/chen-qiu/>Chen Qiu</a>
|
<a href=/people/a/anders-sandholm/>Anders Sandholm</a>
|
<a href=/people/m/michael-ringaard/>Michael Ringaard</a>
|
<a href=/people/a/anders-sogaard/>Anders Søgaard</a><br><a href=/volumes/D19-1/ class=text-muted>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1118><div class="card-body p-3 small">Unresolved coreference is a bottleneck for <a href=https://en.wikipedia.org/wiki/Relation_extraction>relation extraction</a>, and high-quality coreference resolvers may produce an output that makes it a lot easier to extract knowledge triples. We show how to improve coreference resolvers by forwarding their input to a relation extraction system and reward the resolvers for producing triples that are found in <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge bases</a>. Since relation extraction systems can rely on different forms of supervision and be biased in different ways, we obtain the best performance, improving over the state of the art, using multi-task reinforcement learning.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-6130.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-6130 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-6130 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-6130" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-6130/>X-WikiRE : A Large, Multilingual Resource for Relation Extraction as Machine Comprehension<span class=acl-fixed-case>X</span>-<span class=acl-fixed-case>W</span>iki<span class=acl-fixed-case>RE</span>: A Large, Multilingual Resource for Relation Extraction as Machine Comprehension</a></strong><br><a href=/people/m/mostafa-abdou/>Mostafa Abdou</a>
|
<a href=/people/c/cezar-sas/>Cezar Sas</a>
|
<a href=/people/r/rahul-aralikatte/>Rahul Aralikatte</a>
|
<a href=/people/i/isabelle-augenstein/>Isabelle Augenstein</a>
|
<a href=/people/a/anders-sogaard/>Anders Søgaard</a><br><a href=/volumes/D19-61/ class=text-muted>Proceedings of the 2nd Workshop on Deep Learning Approaches for Low-Resource NLP (DeepLo 2019)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-6130><div class="card-body p-3 small">Although the vast majority of knowledge bases (KBs) are heavily biased towards <a href=https://en.wikipedia.org/wiki/English_language>English</a>, Wikipedias do cover very different topics in different languages. Exploiting this, we introduce a new multilingual dataset (X-WikiRE), framing relation extraction as a multilingual machine reading problem. We show that by leveraging this resource it is possible to robustly transfer models cross-lingually and that multilingual support significantly improves (zero-shot) relation extraction, enabling the population of low-resourced KBs from their well-populated counterparts.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K19-1009.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K19-1009 data-toggle=collapse aria-expanded=false aria-controls=abstract-K19-1009 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/K19-1009.Supplementary_Material.pdf data-toggle=tooltip data-placement=top title="Supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=K19-1009" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/K19-1009/>Compositional Generalization in Image Captioning</a></strong><br><a href=/people/m/mitja-nikolaus/>Mitja Nikolaus</a>
|
<a href=/people/m/mostafa-abdou/>Mostafa Abdou</a>
|
<a href=/people/m/matthew-lamm/>Matthew Lamm</a>
|
<a href=/people/r/rahul-aralikatte/>Rahul Aralikatte</a>
|
<a href=/people/d/desmond-elliott/>Desmond Elliott</a><br><a href=/volumes/K19-1/ class=text-muted>Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K19-1009><div class="card-body p-3 small">Image captioning models are usually evaluated on their ability to describe a held-out set of images, not on their ability to generalize to unseen concepts. We study the problem of compositional generalization, which measures how well a <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> composes unseen combinations of concepts when describing <a href=https://en.wikipedia.org/wiki/Image>images</a>. State-of-the-art image captioning models show poor generalization performance on this task. We propose a multi-task model to address the poor performance, that combines caption generation and imagesentence ranking, and uses a decoding mechanism that re-ranks the captions according their similarity to the image. This <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> is substantially better at generalizing to unseen combinations of concepts compared to state-of-the-art captioning models.</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1530.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1530 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1530 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D18-1530.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1530/>Sanskrit Sandhi Splitting using seq2(seq)2<span class=acl-fixed-case>S</span>anskrit Sandhi Splitting using seq2(seq)2</a></strong><br><a href=/people/r/rahul-aralikatte/>Rahul Aralikatte</a>
|
<a href=/people/n/neelamadhav-gantayat/>Neelamadhav Gantayat</a>
|
<a href=/people/n/naveen-panwar/>Naveen Panwar</a>
|
<a href=/people/a/anush-sankaran/>Anush Sankaran</a>
|
<a href=/people/s/senthil-mani/>Senthil Mani</a><br><a href=/volumes/D18-1/ class=text-muted>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1530><div class="card-body p-3 small">In <a href=https://en.wikipedia.org/wiki/Sanskrit>Sanskrit</a>, small words (morphemes) are combined to form <a href=https://en.wikipedia.org/wiki/Compound_(linguistics)>compound words</a> through a process known as <a href=https://en.wikipedia.org/wiki/Sandhi>Sandhi</a>. Sandhi splitting is the process of splitting a given <a href=https://en.wikipedia.org/wiki/Compound_(linguistics)>compound word</a> into its constituent morphemes. Although <a href=https://en.wikipedia.org/wiki/Linguistic_prescription>rules</a> governing <a href=https://en.wikipedia.org/wiki/Word_splitting>word splitting</a> exists in the language, it is highly challenging to identify the location of the splits in a <a href=https://en.wikipedia.org/wiki/Compound_(linguistics)>compound word</a>. Though existing Sandhi splitting systems incorporate these pre-defined splitting rules, they have a low <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> as the same <a href=https://en.wikipedia.org/wiki/Compound_(linguistics)>compound word</a> might be broken down in multiple ways to provide syntactically correct splits. In this research, we propose a novel deep learning architecture called Double Decoder RNN (DD-RNN), which (i) predicts the location of the split(s) with 95 % <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>, and (ii) predicts the constituent words (learning the Sandhi splitting rules) with 79.5 % <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>, outperforming the <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-art</a> by 20 %. Additionally, we show the generalization capability of our deep learning model, by showing competitive results in the problem of Chinese word segmentation, as well.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1156.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1156 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1156 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1156.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-1156" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P18-1156/>DuoRC : Towards Complex Language Understanding with Paraphrased Reading Comprehension<span class=acl-fixed-case>D</span>uo<span class=acl-fixed-case>RC</span>: Towards Complex Language Understanding with Paraphrased Reading Comprehension</a></strong><br><a href=/people/a/amrita-saha/>Amrita Saha</a>
|
<a href=/people/r/rahul-aralikatte/>Rahul Aralikatte</a>
|
<a href=/people/m/mitesh-m-khapra/>Mitesh M. Khapra</a>
|
<a href=/people/k/karthik-sankaranarayanan/>Karthik Sankaranarayanan</a><br><a href=/volumes/P18-1/ class=text-muted>Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1156><div class="card-body p-3 small">We propose DuoRC, a novel dataset for Reading Comprehension (RC) that motivates several new challenges for neural approaches in <a href=https://en.wikipedia.org/wiki/Language_understanding>language understanding</a> beyond those offered by existing RC datasets. DuoRC contains 186,089 unique question-answer pairs created from a collection of 7680 pairs of movie plots where each pair in the <a href=https://en.wikipedia.org/wiki/Collection_(abstract_data_type)>collection</a> reflects two versions of the same movie-one from Wikipedia and the other from IMDb-written by two different authors. We asked crowdsourced workers to create questions from one version of the plot and a different set of workers to extract or synthesize answers from the other version. This unique characteristic of DuoRC where questions and answers are created from different versions of a document narrating the same underlying story, ensures by design, that there is very little lexical overlap between the questions created from one version and the segments containing the answer in the other version. Further, since the two versions have different levels of plot detail, narration style, <a href=https://en.wikipedia.org/wiki/Vocabulary>vocabulary</a>, etc., answering questions from the second version requires deeper <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>language understanding</a> and incorporating external background knowledge. Additionally, the narrative style of passages arising from movie plots (as opposed to typical descriptive passages in existing datasets) exhibits the need to perform complex reasoning over events across multiple sentences. Indeed, we observe that state-of-the-art neural RC models which have achieved near human performance on the SQuAD dataset, even when coupled with traditional NLP techniques to address the challenges presented in DuoRC exhibit very poor performance (F1 score of 37.42 % on DuoRC v / s 86 % on SQuAD dataset).</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Rahul+Aralikatte" title="Search for 'Rahul Aralikatte' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/a/anders-sogaard/ class=align-middle>Anders Søgaard</a>
<span class="badge badge-secondary align-middle ml-2">5</span></li><li class=list-group-item><a href=/people/m/mostafa-abdou/ class=align-middle>Mostafa Abdou</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/m/miryam-de-lhoneux/ class=align-middle>Miryam de Lhoneux</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/n/neelamadhav-gantayat/ class=align-middle>Neelamadhav Gantayat</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/n/naveen-panwar/ class=align-middle>Naveen Panwar</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/a/anush-sankaran/ class=align-middle>Anush Sankaran</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/senthil-mani/ class=align-middle>Senthil Mani</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/heather-lent/ class=align-middle>Heather Lent</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/ana-valeria-gonzalez/ class=align-middle>Ana Valeria González</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/daniel-herschcovich/ class=align-middle>Daniel Herschcovich</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/chen-qiu/ class=align-middle>Chen Qiu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/anders-sandholm/ class=align-middle>Anders Sandholm</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/michael-ringaard/ class=align-middle>Michael Ringaard</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/cezar-sas/ class=align-middle>Cezar Sas</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/i/isabelle-augenstein/ class=align-middle>Isabelle Augenstein</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/anoop-kunchukuttan/ class=align-middle>Anoop Kunchukuttan</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/hector-ricardo-murrieta-bello/ class=align-middle>Héctor Ricardo Murrieta Bello</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/daniel-hershcovich/ class=align-middle>Daniel Hershcovich</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/marcel-bollmann/ class=align-middle>Marcel Bollmann</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/amrita-saha/ class=align-middle>Amrita Saha</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/mitesh-m-khapra/ class=align-middle>Mitesh M. Khapra</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/karthik-sankaranarayanan/ class=align-middle>Karthik Sankaranarayanan</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/mitja-nikolaus/ class=align-middle>Mitja Nikolaus</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/matthew-lamm/ class=align-middle>Matthew Lamm</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/desmond-elliott/ class=align-middle>Desmond Elliott</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/emnlp/ class=align-middle>EMNLP</a><span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/venues/wat/ class=align-middle>WAT</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/lrec/ class=align-middle>LREC</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/acl/ class=align-middle>ACL</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/conll/ class=align-middle>CoNLL</a><span class="badge badge-secondary align-middle ml-2">1</span></li></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>