<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Raquel Fernández - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Raquel</span> <span class=font-weight-bold>Fernández</span></h2><p class="font-weight-light text-muted"><span class=font-italic>Also published as:</span>
Raquel <span class=font-weight-normal>Fernandez</span></p><hr><div class=row><div class=col-lg-9><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.repl4nlp-1.16.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--repl4nlp-1--16 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.repl4nlp-1.16 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.repl4nlp-1.16" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.repl4nlp-1.16/>Probing Cross-Modal Representations in Multi-Step Relational Reasoning</a></strong><br><a href=/people/i/iuliia-parfenova/>Iuliia Parfenova</a>
|
<a href=/people/d/desmond-elliott/>Desmond Elliott</a>
|
<a href=/people/r/raquel-fernandez/>Raquel Fernández</a>
|
<a href=/people/s/sandro-pezzelle/>Sandro Pezzelle</a><br><a href=/volumes/2021.repl4nlp-1/ class=text-muted>Proceedings of the 6th Workshop on Representation Learning for NLP (RepL4NLP-2021)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--repl4nlp-1--16><div class="card-body p-3 small">We investigate the <a href=https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning>representations</a> learned by vision and language models in tasks that require relational reasoning. Focusing on the problem of assessing the relative size of objects in abstract visual contexts, we analyse both one-step and two-step reasoning. For the latter, we construct a new <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> of three-image scenes and define a task that requires <a href=https://en.wikipedia.org/wiki/Reason>reasoning</a> at the level of the individual images and across images in a scene. We probe the learned model representations using diagnostic classifiers. Our experiments show that pretrained multimodal transformer-based architectures can perform higher-level relational reasoning, and are able to learn representations for novel tasks and data that are very different from what was seen in pretraining.</div></div><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.acl-main.365.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--acl-main--365 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.acl-main.365 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://slideslive.com/38929048 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.acl-main.365" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.acl-main.365/>Analysing Lexical Semantic Change with Contextualised Word Representations</a></strong><br><a href=/people/m/mario-giulianelli/>Mario Giulianelli</a>
|
<a href=/people/m/marco-del-tredici/>Marco Del Tredici</a>
|
<a href=/people/r/raquel-fernandez/>Raquel Fernández</a><br><a href=/volumes/2020.acl-main/ class=text-muted>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--acl-main--365><div class="card-body p-3 small">This paper presents the first unsupervised approach to lexical semantic change that makes use of contextualised word representations. We propose a novel method that exploits the BERT neural language model to obtain <a href=https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning>representations of word usages</a>, clusters these <a href=https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning>representations</a> into usage types, and measures change along time with three proposed metrics. We create a new evaluation dataset and show that the model representations and the detected semantic shifts are positively correlated with human judgements. Our extensive qualitative analysis demonstrates that our method captures a variety of synchronic and diachronic linguistic phenomena. We expect our work to inspire further research in this direction.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.conll-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.conll-1.0/>Proceedings of the 24th Conference on Computational Natural Language Learning</a></strong><br><a href=/people/r/raquel-fernandez/>Raquel Fernández</a>
|
<a href=/people/t/tal-linzen/>Tal Linzen</a><br><a href=/volumes/2020.conll-1/ class=text-muted>Proceedings of the 24th Conference on Computational Natural Language Learning</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.477.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--477 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.477 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.477/>Words are the Window to the Soul : Language-based User Representations for Fake News Detection</a></strong><br><a href=/people/m/marco-del-tredici/>Marco Del Tredici</a>
|
<a href=/people/r/raquel-fernandez/>Raquel Fernández</a><br><a href=/volumes/2020.coling-main/ class=text-muted>Proceedings of the 28th International Conference on Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--477><div class="card-body p-3 small">Cognitive and social traits of individuals are reflected in <a href=https://en.wikipedia.org/wiki/Usage_(language)>language use</a>. Moreover, individuals who are prone to spread fake news online often share common traits. Building on these ideas, we introduce a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> that creates representations of individuals on <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> based only on the language they produce, and use them to detect <a href=https://en.wikipedia.org/wiki/Fake_news>fake news</a>. We show that language-based user representations are beneficial for this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. We also present an extended analysis of the language of fake news spreaders, showing that its main features are mostly domain independent and consistent across two English datasets. Finally, we exploit the relation between language use and connections in the <a href=https://en.wikipedia.org/wiki/Social_graph>social graph</a> to assess the presence of the Echo Chamber effect in our data.</div></div><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1477.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1477 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1477 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1477.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D19-1477/>You Shall Know a User by the Company It Keeps : Dynamic Representations for Social Media Users in NLP<span class=acl-fixed-case>NLP</span></a></strong><br><a href=/people/m/marco-del-tredici/>Marco Del Tredici</a>
|
<a href=/people/d/diego-marcheggiani/>Diego Marcheggiani</a>
|
<a href=/people/s/sabine-schulte-im-walde/>Sabine Schulte im Walde</a>
|
<a href=/people/r/raquel-fernandez/>Raquel Fernández</a><br><a href=/volumes/D19-1/ class=text-muted>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1477><div class="card-body p-3 small">Information about individuals can help to better understand what they say, particularly in <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> where texts are short. Current approaches to modelling social media users pay attention to their social connections, but exploit this information in a static way, treating all connections uniformly. This ignores the fact, well known in <a href=https://en.wikipedia.org/wiki/Sociolinguistics>sociolinguistics</a>, that an individual may be part of several communities which are not equally relevant in all communicative situations. We present a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> based on Graph Attention Networks that captures this observation. It dynamically explores the <a href=https://en.wikipedia.org/wiki/Social_graph>social graph</a> of a user, computes a user representation given the most relevant connections for a target task, and combines it with linguistic information to make a prediction. We apply our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> to three different tasks, evaluate it against alternative <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>, and analyse the results extensively, showing that it significantly outperforms other current methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-6403.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-6403 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-6403 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-6403/>Big Generalizations with Small Data : Exploring the Role of Training Samples in Learning Adjectives of Size</a></strong><br><a href=/people/s/sandro-pezzelle/>Sandro Pezzelle</a>
|
<a href=/people/r/raquel-fernandez/>Raquel Fernández</a><br><a href=/volumes/D19-64/ class=text-muted>Proceedings of the Beyond Vision and LANguage: inTEgrating Real-world kNowledge (LANTERN)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-6403><div class="card-body p-3 small">In this paper, we experiment with a recently proposed visual reasoning task dealing with quantities modeling the multimodal, contextually-dependent meaning of size adjectives (&#8216;big&#8217;, &#8216;small&#8217;) and explore the impact of varying the training data on the learning behavior of a state-of-art system. In previous work, <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> have been shown to fail in generalizing to unseen adjective-noun combinations. Here, we investigate whether, and to what extent, seeing some of these cases during training helps a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> understand the rule subtending the task, i.e., that being big implies being not small, and vice versa. We show that relatively few examples are enough to understand this relationship, and that developing a specific, mutually exclusive representation of size adjectives is beneficial to the task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-1800.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-1800/>Proceedings of the Second Workshop on Shortcomings in Vision and Language</a></strong><br><a href=/people/r/raffaella-bernardi/>Raffaella Bernardi</a>
|
<a href=/people/r/raquel-fernandez/>Raquel Fernandez</a>
|
<a href=/people/s/spandana-gella/>Spandana Gella</a>
|
<a href=/people/k/kushal-kafle/>Kushal Kafle</a>
|
<a href=/people/c/christopher-kanan/>Christopher Kanan</a>
|
<a href=/people/s/stefan-lee/>Stefan Lee</a>
|
<a href=/people/m/moin-nabi/>Moin Nabi</a><br><a href=/volumes/W19-18/ class=text-muted>Proceedings of the Second Workshop on Shortcomings in Vision and Language</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1210.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1210 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1210 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/354246126 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1210" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1210/>Short-Term Meaning Shift : A Distributional Exploration</a></strong><br><a href=/people/m/marco-del-tredici/>Marco Del Tredici</a>
|
<a href=/people/r/raquel-fernandez/>Raquel Fernández</a>
|
<a href=/people/g/gemma-boleda/>Gemma Boleda</a><br><a href=/volumes/N19-1/ class=text-muted>Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1210><div class="card-body p-3 small">We present the first exploration of meaning shift over short periods of time in <a href=https://en.wikipedia.org/wiki/Online_community>online communities</a> using distributional representations. We create a small annotated dataset and use it to assess the performance of a standard <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> for meaning shift detection on short-term meaning shift. We find that the model has problems distinguishing meaning shift from referential phenomena, and propose a measure of contextual variability to remedy this.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1350.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1350 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1350 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P19-1350.Supplementary.pdf data-toggle=tooltip data-placement=top title=Supplementary><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/384787273 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1350/>Psycholinguistics Meets Continual Learning : Measuring Catastrophic Forgetting in Visual Question Answering</a></strong><br><a href=/people/c/claudio-greco/>Claudio Greco</a>
|
<a href=/people/b/barbara-plank/>Barbara Plank</a>
|
<a href=/people/r/raquel-fernandez/>Raquel Fernández</a>
|
<a href=/people/r/raffaella-bernardi/>Raffaella Bernardi</a><br><a href=/volumes/P19-1/ class=text-muted>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1350><div class="card-body p-3 small">We study the issue of catastrophic forgetting in the context of neural multimodal approaches to Visual Question Answering (VQA). Motivated by evidence from psycholinguistics, we devise a set of linguistically-informed VQA tasks, which differ by the types of questions involved (Wh-questions and polar questions). We test what impact task difficulty has on continual learning, and whether the order in which a child acquires question types facilitates <a href=https://en.wikipedia.org/wiki/Computational_model>computational models</a>. Our results show that dramatic forgetting is at play and that task difficulty and <a href=https://en.wikipedia.org/wiki/Order_and_disorder>order</a> matter. Two well-known current continual learning methods mitigate the problem only to a limiting degree.</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1104.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1104 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1104 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1104/>Ask No More : Deciding when to guess in referential visual dialogue</a></strong><br><a href=/people/r/ravi-shekhar/>Ravi Shekhar</a>
|
<a href=/people/t/tim-baumgartner/>Tim Baumgärtner</a>
|
<a href=/people/a/aashish-venkatesh/>Aashish Venkatesh</a>
|
<a href=/people/e/elia-bruni/>Elia Bruni</a>
|
<a href=/people/r/raffaella-bernardi/>Raffaella Bernardi</a>
|
<a href=/people/r/raquel-fernandez/>Raquel Fernandez</a><br><a href=/volumes/C18-1/ class=text-muted>Proceedings of the 27th International Conference on Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1104><div class="card-body p-3 small">Our goal is to explore how the abilities brought in by a <a href=https://en.wikipedia.org/wiki/Dialogue_manager>dialogue manager</a> can be included in end-to-end visually grounded conversational agents. We make initial steps towards this general goal by augmenting a task-oriented visual dialogue model with a decision-making component that decides whether to ask a follow-up question to identify a target referent in an image, or to stop the conversation to make a guess. Our analyses show that adding a decision making component produces dialogues that are less repetitive and that include fewer unnecessary questions, thus potentially leading to more efficient and less unnatural interactions.</div></div><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5534.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5534 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5534 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5534/>Adversarial evaluation for open-domain dialogue generation</a></strong><br><a href=/people/e/elia-bruni/>Elia Bruni</a>
|
<a href=/people/r/raquel-fernandez/>Raquel Fernández</a><br><a href=/volumes/W17-55/ class=text-muted>Proceedings of the 18th Annual SIGdial Meeting on Discourse and Dialogue</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5534><div class="card-body p-3 small">We investigate the potential of adversarial evaluation methods for open-domain dialogue generation systems, comparing the performance of a discriminative agent to that of humans on the same task. Our results show that the task is hard, both for automated models and humans, but that a discriminative agent can learn patterns that lead to above-chance performance.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Raquel+Fern%C3%A1ndez" title="Search for 'Raquel Fernández' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/m/marco-del-tredici/ class=align-middle>Marco Del Tredici</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/people/r/raffaella-bernardi/ class=align-middle>Raffaella Bernardi</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/e/elia-bruni/ class=align-middle>Elia Bruni</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/s/sandro-pezzelle/ class=align-middle>Sandro Pezzelle</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/r/ravi-shekhar/ class=align-middle>Ravi Shekhar</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/t/tim-baumgartner/ class=align-middle>Tim Baumgärtner</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/aashish-venkatesh/ class=align-middle>Aashish Venkatesh</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/mario-giulianelli/ class=align-middle>Mario Giulianelli</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/i/iuliia-parfenova/ class=align-middle>Iuliia Parfenova</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/desmond-elliott/ class=align-middle>Desmond Elliott</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/tal-linzen/ class=align-middle>Tal Linzen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/diego-marcheggiani/ class=align-middle>Diego Marcheggiani</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/sabine-schulte-im-walde/ class=align-middle>Sabine Schulte im Walde</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/spandana-gella/ class=align-middle>Spandana Gella</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/kushal-kafle/ class=align-middle>Kushal Kafle</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/christopher-kanan/ class=align-middle>Christopher Kanan</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/stefan-lee/ class=align-middle>Stefan Lee</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/moin-nabi/ class=align-middle>Moin Nabi</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/g/gemma-boleda/ class=align-middle>Gemma Boleda</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/claudio-greco/ class=align-middle>Claudio Greco</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/b/barbara-plank/ class=align-middle>Barbara Plank</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/coling/ class=align-middle>COLING</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/acl/ class=align-middle>ACL</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/ws/ class=align-middle>WS</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/emnlp/ class=align-middle>EMNLP</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/repl4nlp/ class=align-middle>RepL4NLP</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-venues aria-expanded=false aria-controls=more-venues>show all...</li><div class="collapse border-top" id=more-venues><li class=list-group-item><a href=/venues/conll/ class=align-middle>CoNLL</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/naacl/ class=align-middle>NAACL</a><span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>