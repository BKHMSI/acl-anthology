<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Rebecca Sharp - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Rebecca</span> <span class=font-weight-bold>Sharp</span></h2><hr><div class=row><div class=col-lg-9><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wassa-1.26.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wassa-1--26 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wassa-1.26 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wassa-1.26/>Me, myself, and ire : Effects of automatic transcription quality on <a href=https://en.wikipedia.org/wiki/Emotion>emotion</a>, <a href=https://en.wikipedia.org/wiki/Sarcasm>sarcasm</a>, and personality detection</a></strong><br><a href=/people/j/john-culnan/>John Culnan</a>
|
<a href=/people/s/seongjin-park/>Seongjin Park</a>
|
<a href=/people/m/meghavarshini-krishnaswamy/>Meghavarshini Krishnaswamy</a>
|
<a href=/people/r/rebecca-sharp/>Rebecca Sharp</a><br><a href=/volumes/2021.wassa-1/ class=text-muted>Proceedings of the Eleventh Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wassa-1--26><div class="card-body p-3 small">In deployment, <a href=https://en.wikipedia.org/wiki/System>systems</a> that use <a href=https://en.wikipedia.org/wiki/Speech>speech</a> as input must make use of <a href=https://en.wikipedia.org/wiki/Transcription_(linguistics)>automated transcriptions</a>. Yet, typically when these <a href=https://en.wikipedia.org/wiki/System>systems</a> are evaluated, gold transcriptions are assumed. We explicitly examine the impact of transcription errors on the downstream performance of a multi-modal system on three related tasks from three datasets : <a href=https://en.wikipedia.org/wiki/Emotion>emotion</a>, <a href=https://en.wikipedia.org/wiki/Sarcasm>sarcasm</a>, and <a href=https://en.wikipedia.org/wiki/Personality_test>personality detection</a>. We include three separate transcription tools and show that while all automated transcriptions propagate errors that substantially impact downstream performance, the open-source tools fair worse than the paid tool, though not always straightforwardly, and word error rates do not correlate well with downstream performance. We further find that the inclusion of <a href=https://en.wikipedia.org/wiki/Audio_signal>audio features</a> partially mitigates <a href=https://en.wikipedia.org/wiki/Transcription_error>transcription errors</a>, but that a naive usage of a multi-task setup does not.</div></div><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.297.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--297 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.297 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.297/>An Unsupervised Method for Learning Representations of Multi-word Expressions for Semantic Classification</a></strong><br><a href=/people/r/robert-vacareanu/>Robert Vacareanu</a>
|
<a href=/people/m/marco-a-valenzuela-escarcega/>Marco A. Valenzuela-Escárcega</a>
|
<a href=/people/r/rebecca-sharp/>Rebecca Sharp</a>
|
<a href=/people/m/mihai-surdeanu/>Mihai Surdeanu</a><br><a href=/volumes/2020.coling-main/ class=text-muted>Proceedings of the 28th International Conference on Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--297><div class="card-body p-3 small">This paper explores an unsupervised approach to learning a compositional representation function for multi-word expressions (MWEs), and evaluates it on the Tratz dataset, which associates two-word expressions with the semantic relation between the compound constituents (e.g. the label employer is associated with the noun compound government agency) (Tratz, 2011). The <a href=https://en.wikipedia.org/wiki/Composition_function>composition function</a> is based on <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent neural networks</a>, and is trained using the Skip-Gram objective to predict the words in the context of MWEs. Thus our approach can naturally leverage large unlabeled text sources. Further, our method can make use of provided MWEs when available, but can also function as a completely <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised algorithm</a>, using MWE boundaries predicted by a single, domain-agnostic part-of-speech pattern. With pre-defined MWE boundaries, our method outperforms the previous state-of-the-art performance on the coarse-grained evaluation of the Tratz dataset (Tratz, 2011), with an <a href=https://en.wikipedia.org/wiki/F-number>F1 score</a> of 50.4 %. The unsupervised version of our method approaches the performance of the supervised one, and even outperforms it in some configurations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.850.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--850 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.850 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.850/>Towards the Necessity for Debiasing Natural Language Inference Datasets</a></strong><br><a href=/people/m/mithun-paul-panenghat/>Mithun Paul Panenghat</a>
|
<a href=/people/s/sandeep-suntwal/>Sandeep Suntwal</a>
|
<a href=/people/f/faiz-rafique/>Faiz Rafique</a>
|
<a href=/people/r/rebecca-sharp/>Rebecca Sharp</a>
|
<a href=/people/m/mihai-surdeanu/>Mihai Surdeanu</a><br><a href=/volumes/2020.lrec-1/ class=text-muted>Proceedings of the 12th Language Resources and Evaluation Conference</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--850><div class="card-body p-3 small">Modeling natural language inference is a challenging <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. With large annotated data sets available it has now become feasible to train complex neural network based inference methods which achieve state of the art performance. However, it has been shown that these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> also learn from the subtle biases inherent in these <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> (CITATION). In this work we explore two techniques for delexicalization that modify the <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> in such a way that we can control the importance that neural-network based methods place on <a href=https://en.wikipedia.org/wiki/Lexical_item>lexical entities</a>. We demonstrate that the proposed methods not only maintain the performance in-domain but also improve performance in some out-of-domain settings. For example, when using the delexicalized version of the FEVER dataset, the in-domain performance of a state of the art <a href=https://en.wikipedia.org/wiki/Neural_network>neural network method</a> dropped only by 1.12 % while its out-of-domain performance on the FNC dataset improved by 4.63 %. We release the delexicalized versions of three common datasets used in natural language inference. These datasets are delexicalized using two methods : one which replaces the lexical entities in an overlap-aware manner, and a second, which additionally incorporates semantic lifting of nouns and verbs to their WordNet hypernym synsets</div></div><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-1505.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-1505 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-1505 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-1505/>Semi-Supervised Teacher-Student Architecture for Relation Extraction</a></strong><br><a href=/people/f/fan-luo/>Fan Luo</a>
|
<a href=/people/a/ajay-nagesh/>Ajay Nagesh</a>
|
<a href=/people/r/rebecca-sharp/>Rebecca Sharp</a>
|
<a href=/people/m/mihai-surdeanu/>Mihai Surdeanu</a><br><a href=/volumes/W19-15/ class=text-muted>Proceedings of the Third Workshop on Structured Prediction for NLP</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-1505><div class="card-body p-3 small">Generating a large amount of training data for information extraction (IE) is either costly (if annotations are created manually), or runs the risk of introducing noisy instances (if distant supervision is used). On the other hand, semi-supervised learning (SSL) is a cost-efficient solution to combat lack of training data. In this paper, we adapt <a href=https://en.wikipedia.org/wiki/Mean_Teacher>Mean Teacher</a> (Tarvainen and Valpola, 2017), a denoising SSL framework to extract semantic relations between pairs of entities. We explore the sweet spot of amount of supervision required for good performance on this binary relation extraction task. Additionally, different syntax representations are incorporated into our <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> to enhance the learned representation of sentences. We evaluate our approach on the Google-IISc Distant Supervision (GDS) dataset, which removes test data noise present in all previous distance supervision datasets, which makes it a reliable evaluation benchmark (Jat et al., 2017). Our results show that the SSL Mean Teacher approach nears the performance of fully-supervised approaches even with only 10 % of the labeled corpus. Further, the syntax-aware model outperforms other syntax-free approaches across all levels of <a href=https://en.wikipedia.org/wiki/Supervisor>supervision</a>.</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S18-2021.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S18-2021 data-toggle=collapse aria-expanded=false aria-controls=abstract-S18-2021 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=S18-2021" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/S18-2021/>Deep Affix Features Improve Neural Named Entity Recognizers</a></strong><br><a href=/people/v/vikas-yadav/>Vikas Yadav</a>
|
<a href=/people/r/rebecca-sharp/>Rebecca Sharp</a>
|
<a href=/people/s/steven-bethard/>Steven Bethard</a><br><a href=/volumes/S18-2/ class=text-muted>Proceedings of the Seventh Joint Conference on Lexical and Computational Semantics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S18-2021><div class="card-body p-3 small">We propose a practical model for named entity recognition (NER) that combines word and character-level information with a specific learned representation of the prefixes and suffixes of the word. We apply this approach to multilingual and multi-domain NER and show that it achieves state of the art results on the CoNLL 2002 Spanish and Dutch and CoNLL 2003 German NER datasets, consistently achieving 1.5-2.3 percent over the state of the art without relying on any dictionary features. Additionally, we show improvement on SemEval 2013 task 9.1 DrugNER, achieving state of the art results on the MedLine dataset and the second best results overall (-1.3 % from state of the art). We also establish a new benchmark on the I2B2 2010 Clinical NER dataset with 84.70 <a href=https://en.wikipedia.org/wiki/F-score>F-score</a>.</div></div><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K17-1009.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K17-1009 data-toggle=collapse aria-expanded=false aria-controls=abstract-K17-1009 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/K17-1009/>Tell Me Why : Using Question Answering as Distant Supervision for Answer Justification</a></strong><br><a href=/people/r/rebecca-sharp/>Rebecca Sharp</a>
|
<a href=/people/m/mihai-surdeanu/>Mihai Surdeanu</a>
|
<a href=/people/p/peter-jansen/>Peter Jansen</a>
|
<a href=/people/m/marco-a-valenzuela-escarcega/>Marco A. Valenzuela-Escárcega</a>
|
<a href=/people/p/peter-clark/>Peter Clark</a>
|
<a href=/people/m/michael-hammond/>Michael Hammond</a><br><a href=/volumes/K17-1/ class=text-muted>Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K17-1009><div class="card-body p-3 small">For many applications of question answering (QA), being able to explain why a given <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> chose an answer is critical. However, the lack of labeled data for answer justifications makes learning this difficult and expensive. Here we propose an approach that uses answer ranking as distant supervision for learning how to select informative justifications, where justifications serve as inferential connections between the question and the correct answer while often containing little lexical overlap with either. We propose a neural network architecture for <a href=https://en.wikipedia.org/wiki/Quality_assurance>QA</a> that reranks answer justifications as an intermediate (and human-interpretable) step in answer selection. Our approach is informed by a set of <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> designed to combine both learned representations and explicit features to capture the connection between questions, answers, and answer justifications. We show that with this end-to-end approach we are able to significantly improve upon a strong IR baseline in both justification ranking (+9 % rated highly relevant) and answer selection (+6 % P@1).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/J17-2005.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-J17-2005 data-toggle=collapse aria-expanded=false aria-controls=abstract-J17-2005 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/J17-2005/>Framing QA as Building and Ranking Intersentence Answer Justifications<span class=acl-fixed-case>QA</span> as Building and Ranking Intersentence Answer Justifications</a></strong><br><a href=/people/p/peter-jansen/>Peter Jansen</a>
|
<a href=/people/r/rebecca-sharp/>Rebecca Sharp</a>
|
<a href=/people/m/mihai-surdeanu/>Mihai Surdeanu</a>
|
<a href=/people/p/peter-clark/>Peter Clark</a><br><a href=/volumes/J17-2/ class=text-muted>Computational Linguistics, Volume 43, Issue 2 - June 2017</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-J17-2005><div class="card-body p-3 small">We propose a question answering (QA) approach for standardized science exams that both identifies correct answers and produces compelling human-readable justifications for why those answers are correct. Our method first identifies the actual information needed in a question using psycholinguistic concreteness norms, then uses this information need to construct answer justifications by aggregating multiple sentences from different knowledge bases using syntactic and lexical information. We then jointly rank answers and their justifications using a reranking perceptron that treats <a href=https://en.wikipedia.org/wiki/Theory_of_justification>justification quality</a> as a <a href=https://en.wikipedia.org/wiki/Latent_variable>latent variable</a>. We evaluate our method on 1,000 multiple-choice questions from elementary school science exams, and empirically demonstrate that it performs better than several strong baselines, including neural network approaches. Our best configuration answers 44 % of the questions correctly, where the top justifications for 57 % of these correct answers contain a compelling human-readable justification that explains the inference required to arrive at the correct answer. We include a detailed characterization of the justification quality for both our method and a strong <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline</a>, and show that <a href=https://en.wikipedia.org/wiki/Information_aggregation>information aggregation</a> is key to addressing the information need in complex questions.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Rebecca+Sharp" title="Search for 'Rebecca Sharp' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/m/mihai-surdeanu/ class=align-middle>Mihai Surdeanu</a>
<span class="badge badge-secondary align-middle ml-2">5</span></li><li class=list-group-item><a href=/people/p/peter-jansen/ class=align-middle>Peter Jansen</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/m/marco-a-valenzuela-escarcega/ class=align-middle>Marco A. Valenzuela-Escárcega</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/p/peter-clark/ class=align-middle>Peter Clark</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/m/michael-hammond/ class=align-middle>Michael Hammond</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/v/vikas-yadav/ class=align-middle>Vikas Yadav</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/steven-bethard/ class=align-middle>Steven Bethard</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/f/fan-luo/ class=align-middle>Fan Luo</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/ajay-nagesh/ class=align-middle>Ajay Nagesh</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/robert-vacareanu/ class=align-middle>Robert Vacareanu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/mithun-paul-panenghat/ class=align-middle>Mithun Paul Panenghat</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/sandeep-suntwal/ class=align-middle>Sandeep Suntwal</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/f/faiz-rafique/ class=align-middle>Faiz Rafique</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/john-culnan/ class=align-middle>John Culnan</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/seongjin-park/ class=align-middle>Seongjin Park</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/meghavarshini-krishnaswamy/ class=align-middle>Meghavarshini Krishnaswamy</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/conll/ class=align-middle>CoNLL</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/cl/ class=align-middle>CL</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/semeval/ class=align-middle>SemEval</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/ws/ class=align-middle>WS</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/coling/ class=align-middle>COLING</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-venues aria-expanded=false aria-controls=more-venues>show all...</li><div class="collapse border-top" id=more-venues><li class=list-group-item><a href=/venues/lrec/ class=align-middle>LREC</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/wassa/ class=align-middle>WASSA</a><span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>