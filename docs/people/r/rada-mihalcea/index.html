<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Rada Mihalcea - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Rada</span> <span class=font-weight-bold>Mihalcea</span></h2><hr><div class=row><div class=col-lg-9><h4>2022</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.acl-long.209.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--acl-long--209 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.acl-long.209 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.acl-long.209" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2022.acl-long.209/><span class=acl-fixed-case>FIBER</span>: Fill-in-the-Blanks as a Challenging Video Understanding Evaluation Framework</a></strong><br><a href=/people/s/santiago-castro/>Santiago Castro</a>
|
<a href=/people/r/ruoyao-wang/>Ruoyao Wang</a>
|
<a href=/people/p/pingxuan-huang/>Pingxuan Huang</a>
|
<a href=/people/i/ian-stewart/>Ian Stewart</a>
|
<a href=/people/o/oana-ignat/>Oana Ignat</a>
|
<a href=/people/n/nan-liu/>Nan Liu</a>
|
<a href=/people/j/jonathan-stroud/>Jonathan Stroud</a>
|
<a href=/people/r/rada-mihalcea/>Rada Mihalcea</a><br><a href=/volumes/2022.acl-long/ class=text-muted>Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--acl-long--209><div class="card-body p-3 small">We propose fill-in-the-blanks as a video understanding evaluation framework and introduce FIBER &#8211; a novel dataset consisting of 28,000 videos and descriptions in support of this evaluation framework. The fill-in-the-blanks setting tests a model&#8217;s understanding of a video by requiring it to predict a masked noun phrase in the caption of the video, given the video and the surrounding text. The FIBER benchmark does not share the weaknesses of the current state-of-the-art language-informed video understanding tasks, namely: (1) video question answering using multiple-choice questions, where models perform relatively well because they exploit linguistic biases in the task formulation, thus making our framework challenging for the current state-of-the-art systems to solve; and (2) video captioning, which relies on an open-ended evaluation framework that is often inaccurate because system answers may be perceived as incorrect if they differ in form from the ground truth. The FIBER dataset and our code are available at https://lit.eecs.umich.edu/fiber/.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.acl-long.344.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--acl-long--344 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.acl-long.344 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.acl-long.344" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2022.acl-long.344/>CICERO A Dataset for Contextualized Commonsense Inference in Dialogues<span class=acl-fixed-case>CICERO</span>: A Dataset for Contextualized Commonsense Inference in Dialogues</a></strong><br><a href=/people/d/deepanway-ghosal/>Deepanway Ghosal</a>
|
<a href=/people/s/siqi-shen/>Siqi Shen</a>
|
<a href=/people/n/navonil-majumder/>Navonil Majumder</a>
|
<a href=/people/r/rada-mihalcea/>Rada Mihalcea</a>
|
<a href=/people/s/soujanya-poria/>Soujanya Poria</a><br><a href=/volumes/2022.acl-long/ class=text-muted>Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--acl-long--344><div class="card-body p-3 small">This paper addresses the problem of dialogue reasoning with contextualized commonsense inference We curate CICERO a dataset of dyadic conversations with five types of utterance level reasoning based inferences cause subsequent event prerequisite motivation and emotional reaction The <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> contains 53,105 of such inferences from 5,672 dialogues We use this dataset to solve relevant generative and discriminative tasks generation of cause and subsequent event generation of prerequisite motivation and listener&#8217;s emotional reaction and selection of plausible alternatives Our results ascertain the value of such dialogue centric commonsense knowledge datasets It is our hope that CICERO will open new research avenues into commonsense based dialogue reasoning</div></div><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.socialnlp-1.13.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--socialnlp-1--13 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.socialnlp-1.13 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.socialnlp-1.13/>Room to Grow : Understanding Personal Characteristics Behind Self Improvement Using <a href=https://en.wikipedia.org/wiki/Social_media>Social Media</a></a></strong><br><a href=/people/m/meixing-dong/>MeiXing Dong</a>
|
<a href=/people/x/xueming-xu/>Xueming Xu</a>
|
<a href=/people/y/yiwei-zhang/>Yiwei Zhang</a>
|
<a href=/people/i/ian-stewart/>Ian Stewart</a>
|
<a href=/people/r/rada-mihalcea/>Rada Mihalcea</a><br><a href=/volumes/2021.socialnlp-1/ class=text-muted>Proceedings of the Ninth International Workshop on Natural Language Processing for Social Media</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--socialnlp-1--13><div class="card-body p-3 small">Many people aim for change, but not everyone succeeds. While there are a number of <a href=https://en.wikipedia.org/wiki/Social_psychology>social psychology theories</a> that propose motivation-related characteristics of those who persist with change, few <a href=https://en.wikipedia.org/wiki/Computational_psychology>computational studies</a> have explored the motivational stage of personal change. In this paper, we investigate a new <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> consisting of the writings of people who manifest intention to change, some of whom persist while others do not. Using a variety of <a href=https://en.wikipedia.org/wiki/Linguistic_description>linguistic analysis techniques</a>, we first examine the <a href=https://en.wikipedia.org/wiki/Writing_style>writing patterns</a> that distinguish the two groups of people. Persistent people tend to reference more topics related to long-term self-improvement and use a more complicated writing style. Drawing on these consistent differences, we build a <a href=https://en.wikipedia.org/wiki/Classifier_(linguistics)>classifier</a> that can reliably identify the people more likely to persist, based on their language. Our experiments provide new insights into the motivation-related behavior of people who persist with their intention to change.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.sigdial-1.33.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--sigdial-1--33 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.sigdial-1.33 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href="https://www.youtube.com/watch?v=vSNq0OOGRc0" data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.sigdial-1.33" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2021.sigdial-1.33/>CIDER : Commonsense Inference for Dialogue Explanation and Reasoning<span class=acl-fixed-case>CIDER</span>: Commonsense Inference for Dialogue Explanation and Reasoning</a></strong><br><a href=/people/d/deepanway-ghosal/>Deepanway Ghosal</a>
|
<a href=/people/p/pengfei-hong/>Pengfei Hong</a>
|
<a href=/people/s/siqi-shen/>Siqi Shen</a>
|
<a href=/people/n/navonil-majumder/>Navonil Majumder</a>
|
<a href=/people/r/rada-mihalcea/>Rada Mihalcea</a>
|
<a href=/people/s/soujanya-poria/>Soujanya Poria</a><br><a href=/volumes/2021.sigdial-1/ class=text-muted>Proceedings of the 22nd Annual Meeting of the Special Interest Group on Discourse and Dialogue</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--sigdial-1--33><div class="card-body p-3 small">Commonsense inference to understand and explain <a href=https://en.wikipedia.org/wiki/Human_language>human language</a> is a fundamental research problem in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a>. Explaining human conversations poses a great challenge as it requires contextual understanding, planning, <a href=https://en.wikipedia.org/wiki/Inference>inference</a>, and several aspects of <a href=https://en.wikipedia.org/wiki/Reason>reasoning</a> including causal, temporal, and commonsense reasoning. In this work, we introduce CIDER a manually curated dataset that contains dyadic dialogue explanations in the form of implicit and explicit knowledge triplets inferred using contextual commonsense inference. Extracting such rich explanations from <a href=https://en.wikipedia.org/wiki/Conversation>conversations</a> can be conducive to improving several downstream <a href=https://en.wikipedia.org/wiki/Application_software>applications</a>. The annotated triplets are categorized by the type of <a href=https://en.wikipedia.org/wiki/Common_knowledge_(logic)>commonsense knowledge</a> present (e.g., causal, conditional, temporal). We set up three different tasks conditioned on the annotated dataset : Dialogue-level Natural Language Inference, Span Extraction, and Multi-choice Span Selection. Baseline results obtained with transformer-based models reveal that the tasks are difficult, paving the way for promising future research. The <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> and the baseline implementations are publicly available at https://github.com/declare-lab/CIDER.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.515.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--515 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.515 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.515/>Hitting your MARQ : Multimodal ARgument Quality Assessment in Long Debate Video<span class=acl-fixed-case>MARQ</span>: Multimodal <span class=acl-fixed-case>AR</span>gument Quality Assessment in Long Debate Video</a></strong><br><a href=/people/m/md-kamrul-hasan/>Md Kamrul Hasan</a>
|
<a href=/people/j/james-spann/>James Spann</a>
|
<a href=/people/m/masum-hasan/>Masum Hasan</a>
|
<a href=/people/m/md-saiful-islam/>Md Saiful Islam</a>
|
<a href=/people/k/kurtis-haut/>Kurtis Haut</a>
|
<a href=/people/r/rada-mihalcea/>Rada Mihalcea</a>
|
<a href=/people/e/ehsan-hoque/>Ehsan Hoque</a><br><a href=/volumes/2021.emnlp-main/ class=text-muted>Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--515><div class="card-body p-3 small">The combination of <a href=https://en.wikipedia.org/wiki/Gesture>gestures</a>, <a href=https://en.wikipedia.org/wiki/Intonation_(linguistics)>intonations</a>, and <a href=https://en.wikipedia.org/wiki/Text_(literary_theory)>textual content</a> plays a key role in <a href=https://en.wikipedia.org/wiki/Argumentation_theory>argument delivery</a>. However, the current literature mostly considers textual content while assessing the quality of an argument, and it is limited to <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> containing short sequences (18-48 words). In this paper, we study argument quality assessment in a multimodal context, and experiment on DBATES, a publicly available dataset of long debate videos. First, we propose a set of interpretable debate centric features such as clarity, content variation, body movement cues, and pauses, inspired by theories of <a href=https://en.wikipedia.org/wiki/Argumentation_theory>argumentation quality</a>. Second, we design the Multimodal ARgument Quality assessor (MARQ) a hierarchical neural network model that summarizes the multimodal signals on long sequences and enriches the multimodal embedding with debate centric features. Our proposed MARQ model achieves an <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of 81.91 % on the argument quality prediction task and outperforms established <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline models</a> with an <a href=https://en.wikipedia.org/wiki/Error_detection_and_correction>error rate reduction</a> of 22.7 %. Through ablation studies, we demonstrate the importance of multimodal cues in modeling argument quality.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.clpsych-1.18.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--clpsych-1--18 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.clpsych-1.18 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.clpsych-1.18/>Evaluating Automatic Speech Recognition Quality and Its Impact on Counselor Utterance Coding</a></strong><br><a href=/people/d/do-june-min/>Do June Min</a>
|
<a href=/people/v/veronica-perez-rosas/>Verónica Pérez-Rosas</a>
|
<a href=/people/r/rada-mihalcea/>Rada Mihalcea</a><br><a href=/volumes/2021.clpsych-1/ class=text-muted>Proceedings of the Seventh Workshop on Computational Linguistics and Clinical Psychology: Improving Access</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--clpsych-1--18><div class="card-body p-3 small">Automatic speech recognition (ASR) is a crucial step in many natural language processing (NLP) applications, as often available data consists mainly of raw speech. Since the result of the ASR step is considered as a meaningful, informative input to later steps in the NLP pipeline, it is important to understand the behavior and failure mode of this step. In this work, we analyze the quality of ASR in the psychotherapy domain, using motivational interviewing conversations between therapists and clients. We conduct domain agnostic and domain-relevant evaluations using standard evaluation metrics and also identify domain-relevant keywords in the ASR output. Moreover, we empirically study the effect of mixing ASR and manual data during the training of a downstream NLP model, and also demonstrate how additional local context can help alleviate the error introduced by noisy ASR transcripts.</div></div><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.334.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--334 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.334 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939153 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.334" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.334/>Compositional Demographic Word Embeddings</a></strong><br><a href=/people/c/charles-welch/>Charles Welch</a>
|
<a href=/people/j/jonathan-k-kummerfeld/>Jonathan K. Kummerfeld</a>
|
<a href=/people/v/veronica-perez-rosas/>Verónica Pérez-Rosas</a>
|
<a href=/people/r/rada-mihalcea/>Rada Mihalcea</a><br><a href=/volumes/2020.emnlp-main/ class=text-muted>Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--334><div class="card-body p-3 small">Word embeddings are usually derived from corpora containing text from many individuals, thus leading to general purpose representations rather than individually personalized representations. While personalized embeddings can be useful to improve <a href=https://en.wikipedia.org/wiki/Language_model>language model</a> performance and other language processing tasks, they can only be computed for people with a large amount of <a href=https://en.wikipedia.org/wiki/Panel_data>longitudinal data</a>, which is not the case for new users. We propose a new form of personalized word embeddings that use demographic-specific word representations derived compositionally from full or partial demographic information for a user (i.e., gender, age, location, religion). We show that the resulting demographic-aware word representations outperform generic word representations on two tasks for <a href=https://en.wikipedia.org/wiki/English_language>English</a> : <a href=https://en.wikipedia.org/wiki/Language_model>language modeling</a> and word associations. We further explore the trade-off between the number of available attributes and their relative effectiveness and discuss the ethical implications of using them.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.acl-main.292.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--acl-main--292 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.acl-main.292 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://slideslive.com/38929234 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.acl-main.292" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.acl-main.292/>KinGDOM : Knowledge-Guided DOMain Adaptation for <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>Sentiment Analysis</a><span class=acl-fixed-case>K</span>in<span class=acl-fixed-case>GDOM</span>: <span class=acl-fixed-case>K</span>nowledge-<span class=acl-fixed-case>G</span>uided <span class=acl-fixed-case>DOM</span>ain <span class=acl-fixed-case>A</span>daptation for <span class=acl-fixed-case>S</span>entiment <span class=acl-fixed-case>A</span>nalysis</a></strong><br><a href=/people/d/deepanway-ghosal/>Deepanway Ghosal</a>
|
<a href=/people/d/devamanyu-hazarika/>Devamanyu Hazarika</a>
|
<a href=/people/a/abhinaba-roy/>Abhinaba Roy</a>
|
<a href=/people/n/navonil-majumder/>Navonil Majumder</a>
|
<a href=/people/r/rada-mihalcea/>Rada Mihalcea</a>
|
<a href=/people/s/soujanya-poria/>Soujanya Poria</a><br><a href=/volumes/2020.acl-main/ class=text-muted>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--acl-main--292><div class="card-body p-3 small">Cross-domain sentiment analysis has received significant attention in recent years, prompted by the need to combat the domain gap between different <a href=https://en.wikipedia.org/wiki/Application_software>applications</a> that make use of <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a>. In this paper, we take a novel perspective on this task by exploring the role of <a href=https://en.wikipedia.org/wiki/Common_knowledge_(logic)>external commonsense knowledge</a>. We introduce a new framework, KinGDOM, which utilizes the ConceptNet knowledge graph to enrich the semantics of a document by providing both domain-specific and domain-general background concepts. These <a href=https://en.wikipedia.org/wiki/Concept>concepts</a> are learned by training a graph convolutional autoencoder that leverages inter-domain concepts in a domain-invariant manner. Conditioning a popular domain-adversarial baseline method with these learned concepts helps improve its performance over state-of-the-art approaches, demonstrating the efficacy of our proposed framework.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.nlpcovid19-2.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.nlpcovid19-2.0/>Proceedings of the 1st Workshop on <span class=acl-fixed-case>NLP</span> for <span class=acl-fixed-case>COVID</span>-19 (Part 2) at <span class=acl-fixed-case>EMNLP</span> 2020</a></strong><br><a href=/people/k/karin-verspoor/>Karin Verspoor</a>
|
<a href=/people/k/k-bretonnel-cohen/>Kevin Bretonnel Cohen</a>
|
<a href=/people/m/michael-conway/>Michael Conway</a>
|
<a href=/people/b/berry-de-bruijn/>Berry de Bruijn</a>
|
<a href=/people/m/mark-dredze/>Mark Dredze</a>
|
<a href=/people/r/rada-mihalcea/>Rada Mihalcea</a>
|
<a href=/people/b/byron-c-wallace/>Byron Wallace</a><br><a href=/volumes/2020.nlpcovid19-2/ class=text-muted>Proceedings of the 1st Workshop on NLP for COVID-19 (Part 2) at EMNLP 2020</a></span></p><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1122.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1122 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1122 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1122/>Towards Extracting <a href=https://en.wikipedia.org/wiki/Medical_family_history>Medical Family History</a> from <a href=https://en.wikipedia.org/wiki/Natural_language_processing>Natural Language Interactions</a> : A New Dataset and Baselines</a></strong><br><a href=/people/m/mahmoud-azab/>Mahmoud Azab</a>
|
<a href=/people/s/stephane-dadian/>Stephane Dadian</a>
|
<a href=/people/v/vivi-nastase/>Vivi Nastase</a>
|
<a href=/people/l/larry-an/>Larry An</a>
|
<a href=/people/r/rada-mihalcea/>Rada Mihalcea</a><br><a href=/volumes/D19-1/ class=text-muted>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1122><div class="card-body p-3 small">We introduce a new dataset consisting of <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language interactions</a> annotated with <a href=https://en.wikipedia.org/wiki/Family_history_(medicine)>medical family histories</a>, obtained during interactions with a <a href=https://en.wikipedia.org/wiki/Genetic_counseling>genetic counselor</a> and through <a href=https://en.wikipedia.org/wiki/Crowdsourcing>crowdsourcing</a>, following a questionnaire created by experts in the domain. We describe the data collection process and the annotations performed by medical professionals, including illness and personal attributes (name, age, gender, family relationships) for the patient and their family members. An initial system that performs argument identification and <a href=https://en.wikipedia.org/wiki/Relation_extraction>relation extraction</a> shows promising results average <a href=https://en.wikipedia.org/wiki/F-score>F-score</a> of 0.87 on complex sentences on the targeted relations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S19-1000.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/S19-1000/>Proceedings of the Eighth Joint Conference on Lexical and Computational Semantics (*<span class=acl-fixed-case>SEM</span> 2019)</a></strong><br><a href=/people/r/rada-mihalcea/>Rada Mihalcea</a>
|
<a href=/people/e/ekaterina-shutova/>Ekaterina Shutova</a>
|
<a href=/people/l/lun-wei-ku/>Lun-Wei Ku</a>
|
<a href=/people/k/kilian-evang/>Kilian Evang</a>
|
<a href=/people/s/soujanya-poria/>Soujanya Poria</a><br><a href=/volumes/S19-1/ class=text-muted>Proceedings of the Eighth Joint Conference on Lexical and Computational Semantics (*SEM 2019)</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1050.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1050 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1050 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1050" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-1050/>MELD : A Multimodal Multi-Party Dataset for Emotion Recognition in Conversations<span class=acl-fixed-case>MELD</span>: A Multimodal Multi-Party Dataset for Emotion Recognition in Conversations</a></strong><br><a href=/people/s/soujanya-poria/>Soujanya Poria</a>
|
<a href=/people/d/devamanyu-hazarika/>Devamanyu Hazarika</a>
|
<a href=/people/n/navonil-majumder/>Navonil Majumder</a>
|
<a href=/people/g/gautam-naik/>Gautam Naik</a>
|
<a href=/people/e/erik-cambria/>Erik Cambria</a>
|
<a href=/people/r/rada-mihalcea/>Rada Mihalcea</a><br><a href=/volumes/P19-1/ class=text-muted>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1050><div class="card-body p-3 small">Emotion recognition in conversations is a challenging <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> that has recently gained popularity due to its potential applications. Until now, however, a large-scale multimodal multi-party emotional conversational database containing more than two speakers per dialogue was missing. Thus, we propose the Multimodal EmotionLines Dataset (MELD), an extension and enhancement of EmotionLines. MELD contains about 13,000 utterances from 1,433 dialogues from the TV-series Friends. Each utterance is annotated with emotion and sentiment labels, and encompasses audio, visual and textual modalities. We propose several strong multimodal baselines and show the importance of contextual and multimodal information for emotion recognition in conversations. The full dataset is available for use at http://affective-meld.github.io.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1088.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1088 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1088 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/384465038 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1088/>What Makes a Good Counselor? Learning to Distinguish between High-quality and Low-quality Counseling Conversations</a></strong><br><a href=/people/v/veronica-perez-rosas/>Verónica Pérez-Rosas</a>
|
<a href=/people/x/xinyi-wu/>Xinyi Wu</a>
|
<a href=/people/k/kenneth-resnicow/>Kenneth Resnicow</a>
|
<a href=/people/r/rada-mihalcea/>Rada Mihalcea</a><br><a href=/volumes/P19-1/ class=text-muted>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1088><div class="card-body p-3 small">The quality of a <a href=https://en.wikipedia.org/wiki/Intervention_(counseling)>counseling intervention</a> relies highly on the active collaboration between clients and counselors. In this paper, we explore several <a href=https://en.wikipedia.org/wiki/Linguistics>linguistic aspects</a> of the <a href=https://en.wikipedia.org/wiki/Collaboration>collaboration process</a> occurring during <a href=https://en.wikipedia.org/wiki/List_of_counseling_topics>counseling conversations</a>. Specifically, we address the differences between high-quality and low-quality counseling. Our approach examines participants&#8217; turn-by-turn interaction, their linguistic alignment, the sentiment expressed by speakers during the conversation, as well as the different topics being discussed. Our results suggest important language differences in low- and high-quality counseling, which we further use to derive linguistic features able to capture the differences between the two groups. These <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> are then used to build <a href=https://en.wikipedia.org/wiki/Statistical_classification>automatic classifiers</a> that can predict <a href=https://en.wikipedia.org/wiki/Counseling_psychology>counseling quality</a> with <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracies</a> of up to 88 %.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1339.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1339 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1339 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/384801759 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1339/>Women’s Syntactic Resilience and Men’s Grammatical Luck : Gender-Bias in Part-of-Speech Tagging and Dependency Parsing</a></strong><br><a href=/people/a/aparna-garimella/>Aparna Garimella</a>
|
<a href=/people/c/carmen-banea/>Carmen Banea</a>
|
<a href=/people/d/dirk-hovy/>Dirk Hovy</a>
|
<a href=/people/r/rada-mihalcea/>Rada Mihalcea</a><br><a href=/volumes/P19-1/ class=text-muted>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1339><div class="card-body p-3 small">Several linguistic studies have shown the prevalence of various lexical and grammatical patterns in texts authored by a person of a particular gender, but models for <a href=https://en.wikipedia.org/wiki/Part-of-speech_tagging>part-of-speech tagging</a> and <a href=https://en.wikipedia.org/wiki/Dependency_grammar>dependency parsing</a> have still not adapted to account for these differences. To address this, we annotate the <a href=https://en.wikipedia.org/wiki/The_Wall_Street_Journal>Wall Street Journal</a> part of the Penn Treebank with the gender information of the articles&#8217; authors, and build taggers and parsers trained on this data that show performance differences in text written by men and women. Further analyses reveal numerous <a href=https://en.wikipedia.org/wiki/Part-of-speech_tagging>part-of-speech tags</a> and <a href=https://en.wikipedia.org/wiki/Syntax>syntactic relations</a> whose prediction performances benefit from the prevalence of a specific gender in the training data. The results underscore the importance of accounting for gendered differences in syntactic tasks, and outline future venues for developing more accurate taggers and parsers. We release our data to the research community.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1455.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1455 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1455 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P19-1455.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1455" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-1455/>Towards Multimodal Sarcasm Detection (An _ Obviously _ Perfect Paper)<span class=acl-fixed-case>O</span>bviously_ Perfect Paper)</a></strong><br><a href=/people/s/santiago-castro/>Santiago Castro</a>
|
<a href=/people/d/devamanyu-hazarika/>Devamanyu Hazarika</a>
|
<a href=/people/v/veronica-perez-rosas/>Verónica Pérez-Rosas</a>
|
<a href=/people/r/roger-zimmermann/>Roger Zimmermann</a>
|
<a href=/people/r/rada-mihalcea/>Rada Mihalcea</a>
|
<a href=/people/s/soujanya-poria/>Soujanya Poria</a><br><a href=/volumes/P19-1/ class=text-muted>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1455><div class="card-body p-3 small">Sarcasm is often expressed through several verbal and non-verbal cues, e.g., a <a href=https://en.wikipedia.org/wiki/Tone_(linguistics)>change of tone</a>, overemphasis in a word, a drawn-out syllable, or a straight looking face. Most of the recent work in sarcasm detection has been carried out on <a href=https://en.wikipedia.org/wiki/Text_(literary_theory)>textual data</a>. In this paper, we argue that incorporating multimodal cues can improve the automatic classification of sarcasm. As a first step towards enabling the development of multimodal approaches for sarcasm detection, we propose a new sarcasm dataset, Multimodal Sarcasm Detection Dataset (MUStARD), compiled from popular TV shows. MUStARD consists of <a href=https://en.wikipedia.org/wiki/Audiovisual>audiovisual utterances</a> annotated with <a href=https://en.wikipedia.org/wiki/Sarcasm>sarcasm labels</a>. Each utterance is accompanied by its context of historical utterances in the dialogue, which provides additional information on the scenario where the utterance occurs. Our initial results show that the use of multimodal information can reduce the relative error rate of sarcasm detection by up to 12.9 % in <a href=https://en.wikipedia.org/wiki/F-score>F-score</a> when compared to the use of individual modalities. The full dataset is publicly available for use at https://github.com/soujanyaporia/MUStARD.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1643.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1643 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1643 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1643" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-1643/>Identifying Visible Actions in Lifestyle Vlogs</a></strong><br><a href=/people/o/oana-ignat/>Oana Ignat</a>
|
<a href=/people/l/laura-burdick/>Laura Burdick</a>
|
<a href=/people/j/jia-deng/>Jia Deng</a>
|
<a href=/people/r/rada-mihalcea/>Rada Mihalcea</a><br><a href=/volumes/P19-1/ class=text-muted>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1643><div class="card-body p-3 small">We consider the task of identifying <a href=https://en.wikipedia.org/wiki/Human_behavior>human actions</a> visible in <a href=https://en.wikipedia.org/wiki/Online_video_platform>online videos</a>. We focus on the widely spread genre of lifestyle vlogs, which consist of videos of people performing actions while verbally describing them. Our goal is to identify if actions mentioned in the speech description of a video are visually present. We construct a dataset with crowdsourced manual annotations of visible actions, and introduce a multimodal algorithm that leverages information derived from visual and linguistic clues to automatically infer which actions are visible in a video.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K19-1010.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K19-1010 data-toggle=collapse aria-expanded=false aria-controls=abstract-K19-1010 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/K19-1010/>Representing Movie Characters in Dialogues</a></strong><br><a href=/people/m/mahmoud-azab/>Mahmoud Azab</a>
|
<a href=/people/n/noriyuki-kojima/>Noriyuki Kojima</a>
|
<a href=/people/j/jia-deng/>Jia Deng</a>
|
<a href=/people/r/rada-mihalcea/>Rada Mihalcea</a><br><a href=/volumes/K19-1/ class=text-muted>Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K19-1010><div class="card-body p-3 small">We introduce a new embedding model to represent movie characters and their interactions in a <a href=https://en.wikipedia.org/wiki/Dialogue>dialogue</a> by encoding in the same representation the language used by these <a href=https://en.wikipedia.org/wiki/Character_(arts)>characters</a> as well as information about the other participants in the dialogue. We evaluate the performance of these new character embeddings on two tasks : (1) character relatedness, using a dataset we introduce consisting of a dense character interaction matrix for 4,378 unique character pairs over 22 hours of dialogue from eighteen movies ; and (2) character relation classification, for fine- and coarse-grained relations, as well as sentiment relations. Our experiments show that our model significantly outperforms the traditional Word2Vec continuous bag-of-words and skip-gram models, demonstrating the effectiveness of the character embeddings we introduce. We further show how these <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a> can be used in conjunction with a visual question answering system to improve over previous results.</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1156.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1156 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1156 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1156/>CASCADE : Contextual Sarcasm Detection in Online Discussion Forums<span class=acl-fixed-case>CASCADE</span>: Contextual Sarcasm Detection in Online Discussion Forums</a></strong><br><a href=/people/d/devamanyu-hazarika/>Devamanyu Hazarika</a>
|
<a href=/people/s/soujanya-poria/>Soujanya Poria</a>
|
<a href=/people/s/sruthi-gorantla/>Sruthi Gorantla</a>
|
<a href=/people/e/erik-cambria/>Erik Cambria</a>
|
<a href=/people/r/roger-zimmermann/>Roger Zimmermann</a>
|
<a href=/people/r/rada-mihalcea/>Rada Mihalcea</a><br><a href=/volumes/C18-1/ class=text-muted>Proceedings of the 27th International Conference on Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1156><div class="card-body p-3 small">The literature in automated sarcasm detection has mainly focused on lexical-, syntactic- and semantic-level analysis of text. However, a <a href=https://en.wikipedia.org/wiki/Sarcasm>sarcastic sentence</a> can be expressed with contextual presumptions, background and commonsense knowledge. In this paper, we propose a ContextuAl SarCasm DEtector (CASCADE), which adopts a hybrid approach of both content- and context-driven modeling for sarcasm detection in online social media discussions. For the latter, CASCADE aims at extracting <a href=https://en.wikipedia.org/wiki/Context_(language_use)>contextual information</a> from the discourse of a discussion thread. Also, since the sarcastic nature and form of expression can vary from person to person, CASCADE utilizes user embeddings that encode stylometric and personality features of users. When used along with content-based feature extractors such as <a href=https://en.wikipedia.org/wiki/Convolutional_neural_network>convolutional neural networks</a>, we see a significant boost in the <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a> performance on a large Reddit corpus.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1287.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1287 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1287 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1287/>Automatic Detection of Fake News</a></strong><br><a href=/people/v/veronica-perez-rosas/>Verónica Pérez-Rosas</a>
|
<a href=/people/b/bennett-kleinberg/>Bennett Kleinberg</a>
|
<a href=/people/a/alexandra-lefevre/>Alexandra Lefevre</a>
|
<a href=/people/r/rada-mihalcea/>Rada Mihalcea</a><br><a href=/volumes/C18-1/ class=text-muted>Proceedings of the 27th International Conference on Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1287><div class="card-body p-3 small">The proliferation of misleading information in everyday access media outlets such as <a href=https://en.wikipedia.org/wiki/Social_media>social media feeds</a>, <a href=https://en.wikipedia.org/wiki/Blog>news blogs</a>, and <a href=https://en.wikipedia.org/wiki/Online_newspaper>online newspapers</a> have made it challenging to identify trustworthy news sources, thus increasing the need for computational tools able to provide insights into the reliability of online content. In this paper, we focus on the automatic identification of fake content in <a href=https://en.wikipedia.org/wiki/Online_newspaper>online news</a>. Our contribution is twofold. First, we introduce two novel <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> for the task of fake news detection, covering seven different <a href=https://en.wikipedia.org/wiki/News_media>news domains</a>. We describe the collection, annotation, and validation process in detail and present several exploratory analyses on the identification of linguistic differences in fake and legitimate news content. Second, we conduct a set of learning experiments to build accurate fake news detectors, and show that we can achieve accuracies of up to 76 %. In addition, we provide comparative analyses of the automatic and manual identification of fake news.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1280.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1280 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1280 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-1280/>ICON : Interactive Conversational Memory Network for Multimodal Emotion Detection<span class=acl-fixed-case>ICON</span>: Interactive Conversational Memory Network for Multimodal Emotion Detection</a></strong><br><a href=/people/d/devamanyu-hazarika/>Devamanyu Hazarika</a>
|
<a href=/people/s/soujanya-poria/>Soujanya Poria</a>
|
<a href=/people/r/rada-mihalcea/>Rada Mihalcea</a>
|
<a href=/people/e/erik-cambria/>Erik Cambria</a>
|
<a href=/people/r/roger-zimmermann/>Roger Zimmermann</a><br><a href=/volumes/D18-1/ class=text-muted>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1280><div class="card-body p-3 small">Emotion recognition in <a href=https://en.wikipedia.org/wiki/Conversation>conversations</a> is crucial for building empathetic machines. Present works in this domain do not explicitly consider the inter-personal influences that thrive in the emotional dynamics of dialogues. To this end, we propose Interactive COnversational memory Network (ICON), a multimodal emotion detection framework that extracts multimodal features from conversational videos and hierarchically models the self- and inter-speaker emotional influences into global memories. Such <a href=https://en.wikipedia.org/wiki/Memory>memories</a> generate contextual summaries which aid in predicting the emotional orientation of utterance-videos. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms state-of-the-art networks on multiple classification and regression tasks in two benchmark datasets.</div></div><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1040.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1040 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1040 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-1040/>Identifying Usage Expression Sentences in Consumer Product Reviews</a></strong><br><a href=/people/s/shibamouli-lahiri/>Shibamouli Lahiri</a>
|
<a href=/people/v/v-g-vinod-vydiswaran/>V.G.Vinod Vydiswaran</a>
|
<a href=/people/r/rada-mihalcea/>Rada Mihalcea</a><br><a href=/volumes/I17-1/ class=text-muted>Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1040><div class="card-body p-3 small">In this paper we introduce the problem of identifying usage expression sentences in a consumer product review. We create a human-annotated gold standard dataset of 565 reviews spanning five distinct product categories. Our <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> consists of more than 3,000 annotated sentences. We further introduce a classification system to label sentences according to whether or not they describe some usage. The system combines lexical, syntactic, and semantic features in a product-agnostic fashion to yield good <a href=https://en.wikipedia.org/wiki/Categorization>classification</a> performance. We show the effectiveness of our approach using importance ranking of features, error analysis, and cross-product classification experiments.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1067.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1067 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1067 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-1067/>Measuring Semantic Relations between Human Activities</a></strong><br><a href=/people/s/steven-wilson/>Steven Wilson</a>
|
<a href=/people/r/rada-mihalcea/>Rada Mihalcea</a><br><a href=/volumes/I17-1/ class=text-muted>Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1067><div class="card-body p-3 small">The things people do in their daily lives can provide valuable insights into their <a href=https://en.wikipedia.org/wiki/Personality_psychology>personality</a>, values, and interests. Unstructured text data on <a href=https://en.wikipedia.org/wiki/Social_media>social media platforms</a> are rich in behavioral content, and automated systems can be deployed to learn about human activity on a broad scale if these systems are able to reason about the content of interest. In order to aid in the evaluation of such <a href=https://en.wikipedia.org/wiki/System>systems</a>, we introduce a new phrase-level semantic textual similarity dataset comprised of human activity phrases, providing a testbed for automated systems that analyze relationships between phrasal descriptions of people&#8217;s actions. Our set of 1,000 pairs of activities is annotated by human judges across four relational dimensions including <a href=https://en.wikipedia.org/wiki/Similarity_(psychology)>similarity</a>, <a href=https://en.wikipedia.org/wiki/Coefficient_of_relationship>relatedness</a>, motivational alignment, and perceived actor congruence. We evaluate a set of strong baselines for the task of generating scores that correlate highly with human ratings, and we introduce several new approaches to the phrase-level similarity task in the domain of human activities.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1089.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1089 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1089 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-1089/>Identity Deception Detection</a></strong><br><a href=/people/v/veronica-perez-rosas/>Verónica Pérez-Rosas</a>
|
<a href=/people/q/quincy-davenport/>Quincy Davenport</a>
|
<a href=/people/a/anna-mengdan-dai/>Anna Mengdan Dai</a>
|
<a href=/people/m/mohamed-abouelenien/>Mohamed Abouelenien</a>
|
<a href=/people/r/rada-mihalcea/>Rada Mihalcea</a><br><a href=/volumes/I17-1/ class=text-muted>Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1089><div class="card-body p-3 small">This paper addresses the task of detecting identity deception in <a href=https://en.wikipedia.org/wiki/Language>language</a>. Using a novel identity deception dataset, consisting of real and portrayed identities from 600 individuals, we show that we can build accurate identity detectors targeting both age and gender, with accuracies of up to 88. We also perform an analysis of the <a href=https://en.wikipedia.org/wiki/Pattern>linguistic patterns</a> used in <a href=https://en.wikipedia.org/wiki/Identity_deception>identity deception</a>, which lead to interesting insights into identity portrayers.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1131.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1131 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1131 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-1131/>Understanding and Predicting Empathic Behavior in Counseling Therapy</a></strong><br><a href=/people/v/veronica-perez-rosas/>Verónica Pérez-Rosas</a>
|
<a href=/people/r/rada-mihalcea/>Rada Mihalcea</a>
|
<a href=/people/k/kenneth-resnicow/>Kenneth Resnicow</a>
|
<a href=/people/s/satinder-singh/>Satinder Singh</a>
|
<a href=/people/l/lawrence-an/>Lawrence An</a><br><a href=/volumes/P17-1/ class=text-muted>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1131><div class="card-body p-3 small">Counselor empathy is associated with better outcomes in <a href=https://en.wikipedia.org/wiki/List_of_counseling_topics>psychology and behavioral counseling</a>. In this paper, we explore several aspects pertaining to counseling interaction dynamics and their relation to <a href=https://en.wikipedia.org/wiki/Empathy>counselor empathy</a> during motivational interviewing encounters. Particularly, we analyze aspects such as participants&#8217; engagement, participants&#8217; verbal and nonverbal accommodation, as well as topics being discussed during the conversation, with the final goal of identifying linguistic and acoustic markers of counselor empathy. We also show how we can use these findings alongside other raw linguistic and acoustic features to build accurate counselor empathy classifiers with accuracies of up to 80 %.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1242.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1242 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1242 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1242/>Demographic-aware word associations</a></strong><br><a href=/people/a/aparna-garimella/>Aparna Garimella</a>
|
<a href=/people/c/carmen-banea/>Carmen Banea</a>
|
<a href=/people/r/rada-mihalcea/>Rada Mihalcea</a><br><a href=/volumes/D17-1/ class=text-muted>Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1242><div class="card-body p-3 small">Variations of <a href=https://en.wikipedia.org/wiki/Word_association>word associations</a> across different groups of people can provide insights into people&#8217;s psychologies and their world views. To capture these variations, we introduce the task of demographic-aware word associations. We build a new gold standard dataset consisting of word association responses for approximately 300 stimulus words, collected from more than 800 respondents of different gender (male / female) and from different locations (India / United States), and show that there are significant variations in the word associations made by these groups. We also introduce a new demographic-aware word association model based on a neural net skip-gram architecture, and show how computational methods for measuring word associations that specifically account for writer demographics can outperform generic methods that are agnostic to such information.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1106.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1106 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1106 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1106/>Predicting Counselor Behaviors in Motivational Interviewing Encounters</a></strong><br><a href=/people/v/veronica-perez-rosas/>Verónica Pérez-Rosas</a>
|
<a href=/people/r/rada-mihalcea/>Rada Mihalcea</a>
|
<a href=/people/k/kenneth-resnicow/>Kenneth Resnicow</a>
|
<a href=/people/s/satinder-singh/>Satinder Singh</a>
|
<a href=/people/l/lawrence-an/>Lawrence An</a>
|
<a href=/people/k/kathy-j-goggin/>Kathy J. Goggin</a>
|
<a href=/people/d/delwyn-catley/>Delwyn Catley</a><br><a href=/volumes/E17-1/ class=text-muted>Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1106><div class="card-body p-3 small">As the number of people receiving psycho-therapeutic treatment increases, the automatic evaluation of counseling practice arises as an important challenge in the clinical domain. In this paper, we address the automatic evaluation of counseling performance by analyzing counselors&#8217; language during their interaction with clients. In particular, we present a model towards the automation of Motivational Interviewing (MI) coding, which is the current gold standard to evaluate MI counseling. First, we build a dataset of hand labeled MI encounters ; second, we use text-based methods to extract and analyze linguistic patterns associated with counselor behaviors ; and third, we develop an automatic system to predict these behaviors. We introduce a new set of <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> based on <a href=https://en.wikipedia.org/wiki/Semantics>semantic information</a> and <a href=https://en.wikipedia.org/wiki/Syntax>syntactic patterns</a>, and show that they lead to accuracy figures of up to 90 %, which represent a significant improvement with respect to <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> used in the past.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2022.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2022 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2022 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2022/>A Computational Analysis of the Language of Drug Addiction</a></strong><br><a href=/people/c/carlo-strapparava/>Carlo Strapparava</a>
|
<a href=/people/r/rada-mihalcea/>Rada Mihalcea</a><br><a href=/volumes/E17-2/ class=text-muted>Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2022><div class="card-body p-3 small">We present a <a href=https://en.wikipedia.org/wiki/Computational_linguistics>computational analysis</a> of the language of drug users when talking about their drug experiences. We introduce a new dataset of over 4,000 descriptions of experiences reported by users of four main drug types, and show that we can predict with an F1-score of up to 88 % the drug behind a certain experience. We also perform an analysis of the dominant psycholinguistic processes and dominant emotions associated with each drug type, which sheds light on the characteristics of drug users.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Rada+Mihalcea" title="Search for 'Rada Mihalcea' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/s/soujanya-poria/ class=align-middle>Soujanya Poria</a>
<span class="badge badge-secondary align-middle ml-2">8</span></li><li class=list-group-item><a href=/people/v/veronica-perez-rosas/ class=align-middle>Verónica Pérez-Rosas</a>
<span class="badge badge-secondary align-middle ml-2">8</span></li><li class=list-group-item><a href=/people/d/devamanyu-hazarika/ class=align-middle>Devamanyu Hazarika</a>
<span class="badge badge-secondary align-middle ml-2">5</span></li><li class=list-group-item><a href=/people/n/navonil-majumder/ class=align-middle>Navonil Majumder</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/people/e/erik-cambria/ class=align-middle>Erik Cambria</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/r/roger-zimmermann/ class=align-middle>Roger Zimmermann</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/d/deepanway-ghosal/ class=align-middle>Deepanway Ghosal</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/k/kenneth-resnicow/ class=align-middle>Kenneth Resnicow</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/s/satinder-singh/ class=align-middle>Satinder Singh</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/l/lawrence-an/ class=align-middle>Lawrence An</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/i/ian-stewart/ class=align-middle>Ian Stewart</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/s/siqi-shen/ class=align-middle>Siqi Shen</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/s/santiago-castro/ class=align-middle>Santiago Castro</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/o/oana-ignat/ class=align-middle>Oana Ignat</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/m/mahmoud-azab/ class=align-middle>Mahmoud Azab</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/a/aparna-garimella/ class=align-middle>Aparna Garimella</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/c/carmen-banea/ class=align-middle>Carmen Banea</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/j/jia-deng/ class=align-middle>Jia Deng</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/s/sruthi-gorantla/ class=align-middle>Sruthi Gorantla</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/b/bennett-kleinberg/ class=align-middle>Bennett Kleinberg</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/alexandra-lefevre/ class=align-middle>Alexandra Lefevre</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/shibamouli-lahiri/ class=align-middle>Shibamouli Lahiri</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/v/v-g-vinod-vydiswaran/ class=align-middle>V.G.Vinod Vydiswaran</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/steven-wilson/ class=align-middle>Steven Wilson</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/q/quincy-davenport/ class=align-middle>Quincy Davenport</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/anna-mengdan-dai/ class=align-middle>Anna Mengdan Dai</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/mohamed-abouelenien/ class=align-middle>Mohamed Abouelenien</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/charles-welch/ class=align-middle>Charles Welch</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jonathan-k-kummerfeld/ class=align-middle>Jonathan K. Kummerfeld</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/abhinaba-roy/ class=align-middle>Abhinaba Roy</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/meixing-dong/ class=align-middle>MeiXing Dong</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/x/xueming-xu/ class=align-middle>Xueming Xu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yiwei-zhang/ class=align-middle>Yiwei Zhang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/p/pengfei-hong/ class=align-middle>Pengfei Hong</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/ruoyao-wang/ class=align-middle>Ruoyao Wang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/p/pingxuan-huang/ class=align-middle>Pingxuan Huang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/n/nan-liu/ class=align-middle>Nan Liu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jonathan-stroud/ class=align-middle>Jonathan Stroud</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/md-kamrul-hasan/ class=align-middle>Md Kamrul Hasan</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/james-spann/ class=align-middle>James Spann</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/masum-hasan/ class=align-middle>Masum Hasan</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/md-saiful-islam/ class=align-middle>Md Saiful Islam</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/kurtis-haut/ class=align-middle>Kurtis Haut</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/e/ehsan-hoque/ class=align-middle>Ehsan Hoque</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/stephane-dadian/ class=align-middle>Stephane Dadian</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/v/vivi-nastase/ class=align-middle>Vivi Nastase</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/larry-an/ class=align-middle>Larry An</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/e/ekaterina-shutova/ class=align-middle>Ekaterina Shutova</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/lun-wei-ku/ class=align-middle>Lun-Wei Ku</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/kilian-evang/ class=align-middle>Kilian Evang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/do-june-min/ class=align-middle>Do June Min</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/karin-verspoor/ class=align-middle>Karin Verspoor</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/k-bretonnel-cohen/ class=align-middle>K. Bretonnel Cohen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/michael-conway/ class=align-middle>Michael Conway</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/b/berry-de-bruijn/ class=align-middle>Berry de Bruijn</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/mark-dredze/ class=align-middle>Mark Dredze</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/b/byron-c-wallace/ class=align-middle>Byron C. Wallace</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/kathy-j-goggin/ class=align-middle>Kathy J. Goggin</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/delwyn-catley/ class=align-middle>Delwyn Catley</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/carlo-strapparava/ class=align-middle>Carlo Strapparava</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/g/gautam-naik/ class=align-middle>Gautam Naik</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/x/xinyi-wu/ class=align-middle>Xinyi Wu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/dirk-hovy/ class=align-middle>Dirk Hovy</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/laura-burdick/ class=align-middle>Laura Burdick</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/n/noriyuki-kojima/ class=align-middle>Noriyuki Kojima</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/acl/ class=align-middle>ACL</a><span class="badge badge-secondary align-middle ml-2">9</span></li><li class=list-group-item><a href=/venues/emnlp/ class=align-middle>EMNLP</a><span class="badge badge-secondary align-middle ml-2">5</span></li><li class=list-group-item><a href=/venues/ijcnlp/ class=align-middle>IJCNLP</a><span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/venues/coling/ class=align-middle>COLING</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/eacl/ class=align-middle>EACL</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-venues aria-expanded=false aria-controls=more-venues>show all...</li><div class="collapse border-top" id=more-venues><li class=list-group-item><a href=/venues/socialnlp/ class=align-middle>SocialNLP</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/sigdial/ class=align-middle>SIGDIAL</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/semeval/ class=align-middle>SemEval</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/clpsych/ class=align-middle>CLPsych</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/nlpcovid19/ class=align-middle>NLP-COVID19</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/conll/ class=align-middle>CoNLL</a><span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>