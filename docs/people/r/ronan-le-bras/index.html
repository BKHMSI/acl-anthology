<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Ronan Le Bras - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Ronan</span> <span class=font-weight-bold>Le Bras</span></h2><hr><div class=row><div class=col-lg-9><h4>2022</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.acl-long.225.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--acl-long--225 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.acl-long.225 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.acl-long.225" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2022.acl-long.225/>Generated Knowledge Prompting for Commonsense Reasoning</a></strong><br><a href=/people/j/jiacheng-liu/>Jiacheng Liu</a>
|
<a href=/people/a/alisa-liu/>Alisa Liu</a>
|
<a href=/people/x/ximing-lu/>Ximing Lu</a>
|
<a href=/people/s/sean-welleck/>Sean Welleck</a>
|
<a href=/people/p/peter-west/>Peter West</a>
|
<a href=/people/r/ronan-le-bras/>Ronan Le Bras</a>
|
<a href=/people/y/yejin-choi/>Yejin Choi</a>
|
<a href=/people/h/hannaneh-hajishirzi/>Hannaneh Hajishirzi</a><br><a href=/volumes/2022.acl-long/ class=text-muted>Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--acl-long--225><div class="card-body p-3 small">It remains an open question whether incorporating external knowledge benefits <a href=https://en.wikipedia.org/wiki/Commonsense_reasoning>commonsense reasoning</a> while maintaining the flexibility of pretrained sequence models To investigate this question we develop generated knowledge prompting which consists of generating knowledge from a <a href=https://en.wikipedia.org/wiki/Language_model>language model</a> then providing the knowledge as additional input when answering a question Our method does not require task specific supervision for knowledge integration or access to a structured knowledge base yet it improves performance of large scale state of the art models on four commonsense reasoning tasks achieving state of the art results on numerical commonsense NumerSense general commonsense CommonsenseQA 2.0 and scientific commonsense QASC benchmarks Generated knowledge prompting highlights large scale language models as flexible sources of external knowledge for improving <a href=https://en.wikipedia.org/wiki/Commonsense_reasoning>commonsense reasoning</a> Our code is available at \\url<url>github.com/liujch1998/GKP</url>\n</div></div><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.54.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--54 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.54 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.emnlp-main.54.Software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.54" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.54/>Moral Stories : Situated Reasoning about Norms, Intents, Actions, and their Consequences</a></strong><br><a href=/people/d/denis-emelin/>Denis Emelin</a>
|
<a href=/people/r/ronan-le-bras/>Ronan Le Bras</a>
|
<a href=/people/j/jena-d-hwang/>Jena D. Hwang</a>
|
<a href=/people/m/maxwell-forbes/>Maxwell Forbes</a>
|
<a href=/people/y/yejin-choi/>Yejin Choi</a><br><a href=/volumes/2021.emnlp-main/ class=text-muted>Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--54><div class="card-body p-3 small">In <a href=https://en.wikipedia.org/wiki/Social_environment>social settings</a>, much of human behavior is governed by <a href=https://en.wikipedia.org/wiki/Social_norm>unspoken rules of conduct</a> rooted in <a href=https://en.wikipedia.org/wiki/Social_norm>societal norms</a>. For artificial systems to be fully integrated into <a href=https://en.wikipedia.org/wiki/Social_environment>social environments</a>, adherence to such <a href=https://en.wikipedia.org/wiki/Social_norm>norms</a> is a central prerequisite. To investigate whether language generation models can serve as behavioral priors for systems deployed in social settings, we evaluate their ability to generate action descriptions that achieve predefined goals under normative constraints. Moreover, we examine if models can anticipate likely consequences of actions that either observe or violate known norms, or explain why certain actions are preferable by generating relevant norm hypotheses. For this purpose, we introduce Moral Stories, a crowd-sourced dataset of structured, branching narratives for the study of grounded, goal-oriented social reasoning. Finally, we propose decoding strategies that combine multiple expert models to significantly improve the quality of generated actions, consequences, and norms compared to strong baselines.</div></div><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.90.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--90 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.90 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.findings-emnlp.90.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38940138 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.90" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.90/>Generative Data Augmentation for Commonsense Reasoning</a></strong><br><a href=/people/y/yiben-yang/>Yiben Yang</a>
|
<a href=/people/c/chaitanya-malaviya/>Chaitanya Malaviya</a>
|
<a href=/people/j/jared-fernandez/>Jared Fernandez</a>
|
<a href=/people/s/swabha-swayamdipta/>Swabha Swayamdipta</a>
|
<a href=/people/r/ronan-le-bras/>Ronan Le Bras</a>
|
<a href=/people/j/ji-ping-wang/>Ji-Ping Wang</a>
|
<a href=/people/c/chandra-bhagavatula/>Chandra Bhagavatula</a>
|
<a href=/people/y/yejin-choi/>Yejin Choi</a>
|
<a href=/people/d/doug-downey/>Doug Downey</a><br><a href=/volumes/2020.findings-emnlp/ class=text-muted>Findings of the Association for Computational Linguistics: EMNLP 2020</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--90><div class="card-body p-3 small">Recent advances in <a href=https://en.wikipedia.org/wiki/Commonsense_reasoning>commonsense reasoning</a> depend on large-scale human-annotated training sets to achieve peak performance. However, manual curation of training sets is expensive and has been shown to introduce annotation artifacts that neural models can readily exploit and overfit to. We propose a novel generative data augmentation technique, G-DAUGC, that aims to achieve more accurate and robust learning in a low-resource setting. Our approach generates synthetic examples using pretrained language models and selects the most informative and diverse set of examples for <a href=https://en.wikipedia.org/wiki/Data_augmentation>data augmentation</a>. On experiments with multiple commonsense reasoning benchmarks, G-DAUGC consistently outperforms existing data augmentation methods based on back-translation, establishing a new state-of-the-art on WinoGrande, <a href=https://en.wikipedia.org/wiki/CODAH>CODAH</a>, and CommonsenseQA, as well as enhances out-of-distribution generalization, proving to be robust against adversaries or perturbations. Our analysis demonstrates that G-DAUGC produces a diverse set of fluent training examples, and that its selection and training approaches are important for performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.418.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--418 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.418 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38940700 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.418" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.418/>Thinking Like a Skeptic : Defeasible Inference in Natural Language</a></strong><br><a href=/people/r/rachel-rudinger/>Rachel Rudinger</a>
|
<a href=/people/v/vered-shwartz/>Vered Shwartz</a>
|
<a href=/people/j/jena-d-hwang/>Jena D. Hwang</a>
|
<a href=/people/c/chandra-bhagavatula/>Chandra Bhagavatula</a>
|
<a href=/people/m/maxwell-forbes/>Maxwell Forbes</a>
|
<a href=/people/r/ronan-le-bras/>Ronan Le Bras</a>
|
<a href=/people/n/noah-a-smith/>Noah A. Smith</a>
|
<a href=/people/y/yejin-choi/>Yejin Choi</a><br><a href=/volumes/2020.findings-emnlp/ class=text-muted>Findings of the Association for Computational Linguistics: EMNLP 2020</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--418><div class="card-body p-3 small">Defeasible inference is a mode of reasoning in which an inference (X is a bird, therefore X flies) may be weakened or overturned in light of new evidence (X is a penguin). Though long recognized in classical AI and philosophy, defeasible inference has not been extensively studied in the context of contemporary data-driven research on <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language inference</a> and <a href=https://en.wikipedia.org/wiki/Commonsense_reasoning>commonsense reasoning</a>. We introduce Defeasible NLI (abbreviated -NLI), a <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> for defeasible inference in <a href=https://en.wikipedia.org/wiki/Natural_language>natural language</a>. Defeasible NLI contains extensions to three existing inference datasets covering diverse modes of reasoning : <a href=https://en.wikipedia.org/wiki/Common_sense>common sense</a>, natural language inference, and <a href=https://en.wikipedia.org/wiki/Social_norm>social norms</a>. From Defeasible NLI, we develop both a classification and generation task for defeasible inference, and demonstrate that the generation task is much more challenging. Despite lagging human performance, however, <a href=https://en.wikipedia.org/wiki/Generative_model>generative models</a> trained on this <a href=https://en.wikipedia.org/wiki/Data>data</a> are capable of writing sentences that weaken or strengthen a specified <a href=https://en.wikipedia.org/wiki/Inference>inference</a> up to 68 % of the time.<tex-math>\\delta</tex-math>-NLI), a dataset for defeasible inference in natural language. Defeasible NLI contains extensions to three existing inference datasets covering diverse modes of reasoning: common sense, natural language inference, and social norms. From Defeasible NLI, we develop both a classification and generation task for defeasible inference, and demonstrate that the generation task is much more challenging. Despite lagging human performance, however, generative models trained on this data are capable of writing sentences that weaken or strengthen a specified inference up to 68% of the time.</div></div><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1083.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1083 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1083 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1083/>Beyond Sentential Semantic Parsing : Tackling the Math SAT with a Cascade of Tree Transducers<span class=acl-fixed-case>SAT</span> with a Cascade of Tree Transducers</a></strong><br><a href=/people/m/mark-hopkins/>Mark Hopkins</a>
|
<a href=/people/c/cristian-petrescu-prahova/>Cristian Petrescu-Prahova</a>
|
<a href=/people/r/roie-levin/>Roie Levin</a>
|
<a href=/people/r/ronan-le-bras/>Ronan Le Bras</a>
|
<a href=/people/a/alvaro-herrasti/>Alvaro Herrasti</a>
|
<a href=/people/v/vidur-joshi/>Vidur Joshi</a><br><a href=/volumes/D17-1/ class=text-muted>Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1083><div class="card-body p-3 small">We present an approach for answering questions that span multiple sentences and exhibit sophisticated cross-sentence anaphoric phenomena, evaluating on a rich source of such questions the math portion of the Scholastic Aptitude Test (SAT). By using a tree transducer cascade as its basic architecture, our <a href=https://en.wikipedia.org/wiki/System>system</a> propagates uncertainty from multiple sources (e.g. coreference resolution or verb interpretation) until it can be confidently resolved. Experiments show the first-ever results 43 % <a href=https://en.wikipedia.org/wiki/Recall_(memory)>recall</a> and 91 % <a href=https://en.wikipedia.org/wiki/Precision_(computer_science)>precision</a>) on SAT algebra word problems. We also apply our system to the public Dolphin algebra question set, and improve the state-of-the-art F1-score from 73.9 % to 77.0 %.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Ronan+Le+Bras" title="Search for 'Ronan Le Bras' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/y/yejin-choi/ class=align-middle>Yejin Choi</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/people/j/jena-d-hwang/ class=align-middle>Jena D. Hwang</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/m/maxwell-forbes/ class=align-middle>Maxwell Forbes</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/c/chandra-bhagavatula/ class=align-middle>Chandra Bhagavatula</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/j/jiacheng-liu/ class=align-middle>Jiacheng Liu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/a/alisa-liu/ class=align-middle>Alisa Liu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/x/ximing-lu/ class=align-middle>Ximing Lu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/sean-welleck/ class=align-middle>Sean Welleck</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/p/peter-west/ class=align-middle>Peter West</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/hannaneh-hajishirzi/ class=align-middle>Hannaneh Hajishirzi</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/denis-emelin/ class=align-middle>Denis Emelin</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/mark-hopkins/ class=align-middle>Mark Hopkins</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/cristian-petrescu-prahova/ class=align-middle>Cristian Petrescu-Prahova</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/roie-levin/ class=align-middle>Roie Levin</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/alvaro-herrasti/ class=align-middle>Alvaro Herrasti</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/v/vidur-joshi/ class=align-middle>Vidur Joshi</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yiben-yang/ class=align-middle>Yiben Yang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/chaitanya-malaviya/ class=align-middle>Chaitanya Malaviya</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jared-fernandez/ class=align-middle>Jared Fernandez</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/swabha-swayamdipta/ class=align-middle>Swabha Swayamdipta</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/ji-ping-wang/ class=align-middle>Ji-Ping Wang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/doug-downey/ class=align-middle>Doug Downey</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/rachel-rudinger/ class=align-middle>Rachel Rudinger</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/v/vered-shwartz/ class=align-middle>Vered Shwartz</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/n/noah-a-smith/ class=align-middle>Noah A. Smith</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/emnlp/ class=align-middle>EMNLP</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/findings/ class=align-middle>Findings</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/acl/ class=align-middle>ACL</a><span class="badge badge-secondary align-middle ml-2">1</span></li></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>