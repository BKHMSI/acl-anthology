<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Regina Barzilay - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Regina</span> <span class=font-weight-bold>Barzilay</span></h2><hr><div class=row><div class=col-lg-9><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.tacl-1.5.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--tacl-1--5 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.tacl-1.5 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.tacl-1.5/>Deciphering Undersegmented Ancient Scripts Using Phonetic Prior</a></strong><br><a href=/people/j/jiaming-luo/>Jiaming Luo</a>
|
<a href=/people/f/frederik-hartmann/>Frederik Hartmann</a>
|
<a href=/people/e/enrico-santus/>Enrico Santus</a>
|
<a href=/people/r/regina-barzilay/>Regina Barzilay</a>
|
<a href=/people/y/yuan-cao/>Yuan Cao</a><br><a href=/volumes/2021.tacl-1/ class=text-muted>Transactions of the Association for Computational Linguistics, Volume 9</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--tacl-1--5><div class="card-body p-3 small">Most undeciphered lost languages exhibit two characteristics that pose significant decipherment challenges : (1) the scripts are not fully segmented into words ; (2) the closest known language is not determined. We propose a decipherment model that handles both of these challenges by building on rich linguistic constraints reflecting consistent patterns in historical sound change. We capture the natural phonological geometry by learning <a href=https://en.wikipedia.org/wiki/Character_encoding>character embeddings</a> based on the <a href=https://en.wikipedia.org/wiki/International_Phonetic_Alphabet>International Phonetic Alphabet (IPA)</a>. The resulting <a href=https://en.wikipedia.org/wiki/Generative_model>generative framework</a> jointly models <a href=https://en.wikipedia.org/wiki/Word_segmentation>word segmentation</a> and cognate alignment, informed by <a href=https://en.wikipedia.org/wiki/Phonology>phonological constraints</a>. We evaluate the model on both deciphered languages (Gothic, Ugaritic) and an undeciphered one (Iberian). The experiments show that incorporating phonetic geometry leads to clear and consistent gains. Additionally, we propose a measure for language closeness which correctly identifies related languages for <a href=https://en.wikipedia.org/wiki/Gothic_language>Gothic</a> and <a href=https://en.wikipedia.org/wiki/Ugaritic>Ugaritic</a>. For <a href=https://en.wikipedia.org/wiki/Iberian_language>Iberian</a>, the method does not show strong evidence supporting <a href=https://en.wikipedia.org/wiki/Basque_language>Basque</a> as a related language, concurring with the favored position by the current scholarship.1</div></div><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.420.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--420 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.420 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939329 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.420" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.420/>Blank Language Models</a></strong><br><a href=/people/t/tianxiao-shen/>Tianxiao Shen</a>
|
<a href=/people/v/victor-quach/>Victor Quach</a>
|
<a href=/people/r/regina-barzilay/>Regina Barzilay</a>
|
<a href=/people/t/tommi-jaakkola/>Tommi Jaakkola</a><br><a href=/volumes/2020.emnlp-main/ class=text-muted>Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--420><div class="card-body p-3 small">We propose Blank Language Model (BLM), a model that generates sequences by dynamically creating and filling in blanks. The blanks control which part of the sequence to expand, making BLM ideal for a variety of text editing and rewriting tasks. The <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> can start from a single blank or partially completed text with blanks at specified locations. It iteratively determines which word to place in a blank and whether to insert new blanks, and stops generating when no blanks are left to fill. BLM can be efficiently trained using a <a href=https://en.wikipedia.org/wiki/Upper_and_lower_bounds>lower bound</a> of the <a href=https://en.wikipedia.org/wiki/Marginal_distribution>marginal data likelihood</a>. On the task of filling missing text snippets, BLM significantly outperforms all other <a href=https://en.wikipedia.org/wiki/Baseline_(medicine)>baselines</a> in terms of both <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> and <a href=https://en.wikipedia.org/wiki/Fluency>fluency</a>. Experiments on style transfer and damaged ancient text restoration demonstrate the potential of this <a href=https://en.wikipedia.org/wiki/Conceptual_framework>framework</a> for a wide range of <a href=https://en.wikipedia.org/wiki/Application_software>applications</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.cl-2.8.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--cl-2--8 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.cl-2.8 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.cl-2.8/>The Limitations of <a href=https://en.wikipedia.org/wiki/Stylometry>Stylometry</a> for Detecting Machine-Generated Fake News</a></strong><br><a href=/people/t/tal-schuster/>Tal Schuster</a>
|
<a href=/people/r/roei-schuster/>Roei Schuster</a>
|
<a href=/people/d/darsh-j-shah/>Darsh J. Shah</a>
|
<a href=/people/r/regina-barzilay/>Regina Barzilay</a><br><a href=/volumes/2020.cl-2/ class=text-muted>Computational Linguistics, Volume 46, Issue 2 - June 2020</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--cl-2--8><div class="card-body p-3 small">Recent developments in neural language models (LMs) have raised concerns about their potential misuse for automatically spreading misinformation. In light of these concerns, several studies have proposed to detect machine-generated fake news by capturing their stylistic differences from human-written text. These approaches, broadly termed <a href=https://en.wikipedia.org/wiki/Stylometry>stylometry</a>, have found success in source attribution and misinformation detection in human-written texts. However, in this work, we show that <a href=https://en.wikipedia.org/wiki/Stylometry>stylometry</a> is limited against machine-generated misinformation. Whereas humans speak differently when trying to deceive, LMs generate stylistically consistent text, regardless of underlying motive. Thus, though <a href=https://en.wikipedia.org/wiki/Stylometry>stylometry</a> can successfully prevent <a href=https://en.wikipedia.org/wiki/Impersonator>impersonation</a> by identifying text provenance, it fails to distinguish legitimate LM applications from those that introduce false information. We create two <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmarks</a> demonstrating the stylistic similarity between malicious and legitimate uses of LMs, utilized in auto-completion and editing-assistance settings.1 Our findings highlight the need for non-stylometry approaches in detecting machine-generated misinformation, and open up the discussion on the desired evaluation benchmarks.</div></div><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1162.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1162 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1162 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/364708233 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1162" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1162/>Cross-Lingual Alignment of Contextual Word Embeddings, with Applications to Zero-shot Dependency Parsing</a></strong><br><a href=/people/t/tal-schuster/>Tal Schuster</a>
|
<a href=/people/o/ori-ram/>Ori Ram</a>
|
<a href=/people/r/regina-barzilay/>Regina Barzilay</a>
|
<a href=/people/a/amir-globerson/>Amir Globerson</a><br><a href=/volumes/N19-1/ class=text-muted>Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1162><div class="card-body p-3 small">We introduce a novel method for multilingual transfer that utilizes deep contextual embeddings, pretrained in an unsupervised fashion. While contextual embeddings have been shown to yield richer representations of meaning compared to their static counterparts, aligning them poses a challenge due to their dynamic nature. To this end, we construct context-independent variants of the original monolingual spaces and utilize their <a href=https://en.wikipedia.org/wiki/Map_(mathematics)>mapping</a> to derive an <a href=https://en.wikipedia.org/wiki/Sequence_alignment>alignment</a> for the context-dependent spaces. This <a href=https://en.wikipedia.org/wiki/Map_(mathematics)>mapping</a> readily supports processing of a target language, improving <a href=https://en.wikipedia.org/wiki/Language_transfer>transfer</a> by context-aware embeddings. Our experimental results demonstrate the effectiveness of this approach for zero-shot and few-shot learning of dependency parsing. Specifically, our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> consistently outperforms the previous <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a> on 6 tested languages, yielding an improvement of 6.8 LAS points on average.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1303.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1303 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1303 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1303" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-1303/>Neural Decipherment via Minimum-Cost Flow : From Ugaritic to Linear B<span class=acl-fixed-case>U</span>garitic to <span class=acl-fixed-case>L</span>inear <span class=acl-fixed-case>B</span></a></strong><br><a href=/people/j/jiaming-luo/>Jiaming Luo</a>
|
<a href=/people/y/yuan-cao/>Yuan Cao</a>
|
<a href=/people/r/regina-barzilay/>Regina Barzilay</a><br><a href=/volumes/P19-1/ class=text-muted>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1303><div class="card-body p-3 small">In this paper we propose a novel neural approach for automatic decipherment of lost languages. To compensate for the lack of strong supervision signal, our model design is informed by patterns in language change documented in <a href=https://en.wikipedia.org/wiki/Historical_linguistics>historical linguistics</a>. The <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> utilizes an expressive sequence-to-sequence model to capture character-level correspondences between cognates. To effectively train the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> in unsupervised manner, we innovate the <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training procedure</a> by formalizing it as a minimum-cost flow problem. When applied to decipherment of Ugaritic, we achieve 5 % absolute improvement over state-of-the-art results. We also report first automatic results in deciphering Linear B, a <a href=https://en.wikipedia.org/wiki/Syllabary>syllabic language</a> related to <a href=https://en.wikipedia.org/wiki/Ancient_Greek>ancient Greek</a>, where our <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> correctly translates 67.3 % of <a href=https://en.wikipedia.org/wiki/Cognate>cognates</a>.</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1216.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1216 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1216 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D18-1216.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/305661928 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1216" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1216/>Deriving Machine Attention from Human Rationales</a></strong><br><a href=/people/y/yujia-bao/>Yujia Bao</a>
|
<a href=/people/s/shiyu-chang/>Shiyu Chang</a>
|
<a href=/people/m/mo-yu/>Mo Yu</a>
|
<a href=/people/r/regina-barzilay/>Regina Barzilay</a><br><a href=/volumes/D18-1/ class=text-muted>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1216><div class="card-body p-3 small">Attention-based models are successful when trained on large amounts of data. In this paper, we demonstrate that even in the low-resource scenario, <a href=https://en.wikipedia.org/wiki/Attention>attention</a> can be learned effectively. To this end, we start with discrete human-annotated rationales and map them into continuous attention. Our central hypothesis is that this <a href=https://en.wikipedia.org/wiki/Map_(mathematics)>mapping</a> is general across domains, and thus can be transferred from resource-rich domains to low-resource ones. Our model jointly learns a domain-invariant representation and induces the desired mapping between rationales and <a href=https://en.wikipedia.org/wiki/Attention>attention</a>. Our empirical results validate this hypothesis and show that our approach delivers significant gains over state-of-the-art baselines, yielding over 15 % average error reduction on benchmark datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1498.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1498 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1498 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1498" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D18-1498/>Multi-Source Domain Adaptation with Mixture of Experts</a></strong><br><a href=/people/j/jiang-guo/>Jiang Guo</a>
|
<a href=/people/d/darsh-shah/>Darsh Shah</a>
|
<a href=/people/r/regina-barzilay/>Regina Barzilay</a><br><a href=/volumes/D18-1/ class=text-muted>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1498><div class="card-body p-3 small">We propose a mixture-of-experts approach for unsupervised domain adaptation from multiple sources. The key idea is to explicitly capture the relationship between a target example and different source domains. This relationship, expressed by a <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>point-to-set metric</a>, determines how to combine predictors trained on various domains. The <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metric</a> is learned in an unsupervised fashion using meta-training. Experimental results on <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> and <a href=https://en.wikipedia.org/wiki/Part-of-speech_tagging>part-of-speech tagging</a> demonstrate that our approach consistently outperforms multiple baselines and can robustly handle negative transfer.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/Q18-1004.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-Q18-1004 data-toggle=collapse aria-expanded=false aria-controls=abstract-Q18-1004 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285802158 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=Q18-1004" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/Q18-1004/>Representation Learning for Grounded Spatial Reasoning</a></strong><br><a href=/people/m/michael-janner/>Michael Janner</a>
|
<a href=/people/k/karthik-narasimhan/>Karthik Narasimhan</a>
|
<a href=/people/r/regina-barzilay/>Regina Barzilay</a><br><a href=/volumes/Q18-1/ class=text-muted>Transactions of the Association for Computational Linguistics, Volume 6</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-Q18-1004><div class="card-body p-3 small">The interpretation of spatial references is highly contextual, requiring <a href=https://en.wikipedia.org/wiki/Bayesian_inference>joint inference</a> over both language and the environment. We consider the task of <a href=https://en.wikipedia.org/wiki/Spatial&#8211;temporal_reasoning>spatial reasoning</a> in a <a href=https://en.wikipedia.org/wiki/Simulation>simulated environment</a>, where an agent can act and receive rewards. The proposed <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> learns a representation of the world steered by instruction text. This design allows for precise alignment of local neighborhoods with corresponding verbalizations, while also handling global references in the instructions. We train our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> with <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning</a> using a variant of generalized value iteration. The <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms state-of-the-art approaches on several metrics, yielding a 45 % reduction in goal localization error.</div></div><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1000.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-1000/>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></strong><br><a href=/people/r/regina-barzilay/>Regina Barzilay</a>
|
<a href=/people/m/min-yen-kan/>Min-Yen Kan</a><br><a href=/volumes/P17-1/ class=text-muted>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2000.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-2000/>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</a></strong><br><a href=/people/r/regina-barzilay/>Regina Barzilay</a>
|
<a href=/people/m/min-yen-kan/>Min-Yen Kan</a><br><a href=/volumes/P17-2/ class=text-muted>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/Q17-1025.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-Q17-1025 data-toggle=collapse aria-expanded=false aria-controls=abstract-Q17-1025 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234952859 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/Q17-1025/>Unsupervised Learning of Morphological Forests</a></strong><br><a href=/people/j/jiaming-luo/>Jiaming Luo</a>
|
<a href=/people/k/karthik-narasimhan/>Karthik Narasimhan</a>
|
<a href=/people/r/regina-barzilay/>Regina Barzilay</a><br><a href=/volumes/Q17-1/ class=text-muted>Transactions of the Association for Computational Linguistics, Volume 5</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-Q17-1025><div class="card-body p-3 small">This paper focuses on unsupervised modeling of morphological families, collectively comprising a forest over the language vocabulary. This formulation enables us to capture edge-wise properties reflecting single-step morphological derivations, along with global distributional properties of the entire <a href=https://en.wikipedia.org/wiki/Forest>forest</a>. These global properties constrain the size of the affix set and encourage formation of tight morphological families. The resulting <a href=https://en.wikipedia.org/wiki/Loss_function>objective</a> is solved using Integer Linear Programming (ILP) paired with contrastive estimation. We train the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> by alternating between optimizing the local log-linear model and the global ILP objective. We evaluate our system on three tasks : <a href=https://en.wikipedia.org/wiki/Root-finding_algorithm>root detection</a>, clustering of morphological families, and <a href=https://en.wikipedia.org/wiki/Segmentation_(biology)>segmentation</a>. Our experiments demonstrate that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> yields consistent gains in all three <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> compared with the best published results.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/Q17-1036.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-Q17-1036 data-toggle=collapse aria-expanded=false aria-controls=abstract-Q17-1036 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/276406923 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=Q17-1036" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/Q17-1036/>Aspect-augmented Adversarial Networks for Domain Adaptation</a></strong><br><a href=/people/y/yuan-zhang/>Yuan Zhang</a>
|
<a href=/people/r/regina-barzilay/>Regina Barzilay</a>
|
<a href=/people/t/tommi-jaakkola/>Tommi Jaakkola</a><br><a href=/volumes/Q17-1/ class=text-muted>Transactions of the Association for Computational Linguistics, Volume 5</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-Q17-1036><div class="card-body p-3 small">We introduce a <a href=https://en.wikipedia.org/wiki/Artificial_neural_network>neural method</a> for <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a> between two (source and target) <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification tasks</a> or aspects over the same domain. Rather than training on target labels, we use a few <a href=https://en.wikipedia.org/wiki/Index_term>keywords</a> pertaining to source and target aspects indicating sentence relevance instead of document class labels. Documents are encoded by learning to embed and softly select relevant sentences in an aspect-dependent manner. A shared classifier is trained on the source encoded documents and labels, and applied to target encoded documents. We ensure transfer through aspect-adversarial training so that encoded documents are, as sets, aspect-invariant. Experimental results demonstrate that our approach outperforms different baselines and model variants on two <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>, yielding an improvement of 27 % on a pathology dataset and 5 % on a review dataset.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Regina+Barzilay" title="Search for 'Regina Barzilay' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/j/jiaming-luo/ class=align-middle>Jiaming Luo</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/t/tommi-jaakkola/ class=align-middle>Tommi Jaakkola</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/m/min-yen-kan/ class=align-middle>Min-Yen Kan</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/k/karthik-narasimhan/ class=align-middle>Karthik Narasimhan</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/t/tal-schuster/ class=align-middle>Tal Schuster</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/y/yuan-cao/ class=align-middle>Yuan Cao</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/t/tianxiao-shen/ class=align-middle>Tianxiao Shen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/v/victor-quach/ class=align-middle>Victor Quach</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yuan-zhang/ class=align-middle>Yuan Zhang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yujia-bao/ class=align-middle>Yujia Bao</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/shiyu-chang/ class=align-middle>Shiyu Chang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/mo-yu/ class=align-middle>Mo Yu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jiang-guo/ class=align-middle>Jiang Guo</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/darsh-shah/ class=align-middle>Darsh Shah</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/michael-janner/ class=align-middle>Michael Janner</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/o/ori-ram/ class=align-middle>Ori Ram</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/amir-globerson/ class=align-middle>Amir Globerson</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/f/frederik-hartmann/ class=align-middle>Frederik Hartmann</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/e/enrico-santus/ class=align-middle>Enrico Santus</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/roei-schuster/ class=align-middle>Roei Schuster</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/darsh-j-shah/ class=align-middle>Darsh J. Shah</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/tacl/ class=align-middle>TACL</a><span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/venues/emnlp/ class=align-middle>EMNLP</a><span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/venues/acl/ class=align-middle>ACL</a><span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/venues/naacl/ class=align-middle>NAACL</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/cl/ class=align-middle>CL</a><span class="badge badge-secondary align-middle ml-2">1</span></li></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>