<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Ryan Cotterell - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Ryan</span> <span class=font-weight-bold>Cotterell</span></h2><hr><div class=row><div class=col-lg-9><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.404.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--404 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.404 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.404/>A Cognitive Regularizer for <a href=https://en.wikipedia.org/wiki/Language_model>Language Modeling</a></a></strong><br><a href=/people/j/jason-wei/>Jason Wei</a>
|
<a href=/people/c/clara-meister/>Clara Meister</a>
|
<a href=/people/r/ryan-cotterell/>Ryan Cotterell</a><br><a href=/volumes/2021.acl-long/ class=text-muted>Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--404><div class="card-body p-3 small">The uniform information density (UID) hypothesis, which posits that speakers behaving optimally tend to distribute information uniformly across a linguistic signal, has gained traction in <a href=https://en.wikipedia.org/wiki/Psycholinguistics>psycholinguistics</a> as an explanation for certain syntactic, morphological, and prosodic choices. In this work, we explore whether the UID hypothesis can be operationalized as an <a href=https://en.wikipedia.org/wiki/Inductive_bias>inductive bias</a> for <a href=https://en.wikipedia.org/wiki/Language_model>statistical language modeling</a>. Specifically, we augment the canonical MLE objective for training language models with a <a href=https://en.wikipedia.org/wiki/Regularization_(mathematics)>regularizer</a> that encodes UID. In experiments on ten languages spanning five language families, we find that using UID regularization consistently improves <a href=https://en.wikipedia.org/wiki/Perplexity>perplexity</a> in language models, having a larger effect when training data is limited. Moreover, via an analysis of generated sequences, we find that UID-regularized language models have other desirable properties, e.g., they generate text that is more lexically diverse. Our results not only suggest that UID is a reasonable <a href=https://en.wikipedia.org/wiki/Inductive_bias>inductive bias</a> for <a href=https://en.wikipedia.org/wiki/Language_model>language modeling</a>, but also provide an alternative validation of the UID hypothesis using modern-day NLP tools.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.414.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--414 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.414 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.acl-long.414.OptionalSupplementaryMaterial.pdf data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2021.acl-long.414/>Language Model Evaluation Beyond Perplexity</a></strong><br><a href=/people/c/clara-meister/>Clara Meister</a>
|
<a href=/people/r/ryan-cotterell/>Ryan Cotterell</a><br><a href=/volumes/2021.acl-long/ class=text-muted>Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--414><div class="card-body p-3 small">We propose an alternate approach to quantifying how well language models learn <a href=https://en.wikipedia.org/wiki/Natural_language>natural language</a> : we ask how well they match the statistical tendencies of <a href=https://en.wikipedia.org/wiki/Natural_language>natural language</a>. To answer this question, we analyze whether text generated from language models exhibits the statistical tendencies present in the human-generated text on which they were trained. We provide a frameworkpaired with significance testsfor evaluating the fit of <a href=https://en.wikipedia.org/wiki/Language_model>language models</a> to these trends. We find that neural language models appear to learn only a subset of the tendencies considered, but align much more closely with empirical trends than proposed theoretical distributions (when present). Further, the fit to different <a href=https://en.wikipedia.org/wiki/Probability_distribution>distributions</a> is highly-dependent on both model architecture and generation strategy. As concrete examples, text generated under the nucleus sampling scheme adheres more closely to the typetoken relationship of natural language than text produced using standard ancestral sampling ; text from LSTMs reflects the natural language distributions over length, stopwords, and symbols surprisingly well.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-short.32.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-short--32 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-short.32 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-short.32/>Higher-order Derivatives of Weighted Finite-state Machines</a></strong><br><a href=/people/r/ran-zmigrod/>Ran Zmigrod</a>
|
<a href=/people/t/tim-vieira/>Tim Vieira</a>
|
<a href=/people/r/ryan-cotterell/>Ryan Cotterell</a><br><a href=/volumes/2021.acl-short/ class=text-muted>Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-short--32><div class="card-body p-3 small">Weighted finite-state machines are a fundamental building block of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP systems</a>. They have withstood the test of timefrom their early use in noisy channel models in the 1990s up to modern-day neurally parameterized conditional random fields. This work examines the computation of higher-order derivatives with respect to the <a href=https://en.wikipedia.org/wiki/Normalization_constant>normalization constant</a> for weighted finite-state machines. We provide a general <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> for evaluating <a href=https://en.wikipedia.org/wiki/Derivative_(finance)>derivatives</a> of all orders, which has not been previously described in the literature. In the case of second-order derivatives, our scheme runs in the optimal O(A2 N4) time where A is the alphabet size and N is the number of states. Our <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> is significantly faster than prior algorithms. Additionally, our approach leads to a significantly faster <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> for computing second-order expectations, such as <a href=https://en.wikipedia.org/wiki/Covariance_matrix>covariance matrices</a> and gradients of first-order expectations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.3.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--3 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.3 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.eacl-main.3" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.3/>Disambiguatory Signals are Stronger in Word-initial Positions</a></strong><br><a href=/people/t/tiago-pimentel/>Tiago Pimentel</a>
|
<a href=/people/r/ryan-cotterell/>Ryan Cotterell</a>
|
<a href=/people/b/brian-roark/>Brian Roark</a><br><a href=/volumes/2021.eacl-main/ class=text-muted>Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--3><div class="card-body p-3 small">Psycholinguistic studies of human word processing and <a href=https://en.wikipedia.org/wiki/Lexical_access>lexical access</a> provide ample evidence of the preferred nature of word-initial versus word-final segments, e.g., in terms of attention paid by listeners (greater) or the likelihood of reduction by speakers (lower). This has led to the conjectureas in Wedel et al. (2019b), but common elsewherethat languages have evolved to provide more information earlier in words than later. Information-theoretic methods to establish such tendencies in lexicons have suffered from several methodological shortcomings that leave open the question of whether this high word-initial informativeness is actually a property of the lexicon or simply an artefact of the incremental nature of recognition. In this paper, we point out the confounds in existing methods for comparing the informativeness of segments early in the word versus later in the word, and present several new measures that avoid these confounds. When controlling for these confounds, we still find evidence across hundreds of languages that indeed there is a cross-linguistic tendency to front-load information in words.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.118.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--118 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.118 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.118/>Searching for Search Errors in Neural Morphological Inflection</a></strong><br><a href=/people/m/martina-forster/>Martina Forster</a>
|
<a href=/people/c/clara-meister/>Clara Meister</a>
|
<a href=/people/r/ryan-cotterell/>Ryan Cotterell</a><br><a href=/volumes/2021.eacl-main/ class=text-muted>Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--118><div class="card-body p-3 small">Neural sequence-to-sequence models are currently the predominant choice for language generation tasks. Yet, on word-level tasks, exact inference of these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> reveals the empty string is often the global optimum. Prior works have speculated this phenomenon is a result of the inadequacy of neural models for <a href=https://en.wikipedia.org/wiki/Natural_language_processing>language generation</a>. However, in the case of morphological inflection, we find that the empty string is almost never the most probable solution under the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>. Further, <a href=https://en.wikipedia.org/wiki/Greedy_search>greedy search</a> often finds the <a href=https://en.wikipedia.org/wiki/Maxima_and_minima>global optimum</a>. These observations suggest that the poor calibration of many neural models may stem from characteristics of a specific subset of tasks rather than general ill-suitedness of such <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> for <a href=https://en.wikipedia.org/wiki/Natural_language_processing>language generation</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.sigmorphon-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.sigmorphon-1.0/>Proceedings of the 18th SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology</a></strong><br><a href=/people/g/garrett-nicolai/>Garrett Nicolai</a>
|
<a href=/people/k/kyle-gorman/>Kyle Gorman</a>
|
<a href=/people/r/ryan-cotterell/>Ryan Cotterell</a><br><a href=/volumes/2021.sigmorphon-1/ class=text-muted>Proceedings of the 18th SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.52.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--52 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.52 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.52/>Conditional Poisson Stochastic Beams<span class=acl-fixed-case>P</span>oisson Stochastic Beams</a></strong><br><a href=/people/c/clara-meister/>Clara Meister</a>
|
<a href=/people/a/afra-amini/>Afra Amini</a>
|
<a href=/people/t/tim-vieira/>Tim Vieira</a>
|
<a href=/people/r/ryan-cotterell/>Ryan Cotterell</a><br><a href=/volumes/2021.emnlp-main/ class=text-muted>Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--52><div class="card-body p-3 small">Beam search is the default decoding strategy for many sequence generation tasks in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a>. The set of approximate K-best items returned by the <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> is a useful summary of the distribution for many applications ; however, the candidates typically exhibit high overlap and may give a highly biased estimate for expectations under our model. These problems can be addressed by instead using stochastic decoding strategies. In this work, we propose a new method for turning <a href=https://en.wikipedia.org/wiki/Beam_search>beam search</a> into a <a href=https://en.wikipedia.org/wiki/Stochastic_process>stochastic process</a> : Conditional Poisson stochastic beam search. Rather than taking the maximizing set at each iteration, we sample K candidates without replacement according to the conditional Poisson sampling design. We view this as a more natural alternative to Kool et al. (2019)&#8217;s stochastic beam search (SBS). Furthermore, we show how samples generated under the CPSBS design can be used to build consistent estimators and sample diverse sets from sequence models. In our experiments, we observe CPSBS produces lower variance and more efficient estimators than SBS, even showing improvements in high entropy settings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.74.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--74 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.74 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.74/>Revisiting the Uniform Information Density Hypothesis<span class=acl-fixed-case>U</span>niform <span class=acl-fixed-case>I</span>nformation <span class=acl-fixed-case>D</span>ensity Hypothesis</a></strong><br><a href=/people/c/clara-meister/>Clara Meister</a>
|
<a href=/people/t/tiago-pimentel/>Tiago Pimentel</a>
|
<a href=/people/p/patrick-haller/>Patrick Haller</a>
|
<a href=/people/l/lena-jager/>Lena Jäger</a>
|
<a href=/people/r/ryan-cotterell/>Ryan Cotterell</a>
|
<a href=/people/r/roger-levy/>Roger Levy</a><br><a href=/volumes/2021.emnlp-main/ class=text-muted>Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--74><div class="card-body p-3 small">The uniform information density (UID) hypothesis posits a preference among language users for utterances structured such that information is distributed uniformly across a signal. While its implications on <a href=https://en.wikipedia.org/wiki/Language_production>language production</a> have been well explored, the <a href=https://en.wikipedia.org/wiki/Hypothesis>hypothesis</a> potentially makes predictions about <a href=https://en.wikipedia.org/wiki/Sentence_processing>language comprehension</a> and <a href=https://en.wikipedia.org/wiki/Linguistic_prescription>linguistic acceptability</a> as well. Further, it is unclear how uniformity in a linguistic signalor lack thereofshould be measured, and over which linguistic unit, e.g., the sentence or language level, this uniformity should hold. Here we investigate these facets of the UID hypothesis using reading time and acceptability data. While our reading time results are generally consistent with previous work, they are also consistent with a weakly super-linear effect of surprisal, which would be compatible with UID&#8217;s predictions. For <a href=https://en.wikipedia.org/wiki/Acceptability>acceptability judgments</a>, we find clearer evidence that non-uniformity in <a href=https://en.wikipedia.org/wiki/Information_density>information density</a> is predictive of lower <a href=https://en.wikipedia.org/wiki/Acceptability>acceptability</a>. We then explore multiple operationalizations of UID, motivated by different interpretations of the original hypothesis, and analyze the scope over which the pressure towards uniformity is exerted. The explanatory power of a subset of the proposed operationalizations suggests that the strongest trend may be a regression towards a mean surprisal across the language, rather than the phrase, sentence, or documenta finding that supports a typical interpretation of UID, namely that it is the byproduct of language users maximizing the use of a (hypothetical) communication channel.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.229.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--229 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.229 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.229" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.229/>A Bayesian Framework for Information-Theoretic Probing<span class=acl-fixed-case>B</span>ayesian Framework for Information-Theoretic Probing</a></strong><br><a href=/people/t/tiago-pimentel/>Tiago Pimentel</a>
|
<a href=/people/r/ryan-cotterell/>Ryan Cotterell</a><br><a href=/volumes/2021.emnlp-main/ class=text-muted>Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--229><div class="card-body p-3 small">Pimentel et al. (2020) recently analysed probing from an information-theoretic perspective. They argue that <a href=https://en.wikipedia.org/wiki/Probing>probing</a> should be seen as approximating a <a href=https://en.wikipedia.org/wiki/Mutual_information>mutual information</a>. This led to the rather unintuitive conclusion that representations encode exactly the same information about a target task as the original sentences. The <a href=https://en.wikipedia.org/wiki/Mutual_information>mutual information</a>, however, assumes the true <a href=https://en.wikipedia.org/wiki/Probability_distribution>probability distribution</a> of a pair of <a href=https://en.wikipedia.org/wiki/Random_variable>random variables</a> is known, leading to unintuitive results in settings where it is not. This paper proposes a new framework to measure what we term Bayesian mutual information, which analyses information from the perspective of Bayesian agentsallowing for more intuitive findings in scenarios with finite data. For instance, under Bayesian MI we have that data can add information, processing can help, and <a href=https://en.wikipedia.org/wiki/Information>information</a> can hurt, which makes it more intuitive for machine learning applications. Finally, we apply our framework to probing where we believe Bayesian mutual information naturally operationalises ease of extraction by explicitly limiting the available background knowledge to solve a task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.824.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--824 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.824 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.824" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.824/>Efficient Sampling of Dependency Structure</a></strong><br><a href=/people/r/ran-zmigrod/>Ran Zmigrod</a>
|
<a href=/people/t/tim-vieira/>Tim Vieira</a>
|
<a href=/people/r/ryan-cotterell/>Ryan Cotterell</a><br><a href=/volumes/2021.emnlp-main/ class=text-muted>Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--824><div class="card-body p-3 small">Probabilistic distributions over spanning trees in <a href=https://en.wikipedia.org/wiki/Directed_graph>directed graphs</a> are a fundamental model of dependency structure in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a>, syntactic dependency trees. In <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a>, dependency trees often have an additional root constraint : only one edge may emanate from the root. However, no <a href=https://en.wikipedia.org/wiki/Sampling_(statistics)>sampling algorithm</a> has been presented in the literature to account for this additional <a href=https://en.wikipedia.org/wiki/Constraint_(mathematics)>constraint</a>. In this paper, we adapt two spanning tree sampling algorithms to faithfully sample dependency trees from a <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph</a> subject to the root constraint. Wilson (1996 (&#8217;s sampling algorithm has a <a href=https://en.wikipedia.org/wiki/Time_complexity>running time</a> of O(H) where H is the mean hitting time of the <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph</a>. Colbourn (1996)&#8217;s sampling algorithm has a <a href=https://en.wikipedia.org/wiki/Time_complexity>running time</a> of O(N3), which is often greater than the mean hitting time of a <a href=https://en.wikipedia.org/wiki/Directed_graph>directed graph</a>. Additionally, we build upon Colbourn&#8217;s algorithm and present a novel extension that can sample K trees without replacement in O(K N3 + K2 N) time. To the best of our knowledge, no <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> has been given for sampling spanning trees without replacement from a <a href=https://en.wikipedia.org/wiki/Directed_graph>directed graph</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.sigtyp-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.sigtyp-1.0/>Proceedings of the Third Workshop on Computational Typology and Multilingual NLP</a></strong><br><a href=/people/e/ekaterina-vylomova/>Ekaterina Vylomova</a>
|
<a href=/people/e/elizabeth-salesky/>Elizabeth Salesky</a>
|
<a href=/people/s/sabrina-mielke/>Sabrina Mielke</a>
|
<a href=/people/g/gabriella-lapesa/>Gabriella Lapesa</a>
|
<a href=/people/r/ritesh-kumar/>Ritesh Kumar</a>
|
<a href=/people/h/harald-hammarstrom/>Harald Hammarström</a>
|
<a href=/people/i/ivan-vulic/>Ivan Vulić</a>
|
<a href=/people/a/anna-korhonen/>Anna Korhonen</a>
|
<a href=/people/r/roi-reichart/>Roi Reichart</a>
|
<a href=/people/e/edoardo-maria-ponti/>Edoardo Maria Ponti</a>
|
<a href=/people/r/ryan-cotterell/>Ryan Cotterell</a><br><a href=/volumes/2021.sigtyp-1/ class=text-muted>Proceedings of the Third Workshop on Computational Typology and Multilingual NLP</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.0/>Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</a></strong><br><a href=/people/k/kristina-toutanova/>Kristina Toutanova</a>
|
<a href=/people/a/anna-rumshisky/>Anna Rumshisky</a>
|
<a href=/people/l/luke-zettlemoyer/>Luke Zettlemoyer</a>
|
<a href=/people/d/dilek-hakkani-tur/>Dilek Hakkani-Tur</a>
|
<a href=/people/i/iz-beltagy/>Iz Beltagy</a>
|
<a href=/people/s/steven-bethard/>Steven Bethard</a>
|
<a href=/people/r/ryan-cotterell/>Ryan Cotterell</a>
|
<a href=/people/t/tanmoy-chakraborty/>Tanmoy Chakraborty</a>
|
<a href=/people/y/yichao-zhou/>Yichao Zhou</a><br><a href=/volumes/2021.naacl-main/ class=text-muted>Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.12.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--12 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.12 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.12/>A Non-Linear Structural Probe</a></strong><br><a href=/people/j/jennifer-c-white/>Jennifer C. White</a>
|
<a href=/people/t/tiago-pimentel/>Tiago Pimentel</a>
|
<a href=/people/n/naomi-saphra/>Naomi Saphra</a>
|
<a href=/people/r/ryan-cotterell/>Ryan Cotterell</a><br><a href=/volumes/2021.naacl-main/ class=text-muted>Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--12><div class="card-body p-3 small">Probes are models devised to investigate the encoding of knowledgee.g. syntactic structurein <a href=https://en.wikipedia.org/wiki/Context_(language_use)>contextual representations</a>. Probes are often designed for simplicity, which has led to restrictions on probe design that may not allow for the full exploitation of the structure of encoded information ; one such restriction is <a href=https://en.wikipedia.org/wiki/Linearity>linearity</a>. We examine the case of a structural probe (Hewitt and Manning, 2019), which aims to investigate the encoding of syntactic structure in contextual representations through learning only <a href=https://en.wikipedia.org/wiki/Linear_map>linear transformations</a>. By observing that the structural probe learns a <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metric</a>, we are able to kernelize it and develop a novel non-linear variant with an identical number of parameters. We test on 6 languages and find that the radial-basis function (RBF) kernel, in conjunction with <a href=https://en.wikipedia.org/wiki/Regularization_(mathematics)>regularization</a>, achieves a statistically significant improvement over the baseline in all languagesimplying that at least part of the syntactic knowledge is encoded non-linearly. We conclude by discussing how the RBF kernel resembles BERT&#8217;s self-attention layers and speculate that this resemblance leads to the RBF-based probe&#8217;s stronger performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.349.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--349 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.349 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.naacl-main.349" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.349/>Finding Concept-specific Biases in FormMeaning Associations</a></strong><br><a href=/people/t/tiago-pimentel/>Tiago Pimentel</a>
|
<a href=/people/b/brian-roark/>Brian Roark</a>
|
<a href=/people/s/soren-wichmann/>Søren Wichmann</a>
|
<a href=/people/r/ryan-cotterell/>Ryan Cotterell</a>
|
<a href=/people/d/damian-blasi/>Damián Blasi</a><br><a href=/volumes/2021.naacl-main/ class=text-muted>Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--349><div class="card-body p-3 small">This work presents an information-theoretic operationalisation of cross-linguistic non-arbitrariness. It is not a new idea that there are small, cross-linguistic associations between the forms and meanings of words. For instance, it has been claimed (Blasi et al., 2016) that the word for tongue is more likely than chance to contain the phone [ l ]. By controlling for the influence of language family and geographic proximity within a very large concept-aligned, cross-lingual lexicon, we extend methods previously used to detect within language non-arbitrariness (Pimentel et al., 2019) to measure cross-linguistic associations. We find that there is a significant effect of non-arbitrariness, but it is unsurprisingly small (less than 0.5 % on average according to our information-theoretic estimate). We also provide a concept-level analysis which shows that a quarter of the <a href=https://en.wikipedia.org/wiki/Concept>concepts</a> considered in our work exhibit a significant level of cross-linguistic non-arbitrariness. In sum, the paper provides new methods to detect cross-linguistic associations at scale, and confirms their effects are minor.</div></div><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.232.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--232 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.232 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.emnlp-main.232.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938828 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.232" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.232/>Exploring the Linear Subspace Hypothesis in Gender Bias Mitigation</a></strong><br><a href=/people/f/francisco-vargas/>Francisco Vargas</a>
|
<a href=/people/r/ryan-cotterell/>Ryan Cotterell</a><br><a href=/volumes/2020.emnlp-main/ class=text-muted>Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--232><div class="card-body p-3 small">Bolukbasi et al. (2016) presents one of the first gender bias mitigation techniques for <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a>. Their method takes pre-trained <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> as input and attempts to isolate a <a href=https://en.wikipedia.org/wiki/Linear_subspace>linear subspace</a> that captures most of the <a href=https://en.wikipedia.org/wiki/Gender_bias>gender bias</a> in the embeddings. As judged by an analogical evaluation task, their method virtually eliminates <a href=https://en.wikipedia.org/wiki/Sexism>gender bias</a> in the <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a>. However, an implicit and untested assumption of their method is that the bias subspace is actually linear. In this work, we generalize their method to a kernelized, non-linear version. We take inspiration from kernel principal component analysis and derive a non-linear bias isolation technique. We discuss and overcome some of the practical drawbacks of our method for non-linear gender bias mitigation in <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> and analyze empirically whether the bias subspace is actually linear. Our analysis shows that <a href=https://en.wikipedia.org/wiki/Gender_bias>gender bias</a> is in fact well captured by a <a href=https://en.wikipedia.org/wiki/Linear_subspace>linear subspace</a>, justifying the assumption of Bolukbasi et al.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.acl-main.420.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--acl-main--420 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.acl-main.420 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://slideslive.com/38928922 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.acl-main.420" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.acl-main.420/>Information-Theoretic Probing for Linguistic Structure</a></strong><br><a href=/people/t/tiago-pimentel/>Tiago Pimentel</a>
|
<a href=/people/j/josef-valvoda/>Josef Valvoda</a>
|
<a href=/people/r/rowan-hall-maudslay/>Rowan Hall Maudslay</a>
|
<a href=/people/r/ran-zmigrod/>Ran Zmigrod</a>
|
<a href=/people/a/adina-williams/>Adina Williams</a>
|
<a href=/people/r/ryan-cotterell/>Ryan Cotterell</a><br><a href=/volumes/2020.acl-main/ class=text-muted>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--acl-main--420><div class="card-body p-3 small">The success of <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a> on a diverse set of NLP tasks has led researchers to question how much these <a href=https://en.wikipedia.org/wiki/Neural_network>networks</a> actually know about <a href=https://en.wikipedia.org/wiki/Natural_language>natural language</a>. Probes are a natural way of assessing this. When probing, a researcher chooses a <a href=https://en.wikipedia.org/wiki/Linguistic_description>linguistic task</a> and trains a <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised model</a> to predict annotations in that <a href=https://en.wikipedia.org/wiki/Linguistic_description>linguistic task</a> from the network&#8217;s learned representations. If the probe does well, the researcher may conclude that the <a href=https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning>representations</a> encode knowledge related to the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. A commonly held belief is that using simpler <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> as probes is better ; the logic is that simpler models will identify linguistic structure, but not learn the task itself. We propose an information-theoretic operationalization of probing as estimating mutual information that contradicts this received wisdom : one should always select the highest performing probe one can, even if it is more complex, since it will result in a tighter estimate, and thus reveal more of the linguistic information inherent in the representation. The experimental portion of our paper focuses on empirically estimating the <a href=https://en.wikipedia.org/wiki/Mutual_information>mutual information</a> between a linguistic property and BERT, comparing these estimates to several baselines. We evaluate on a set of ten typologically diverse languages often underrepresented in NLP researchplus Englishtotalling eleven languages. Our implementation is available in https://github.com/rycolab/info-theoretic-probing.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.acl-main.695.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--acl-main--695 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.acl-main.695 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://slideslive.com/38928868 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.acl-main.695" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.acl-main.695/>The Paradigm Discovery Problem</a></strong><br><a href=/people/a/alexander-erdmann/>Alexander Erdmann</a>
|
<a href=/people/m/micha-elsner/>Micha Elsner</a>
|
<a href=/people/s/shijie-wu/>Shijie Wu</a>
|
<a href=/people/r/ryan-cotterell/>Ryan Cotterell</a>
|
<a href=/people/n/nizar-habash/>Nizar Habash</a><br><a href=/volumes/2020.acl-main/ class=text-muted>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--acl-main--695><div class="card-body p-3 small">This work treats the paradigm discovery problem (PDP), the task of learning an inflectional morphological system from unannotated sentences. We formalize the <a href=https://en.wikipedia.org/wiki/Programmable_Data_Processor>PDP</a> and develop evaluation metrics for judging systems. Using currently available resources, we construct datasets for the <a href=https://en.wikipedia.org/wiki/Task_(computing)>task</a>. We also devise a heuristic benchmark for the <a href=https://en.wikipedia.org/wiki/Programmed_Data_Processor>PDP</a> and report empirical results on five diverse languages. Our benchmark system first makes use of <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> and <a href=https://en.wikipedia.org/wiki/String_similarity>string similarity</a> to cluster forms by cell and by <a href=https://en.wikipedia.org/wiki/Paradigm>paradigm</a>. Then, we bootstrap a neural transducer on top of the clustered data to predict words to realize the empty paradigm slots. An error analysis of our <a href=https://en.wikipedia.org/wiki/System>system</a> suggests clustering by cell across different inflection classes is the most pressing challenge for future work.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.sigmorphon-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.sigmorphon-1.0/>Proceedings of the 17th SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology</a></strong><br><a href=/people/g/garrett-nicolai/>Garrett Nicolai</a>
|
<a href=/people/k/kyle-gorman/>Kyle Gorman</a>
|
<a href=/people/r/ryan-cotterell/>Ryan Cotterell</a><br><a href=/volumes/2020.sigmorphon-1/ class=text-muted>Proceedings of the 17th SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.sigtyp-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.sigtyp-1.0/>Proceedings of the Second Workshop on Computational Research in Linguistic Typology</a></strong><br><a href=/people/e/ekaterina-vylomova/>Ekaterina Vylomova</a>
|
<a href=/people/e/edoardo-m-ponti/>Edoardo M. Ponti</a>
|
<a href=/people/e/eitan-grossman/>Eitan Grossman</a>
|
<a href=/people/a/arya-d-mccarthy/>Arya D. McCarthy</a>
|
<a href=/people/y/yevgeni-berzak/>Yevgeni Berzak</a>
|
<a href=/people/h/haim-dubossarsky/>Haim Dubossarsky</a>
|
<a href=/people/i/ivan-vulic/>Ivan Vulić</a>
|
<a href=/people/r/roi-reichart/>Roi Reichart</a>
|
<a href=/people/a/anna-korhonen/>Anna Korhonen</a>
|
<a href=/people/r/ryan-cotterell/>Ryan Cotterell</a><br><a href=/volumes/2020.sigtyp-1/ class=text-muted>Proceedings of the Second Workshop on Computational Research in Linguistic Typology</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.sigtyp-1.1.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--sigtyp-1--1 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.sigtyp-1.1 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939790 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.sigtyp-1.1/>SIGTYP 2020 Shared Task : Prediction of Typological Features<span class=acl-fixed-case>SIGTYP</span> 2020 Shared Task: Prediction of Typological Features</a></strong><br><a href=/people/j/johannes-bjerva/>Johannes Bjerva</a>
|
<a href=/people/e/elizabeth-salesky/>Elizabeth Salesky</a>
|
<a href=/people/s/sabrina-j-mielke/>Sabrina J. Mielke</a>
|
<a href=/people/a/aditi-chaudhary/>Aditi Chaudhary</a>
|
<a href=/people/g/giuseppe-g-a-celano/>Giuseppe G. A. Celano</a>
|
<a href=/people/e/edoardo-maria-ponti/>Edoardo Maria Ponti</a>
|
<a href=/people/e/ekaterina-vylomova/>Ekaterina Vylomova</a>
|
<a href=/people/r/ryan-cotterell/>Ryan Cotterell</a>
|
<a href=/people/i/isabelle-augenstein/>Isabelle Augenstein</a><br><a href=/volumes/2020.sigtyp-1/ class=text-muted>Proceedings of the Second Workshop on Computational Research in Linguistic Typology</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--sigtyp-1--1><div class="card-body p-3 small">Typological knowledge bases (KBs) such as WALS (Dryer and Haspelmath, 2013) contain information about linguistic properties of the world&#8217;s languages. They have been shown to be useful for downstream applications, including cross-lingual transfer learning and linguistic probing. A major drawback hampering broader adoption of typological KBs is that they are sparsely populated, in the sense that most languages only have annotations for some <a href=https://en.wikipedia.org/wiki/Feature_(linguistics)>features</a>, and skewed, in that few <a href=https://en.wikipedia.org/wiki/Feature_(linguistics)>features</a> have wide coverage. As typological features often correlate with one another, it is possible to predict them and thus automatically populate typological KBs, which is also the focus of this shared task. Overall, the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> attracted 8 submissions from 5 teams, out of which the most successful <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> make use of such feature correlations. However, our <a href=https://en.wikipedia.org/wiki/Error_detection_and_correction>error analysis</a> reveals that even the strongest submitted systems struggle with predicting feature values for languages where few features are known.</div></div><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1288.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1288 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1288 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1288.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D19-1288/>Towards Zero-shot Language Modeling</a></strong><br><a href=/people/e/edoardo-maria-ponti/>Edoardo Maria Ponti</a>
|
<a href=/people/i/ivan-vulic/>Ivan Vulić</a>
|
<a href=/people/r/ryan-cotterell/>Ryan Cotterell</a>
|
<a href=/people/r/roi-reichart/>Roi Reichart</a>
|
<a href=/people/a/anna-korhonen/>Anna Korhonen</a><br><a href=/volumes/D19-1/ class=text-muted>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1288><div class="card-body p-3 small">Can we construct a neural language model which is inductively biased towards learning <a href=https://en.wikipedia.org/wiki/Human_language>human language</a>? Motivated by this question, we aim at constructing an informative prior for held-out languages on the task of character-level, open-vocabulary language modelling. We obtain this <a href=https://en.wikipedia.org/wiki/Prior_probability>prior</a> as the posterior over network weights conditioned on the data from a sample of training languages, which is approximated through <a href=https://en.wikipedia.org/wiki/Laplace&#8217;s_method>Laplace&#8217;s method</a>. Based on a large and diverse sample of languages, the use of our <a href=https://en.wikipedia.org/wiki/Prior_probability>prior</a> outperforms baseline models with an uninformative prior in both zero-shot and few-shot settings, showing that the <a href=https://en.wikipedia.org/wiki/Prior_probability>prior</a> is imbued with universal linguistic knowledge. Moreover, we harness broad language-specific information available for most languages of the world, i.e., <a href=https://en.wikipedia.org/wiki/Feature_(linguistics)>features</a> from <a href=https://en.wikipedia.org/wiki/Linguistic_description>typological databases</a>, as distant supervision for held-out languages. We explore several language modelling conditioning techniques, including concatenation and meta-networks for parameter generation. They appear beneficial in the <a href=https://en.wikipedia.org/wiki/Limited_series_(comics)>few-shot setting</a>, but ineffective in the <a href=https://en.wikipedia.org/wiki/Limited_series_(comics)>zero-shot setting</a>. Since the paucity of even plain digital text affects the majority of the world&#8217;s languages, we hope that these insights will broaden the scope of applications for <a href=https://en.wikipedia.org/wiki/Language_technology>language technology</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1531.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1531 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1531 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1531" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1531/>Examining Gender Bias in <a href=https://en.wikipedia.org/wiki/Language>Languages</a> with Grammatical Gender</a></strong><br><a href=/people/p/pei-zhou/>Pei Zhou</a>
|
<a href=/people/w/weijia-shi/>Weijia Shi</a>
|
<a href=/people/j/jieyu-zhao/>Jieyu Zhao</a>
|
<a href=/people/k/kuan-hao-huang/>Kuan-Hao Huang</a>
|
<a href=/people/m/muhao-chen/>Muhao Chen</a>
|
<a href=/people/r/ryan-cotterell/>Ryan Cotterell</a>
|
<a href=/people/k/kai-wei-chang/>Kai-Wei Chang</a><br><a href=/volumes/D19-1/ class=text-muted>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1531><div class="card-body p-3 small">Recent studies have shown that <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> exhibit <a href=https://en.wikipedia.org/wiki/Gender_bias>gender bias</a> inherited from the training corpora. However, most studies to date have focused on quantifying and mitigating such <a href=https://en.wikipedia.org/wiki/Bias>bias</a> only in <a href=https://en.wikipedia.org/wiki/English_language>English</a>. These analyses can not be directly extended to <a href=https://en.wikipedia.org/wiki/Language>languages</a> that exhibit <a href=https://en.wikipedia.org/wiki/Agreement_(linguistics)>morphological agreement</a> on gender, such as <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a> and <a href=https://en.wikipedia.org/wiki/French_language>French</a>. In this paper, we propose new metrics for evaluating <a href=https://en.wikipedia.org/wiki/Gender_bias>gender bias</a> in <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> of these languages and further demonstrate evidence of <a href=https://en.wikipedia.org/wiki/Gender_bias>gender bias</a> in bilingual embeddings which align these languages with <a href=https://en.wikipedia.org/wiki/English_language>English</a>. Finally, we extend an existing approach to mitigate gender bias in <a href=https://en.wikipedia.org/wiki/Word_embedding>word embedding</a> of these <a href=https://en.wikipedia.org/wiki/Language>languages</a> under both monolingual and bilingual settings. Experiments on modified Word Embedding Association Test, word similarity, word translation, and word pair translation tasks show that the proposed approaches can effectively reduce the gender bias while preserving the utility of the original embeddings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1577.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1577 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1577 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1577/>Quantifying the Semantic Core of Gender Systems</a></strong><br><a href=/people/a/adina-williams/>Adina Williams</a>
|
<a href=/people/d/damian-blasi/>Damian Blasi</a>
|
<a href=/people/l/lawrence-wolf-sonkin/>Lawrence Wolf-Sonkin</a>
|
<a href=/people/h/hanna-wallach/>Hanna Wallach</a>
|
<a href=/people/r/ryan-cotterell/>Ryan Cotterell</a><br><a href=/volumes/D19-1/ class=text-muted>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1577><div class="card-body p-3 small">Many of the world&#8217;s languages employ <a href=https://en.wikipedia.org/wiki/Grammatical_gender>grammatical gender</a> on the <a href=https://en.wikipedia.org/wiki/Lexeme>lexeme</a>. For instance, in <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a>, house casa is feminine, whereas the word for paper papel is masculine. To a speaker of a <a href=https://en.wikipedia.org/wiki/Genderless_language>genderless language</a>, this categorization seems to exist with neither <a href=https://en.wikipedia.org/wiki/Rhyme>rhyme</a> nor reason. But, is the association of <a href=https://en.wikipedia.org/wiki/Noun>nouns</a> to <a href=https://en.wikipedia.org/wiki/Gender>gender classes</a> truly arbitrary? In this work, we present the first large-scale investigation of the arbitrariness of gender assignment that uses canonical correlation analysis as a method for correlating the gender of inanimate nouns with their lexical semantic meaning. We find that the gender systems of 18 languages exhibit a significant correlation with an externally grounded definition of lexical semantics.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3628 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3628 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3628/>Rethinking Phonotactic Complexity</a></strong><br><a href=/people/t/tiago-pimentel/>Tiago Pimentel</a>
|
<a href=/people/b/brian-roark/>Brian Roark</a>
|
<a href=/people/r/ryan-cotterell/>Ryan Cotterell</a><br><a href=/volumes/W19-36/ class=text-muted>Proceedings of the 2019 Workshop on Widening NLP</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3628><div class="card-body p-3 small">In this work, we propose the use of phone-level language models to estimate phonotactic complexity&#8212;measured in bits per phoneme&#8212;which makes cross-linguistic comparison straightforward. We compare the entropy across languages using this simple measure, gaining insight on how complex different language&#8217;s phonotactics are. Finally, we show a very strong negative correlation between phonotactic complexity and the average length of words&#8212;Spearman rho=-0.744&#8212;when analysing a collection of 106 languages with 1016 basic concepts each.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4200.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4200/>Proceedings of the 16th Workshop on Computational Research in Phonetics, Phonology, and Morphology</a></strong><br><a href=/people/g/garrett-nicolai/>Garrett Nicolai</a>
|
<a href=/people/r/ryan-cotterell/>Ryan Cotterell</a><br><a href=/volumes/W19-42/ class=text-muted>Proceedings of the 16th Workshop on Computational Research in Phonetics, Phonology, and Morphology</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4226.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4226 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4226 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4226/>The SIGMORPHON 2019 Shared Task : Morphological Analysis in Context and Cross-Lingual Transfer for <a href=https://en.wikipedia.org/wiki/Inflection>Inflection</a><span class=acl-fixed-case>SIGMORPHON</span> 2019 Shared Task: Morphological Analysis in Context and Cross-Lingual Transfer for Inflection</a></strong><br><a href=/people/a/arya-d-mccarthy/>Arya D. McCarthy</a>
|
<a href=/people/e/ekaterina-vylomova/>Ekaterina Vylomova</a>
|
<a href=/people/s/shijie-wu/>Shijie Wu</a>
|
<a href=/people/c/chaitanya-malaviya/>Chaitanya Malaviya</a>
|
<a href=/people/l/lawrence-wolf-sonkin/>Lawrence Wolf-Sonkin</a>
|
<a href=/people/g/garrett-nicolai/>Garrett Nicolai</a>
|
<a href=/people/c/christo-kirov/>Christo Kirov</a>
|
<a href=/people/m/miikka-silfverberg/>Miikka Silfverberg</a>
|
<a href=/people/s/sabrina-j-mielke/>Sabrina J. Mielke</a>
|
<a href=/people/j/jeffrey-heinz/>Jeffrey Heinz</a>
|
<a href=/people/r/ryan-cotterell/>Ryan Cotterell</a>
|
<a href=/people/m/mans-hulden/>Mans Hulden</a><br><a href=/volumes/W19-42/ class=text-muted>Proceedings of the 16th Workshop on Computational Research in Phonetics, Phonology, and Morphology</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4226><div class="card-body p-3 small">The SIGMORPHON 2019 shared task on cross-lingual transfer and contextual analysis in morphology examined transfer learning of inflection between 100 language pairs, as well as contextual lemmatization and morphosyntactic description in 66 languages. The first task evolves past years&#8217; inflection tasks by examining transfer of morphological inflection knowledge from a high-resource language to a low-resource language. This year also presents a new second challenge on <a href=https://en.wikipedia.org/wiki/Lemmatization>lemmatization</a> and morphological feature analysis in context. All submissions featured a neural component and built on either this year&#8217;s strong baselines or highly ranked systems from previous years&#8217; shared tasks. Every participating team improved in accuracy over the baselines for the inflection task (though not Levenshtein distance), and every team in the contextual analysis task improved on both state-of-the-art neural and non-neural baselines.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4900.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4900/>Proceedings of TyP-NLP: The First Workshop on Typology for Polyglot NLP</a></strong><br><a href=/people/h/haim-dubossarsky/>Haim Dubossarsky</a>
|
<a href=/people/a/arya-d-mccarthy/>Arya D. McCarthy</a>
|
<a href=/people/e/edoardo-maria-ponti/>Edoardo Maria Ponti</a>
|
<a href=/people/i/ivan-vulic/>Ivan Vulić</a>
|
<a href=/people/e/ekaterina-vylomova/>Ekaterina Vylomova</a>
|
<a href=/people/y/yevgeni-berzak/>Yevgeni Berzak</a>
|
<a href=/people/r/ryan-cotterell/>Ryan Cotterell</a>
|
<a href=/people/m/manaal-faruqui/>Manaal Faruqui</a>
|
<a href=/people/a/anna-korhonen/>Anna Korhonen</a>
|
<a href=/people/r/roi-reichart/>Roi Reichart</a><br><a href=/volumes/W19-49/ class=text-muted>Proceedings of TyP-NLP: The First Workshop on Typology for Polyglot NLP</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1064.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1064 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1064 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/347396468 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1064/>Gender Bias in Contextualized Word Embeddings</a></strong><br><a href=/people/j/jieyu-zhao/>Jieyu Zhao</a>
|
<a href=/people/t/tianlu-wang/>Tianlu Wang</a>
|
<a href=/people/m/mark-yatskar/>Mark Yatskar</a>
|
<a href=/people/r/ryan-cotterell/>Ryan Cotterell</a>
|
<a href=/people/v/vicente-ordonez/>Vicente Ordonez</a>
|
<a href=/people/k/kai-wei-chang/>Kai-Wei Chang</a><br><a href=/volumes/N19-1/ class=text-muted>Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1064><div class="card-body p-3 small">In this paper, we quantify, analyze and mitigate <a href=https://en.wikipedia.org/wiki/Gender_bias>gender bias</a> exhibited in ELMo&#8217;s contextualized word vectors. First, we conduct several intrinsic analyses and find that (1) training data for ELMo contains significantly more male than female entities, (2) the trained ELMo embeddings systematically encode gender information and (3) ELMo unequally encodes gender information about male and female entities. Then, we show that a state-of-the-art coreference system that depends on ELMo inherits its bias and demonstrates significant bias on the WinoBias probing corpus. Finally, we explore two methods to mitigate such gender bias and show that the bias demonstrated on WinoBias can be eliminated.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1065.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1065 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1065 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/356020948 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1065" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1065/>Combining Sentiment Lexica with a Multi-View Variational Autoencoder<span class=acl-fixed-case>C</span>ombining <span class=acl-fixed-case>S</span>entiment <span class=acl-fixed-case>L</span>exica with a <span class=acl-fixed-case>M</span>ulti-<span class=acl-fixed-case>V</span>iew <span class=acl-fixed-case>V</span>ariational <span class=acl-fixed-case>A</span>utoencoder</a></strong><br><a href=/people/a/alexander-miserlis-hoyle/>Alexander Miserlis Hoyle</a>
|
<a href=/people/l/lawrence-wolf-sonkin/>Lawrence Wolf-Sonkin</a>
|
<a href=/people/h/hanna-wallach/>Hanna Wallach</a>
|
<a href=/people/r/ryan-cotterell/>Ryan Cotterell</a>
|
<a href=/people/i/isabelle-augenstein/>Isabelle Augenstein</a><br><a href=/volumes/N19-1/ class=text-muted>Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1065><div class="card-body p-3 small">When assigning quantitative labels to a dataset, different <a href=https://en.wikipedia.org/wiki/Methodology>methodologies</a> may rely on different scales. In particular, when assigning polarities to words in a sentiment lexicon, annotators may use binary, categorical, or continuous labels. Naturally, it is of interest to unify these labels from disparate scales to both achieve maximal coverage over words and to create a single, more robust sentiment lexicon while retaining scale coherence. We introduce a generative model of sentiment lexica to combine disparate scales into a common latent representation. We realize this <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> with a novel multi-view variational autoencoder (VAE), called SentiVAE. We evaluate our approach via a downstream text classification task involving nine English-Language sentiment analysis datasets ; our representation outperforms six individual sentiment lexica, as well as a straightforward combination thereof.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1155.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1155 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1155 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/360705702 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1155/>A Simple Joint Model for Improved Contextual Neural Lemmatization</a></strong><br><a href=/people/c/chaitanya-malaviya/>Chaitanya Malaviya</a>
|
<a href=/people/s/shijie-wu/>Shijie Wu</a>
|
<a href=/people/r/ryan-cotterell/>Ryan Cotterell</a><br><a href=/volumes/N19-1/ class=text-muted>Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1155><div class="card-body p-3 small">English verbs have multiple forms. For instance, talk may also appear as talks, talked or talking, depending on the context. The NLP task of <a href=https://en.wikipedia.org/wiki/Lemmatization>lemmatization</a> seeks to map these diverse forms back to a canonical one, known as the lemma. We present a simple joint neural model for <a href=https://en.wikipedia.org/wiki/Lemmatization>lemmatization</a> and morphological tagging that achieves state-of-the-art results on 20 languages from the Universal Dependencies corpora. Our paper describes the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> in addition to training and decoding procedures. Error analysis indicates that joint morphological tagging and <a href=https://en.wikipedia.org/wiki/Lemmatization>lemmatization</a> is especially helpful in low-resource lemmatization and languages that display a larger degree of morphological complexity.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1203.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1203 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1203 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/354264026 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1203/>Contextualization of Morphological Inflection</a></strong><br><a href=/people/e/ekaterina-vylomova/>Ekaterina Vylomova</a>
|
<a href=/people/r/ryan-cotterell/>Ryan Cotterell</a>
|
<a href=/people/t/trevor-cohn/>Trevor Cohn</a>
|
<a href=/people/t/timothy-baldwin/>Timothy Baldwin</a>
|
<a href=/people/j/jason-eisner/>Jason Eisner</a><br><a href=/volumes/N19-1/ class=text-muted>Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1203><div class="card-body p-3 small">Critical to <a href=https://en.wikipedia.org/wiki/Natural-language_generation>natural language generation</a> is the production of correctly inflected text. In this paper, we isolate the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> of predicting a fully inflected sentence from its partially lemmatized version. Unlike traditional <a href=https://en.wikipedia.org/wiki/Inflection>morphological inflection</a> or surface realization, our task input does not provide gold tags that specify what morphological features to realize on each lemmatized word ; rather, such features must be inferred from sentential context. We develop a neural hybrid graphical model that explicitly reconstructs <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphological features</a> before predicting the <a href=https://en.wikipedia.org/wiki/Inflection>inflected forms</a>, and compare this to a system that directly predicts the <a href=https://en.wikipedia.org/wiki/Inflection>inflected forms</a> without relying on any morphological annotation. We experiment on several typologically diverse languages from the Universal Dependencies treebanks, showing the utility of incorporating linguistically-motivated latent variables into NLP models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1415.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1415 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1415 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/359721173 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1415/>On the Idiosyncrasies of the Mandarin Chinese Classifier System<span class=acl-fixed-case>M</span>andarin <span class=acl-fixed-case>C</span>hinese Classifier System</a></strong><br><a href=/people/s/shijia-liu/>Shijia Liu</a>
|
<a href=/people/h/hongyuan-mei/>Hongyuan Mei</a>
|
<a href=/people/a/adina-williams/>Adina Williams</a>
|
<a href=/people/r/ryan-cotterell/>Ryan Cotterell</a><br><a href=/volumes/N19-1/ class=text-muted>Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1415><div class="card-body p-3 small">While idiosyncrasies of the Chinese classifier system have been a richly studied topic among linguists (Adams and Conklin, 1973 ; Erbaugh, 1986 ; Lakoff, 1986), not much work has been done to quantify them with statistical methods. In this paper, we introduce an information-theoretic approach to measuring idiosyncrasy ; we examine how much the uncertainty in Mandarin Chinese classifiers can be reduced by knowing semantic information about the nouns that the <a href=https://en.wikipedia.org/wiki/Classifier_(linguistics)>classifiers</a> modify. Using the empirical distribution of <a href=https://en.wikipedia.org/wiki/Classifier_(linguistics)>classifiers</a> from the parsed Chinese Gigaword corpus (Graff et al., 2005), we compute the <a href=https://en.wikipedia.org/wiki/Mutual_information>mutual information</a> (in bits) between the distribution over <a href=https://en.wikipedia.org/wiki/Classifier_(linguistics)>classifiers</a> and distributions over other linguistic quantities. We investigate whether semantic classes of nouns and adjectives differ in how much they reduce uncertainty in classifier choice, and find that it is not fully idiosyncratic ; while there are no obvious trends for the majority of semantic classes, shape nouns reduce uncertainty in classifier choice the most.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1167.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1167 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1167 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P19-1167.Software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1167/>Unsupervised Discovery of Gendered Language through <a href=https://en.wikipedia.org/wiki/Latent-variable_model>Latent-Variable Modeling</a></a></strong><br><a href=/people/a/alexander-miserlis-hoyle/>Alexander Miserlis Hoyle</a>
|
<a href=/people/l/lawrence-wolf-sonkin/>Lawrence Wolf-Sonkin</a>
|
<a href=/people/h/hanna-wallach/>Hanna Wallach</a>
|
<a href=/people/i/isabelle-augenstein/>Isabelle Augenstein</a>
|
<a href=/people/r/ryan-cotterell/>Ryan Cotterell</a><br><a href=/volumes/P19-1/ class=text-muted>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1167><div class="card-body p-3 small">Studying the ways in which language is gendered has long been an area of interest in <a href=https://en.wikipedia.org/wiki/Sociolinguistics>sociolinguistics</a>. Studies have explored, for example, the speech of male and female characters in film and the language used to describe male and female politicians. In this paper, we aim not to merely study this phenomenon qualitatively, but instead to quantify the degree to which the language used to describe men and women is different and, moreover, different in a positive or negative way. To that end, we introduce a generative latent-variable model that jointly represents adjective (or verb) choice, with its sentiment, given the natural gender of a head (or dependent) noun. We find that there are significant differences between descriptions of male and female nouns and that these differences align with common gender stereotypes : Positive adjectives used to describe women are more often related to their bodies than <a href=https://en.wikipedia.org/wiki/Adjective>adjectives</a> used to describe men.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K19-1014.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K19-1014 data-toggle=collapse aria-expanded=false aria-controls=abstract-K19-1014 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/K19-1014/>Weird Inflects but OK : Making Sense of Morphological Generation Errors<span class=acl-fixed-case>OK</span>: Making Sense of Morphological Generation Errors</a></strong><br><a href=/people/k/kyle-gorman/>Kyle Gorman</a>
|
<a href=/people/a/arya-d-mccarthy/>Arya D. McCarthy</a>
|
<a href=/people/r/ryan-cotterell/>Ryan Cotterell</a>
|
<a href=/people/e/ekaterina-vylomova/>Ekaterina Vylomova</a>
|
<a href=/people/m/miikka-silfverberg/>Miikka Silfverberg</a>
|
<a href=/people/m/magdalena-markowska/>Magdalena Markowska</a><br><a href=/volumes/K19-1/ class=text-muted>Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K19-1014><div class="card-body p-3 small">We conduct a manual error analysis of the CoNLL-SIGMORPHON Shared Task on Morphological Reinflection. This task involves <a href=https://en.wikipedia.org/wiki/Natural-language_generation>natural language generation</a> : systems are given a word in citation form (e.g., hug) and asked to produce the corresponding <a href=https://en.wikipedia.org/wiki/Grammatical_conjugation>inflected form</a> (e.g., the simple past hugged). We propose an error taxonomy and use it to annotate errors made by the top two <a href=https://en.wikipedia.org/wiki/List_of_systems_of_plant_taxonomy>systems</a> across twelve languages. Many of the observed errors are related to inflectional patterns sensitive to inherent linguistic properties such as <a href=https://en.wikipedia.org/wiki/Animacy>animacy</a> or <a href=https://en.wikipedia.org/wiki/Affect_(linguistics)>affect</a> ; many others are failures to predict truly unpredictable inflectional behaviors. We also find nearly one quarter of the residual errors reflect errors in the gold data.</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1042.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1042 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1042 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1042" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D18-1042/>A Discriminative Latent-Variable Model for Bilingual Lexicon Induction</a></strong><br><a href=/people/s/sebastian-ruder/>Sebastian Ruder</a>
|
<a href=/people/r/ryan-cotterell/>Ryan Cotterell</a>
|
<a href=/people/y/yova-kementchedjhieva/>Yova Kementchedjhieva</a>
|
<a href=/people/a/anders-sogaard/>Anders Søgaard</a><br><a href=/volumes/D18-1/ class=text-muted>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1042><div class="card-body p-3 small">We introduce a novel discriminative latent-variable model for the task of bilingual lexicon induction. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> combines the bipartite matching dictionary prior of Haghighi et al. (2008) with a state-of-the-art embedding-based approach. To train the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>, we derive an efficient Viterbi EM algorithm. We provide empirical improvements on six language pairs under two metrics and show that the prior theoretically and empirically helps to mitigate the hubness problem. We also demonstrate how previous work may be viewed as a similarly fashioned latent-variable model, albeit with a different prior.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1473.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1473 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1473 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1473" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D18-1473/>Hard Non-Monotonic Attention for Character-Level Transduction</a></strong><br><a href=/people/s/shijie-wu/>Shijie Wu</a>
|
<a href=/people/p/pamela-shapiro/>Pamela Shapiro</a>
|
<a href=/people/r/ryan-cotterell/>Ryan Cotterell</a><br><a href=/volumes/D18-1/ class=text-muted>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1473><div class="card-body p-3 small">Character-level string-to-string transduction is an important component of various NLP tasks. The goal is to map an input string to an output string, where the strings may be of different lengths and have characters taken from different alphabets. Recent approaches have used sequence-to-sequence models with an attention mechanism to learn which parts of the input string the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> should focus on during the generation of the output string. Both soft attention and hard monotonic attention have been used, but hard non-monotonic attention has only been used in other sequence modeling tasks and has required a stochastic approximation to compute the <a href=https://en.wikipedia.org/wiki/Gradient>gradient</a>. In this work, we introduce an exact, polynomial-time algorithm for marginalizing over the exponential number of non-monotonic alignments between two strings, showing that hard attention models can be viewed as neural reparameterizations of the classical IBM Model 1. We compare soft and hard non-monotonic attention experimentally and find that the exact <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> significantly improves performance over the <a href=https://en.wikipedia.org/wiki/Stochastic_approximation>stochastic approximation</a> and outperforms soft attention.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6011.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6011 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6011 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6011/>Marrying Universal Dependencies and Universal Morphology<span class=acl-fixed-case>U</span>niversal <span class=acl-fixed-case>D</span>ependencies and <span class=acl-fixed-case>U</span>niversal <span class=acl-fixed-case>M</span>orphology</a></strong><br><a href=/people/a/arya-d-mccarthy/>Arya D. McCarthy</a>
|
<a href=/people/m/miikka-silfverberg/>Miikka Silfverberg</a>
|
<a href=/people/r/ryan-cotterell/>Ryan Cotterell</a>
|
<a href=/people/m/mans-hulden/>Mans Hulden</a>
|
<a href=/people/d/david-yarowsky/>David Yarowsky</a><br><a href=/volumes/W18-60/ class=text-muted>Proceedings of the Second Workshop on Universal Dependencies (UDW 2018)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6011><div class="card-body p-3 small">The Universal Dependencies (UD) and Universal Morphology (UniMorph) projects each present schemata for annotating the morphosyntactic details of language. Each project also provides corpora of annotated text in many languagesUD at the token level and UniMorph at the type level. As each <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> is built by different annotators, language-specific decisions hinder the goal of universal schemata. With compatibility of tags, each project&#8217;s annotations could be used to validate the other&#8217;s. Additionally, the availability of both type- and token-level resources would be a boon to tasks such as <a href=https://en.wikipedia.org/wiki/Parsing>parsing</a> and homograph disambiguation. To ease this interoperability, we present a deterministic mapping from Universal Dependencies v2 features into the UniMorph schema. We validate our approach by lookup in the UniMorph corpora and find a macro-average of 64.13 % <a href=https://en.wikipedia.org/wiki/Recall_(memory)>recall</a>. We also note incompatibilities due to paucity of data on either side. Finally, we present a critical evaluation of the foundations, strengths, and weaknesses of the two annotation projects.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1004.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1004 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1004 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://vimeo.com/276386708 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N18-1004/>A Deep Generative Model of Vowel Formant Typology</a></strong><br><a href=/people/r/ryan-cotterell/>Ryan Cotterell</a>
|
<a href=/people/j/jason-eisner/>Jason Eisner</a><br><a href=/volumes/N18-1/ class=text-muted>Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1004><div class="card-body p-3 small">What makes some types of <a href=https://en.wikipedia.org/wiki/Language>languages</a> more probable than others? For instance, we know that almost all <a href=https://en.wikipedia.org/wiki/Spoken_language>spoken languages</a> contain the <a href=https://en.wikipedia.org/wiki/Vowel>vowel phoneme</a> /i/ ; why should that be? The field of <a href=https://en.wikipedia.org/wiki/Linguistic_typology>linguistic typology</a> seeks to answer these questions and, thereby, divine the mechanisms that underlie <a href=https://en.wikipedia.org/wiki/Human_language>human language</a>. In our work, we tackle the problem of vowel system typology, i.e., we propose a <a href=https://en.wikipedia.org/wiki/Generative_model>generative probability model</a> of which vowels a language contains. In contrast to previous work, we work directly with the acoustic informationthe first two formant valuesrather than modeling discrete sets of symbols from the <a href=https://en.wikipedia.org/wiki/International_Phonetic_Alphabet>international phonetic alphabet</a>. We develop a novel generative probability model and report results on over 200 languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K18-3000.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/K18-3000/>Proceedings of the <span class=acl-fixed-case>C</span>o<span class=acl-fixed-case>NLL</span>–<span class=acl-fixed-case>SIGMORPHON</span> 2018 Shared Task: Universal Morphological Reinflection</a></strong><br><a href=/people/m/mans-hulden/>Mans Hulden</a>
|
<a href=/people/r/ryan-cotterell/>Ryan Cotterell</a><br><a href=/volumes/K18-3/ class=text-muted>Proceedings of the CoNLL–SIGMORPHON 2018 Shared Task: Universal Morphological Reinflection</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1245.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1245 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1245 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1245.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1245.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-1245" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P18-1245/>A Structured Variational Autoencoder for Contextual Morphological Inflection</a></strong><br><a href=/people/l/lawrence-wolf-sonkin/>Lawrence Wolf-Sonkin</a>
|
<a href=/people/j/jason-naradowsky/>Jason Naradowsky</a>
|
<a href=/people/s/sabrina-j-mielke/>Sabrina J. Mielke</a>
|
<a href=/people/r/ryan-cotterell/>Ryan Cotterell</a><br><a href=/volumes/P18-1/ class=text-muted>Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1245><div class="card-body p-3 small">Statistical morphological inflectors are typically trained on fully supervised, type-level data. One remaining open research question is the following : How can we effectively exploit raw, token-level data to improve their performance? To this end, we introduce a novel generative latent-variable model for the semi-supervised learning of inflection generation. To enable <a href=https://en.wikipedia.org/wiki/Posterior_probability>posterior inference</a> over the <a href=https://en.wikipedia.org/wiki/Latent_variable>latent variables</a>, we derive an efficient variational inference procedure based on the <a href=https://en.wikipedia.org/wiki/Wake-sleep_algorithm>wake-sleep algorithm</a>. We experiment on 23 languages, using the Universal Dependencies corpora in a simulated low-resource setting, and find improvements of over 10 % absolute accuracy in some cases.</div></div><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-2016.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-2016 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-2016 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-2016/>Low-Resource Named Entity Recognition with Cross-lingual, Character-Level Neural Conditional Random Fields</a></strong><br><a href=/people/r/ryan-cotterell/>Ryan Cotterell</a>
|
<a href=/people/k/kevin-duh/>Kevin Duh</a><br><a href=/volumes/I17-2/ class=text-muted>Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-2016><div class="card-body p-3 small">Low-resource named entity recognition is still an open problem in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a>. Most state-of-the-art systems require tens of thousands of annotated sentences in order to obtain high performance. However, for most of the world&#8217;s languages it is unfeasible to obtain such annotation. In this paper, we present a transfer learning scheme, whereby we train character-level neural CRFs to predict <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>named entities</a> for both high-resource languages and low-resource languages jointly. Learning character representations for multiple related languages allows <a href=https://en.wikipedia.org/wiki/Knowledge_transfer>knowledge transfer</a> from the high-resource languages to the low-resource ones, improving F1 by up to 9.8 points.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1109.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1109 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1109 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234959176 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-1109/>Probabilistic Typology : Deep Generative Models of Vowel Inventories</a></strong><br><a href=/people/r/ryan-cotterell/>Ryan Cotterell</a>
|
<a href=/people/j/jason-eisner/>Jason Eisner</a><br><a href=/volumes/P17-1/ class=text-muted>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1109><div class="card-body p-3 small">Linguistic typology studies the range of structures present in <a href=https://en.wikipedia.org/wiki/Human_language>human language</a>. The main goal of the field is to discover which sets of possible phenomena are universal, and which are merely frequent. For example, all languages have vowels, while mostbut not alllanguages have an /u/ sound. In this paper we present the first probabilistic treatment of a basic question in phonological typology : What makes a natural vowel inventory? We introduce a series of deep stochastic point processes, and contrast them with previous computational, simulation-based approaches. We provide a comprehensive suite of experiments on over 200 distinct languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1182.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1182 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1182 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-1182/>One-Shot Neural Cross-Lingual Transfer for Paradigm Completion</a></strong><br><a href=/people/k/katharina-kann/>Katharina Kann</a>
|
<a href=/people/r/ryan-cotterell/>Ryan Cotterell</a>
|
<a href=/people/h/hinrich-schutze/>Hinrich Schütze</a><br><a href=/volumes/P17-1/ class=text-muted>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1182><div class="card-body p-3 small">We present a novel cross-lingual transfer method for paradigm completion, the task of mapping a lemma to its inflected forms, using a neural encoder-decoder model, the state of the art for the monolingual task. We use labeled data from a high-resource language to increase performance on a low-resource language. In experiments on 21 language pairs from four different language families, we obtain up to 58 % higher <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> than without transfer and show that even zero-shot and one-shot learning are possible. We further find that the degree of <a href=https://en.wikipedia.org/wiki/Language_family>language relatedness</a> strongly influences the ability to transfer <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphological knowledge</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S17-1011.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S17-1011 data-toggle=collapse aria-expanded=false aria-controls=abstract-S17-1011 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=S17-1011" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/S17-1011/>Frame-Based Continuous Lexical Semantics through Exponential Family Tensor Factorization and Semantic Proto-Roles</a></strong><br><a href=/people/f/francis-ferraro/>Francis Ferraro</a>
|
<a href=/people/a/adam-poliak/>Adam Poliak</a>
|
<a href=/people/r/ryan-cotterell/>Ryan Cotterell</a>
|
<a href=/people/b/benjamin-van-durme/>Benjamin Van Durme</a><br><a href=/volumes/S17-1/ class=text-muted>Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (*SEM 2017)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S17-1011><div class="card-body p-3 small">We study how different frame annotations complement one another when learning continuous lexical semantics. We learn the representations from a tensorized skip-gram model that consistently encodes syntactic-semantic content better, with multiple 10 % gains over baselines.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1074.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1074 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1074 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1074/>Paradigm Completion for Derivational Morphology</a></strong><br><a href=/people/r/ryan-cotterell/>Ryan Cotterell</a>
|
<a href=/people/e/ekaterina-vylomova/>Ekaterina Vylomova</a>
|
<a href=/people/h/huda-khayrallah/>Huda Khayrallah</a>
|
<a href=/people/c/christo-kirov/>Christo Kirov</a>
|
<a href=/people/d/david-yarowsky/>David Yarowsky</a><br><a href=/volumes/D17-1/ class=text-muted>Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1074><div class="card-body p-3 small">The generation of complex derived word forms has been an overlooked problem in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> ; we fill this gap by applying neural sequence-to-sequence models to the task. We overview the theoretical motivation for a paradigmatic treatment of <a href=https://en.wikipedia.org/wiki/Morphological_derivation>derivational morphology</a>, and introduce the task of <a href=https://en.wikipedia.org/wiki/Morphological_derivation>derivational paradigm completion</a> as a parallel to <a href=https://en.wikipedia.org/wiki/Morphological_derivation>inflectional paradigm completion</a>. State-of-the-art neural models adapted from the inflection task are able to learn the range of derivation patterns, and outperform a <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>non-neural baseline</a> by 16.4 %. However, due to semantic, historical, and lexical considerations involved in <a href=https://en.wikipedia.org/wiki/Morphological_derivation>derivational morphology</a>, future work will be needed to achieve performance parity with inflection-generating systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1049.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1049 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1049 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1049/>Neural Multi-Source Morphological Reinflection</a></strong><br><a href=/people/k/katharina-kann/>Katharina Kann</a>
|
<a href=/people/r/ryan-cotterell/>Ryan Cotterell</a>
|
<a href=/people/h/hinrich-schutze/>Hinrich Schütze</a><br><a href=/volumes/E17-1/ class=text-muted>Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1049><div class="card-body p-3 small">We explore the task of multi-source morphological reinflection, which generalizes the standard, single-source version. The input consists of (i) a target tag and (ii) multiple pairs of source form and source tag for a lemma. The motivation is that it is beneficial to have access to more than one <a href=https://en.wikipedia.org/wiki/Form_(document)>source form</a> since different <a href=https://en.wikipedia.org/wiki/Form_(document)>source forms</a> can provide complementary information, e.g., different <a href=https://en.wikipedia.org/wiki/Word_stem>stems</a>. We further present a novel extension to the encoder-decoder recurrent neural architecture, consisting of multiple encoders, to better solve the task. We show that our new architecture outperforms single-source reinflection models and publish our dataset for multi-source morphological reinflection to facilitate future research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2018.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2018 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2018 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2018/>A Rich Morphological Tagger for <a href=https://en.wikipedia.org/wiki/English_language>English</a> : Exploring the Cross-Linguistic Tradeoff Between <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>Morphology</a> and <a href=https://en.wikipedia.org/wiki/Syntax>Syntax</a><span class=acl-fixed-case>E</span>nglish: Exploring the Cross-Linguistic Tradeoff Between Morphology and Syntax</a></strong><br><a href=/people/c/christo-kirov/>Christo Kirov</a>
|
<a href=/people/j/john-sylak-glassman/>John Sylak-Glassman</a>
|
<a href=/people/r/rebecca-knowles/>Rebecca Knowles</a>
|
<a href=/people/r/ryan-cotterell/>Ryan Cotterell</a>
|
<a href=/people/m/matt-post/>Matt Post</a><br><a href=/volumes/E17-2/ class=text-muted>Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2018><div class="card-body p-3 small">A traditional claim in <a href=https://en.wikipedia.org/wiki/Linguistics>linguistics</a> is that all human languages are equally expressiveable to convey the same wide range of meanings. Morphologically rich languages, such as <a href=https://en.wikipedia.org/wiki/Czech_language>Czech</a>, rely on overt inflectional and derivational morphology to convey many semantic distinctions. Languages with comparatively limited morphology, such as <a href=https://en.wikipedia.org/wiki/English_language>English</a>, should be able to accomplish the same using a combination of syntactic and contextual cues. We capitalize on this idea by training a tagger for <a href=https://en.wikipedia.org/wiki/English_language>English</a> that uses syntactic features obtained by automatic parsing to recover complex morphological tags projected from <a href=https://en.wikipedia.org/wiki/Czech_language>Czech</a>. The high <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of the resulting model provides quantitative confirmation of the underlying linguistic hypothesis of equal expressivity, and bodes well for future improvements in downstream HLT tasks including <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2019.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2019 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2019 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=E17-2019" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/E17-2019/>Context-Aware Prediction of Derivational Word-forms</a></strong><br><a href=/people/e/ekaterina-vylomova/>Ekaterina Vylomova</a>
|
<a href=/people/r/ryan-cotterell/>Ryan Cotterell</a>
|
<a href=/people/t/timothy-baldwin/>Timothy Baldwin</a>
|
<a href=/people/t/trevor-cohn/>Trevor Cohn</a><br><a href=/volumes/E17-2/ class=text-muted>Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2019><div class="card-body p-3 small">Derivational morphology is a fundamental and complex characteristic of language. In this paper we propose a new task of predicting the <a href=https://en.wikipedia.org/wiki/Derivation_(differential_algebra)>derivational form</a> of a given base-form lemma that is appropriate for a given context. We present an encoder-decoder style neural network to produce a derived form character-by-character, based on its corresponding character-level representation of the base form and the context. We demonstrate that our model is able to generate valid context-sensitive derivations from known base forms, but is less accurate under lexicon agnostic setting.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2028.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2028 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2028 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2028/>Explaining and Generalizing Skip-Gram through Exponential Family Principal Component Analysis</a></strong><br><a href=/people/r/ryan-cotterell/>Ryan Cotterell</a>
|
<a href=/people/a/adam-poliak/>Adam Poliak</a>
|
<a href=/people/b/benjamin-van-durme/>Benjamin Van Durme</a>
|
<a href=/people/j/jason-eisner/>Jason Eisner</a><br><a href=/volumes/E17-2/ class=text-muted>Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2028><div class="card-body p-3 small">The popular skip-gram model induces <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> by exploiting the signal from word-context coocurrence. We offer a new interpretation of <a href=https://en.wikipedia.org/wiki/Skip-gram>skip-gram</a> based on exponential family PCA-a form of matrix factorization to generalize the <a href=https://en.wikipedia.org/wiki/Skip-gram>skip-gram model</a> to tensor factorization. In turn, this lets us train <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a> through richer higher-order coocurrences, e.g., triples that include positional information (to incorporate syntax) or <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphological information</a> (to share parameters across related words). We experiment on 40 languages and show our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> improves upon <a href=https://en.wikipedia.org/wiki/Skip-gram>skip-gram</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2035.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2035 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2035 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2035/>Morphological Analysis of the Dravidian Language Family<span class=acl-fixed-case>D</span>ravidian Language Family</a></strong><br><a href=/people/a/arun-kumar/>Arun Kumar</a>
|
<a href=/people/r/ryan-cotterell/>Ryan Cotterell</a>
|
<a href=/people/l/lluis-padro/>Lluís Padró</a>
|
<a href=/people/a/antoni-oliver/>Antoni Oliver</a><br><a href=/volumes/E17-2/ class=text-muted>Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2035><div class="card-body p-3 small">The <a href=https://en.wikipedia.org/wiki/Dravidian_languages>Dravidian languages</a> are one of the most widely spoken language families in the world, yet there are very few annotated resources available to NLP researchers. To remedy this, we create DravMorph, a <a href=https://en.wikipedia.org/wiki/Speech_corpus>corpus</a> annotated for morphological segmentation and <a href=https://en.wikipedia.org/wiki/Part_of_speech>part-of-speech</a>. Additionally, we exploit novel features and higher-order models to set state-of-the-art results on these corpora on both tasks, beating techniques proposed in the literature by as much as 4 points in segmentation F1.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Ryan+Cotterell" title="Search for 'Ryan Cotterell' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/e/ekaterina-vylomova/ class=align-middle>Ekaterina Vylomova</a>
<span class="badge badge-secondary align-middle ml-2">9</span></li><li class=list-group-item><a href=/people/t/tiago-pimentel/ class=align-middle>Tiago Pimentel</a>
<span class="badge badge-secondary align-middle ml-2">7</span></li><li class=list-group-item><a href=/people/c/clara-meister/ class=align-middle>Clara Meister</a>
<span class="badge badge-secondary align-middle ml-2">5</span></li><li class=list-group-item><a href=/people/l/lawrence-wolf-sonkin/ class=align-middle>Lawrence Wolf-Sonkin</a>
<span class="badge badge-secondary align-middle ml-2">5</span></li><li class=list-group-item><a href=/people/a/arya-d-mccarthy/ class=align-middle>Arya D. McCarthy</a>
<span class="badge badge-secondary align-middle ml-2">5</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/s/shijie-wu/ class=align-middle>Shijie Wu</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/people/j/jason-eisner/ class=align-middle>Jason Eisner</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/people/g/garrett-nicolai/ class=align-middle>Garrett Nicolai</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/people/e/edoardo-maria-ponti/ class=align-middle>Edoardo Maria Ponti</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/people/i/ivan-vulic/ class=align-middle>Ivan Vulić</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/people/r/roi-reichart/ class=align-middle>Roi Reichart</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/people/a/anna-korhonen/ class=align-middle>Anna Korhonen</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/people/r/ran-zmigrod/ class=align-middle>Ran Zmigrod</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/t/tim-vieira/ class=align-middle>Tim Vieira</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/a/adina-williams/ class=align-middle>Adina Williams</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/b/brian-roark/ class=align-middle>Brian Roark</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/k/kyle-gorman/ class=align-middle>Kyle Gorman</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/h/hanna-wallach/ class=align-middle>Hanna Wallach</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/c/christo-kirov/ class=align-middle>Christo Kirov</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/m/miikka-silfverberg/ class=align-middle>Miikka Silfverberg</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/m/mans-hulden/ class=align-middle>Mans Hulden</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/s/sabrina-j-mielke/ class=align-middle>Sabrina J. Mielke</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/i/isabelle-augenstein/ class=align-middle>Isabelle Augenstein</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/k/katharina-kann/ class=align-middle>Katharina Kann</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/h/hinrich-schutze/ class=align-middle>Hinrich Schütze</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/j/jieyu-zhao/ class=align-middle>Jieyu Zhao</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/k/kai-wei-chang/ class=align-middle>Kai-Wei Chang</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/d/damian-blasi/ class=align-middle>Damián Blasi</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/a/adam-poliak/ class=align-middle>Adam Poliak</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/b/benjamin-van-durme/ class=align-middle>Benjamin Van Durme</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/e/elizabeth-salesky/ class=align-middle>Elizabeth Salesky</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/d/david-yarowsky/ class=align-middle>David Yarowsky</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/c/chaitanya-malaviya/ class=align-middle>Chaitanya Malaviya</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/h/haim-dubossarsky/ class=align-middle>Haim Dubossarsky</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/y/yevgeni-berzak/ class=align-middle>Yevgeni Berzak</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/a/alexander-miserlis-hoyle/ class=align-middle>Alexander Miserlis Hoyle</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/t/trevor-cohn/ class=align-middle>Trevor Cohn</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/t/timothy-baldwin/ class=align-middle>Timothy Baldwin</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/j/jason-wei/ class=align-middle>Jason Wei</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/kevin-duh/ class=align-middle>Kevin Duh</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/f/francisco-vargas/ class=align-middle>Francisco Vargas</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/josef-valvoda/ class=align-middle>Josef Valvoda</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/rowan-hall-maudslay/ class=align-middle>Rowan Hall Maudslay</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/alexander-erdmann/ class=align-middle>Alexander Erdmann</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/micha-elsner/ class=align-middle>Micha Elsner</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/n/nizar-habash/ class=align-middle>Nizar Habash</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/martina-forster/ class=align-middle>Martina Forster</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/sebastian-ruder/ class=align-middle>Sebastian Ruder</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yova-kementchedjhieva/ class=align-middle>Yova Kementchedjhieva</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/anders-sogaard/ class=align-middle>Anders Søgaard</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/p/pamela-shapiro/ class=align-middle>Pamela Shapiro</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/afra-amini/ class=align-middle>Afra Amini</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/p/patrick-haller/ class=align-middle>Patrick Haller</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/lena-jager/ class=align-middle>Lena Jäger</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/roger-levy/ class=align-middle>Roger Levy</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/p/pei-zhou/ class=align-middle>Pei Zhou</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/w/weijia-shi/ class=align-middle>Weijia Shi</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/kuan-hao-huang/ class=align-middle>Kuan-Hao Huang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/muhao-chen/ class=align-middle>Muhao Chen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/f/francis-ferraro/ class=align-middle>Francis Ferraro</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/sabrina-mielke/ class=align-middle>Sabrina Mielke</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/g/gabriella-lapesa/ class=align-middle>Gabriella Lapesa</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/ritesh-kumar/ class=align-middle>Ritesh Kumar</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/harald-hammarstrom/ class=align-middle>Harald Hammarström</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/huda-khayrallah/ class=align-middle>Huda Khayrallah</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/kristina-toutanova/ class=align-middle>Kristina Toutanova</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/anna-rumshisky/ class=align-middle>Anna Rumshisky</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/luke-zettlemoyer/ class=align-middle>Luke Zettlemoyer</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/dilek-hakkani-tur/ class=align-middle>Dilek Hakkani-Tur</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/i/iz-beltagy/ class=align-middle>Iz Beltagy</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/steven-bethard/ class=align-middle>Steven Bethard</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/tanmoy-chakraborty/ class=align-middle>Tanmoy Chakraborty</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yichao-zhou/ class=align-middle>Yichao Zhou</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jennifer-c-white/ class=align-middle>Jennifer C. White</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/n/naomi-saphra/ class=align-middle>Naomi Saphra</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/soren-wichmann/ class=align-middle>Søren Wichmann</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jeffrey-heinz/ class=align-middle>Jeffrey Heinz</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/manaal-faruqui/ class=align-middle>Manaal Faruqui</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/tianlu-wang/ class=align-middle>Tianlu Wang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/mark-yatskar/ class=align-middle>Mark Yatskar</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/v/vicente-ordonez/ class=align-middle>Vicente Ordonez</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/shijia-liu/ class=align-middle>Shijia Liu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/hongyuan-mei/ class=align-middle>Hongyuan Mei</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/john-sylak-glassman/ class=align-middle>John Sylak-Glassman</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/rebecca-knowles/ class=align-middle>Rebecca Knowles</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/matt-post/ class=align-middle>Matt Post</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/arun-kumar/ class=align-middle>Arun Kumar</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/lluis-padro/ class=align-middle>Lluís Padró</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/antoni-oliver/ class=align-middle>Antoni Oliver</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jason-naradowsky/ class=align-middle>Jason Naradowsky</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/e/edoardo-m-ponti/ class=align-middle>Edoardo M. Ponti</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/e/eitan-grossman/ class=align-middle>Eitan Grossman</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/johannes-bjerva/ class=align-middle>Johannes Bjerva</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/aditi-chaudhary/ class=align-middle>Aditi Chaudhary</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/g/giuseppe-g-a-celano/ class=align-middle>Giuseppe G. A. Celano</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/magdalena-markowska/ class=align-middle>Magdalena Markowska</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/emnlp/ class=align-middle>EMNLP</a><span class="badge badge-secondary align-middle ml-2">11</span></li><li class=list-group-item><a href=/venues/acl/ class=align-middle>ACL</a><span class="badge badge-secondary align-middle ml-2">9</span></li><li class=list-group-item><a href=/venues/naacl/ class=align-middle>NAACL</a><span class="badge badge-secondary align-middle ml-2">9</span></li><li class=list-group-item><a href=/venues/eacl/ class=align-middle>EACL</a><span class="badge badge-secondary align-middle ml-2">7</span></li><li class=list-group-item><a href=/venues/ws/ class=align-middle>WS</a><span class="badge badge-secondary align-middle ml-2">5</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-venues aria-expanded=false aria-controls=more-venues>show all...</li><div class="collapse border-top" id=more-venues><li class=list-group-item><a href=/venues/sigtyp/ class=align-middle>SIGTYP</a><span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/venues/sigmorphon/ class=align-middle>SIGMORPHON</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/conll/ class=align-middle>CoNLL</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/ijcnlp/ class=align-middle>IJCNLP</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/semeval/ class=align-middle>SemEval</a><span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>