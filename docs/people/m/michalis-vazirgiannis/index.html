<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Michalis Vazirgiannis - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Michalis</span> <span class=font-weight-bold>Vazirgiannis</span></h2><hr><div class=row><div class=col-lg-9><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.297.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--297 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.297 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.eacl-main.297" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.297/>Unsupervised Word Polysemy Quantification with Multiresolution Grids of Contextual Embeddings</a></strong><br><a href=/people/c/christos-xypolopoulos/>Christos Xypolopoulos</a>
|
<a href=/people/a/antoine-tixier/>Antoine Tixier</a>
|
<a href=/people/m/michalis-vazirgiannis/>Michalis Vazirgiannis</a><br><a href=/volumes/2021.eacl-main/ class=text-muted>Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--297><div class="card-body p-3 small">The number of senses of a given word, or <a href=https://en.wikipedia.org/wiki/Polysemy>polysemy</a>, is a very subjective notion, which varies widely across annotators and resources. We propose a novel method to estimate <a href=https://en.wikipedia.org/wiki/Polysemy>polysemy</a> based on simple geometry in the contextual embedding space. Our <a href=https://en.wikipedia.org/wiki/Software_development_process>approach</a> is fully unsupervised and purely data-driven. Through rigorous experiments, we show that our rankings are well correlated, with strong statistical significance, with 6 different rankings derived from famous human-constructed resources such as <a href=https://en.wikipedia.org/wiki/WordNet>WordNet</a>, OntoNotes, <a href=https://en.wikipedia.org/wiki/Oxford>Oxford</a>, <a href=https://en.wikipedia.org/wiki/Wikipedia>Wikipedia</a>, etc., for 6 different standard metrics. We also visualize and analyze the correlation between the human rankings and make interesting observations. A valuable by-product of our method is the ability to sample, at no extra cost, sentences containing different senses of a given word. Finally, the fully unsupervised nature of our approach makes <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> applicable to any language. Code and data are publicly available https://github.com/ksipos/polysemy-assessment.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.nllp-1.9.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--nllp-1--9 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.nllp-1.9 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.nllp-1.9/>JuriBERT : A Masked-Language Model Adaptation for French Legal Text<span class=acl-fixed-case>J</span>uri<span class=acl-fixed-case>BERT</span>: A Masked-Language Model Adaptation for <span class=acl-fixed-case>F</span>rench Legal Text</a></strong><br><a href=/people/s/stella-douka/>Stella Douka</a>
|
<a href=/people/h/hadi-abdine/>Hadi Abdine</a>
|
<a href=/people/m/michalis-vazirgiannis/>Michalis Vazirgiannis</a>
|
<a href=/people/r/rajaa-el-hamdani/>Rajaa El Hamdani</a>
|
<a href=/people/d/david-restrepo-amariles/>David Restrepo Amariles</a><br><a href=/volumes/2021.nllp-1/ class=text-muted>Proceedings of the Natural Legal Language Processing Workshop 2021</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--nllp-1--9><div class="card-body p-3 small">Language models have proven to be very useful when adapted to specific domains. Nonetheless, little research has been done on the adaptation of domain-specific BERT models in the <a href=https://en.wikipedia.org/wiki/French_language>French language</a>. In this paper, we focus on creating a <a href=https://en.wikipedia.org/wiki/Language_model>language model</a> adapted to <a href=https://en.wikipedia.org/wiki/Law_of_France>French legal text</a> with the goal of helping law professionals. We conclude that some specific tasks do not benefit from generic language models pre-trained on large amounts of data. We explore the use of smaller <a href=https://en.wikipedia.org/wiki/Software_architecture>architectures</a> in domain-specific sub-languages and their benefits for <a href=https://en.wikipedia.org/wiki/Law_of_France>French legal text</a>. We prove that domain-specific pre-trained models can perform better than their equivalent generalised ones in the legal domain. Finally, we release JuriBERT, a new set of BERT models adapted to the <a href=https://en.wikipedia.org/wiki/Law_of_France>French legal domain</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wnut-1.49.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wnut-1--49 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wnut-1.49 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.wnut-1.49" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.wnut-1.49/>BERTweetFR : Domain Adaptation of Pre-Trained Language Models for French Tweets<span class=acl-fixed-case>BERT</span>weet<span class=acl-fixed-case>FR</span> : Domain Adaptation of Pre-Trained Language Models for <span class=acl-fixed-case>F</span>rench Tweets</a></strong><br><a href=/people/y/yanzhu-guo/>Yanzhu Guo</a>
|
<a href=/people/v/virgile-rennard/>Virgile Rennard</a>
|
<a href=/people/c/christos-xypolopoulos/>Christos Xypolopoulos</a>
|
<a href=/people/m/michalis-vazirgiannis/>Michalis Vazirgiannis</a><br><a href=/volumes/2021.wnut-1/ class=text-muted>Proceedings of the Seventh Workshop on Noisy User-generated Text (W-NUT 2021)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wnut-1--49><div class="card-body p-3 small">We introduce BERTweetFR, the first large-scale pre-trained language model for French tweets. Our <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> is initialised using a general-domain French language model CamemBERT which follows the base architecture of BERT. Experiments show that BERTweetFR outperforms all previous general-domain French language models on two downstream Twitter NLP tasks of offensiveness identification and <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>named entity recognition</a>. The <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> used in the offensiveness detection task is first created and annotated by our team, filling in the gap of such analytic datasets in <a href=https://en.wikipedia.org/wiki/French_language>French</a>. We make our model publicly available in the transformers library with the aim of promoting future research in analytic tasks for French tweets.</div></div><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.310.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--310 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.310 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.310/>Evaluation of Greek Word Embeddings<span class=acl-fixed-case>G</span>reek Word Embeddings</a></strong><br><a href=/people/s/stamatis-outsios/>Stamatis Outsios</a>
|
<a href=/people/c/christos-karatsalos/>Christos Karatsalos</a>
|
<a href=/people/k/konstantinos-skianis/>Konstantinos Skianis</a>
|
<a href=/people/m/michalis-vazirgiannis/>Michalis Vazirgiannis</a><br><a href=/volumes/2020.lrec-1/ class=text-muted>Proceedings of the 12th Language Resources and Evaluation Conference</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--310><div class="card-body p-3 small">Since <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> have been the most popular input for many NLP tasks, evaluating their quality is critical. Most research efforts are focusing on English word embeddings. This paper addresses the problem of training and evaluating such <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> for the <a href=https://en.wikipedia.org/wiki/Greek_language>Greek language</a>. We present a new word analogy test set considering the original English Word2vec analogy test set and some specific linguistic aspects of the <a href=https://en.wikipedia.org/wiki/Greek_language>Greek language</a> as well. Moreover, we create a Greek version of WordSim353 test collection for a basic evaluation of word similarities. Produced resources are available for download. We test seven word vector models and our evaluation shows that we are able to create meaningful <a href=https://en.wikipedia.org/wiki/Representation_(mathematics)>representations</a>. Last, we discover that the <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphological complexity</a> of the <a href=https://en.wikipedia.org/wiki/Greek_language>Greek language</a> and <a href=https://en.wikipedia.org/wiki/Polysemy>polysemy</a> can influence the quality of the resulting <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a>.</div></div><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5300.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5300/>Proceedings of the Thirteenth Workshop on Graph-Based Methods for Natural Language Processing (TextGraphs-13)</a></strong><br><a href=/people/d/dmitry-ustalov/>Dmitry Ustalov</a>
|
<a href=/people/s/swapna-somasundaran/>Swapna Somasundaran</a>
|
<a href=/people/p/peter-jansen/>Peter Jansen</a>
|
<a href=/people/g/goran-glavas/>Goran Glavaš</a>
|
<a href=/people/m/martin-riedl/>Martin Riedl</a>
|
<a href=/people/m/mihai-surdeanu/>Mihai Surdeanu</a>
|
<a href=/people/m/michalis-vazirgiannis/>Michalis Vazirgiannis</a><br><a href=/volumes/D19-53/ class=text-muted>Proceedings of the Thirteenth Workshop on Graph-Based Methods for Natural Language Processing (TextGraphs-13)</a></span></p><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6113.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6113 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6113 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-6113" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-6113/>Orthogonal Matching Pursuit for Text Classification</a></strong><br><a href=/people/k/konstantinos-skianis/>Konstantinos Skianis</a>
|
<a href=/people/n/nikolaos-tziortziotis/>Nikolaos Tziortziotis</a>
|
<a href=/people/m/michalis-vazirgiannis/>Michalis Vazirgiannis</a><br><a href=/volumes/W18-61/ class=text-muted>Proceedings of the 2018 EMNLP Workshop W-NUT: The 4th Workshop on Noisy User-generated Text</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6113><div class="card-body p-3 small">In <a href=https://en.wikipedia.org/wiki/Text_classification>text classification</a>, the problem of <a href=https://en.wikipedia.org/wiki/Overfitting>overfitting</a> arises due to the high dimensionality, making <a href=https://en.wikipedia.org/wiki/Regularization_(mathematics)>regularization</a> essential. Although classic <a href=https://en.wikipedia.org/wiki/Regularization_(mathematics)>regularizers</a> provide sparsity, they fail to return highly accurate models. On the contrary, state-of-the-art group-lasso regularizers provide better results at the expense of low sparsity. In this paper, we apply a greedy variable selection algorithm, called Orthogonal Matching Pursuit, for the text classification task. We also extend standard group OMP by introducing overlapping Group OMP to handle overlapping groups of features. Empirical analysis verifies that both OMP and overlapping GOMP constitute powerful <a href=https://en.wikipedia.org/wiki/Regularization_(mathematics)>regularizers</a>, able to produce effective and very sparse models. Code and data are available online.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1062.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1062 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1062 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1062.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1062.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-1062" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P18-1062/>Unsupervised Abstractive Meeting Summarization with Multi-Sentence Compression and Budgeted Submodular Maximization</a></strong><br><a href=/people/g/guokan-shang/>Guokan Shang</a>
|
<a href=/people/w/wensi-ding/>Wensi Ding</a>
|
<a href=/people/z/zekun-zhang/>Zekun Zhang</a>
|
<a href=/people/a/antoine-tixier/>Antoine Tixier</a>
|
<a href=/people/p/polykarpos-meladianos/>Polykarpos Meladianos</a>
|
<a href=/people/m/michalis-vazirgiannis/>Michalis Vazirgiannis</a>
|
<a href=/people/j/jean-pierre-lorre/>Jean-Pierre Lorré</a><br><a href=/volumes/P18-1/ class=text-muted>Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1062><div class="card-body p-3 small">We introduce a novel graph-based framework for abstractive meeting speech summarization that is fully unsupervised and does not rely on any annotations. Our <a href=https://en.wikipedia.org/wiki/Work_(physics)>work</a> combines the strengths of multiple recent <a href=https://en.wikipedia.org/wiki/Scientific_method>approaches</a> while addressing their weaknesses. Moreover, we leverage recent advances in <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> and <a href=https://en.wikipedia.org/wiki/Graph_degeneracy>graph degeneracy</a> applied to <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> to take exterior semantic knowledge into account, and to design custom diversity and informativeness measures. Experiments on the AMI and ICSI corpus show that our system improves on the <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a>. Code and data are publicly available, and our <a href=https://en.wikipedia.org/wiki/System>system</a> can be interactively tested.</div></div><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4507.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4507 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4507 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W17-4507" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W17-4507/>Combining <a href=https://en.wikipedia.org/wiki/Graph_degeneracy>Graph Degeneracy</a> and Submodularity for Unsupervised Extractive Summarization</a></strong><br><a href=/people/a/antoine-tixier/>Antoine Tixier</a>
|
<a href=/people/p/polykarpos-meladianos/>Polykarpos Meladianos</a>
|
<a href=/people/m/michalis-vazirgiannis/>Michalis Vazirgiannis</a><br><a href=/volumes/W17-45/ class=text-muted>Proceedings of the Workshop on New Frontiers in Summarization</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4507><div class="card-body p-3 small">We present a fully unsupervised, extractive text summarization system that leverages a submodularity framework introduced by past research. The <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> allows summaries to be generated in a greedy way while preserving near-optimal performance guarantees. Our main contribution is the novel coverage reward term of the <a href=https://en.wikipedia.org/wiki/Loss_function>objective function</a> optimized by the <a href=https://en.wikipedia.org/wiki/Greedy_algorithm>greedy algorithm</a>. This component builds on the graph-of-words representation of text and the k-core decomposition algorithm to assign meaningful scores to words. We evaluate our approach on the AMI and ICSI meeting speech corpora, and on the DUC2001 news corpus. We reach state-of-the-art performance on all <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>. Results indicate that our method is particularly well-suited to the meeting domain.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1202.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1202 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1202 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1202/>Shortest-Path Graph Kernels for Document Similarity</a></strong><br><a href=/people/g/giannis-nikolentzos/>Giannis Nikolentzos</a>
|
<a href=/people/p/polykarpos-meladianos/>Polykarpos Meladianos</a>
|
<a href=/people/f/francois-rousseau/>François Rousseau</a>
|
<a href=/people/y/yannis-stavrakas/>Yannis Stavrakas</a>
|
<a href=/people/m/michalis-vazirgiannis/>Michalis Vazirgiannis</a><br><a href=/volumes/D17-1/ class=text-muted>Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1202><div class="card-body p-3 small">In this paper, we present a novel document similarity measure based on the definition of a <a href=https://en.wikipedia.org/wiki/Graph_kernel>graph kernel</a> between pairs of documents. The proposed measure takes into account both the terms contained in the documents and the relationships between them. By representing each document as a graph-of-words, we are able to model these relationships and then determine how similar two documents are by using a modified shortest-path graph kernel. We evaluate our approach on two tasks and compare it against several baseline approaches using various <a href=https://en.wikipedia.org/wiki/Performance_metric>performance metrics</a> such as DET curves and macro-average F1-score. Experimental results on a range of <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> showed that our proposed approach outperforms traditional techniques and is capable of measuring more accurately the similarity between two documents.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-info align-middle mr-1" href=#abstract-D17-3003 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-3003 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-3003/>Graph-based Text Representations: Boosting Text Mining, <span class=acl-fixed-case>NLP</span> and Information Retrieval with Graphs</a></strong><br><a href=/people/f/fragkiskos-d-malliaros/>Fragkiskos D. Malliaros</a>
|
<a href=/people/m/michalis-vazirgiannis/>Michalis Vazirgiannis</a><br><a href=/volumes/D17-3/ class=text-muted>Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing: Tutorial Abstracts</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-3003><div class="card-body p-3 small">Graphs or networks have been widely used as modeling tools in Natural Language Processing (NLP), Text Mining (TM) and Information Retrieval (IR). Traditionally, the unigram bag-of-words representation is applied; that way, a document is represented as a multiset of its terms, disregarding dependencies between the terms. Although several variants and extensions of this modeling approach have been proposed (e.g., the n-gram model), the main weakness comes from the underlying term independence assumption. The order of the terms within a document is completely disregarded and any relationship between terms is not taken into account in the final task (e.g., text categorization). Nevertheless, as the heterogeneity of text collections is increasing (especially with respect to document length and vocabulary), the research community has started exploring different document representations aiming to capture more fine-grained contexts of co-occurrence between different terms, challenging the well-established unigram bag-of-words model. To this direction, graphs constitute a well-developed model that has been adopted for text representation. The goal of this tutorial is to offer a comprehensive presentation of recent methods that rely on graph-based text representations to deal with various tasks in NLP and IR. We will describe basic as well as novel graph theoretic concepts and we will examine how they can be applied in a wide range of text-related application domains.\n\nAll the material associated to the tutorial will be available at: http://fragkiskosm.github.io/projects/graph_text_tutorial</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2072.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2072 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2072 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2072/>Multivariate Gaussian Document Representation from Word Embeddings for Text Categorization<span class=acl-fixed-case>G</span>aussian Document Representation from Word Embeddings for Text Categorization</a></strong><br><a href=/people/g/giannis-nikolentzos/>Giannis Nikolentzos</a>
|
<a href=/people/p/polykarpos-meladianos/>Polykarpos Meladianos</a>
|
<a href=/people/f/francois-rousseau/>François Rousseau</a>
|
<a href=/people/y/yannis-stavrakas/>Yannis Stavrakas</a>
|
<a href=/people/m/michalis-vazirgiannis/>Michalis Vazirgiannis</a><br><a href=/volumes/E17-2/ class=text-muted>Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2072><div class="card-body p-3 small">Recently, there has been a lot of activity in learning distributed representations of words in <a href=https://en.wikipedia.org/wiki/Vector_space>vector spaces</a>. Although there are <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> capable of learning high-quality distributed representations of words, how to generate <a href=https://en.wikipedia.org/wiki/Vector_graphics>vector representations</a> of the same quality for phrases or documents still remains a challenge. In this paper, we propose to model each document as a multivariate Gaussian distribution based on the distributed representations of its words. We then measure the <a href=https://en.wikipedia.org/wiki/Similarity_measure>similarity</a> between two documents based on the similarity of their distributions. Experiments on eight standard text categorization datasets demonstrate the effectiveness of the proposed approach in comparison with state-of-the-art methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2074.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2074 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2074 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2074/>Real-Time Keyword Extraction from Conversations</a></strong><br><a href=/people/p/polykarpos-meladianos/>Polykarpos Meladianos</a>
|
<a href=/people/a/antoine-tixier/>Antoine Tixier</a>
|
<a href=/people/i/ioannis-nikolentzos/>Ioannis Nikolentzos</a>
|
<a href=/people/m/michalis-vazirgiannis/>Michalis Vazirgiannis</a><br><a href=/volumes/E17-2/ class=text-muted>Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2074><div class="card-body p-3 small">We introduce a novel <a href=https://en.wikipedia.org/wiki/Scientific_method>method</a> to extract <a href=https://en.wikipedia.org/wiki/Index_term>keywords</a> from meeting speech in real-time. Our approach builds on the graph-of-words representation of text and leverages the k-core decomposition algorithm and properties of submodular functions. We outperform multiple baselines in a real-time scenario emulated from the AMI and ICSI meeting corpora. Evaluation is conducted against both extractive and abstractive gold standard using two standard <a href=https://en.wikipedia.org/wiki/Performance_metric>performance metrics</a> and a newer one based on <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a>.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Michalis+Vazirgiannis" title="Search for 'Michalis Vazirgiannis' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/p/polykarpos-meladianos/ class=align-middle>Polykarpos Meladianos</a>
<span class="badge badge-secondary align-middle ml-2">5</span></li><li class=list-group-item><a href=/people/a/antoine-tixier/ class=align-middle>Antoine Tixier</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/people/c/christos-xypolopoulos/ class=align-middle>Christos Xypolopoulos</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/g/giannis-nikolentzos/ class=align-middle>Giannis Nikolentzos</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/f/francois-rousseau/ class=align-middle>François Rousseau</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/y/yannis-stavrakas/ class=align-middle>Yannis Stavrakas</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/k/konstantinos-skianis/ class=align-middle>Konstantinos Skianis</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/s/stella-douka/ class=align-middle>Stella Douka</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/hadi-abdine/ class=align-middle>Hadi Abdine</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/rajaa-el-hamdani/ class=align-middle>Rajaa El Hamdani</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/david-restrepo-amariles/ class=align-middle>David Restrepo Amariles</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/dmitry-ustalov/ class=align-middle>Dmitry Ustalov</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/swapna-somasundaran/ class=align-middle>Swapna Somasundaran</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/p/peter-jansen/ class=align-middle>Peter Jansen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/g/goran-glavas/ class=align-middle>Goran Glavaš</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/martin-riedl/ class=align-middle>Martin Riedl</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/mihai-surdeanu/ class=align-middle>Mihai Surdeanu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/f/fragkiskos-d-malliaros/ class=align-middle>Fragkiskos D. Malliaros</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/n/nikolaos-tziortziotis/ class=align-middle>Nikolaos Tziortziotis</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/stamatis-outsios/ class=align-middle>Stamatis Outsios</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/christos-karatsalos/ class=align-middle>Christos Karatsalos</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yanzhu-guo/ class=align-middle>Yanzhu Guo</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/v/virgile-rennard/ class=align-middle>Virgile Rennard</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/i/ioannis-nikolentzos/ class=align-middle>Ioannis Nikolentzos</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/g/guokan-shang/ class=align-middle>Guokan Shang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/w/wensi-ding/ class=align-middle>Wensi Ding</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/zekun-zhang/ class=align-middle>Zekun Zhang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jean-pierre-lorre/ class=align-middle>Jean-Pierre Lorré</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/eacl/ class=align-middle>EACL</a><span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/venues/emnlp/ class=align-middle>EMNLP</a><span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/venues/ws/ class=align-middle>WS</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/nllp/ class=align-middle>NLLP</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/lrec/ class=align-middle>LREC</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-venues aria-expanded=false aria-controls=more-venues>show all...</li><div class="collapse border-top" id=more-venues><li class=list-group-item><a href=/venues/wnut/ class=align-middle>WNUT</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/acl/ class=align-middle>ACL</a><span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>