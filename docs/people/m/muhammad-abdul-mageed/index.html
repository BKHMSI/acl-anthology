<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Muhammad Abdul-Mageed - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Muhammad</span> <span class=font-weight-bold>Abdul-Mageed</span></h2><p class="font-weight-light text-muted"><span class=font-italic>Also published as:</span>
Muhammad <span class=font-weight-normal>Abdul Mageed</span></p><hr><div class=row><div class=col-lg-9><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.551.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--551 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.551 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.551/>ARBERT & MARBERT : Deep Bidirectional Transformers for Arabic<span class=acl-fixed-case>ARBERT</span> & <span class=acl-fixed-case>MARBERT</span>: Deep Bidirectional Transformers for <span class=acl-fixed-case>A</span>rabic</a></strong><br><a href=/people/m/muhammad-abdul-mageed/>Muhammad Abdul-Mageed</a>
|
<a href=/people/a/abdelrahim-elmadany/>AbdelRahim Elmadany</a>
|
<a href=/people/e/el-moatez-billah-nagoudi/>El Moatez Billah Nagoudi</a><br><a href=/volumes/2021.acl-long/ class=text-muted>Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--551><div class="card-body p-3 small">Pre-trained language models (LMs) are currently integral to many <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing systems</a>. Although multilingual LMs were also introduced to serve many languages, these have limitations such as being costly at inference time and the size and diversity of non-English data involved in their pre-training. We remedy these issues for a collection of diverse Arabic varieties by introducing two powerful deep bidirectional transformer-based models, ARBERT and MARBERT. To evaluate our models, we also introduce ARLUE, a new <a href=https://en.wikipedia.org/wiki/Benchmarking>benchmark</a> for multi-dialectal Arabic language understanding evaluation. ARLUE is built using 42 <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> targeting six different task clusters, allowing us to offer a series of standardized experiments under rich conditions. When fine-tuned on ARLUE, our models collectively achieve new state-of-the-art results across the majority of tasks (37 out of 48 classification tasks, on the 42 datasets). Our best <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> acquires the highest ARLUE score (77.40) across all six task clusters, outperforming all other <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> including XLM-R Large (3.4x larger size). Our models are publicly available at https://github.com/UBC-NLP/marbert and ARLUE will be released through the same repository.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.calcs-1.6.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--calcs-1--6 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.calcs-1.6 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.calcs-1.6/>Exploring Text-to-Text Transformers for English to Hinglish Machine Translation with Synthetic Code-Mixing<span class=acl-fixed-case>E</span>nglish to <span class=acl-fixed-case>H</span>inglish Machine Translation with Synthetic Code-Mixing</a></strong><br><a href=/people/g/ganesh-jawahar/>Ganesh Jawahar</a>
|
<a href=/people/e/el-moatez-billah-nagoudi/>El Moatez Billah Nagoudi</a>
|
<a href=/people/m/muhammad-abdul-mageed/>Muhammad Abdul-Mageed</a>
|
<a href=/people/l/laks-lakshmanan-v-s/>Laks Lakshmanan, V.S.</a><br><a href=/volumes/2021.calcs-1/ class=text-muted>Proceedings of the Fifth Workshop on Computational Approaches to Linguistic Code-Switching</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--calcs-1--6><div class="card-body p-3 small">We describe models focused at the understudied problem of <a href=https://en.wikipedia.org/wiki/Translation>translating</a> between monolingual and code-mixed language pairs. More specifically, we offer a wide range of models that convert monolingual English text into Hinglish (code-mixed Hindi and English). Given the recent success of pretrained language models, we also test the utility of two recent Transformer-based encoder-decoder models (i.e., mT5 and mBART) on the task finding both to work well. Given the paucity of training data for <a href=https://en.wikipedia.org/wiki/Code_mixing>code-mixing</a>, we also propose a dependency-free method for generating code-mixed texts from bilingual distributed representations that we exploit for improving <a href=https://en.wikipedia.org/wiki/Language_model>language model</a> performance. In particular, armed with this additional data, we adopt a curriculum learning approach where we first finetune the <a href=https://en.wikipedia.org/wiki/Language_model>language models</a> on <a href=https://en.wikipedia.org/wiki/Synthetic_data>synthetic data</a> then on gold code-mixed data. We find that, although simple, our synthetic code-mixing method is competitive with (and in some cases is even superior to) several standard methods (backtranslation, <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>method</a> based on equivalence constraint theory) under a diverse set of conditions. Our work shows that the mT5 model, finetuned following the <a href=https://en.wikipedia.org/wiki/Curriculum>curriculum learning procedure</a>, achieves best translation performance (12.67 BLEU). Our <a href=https://en.wikipedia.org/wiki/Model_(person)>models</a> place first in the overall ranking of the English-Hinglish official shared task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.65.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--65 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.65 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.eacl-main.65" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.65/>Self-Training Pre-Trained Language Models for Zero- and Few-Shot Multi-Dialectal Arabic Sequence Labeling<span class=acl-fixed-case>A</span>rabic Sequence Labeling</a></strong><br><a href=/people/m/muhammad-khalifa/>Muhammad Khalifa</a>
|
<a href=/people/m/muhammad-abdul-mageed/>Muhammad Abdul-Mageed</a>
|
<a href=/people/k/khaled-shaalan/>Khaled Shaalan</a><br><a href=/volumes/2021.eacl-main/ class=text-muted>Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--65><div class="card-body p-3 small">A sufficient amount of annotated data is usually required to fine-tune pre-trained language models for downstream tasks. Unfortunately, attaining <a href=https://en.wikipedia.org/wiki/Labeled_data>labeled data</a> can be costly, especially for <a href=https://en.wikipedia.org/wiki/Variety_(linguistics)>multiple language varieties</a> and dialects. We propose to self-train pre-trained language models in zero- and few-shot scenarios to improve performance on data-scarce varieties using only resources from data-rich ones. We demonstrate the utility of our approach in the context of Arabic sequence labeling by using a language model fine-tuned on Modern Standard Arabic (MSA) only to predict named entities (NE) and part-of-speech (POS) tags on several dialectal Arabic (DA) varieties. We show that self-training is indeed powerful, improving zero-shot MSA-to-DA transfer by as large as 10 % F_1 (NER) and 2 % accuracy (POS tagging). We acquire even better performance in few-shot scenarios with limited amounts of <a href=https://en.wikipedia.org/wiki/Label_(computer_science)>labeled data</a>. We conduct an ablation study and show that the performance boost observed directly results from training data augmentation possible with DA examples via self-training. This opens up opportunities for developing DA models exploiting only MSA resources. Our approach can also be extended to other languages and tasks.<tex-math>_1</tex-math> (NER) and 2% accuracy (POS tagging). We acquire even better performance in few-shot scenarios with limited amounts of labeled data. We conduct an ablation study and show that the performance boost observed directly results from training data augmentation possible with DA examples via self-training. This opens up opportunities for developing DA models exploiting only MSA resources. Our approach can also be extended to other languages and tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wmt-1.27.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wmt-1--27 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wmt-1.27 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wmt-1.27/>Improving Similar Language Translation With <a href=https://en.wikipedia.org/wiki/Transfer_of_learning>Transfer Learning</a></a></strong><br><a href=/people/i/ife-adebara/>Ife Adebara</a>
|
<a href=/people/m/muhammad-abdul-mageed/>Muhammad Abdul-Mageed</a><br><a href=/volumes/2021.wmt-1/ class=text-muted>Proceedings of the Sixth Conference on Machine Translation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wmt-1--27><div class="card-body p-3 small">We investigate <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a> based on pre-trained neural machine translation models to translate between (low-resource) similar languages. This work is part of our contribution to the WMT 2021 Similar Languages Translation Shared Task where we submitted models for different language pairs, including French-Bambara, Spanish-Catalan, and Spanish-Portuguese in both directions. Our <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> for Catalan-Spanish (82.79 BLEU)and Portuguese-Spanish (87.11 BLEU) rank top 1 in the official shared task evaluation, and we are the only team to submit <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> for the <a href=https://en.wikipedia.org/wiki/Bambara_language>French-Bambara pairs</a>.</div></div><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.208.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--208 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.208 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.coling-main.208" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.208/>Automatic Detection of Machine Generated Text : A Critical Survey</a></strong><br><a href=/people/g/ganesh-jawahar/>Ganesh Jawahar</a>
|
<a href=/people/m/muhammad-abdul-mageed/>Muhammad Abdul-Mageed</a>
|
<a href=/people/l/laks-lakshmanan-v-s/>Laks Lakshmanan, V.S.</a><br><a href=/volumes/2020.coling-main/ class=text-muted>Proceedings of the 28th International Conference on Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--208><div class="card-body p-3 small">Text generative models (TGMs) excel in producing text that matches the style of human language reasonably well. Such TGMs can be misused by adversaries, e.g., by automatically generating fake news and fake product reviews that can look authentic and fool humans. Detectors that can distinguish text generated by <a href=https://en.wikipedia.org/wiki/Time-division_multiple_access>TGM</a> from human written text play a vital role in mitigating such misuse of <a href=https://en.wikipedia.org/wiki/Time-division_multiple_access>TGMs</a>. Recently, there has been a flurry of works from both <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing (NLP)</a> and <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning (ML) communities</a> to build accurate detectors for <a href=https://en.wikipedia.org/wiki/English_language>English</a>. Despite the importance of this problem, there is currently no work that surveys this fast-growing literature and introduces newcomers to important research challenges. In this work, we fill this void by providing a critical survey and review of this literature to facilitate a comprehensive understanding of this problem. We conduct an in-depth error analysis of the state-of-the-art detector and discuss research directions to guide future work in this exciting area.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wanlp-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.wanlp-1.0/>Proceedings of the Fifth Arabic Natural Language Processing Workshop</a></strong><br><a href=/people/i/imed-zitouni/>Imed Zitouni</a>
|
<a href=/people/m/muhammad-abdul-mageed/>Muhammad Abdul-Mageed</a>
|
<a href=/people/h/houda-bouamor/>Houda Bouamor</a>
|
<a href=/people/f/fethi-bougares/>Fethi Bougares</a>
|
<a href=/people/m/mahmoud-el-haj/>Mahmoud El-Haj</a>
|
<a href=/people/n/nadi-tomeh/>Nadi Tomeh</a>
|
<a href=/people/w/wajdi-zaghouani/>Wajdi Zaghouani</a><br><a href=/volumes/2020.wanlp-1/ class=text-muted>Proceedings of the Fifth Arabic Natural Language Processing Workshop</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wmt-1.42.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wmt-1--42 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wmt-1.42 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939640 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.wmt-1.42/>Translating Similar Languages : Role of <a href=https://en.wikipedia.org/wiki/Mutual_intelligibility>Mutual Intelligibility</a> in Multilingual Transformers</a></strong><br><a href=/people/i/ife-adebara/>Ife Adebara</a>
|
<a href=/people/e/el-moatez-billah-nagoudi/>El Moatez Billah Nagoudi</a>
|
<a href=/people/m/muhammad-abdul-mageed/>Muhammad Abdul Mageed</a><br><a href=/volumes/2020.wmt-1/ class=text-muted>Proceedings of the Fifth Conference on Machine Translation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wmt-1--42><div class="card-body p-3 small">In this work we investigate different approaches to translate between similar languages despite low resource limitations. This work is done as the participation of the UBC NLP research group in the WMT 2019 Similar Languages Translation Shared Task. We participated in all language pairs and performed various experiments. We used a transformer architecture for all the <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> and used <a href=https://en.wikipedia.org/wiki/Back-translation>back-translation</a> for one of the language pairs. We explore both bilingual and multi-lingual approaches. We describe the pre-processing, training, translation and results for each <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a>. We also investigate the role of <a href=https://en.wikipedia.org/wiki/Mutual_intelligibility>mutual intelligibility</a> in <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> performance.</div></div><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S19-2136.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S19-2136 data-toggle=collapse aria-expanded=false aria-controls=abstract-S19-2136 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/S19-2136/>UBC-NLP at SemEval-2019 Task 6 : Ensemble Learning of Offensive Content With Enhanced Training Data<span class=acl-fixed-case>UBC</span>-<span class=acl-fixed-case>NLP</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2019 Task 6: Ensemble Learning of Offensive Content With Enhanced Training Data</a></strong><br><a href=/people/a/arun-rajendran/>Arun Rajendran</a>
|
<a href=/people/c/chiyu-zhang/>Chiyu Zhang</a>
|
<a href=/people/m/muhammad-abdul-mageed/>Muhammad Abdul-Mageed</a><br><a href=/volumes/S19-2/ class=text-muted>Proceedings of the 13th International Workshop on Semantic Evaluation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S19-2136><div class="card-body p-3 small">We examine learning offensive content on <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a> with limited, imbalanced data. For the purpose, we investigate the utility of using various data enhancement methods with a host of classical <a href=https://en.wikipedia.org/wiki/Ensemble_learning>ensemble classifiers</a>. Among the 75 participating teams in SemEval-2019 sub-task B, our <a href=https://en.wikipedia.org/wiki/System>system</a> ranks 6th (with 0.706 macro F1-score). For sub-task C, among the 65 participating teams, our <a href=https://en.wikipedia.org/wiki/System>system</a> ranks 9th (with 0.587 macro F1-score).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5431.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5431 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5431 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5431/>Neural Machine Translation of Low-Resource and Similar Languages with Backtranslation</a></strong><br><a href=/people/m/michael-przystupa/>Michael Przystupa</a>
|
<a href=/people/m/muhammad-abdul-mageed/>Muhammad Abdul-Mageed</a><br><a href=/volumes/W19-54/ class=text-muted>Proceedings of the Fourth Conference on Machine Translation (Volume 3: Shared Task Papers, Day 2)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5431><div class="card-body p-3 small">We present our contribution to the WMT19 Similar Language Translation shared task. We investigate the utility of <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a> on three low-resource, similar language pairs : <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish Portuguese</a>, <a href=https://en.wikipedia.org/wiki/Czech_language>Czech Polish</a>, and <a href=https://en.wikipedia.org/wiki/Hindi>Hindi Nepali</a>. Since state-of-the-art neural machine translation systems still require large amounts of bitext, which we do not have for the pairs we consider, we focus primarily on incorporating monolingual data into our models with backtranslation. In our analysis, we found Transformer models to work best on Spanish Portuguese and Czech Polish translation, whereas LSTMs with global attention worked best on Hindi Nepali translation.</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3930.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3930 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3930 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3930/>Deep Models for Arabic Dialect Identification on Benchmarked Data<span class=acl-fixed-case>A</span>rabic Dialect Identification on Benchmarked Data</a></strong><br><a href=/people/m/mohamed-elaraby/>Mohamed Elaraby</a>
|
<a href=/people/m/muhammad-abdul-mageed/>Muhammad Abdul-Mageed</a><br><a href=/volumes/W18-39/ class=text-muted>Proceedings of the Fifth Workshop on NLP for Similar Languages, Varieties and Dialects (VarDial 2018)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3930><div class="card-body p-3 small">The Arabic Online Commentary (AOC) (Zaidan and Callison-Burch, 2011) is a large-scale repos-itory of <a href=https://en.wikipedia.org/wiki/Varieties_of_Arabic>Arabic dialects</a> with manual labels for4varieties of the language. Existing dialect iden-tification models exploiting the dataset pre-date the recent boost <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning</a> brought to NLPand hence the data are not benchmarked for use with <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning</a>, nor is it clear how much neural networks can help tease the categories in the data apart. We treat these two limitations : We (1) benchmark the data, and (2) empirically test6different deep learning methods on <a href=https://en.wikipedia.org/wiki/Task_(project_management)>thetask</a>, comparing peformance to several classical machine learning models under different condi-tions (i.e., both binary and multi-way classification). Our experimental results show that variantsof (attention-based) bidirectional recurrent neural networks achieve best accuracy (acc) on thetask, significantly outperforming all competitive baselines. On <a href=https://en.wikipedia.org/wiki/Blinded_experiment>blind test data</a>, our <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> reach87.65%acc on the binary task (MSA vs. dialects),87.4%acc on the 3-way dialect task (Egyptianvs. Gulf vs. Levantine), and82.45%acc on the 4-way variants task (MSA vs. Egyptian vs. Gulfvs. Levantine). We release our benchmark for future work on the dataset</div></div><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1067.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1067 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1067 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234955738 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-1067/>EmoNet : Fine-Grained Emotion Detection with Gated Recurrent Neural Networks<span class=acl-fixed-case>E</span>mo<span class=acl-fixed-case>N</span>et: Fine-Grained Emotion Detection with Gated Recurrent Neural Networks</a></strong><br><a href=/people/m/muhammad-abdul-mageed/>Muhammad Abdul-Mageed</a>
|
<a href=/people/l/lyle-ungar/>Lyle Ungar</a><br><a href=/volumes/P17-1/ class=text-muted>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1067><div class="card-body p-3 small">Accurate <a href=https://en.wikipedia.org/wiki/Emotion_detection>detection of emotion</a> from <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language</a> has applications ranging from building emotional chatbots to better understanding individuals and their lives. However, progress on <a href=https://en.wikipedia.org/wiki/Emotion_detection>emotion detection</a> has been hampered by the absence of large labeled datasets. In this work, we build a very large dataset for fine-grained emotions and develop <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning models</a> on it. We achieve a new <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a> on 24 fine-grained types of emotions (with an average <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of 87.58 %). We also extend the task beyond <a href=https://en.wikipedia.org/wiki/Emotion_classification>emotion types</a> to model Robert Plutick&#8217;s 8 primary emotion dimensions, acquiring a superior <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of 95.68 %.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1318.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1318 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1318 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1318/>Not All Segments are Created Equal : Syntactically Motivated Sentiment Analysis in Lexical Space</a></strong><br><a href=/people/m/muhammad-abdul-mageed/>Muhammad Abdul-Mageed</a><br><a href=/volumes/W17-13/ class=text-muted>Proceedings of the Third Arabic Natural Language Processing Workshop</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1318><div class="card-body p-3 small">Although there is by now a considerable amount of research on subjectivity and sentiment analysis on morphologically-rich languages, it is still unclear how lexical information can best be modeled in these <a href=https://en.wikipedia.org/wiki/Language>languages</a>. To bridge this gap, we build effective models exploiting exclusively gold- and machine-segmented lexical input and successfully employ syntactically motivated feature selection to improve <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a>. Our best <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> achieve significantly above the baselines, with 67.93 % and 69.37 % accuracies for subjectivity and sentiment classification respectively.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Muhammad+Abdul-Mageed" title="Search for 'Muhammad Abdul-Mageed' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/e/el-moatez-billah-nagoudi/ class=align-middle>El Moatez Billah Nagoudi</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/g/ganesh-jawahar/ class=align-middle>Ganesh Jawahar</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/l/laks-lakshmanan-v-s/ class=align-middle>Laks Lakshmanan, V.S.</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/i/ife-adebara/ class=align-middle>Ife Adebara</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/a/abdelrahim-elmadany/ class=align-middle>AbdelRahim Elmadany</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/l/lyle-ungar/ class=align-middle>Lyle Ungar</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/muhammad-khalifa/ class=align-middle>Muhammad Khalifa</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/khaled-shaalan/ class=align-middle>Khaled Shaalan</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/arun-rajendran/ class=align-middle>Arun Rajendran</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/chiyu-zhang/ class=align-middle>Chiyu Zhang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/mohamed-elaraby/ class=align-middle>Mohamed Elaraby</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/michael-przystupa/ class=align-middle>Michael Przystupa</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/i/imed-zitouni/ class=align-middle>Imed Zitouni</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/houda-bouamor/ class=align-middle>Houda Bouamor</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/f/fethi-bougares/ class=align-middle>Fethi Bougares</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/mahmoud-el-haj/ class=align-middle>Mahmoud El-Haj</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/n/nadi-tomeh/ class=align-middle>Nadi Tomeh</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/w/wajdi-zaghouani/ class=align-middle>Wajdi Zaghouani</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/ws/ class=align-middle>WS</a><span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/venues/acl/ class=align-middle>ACL</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/wmt/ class=align-middle>WMT</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/calcs/ class=align-middle>CALCS</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/eacl/ class=align-middle>EACL</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-venues aria-expanded=false aria-controls=more-venues>show all...</li><div class="collapse border-top" id=more-venues><li class=list-group-item><a href=/venues/semeval/ class=align-middle>SemEval</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/coling/ class=align-middle>COLING</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/wanlp/ class=align-middle>WANLP</a><span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>