<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Marine Carpuat - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Marine</span> <span class=font-weight-bold>Carpuat</span></h2><hr><div class=row><div class=col-lg-9><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eval4nlp-1.22.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eval4nlp-1--22 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eval4nlp-1.22 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.eval4nlp-1.22/>The UMD Submission to the Explainable MT Quality Estimation Shared Task : Combining Explanation Models with Sequence Labeling<span class=acl-fixed-case>UMD</span> Submission to the Explainable <span class=acl-fixed-case>MT</span> Quality Estimation Shared Task: Combining Explanation Models with Sequence Labeling</a></strong><br><a href=/people/t/tasnim-kabir/>Tasnim Kabir</a>
|
<a href=/people/m/marine-carpuat/>Marine Carpuat</a><br><a href=/volumes/2021.eval4nlp-1/ class=text-muted>Proceedings of the 2nd Workshop on Evaluation and Comparison of NLP Systems</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eval4nlp-1--22><div class="card-body p-3 small">This paper describes the UMD submission to the Explainable Quality Estimation Shared Task at the EMNLP 2021 Workshop on Evaluation & Comparison of NLP Systems. We participated in the word-level and sentence-level MT Quality Estimation (QE) constrained tasks for all language pairs : Estonian-English, Romanian-English, German-Chinese, and Russian-German. Our approach combines the predictions of a word-level explainer model on top of a sentence-level QE model and a sequence labeler trained on synthetic data. These <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> are based on pre-trained multilingual language models and do not require any word-level annotations for training, making them well suited to zero-shot settings. Our best-performing system improves over the best baseline across all metrics and language pairs, with an average gain of 0.1 in AUC, <a href=https://en.wikipedia.org/wiki/Average_precision>Average Precision</a>, and <a href=https://en.wikipedia.org/wiki/Recall_(memory)>Recall</a> at Top-K score.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.562.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--562 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.562 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.562" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.562/>Beyond Noise : Mitigating the Impact of Fine-grained Semantic Divergences on Neural Machine Translation</a></strong><br><a href=/people/e/eleftheria-briakou/>Eleftheria Briakou</a>
|
<a href=/people/m/marine-carpuat/>Marine Carpuat</a><br><a href=/volumes/2021.acl-long/ class=text-muted>Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--562><div class="card-body p-3 small">While it has been shown that Neural Machine Translation (NMT) is highly sensitive to noisy parallel training samples, prior work treats all types of mismatches between source and target as <a href=https://en.wikipedia.org/wiki/Noise_(signal_processing)>noise</a>. As a result, it remains unclear how samples that are mostly equivalent but contain a small number of semantically divergent tokens impact NMT training. To close this gap, we analyze the impact of different types of fine-grained semantic divergences on Transformer models. We show that <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> trained on synthetic divergences output degenerated text more frequently and are less confident in their predictions. Based on these findings, we introduce a divergent-aware NMT framework that uses factors to help NMT recover from the degradation caused by naturally occurring divergences, improving both translation quality and model calibration on EN-FR tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.477.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--477 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.477 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.477" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.477/>Rule-based Morphological Inflection Improves Neural Terminology Translation</a></strong><br><a href=/people/w/weijia-xu/>Weijia Xu</a>
|
<a href=/people/m/marine-carpuat/>Marine Carpuat</a><br><a href=/volumes/2021.emnlp-main/ class=text-muted>Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--477><div class="card-body p-3 small">Current approaches to incorporating terminology constraints in <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation (MT)</a> typically assume that the constraint terms are provided in their correct <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphological forms</a>. This limits their application to real-world scenarios where <a href=https://en.wikipedia.org/wiki/Constraint_(mathematics)>constraint terms</a> are provided as lemmas. In this paper, we introduce a modular framework for incorporating <a href=https://en.wikipedia.org/wiki/Constraint_(mathematics)>lemma constraints</a> in neural MT (NMT) in which linguistic knowledge and diverse types of NMT models can be flexibly applied. It is based on a novel cross-lingual inflection module that inflects the target lemma constraints based on the source context. We explore linguistically motivated rule-based and data-driven neural-based inflection modules and design English-German health and English-Lithuanian news test suites to evaluate them in domain adaptation and low-resource MT settings. Results show that our rule-based inflection module helps NMT models incorporate <a href=https://en.wikipedia.org/wiki/Constraint_(mathematics)>lemma constraints</a> more accurately than a neural module and outperforms the existing end-to-end approach with lower training costs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.mmtlrl-1.1.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--mmtlrl-1--1 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.mmtlrl-1.1 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.mmtlrl-1.1/>Models and Tasks for Human-Centered Machine Translation</a></strong><br><a href=/people/m/marine-carpuat/>Marine Carpuat</a><br><a href=/volumes/2021.mmtlrl-1/ class=text-muted>Proceedings of the First Workshop on Multimodal Machine Translation for Low Resource Languages (MMTLRL 2021)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--mmtlrl-1--1><div class="card-body p-3 small">In this talk, I will describe current research directions in my group that aim to make machine translation (MT) more human-centered. Instead of viewing <a href=https://en.wikipedia.org/wiki/Machine_to_machine>MT</a> solely as a task that aims to transduce a source sentence into a well-formed target language equivalent, we revisit all steps of the <a href=https://en.wikipedia.org/wiki/Machine_to_machine>MT research and development lifecycle</a> with the goal of designing <a href=https://en.wikipedia.org/wiki/Machine_to_machine>MT systems</a> that are able to help people communicate across language barriers. I will present methods to better characterize the parallel training data that powers MT systems, and how the degree of equivalence impacts translation quality. I will introduce models that enable flexible conditional language generation, and will discuss recent work on framing machine translation tasks and evaluation to center human factors.</div></div><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.182.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--182 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.182 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.182/>Dual Reconstruction : a Unifying Objective for Semi-Supervised Neural Machine Translation</a></strong><br><a href=/people/w/weijia-xu/>Weijia Xu</a>
|
<a href=/people/x/xing-niu/>Xing Niu</a>
|
<a href=/people/m/marine-carpuat/>Marine Carpuat</a><br><a href=/volumes/2020.findings-emnlp/ class=text-muted>Findings of the Association for Computational Linguistics: EMNLP 2020</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--182><div class="card-body p-3 small">While Iterative Back-Translation and Dual Learning effectively incorporate monolingual training data in <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a>, they use different objectives and heuristic gradient approximation strategies, and have not been extensively compared. We introduce a novel dual reconstruction objective that provides a unified view of Iterative Back-Translation and Dual Learning. It motivates a theoretical analysis and controlled empirical study on German-English and Turkish-English tasks, which both suggest that Iterative Back-Translation is more effective than Dual Learning despite its relative simplicity.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wmt-1.56.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wmt-1--56 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wmt-1.56 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939647 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.wmt-1.56/>The University of Maryland’s Submissions to the WMT20 Chat Translation Task : Searching for More Data to Adapt Discourse-Aware Neural Machine Translation<span class=acl-fixed-case>U</span>niversity of <span class=acl-fixed-case>M</span>aryland’s Submissions to the <span class=acl-fixed-case>WMT</span>20 Chat Translation Task: Searching for More Data to Adapt Discourse-Aware Neural Machine Translation</a></strong><br><a href=/people/c/calvin-bao/>Calvin Bao</a>
|
<a href=/people/y/yow-ting-shiue/>Yow-Ting Shiue</a>
|
<a href=/people/c/chujun-song/>Chujun Song</a>
|
<a href=/people/j/jie-li/>Jie Li</a>
|
<a href=/people/m/marine-carpuat/>Marine Carpuat</a><br><a href=/volumes/2020.wmt-1/ class=text-muted>Proceedings of the Fifth Conference on Machine Translation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wmt-1--56><div class="card-body p-3 small">This paper describes the University of Maryland&#8217;s submissions to the WMT20 Shared Task on Chat Translation. We focus on translating agent-side utterances from <a href=https://en.wikipedia.org/wiki/English_language>English</a> to <a href=https://en.wikipedia.org/wiki/German_language>German</a>. We started from an off-the-shelf BPE-based standard transformer model trained with WMT17 news and fine-tuned it with the provided in-domain training data. In addition, we augment the training set with its best matches in the WMT19 news dataset. Our primary submission uses a standard <a href=https://en.wikipedia.org/wiki/Transformers_(toy_line)>Transformer</a>, while our contrastive submissions use multi-encoder Transformers to attend to previous utterances. Our primary submission achieves 56.7 <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a> on the agent side (ende), outperforming a baseline system provided by the task organizers by more than 13 BLEU points. Moreover, according to an evaluation on a set of carefully-designed examples, the multi-encoder architecture is able to generate more coherent translations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wmt-1.141.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wmt-1--141 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wmt-1.141 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939650 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.wmt-1.141" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.wmt-1.141/>Incorporating Terminology Constraints in Automatic Post-Editing</a></strong><br><a href=/people/d/david-wan/>David Wan</a>
|
<a href=/people/c/chris-kedzie/>Chris Kedzie</a>
|
<a href=/people/f/faisal-ladhak/>Faisal Ladhak</a>
|
<a href=/people/m/marine-carpuat/>Marine Carpuat</a>
|
<a href=/people/k/kathleen-mckeown/>Kathleen McKeown</a><br><a href=/volumes/2020.wmt-1/ class=text-muted>Proceedings of the Fifth Conference on Machine Translation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wmt-1--141><div class="card-body p-3 small">Users of <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation (MT)</a> may want to ensure the use of specific <a href=https://en.wikipedia.org/wiki/Terminology>lexical terminologies</a>. While there exist techniques for incorporating terminology constraints during <a href=https://en.wikipedia.org/wiki/Inference>inference</a> for MT, current APE approaches can not ensure that they will appear in the final translation. In this paper, we present both autoregressive and non-autoregressive models for lexically constrained APE, demonstrating that our approach enables preservation of 95 % of the terminologies and also improves translation quality on English-German benchmarks. Even when applied to lexically constrained MT output, our approach is able to improve preservation of the terminologies. However, we show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> do not learn to copy constraints systematically and suggest a simple data augmentation technique that leads to improved performance and <a href=https://en.wikipedia.org/wiki/Robust_statistics>robustness</a>.</div></div><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1166.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1166 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1166 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1166.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1166" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1166/>Controlling Text Complexity in <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a></a></strong><br><a href=/people/s/sweta-agrawal/>Sweta Agrawal</a>
|
<a href=/people/m/marine-carpuat/>Marine Carpuat</a><br><a href=/volumes/D19-1/ class=text-muted>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1166><div class="card-body p-3 small">This work introduces a machine translation task where the output is aimed at audiences of different levels of target language proficiency. We collect a high quality dataset of <a href=https://en.wikipedia.org/wiki/Article_(publishing)>news articles</a> available in English and Spanish, written for diverse grade levels and propose a method to align segments across comparable bilingual articles. The resulting <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> makes it possible to train multi-task sequence to sequence models that can translate and simplify text jointly. We show that these multi-task models outperform <a href=https://en.wikipedia.org/wiki/Pipeline_(software)>pipeline approaches</a> that translate and simplify text independently.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1532.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1532 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1532 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1532.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D19-1532/>Weakly Supervised Cross-lingual Semantic Relation Classification via Knowledge Distillation</a></strong><br><a href=/people/y/yogarshi-vyas/>Yogarshi Vyas</a>
|
<a href=/people/m/marine-carpuat/>Marine Carpuat</a><br><a href=/volumes/D19-1/ class=text-muted>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1532><div class="card-body p-3 small">Words in different languages rarely cover the exact same <a href=https://en.wikipedia.org/wiki/Semantic_space>semantic space</a>. This work characterizes differences in meaning between words across languages using semantic relations that have been used to relate the meaning of English words. However, because of translation ambiguity, semantic relations are not always preserved by translation. We introduce a cross-lingual relation classifier trained only with English examples and a <a href=https://en.wikipedia.org/wiki/Bilingual_dictionary>bilingual dictionary</a>. Our <a href=https://en.wikipedia.org/wiki/Classifier_(linguistics)>classifier</a> relies on a novel attention-based distillation approach to account for translation ambiguity when transferring knowledge from <a href=https://en.wikipedia.org/wiki/English_language>English</a> to cross-lingual settings. On new English-Chinese and English-Hindi test sets, the resulting models largely outperform baselines that more naively rely on bilingual embeddings or dictionaries for cross-lingual transfer, and approach the performance of fully supervised systems on English tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1189.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1189 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1189 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-1189/>Curriculum Learning for Domain Adaptation in Neural Machine Translation</a></strong><br><a href=/people/x/xuan-zhang/>Xuan Zhang</a>
|
<a href=/people/p/pamela-shapiro/>Pamela Shapiro</a>
|
<a href=/people/g/gaurav-kumar/>Gaurav Kumar</a>
|
<a href=/people/p/paul-mcnamee/>Paul McNamee</a>
|
<a href=/people/m/marine-carpuat/>Marine Carpuat</a>
|
<a href=/people/k/kevin-duh/>Kevin Duh</a><br><a href=/volumes/N19-1/ class=text-muted>Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1189><div class="card-body p-3 small">We introduce a curriculum learning approach to adapt generic neural machine translation models to a specific domain. Samples are grouped by their similarities to the domain of interest and each group is fed to the <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training algorithm</a> with a particular schedule. This approach is simple to implement on top of any neural framework or architecture, and consistently outperforms both unadapted and adapted baselines in experiments with two distinct domains and two language pairs.</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S18-1000.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/S18-1000/>Proceedings of The 12th International Workshop on Semantic Evaluation</a></strong><br><a href=/people/m/marianna-apidianaki/>Marianna Apidianaki</a>
|
<a href=/people/s/saif-mohammad/>Saif M. Mohammad</a>
|
<a href=/people/j/jonathan-may/>Jonathan May</a>
|
<a href=/people/e/ekaterina-shutova/>Ekaterina Shutova</a>
|
<a href=/people/s/steven-bethard/>Steven Bethard</a>
|
<a href=/people/m/marine-carpuat/>Marine Carpuat</a><br><a href=/volumes/S18-1/ class=text-muted>Proceedings of The 12th International Workshop on Semantic Evaluation</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S18-1170.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S18-1170 data-toggle=collapse aria-expanded=false aria-controls=abstract-S18-1170 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/S18-1170/>UMD at SemEval-2018 Task 10 : Can Word Embeddings Capture Discriminative Attributes?<span class=acl-fixed-case>UMD</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2018 Task 10: Can Word Embeddings Capture Discriminative Attributes?</a></strong><br><a href=/people/a/alexander-zhang/>Alexander Zhang</a>
|
<a href=/people/m/marine-carpuat/>Marine Carpuat</a><br><a href=/volumes/S18-1/ class=text-muted>Proceedings of The 12th International Workshop on Semantic Evaluation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S18-1170><div class="card-body p-3 small">We describe the University of Maryland&#8217;s submission to SemEval-018 Task 10, Capturing Discriminative Attributes : given word triples (w1, w2, d), the goal is to determine whether d is a discriminating attribute belonging to w1 but not w2. Our study aims to determine whether <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> can address this challenging task. Our submission casts this <a href=https://en.wikipedia.org/wiki/Problem_solving>problem</a> as supervised binary classification using only word embedding features. Using a gaussian SVM model trained only on <a href=https://en.wikipedia.org/wiki/Data_validation>validation data</a> results in an <a href=https://en.wikipedia.org/wiki/F-score>F-score</a> of 60 %. We also show that cosine similarity features are more effective, both in <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised systems</a> (F-score of 65 %) and <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised systems</a> (F-score of 67 %).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6431.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6431 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6431 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6431/>The University of Maryland’s Chinese-English Neural Machine Translation Systems at WMT18<span class=acl-fixed-case>U</span>niversity of <span class=acl-fixed-case>M</span>aryland’s <span class=acl-fixed-case>C</span>hinese-<span class=acl-fixed-case>E</span>nglish Neural Machine Translation Systems at <span class=acl-fixed-case>WMT</span>18</a></strong><br><a href=/people/w/weijia-xu/>Weijia Xu</a>
|
<a href=/people/m/marine-carpuat/>Marine Carpuat</a><br><a href=/volumes/W18-64/ class=text-muted>Proceedings of the Third Conference on Machine Translation: Shared Task Papers</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6431><div class="card-body p-3 small">This paper describes the University of Maryland&#8217;s submission to the WMT 2018 ChineseEnglish news translation tasks. Our systems are BPE-based self-attentional Transformer networks with parallel and backtranslated monolingual training data. Using ensembling and <a href=https://en.wikipedia.org/wiki/Ranking>reranking</a>, we improve over the Transformer baseline by +1.4 <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a> for ChineseEnglish and +3.97 BLEU for EnglishChinese on newstest2017. Our best systems reach <a href=https://en.wikipedia.org/wiki/BLEU>BLEU scores</a> of 24.4 for <a href=https://en.wikipedia.org/wiki/Chinese_English>ChineseEnglish</a> and 39.0 for <a href=https://en.wikipedia.org/wiki/Chinese_English>EnglishChinese</a> on newstest2018.<i>newstest2017</i>. Our best systems reach BLEU scores of 24.4 for Chinese&#8594;English and 39.0 for English&#8594;Chinese on <i>newstest2018</i>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1136.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1136 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1136 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N18-1136.Datasets.tgz data-toggle=tooltip data-placement=top title=Dataset><i class="fas fa-file-archive"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=http://vimeo.com/277672916 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N18-1136" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/N18-1136/>Identifying Semantic Divergences in <a href=https://en.wikipedia.org/wiki/Parallel_text>Parallel Text</a> without Annotations</a></strong><br><a href=/people/y/yogarshi-vyas/>Yogarshi Vyas</a>
|
<a href=/people/x/xing-niu/>Xing Niu</a>
|
<a href=/people/m/marine-carpuat/>Marine Carpuat</a><br><a href=/volumes/N18-1/ class=text-muted>Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1136><div class="card-body p-3 small">Recognizing that even correct translations are not always semantically equivalent, we automatically detect meaning divergences in parallel sentence pairs with a deep neural model of bilingual semantic similarity which can be trained for any parallel corpus without any manual annotation. We show that our <a href=https://en.wikipedia.org/wiki/Semantic_model>semantic model</a> detects divergences more accurately than models based on surface features derived from word alignments, and that these divergences matter for neural machine translation.</div></div><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-3000.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-3000/>Proceedings of <span class=acl-fixed-case>ACL</span> 2017, Student Research Workshop</a></strong><br><a href=/people/a/allyson-ettinger/>Allyson Ettinger</a>
|
<a href=/people/s/spandana-gella/>Spandana Gella</a>
|
<a href=/people/m/matthieu-labeau/>Matthieu Labeau</a>
|
<a href=/people/c/cecilia-ovesdotter-alm/>Cecilia Ovesdotter Alm</a>
|
<a href=/people/m/marine-carpuat/>Marine Carpuat</a>
|
<a href=/people/m/mark-dredze/>Mark Dredze</a><br><a href=/volumes/P17-3/ class=text-muted>Proceedings of ACL 2017, Student Research Workshop</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-3209.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-3209 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-3209 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-3209/>Detecting Cross-Lingual Semantic Divergence for Neural Machine Translation</a></strong><br><a href=/people/m/marine-carpuat/>Marine Carpuat</a>
|
<a href=/people/y/yogarshi-vyas/>Yogarshi Vyas</a>
|
<a href=/people/x/xing-niu/>Xing Niu</a><br><a href=/volumes/W17-32/ class=text-muted>Proceedings of the First Workshop on Neural Machine Translation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-3209><div class="card-body p-3 small">Parallel corpora are often not as parallel as one might assume : non-literal translations and noisy translations abound, even in curated corpora routinely used for training and evaluation. We use a cross-lingual textual entailment system to distinguish sentence pairs that are parallel in meaning from those that are not, and show that filtering out divergent examples from training improves translation quality.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4903.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4903 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4903 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4903/>Discovering Stylistic Variations in Distributional Vector Space Models via Lexical Paraphrases</a></strong><br><a href=/people/x/xing-niu/>Xing Niu</a>
|
<a href=/people/m/marine-carpuat/>Marine Carpuat</a><br><a href=/volumes/W17-49/ class=text-muted>Proceedings of the Workshop on Stylistic Variation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4903><div class="card-body p-3 small">Detecting and analyzing stylistic variation in <a href=https://en.wikipedia.org/wiki/Language>language</a> is relevant to diverse Natural Language Processing applications. In this work, we investigate whether salient dimensions of style variations are embedded in standard distributional vector spaces of word meaning. We hypothesizes that distances between embeddings of lexical paraphrases can help isolate <a href=https://en.wikipedia.org/wiki/Style_(manner_of_address)>style</a> from meaning variations and help identify latent style dimensions. We conduct a qualitative analysis of latent style dimensions, and show the effectiveness of identified style subspaces on a lexical formality prediction task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S17-1004.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S17-1004 data-toggle=collapse aria-expanded=false aria-controls=abstract-S17-1004 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/S17-1004/>Detecting Asymmetric Semantic Relations in Context : A Case-Study on Hypernymy Detection</a></strong><br><a href=/people/y/yogarshi-vyas/>Yogarshi Vyas</a>
|
<a href=/people/m/marine-carpuat/>Marine Carpuat</a><br><a href=/volumes/S17-1/ class=text-muted>Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (*SEM 2017)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S17-1004><div class="card-body p-3 small">We introduce WHiC, a challenging testbed for detecting hypernymy, an asymmetric relation between words. While previous work has focused on detecting hypernymy between word types, we ground the meaning of words in specific contexts drawn from WordNet examples, and require predictions to be sensitive to changes in contexts. WHiC lets us analyze complementary properties of two approaches of inducing vector representations of word meaning in context. We show that such contextualized word representations also improve detection of a wider range of semantic relations in context.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S17-2000.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/S17-2000/>Proceedings of the 11th International Workshop on Semantic Evaluation (<span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2017)</a></strong><br><a href=/people/s/steven-bethard/>Steven Bethard</a>
|
<a href=/people/m/marine-carpuat/>Marine Carpuat</a>
|
<a href=/people/m/marianna-apidianaki/>Marianna Apidianaki</a>
|
<a href=/people/s/saif-mohammad/>Saif M. Mohammad</a>
|
<a href=/people/d/daniel-cer/>Daniel Cer</a>
|
<a href=/people/d/david-jurgens/>David Jurgens</a><br><a href=/volumes/S17-2/ class=text-muted>Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017)</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1299.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1299 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1299 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/238231330 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D17-1299/>A Study of Style in <a href=https://en.wikipedia.org/wiki/Machine_translation>Machine Translation</a> : Controlling the Formality of Machine Translation Output</a></strong><br><a href=/people/x/xing-niu/>Xing Niu</a>
|
<a href=/people/m/marianna-martindale/>Marianna Martindale</a>
|
<a href=/people/m/marine-carpuat/>Marine Carpuat</a><br><a href=/volumes/D17-1/ class=text-muted>Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1299><div class="card-body p-3 small">Stylistic variations of language, such as <a href=https://en.wikipedia.org/wiki/Formality>formality</a>, carry speakers&#8217; intention beyond literal meaning and should be conveyed adequately in <a href=https://en.wikipedia.org/wiki/Translation>translation</a>. We propose to use lexical formality models to control the formality level of machine translation output. We demonstrate the effectiveness of our approach in empirical evaluations, as measured by <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>automatic metrics</a> and <a href=https://en.wikipedia.org/wiki/Human_factors_and_ergonomics>human assessments</a>.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Marine+Carpuat" title="Search for 'Marine Carpuat' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/x/xing-niu/ class=align-middle>Xing Niu</a>
<span class="badge badge-secondary align-middle ml-2">5</span></li><li class=list-group-item><a href=/people/y/yogarshi-vyas/ class=align-middle>Yogarshi Vyas</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/people/w/weijia-xu/ class=align-middle>Weijia Xu</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/s/steven-bethard/ class=align-middle>Steven Bethard</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/m/marianna-apidianaki/ class=align-middle>Marianna Apidianaki</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/s/saif-mohammad/ class=align-middle>Saif Mohammad</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/t/tasnim-kabir/ class=align-middle>Tasnim Kabir</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/e/eleftheria-briakou/ class=align-middle>Eleftheria Briakou</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/allyson-ettinger/ class=align-middle>Allyson Ettinger</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/spandana-gella/ class=align-middle>Spandana Gella</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/matthieu-labeau/ class=align-middle>Matthieu Labeau</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/cecilia-ovesdotter-alm/ class=align-middle>Cecilia Ovesdotter Alm</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/mark-dredze/ class=align-middle>Mark Dredze</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/sweta-agrawal/ class=align-middle>Sweta Agrawal</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/daniel-cer/ class=align-middle>Daniel Cer</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/david-jurgens/ class=align-middle>David Jurgens</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/marianna-martindale/ class=align-middle>Marianna Martindale</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jonathan-may/ class=align-middle>Jonathan May</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/e/ekaterina-shutova/ class=align-middle>Ekaterina Shutova</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/alexander-zhang/ class=align-middle>Alexander Zhang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/x/xuan-zhang/ class=align-middle>Xuan Zhang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/p/pamela-shapiro/ class=align-middle>Pamela Shapiro</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/g/gaurav-kumar/ class=align-middle>Gaurav Kumar</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/p/paul-mcnamee/ class=align-middle>Paul McNamee</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/kevin-duh/ class=align-middle>Kevin Duh</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/calvin-bao/ class=align-middle>Calvin Bao</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yow-ting-shiue/ class=align-middle>Yow-Ting Shiue</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/chujun-song/ class=align-middle>Chujun Song</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jie-li/ class=align-middle>Jie Li</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/david-wan/ class=align-middle>David Wan</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/chris-kedzie/ class=align-middle>Chris Kedzie</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/f/faisal-ladhak/ class=align-middle>Faisal Ladhak</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/kathleen-mckeown/ class=align-middle>Kathleen McKeown</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/emnlp/ class=align-middle>EMNLP</a><span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/venues/semeval/ class=align-middle>SemEval</a><span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/venues/ws/ class=align-middle>WS</a><span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/venues/acl/ class=align-middle>ACL</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/naacl/ class=align-middle>NAACL</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-venues aria-expanded=false aria-controls=more-venues>show all...</li><div class="collapse border-top" id=more-venues><li class=list-group-item><a href=/venues/wmt/ class=align-middle>WMT</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/eval4nlp/ class=align-middle>Eval4NLP</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/findings/ class=align-middle>Findings</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/mmtlrl/ class=align-middle>MMTLRL</a><span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>