<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Mehdi Rezagholizadeh - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Mehdi</span> <span class=font-weight-bold>Rezagholizadeh</span></h2><hr><div class=row><div class=col-lg-9><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.526.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--526 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.526 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.526/>Towards Zero-Shot Knowledge Distillation for <a href=https://en.wikipedia.org/wiki/Natural_language_processing>Natural Language Processing</a></a></strong><br><a href=/people/a/ahmad-rashid/>Ahmad Rashid</a>
|
<a href=/people/v/vasileios-lioutas/>Vasileios Lioutas</a>
|
<a href=/people/a/abbas-ghaddar/>Abbas Ghaddar</a>
|
<a href=/people/m/mehdi-rezagholizadeh/>Mehdi Rezagholizadeh</a><br><a href=/volumes/2021.emnlp-main/ class=text-muted>Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--526><div class="card-body p-3 small">Knowledge distillation (KD) is a common <a href=https://en.wikipedia.org/wiki/Knowledge_transfer>knowledge transfer algorithm</a> used for <a href=https://en.wikipedia.org/wiki/Data_compression>model compression</a> across a variety of deep learning based natural language processing (NLP) solutions. In its regular manifestations, KD requires access to the teacher&#8217;s training data for <a href=https://en.wikipedia.org/wiki/Knowledge_transfer>knowledge transfer</a> to the student network. However, privacy concerns, data regulations and proprietary reasons may prevent access to such <a href=https://en.wikipedia.org/wiki/Data_(computing)>data</a>. We present, to the best of our knowledge, the first work on Zero-shot Knowledge Distillation for <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a>, where the student learns from the much larger teacher without any task specific data. Our solution combines out-of-domain data and <a href=https://en.wikipedia.org/wiki/Adversarial_system>adversarial training</a> to learn the teacher&#8217;s output distribution. We investigate six tasks from the GLUE benchmark and demonstrate that we can achieve between 75 % and 92 % of the teacher&#8217;s classification score (accuracy or F1) while compressing the model 30 times.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.603.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--603 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.603 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.603/>Universal-KD : Attention-based Output-Grounded Intermediate Layer Knowledge Distillation<span class=acl-fixed-case>KD</span>: Attention-based Output-Grounded Intermediate Layer Knowledge Distillation</a></strong><br><a href=/people/y/yimeng-wu/>Yimeng Wu</a>
|
<a href=/people/m/mehdi-rezagholizadeh/>Mehdi Rezagholizadeh</a>
|
<a href=/people/a/abbas-ghaddar/>Abbas Ghaddar</a>
|
<a href=/people/m/md-akmal-haidar/>Md Akmal Haidar</a>
|
<a href=/people/a/ali-ghodsi/>Ali Ghodsi</a><br><a href=/volumes/2021.emnlp-main/ class=text-muted>Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--603><div class="card-body p-3 small">Intermediate layer matching is shown as an effective approach for improving knowledge distillation (KD). However, this technique applies <a href=https://en.wikipedia.org/wiki/Matching_(graph_theory)>matching</a> in the hidden spaces of two different <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>networks</a> (i.e. student and teacher), which lacks clear interpretability. Moreover, intermediate layer KD can not easily deal with other problems such as layer mapping search and architecture mismatch (i.e. it requires the teacher and student to be of the same model type). To tackle the aforementioned problems all together, we propose Universal-KD to match intermediate layers of the teacher and the student in the output space (by adding pseudo classifiers on intermediate layers) via the attention-based layer projection. By doing this, our unified approach has three merits : (i) it can be flexibly combined with current intermediate layer distillation techniques to improve their results (ii) the pseudo classifiers of the teacher can be deployed instead of extra expensive teacher assistant networks to address the capacity gap problem in KD which is a common issue when the gap between the size of the teacher and student networks becomes too large ; (iii) it can be used in cross-architecture intermediate layer KD. We did comprehensive experiments in distilling BERT-base into BERT-4, RoBERTa-large into DistilRoBERTa and BERT-base into CNN and LSTM-based models. Results on the GLUE tasks show that our approach is able to outperform other KD techniques.</div></div><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.1.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--1 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.1 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.1/>Fully Quantized Transformer for <a href=https://en.wikipedia.org/wiki/Machine_translation>Machine Translation</a></a></strong><br><a href=/people/g/gabriele-prato/>Gabriele Prato</a>
|
<a href=/people/e/ella-charlaix/>Ella Charlaix</a>
|
<a href=/people/m/mehdi-rezagholizadeh/>Mehdi Rezagholizadeh</a><br><a href=/volumes/2020.findings-emnlp/ class=text-muted>Findings of the Association for Computational Linguistics: EMNLP 2020</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--1><div class="card-body p-3 small">State-of-the-art neural machine translation methods employ massive amounts of parameters. Drastically reducing <a href=https://en.wikipedia.org/wiki/Computational_cost>computational costs</a> of such <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>methods</a> without affecting performance has been up to this point unsuccessful. To this end, we propose FullyQT : an all-inclusive quantization strategy for the Transformer. To the best of our knowledge, we are the first to show that it is possible to avoid any loss in translation quality with a fully quantized Transformer. Indeed, compared to <a href=https://en.wikipedia.org/wiki/Significant_figures>full-precision</a>, our 8-bit models score greater or equal <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a> on most tasks. Comparing ourselves to all previously proposed <a href=https://en.wikipedia.org/wiki/Methodology>methods</a>, we achieve state-of-the-art <a href=https://en.wikipedia.org/wiki/Quantization_(signal_processing)>quantization</a> results.</div></div><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2307.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-2307 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-2307 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-2307/>Bilingual-GAN : A Step Towards Parallel Text Generation<span class=acl-fixed-case>GAN</span>: A Step Towards Parallel Text Generation</a></strong><br><a href=/people/a/ahmad-rashid/>Ahmad Rashid</a>
|
<a href=/people/a/alan-do-omri/>Alan Do-Omri</a>
|
<a href=/people/m/md-akmal-haidar/>Md. Akmal Haidar</a>
|
<a href=/people/q/qun-liu/>Qun Liu</a>
|
<a href=/people/m/mehdi-rezagholizadeh/>Mehdi Rezagholizadeh</a><br><a href=/volumes/W19-23/ class=text-muted>Proceedings of the Workshop on Methods for Optimizing and Evaluating Neural Language Generation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-2307><div class="card-body p-3 small">Latent space based GAN methods and attention based sequence to sequence models have achieved impressive results in text generation and unsupervised machine translation respectively. Leveraging the two domains, we propose an adversarial latent space based model capable of generating parallel sentences in two languages concurrently and translating bidirectionally. The bilingual generation goal is achieved by sampling from the latent space that is shared between both languages. First two denoising autoencoders are trained, with shared encoders and back-translation to enforce a shared latent state between the two languages. The <a href=https://en.wikipedia.org/wiki/Code>decoder</a> is shared for the two translation directions. Next, a GAN is trained to generate synthetic &#8216;code&#8217; mimicking the languages&#8217; shared latent space. This <a href=https://en.wikipedia.org/wiki/Code>code</a> is then fed into the <a href=https://en.wikipedia.org/wiki/Code_generation_(compiler)>decoder</a> to generate text in either language. We perform our experiments on Europarl and Multi30k datasets, on the English-French language pair, and document our performance using both supervised and unsupervised machine translation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1331.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1331 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1331 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/384771870 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1331" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1331/>EditNTS : An Neural Programmer-Interpreter Model for Sentence Simplification through Explicit Editing<span class=acl-fixed-case>E</span>dit<span class=acl-fixed-case>NTS</span>: An Neural Programmer-Interpreter Model for Sentence Simplification through Explicit Editing</a></strong><br><a href=/people/y/yue-dong/>Yue Dong</a>
|
<a href=/people/z/zichao-li/>Zichao Li</a>
|
<a href=/people/m/mehdi-rezagholizadeh/>Mehdi Rezagholizadeh</a>
|
<a href=/people/j/jackie-chi-kit-cheung/>Jackie Chi Kit Cheung</a><br><a href=/volumes/P19-1/ class=text-muted>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1331><div class="card-body p-3 small">We present the first sentence simplification model that learns explicit edit operations (ADD, DELETE, and KEEP) via a neural programmer-interpreter approach. Most current neural sentence simplification systems are variants of sequence-to-sequence models adopted from <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>. These methods learn to simplify sentences as a byproduct of the fact that they are trained on complex-simple sentence pairs. By contrast, our neural programmer-interpreter is directly trained to predict explicit edit operations on targeted parts of the input sentence, resembling the way that humans perform <a href=https://en.wikipedia.org/wiki/Simplification>simplification</a> and revision. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms previous state-of-the-art neural sentence simplification models (without external knowledge) by large margins on three benchmark text simplification corpora in terms of SARI (+0.95 WikiLarge, +1.89 WikiSmall, +1.41 Newsela), and is judged by humans to produce overall better and simpler output sentences.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Mehdi+Rezagholizadeh" title="Search for 'Mehdi Rezagholizadeh' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/a/ahmad-rashid/ class=align-middle>Ahmad Rashid</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/a/abbas-ghaddar/ class=align-middle>Abbas Ghaddar</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/m/md-akmal-haidar/ class=align-middle>Md. Akmal Haidar</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/v/vasileios-lioutas/ class=align-middle>Vasileios Lioutas</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yimeng-wu/ class=align-middle>Yimeng Wu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/a/ali-ghodsi/ class=align-middle>Ali Ghodsi</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/g/gabriele-prato/ class=align-middle>Gabriele Prato</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/e/ella-charlaix/ class=align-middle>Ella Charlaix</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/alan-do-omri/ class=align-middle>Alan Do-Omri</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/q/qun-liu/ class=align-middle>Qun Liu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yue-dong/ class=align-middle>Yue Dong</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/zichao-li/ class=align-middle>Zichao Li</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jackie-chi-kit-cheung/ class=align-middle>Jackie Chi Kit Cheung</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/emnlp/ class=align-middle>EMNLP</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/findings/ class=align-middle>Findings</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/ws/ class=align-middle>WS</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/acl/ class=align-middle>ACL</a><span class="badge badge-secondary align-middle ml-2">1</span></li></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>