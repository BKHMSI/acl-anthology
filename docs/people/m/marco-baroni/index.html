<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Marco Baroni - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Marco</span> <span class=font-weight-bold>Baroni</span></h2><hr><div class=row><div class=col-lg-9><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.blackboxnlp-1.37.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--blackboxnlp-1--37 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.blackboxnlp-1.37 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.blackboxnlp-1.37" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.blackboxnlp-1.37/>Controlled tasks for model analysis : Retrieving discrete information from sequences</a></strong><br><a href=/people/i/ionut-sorodoc/>Ionut-Teodor Sorodoc</a>
|
<a href=/people/g/gemma-boleda/>Gemma Boleda</a>
|
<a href=/people/m/marco-baroni/>Marco Baroni</a><br><a href=/volumes/2021.blackboxnlp-1/ class=text-muted>Proceedings of the Fourth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--blackboxnlp-1--37><div class="card-body p-3 small">In recent years, the NLP community has shown increasing interest in analysing how <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning models</a> work. Given that large models trained on complex <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> are difficult to inspect, some of this work has focused on controlled tasks that emulate specific aspects of <a href=https://en.wikipedia.org/wiki/Language>language</a>. We propose a new set of such controlled tasks to explore a crucial aspect of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a> that has not received enough attention : the need to retrieve discrete information from sequences. We also study model behavior on the tasks with simple instantiations of <a href=https://en.wikipedia.org/wiki/Transformers_(toy_line)>Transformers</a> and <a href=https://en.wikipedia.org/wiki/Light-emitting_diode>LSTMs</a>. Our results highlight the beneficial role of decoder attention and its sometimes unexpected interaction with other components. Moreover, we show that, for most of the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a>, these simple models still show significant difficulties. We hope that the community will take up the analysis possibilities that our tasks afford, and that a clearer understanding of model behavior on the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> will lead to better and more transparent models.</div></div><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-3010.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-3010 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-3010 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-3010/>EGG : a toolkit for research on Emergence of lanGuage in Games<span class=acl-fixed-case>EGG</span>: a toolkit for research on Emergence of lan<span class=acl-fixed-case>G</span>uage in Games</a></strong><br><a href=/people/e/eugene-kharitonov/>Eugene Kharitonov</a>
|
<a href=/people/r/rahma-chaabouni/>Rahma Chaabouni</a>
|
<a href=/people/d/diane-bouchacourt/>Diane Bouchacourt</a>
|
<a href=/people/m/marco-baroni/>Marco Baroni</a><br><a href=/volumes/D19-3/ class=text-muted>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP): System Demonstrations</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-3010><div class="card-body p-3 small">There is renewed interest in simulating <a href=https://en.wikipedia.org/wiki/Language_emergence>language emergence</a> among <a href=https://en.wikipedia.org/wiki/Deep_learning>deep neural agents</a> that communicate to jointly solve a task, spurred by the practical aim to develop language-enabled interactive AIs, as well as by theoretical questions about the <a href=https://en.wikipedia.org/wiki/Evolution_of_language>evolution of human language</a>. However, optimizing deep architectures connected by a <a href=https://en.wikipedia.org/wiki/Communication_channel>discrete communication channel</a> (such as that in which <a href=https://en.wikipedia.org/wiki/Language>language</a> emerges) is technically challenging. We introduce EGG, a <a href=https://en.wikipedia.org/wiki/List_of_toolkits>toolkit</a> that greatly simplifies the implementation of emergent-language communication games. EGG&#8217;s modular design provides a set of building blocks that the user can combine to create new games, easily navigating the optimization and architecture space. We hope that the tool will lower the technical barrier, and encourage researchers from various backgrounds to do original work in this exciting area.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1002.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1002 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1002 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N19-1002.Supplementary.pdf data-toggle=tooltip data-placement=top title=Supplementary><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/347368203 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1002" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1002/>The emergence of number and syntax units in LSTM language models<span class=acl-fixed-case>LSTM</span> language models</a></strong><br><a href=/people/y/yair-lakretz/>Yair Lakretz</a>
|
<a href=/people/g/german-kruszewski/>German Kruszewski</a>
|
<a href=/people/t/theo-desbordes/>Theo Desbordes</a>
|
<a href=/people/d/dieuwke-hupkes/>Dieuwke Hupkes</a>
|
<a href=/people/s/stanislas-dehaene/>Stanislas Dehaene</a>
|
<a href=/people/m/marco-baroni/>Marco Baroni</a><br><a href=/volumes/N19-1/ class=text-muted>Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1002><div class="card-body p-3 small">Recent work has shown that LSTMs trained on a generic language modeling objective capture syntax-sensitive generalizations such as long-distance number agreement. We have however no mechanistic understanding of how they accomplish this remarkable feat. Some have conjectured it depends on <a href=https://en.wikipedia.org/wiki/Heuristic>heuristics</a> that do not truly take <a href=https://en.wikipedia.org/wiki/Hierarchical_organization>hierarchical structure</a> into account. We present here a detailed study of the inner mechanics of number tracking in LSTMs at the single neuron level. We discover that long-distance number information is largely managed by two number units. Importantly, the behaviour of these <a href=https://en.wikipedia.org/wiki/Unit_of_measurement>units</a> is partially controlled by other units independently shown to track <a href=https://en.wikipedia.org/wiki/Syntax>syntactic structure</a>. We conclude that LSTMs are, to some extent, implementing genuinely syntactic processing mechanisms, paving the way to a more general understanding of grammatical encoding in LSTMs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1380.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1380 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1380 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P19-1380.Supplementary.zip data-toggle=tooltip data-placement=top title=Supplementary><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1380/>Miss Tools and Mr Fruit : Emergent Communication in Agents Learning about Object Affordances</a></strong><br><a href=/people/d/diane-bouchacourt/>Diane Bouchacourt</a>
|
<a href=/people/m/marco-baroni/>Marco Baroni</a><br><a href=/volumes/P19-1/ class=text-muted>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1380><div class="card-body p-3 small">Recent research studies communication emergence in communities of deep network agents assigned a joint task, hoping to gain insights on human language evolution. We propose here a new task capturing crucial aspects of the human environment, such as natural object affordances, and of human conversation, such as full symmetry among the participants. By conducting a thorough pragmatic and semantic analysis of the emergent protocol, we show that the <a href=https://en.wikipedia.org/wiki/Intelligent_agent>agents</a> solve the shared task through genuine bilateral, referential communication. However, the <a href=https://en.wikipedia.org/wiki/Agency_(philosophy)>agents</a> develop multiple idiolects, which makes us conclude that full symmetry is not a sufficient condition for a <a href=https://en.wikipedia.org/wiki/Natural_language>common language</a> to emerge.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1381.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1381 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1381 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-1381/>CNNs found to jump around more skillfully than RNNs : Compositional Generalization in Seq2seq Convolutional Networks<span class=acl-fixed-case>CNN</span>s found to jump around more skillfully than <span class=acl-fixed-case>RNN</span>s: Compositional Generalization in Seq2seq Convolutional Networks</a></strong><br><a href=/people/r/roberto-dessi/>Roberto Dessì</a>
|
<a href=/people/m/marco-baroni/>Marco Baroni</a><br><a href=/volumes/P19-1/ class=text-muted>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1381><div class="card-body p-3 small">Lake and Baroni (2018) introduced the SCAN dataset probing the ability of seq2seq models to capture compositional generalizations, such as inferring the meaning of jump around 0-shot from the component words. Recurrent networks (RNNs) were found to completely fail the most challenging <a href=https://en.wikipedia.org/wiki/Generalization>generalization cases</a>. We test here a convolutional network (CNN) on these tasks, reporting hugely improved performance with respect to RNNs. Despite the big improvement, the CNN has however not induced systematic rules, suggesting that the difference between compositional and non-compositional behaviour is not clear-cut.</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1119.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1119 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1119 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/306362292 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1119/>How agents see things : On visual representations in an emergent language game</a></strong><br><a href=/people/d/diane-bouchacourt/>Diane Bouchacourt</a>
|
<a href=/people/m/marco-baroni/>Marco Baroni</a><br><a href=/volumes/D18-1/ class=text-muted>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1119><div class="card-body p-3 small">There is growing interest in the <a href=https://en.wikipedia.org/wiki/Language>language</a> developed by agents interacting in emergent-communication settings. Earlier studies have focused on the agents&#8217; symbol usage, rather than on their <a href=https://en.wikipedia.org/wiki/Representation_(arts)>representation of visual input</a>. In this paper, we consider the referential games of Lazaridou et al. (2017), and investigate the representations the <a href=https://en.wikipedia.org/wiki/Intelligent_agent>agents</a> develop during their evolving interaction. We find that the <a href=https://en.wikipedia.org/wiki/Intelligent_agent>agents</a> establish successful communication by inducing visual representations that almost perfectly align with each other, but, surprisingly, do not capture the conceptual properties of the objects depicted in the input images. We conclude that, if we care about developing language-like communication systems, we must pay more attention to the visual semantics agents associate to the symbols they use.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5413.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5413 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5413 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5413/>Rearranging the Familiar : Testing Compositional Generalization in Recurrent Networks</a></strong><br><a href=/people/j/joao-loula/>João Loula</a>
|
<a href=/people/m/marco-baroni/>Marco Baroni</a>
|
<a href=/people/b/brenden-lake/>Brenden Lake</a><br><a href=/volumes/W18-54/ class=text-muted>Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5413><div class="card-body p-3 small">Systematic compositionality is the ability to recombine meaningful units with regular and predictable outcomes, and it&#8217;s seen as key to the human capacity for generalization in language. Recent work (Lake and Baroni, 2018) has studied systematic compositionality in modern seq2seq models using generalization to novel navigation instructions in a grounded environment as a probing tool. Lake and Baroni&#8217;s main experiment required the <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> to quickly bootstrap the meaning of new words. We extend this framework here to settings where the model needs only to recombine well-trained functional words (such as around and right) in novel contexts. Our findings confirm and strengthen the earlier ones : seq2seq models can be impressively good at generalizing to novel combinations of previously-seen input, but only when they receive extensive training on the specific pattern to be generalized (e.g., generalizing from many examples of X around right to jump around right), while failing when generalization requires novel application of compositional rules (e.g., inferring the meaning of around right from those of right and around).<i>around</i>&#8221; and &#8220;<i>right</i>&#8221;) in novel contexts. Our findings confirm and strengthen the earlier ones: seq2seq models can be impressively good at generalizing to novel combinations of previously-seen input, but only when they receive extensive training on the specific pattern to be generalized (e.g., generalizing from many examples of &#8220;X <i>around right</i>&#8221; to &#8220;<i>jump around right</i>&#8221;), while failing when generalization requires novel application of compositional rules (e.g., inferring the meaning of &#8220;<i>around right</i>&#8221; from those of &#8220;<i>right</i>&#8221; and &#8220;<i>around</i>&#8221;).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1108.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1108 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1108 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://vimeo.com/276898233 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N18-1108" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/N18-1108/>Colorless Green Recurrent Networks Dream Hierarchically</a></strong><br><a href=/people/k/kristina-gulordava/>Kristina Gulordava</a>
|
<a href=/people/p/piotr-bojanowski/>Piotr Bojanowski</a>
|
<a href=/people/e/edouard-grave/>Edouard Grave</a>
|
<a href=/people/t/tal-linzen/>Tal Linzen</a>
|
<a href=/people/m/marco-baroni/>Marco Baroni</a><br><a href=/volumes/N18-1/ class=text-muted>Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1108><div class="card-body p-3 small">Recurrent neural networks (RNNs) achieved impressive results in a variety of linguistic processing tasks, suggesting that they can induce non-trivial properties of language. We investigate to what extent <a href=https://en.wikipedia.org/wiki/Radio-frequency_identification>RNNs</a> learn to track abstract hierarchical syntactic structure. We test whether RNNs trained with a generic language modeling objective in four languages (Italian, English, Hebrew, Russian) can predict long-distance number agreement in various constructions. We include in our evaluation nonsensical sentences where RNNs can not rely on semantic or lexical cues (The colorless green ideas I ate with the chair sleep furiously), and, for <a href=https://en.wikipedia.org/wiki/Italian_language>Italian</a>, we compare model performance to human intuitions. Our language-model-trained RNNs make reliable predictions about <a href=https://en.wikipedia.org/wiki/Agreement_(linguistics)>long-distance agreement</a>, and do not lag much behind human performance. We thus bring support to the hypothesis that RNNs are not just shallow-pattern extractors, but they also acquire deeper grammatical competence.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1198.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1198 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1198 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1198.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1198.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285805339 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-1198/>What you can cram into a single $ & ! # * vector : Probing sentence embeddings for linguistic properties</a></strong><br><a href=/people/a/alexis-conneau/>Alexis Conneau</a>
|
<a href=/people/g/german-kruszewski/>German Kruszewski</a>
|
<a href=/people/g/guillaume-lample/>Guillaume Lample</a>
|
<a href=/people/l/loic-barrault/>Loïc Barrault</a>
|
<a href=/people/m/marco-baroni/>Marco Baroni</a><br><a href=/volumes/P18-1/ class=text-muted>Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1198><div class="card-body p-3 small">Although much effort has recently been devoted to training high-quality <a href=https://en.wikipedia.org/wiki/Sentence_embedding>sentence embeddings</a>, we still have a poor understanding of what they are capturing. Downstream tasks, often based on sentence classification, are commonly used to evaluate the quality of sentence representations. The <a href=https://en.wikipedia.org/wiki/Complexity>complexity</a> of the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> makes it however difficult to infer what kind of information is present in the <a href=https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning>representations</a>. We introduce here 10 probing tasks designed to capture simple linguistic features of sentences, and we use them to study embeddings generated by three different <a href=https://en.wikipedia.org/wiki/Encoder>encoders</a> trained in eight distinct ways, uncovering intriguing properties of both <a href=https://en.wikipedia.org/wiki/Encoder>encoders</a> and training methods.</div></div><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1030.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1030 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1030 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D17-1030.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D17-1030" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D17-1030/>High-risk learning : acquiring new word vectors from tiny data</a></strong><br><a href=/people/a/aurelie-herbelot/>Aurélie Herbelot</a>
|
<a href=/people/m/marco-baroni/>Marco Baroni</a><br><a href=/volumes/D17-1/ class=text-muted>Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1030><div class="card-body p-3 small">Distributional semantics models are known to struggle with <a href=https://en.wikipedia.org/wiki/Small_data>small data</a>. It is generally accepted that in order to learn &#8216;a good vector&#8217; for a word, a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> must have sufficient examples of its usage. This contradicts the fact that humans can guess the meaning of a word from a few occurrences only. In this paper, we show that a neural language model such as Word2Vec only necessitates minor modifications to its standard architecture to learn new terms from tiny data, using background knowledge from a previously learnt semantic space. We test our model on word definitions and on a nonce task involving 2-6 sentences&#8217; worth of context, showing a large increase in performance over state-of-the-art models on the definitional task.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Marco+Baroni" title="Search for 'Marco Baroni' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/d/diane-bouchacourt/ class=align-middle>Diane Bouchacourt</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/g/german-kruszewski/ class=align-middle>Germán Kruszewski</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/e/eugene-kharitonov/ class=align-middle>Eugene Kharitonov</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/rahma-chaabouni/ class=align-middle>Rahma Chaabouni</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/aurelie-herbelot/ class=align-middle>Aurélie Herbelot</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/i/ionut-sorodoc/ class=align-middle>Ionut Sorodoc</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/g/gemma-boleda/ class=align-middle>Gemma Boleda</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/joao-loula/ class=align-middle>João Loula</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/b/brenden-lake/ class=align-middle>Brenden Lake</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yair-lakretz/ class=align-middle>Yair Lakretz</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/theo-desbordes/ class=align-middle>Théo Desbordes</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/dieuwke-hupkes/ class=align-middle>Dieuwke Hupkes</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/stanislas-dehaene/ class=align-middle>Stanislas Dehaene</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/kristina-gulordava/ class=align-middle>Kristina Gulordava</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/p/piotr-bojanowski/ class=align-middle>Piotr Bojanowski</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/e/edouard-grave/ class=align-middle>Édouard Grave</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/tal-linzen/ class=align-middle>Tal Linzen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/alexis-conneau/ class=align-middle>Alexis Conneau</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/g/guillaume-lample/ class=align-middle>Guillaume Lample</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/loic-barrault/ class=align-middle>Loïc Barrault</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/roberto-dessi/ class=align-middle>Roberto Dessì</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/emnlp/ class=align-middle>EMNLP</a><span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/venues/acl/ class=align-middle>ACL</a><span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/venues/naacl/ class=align-middle>NAACL</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/blackboxnlp/ class=align-middle>BlackboxNLP</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/ws/ class=align-middle>WS</a><span class="badge badge-secondary align-middle ml-2">1</span></li></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>