<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Matteo Negri - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Matteo</span> <span class=font-weight-bold>Negri</span></h2><hr><div class=row><div class=col-lg-9><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.224.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--224 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.224 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.224/>Cascade versus Direct Speech Translation : Do the Differences Still Make a Difference?</a></strong><br><a href=/people/l/luisa-bentivogli/>Luisa Bentivogli</a>
|
<a href=/people/m/mauro-cettolo/>Mauro Cettolo</a>
|
<a href=/people/m/marco-gaido/>Marco Gaido</a>
|
<a href=/people/a/alina-karakanta/>Alina Karakanta</a>
|
<a href=/people/a/alberto-martinelli/>Alberto Martinelli</a>
|
<a href=/people/m/matteo-negri/>Matteo Negri</a>
|
<a href=/people/m/marco-turchi/>Marco Turchi</a><br><a href=/volumes/2021.acl-long/ class=text-muted>Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--224><div class="card-body p-3 small">Five years after the first published proofs of concept, direct approaches to speech translation (ST) are now competing with traditional cascade solutions. In light of this steady progress, can we claim that the performance gap between the two is closed? Starting from this question, we present a systematic comparison between state-of-the-art <a href=https://en.wikipedia.org/wiki/System>systems</a> representative of the two paradigms. Focusing on three language directions (English-German / Italian / Spanish), we conduct automatic and manual evaluations, exploiting high-quality professional post-edits and annotations. Our multi-faceted analysis on one of the few publicly available ST benchmarks attests for the first time that : i) the gap between the two paradigms is now closed, and ii) the subtle differences observed in their behavior are not sufficient for humans neither to distinguish them nor to prefer one over the other.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.57.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--57 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.57 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.eacl-main.57" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.57/>CTC-based Compression for Direct Speech Translation<span class=acl-fixed-case>CTC</span>-based Compression for Direct Speech Translation</a></strong><br><a href=/people/m/marco-gaido/>Marco Gaido</a>
|
<a href=/people/m/mauro-cettolo/>Mauro Cettolo</a>
|
<a href=/people/m/matteo-negri/>Matteo Negri</a>
|
<a href=/people/m/marco-turchi/>Marco Turchi</a><br><a href=/volumes/2021.eacl-main/ class=text-muted>Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--57><div class="card-body p-3 small">Previous studies demonstrated that a dynamic phone-informed compression of the input audio is beneficial for speech translation (ST). However, they required a dedicated <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> for phone recognition and did not test this solution for direct ST, in which a single <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> translates the input audio into the target language without intermediate representations. In this work, we propose the first <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>method</a> able to perform a <a href=https://en.wikipedia.org/wiki/Dynamic_compression>dynamic compression</a> of the input in direct ST models. In particular, we exploit the Connectionist Temporal Classification (CTC) to compress the input sequence according to its phonetic characteristics. Our experiments demonstrate that our <a href=https://en.wikipedia.org/wiki/Solution>solution</a> brings a 1.3-1.5 BLEU improvement over a strong <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline</a> on two language pairs (English-Italian and English-German), contextually reducing the <a href=https://en.wikipedia.org/wiki/Memory_footprint>memory footprint</a> by more than 10 %.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-tutorials.3.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-tutorials--3 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-tutorials.3 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-tutorials.3/>Tutorial Proposal : End-to-End Speech Translation</a></strong><br><a href=/people/j/jan-niehues/>Jan Niehues</a>
|
<a href=/people/e/elizabeth-salesky/>Elizabeth Salesky</a>
|
<a href=/people/m/marco-turchi/>Marco Turchi</a>
|
<a href=/people/m/matteo-negri/>Matteo Negri</a><br><a href=/volumes/2021.eacl-tutorials/ class=text-muted>Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Tutorial Abstracts</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-tutorials--3><div class="card-body p-3 small">Speech translation is the translation of speech in one language typically to text in another, traditionally accomplished through a combination of <a href=https://en.wikipedia.org/wiki/Speech_recognition>automatic speech recognition</a> and <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>. Speech translation has attracted interest for many years, but the recent successful applications of <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning</a> to both individual tasks have enabled new opportunities through joint modeling, in what we today call &#8216;end-to-end speech translation.&#8217; In this tutorial we will introduce the techniques used in cutting-edge research on <a href=https://en.wikipedia.org/wiki/Speech_translation>speech translation</a>. Starting from the traditional cascaded approach, we will given an overview on data sources and model architectures to achieve state-of-the art performance with end-to-end speech translation for both high- and low-resource languages. In addition, we will discuss methods to evaluate analyze the proposed solutions, as well as the challenges faced when applying speech translation models for real-world applications.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.127.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--127 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.127 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.127" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.127/>Speechformer : Reducing <a href=https://en.wikipedia.org/wiki/Information_loss>Information Loss</a> in Direct Speech Translation</a></strong><br><a href=/people/s/sara-papi/>Sara Papi</a>
|
<a href=/people/m/marco-gaido/>Marco Gaido</a>
|
<a href=/people/m/matteo-negri/>Matteo Negri</a>
|
<a href=/people/m/marco-turchi/>Marco Turchi</a><br><a href=/volumes/2021.emnlp-main/ class=text-muted>Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--127><div class="card-body p-3 small">Transformer-based models have gained increasing popularity achieving state-of-the-art performance in many research fields including <a href=https://en.wikipedia.org/wiki/Speech_translation>speech translation</a>. However, <a href=https://en.wikipedia.org/wiki/Transformer>Transformer&#8217;s quadratic complexity</a> with respect to the input sequence length prevents its adoption as is with audio signals, which are typically represented by long sequences. Current solutions resort to an initial sub-optimal compression based on a fixed sampling of raw audio features. Therefore, potentially useful <a href=https://en.wikipedia.org/wiki/Linguistic_description>linguistic information</a> is not accessible to higher-level layers in the <a href=https://en.wikipedia.org/wiki/Architecture>architecture</a>. To solve this issue, we propose Speechformer, an architecture that, thanks to reduced memory usage in the attention layers, avoids the initial <a href=https://en.wikipedia.org/wiki/Lossy_compression>lossy compression</a> and aggregates information only at a higher level according to more informed linguistic criteria. Experiments on three language pairs (ende / es / nl) show the efficacy of our solution, with gains of up to 0.8 BLEU on the standard MuST-C corpus and of up to 4.0 BLEU in a low resource scenario.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.iwslt-1.1.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--iwslt-1--1 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.iwslt-1.1 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.iwslt-1.1/>FINDINGS OF THE IWSLT 2021 EVALUATION CAMPAIGN<span class=acl-fixed-case>FINDINGS</span> <span class=acl-fixed-case>OF</span> <span class=acl-fixed-case>THE</span> <span class=acl-fixed-case>IWSLT</span> 2021 <span class=acl-fixed-case>EVALUATION</span> <span class=acl-fixed-case>CAMPAIGN</span></a></strong><br><a href=/people/a/antonios-anastasopoulos/>Antonios Anastasopoulos</a>
|
<a href=/people/o/ondrej-bojar/>Ondřej Bojar</a>
|
<a href=/people/j/jacob-bremerman/>Jacob Bremerman</a>
|
<a href=/people/r/roldano-cattoni/>Roldano Cattoni</a>
|
<a href=/people/m/maha-elbayad/>Maha Elbayad</a>
|
<a href=/people/m/marcello-federico/>Marcello Federico</a>
|
<a href=/people/x/xutai-ma/>Xutai Ma</a>
|
<a href=/people/s/satoshi-nakamura/>Satoshi Nakamura</a>
|
<a href=/people/m/matteo-negri/>Matteo Negri</a>
|
<a href=/people/j/jan-niehues/>Jan Niehues</a>
|
<a href=/people/j/juan-pino/>Juan Pino</a>
|
<a href=/people/e/elizabeth-salesky/>Elizabeth Salesky</a>
|
<a href=/people/s/sebastian-stuker/>Sebastian Stüker</a>
|
<a href=/people/k/katsuhito-sudoh/>Katsuhito Sudoh</a>
|
<a href=/people/m/marco-turchi/>Marco Turchi</a>
|
<a href=/people/a/alex-waibel/>Alexander Waibel</a>
|
<a href=/people/c/changhan-wang/>Changhan Wang</a>
|
<a href=/people/m/matthew-wiesner/>Matthew Wiesner</a><br><a href=/volumes/2021.iwslt-1/ class=text-muted>Proceedings of the 18th International Conference on Spoken Language Translation (IWSLT 2021)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--iwslt-1--1><div class="card-body p-3 small">The evaluation campaign of the International Conference on Spoken Language Translation (IWSLT 2021) featured this year four shared tasks : (i) Simultaneous speech translation, (ii) Offline speech translation, (iii) Multilingual speech translation, (iv) Low-resource speech translation. A total of 22 teams participated in at least one of the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a>. This paper describes each shared task, data and evaluation metrics, and reports results of the received submissions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.iwslt-1.8.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--iwslt-1--8 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.iwslt-1.8 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.iwslt-1.8/>Dealing with training and test segmentation mismatch : FBK@IWSLT2021<span class=acl-fixed-case>FBK</span>@<span class=acl-fixed-case>IWSLT</span>2021</a></strong><br><a href=/people/s/sara-papi/>Sara Papi</a>
|
<a href=/people/m/marco-gaido/>Marco Gaido</a>
|
<a href=/people/m/matteo-negri/>Matteo Negri</a>
|
<a href=/people/m/marco-turchi/>Marco Turchi</a><br><a href=/volumes/2021.iwslt-1/ class=text-muted>Proceedings of the 18th International Conference on Spoken Language Translation (IWSLT 2021)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--iwslt-1--8><div class="card-body p-3 small">This paper describes FBK&#8217;s system submission to the IWSLT 2021 Offline Speech Translation task. We participated with a direct model, which is a Transformer-based architecture trained to translate English speech audio data into German texts. The training pipeline is characterized by knowledge distillation and a two-step fine-tuning procedure. Both knowledge distillation and the first fine-tuning step are carried out on manually segmented real and synthetic data, the latter being generated with an MT system trained on the available corpora. Differently, the second fine-tuning step is carried out on a random segmentation of the MuST-C v2 En-De dataset. Its main goal is to reduce the performance drops occurring when a <a href=https://en.wikipedia.org/wiki/Speech_recognition>speech translation model</a> trained on manually segmented data (i.e. an ideal, sentence-like segmentation) is evaluated on <a href=https://en.wikipedia.org/wiki/Audio_signal_processing>automatically segmented audio</a> (i.e. actual, more realistic testing conditions). For the same purpose, a custom hybrid segmentation procedure that accounts for both audio content (pauses) and for the length of the produced segments is applied to the test data before passing them to the system. At inference time, we compared this <a href=https://en.wikipedia.org/wiki/Procedure_(term)>procedure</a> with a baseline segmentation method based on Voice Activity Detection (VAD). Our results indicate the effectiveness of the proposed hybrid approach, shown by a reduction of the gap with manual segmentation from 8.3 to 1.4 BLEU points.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.iwslt-1.26.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--iwslt-1--26 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.iwslt-1.26 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.iwslt-1.26" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.iwslt-1.26/>Between Flexibility and Consistency : Joint Generation of Captions and Subtitles</a></strong><br><a href=/people/a/alina-karakanta/>Alina Karakanta</a>
|
<a href=/people/m/marco-gaido/>Marco Gaido</a>
|
<a href=/people/m/matteo-negri/>Matteo Negri</a>
|
<a href=/people/m/marco-turchi/>Marco Turchi</a><br><a href=/volumes/2021.iwslt-1/ class=text-muted>Proceedings of the 18th International Conference on Spoken Language Translation (IWSLT 2021)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--iwslt-1--26><div class="card-body p-3 small">Speech translation (ST) has lately received growing interest for the generation of subtitles without the need for an intermediate source language transcription and timing (i.e. captions). However, the joint generation of source captions and target subtitles does not only bring potential output quality advantages when the two decoding processes inform each other, but it is also often required in multilingual scenarios. In this work, we focus on ST models which generate consistent captions-subtitles in terms of <a href=https://en.wikipedia.org/wiki/Structure>structure</a> and <a href=https://en.wikipedia.org/wiki/Content_(media)>lexical content</a>. We further introduce new <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> for evaluating subtitling consistency. Our findings show that joint decoding leads to increased performance and consistency between the generated captions and <a href=https://en.wikipedia.org/wiki/Subtitle_(titling)>subtitles</a> while still allowing for sufficient flexibility to produce <a href=https://en.wikipedia.org/wiki/Subtitle_(titling)>subtitles</a> conforming to language-specific needs and norms.</div></div><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.acl-main.619.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--acl-main--619 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.acl-main.619 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://slideslive.com/38929196 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.acl-main.619/>Gender in Danger? Evaluating Speech Translation Technology on the MuST-SHE Corpus<span class=acl-fixed-case>M</span>u<span class=acl-fixed-case>ST</span>-<span class=acl-fixed-case>SHE</span> Corpus</a></strong><br><a href=/people/l/luisa-bentivogli/>Luisa Bentivogli</a>
|
<a href=/people/b/beatrice-savoldi/>Beatrice Savoldi</a>
|
<a href=/people/m/matteo-negri/>Matteo Negri</a>
|
<a href=/people/m/mattia-a-di-gangi/>Mattia A. Di Gangi</a>
|
<a href=/people/r/roldano-cattoni/>Roldano Cattoni</a>
|
<a href=/people/m/marco-turchi/>Marco Turchi</a><br><a href=/volumes/2020.acl-main/ class=text-muted>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--acl-main--619><div class="card-body p-3 small">Translating from languages without productive <a href=https://en.wikipedia.org/wiki/Grammatical_gender>grammatical gender</a> like <a href=https://en.wikipedia.org/wiki/English_language>English</a> into gender-marked languages is a well-known difficulty for <a href=https://en.wikipedia.org/wiki/Machine_translation>machines</a>. This difficulty is also due to the fact that the training data on which <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> are built typically reflect the asymmetries of natural languages, <a href=https://en.wikipedia.org/wiki/Gender_bias>gender bias</a> included. Exclusively fed with textual data, <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> is intrinsically constrained by the fact that the input sentence does not always contain clues about the gender identity of the referred human entities. But what happens with <a href=https://en.wikipedia.org/wiki/Speech_translation>speech translation</a>, where the input is an <a href=https://en.wikipedia.org/wiki/Audio_signal>audio signal</a>? Can <a href=https://en.wikipedia.org/wiki/Sound>audio</a> provide additional information to reduce gender bias? We present the first thorough investigation of gender bias in speech translation, contributing with : i) the release of a benchmark useful for future studies, and ii) the comparison of different technologies (cascade and end-to-end) on two language directions (English-Italian / French).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.vardial-1.4.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--vardial-1--4 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.vardial-1.4 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.vardial-1.4/>Machine-oriented NMT Adaptation for Zero-shot NLP tasks : Comparing the Usefulness of Close and Distant Languages<span class=acl-fixed-case>NMT</span> Adaptation for Zero-shot <span class=acl-fixed-case>NLP</span> tasks: Comparing the Usefulness of Close and Distant Languages</a></strong><br><a href=/people/a/amirhossein-tebbifakhr/>Amirhossein Tebbifakhr</a>
|
<a href=/people/m/matteo-negri/>Matteo Negri</a>
|
<a href=/people/m/marco-turchi/>Marco Turchi</a><br><a href=/volumes/2020.vardial-1/ class=text-muted>Proceedings of the 7th Workshop on NLP for Similar Languages, Varieties and Dialects</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--vardial-1--4><div class="card-body p-3 small">Neural Machine Translation (NMT) models are typically trained by considering humans as end-users and maximizing human-oriented objectives. However, in some scenarios, their output is consumed by automatic NLP components rather than by humans. In these scenarios, translations&#8217; quality is measured in terms of their fitness for purpose (i.e. maximizing performance of external NLP tools) rather than in terms of standard human fluency / adequacy criteria. Recently, <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning techniques</a> exploiting the feedback from downstream NLP tools have been proposed for machine-oriented NMT adaptation. In this work, we tackle the problem in a multilingual setting where a single NMT model translates from multiple languages for downstream automatic processing in the target language. Knowledge sharing across close and distant languages allows to apply our machine-oriented approach in the zero-shot setting where no labeled data for the test language is seen at training time. Moreover, we incorporate multi-lingual BERT in the source side of our NMT system to benefit from the knowledge embedded in this model. Our experiments show coherent performance gains, for different language directions over both i) generic NMT models (trained for human consumption), and ii) fine-tuned multilingual BERT. This gain for zero-shot language directions (e.g. SpanishEnglish) is higher when the models are fine-tuned on a closely-related source language (Italian) than a distant one (German).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wmt-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.wmt-1.0/>Proceedings of the Fifth Conference on Machine Translation</a></strong><br><a href=/people/l/loic-barrault/>Loïc Barrault</a>
|
<a href=/people/o/ondrej-bojar/>Ondřej Bojar</a>
|
<a href=/people/f/fethi-bougares/>Fethi Bougares</a>
|
<a href=/people/r/rajen-chatterjee/>Rajen Chatterjee</a>
|
<a href=/people/m/marta-r-costa-jussa/>Marta R. Costa-jussà</a>
|
<a href=/people/c/christian-federmann/>Christian Federmann</a>
|
<a href=/people/m/mark-fishel/>Mark Fishel</a>
|
<a href=/people/a/alexander-fraser/>Alexander Fraser</a>
|
<a href=/people/y/yvette-graham/>Yvette Graham</a>
|
<a href=/people/p/paco-guzman/>Paco Guzman</a>
|
<a href=/people/b/barry-haddow/>Barry Haddow</a>
|
<a href=/people/m/matthias-huck/>Matthias Huck</a>
|
<a href=/people/a/antonio-jimeno-yepes/>Antonio Jimeno Yepes</a>
|
<a href=/people/p/philipp-koehn/>Philipp Koehn</a>
|
<a href=/people/a/andre-f-t-martins/>André Martins</a>
|
<a href=/people/m/makoto-morishita/>Makoto Morishita</a>
|
<a href=/people/c/christof-monz/>Christof Monz</a>
|
<a href=/people/m/masaaki-nagata/>Masaaki Nagata</a>
|
<a href=/people/t/toshiaki-nakazawa/>Toshiaki Nakazawa</a>
|
<a href=/people/m/matteo-negri/>Matteo Negri</a><br><a href=/volumes/2020.wmt-1/ class=text-muted>Proceedings of the Fifth Conference on Machine Translation</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wmt-1.75.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wmt-1--75 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wmt-1.75 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939672 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.wmt-1.75/>Findings of the WMT 2020 Shared Task on Automatic Post-Editing<span class=acl-fixed-case>WMT</span> 2020 Shared Task on Automatic Post-Editing</a></strong><br><a href=/people/r/rajen-chatterjee/>Rajen Chatterjee</a>
|
<a href=/people/m/markus-freitag/>Markus Freitag</a>
|
<a href=/people/m/matteo-negri/>Matteo Negri</a>
|
<a href=/people/m/marco-turchi/>Marco Turchi</a><br><a href=/volumes/2020.wmt-1/ class=text-muted>Proceedings of the Fifth Conference on Machine Translation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wmt-1--75><div class="card-body p-3 small">We present the results of the 6th round of the WMT task on MT Automatic Post-Editing. The task consists in automatically correcting the output of a black-box machine translation system by learning from existing human corrections of different sentences. This year, the challenge consisted of fixing the errors present in English Wikipedia pages translated into <a href=https://en.wikipedia.org/wiki/German_language>German</a> and <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a> by state-ofthe-art, not domain-adapted neural MT (NMT) systems unknown to participants. Six teams participated in the English-German task, submitting a total of 11 runs. Two teams participated in the English-Chinese task submitting 2 runs each. Due to i) the different source / domain of data compared to the past (Wikipedia vs Information Technology), ii) the different quality of the initial translations to be corrected and iii) the introduction of a new language pair (English-Chinese), this year&#8217;s results are not directly comparable with last year&#8217;s round. However, on both language directions, participants&#8217; submissions show considerable improvements over the baseline results. On <a href=https://en.wikipedia.org/wiki/German_language>English-German</a>, the top ranked system improves over the <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline</a> by -11.35 TER and +16.68 BLEU points, while on <a href=https://en.wikipedia.org/wiki/Chinese_language>EnglishChinese</a> the improvements are respectively up to -12.13 TER and +14.57 BLEU points. Overall, coherent gains are also highlighted by the outcomes of human evaluation, which confirms the effectiveness of APE to improve MT quality, especially in the new generic domain selected for this year&#8217;s round.</div></div><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5200.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5200/>Proceedings of the Fourth Conference on Machine Translation (Volume 1: Research Papers)</a></strong><br><a href=/people/o/ondrej-bojar/>Ondřej Bojar</a>
|
<a href=/people/r/rajen-chatterjee/>Rajen Chatterjee</a>
|
<a href=/people/c/christian-federmann/>Christian Federmann</a>
|
<a href=/people/m/mark-fishel/>Mark Fishel</a>
|
<a href=/people/y/yvette-graham/>Yvette Graham</a>
|
<a href=/people/b/barry-haddow/>Barry Haddow</a>
|
<a href=/people/m/matthias-huck/>Matthias Huck</a>
|
<a href=/people/a/antonio-jimeno-yepes/>Antonio Jimeno Yepes</a>
|
<a href=/people/p/philipp-koehn/>Philipp Koehn</a>
|
<a href=/people/a/andre-f-t-martins/>André Martins</a>
|
<a href=/people/c/christof-monz/>Christof Monz</a>
|
<a href=/people/m/matteo-negri/>Matteo Negri</a>
|
<a href=/people/a/aurelie-neveol/>Aurélie Névéol</a>
|
<a href=/people/m/mariana-neves/>Mariana Neves</a>
|
<a href=/people/m/matt-post/>Matt Post</a>
|
<a href=/people/m/marco-turchi/>Marco Turchi</a>
|
<a href=/people/k/karin-verspoor/>Karin Verspoor</a><br><a href=/volumes/W19-52/ class=text-muted>Proceedings of the Fourth Conference on Machine Translation (Volume 1: Research Papers)</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5300.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5300/>Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)</a></strong><br><a href=/people/o/ondrej-bojar/>Ondřej Bojar</a>
|
<a href=/people/r/rajen-chatterjee/>Rajen Chatterjee</a>
|
<a href=/people/c/christian-federmann/>Christian Federmann</a>
|
<a href=/people/m/mark-fishel/>Mark Fishel</a>
|
<a href=/people/y/yvette-graham/>Yvette Graham</a>
|
<a href=/people/b/barry-haddow/>Barry Haddow</a>
|
<a href=/people/m/matthias-huck/>Matthias Huck</a>
|
<a href=/people/a/antonio-jimeno-yepes/>Antonio Jimeno Yepes</a>
|
<a href=/people/p/philipp-koehn/>Philipp Koehn</a>
|
<a href=/people/a/andre-f-t-martins/>André Martins</a>
|
<a href=/people/c/christof-monz/>Christof Monz</a>
|
<a href=/people/m/matteo-negri/>Matteo Negri</a>
|
<a href=/people/a/aurelie-neveol/>Aurélie Névéol</a>
|
<a href=/people/m/mariana-neves/>Mariana Neves</a>
|
<a href=/people/m/matt-post/>Matt Post</a>
|
<a href=/people/m/marco-turchi/>Marco Turchi</a>
|
<a href=/people/k/karin-verspoor/>Karin Verspoor</a><br><a href=/volumes/W19-53/ class=text-muted>Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5400.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5400/>Proceedings of the Fourth Conference on Machine Translation (Volume 3: Shared Task Papers, Day 2)</a></strong><br><a href=/people/o/ondrej-bojar/>Ondřej Bojar</a>
|
<a href=/people/r/rajen-chatterjee/>Rajen Chatterjee</a>
|
<a href=/people/c/christian-federmann/>Christian Federmann</a>
|
<a href=/people/m/mark-fishel/>Mark Fishel</a>
|
<a href=/people/y/yvette-graham/>Yvette Graham</a>
|
<a href=/people/b/barry-haddow/>Barry Haddow</a>
|
<a href=/people/m/matthias-huck/>Matthias Huck</a>
|
<a href=/people/a/antonio-jimeno-yepes/>Antonio Jimeno Yepes</a>
|
<a href=/people/p/philipp-koehn/>Philipp Koehn</a>
|
<a href=/people/a/andre-f-t-martins/>André Martins</a>
|
<a href=/people/c/christof-monz/>Christof Monz</a>
|
<a href=/people/m/matteo-negri/>Matteo Negri</a>
|
<a href=/people/a/aurelie-neveol/>Aurélie Névéol</a>
|
<a href=/people/m/mariana-neves/>Mariana Neves</a>
|
<a href=/people/m/matt-post/>Matt Post</a>
|
<a href=/people/m/marco-turchi/>Marco Turchi</a>
|
<a href=/people/k/karin-verspoor/>Karin Verspoor</a><br><a href=/volumes/W19-54/ class=text-muted>Proceedings of the Fourth Conference on Machine Translation (Volume 3: Shared Task Papers, Day 2)</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5416.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5416 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5416 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5416/>Effort-Aware Neural Automatic Post-Editing</a></strong><br><a href=/people/a/amirhossein-tebbifakhr/>Amirhossein Tebbifakhr</a>
|
<a href=/people/m/matteo-negri/>Matteo Negri</a>
|
<a href=/people/m/marco-turchi/>Marco Turchi</a><br><a href=/volumes/W19-54/ class=text-muted>Proceedings of the Fourth Conference on Machine Translation (Volume 3: Shared Task Papers, Day 2)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5416><div class="card-body p-3 small">For this round of the WMT 2019 APE shared task, our submission focuses on addressing the over-correction problem in <a href=https://en.wikipedia.org/wiki/Application_programming_interface>APE</a>. Over-correction occurs when the APE system tends to rephrase an already correct MT output, and the resulting sentence is penalized by a reference-based evaluation against human post-edits. Our intuition is that this problem can be prevented by informing the <a href=https://en.wikipedia.org/wiki/System>system</a> about the predicted quality of the MT output or, in other terms, the expected amount of needed corrections. For this purpose, following the common approach in multilingual NMT, we prepend a special token to the beginning of both the source text and the MT output indicating the required amount of <a href=https://en.wikipedia.org/wiki/Post-editing>post-editing</a>. Following the best submissions to the WMT 2018 APE shared task, our backbone architecture is based on multi-source Transformer to encode both the MT output and the corresponding source text. We participated both in the English-German and English-Russian subtasks. In the first subtask, our best submission improved the original MT output quality up to +0.98 BLEU and -0.47 TER. In the second subtask, where the higher quality of the MT output increases the risk of over-correction, none of our submitted runs was able to improve the MT output.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2019.iwslt-1.1.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2019--iwslt-1--1 data-toggle=collapse aria-expanded=false aria-controls=abstract-2019.iwslt-1.1 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2019.iwslt-1.1/>The IWSLT 2019 Evaluation Campaign<span class=acl-fixed-case>IWSLT</span> 2019 Evaluation Campaign</a></strong><br><a href=/people/j/jan-niehues/>Jan Niehues</a>
|
<a href=/people/r/rolando-cattoni/>Rolando Cattoni</a>
|
<a href=/people/s/sebastian-stuker/>Sebastian Stüker</a>
|
<a href=/people/m/matteo-negri/>Matteo Negri</a>
|
<a href=/people/m/marco-turchi/>Marco Turchi</a>
|
<a href=/people/t/thanh-le-ha/>Thanh-Le Ha</a>
|
<a href=/people/e/elizabeth-salesky/>Elizabeth Salesky</a>
|
<a href=/people/r/ramon-sanabria/>Ramon Sanabria</a>
|
<a href=/people/l/loic-barrault/>Loic Barrault</a>
|
<a href=/people/l/lucia-specia/>Lucia Specia</a>
|
<a href=/people/m/marcello-federico/>Marcello Federico</a><br><a href=/volumes/2019.iwslt-1/ class=text-muted>Proceedings of the 16th International Conference on Spoken Language Translation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2019--iwslt-1--1><div class="card-body p-3 small">The IWSLT 2019 evaluation campaign featured three tasks : speech translation of (i) <a href=https://en.wikipedia.org/wiki/TED_(conference)>TED talks</a> and (ii) How2 instructional videos from <a href=https://en.wikipedia.org/wiki/English_language>English</a> into <a href=https://en.wikipedia.org/wiki/German_language>German</a> and <a href=https://en.wikipedia.org/wiki/Portuguese_language>Portuguese</a>, and (iii) text translation of <a href=https://en.wikipedia.org/wiki/TED_(conference)>TED talks</a> from <a href=https://en.wikipedia.org/wiki/English_language>English</a> into <a href=https://en.wikipedia.org/wiki/Czech_language>Czech</a>. For the first two tasks we encouraged submissions of end- to-end speech-to-text systems, and for the second task participants could also use the video as additional input. We received submissions by 12 research teams. This overview provides detailed descriptions of the data and evaluation conditions of each task and reports results of the participating systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2019.iwslt-1.16.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2019--iwslt-1--16 data-toggle=collapse aria-expanded=false aria-controls=abstract-2019.iwslt-1.16 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2019.iwslt-1.16" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2019.iwslt-1.16/>Adapting Multilingual Neural Machine Translation to Unseen Languages</a></strong><br><a href=/people/s/surafel-m-lakew/>Surafel M. Lakew</a>
|
<a href=/people/a/alina-karakanta/>Alina Karakanta</a>
|
<a href=/people/m/marcello-federico/>Marcello Federico</a>
|
<a href=/people/m/matteo-negri/>Matteo Negri</a>
|
<a href=/people/m/marco-turchi/>Marco Turchi</a><br><a href=/volumes/2019.iwslt-1/ class=text-muted>Proceedings of the 16th International Conference on Spoken Language Translation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2019--iwslt-1--16><div class="card-body p-3 small">Multilingual Neural Machine Translation (MNMT) for low- resource languages (LRL) can be enhanced by the presence of related high-resource languages (HRL), but the relatedness of HRL usually relies on predefined linguistic assumptions about language similarity. Recently, adapting MNMT to a <a href=https://en.wikipedia.org/wiki/Linear_regression>LRL</a> has shown to greatly improve performance. In this work, we explore the problem of adapting an MNMT model to an unseen <a href=https://en.wikipedia.org/wiki/Linear_regression>LRL</a> using data selection and model adapta- tion. In order to improve NMT for <a href=https://en.wikipedia.org/wiki/Linguistic_description>LRL</a>, we employ perplexity to select HRL data that are most similar to the <a href=https://en.wikipedia.org/wiki/Linguistic_description>LRL</a> on the basis of <a href=https://en.wikipedia.org/wiki/Language_distance>language distance</a>. We extensively explore data selection in popular multilingual NMT settings, namely in (zero-shot) translation, and in adaptation from a multilingual pre-trained model, for both directions (LRLen). We further show that dynamic adaptation of the model&#8217;s vocabulary results in a more favourable segmentation for the LRL in comparison with direct adaptation. Experiments show re- ductions in training time and significant performance gains over LRL baselines, even with zero LRL data (+13.0 BLEU), up to +17.0 BLEU for pre-trained multilingual model dynamic adaptation with related data selection. Our method outperforms current approaches, such as massively multilingual models and <a href=https://en.wikipedia.org/wiki/Data_augmentation>data augmentation</a>, on four <a href=https://en.wikipedia.org/wiki/Linear_regression>LRL</a>.</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6300.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6300/>Proceedings of the Third Conference on Machine Translation: Research Papers</a></strong><br><a href=/people/o/ondrej-bojar/>Ondřej Bojar</a>
|
<a href=/people/r/rajen-chatterjee/>Rajen Chatterjee</a>
|
<a href=/people/c/christian-federmann/>Christian Federmann</a>
|
<a href=/people/m/mark-fishel/>Mark Fishel</a>
|
<a href=/people/y/yvette-graham/>Yvette Graham</a>
|
<a href=/people/b/barry-haddow/>Barry Haddow</a>
|
<a href=/people/m/matthias-huck/>Matthias Huck</a>
|
<a href=/people/a/antonio-jimeno-yepes/>Antonio Jimeno Yepes</a>
|
<a href=/people/p/philipp-koehn/>Philipp Koehn</a>
|
<a href=/people/c/christof-monz/>Christof Monz</a>
|
<a href=/people/m/matteo-negri/>Matteo Negri</a>
|
<a href=/people/a/aurelie-neveol/>Aurélie Névéol</a>
|
<a href=/people/m/mariana-neves/>Mariana Neves</a>
|
<a href=/people/m/matt-post/>Matt Post</a>
|
<a href=/people/l/lucia-specia/>Lucia Specia</a>
|
<a href=/people/m/marco-turchi/>Marco Turchi</a>
|
<a href=/people/k/karin-verspoor/>Karin Verspoor</a><br><a href=/volumes/W18-63/ class=text-muted>Proceedings of the Third Conference on Machine Translation: Research Papers</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6400/>Proceedings of the Third Conference on Machine Translation: Shared Task Papers</a></strong><br><a href=/people/o/ondrej-bojar/>Ondřej Bojar</a>
|
<a href=/people/r/rajen-chatterjee/>Rajen Chatterjee</a>
|
<a href=/people/c/christian-federmann/>Christian Federmann</a>
|
<a href=/people/m/mark-fishel/>Mark Fishel</a>
|
<a href=/people/y/yvette-graham/>Yvette Graham</a>
|
<a href=/people/b/barry-haddow/>Barry Haddow</a>
|
<a href=/people/m/matthias-huck/>Matthias Huck</a>
|
<a href=/people/a/antonio-jimeno-yepes/>Antonio Jimeno Yepes</a>
|
<a href=/people/p/philipp-koehn/>Philipp Koehn</a>
|
<a href=/people/c/christof-monz/>Christof Monz</a>
|
<a href=/people/m/matteo-negri/>Matteo Negri</a>
|
<a href=/people/a/aurelie-neveol/>Aurélie Névéol</a>
|
<a href=/people/m/mariana-neves/>Mariana Neves</a>
|
<a href=/people/m/matt-post/>Matt Post</a>
|
<a href=/people/l/lucia-specia/>Lucia Specia</a>
|
<a href=/people/m/marco-turchi/>Marco Turchi</a>
|
<a href=/people/k/karin-verspoor/>Karin Verspoor</a><br><a href=/volumes/W18-64/ class=text-muted>Proceedings of the Third Conference on Machine Translation: Shared Task Papers</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6530.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6530 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6530 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6530/>Generating E-Commerce Product Titles and Predicting their Quality<span class=acl-fixed-case>E</span>-Commerce Product Titles and Predicting their Quality</a></strong><br><a href=/people/j/jose-g-c-de-souza/>José G. Camargo de Souza</a>
|
<a href=/people/m/michael-kozielski/>Michael Kozielski</a>
|
<a href=/people/p/prashant-mathur/>Prashant Mathur</a>
|
<a href=/people/e/ernie-chang/>Ernie Chang</a>
|
<a href=/people/m/marco-guerini/>Marco Guerini</a>
|
<a href=/people/m/matteo-negri/>Matteo Negri</a>
|
<a href=/people/m/marco-turchi/>Marco Turchi</a>
|
<a href=/people/e/evgeny-matusov/>Evgeny Matusov</a><br><a href=/volumes/W18-65/ class=text-muted>Proceedings of the 11th International Conference on Natural Language Generation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6530><div class="card-body p-3 small">E-commerce platforms present products using titles that summarize product information. These titles can not be created by hand, therefore an algorithmic solution is required. The task of automatically generating these <a href=https://en.wikipedia.org/wiki/Title_(publishing)>titles</a> given noisy user provided titles is one way to achieve the goal. The setting requires the generation process to be fast and the generated title to be both human-readable and concise. Furthermore, we need to understand if such generated titles are usable. As such, we propose approaches that (i) automatically generate product titles, (ii) predict their quality. Our approach scales to millions of products and both automatic and human evaluations performed on real-world data indicate our approaches are effective and applicable to existing e-commerce scenarios.</div></div><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2017.iwslt-1.5.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2017--iwslt-1--5 data-toggle=collapse aria-expanded=false aria-controls=abstract-2017.iwslt-1.5 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2017.iwslt-1.5/>FBK’s Multilingual Neural Machine Translation System for IWSLT 2017<span class=acl-fixed-case>FBK</span>’s Multilingual Neural Machine Translation System for <span class=acl-fixed-case>IWSLT</span> 2017</a></strong><br><a href=/people/s/surafel-m-lakew/>Surafel M. Lakew</a>
|
<a href=/people/q/quintino-f-lotito/>Quintino F. Lotito</a>
|
<a href=/people/m/marco-turchi/>Marco Turchi</a>
|
<a href=/people/m/matteo-negri/>Matteo Negri</a>
|
<a href=/people/m/marcello-federico/>Marcello Federico</a><br><a href=/volumes/2017.iwslt-1/ class=text-muted>Proceedings of the 14th International Conference on Spoken Language Translation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2017--iwslt-1--5><div class="card-body p-3 small">Neural Machine Translation has been shown to enable <a href=https://en.wikipedia.org/wiki/Inference>inference</a> and cross-lingual knowledge transfer across multiple language directions using a single multilingual model. Focusing on this multilingual translation scenario, this work summarizes FBK&#8217;s participation in the IWSLT 2017 shared task. Our submissions rely on two multilingual systems trained on five languages (English, <a href=https://en.wikipedia.org/wiki/Dutch_language>Dutch</a>, <a href=https://en.wikipedia.org/wiki/German_language>German</a>, <a href=https://en.wikipedia.org/wiki/Italian_language>Italian</a>, and Romanian). The first one is a 20 language direction model, which handles all possible combinations of the five languages. The second multilingual system is trained only on 16 directions, leaving the others as zero-shot translation directions (i.e representing a more complex inference task on language pairs not seen at training time). More specifically, our zero-shot directions are <a href=https://en.wikipedia.org/wiki/German_language>Dutch$German</a> and <a href=https://en.wikipedia.org/wiki/Romanian_language>Italian$Romanian</a> (resulting in four language combinations). Despite the small amount of parallel data used for training these systems, the resulting multilingual models are effective, even in comparison with <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> trained separately for every language pair (i.e. in more favorable conditions). We compare and show the results of the two multilingual models against a baseline single language pair systems. Particularly, we focus on the four zero-shot directions and show how a multilingual model trained with small data can provide reasonable results. Furthermore, we investigate how pivoting (i.e using a bridge / pivot language for <a href=https://en.wikipedia.org/wiki/Inference>inference</a> in a source!pivot!target translations) using a multilingual model can be an alternative to enable zero-shot translation in a low resource setting.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2017.iwslt-1.16.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2017--iwslt-1--16 data-toggle=collapse aria-expanded=false aria-controls=abstract-2017.iwslt-1.16 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2017.iwslt-1.16/>Improving Zero-Shot Translation of Low-Resource Languages</a></strong><br><a href=/people/s/surafel-m-lakew/>Surafel M. Lakew</a>
|
<a href=/people/q/quintino-f-lotito/>Quintino F. Lotito</a>
|
<a href=/people/m/matteo-negri/>Matteo Negri</a>
|
<a href=/people/m/marco-turchi/>Marco Turchi</a>
|
<a href=/people/m/marcello-federico/>Marcello Federico</a><br><a href=/volumes/2017.iwslt-1/ class=text-muted>Proceedings of the 14th International Conference on Spoken Language Translation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2017--iwslt-1--16><div class="card-body p-3 small">Recent work on multilingual neural machine translation reported competitive performance with respect to bilingual models and surprisingly good performance even on (zero-shot) translation directions not observed at training time. We investigate here a zero-shot translation in a particularly low-resource multilingual setting. We propose a simple iterative training procedure that leverages a duality of translations directly generated by the <a href=https://en.wikipedia.org/wiki/System>system</a> for the zero-shot directions. The translations produced by the system (sub-optimal since they contain mixed language from the shared vocabulary), are then used together with the original parallel data to feed and iteratively re-train the multilingual network. Over time, this allows the <a href=https://en.wikipedia.org/wiki/System>system</a> to learn from its own generated and increasingly better output. Our approach shows to be effective in improving the two zero-shot directions of our multilingual model. In particular, we observed gains of about 9 BLEU points over a baseline multilingual model and up to 2.08 BLEU over a pivoting mechanism using two bilingual models. Further analysis shows that there is also a slight improvement in the non-zero-shot language directions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1050.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1050 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1050 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1050/>Online Automatic Post-editing for MT in a Multi-Domain Translation Environment<span class=acl-fixed-case>MT</span> in a Multi-Domain Translation Environment</a></strong><br><a href=/people/r/rajen-chatterjee/>Rajen Chatterjee</a>
|
<a href=/people/g/gebremedhen-gebremelak/>Gebremedhen Gebremelak</a>
|
<a href=/people/m/matteo-negri/>Matteo Negri</a>
|
<a href=/people/m/marco-turchi/>Marco Turchi</a><br><a href=/volumes/E17-1/ class=text-muted>Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1050><div class="card-body p-3 small">Automatic post-editing (APE) for <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation (MT)</a> aims to fix recurrent errors made by the <a href=https://en.wikipedia.org/wiki/Machine_translation>MT decoder</a> by learning from correction examples. In controlled evaluation scenarios, the representativeness of the training set with respect to the test data is a key factor to achieve good performance. Real-life scenarios, however, do not guarantee such favorable learning conditions. Ideally, to be integrated in a real professional translation workflow (e.g. to play a role in computer-assisted translation framework), APE tools should be flexible enough to cope with continuous streams of diverse data coming from different domains / genres. To cope with this problem, we propose an online APE framework that is : i) robust to data diversity (i.e. capable to learn and apply correction rules in the right contexts) and ii) able to evolve over time (by continuously extending and refining its knowledge). In a comparative evaluation, with English-German test data coming in random order from two different domains, we show the effectiveness of our approach, which outperforms a strong <a href=https://en.wikipedia.org/wiki/Batch_processing>batch system</a> and the <a href=https://en.wikipedia.org/wiki/State_of_the_art>state of the art</a> in online APE.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Matteo+Negri" title="Search for 'Matteo Negri' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/m/marco-turchi/ class=align-middle>Marco Turchi</a>
<span class="badge badge-secondary align-middle ml-2">22</span></li><li class=list-group-item><a href=/people/r/rajen-chatterjee/ class=align-middle>Rajen Chatterjee</a>
<span class="badge badge-secondary align-middle ml-2">8</span></li><li class=list-group-item><a href=/people/o/ondrej-bojar/ class=align-middle>Ondřej Bojar</a>
<span class="badge badge-secondary align-middle ml-2">7</span></li><li class=list-group-item><a href=/people/c/christian-federmann/ class=align-middle>Christian Federmann</a>
<span class="badge badge-secondary align-middle ml-2">6</span></li><li class=list-group-item><a href=/people/m/mark-fishel/ class=align-middle>Mark Fishel</a>
<span class="badge badge-secondary align-middle ml-2">6</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/y/yvette-graham/ class=align-middle>Yvette Graham</a>
<span class="badge badge-secondary align-middle ml-2">6</span></li><li class=list-group-item><a href=/people/b/barry-haddow/ class=align-middle>Barry Haddow</a>
<span class="badge badge-secondary align-middle ml-2">6</span></li><li class=list-group-item><a href=/people/m/matthias-huck/ class=align-middle>Matthias Huck</a>
<span class="badge badge-secondary align-middle ml-2">6</span></li><li class=list-group-item><a href=/people/a/antonio-jimeno-yepes/ class=align-middle>Antonio Jimeno Yepes</a>
<span class="badge badge-secondary align-middle ml-2">6</span></li><li class=list-group-item><a href=/people/p/philipp-koehn/ class=align-middle>Philipp Koehn</a>
<span class="badge badge-secondary align-middle ml-2">6</span></li><li class=list-group-item><a href=/people/c/christof-monz/ class=align-middle>Christof Monz</a>
<span class="badge badge-secondary align-middle ml-2">6</span></li><li class=list-group-item><a href=/people/m/marco-gaido/ class=align-middle>Marco Gaido</a>
<span class="badge badge-secondary align-middle ml-2">5</span></li><li class=list-group-item><a href=/people/m/marcello-federico/ class=align-middle>Marcello Federico</a>
<span class="badge badge-secondary align-middle ml-2">5</span></li><li class=list-group-item><a href=/people/a/aurelie-neveol/ class=align-middle>Aurelie Neveol</a>
<span class="badge badge-secondary align-middle ml-2">5</span></li><li class=list-group-item><a href=/people/m/mariana-neves/ class=align-middle>Mariana Neves</a>
<span class="badge badge-secondary align-middle ml-2">5</span></li><li class=list-group-item><a href=/people/m/matt-post/ class=align-middle>Matt Post</a>
<span class="badge badge-secondary align-middle ml-2">5</span></li><li class=list-group-item><a href=/people/k/karin-verspoor/ class=align-middle>Karin Verspoor</a>
<span class="badge badge-secondary align-middle ml-2">5</span></li><li class=list-group-item><a href=/people/a/andre-f-t-martins/ class=align-middle>André F. T. Martins</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/people/a/alina-karakanta/ class=align-middle>Alina Karakanta</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/j/jan-niehues/ class=align-middle>Jan Niehues</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/e/elizabeth-salesky/ class=align-middle>Elizabeth Salesky</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/s/surafel-m-lakew/ class=align-middle>Surafel M. Lakew</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/l/lucia-specia/ class=align-middle>Lucia Specia</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/l/luisa-bentivogli/ class=align-middle>Luisa Bentivogli</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/m/mauro-cettolo/ class=align-middle>Mauro Cettolo</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/r/roldano-cattoni/ class=align-middle>Roldano Cattoni</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/q/quintino-f-lotito/ class=align-middle>Quintino F. Lotito</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/s/sara-papi/ class=align-middle>Sara Papi</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/s/sebastian-stuker/ class=align-middle>Sebastian Stüker</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/a/amirhossein-tebbifakhr/ class=align-middle>Amirhossein Tebbifakhr</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/l/loic-barrault/ class=align-middle>Loïc Barrault</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/a/alberto-martinelli/ class=align-middle>Alberto Martinelli</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/b/beatrice-savoldi/ class=align-middle>Beatrice Savoldi</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/mattia-a-di-gangi/ class=align-middle>Mattia A. Di Gangi</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/antonios-anastasopoulos/ class=align-middle>Antonios Anastasopoulos</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jacob-bremerman/ class=align-middle>Jacob Bremerman</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/maha-elbayad/ class=align-middle>Maha Elbayad</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/x/xutai-ma/ class=align-middle>Xutai Ma</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/satoshi-nakamura/ class=align-middle>Satoshi Nakamura</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/juan-pino/ class=align-middle>Juan Pino</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/katsuhito-sudoh/ class=align-middle>Katsuhito Sudoh</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/alex-waibel/ class=align-middle>Alex Waibel</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/changhan-wang/ class=align-middle>Changhan Wang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/matthew-wiesner/ class=align-middle>Matthew Wiesner</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jose-g-c-de-souza/ class=align-middle>José G. C. de Souza</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/michael-kozielski/ class=align-middle>Michael Kozielski</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/p/prashant-mathur/ class=align-middle>Prashant Mathur</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/e/ernie-chang/ class=align-middle>Ernie Chang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/marco-guerini/ class=align-middle>Marco Guerini</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/e/evgeny-matusov/ class=align-middle>Evgeny Matusov</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/f/fethi-bougares/ class=align-middle>Fethi Bougares</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/marta-r-costa-jussa/ class=align-middle>Marta R. Costa-jussà</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/alexander-fraser/ class=align-middle>Alexander Fraser</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/p/paco-guzman/ class=align-middle>Paco Guzman</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/makoto-morishita/ class=align-middle>Makoto Morishita</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/masaaki-nagata/ class=align-middle>Masaaki Nagata</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/toshiaki-nakazawa/ class=align-middle>Toshiaki Nakazawa</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/markus-freitag/ class=align-middle>Markus Freitag</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/rolando-cattoni/ class=align-middle>Rolando Cattoni</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/thanh-le-ha/ class=align-middle>Thanh-Le Ha</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/ramon-sanabria/ class=align-middle>Ramon Sanabria</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/g/gebremedhen-gebremelak/ class=align-middle>Gebremedhen Gebremelak</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/iwslt/ class=align-middle>IWSLT</a><span class="badge badge-secondary align-middle ml-2">7</span></li><li class=list-group-item><a href=/venues/ws/ class=align-middle>WS</a><span class="badge badge-secondary align-middle ml-2">7</span></li><li class=list-group-item><a href=/venues/eacl/ class=align-middle>EACL</a><span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/venues/acl/ class=align-middle>ACL</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/wmt/ class=align-middle>WMT</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-venues aria-expanded=false aria-controls=more-venues>show all...</li><div class="collapse border-top" id=more-venues><li class=list-group-item><a href=/venues/emnlp/ class=align-middle>EMNLP</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/vardial/ class=align-middle>VarDial</a><span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>