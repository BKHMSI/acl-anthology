<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Mona Diab - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Mona</span> <span class=font-weight-bold>Diab</span></h2><hr><div class=row><div class=col-lg-9><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.66.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--66 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.66 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.66" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.66/>Adapting High-resource NMT Models to Translate Low-resource Related Languages without Parallel Data<span class=acl-fixed-case>NMT</span> Models to Translate Low-resource Related Languages without Parallel Data</a></strong><br><a href=/people/w/wei-jen-ko/>Wei-Jen Ko</a>
|
<a href=/people/a/ahmed-el-kishky/>Ahmed El-Kishky</a>
|
<a href=/people/a/adithya-renduchintala/>Adithya Renduchintala</a>
|
<a href=/people/v/vishrav-chaudhary/>Vishrav Chaudhary</a>
|
<a href=/people/n/naman-goyal/>Naman Goyal</a>
|
<a href=/people/f/francisco-guzman/>Francisco Guzm√°n</a>
|
<a href=/people/p/pascale-fung/>Pascale Fung</a>
|
<a href=/people/p/philipp-koehn/>Philipp Koehn</a>
|
<a href=/people/m/mona-diab/>Mona Diab</a><br><a href=/volumes/2021.acl-long/ class=text-muted>Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--66><div class="card-body p-3 small">The scarcity of parallel data is a major obstacle for training high-quality <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation systems</a> for low-resource languages. Fortunately, some low-resource languages are linguistically related or similar to high-resource languages ; these related languages may share many lexical or syntactic structures. In this work, we exploit this linguistic overlap to facilitate translating to and from a low-resource language with only monolingual data, in addition to any parallel data in the related high-resource language. Our method, NMT-Adapt, combines denoising autoencoding, back-translation and adversarial objectives to utilize monolingual data for low-resource adaptation. We experiment on 7 languages from three different language families and show that our technique significantly improves <a href=https://en.wikipedia.org/wiki/Translation>translation</a> into low-resource language compared to other <a href=https://en.wikipedia.org/wiki/Translation>translation baselines</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.calcs-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.calcs-1.0/>Proceedings of the Fifth Workshop on Computational Approaches to Linguistic Code-Switching</a></strong><br><a href=/people/t/thamar-solorio/>Thamar Solorio</a>
|
<a href=/people/s/shuguang-chen/>Shuguang Chen</a>
|
<a href=/people/a/alan-w-black/>Alan W. Black</a>
|
<a href=/people/m/mona-diab/>Mona Diab</a>
|
<a href=/people/s/sunayana-sitaram/>Sunayana Sitaram</a>
|
<a href=/people/v/victor-soto/>Victor Soto</a>
|
<a href=/people/e/emre-yilmaz/>Emre Yilmaz</a>
|
<a href=/people/a/anirudh-srinivasan/>Anirudh Srinivasan</a><br><a href=/volumes/2021.calcs-1/ class=text-muted>Proceedings of the Fifth Workshop on Computational Approaches to Linguistic Code-Switching</a></span></p><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.acl-main.454.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--acl-main--454 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.acl-main.454 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://slideslive.com/38929353 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.acl-main.454" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.acl-main.454/>FEQA : A Question Answering Evaluation Framework for Faithfulness Assessment in Abstractive Summarization<span class=acl-fixed-case>FEQA</span>: A Question Answering Evaluation Framework for Faithfulness Assessment in Abstractive Summarization</a></strong><br><a href=/people/e/esin-durmus/>Esin Durmus</a>
|
<a href=/people/h/he-he/>He He</a>
|
<a href=/people/m/mona-diab/>Mona Diab</a><br><a href=/volumes/2020.acl-main/ class=text-muted>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--acl-main--454><div class="card-body p-3 small">Neural abstractive summarization models are prone to generate content inconsistent with the source document, i.e. unfaithful. Existing <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>automatic metrics</a> do not capture such mistakes effectively. We tackle the problem of evaluating faithfulness of a generated summary given its source document. We first collected human annotations of faithfulness for outputs from numerous <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> on two datasets. We find that current models exhibit a trade-off between <a href=https://en.wikipedia.org/wiki/Abstraction>abstractiveness</a> and <a href=https://en.wikipedia.org/wiki/Faithfulness>faithfulness</a> : outputs with less word overlap with the source document are more likely to be unfaithful. Next, we propose an automatic question answering (QA) based metric for <a href=https://en.wikipedia.org/wiki/Faithfulness>faithfulness</a>, FEQA, which leverages recent advances in <a href=https://en.wikipedia.org/wiki/Reading_comprehension>reading comprehension</a>. Given question-answer pairs generated from the summary, a QA model extracts answers from the document ; non-matched answers indicate unfaithful information in the summary. Among metrics based on word overlap, embedding similarity, and learned language understanding models, our QA-based metric has significantly higher correlation with human faithfulness scores, especially on highly abstractive summaries.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.calcs-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.calcs-1.0/>Proceedings of the The 4th Workshop on Computational Approaches to Code Switching</a></strong><br><a href=/people/t/thamar-solorio/>Thamar Solorio</a>
|
<a href=/people/m/monojit-choudhury/>Monojit Choudhury</a>
|
<a href=/people/k/kalika-bali/>Kalika Bali</a>
|
<a href=/people/s/sunayana-sitaram/>Sunayana Sitaram</a>
|
<a href=/people/a/amitava-das/>Amitava Das</a>
|
<a href=/people/m/mona-diab/>Mona Diab</a><br><a href=/volumes/2020.calcs-1/ class=text-muted>Proceedings of the The 4th Workshop on Computational Approaches to Code Switching</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.nlpmc-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.nlpmc-1.0/>Proceedings of the First Workshop on Natural Language Processing for Medical Conversations</a></strong><br><a href=/people/p/parminder-bhatia/>Parminder Bhatia</a>
|
<a href=/people/s/steven-lin/>Steven Lin</a>
|
<a href=/people/r/rashmi-gangadharaiah/>Rashmi Gangadharaiah</a>
|
<a href=/people/b/byron-c-wallace/>Byron Wallace</a>
|
<a href=/people/i/izhak-shafran/>Izhak Shafran</a>
|
<a href=/people/c/chaitanya-shivade/>Chaitanya Shivade</a>
|
<a href=/people/n/nan-du/>Nan Du</a>
|
<a href=/people/m/mona-diab/>Mona Diab</a><br><a href=/volumes/2020.nlpmc-1/ class=text-muted>Proceedings of the First Workshop on Natural Language Processing for Medical Conversations</a></span></p><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1127.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1127 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1127 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1127.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D19-1127/>CASA-NLU : Context-Aware Self-Attentive Natural Language Understanding for Task-Oriented Chatbots<span class=acl-fixed-case>CASA</span>-<span class=acl-fixed-case>NLU</span>: Context-Aware Self-Attentive Natural Language Understanding for Task-Oriented Chatbots</a></strong><br><a href=/people/a/arshit-gupta/>Arshit Gupta</a>
|
<a href=/people/p/peng-zhang/>Peng Zhang</a>
|
<a href=/people/g/garima-lalwani/>Garima Lalwani</a>
|
<a href=/people/m/mona-diab/>Mona Diab</a><br><a href=/volumes/D19-1/ class=text-muted>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1127><div class="card-body p-3 small">Natural Language Understanding (NLU) is a core component of <a href=https://en.wikipedia.org/wiki/Dialog_(software)>dialog systems</a>. It typically involves two tasks-Intent Classification (IC) and Slot Labeling (SL), which are then followed by a dialogue management (DM) component. Such NLU systems cater to utterances in isolation, thus pushing the problem of <a href=https://en.wikipedia.org/wiki/Context_management>context management</a> to DM. However, <a href=https://en.wikipedia.org/wiki/Context_(language_use)>contextual information</a> is critical to the correct prediction of intents in a conversation. Prior work on contextual NLU has been limited in terms of the types of contextual signals used and the understanding of their impact on the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a>. In this work, we propose a context-aware self-attentive NLU (CASA-NLU) model that uses multiple signals over a variable context window, such as previous intents, slots, dialog acts and utterances, in addition to the current user utterance. CASA-NLU outperforms a recurrent contextual NLU baseline on two conversational datasets, yielding a gain of up to 7 % on the IC task. Moreover, a non-contextual variant of CASA-NLU achieves state-of-the-art performance on standard public datasets-SNIPS and ATIS.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1380.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1380 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1380 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1380" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1380/>Efficient Sentence Embedding using <a href=https://en.wikipedia.org/wiki/Discrete_cosine_transform>Discrete Cosine Transform</a></a></strong><br><a href=/people/n/nada-almarwani/>Nada Almarwani</a>
|
<a href=/people/h/hanan-aldarmaki/>Hanan Aldarmaki</a>
|
<a href=/people/m/mona-diab/>Mona Diab</a><br><a href=/volumes/D19-1/ class=text-muted>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1380><div class="card-body p-3 small">Vector averaging remains one of the most popular sentence embedding methods in spite of its obvious disregard for <a href=https://en.wikipedia.org/wiki/Syntax>syntactic structure</a>. While more complex sequential or convolutional networks potentially yield superior <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a> performance, the improvements in <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification accuracy</a> are typically mediocre compared to the simple vector averaging. As an efficient alternative, we propose the use of discrete cosine transform (DCT) to compress word sequences in an order-preserving manner. The lower order DCT coefficients represent the overall feature patterns in sentences, which results in suitable embeddings for tasks that could benefit from syntactic features. Our results in semantic probing tasks demonstrate that DCT embeddings indeed preserve more syntactic information compared with vector averaging. With practically equivalent complexity, the model yields better overall performance in downstream classification tasks that correlate with syntactic features, which illustrates the capacity of <a href=https://en.wikipedia.org/wiki/Discrete_cosine_transform>DCT</a> to preserve word order information.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1460.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1460 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1460 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1460.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D19-1460/>Multi-Domain Goal-Oriented Dialogues (MultiDoGO): Strategies toward Curating and Annotating Large Scale Dialogue Data<span class=acl-fixed-case>M</span>ulti<span class=acl-fixed-case>D</span>o<span class=acl-fixed-case>GO</span>): Strategies toward Curating and Annotating Large Scale Dialogue Data</a></strong><br><a href=/people/d/denis-peskov/>Denis Peskov</a>
|
<a href=/people/n/nancy-clarke/>Nancy Clarke</a>
|
<a href=/people/j/jason-krone/>Jason Krone</a>
|
<a href=/people/b/brigi-fodor/>Brigi Fodor</a>
|
<a href=/people/y/yi-zhang/>Yi Zhang</a>
|
<a href=/people/a/adel-youssef/>Adel Youssef</a>
|
<a href=/people/m/mona-diab/>Mona Diab</a><br><a href=/volumes/D19-1/ class=text-muted>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1460><div class="card-body p-3 small">The need for high-quality, large-scale, goal-oriented dialogue datasets continues to grow as <a href=https://en.wikipedia.org/wiki/Virtual_assistant>virtual assistants</a> become increasingly wide-spread. However, publicly available datasets useful for this area are limited either in their size, <a href=https://en.wikipedia.org/wiki/Linguistic_diversity>linguistic diversity</a>, domain coverage, or <a href=https://en.wikipedia.org/wiki/Granularity>annotation granularity</a>. In this paper, we present <a href=https://en.wikipedia.org/wiki/Strategy>strategies</a> toward curating and annotating large scale goal oriented dialogue data. We introduce the MultiDoGO dataset to overcome these limitations. With a total of over 81 K dialogues harvested across six domains, MultiDoGO is over 8 times the size of MultiWOZ, the other largest comparable dialogue dataset currently available to the public. Over 54 K of these harvested conversations are annotated for intent classes and slot labels. We adopt a Wizard-of-Oz approach wherein a crowd-sourced worker (the customer) is paired with a trained annotator (the agent). The data curation process was controlled via biases to ensure a diversity in dialogue flows following variable dialogue policies. We provide distinct class label tags for agents vs. customer utterances, along with applicable slot labels. We also compare and contrast our <a href=https://en.wikipedia.org/wiki/Strategy>strategies</a> on annotation granularity, i.e. turn vs. sentence level. Furthermore, we compare and contrast annotations curated by leveraging professional annotators vs the crowd. We believe our strategies for eliciting and annotating such a dialogue dataset scales across modalities and domains and potentially languages in the future. To demonstrate the efficacy of our devised strategies we establish neural baselines for <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a> on the agent and customer utterances as well as slot labeling for each domain.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5004.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5004 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5004 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-5004" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-5004/>Identifying Nuances in <a href=https://en.wikipedia.org/wiki/Fake_news>Fake News</a> vs. <a href=https://en.wikipedia.org/wiki/Satire>Satire</a> : Using Semantic and Linguistic Cues</a></strong><br><a href=/people/o/or-levi/>Or Levi</a>
|
<a href=/people/p/pedram-hosseini/>Pedram Hosseini</a>
|
<a href=/people/m/mona-diab/>Mona Diab</a>
|
<a href=/people/d/david-broniatowski/>David Broniatowski</a><br><a href=/volumes/D19-50/ class=text-muted>Proceedings of the Second Workshop on Natural Language Processing for Internet Freedom: Censorship, Disinformation, and Propaganda</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5004><div class="card-body p-3 small">The blurry line between nefarious fake news and protected-speech satire has been a notorious struggle for <a href=https://en.wikipedia.org/wiki/Social_media>social media platforms</a>. Further to the efforts of reducing exposure to <a href=https://en.wikipedia.org/wiki/Misinformation>misinformation</a> on <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>, purveyors of <a href=https://en.wikipedia.org/wiki/Fake_news>fake news</a> have begun to masquerade as satire sites to avoid being demoted. In this work, we address the challenge of automatically classifying fake news versus <a href=https://en.wikipedia.org/wiki/Satire>satire</a>. Previous work have studied whether <a href=https://en.wikipedia.org/wiki/Fake_news>fake news</a> and <a href=https://en.wikipedia.org/wiki/Satire>satire</a> can be distinguished based on <a href=https://en.wikipedia.org/wiki/Language>language differences</a>. Contrary to <a href=https://en.wikipedia.org/wiki/Fake_news>fake news</a>, <a href=https://en.wikipedia.org/wiki/Satire>satire stories</a> are usually humorous and carry some political or social message. We hypothesize that these nuances could be identified using semantic and linguistic cues. Consequently, we train a machine learning method using semantic representation, with a state-of-the-art contextual language model, and with <a href=https://en.wikipedia.org/wiki/Feature_(linguistics)>linguistic features</a> based on textual coherence metrics. Empirical evaluation attests to the merits of our approach compared to the language-based baseline and sheds light on the nuances between <a href=https://en.wikipedia.org/wiki/Fake_news>fake news</a> and <a href=https://en.wikipedia.org/wiki/Satire>satire</a>. As avenues for future work, we consider studying additional linguistic features related to the humor aspect, and enriching the <a href=https://en.wikipedia.org/wiki/Data>data</a> with current news events, to help identify a political or social message.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S19-2195.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S19-2195 data-toggle=collapse aria-expanded=false aria-controls=abstract-S19-2195 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/S19-2195/>GWU NLP at SemEval-2019 Task 7 : Hybrid Pipeline for Rumour Veracity and Stance Classification on Social Media<span class=acl-fixed-case>GWU</span> <span class=acl-fixed-case>NLP</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2019 Task 7: Hybrid Pipeline for Rumour Veracity and Stance Classification on Social Media</a></strong><br><a href=/people/s/sardar-hamidian/>Sardar Hamidian</a>
|
<a href=/people/m/mona-diab/>Mona Diab</a><br><a href=/volumes/S19-2/ class=text-muted>Proceedings of the 13th International Workshop on Semantic Evaluation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S19-2195><div class="card-body p-3 small">Social media plays a crucial role as the main resource news for information seekers online. However, the unmoderated feature of <a href=https://en.wikipedia.org/wiki/Social_media>social media platforms</a> lead to the emergence and spread of untrustworthy contents which harm individuals or even societies. Most of the current automated approaches for automatically determining the veracity of a rumor are not generalizable for novel emerging topics. This paper describes our hybrid system comprising <a href=https://en.wikipedia.org/wiki/Rule-based_system>rules</a> and a <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning model</a> which makes use of replied tweets to identify the veracity of the source tweet. The proposed system in this paper achieved 0.435 F-Macro in stance classification, and 0.262 F-macro and 0.801 <a href=https://en.wikipedia.org/wiki/RMSE>RMSE</a> in rumor verification tasks in Task7 of SemEval 2019.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1391.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1391 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1391 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1391" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N19-1391/>Context-Aware Cross-Lingual Mapping</a></strong><br><a href=/people/h/hanan-aldarmaki/>Hanan Aldarmaki</a>
|
<a href=/people/m/mona-diab/>Mona Diab</a><br><a href=/volumes/N19-1/ class=text-muted>Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1391><div class="card-body p-3 small">Cross-lingual word vectors are typically obtained by fitting an <a href=https://en.wikipedia.org/wiki/Orthogonal_matrix>orthogonal matrix</a> that maps the entries of a <a href=https://en.wikipedia.org/wiki/Bilingual_dictionary>bilingual dictionary</a> from a source to a target vector space. Word vectors, however, are most commonly used for sentence or document-level representations that are calculated as the weighted average of word embeddings. In this paper, we propose an alternative to word-level mapping that better reflects sentence-level cross-lingual similarity. We incorporate <a href=https://en.wikipedia.org/wiki/Context_(language_use)>context</a> in the <a href=https://en.wikipedia.org/wiki/Transformation_matrix>transformation matrix</a> by directly mapping the averaged embeddings of aligned sentences in a <a href=https://en.wikipedia.org/wiki/Parallel_text>parallel corpus</a>. We also implement cross-lingual mapping of deep contextualized word embeddings using parallel sentences with word alignments. In our experiments, both approaches resulted in cross-lingual sentence embeddings that outperformed context-independent word mapping in sentence translation retrieval. Furthermore, the sentence-level transformation could be used for word-level mapping without loss in word translation quality.</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1246.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1246 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1246 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1246/>Emotion Detection and Classification in a Multigenre Corpus with Joint Multi-Task Deep Learning</a></strong><br><a href=/people/s/shabnam-tafreshi/>Shabnam Tafreshi</a>
|
<a href=/people/m/mona-diab/>Mona Diab</a><br><a href=/volumes/C18-1/ class=text-muted>Proceedings of the 27th International Conference on Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1246><div class="card-body p-3 small">Detection and classification of emotion categories expressed by a sentence is a challenging task due to <a href=https://en.wikipedia.org/wiki/Subjectivity>subjectivity of emotion</a>. To date, most of the <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> are trained and evaluated on single genre and when used to predict emotion in different genre their performance drops by a large margin. To address the issue of <a href=https://en.wikipedia.org/wiki/Robustness_(computer_science)>robustness</a>, we model the <a href=https://en.wikipedia.org/wiki/Problem_solving>problem</a> within a joint multi-task learning framework. We train this <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> with a multigenre emotion corpus to predict emotions across various genre. Each genre is represented as a separate task, we use soft parameter shared layers across the various tasks. our experimental results show that this model improves the results across the various genres, compared to a single genre training in the same neural net architecture.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3200.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3200/>Proceedings of the Third Workshop on Computational Approaches to Linguistic Code-Switching</a></strong><br><a href=/people/g/gustavo-aguilar/>Gustavo Aguilar</a>
|
<a href=/people/f/fahad-alghamdi/>Fahad AlGhamdi</a>
|
<a href=/people/v/victor-soto/>Victor Soto</a>
|
<a href=/people/t/thamar-solorio/>Thamar Solorio</a>
|
<a href=/people/m/mona-diab/>Mona Diab</a>
|
<a href=/people/j/julia-hirschberg/>Julia Hirschberg</a><br><a href=/volumes/W18-32/ class=text-muted>Proceedings of the Third Workshop on Computational Approaches to Linguistic Code-Switching</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5525.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5525 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5525 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5525/>Team SWEEPer : Joint Sentence Extraction and Fact Checking with Pointer Networks<span class=acl-fixed-case>SWEEP</span>er: Joint Sentence Extraction and Fact Checking with Pointer Networks</a></strong><br><a href=/people/c/christopher-hidey/>Christopher Hidey</a>
|
<a href=/people/m/mona-diab/>Mona Diab</a><br><a href=/volumes/W18-55/ class=text-muted>Proceedings of the First Workshop on Fact Extraction and VERification (FEVER)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5525><div class="card-body p-3 small">Many <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> such as <a href=https://en.wikipedia.org/wiki/Question_answering>question answering</a> and <a href=https://en.wikipedia.org/wiki/Reading_comprehension>reading comprehension</a> rely on information extracted from unreliable sources. These <a href=https://en.wikipedia.org/wiki/System>systems</a> would thus benefit from knowing whether a statement from an unreliable source is correct. We present experiments on the FEVER (Fact Extraction and VERification) task, a shared task that involves selecting sentences from <a href=https://en.wikipedia.org/wiki/Wikipedia>Wikipedia</a> and predicting whether a claim is supported by those sentences, refuted, or there is not enough information. Fact checking is a <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> that benefits from not only asserting or disputing the veracity of a claim but also finding evidence for that position. As these tasks are dependent on each other, an ideal model would consider the veracity of the claim when finding evidence and also find only the evidence that is relevant. We thus jointly model <a href=https://en.wikipedia.org/wiki/Sentence_extraction>sentence extraction</a> and <a href=https://en.wikipedia.org/wiki/Verification_and_validation>verification</a> on the FEVER shared task. Among all participants, we ranked 5th on the <a href=https://en.wikipedia.org/wiki/Blinded_experiment>blind test set</a> (prior to any additional human evaluation of the evidence).</div></div><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-2003.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-2003 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-2003 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-2003/>Transferring Semantic Roles Using Translation and Syntactic Information</a></strong><br><a href=/people/m/maryam-aminian/>Maryam Aminian</a>
|
<a href=/people/m/mohammad-sadegh-rasooli/>Mohammad Sadegh Rasooli</a>
|
<a href=/people/m/mona-diab/>Mona Diab</a><br><a href=/volumes/I17-2/ class=text-muted>Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-2003><div class="card-body p-3 small">Our paper addresses the problem of annotation projection for semantic role labeling for resource-poor languages using supervised annotations from a resource-rich language through parallel data. We propose a transfer method that employs information from source and target syntactic dependencies as well as word alignment density to improve the quality of an iterative bootstrapping method. Our experiments yield a 3.5 absolute labeled F-score improvement over a standard annotation projection method.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1300.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1300/>Proceedings of the Third <span class=acl-fixed-case>A</span>rabic Natural Language Processing Workshop</a></strong><br><a href=/people/n/nizar-habash/>Nizar Habash</a>
|
<a href=/people/m/mona-diab/>Mona Diab</a>
|
<a href=/people/k/kareem-darwish/>Kareem Darwish</a>
|
<a href=/people/w/wassim-el-hajj/>Wassim El-Hajj</a>
|
<a href=/people/h/hend-al-khalifa/>Hend Al-Khalifa</a>
|
<a href=/people/h/houda-bouamor/>Houda Bouamor</a>
|
<a href=/people/n/nadi-tomeh/>Nadi Tomeh</a>
|
<a href=/people/m/mahmoud-el-haj/>Mahmoud El-Haj</a>
|
<a href=/people/w/wajdi-zaghouani/>Wajdi Zaghouani</a><br><a href=/volumes/W17-13/ class=text-muted>Proceedings of the Third Arabic Natural Language Processing Workshop</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1321.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1321 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1321 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1321/>A Layered Language Model based Hybrid Approach to Automatic Full Diacritization of Arabic<span class=acl-fixed-case>A</span>rabic</a></strong><br><a href=/people/m/mohamed-al-badrashiny/>Mohamed Al-Badrashiny</a>
|
<a href=/people/a/abdelati-hawwari/>Abdelati Hawwari</a>
|
<a href=/people/m/mona-diab/>Mona Diab</a><br><a href=/volumes/W17-13/ class=text-muted>Proceedings of the Third Arabic Natural Language Processing Workshop</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1321><div class="card-body p-3 small">In this paper we present a system for automatic Arabic text diacritization using three levels of analysis granularity in a layered back off manner. We build and exploit diacritized language models (LM) for each of three different levels of granularity : surface form, morphologically segmented into prefix / stem / suffix, and character level. For each of the passes, we use <a href=https://en.wikipedia.org/wiki/Viterbi_search>Viterbi search</a> to pick the most probable diacritization per word in the input. We start with the surface form LM, followed by the morphological level, then finally we leverage the character level LM. Our <a href=https://en.wikipedia.org/wiki/System>system</a> outperforms all of the published <a href=https://en.wikipedia.org/wiki/System>systems</a> evaluated against the same <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training and test data</a>. It achieves a 10.87 % <a href=https://en.wikipedia.org/wiki/Word_error_rate>WER</a> for complete full diacritization including lexical and syntactic diacritization, and 3.0 % <a href=https://en.wikipedia.org/wiki/Word_error_rate>WER</a> for lexical diacritization, ignoring syntactic diacritization.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1322.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1322 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1322 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1322/>Arabic Textual Entailment with Word Embeddings<span class=acl-fixed-case>A</span>rabic Textual Entailment with Word Embeddings</a></strong><br><a href=/people/n/nada-almarwani/>Nada Almarwani</a>
|
<a href=/people/m/mona-diab/>Mona Diab</a><br><a href=/volumes/W17-13/ class=text-muted>Proceedings of the Third Arabic Natural Language Processing Workshop</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1322><div class="card-body p-3 small">Determining the <a href=https://en.wikipedia.org/wiki/Textual_entailment>textual entailment</a> between texts is important in many NLP tasks, such as <a href=https://en.wikipedia.org/wiki/Automatic_summarization>summarization</a>, <a href=https://en.wikipedia.org/wiki/Question_answering>question answering</a>, and <a href=https://en.wikipedia.org/wiki/Information_retrieval>information extraction and retrieval</a>. Various methods have been suggested based on external knowledge sources ; however, such resources are not always available in all languages and their acquisition is typically laborious and very costly. Distributional word representations such as <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> learned over large corpora have been shown to capture syntactic and semantic word relationships. Such <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> have contributed to improving the performance of several <a href=https://en.wikipedia.org/wiki/Neuro-linguistic_programming>NLP tasks</a>. In this paper, we address the problem of <a href=https://en.wikipedia.org/wiki/Textual_entailment>textual entailment</a> in <a href=https://en.wikipedia.org/wiki/Arabic>Arabic</a>. We employ both traditional <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> and distributional representations. Crucially, we do not depend on any external resources in the process. Our suggested approach yields state of the art performance on a standard data set, ArbTE, achieving an accuracy of 76.2 % compared to state of the art of 69.3 %.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S17-2056.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S17-2056 data-toggle=collapse aria-expanded=false aria-controls=abstract-S17-2056 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/S17-2056/>GW_QA at SemEval-2017 Task 3 : Question Answer Re-ranking on Arabic Fora<span class=acl-fixed-case>GW</span>_<span class=acl-fixed-case>QA</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2017 Task 3: Question Answer Re-ranking on <span class=acl-fixed-case>A</span>rabic Fora</a></strong><br><a href=/people/n/nada-almarwani/>Nada Almarwani</a>
|
<a href=/people/m/mona-diab/>Mona Diab</a><br><a href=/volumes/S17-2/ class=text-muted>Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S17-2056><div class="card-body p-3 small">This paper describes our submission to SemEval-2017 Task 3 Subtask D, Question Answer Ranking in Arabic Community Question Answering. In this work, we applied a supervised machine learning approach to automatically re-rank a set of QA pairs according to their relevance to a given question. We employ <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> based on latent semantic models, namely WTMF, as well as a set of lexical features based on string lengths and surface level matching. The proposed <a href=https://en.wikipedia.org/wiki/System>system</a> ranked first out of 3 submissions, with a <a href=https://en.wikipedia.org/wiki/Score_(statistics)>MAP score</a> of 61.16 %.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Mona+Diab" title="Search for 'Mona Diab' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/t/thamar-solorio/ class=align-middle>Thamar Solorio</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/n/nada-almarwani/ class=align-middle>Nada Almarwani</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/s/sunayana-sitaram/ class=align-middle>Sunayana Sitaram</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/v/victor-soto/ class=align-middle>Victor Soto</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/h/hanan-aldarmaki/ class=align-middle>Hanan Aldarmaki</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/s/shabnam-tafreshi/ class=align-middle>Shabnam Tafreshi</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/w/wei-jen-ko/ class=align-middle>Wei-Jen Ko</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/ahmed-el-kishky/ class=align-middle>Ahmed El-Kishky</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/adithya-renduchintala/ class=align-middle>Adithya Renduchintala</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/v/vishrav-chaudhary/ class=align-middle>Vishrav Chaudhary</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/n/naman-goyal/ class=align-middle>Naman Goyal</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/f/francisco-guzman/ class=align-middle>Francisco Guzm√°n</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/p/pascale-fung/ class=align-middle>Pascale Fung</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/p/philipp-koehn/ class=align-middle>Philipp Koehn</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/maryam-aminian/ class=align-middle>Maryam Aminian</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/mohammad-sadegh-rasooli/ class=align-middle>Mohammad Sadegh Rasooli</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/shuguang-chen/ class=align-middle>Shuguang Chen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/alan-w-black/ class=align-middle>Alan W. Black</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/e/emre-yilmaz/ class=align-middle>Emre Yilmaz</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/anirudh-srinivasan/ class=align-middle>Anirudh Srinivasan</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/e/esin-durmus/ class=align-middle>Esin Durmus</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/he-he/ class=align-middle>He He</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/n/nizar-habash/ class=align-middle>Nizar Habash</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/kareem-darwish/ class=align-middle>Kareem Darwish</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/w/wassim-el-hajj/ class=align-middle>Wassim El-Hajj</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/hend-al-khalifa/ class=align-middle>Hend Al-Khalifa</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/houda-bouamor/ class=align-middle>Houda Bouamor</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/n/nadi-tomeh/ class=align-middle>Nadi Tomeh</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/mahmoud-el-haj/ class=align-middle>Mahmoud El-Haj</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/w/wajdi-zaghouani/ class=align-middle>Wajdi Zaghouani</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/mohamed-al-badrashiny/ class=align-middle>Mohamed Al-Badrashiny</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/abdelati-hawwari/ class=align-middle>Abdelati Hawwari</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/monojit-choudhury/ class=align-middle>Monojit Choudhury</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/kalika-bali/ class=align-middle>Kalika Bali</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/amitava-das/ class=align-middle>Amitava Das</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/arshit-gupta/ class=align-middle>Arshit Gupta</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/p/peng-zhang/ class=align-middle>Peng Zhang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/g/garima-lalwani/ class=align-middle>Garima Lalwani</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/denis-peskov/ class=align-middle>Denis Peskov</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/n/nancy-clarke/ class=align-middle>Nancy Clarke</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jason-krone/ class=align-middle>Jason Krone</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/b/brigi-fodor/ class=align-middle>Brigi Fodor</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yi-zhang/ class=align-middle>Yi Zhang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/adel-youssef/ class=align-middle>Adel Youssef</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/o/or-levi/ class=align-middle>Or Levi</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/p/pedram-hosseini/ class=align-middle>Pedram Hosseini</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/david-broniatowski/ class=align-middle>David Broniatowski</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/sardar-hamidian/ class=align-middle>Sardar Hamidian</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/g/gustavo-aguilar/ class=align-middle>Gustavo Aguilar</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/f/fahad-alghamdi/ class=align-middle>Fahad AlGhamdi</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/julia-hirschberg/ class=align-middle>Julia Hirschberg</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/christopher-hidey/ class=align-middle>Christopher Hidey</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/p/parminder-bhatia/ class=align-middle>Parminder Bhatia</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/steven-lin/ class=align-middle>Steven Lin</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/rashmi-gangadharaiah/ class=align-middle>Rashmi Gangadharaiah</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/b/byron-c-wallace/ class=align-middle>Byron C. Wallace</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/i/izhak-shafran/ class=align-middle>Izhak Shafran</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/chaitanya-shivade/ class=align-middle>Chaitanya Shivade</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/n/nan-du/ class=align-middle>Nan Du</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/ws/ class=align-middle>WS</a><span class="badge badge-secondary align-middle ml-2">5</span></li><li class=list-group-item><a href=/venues/emnlp/ class=align-middle>EMNLP</a><span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/venues/acl/ class=align-middle>ACL</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/calcs/ class=align-middle>CALCS</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/semeval/ class=align-middle>SemEval</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-venues aria-expanded=false aria-controls=more-venues>show all...</li><div class="collapse border-top" id=more-venues><li class=list-group-item><a href=/venues/coling/ class=align-middle>COLING</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/ijcnlp/ class=align-middle>IJCNLP</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/nlpmc/ class=align-middle>NLPMC</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/naacl/ class=align-middle>NAACL</a><span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ¬©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>