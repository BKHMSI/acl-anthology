<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Mark Steedman - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Mark</span> <span class=font-weight-bold>Steedman</span></h2><hr><div class=row><div class=col-lg-9><h4>2022</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-acl.96.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-acl--96 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-acl.96 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2022.findings-acl.96.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.findings-acl.96" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2022.findings-acl.96/>Cross-lingual Inference with A <span class=acl-fixed-case>C</span>hinese Entailment Graph</a></strong><br><a href=/people/t/tianyi-li/>Tianyi Li</a>
|
<a href=/people/s/sabine-weber/>Sabine Weber</a>
|
<a href=/people/m/mohammad-javad-hosseini/>Mohammad Javad Hosseini</a>
|
<a href=/people/l/liane-guillou/>Liane Guillou</a>
|
<a href=/people/m/mark-steedman/>Mark Steedman</a><br><a href=/volumes/2022.findings-acl/ class=text-muted>Findings of the Association for Computational Linguistics: ACL 2022</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-acl--96><div class="card-body p-3 small">Predicate entailment detection is a crucial task for question-answering from text, where previous work has explored unsupervised learning of entailment graphs from typed open relation triples. In this paper, we present the first pipeline for building Chinese entailment graphs, which involves a novel high-recall open relation extraction (ORE) method and the first Chinese fine-grained entity typing dataset under the FIGER type ontology. Through experiments on the Levy-Holt dataset, we verify the strength of our Chinese entailment graph, and reveal the cross-lingual complementarity: on the parallel Levy-Holt dataset, an ensemble of Chinese and English entailment graphs outperforms both monolingual graphs, and raises unsupervised SOTA by 4.7 AUC points.</div></div><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.cl-1.2.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--cl-1--2 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.cl-1.2 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.cl-1.2/>Formal Basis of a Language Universal</a></strong><br><a href=/people/m/milos-stanojevic/>Miloš Stanojević</a>
|
<a href=/people/m/mark-steedman/>Mark Steedman</a><br><a href=/volumes/2021.cl-1/ class=text-muted>Computational Linguistics, Volume 47, Issue 1 - March 2021</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--cl-1--2><div class="card-body p-3 small">Abstract Steedman (2020) proposes as a formal universal of natural language grammar that grammatical permutations of the kind that have given rise to transformational rules are limited to a class known to mathematicians and computer scientists as the separable permutations. This <a href=https://en.wikipedia.org/wiki/Class_(set_theory)>class of permutations</a> is exactly the <a href=https://en.wikipedia.org/wiki/Class_(set_theory)>class</a> that can be expressed in combinatory categorial grammars (CCGs). The excluded non-separable permutations do in fact seem to be absent in a number of studies of crosslinguistic variation in <a href=https://en.wikipedia.org/wiki/Word_order>word order</a> in nominal and verbal constructions. The number of <a href=https://en.wikipedia.org/wiki/Permutation>permutations</a> that are separable grows in the number n of lexical elements in the construction as the Large Schrder Number Sn1. Because that <a href=https://en.wikipedia.org/wiki/Number>number</a> grows much more slowly than the n ! number of all permutations, this <a href=https://en.wikipedia.org/wiki/Generalization>generalization</a> is also of considerable practical interest for computational applications such as <a href=https://en.wikipedia.org/wiki/Parsing>parsing</a> and <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>. The present article examines the mathematical and computational origins of this <a href=https://en.wikipedia.org/wiki/Restriction_(mathematics)>restriction</a>, and the reason it is exactly captured in CCG without the imposition of any further constraints.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.87.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--87 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.87 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.87" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.87/>Cross-lingual Intermediate Fine-tuning improves Dialogue State Tracking</a></strong><br><a href=/people/n/nikita-moghe/>Nikita Moghe</a>
|
<a href=/people/m/mark-steedman/>Mark Steedman</a>
|
<a href=/people/a/alexandra-birch/>Alexandra Birch</a><br><a href=/volumes/2021.emnlp-main/ class=text-muted>Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--87><div class="card-body p-3 small">Recent progress in task-oriented neural dialogue systems is largely focused on a handful of languages, as annotation of training data is tedious and expensive. Machine translation has been used to make systems multilingual, but this can introduce a pipeline of errors. Another promising solution is using cross-lingual transfer learning through pretrained multilingual models. Existing methods train multilingual models with additional code-mixed task data or refine the cross-lingual representations through parallel ontologies. In this work, we enhance the transfer learning process by intermediate fine-tuning of pretrained multilingual models, where the multilingual models are fine-tuned with different but related data and/or tasks. Specifically, we use parallel and conversational movie subtitles datasets to design cross-lingual intermediate tasks suitable for downstream dialogue tasks. We use only 200 K lines of parallel data for intermediate fine-tuning which is already available for 1782 language pairs. We test our approach on the cross-lingual dialogue state tracking task for the parallel MultiWoZ (English-Chinese, Chinese-English) and Multilingual WoZ (English-German, English-Italian) datasets. We achieve impressive improvements (20 % on joint goal accuracy) on the parallel MultiWoZ dataset and the Multilingual WoZ dataset over the vanilla baseline with only 10 % of the target language task data and zero-shot setup respectively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.cmcl-1.3.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--cmcl-1--3 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.cmcl-1.3 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.cmcl-1.3/>Modeling Incremental Language Comprehension in the Brain with Combinatory Categorial Grammar<span class=acl-fixed-case>C</span>ombinatory <span class=acl-fixed-case>C</span>ategorial <span class=acl-fixed-case>G</span>rammar</a></strong><br><a href=/people/m/milos-stanojevic/>Miloš Stanojević</a>
|
<a href=/people/s/shohini-bhattasali/>Shohini Bhattasali</a>
|
<a href=/people/d/donald-dunagan/>Donald Dunagan</a>
|
<a href=/people/l/luca-campanelli/>Luca Campanelli</a>
|
<a href=/people/m/mark-steedman/>Mark Steedman</a>
|
<a href=/people/j/jonathan-brennan/>Jonathan Brennan</a>
|
<a href=/people/j/john-hale/>John Hale</a><br><a href=/volumes/2021.cmcl-1/ class=text-muted>Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--cmcl-1--3><div class="card-body p-3 small">Hierarchical sentence structure plays a role in word-by-word human sentence comprehension, but it remains unclear how best to characterize this <a href=https://en.wikipedia.org/wiki/Structure>structure</a> and unknown how exactly it would be recognized in a step-by-step process model. With a view towards sharpening this picture, we model the time course of <a href=https://en.wikipedia.org/wiki/Hemodynamics>hemodynamic activity</a> within the brain during an extended episode of <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>naturalistic language comprehension</a> using Combinatory Categorial Grammar (CCG). CCG has well-defined incremental parsing algorithms, surface compositional semantics, and can explain long-range dependencies as well as complicated cases of coordination. We find that CCG-derived predictors improve a regression model of fMRI time course in six language-relevant brain regions, over and above <a href=https://en.wikipedia.org/wiki/Prediction>predictors</a> derived from context-free phrase structure. Adding a special Revealing operator to CCG parsing, one designed to handle right-adjunction, improves the fit in three of these regions. This evidence for CCG from <a href=https://en.wikipedia.org/wiki/Neuroimaging>neuroimaging</a> bolsters the more general case for mildly context-sensitive grammars in the cognitive science of language.</div></div><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.starsem-1.15.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--starsem-1--15 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.starsem-1.15 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.starsem-1.15/>Learning Negation Scope from <a href=https://en.wikipedia.org/wiki/Syntactic_structure>Syntactic Structure</a></a></strong><br><a href=/people/n/nick-mckenna/>Nick McKenna</a>
|
<a href=/people/m/mark-steedman/>Mark Steedman</a><br><a href=/volumes/2020.starsem-1/ class=text-muted>Proceedings of the Ninth Joint Conference on Lexical and Computational Semantics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--starsem-1--15><div class="card-body p-3 small">We present a <a href=https://en.wikipedia.org/wiki/Semi-supervised_learning>semi-supervised model</a> which learns the semantics of negation purely through <a href=https://en.wikipedia.org/wiki/Syntactic_analysis>analysis of syntactic structure</a>. Linguistic theory posits that the semantics of negation can be understood purely syntactically, though recent research relies on combining a variety of features including <a href=https://en.wikipedia.org/wiki/Part-of-speech_tagging>part-of-speech tags</a>, <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a>, and semantic representations to achieve high task performance. Our simplified model returns to syntactic theory and achieves state-of-the-art performance on the task of Negation Scope Detection while demonstrating the tight relationship between the <a href=https://en.wikipedia.org/wiki/Syntax>syntax</a> and <a href=https://en.wikipedia.org/wiki/Negation>semantics of negation</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.401.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--401 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.401 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.401/>Aspectuality Across Genre : A Distributional Semantics Approach</a></strong><br><a href=/people/t/thomas-kober/>Thomas Kober</a>
|
<a href=/people/m/malihe-alikhani/>Malihe Alikhani</a>
|
<a href=/people/m/matthew-stone/>Matthew Stone</a>
|
<a href=/people/m/mark-steedman/>Mark Steedman</a><br><a href=/volumes/2020.coling-main/ class=text-muted>Proceedings of the 28th International Conference on Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--401><div class="card-body p-3 small">The interpretation of the <a href=https://en.wikipedia.org/wiki/Lexical_aspect>lexical aspect</a> of verbs in <a href=https://en.wikipedia.org/wiki/English_language>English</a> plays a crucial role in tasks such as recognizing textual entailment and learning discourse-level inferences. We show that two elementary dimensions of aspectual class, states vs. events, and telic vs. atelic events, can be modelled effectively with <a href=https://en.wikipedia.org/wiki/Distributional_semantics>distributional semantics</a>. We find that a verb&#8217;s local context is most indicative of its <a href=https://en.wikipedia.org/wiki/Grammatical_aspect>aspectual class</a>, and we demonstrate that closed class words tend to be stronger discriminating contexts than <a href=https://en.wikipedia.org/wiki/Content_word>content words</a>. Our approach outperforms previous work on three datasets. Further, we present a new dataset of human-human conversations annotated with lexical aspects and present experiments that show the correlation of <a href=https://en.wikipedia.org/wiki/Telicity>telicity</a> with genre and discourse goals.</div></div><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5321.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5321 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5321 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5321/>Node Embeddings for Graph Merging : Case of Knowledge Graph Construction</a></strong><br><a href=/people/i/ida-szubert/>Ida Szubert</a>
|
<a href=/people/m/mark-steedman/>Mark Steedman</a><br><a href=/volumes/D19-53/ class=text-muted>Proceedings of the Thirteenth Workshop on Graph-Based Methods for Natural Language Processing (TextGraphs-13)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5321><div class="card-body p-3 small">Combining two <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graphs</a> requires merging the nodes which are counterparts of each other. In this process errors occur, resulting in incorrect merging or incorrect failure to merge. We find a high prevalence of such errors when using AskNET, an <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> for building Knowledge Graphs from <a href=https://en.wikipedia.org/wiki/Text_corpus>text corpora</a>. AskNET node matching method uses <a href=https://en.wikipedia.org/wiki/String_similarity>string similarity</a>, which we propose to replace with vector embedding similarity. We explore graph-based and word-based embedding models and show an overall <a href=https://en.wikipedia.org/wiki/Error_detection_and_correction>error reduction</a> of from 56 % to 23.6 %, with a reduction of over a half in both types of incorrect node matching.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-0409.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-0409 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-0409 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-0409" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-0409/>Temporal and Aspectual Entailment</a></strong><br><a href=/people/t/thomas-kober/>Thomas Kober</a>
|
<a href=/people/s/sander-bijl-de-vroe/>Sander Bijl de Vroe</a>
|
<a href=/people/m/mark-steedman/>Mark Steedman</a><br><a href=/volumes/W19-04/ class=text-muted>Proceedings of the 13th International Conference on Computational Semantics - Long Papers</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-0409><div class="card-body p-3 small">Inferences regarding Jane&#8217;s arrival in London from <a href=https://en.wikipedia.org/wiki/Predicate_(grammar)>predications</a> such as Jane is going to London or Jane has gone to London depend on <a href=https://en.wikipedia.org/wiki/Grammatical_tense>tense</a> and aspect of the predications. Tense determines the temporal location of the predication in the past, present or future of the time of utterance. The aspectual auxiliaries on the other hand specify the internal constituency of the event, i.e. whether the event of going to London is completed and whether its consequences hold at that time or not. While <a href=https://en.wikipedia.org/wiki/Grammatical_tense>tense</a> and <a href=https://en.wikipedia.org/wiki/Grammatical_aspect>aspect</a> are among the most important factors for determining <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language inference</a>, there has been very little work to show whether modern <a href=https://en.wikipedia.org/wiki/Embedding>embedding models</a> capture these semantic concepts. In this paper we propose a novel entailment dataset and analyse the ability of contextualised word representations to perform <a href=https://en.wikipedia.org/wiki/Inference>inference</a> on <a href=https://en.wikipedia.org/wiki/Predicate_(grammar)>predications</a> across aspectual types and tenses. We show that they encode a substantial amount of information relating to <a href=https://en.wikipedia.org/wiki/Grammatical_tense>tense</a> and <a href=https://en.wikipedia.org/wiki/Grammatical_aspect>aspect</a>, but fail to consistently model inferences that require reasoning with these semantic properties.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3625 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3625 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3625/>Construction and Alignment of Multilingual Entailment Graphs for Semantic Inference</a></strong><br><a href=/people/s/sabine-weber/>Sabine Weber</a>
|
<a href=/people/m/mark-steedman/>Mark Steedman</a><br><a href=/volumes/W19-36/ class=text-muted>Proceedings of the 2019 Workshop on Widening NLP</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3625><div class="card-body p-3 small">This paper presents ongoing work on the construction and alignment of predicate entailment graphs in English and German. We extract predicate-argument pairs from large corpora of monolingual English and German news text and construct monolingual paraphrase clusters and entailment graphs. We use an aligned subset of entities to derive the bilingual alignment of entities and relations, and achieve better than baseline results on a translated subset of a predicate entailment data set (Levy and Dagan, 2016) and the German portion of XNLI (Conneau et al., 2018).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1020.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1020 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1020 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/360516550 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1020" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1020/>CCG Parsing Algorithm with Incremental Tree Rotation<span class=acl-fixed-case>CCG</span> Parsing Algorithm with Incremental Tree Rotation</a></strong><br><a href=/people/m/milos-stanojevic/>Miloš Stanojević</a>
|
<a href=/people/m/mark-steedman/>Mark Steedman</a><br><a href=/volumes/N19-1/ class=text-muted>Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1020><div class="card-body p-3 small">The main obstacle to incremental sentence processing arises from right-branching constituent structures, which are present in the majority of English sentences, as well as optional constituents that adjoin on the right, such as <a href=https://en.wikipedia.org/wiki/Adjunct_(grammar)>right adjuncts</a> and right conjuncts. In CCG, many right-branching derivations can be replaced by semantically equivalent left-branching incremental derivations. The problem of right-adjunction is more resistant to solution, and has been tackled in the past using revealing-based approaches that often rely either on the <a href=https://en.wikipedia.org/wiki/Unification_(computer_science)>higher-order unification</a> over lambda terms (Pareschi and Steedman,1987) or heuristics over dependency representations that do not cover the whole CCGbank (Ambati et al., 2015). We propose a new incremental parsing algorithm for CCG following the same revealing tradition of work but having a purely syntactic approach that does not depend on access to a distinct level of semantic representation. This <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> can cover the whole CCGbank, with greater incrementality and <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> than previous proposals.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1468.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1468 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1468 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1468" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-1468/>Duality of Link Prediction and Entailment Graph Induction</a></strong><br><a href=/people/m/mohammad-javad-hosseini/>Mohammad Javad Hosseini</a>
|
<a href=/people/s/shay-b-cohen/>Shay B. Cohen</a>
|
<a href=/people/m/mark-johnson/>Mark Johnson</a>
|
<a href=/people/m/mark-steedman/>Mark Steedman</a><br><a href=/volumes/P19-1/ class=text-muted>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1468><div class="card-body p-3 small">Link prediction and entailment graph induction are often treated as different problems. In this paper, we show that these two <a href=https://en.wikipedia.org/wiki/Problem_solving>problems</a> are actually complementary. We train a link prediction model on a knowledge graph of assertions extracted from raw text. We propose an <a href=https://en.wikipedia.org/wiki/Logical_consequence>entailment score</a> that exploits the new facts discovered by the link prediction model, and then form entailment graphs between relations. We further use the learned <a href=https://en.wikipedia.org/wiki/Logical_consequence>entailments</a> to predict improved link prediction scores. Our results show that the two <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> can benefit from each other. The new entailment score outperforms prior state-of-the-art results on a standard entialment dataset and the new link prediction scores show improvements over the raw link prediction scores.</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1545.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1545 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1545 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1545" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D18-1545/>Data Augmentation via Dependency Tree Morphing for Low-Resource Languages</a></strong><br><a href=/people/g/gozde-gul-sahin/>Gözde Gül Şahin</a>
|
<a href=/people/m/mark-steedman/>Mark Steedman</a><br><a href=/volumes/D18-1/ class=text-muted>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1545><div class="card-body p-3 small">Neural NLP systems achieve high scores in the presence of sizable training dataset. Lack of such <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> leads to poor <a href=https://en.wikipedia.org/wiki/System>system</a> performances in the case low-resource languages. We present two simple text augmentation techniques using dependency trees, inspired from <a href=https://en.wikipedia.org/wiki/Digital_image_processing>image processing</a>. We crop sentences by removing dependency links, and we rotate sentences by moving the tree fragments around the root. We apply these techniques to augment the training sets of low-resource languages in Universal Dependencies project. We implement a character-level sequence tagging model and evaluate the augmented datasets on part-of-speech tagging task. We show that <a href=https://en.wikipedia.org/wiki/Crop_factor>crop</a> and rotate provides improvements over the <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> trained with non-augmented data for majority of the languages, especially for languages with rich case marking systems.</div></div><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1009.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1009 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1009 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D17-1009.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/238236598 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D17-1009" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/D17-1009/>Universal Semantic Parsing</a></strong><br><a href=/people/s/siva-reddy/>Siva Reddy</a>
|
<a href=/people/o/oscar-tackstrom/>Oscar Täckström</a>
|
<a href=/people/s/slav-petrov/>Slav Petrov</a>
|
<a href=/people/m/mark-steedman/>Mark Steedman</a>
|
<a href=/people/m/mirella-lapata/>Mirella Lapata</a><br><a href=/volumes/D17-1/ class=text-muted>Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1009><div class="card-body p-3 small">Universal Dependencies (UD) offer a uniform cross-lingual syntactic representation, with the aim of advancing multilingual applications. Recent work shows that <a href=https://en.wikipedia.org/wiki/Semantic_parsing>semantic parsing</a> can be accomplished by transforming syntactic dependencies to logical forms. However, this work is limited to <a href=https://en.wikipedia.org/wiki/English_language>English</a>, and can not process <a href=https://en.wikipedia.org/wiki/Dependency_graph>dependency graphs</a>, which allow handling complex phenomena such as <a href=https://en.wikipedia.org/wiki/Control_flow>control</a>. In this work, we introduce UDepLambda, a semantic interface for UD, which maps <a href=https://en.wikipedia.org/wiki/Natural_language>natural language</a> to logical forms in an almost language-independent fashion and can process dependency graphs. We perform experiments on <a href=https://en.wikipedia.org/wiki/Question_answering>question answering</a> against <a href=https://en.wikipedia.org/wiki/Freebase>Freebase</a> and provide German and Spanish translations of the WebQuestions and GraphQuestions datasets to facilitate multilingual evaluation. Results show that UDepLambda outperforms strong baselines across languages and datasets. For <a href=https://en.wikipedia.org/wiki/English_language>English</a>, <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> achieves a 4.9 F1 point improvement over the <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a> on GraphQuestions.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Mark+Steedman" title="Search for 'Mark Steedman' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/m/milos-stanojevic/ class=align-middle>Miloš Stanojević</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/s/sabine-weber/ class=align-middle>Sabine Weber</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/m/mohammad-javad-hosseini/ class=align-middle>Mohammad Javad Hosseini</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/t/thomas-kober/ class=align-middle>Thomas Kober</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/e/elizabeth-nielsen/ class=align-middle>Elizabeth Nielsen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/s/sharon-goldwater/ class=align-middle>Sharon Goldwater</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/n/nick-mckenna/ class=align-middle>Nick McKenna</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/g/gozde-gul-sahin/ class=align-middle>Gözde Gül Şahin</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/n/nikita-moghe/ class=align-middle>Nikita Moghe</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/alexandra-birch/ class=align-middle>Alexandra Birch</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/i/ida-szubert/ class=align-middle>Ida Szubert</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/tianyi-li/ class=align-middle>Tianyi Li</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/liane-guillou/ class=align-middle>Liane Guillou</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/siva-reddy/ class=align-middle>Siva Reddy</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/o/oscar-tackstrom/ class=align-middle>Oscar Täckström</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/slav-petrov/ class=align-middle>Slav Petrov</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/mirella-lapata/ class=align-middle>Mirella Lapata</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/shohini-bhattasali/ class=align-middle>Shohini Bhattasali</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/donald-dunagan/ class=align-middle>Donald Dunagan</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/luca-campanelli/ class=align-middle>Luca Campanelli</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jonathan-brennan/ class=align-middle>Jonathan Brennan</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/john-hale/ class=align-middle>John Hale</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/sander-bijl-de-vroe/ class=align-middle>Sander Bijl de Vroe</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/malihe-alikhani/ class=align-middle>Malihe Alikhani</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/matthew-stone/ class=align-middle>Matthew Stone</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/shay-b-cohen/ class=align-middle>Shay B. Cohen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/mark-johnson/ class=align-middle>Mark Johnson</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/emnlp/ class=align-middle>EMNLP</a><span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/venues/acl/ class=align-middle>ACL</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/ws/ class=align-middle>WS</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/starsem/ class=align-middle>*SEM</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/cl/ class=align-middle>CL</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-venues aria-expanded=false aria-controls=more-venues>show all...</li><div class="collapse border-top" id=more-venues><li class=list-group-item><a href=/venues/findings/ class=align-middle>Findings</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/cmcl/ class=align-middle>CMCL</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/coling/ class=align-middle>COLING</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/naacl/ class=align-middle>NAACL</a><span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>