<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Makoto Miwa - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Makoto</span> <span class=font-weight-bold>Miwa</span></h2><hr><div class=row><div class=col-lg-9><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wnut-1.38.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wnut-1--38 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wnut-1.38 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.wnut-1.38/>mgsohrab at WNUT 2020 Shared Task-1 : Neural Exhaustive Approach for Entity and Relation Recognition Over Wet Lab Protocols<span class=acl-fixed-case>WNUT</span> 2020 Shared Task-1: Neural Exhaustive Approach for Entity and Relation Recognition Over Wet Lab Protocols</a></strong><br><a href=/people/m/mohammad-golam-sohrab/>Mohammad Golam Sohrab</a>
|
<a href=/people/a/anh-khoa-duong-nguyen/>Anh-Khoa Duong Nguyen</a>
|
<a href=/people/m/makoto-miwa/>Makoto Miwa</a>
|
<a href=/people/h/hiroya-takamura/>Hiroya Takamura</a><br><a href=/volumes/2020.wnut-1/ class=text-muted>Proceedings of the Sixth Workshop on Noisy User-generated Text (W-NUT 2020)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wnut-1--38><div class="card-body p-3 small">We present a neural exhaustive approach that addresses named entity recognition (NER) and relation recognition (RE), for the entity and re- lation recognition over the wet-lab protocols shared task. We introduce BERT-based neural exhaustive approach that enumerates all pos- sible spans as potential entity mentions and classifies them into entity types or no entity with deep neural networks to address NER. To solve relation extraction task, based on the NER predictions or given gold mentions we create all possible trigger-argument pairs and classify them into relation types or no relation. In NER task, we achieved 76.60 % in terms of <a href=https://en.wikipedia.org/wiki/F-score>F-score</a> as third rank system among the partic- ipated systems. In relation extraction task, we achieved 80.46 % in terms of <a href=https://en.wikipedia.org/wiki/F-score>F-score</a> as the top system in the relation extraction or recognition task. Besides we compare our model based on the wet lab protocols corpus (WLPC) with the WLPC baseline and dynamic graph-based in- formation extraction (DyGIE) systems.</div></div><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1381.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1381 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1381 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1381/>A Search-based Neural Model for Biomedical Nested and Overlapping Event Detection</a></strong><br><a href=/people/k/kurt-junshean-espinosa/>Kurt Junshean Espinosa</a>
|
<a href=/people/m/makoto-miwa/>Makoto Miwa</a>
|
<a href=/people/s/sophia-ananiadou/>Sophia Ananiadou</a><br><a href=/volumes/D19-1/ class=text-muted>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1381><div class="card-body p-3 small">We tackle the nested and overlapping event detection task and propose a novel search-based neural network (SBNN) structured prediction model that treats the task as a search problem on a relation graph of trigger-argument structures. Unlike existing structured prediction tasks such as dependency parsing, the task targets to detect DAG structures, which constitute <a href=https://en.wikipedia.org/wiki/Event_(computing)>events</a>, from the relation graph. We define actions to construct events and use all the beams in a <a href=https://en.wikipedia.org/wiki/Beam_search>beam search</a> to detect all event structures that may be overlapping and nested. The search process constructs <a href=https://en.wikipedia.org/wiki/Event_(probability_theory)>events</a> in a bottom-up manner while modelling the global properties for nested and overlapping structures simultaneously using <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a>. We show that the model achieves performance comparable to the state-of-the-art model Turku Event Extraction System (TEES) on the BioNLP Cancer Genetics (CG) Shared Task 2013 without the use of any syntactic and hand-engineered features. Further analyses on the development set show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is more computationally efficient while yielding higher <a href=https://en.wikipedia.org/wiki/F-number>F1-score</a> performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5708.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5708 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5708 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5708/>A Neural Pipeline Approach for the PharmaCoNER Shared Task using Contextual Exhaustive Models<span class=acl-fixed-case>P</span>harma<span class=acl-fixed-case>C</span>o<span class=acl-fixed-case>NER</span> Shared Task using Contextual Exhaustive Models</a></strong><br><a href=/people/m/mohammad-golam-sohrab/>Mohammad Golam Sohrab</a>
|
<a href=/people/m/minh-thang-pham/>Minh Thang Pham</a>
|
<a href=/people/m/makoto-miwa/>Makoto Miwa</a>
|
<a href=/people/h/hiroya-takamura/>Hiroya Takamura</a><br><a href=/volumes/D19-57/ class=text-muted>Proceedings of The 5th Workshop on BioNLP Open Shared Tasks</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5708><div class="card-body p-3 small">We present a neural pipeline approach that performs named entity recognition (NER) and concept indexing (CI), which links them to concept unique identifiers (CUIs) in a knowledge base, for the PharmaCoNER shared task on pharmaceutical drugs and chemical entities. We proposed a neural NER model that captures the surrounding semantic information of a given sequence by capturing the forward- and backward-context of bidirectional LSTM (Bi-LSTM) output of a target span using contextual span representation-based exhaustive approach. The <a href=https://en.wikipedia.org/wiki/NER_model>NER model</a> enumerates all possible spans as potential entity mentions and classify them into entity types or no entity with <a href=https://en.wikipedia.org/wiki/Deep_learning>deep neural networks</a>. For representing span, we compare several different neural network architectures and their <a href=https://en.wikipedia.org/wiki/Network_topology>ensembling</a> for the <a href=https://en.wikipedia.org/wiki/NER_model>NER model</a>. We then perform dictionary matching for CI and, if there is no matching, we further compute similarity scores between a mention and CUIs using entity embeddings to assign the CUI with the highest score to the mention. We evaluate our <a href=https://en.wikipedia.org/wiki/Software_development_process>approach</a> on the two sub-tasks in the shared task. Among the five submitted runs, the best run for each <a href=https://en.wikipedia.org/wiki/Task_(project_management)>sub-task</a> achieved the F-score of 86.76 % on Sub-task 1 (NER) and the F-score of 79.97 % (strict) on Sub-task 2 (CI).</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1309.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1309 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1309 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/306057139 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1309/>Deep Exhaustive Model for Nested Named Entity Recognition</a></strong><br><a href=/people/m/mohammad-golam-sohrab/>Mohammad Golam Sohrab</a>
|
<a href=/people/m/makoto-miwa/>Makoto Miwa</a><br><a href=/volumes/D18-1/ class=text-muted>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1309><div class="card-body p-3 small">We propose a simple <a href=https://en.wikipedia.org/wiki/Deep_learning>deep neural model</a> for nested named entity recognition (NER). Most NER models focused on flat entities and ignored nested entities, which failed to fully capture underlying semantic information in texts. The key idea of our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is to enumerate all possible regions or spans as potential entity mentions and classify them with <a href=https://en.wikipedia.org/wiki/Deep_learning>deep neural networks</a>. To reduce the computational costs and capture the information of the contexts around the regions, the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> represents the regions using the outputs of shared underlying bidirectional long short-term memory. We evaluate our exhaustive <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> on the GENIA and JNLPBA corpora in biomedical domain, and the results show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms state-of-the-art models on nested and flat NER, achieving 77.1 % and 78.4 % respectively in terms of <a href=https://en.wikipedia.org/wiki/F-score>F-score</a>, without any external knowledge resources.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1131.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1131 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1131 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/277349441 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N18-1131/>A Neural Layered Model for Nested Named Entity Recognition</a></strong><br><a href=/people/m/meizhi-ju/>Meizhi Ju</a>
|
<a href=/people/m/makoto-miwa/>Makoto Miwa</a>
|
<a href=/people/s/sophia-ananiadou/>Sophia Ananiadou</a><br><a href=/volumes/N18-1/ class=text-muted>Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1131><div class="card-body p-3 small">Entity mentions embedded in longer entity mentions are referred to as nested entities. Most named entity recognition (NER) systems deal only with the flat entities and ignore the inner nested ones, which fails to capture finer-grained semantic information in underlying texts. To address this issue, we propose a novel neural model to identify nested entities by dynamically stacking flat NER layers. Each flat NER layer is based on the state-of-the-art flat NER model that captures sequential context representation with bidirectional Long Short-Term Memory (LSTM) layer and feeds it to the cascaded CRF layer. Our model merges the output of the LSTM layer in the current flat NER layer to build new representation for detected entities and subsequently feeds them into the next flat NER layer. This allows our <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> to extract outer entities by taking full advantage of information encoded in their corresponding inner entities, in an inside-to-outside way. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> dynamically stacks the flat NER layers until no outer entities are extracted. Extensive evaluation shows that our <a href=https://en.wikipedia.org/wiki/Dynamical_system>dynamic model</a> outperforms state-of-the-art feature-based systems on nested NER, achieving 74.7 % and 72.2 % on GENIA and ACE2005 datasets, respectively, in terms of <a href=https://en.wikipedia.org/wiki/F-score>F-score</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2014.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2014 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2014 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2014.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-2014" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P18-2014/>A Walk-based Model on <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>Entity Graphs</a> for Relation Extraction</a></strong><br><a href=/people/f/fenia-christopoulou/>Fenia Christopoulou</a>
|
<a href=/people/m/makoto-miwa/>Makoto Miwa</a>
|
<a href=/people/s/sophia-ananiadou/>Sophia Ananiadou</a><br><a href=/volumes/P18-2/ class=text-muted>Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2014><div class="card-body p-3 small">We present a novel graph-based neural network model for <a href=https://en.wikipedia.org/wiki/Relation_extraction>relation extraction</a>. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> treats multiple pairs in a sentence simultaneously and considers interactions among them. All the entities in a sentence are placed as <a href=https://en.wikipedia.org/wiki/Vertex_(graph_theory)>nodes</a> in a fully-connected graph structure. The <a href=https://en.wikipedia.org/wiki/Glossary_of_graph_theory_terms>edges</a> are represented with <a href=https://en.wikipedia.org/wiki/Glossary_of_graph_theory_terms>position-aware contexts</a> around the entity pairs. In order to consider different <a href=https://en.wikipedia.org/wiki/Path_(graph_theory)>relation paths</a> between two entities, we construct up to l-length walks between each pair. The resulting <a href=https://en.wikipedia.org/wiki/Walk_(graph_theory)>walks</a> are merged and iteratively used to update the edge representations into longer walks representations. We show that the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieves performance comparable to the state-of-the-art systems on the ACE 2005 dataset without using any external tools.<tex-math>l</tex-math>-length walks between each pair. The resulting walks are merged and iteratively used to update the edge representations into longer walks representations. We show that the model achieves performance comparable to the state-of-the-art systems on the ACE 2005 dataset without using any external tools.</div></div><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-2005.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-2005 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-2005 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-2005/>Analyzing Well-Formedness of Syllables in <a href=https://en.wikipedia.org/wiki/Japanese_Sign_Language>Japanese Sign Language</a><span class=acl-fixed-case>J</span>apanese <span class=acl-fixed-case>S</span>ign <span class=acl-fixed-case>L</span>anguage</a></strong><br><a href=/people/s/satoshi-yawata/>Satoshi Yawata</a>
|
<a href=/people/m/makoto-miwa/>Makoto Miwa</a>
|
<a href=/people/y/yutaka-sasaki/>Yutaka Sasaki</a>
|
<a href=/people/d/daisuke-hara/>Daisuke Hara</a><br><a href=/volumes/I17-2/ class=text-muted>Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-2005><div class="card-body p-3 small">This paper tackles a problem of analyzing the well-formedness of syllables in <a href=https://en.wikipedia.org/wiki/Japanese_Sign_Language>Japanese Sign Language (JSL)</a>. We formulate the <a href=https://en.wikipedia.org/wiki/Problem_solving>problem</a> as a classification problem that classifies syllables into well-formed or ill-formed. We build a <a href=https://en.wikipedia.org/wiki/Data_set>data set</a> that contains hand-coded syllables and their <a href=https://en.wikipedia.org/wiki/Well-formedness>well-formedness</a>. We define a fine-grained feature set based on the hand-coded syllables and train a logistic regression classifier on labeled syllables, expecting to find the discriminative features from the trained <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifier</a>. We also perform pseudo active learning to investigate the applicability of <a href=https://en.wikipedia.org/wiki/Active_learning>active learning</a> in analyzing syllables. In the experiments, the best <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifier</a> with our combinatorial features achieved the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of 87.0 %. The pseudo active learning is also shown to be effective showing that <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> could reduce about 84 % of training instances to achieve the accuracy of 82.0 % when compared to the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> without <a href=https://en.wikipedia.org/wiki/Active_learning_(machine_learning)>active learning</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-2064.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-2064 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-2064 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-2064/>Utilizing Visual Forms of Japanese Characters for Neural Review Classification<span class=acl-fixed-case>J</span>apanese Characters for Neural Review Classification</a></strong><br><a href=/people/y/yota-toyama/>Yota Toyama</a>
|
<a href=/people/m/makoto-miwa/>Makoto Miwa</a>
|
<a href=/people/y/yutaka-sasaki/>Yutaka Sasaki</a><br><a href=/volumes/I17-2/ class=text-muted>Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-2064><div class="card-body p-3 small">We propose a novel method that exploits visual information of ideograms and logograms in analyzing Japanese review documents. Our method first converts font images of Japanese characters into <a href=https://en.wikipedia.org/wiki/Character_encoding>character embeddings</a> using <a href=https://en.wikipedia.org/wiki/Convolutional_neural_network>convolutional neural networks</a>. It then constructs document embeddings from the character embeddings based on Hierarchical Attention Networks, which represent the documents based on attention mechanisms from a character level to a sentence level. The document embeddings are finally used to predict the labels of documents. Our method provides a way to exploit visual features of characters in languages with <a href=https://en.wikipedia.org/wiki/Ideogram>ideograms</a> and <a href=https://en.wikipedia.org/wiki/Logogram>logograms</a>. In the experiments, our method achieved an <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> comparable to a character embedding-based model while our method has much fewer parameters since it does not need to keep embeddings of thousands of characters.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2302.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2302 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2302 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-2302/>Extracting Drug-Drug Interactions with Attention CNNs<span class=acl-fixed-case>CNN</span>s</a></strong><br><a href=/people/m/masaki-asada/>Masaki Asada</a>
|
<a href=/people/m/makoto-miwa/>Makoto Miwa</a>
|
<a href=/people/y/yutaka-sasaki/>Yutaka Sasaki</a><br><a href=/volumes/W17-23/ class=text-muted>BioNLP 2017</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2302><div class="card-body p-3 small">We propose a novel attention mechanism for a Convolutional Neural Network (CNN)-based Drug-Drug Interaction (DDI) extraction model. CNNs have been shown to have a great potential on DDI extraction tasks ; however, <a href=https://en.wikipedia.org/wiki/Attentional_control>attention mechanisms</a>, which emphasize important words in the sentence of a target-entity pair, have not been investigated with the CNNs despite the fact that <a href=https://en.wikipedia.org/wiki/Attentional_control>attention mechanisms</a> are shown to be effective for a general domain relation classification task. We evaluated our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> on the Task 9.2 of the DDIExtraction-2013 shared task. As a result, our attention mechanism improved the performance of our base CNN-based DDI model, and the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieved an F-score of 69.12 %, which is competitive with the state-of-the-art models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S17-2172.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S17-2172 data-toggle=collapse aria-expanded=false aria-controls=abstract-S17-2172 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/S17-2172/>TTI-COIN at SemEval-2017 Task 10 : Investigating Embeddings for End-to-End Relation Extraction from Scientific Papers<span class=acl-fixed-case>TTI</span>-<span class=acl-fixed-case>COIN</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2017 Task 10: Investigating Embeddings for End-to-End Relation Extraction from Scientific Papers</a></strong><br><a href=/people/t/tomoki-tsujimura/>Tomoki Tsujimura</a>
|
<a href=/people/m/makoto-miwa/>Makoto Miwa</a>
|
<a href=/people/y/yutaka-sasaki/>Yutaka Sasaki</a><br><a href=/volumes/S17-2/ class=text-muted>Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S17-2172><div class="card-body p-3 small">This paper describes our TTI-COIN system that participated in SemEval-2017 Task 10. We investigated appropriate <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a> to adapt a neural end-to-end entity and relation extraction system LSTM-ER to this task. We participated in the full task setting of the <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entity segmentation</a>, entity classification and relation classification (scenario 1) and the setting of <a href=https://en.wikipedia.org/wiki/Relation_(database)>relation classification</a> only (scenario 3). The <a href=https://en.wikipedia.org/wiki/System>system</a> was directly applied to the scenario 1 without modifying the codes thanks to its generality and flexibility. Our evaluation results show that the choice of appropriate pre-trained embeddings affected the performance significantly. With the best <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a>, our <a href=https://en.wikipedia.org/wiki/System>system</a> was ranked third in the scenario 1 with the micro F1 score of 0.38. We also confirm that our <a href=https://en.wikipedia.org/wiki/System>system</a> can produce the micro F1 score of 0.48 for the scenario 3 on the test data, and this score is close to the score of the 3rd ranked system in the task.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Makoto+Miwa" title="Search for 'Makoto Miwa' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/y/yutaka-sasaki/ class=align-middle>Yutaka Sasaki</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/people/m/mohammad-golam-sohrab/ class=align-middle>Mohammad Golam Sohrab</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/s/sophia-ananiadou/ class=align-middle>Sophia Ananiadou</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/h/hiroya-takamura/ class=align-middle>Hiroya Takamura</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/s/satoshi-yawata/ class=align-middle>Satoshi Yawata</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/d/daisuke-hara/ class=align-middle>Daisuke Hara</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yota-toyama/ class=align-middle>Yota Toyama</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/masaki-asada/ class=align-middle>Masaki Asada</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/kurt-junshean-espinosa/ class=align-middle>Kurt Junshean Espinosa</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/minh-thang-pham/ class=align-middle>Minh Thang Pham</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/tomoki-tsujimura/ class=align-middle>Tomoki Tsujimura</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/meizhi-ju/ class=align-middle>Meizhi Ju</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/anh-khoa-duong-nguyen/ class=align-middle>Anh-Khoa Duong Nguyen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/f/fenia-christopoulou/ class=align-middle>Fenia Christopoulou</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/emnlp/ class=align-middle>EMNLP</a><span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/venues/ijcnlp/ class=align-middle>IJCNLP</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/ws/ class=align-middle>WS</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/semeval/ class=align-middle>SemEval</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/naacl/ class=align-middle>NAACL</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-venues aria-expanded=false aria-controls=more-venues>show all...</li><div class="collapse border-top" id=more-venues><li class=list-group-item><a href=/venues/wnut/ class=align-middle>WNUT</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/acl/ class=align-middle>ACL</a><span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>