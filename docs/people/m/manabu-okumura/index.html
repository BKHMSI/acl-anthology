<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Manabu Okumura - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Manabu</span> <span class=font-weight-bold>Okumura</span></h2><hr><div class=row><div class=col-lg-9><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.ranlp-1.31.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--ranlp-1--31 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.ranlp-1.31 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.ranlp-1.31" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.ranlp-1.31/>Character-based Thai Word Segmentation with Multiple Attentions<span class=acl-fixed-case>T</span>hai Word Segmentation with Multiple Attentions</a></strong><br><a href=/people/t/thodsaporn-chay-intr/>Thodsaporn Chay-intr</a>
|
<a href=/people/h/hidetaka-kamigaito/>Hidetaka Kamigaito</a>
|
<a href=/people/m/manabu-okumura/>Manabu Okumura</a><br><a href=/volumes/2021.ranlp-1/ class=text-muted>Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2021)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--ranlp-1--31><div class="card-body p-3 small">Character-based word-segmentation models have been extensively applied to <a href=https://en.wikipedia.org/wiki/Agglutinative_language>agglutinative languages</a>, including <a href=https://en.wikipedia.org/wiki/Thai_language>Thai</a>, due to their high performance. These <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> estimate <a href=https://en.wikipedia.org/wiki/Word_(computer_architecture)>word boundaries</a> from a <a href=https://en.wikipedia.org/wiki/Character_(computing)>character sequence</a>. However, a <a href=https://en.wikipedia.org/wiki/Character_(computing)>character unit</a> in sequences has no essential meaning, compared with word, subword, and character cluster units. We propose a Thai word-segmentation model that uses various types of information, including <a href=https://en.wikipedia.org/wiki/Word>words</a>, <a href=https://en.wikipedia.org/wiki/Subword>subwords</a>, and character clusters, from a character sequence. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> applies multiple attentions to refine segmentation inferences by estimating the significant relationships among characters and various unit types. The experimental results indicate that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> can outperform other state-of-the-art Thai word-segmentation models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.ranlp-1.88.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--ranlp-1--88 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.ranlp-1.88 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.ranlp-1.88/>Making Your Tweets More Fancy : Emoji Insertion to Texts</a></strong><br><a href=/people/j/jingun-kwon/>Jingun Kwon</a>
|
<a href=/people/n/naoki-kobayashi/>Naoki Kobayashi</a>
|
<a href=/people/h/hidetaka-kamigaito/>Hidetaka Kamigaito</a>
|
<a href=/people/h/hiroya-takamura/>Hiroya Takamura</a>
|
<a href=/people/m/manabu-okumura/>Manabu Okumura</a><br><a href=/volumes/2021.ranlp-1/ class=text-muted>Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2021)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--ranlp-1--88><div class="card-body p-3 small">In the <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>, users frequently use small images called <a href=https://en.wikipedia.org/wiki/Emoji>emojis</a> in their posts. Although using <a href=https://en.wikipedia.org/wiki/Emoji>emojis</a> in texts plays a key role in recent communication systems, less attention has been paid on their positions in the given texts, despite that users carefully choose and put an <a href=https://en.wikipedia.org/wiki/Emoji>emoji</a> that matches their post. Exploring positions of <a href=https://en.wikipedia.org/wiki/Emoji>emojis</a> in texts will enhance understanding of the relationship between <a href=https://en.wikipedia.org/wiki/Emoji>emojis</a> and texts. We extend an emoji label prediction task taking into account the information of emoji positions, by jointly learning the emoji position in a tweet to predict the emoji label. The results demonstrate that the position of <a href=https://en.wikipedia.org/wiki/Emoji>emojis</a> in texts is a good clue to boost the performance of emoji label prediction. Human evaluation validates that there exists a suitable emoji position in a tweet, and our proposed task is able to make tweets more fancy and natural. In addition, considering emoji position can further improve the performance for the irony detection task compared to the emoji label prediction. We also report the experimental results for the modified <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>, due to the problem of the original <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> for the first shared task to predict an <a href=https://en.wikipedia.org/wiki/Emoji>emoji label</a> in SemEval2018.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.ranlp-1.180.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--ranlp-1--180 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.ranlp-1.180 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.ranlp-1.180/>Generic Mechanism for Reducing Repetitions in Encoder-Decoder Models</a></strong><br><a href=/people/y/ying-zhang/>Ying Zhang</a>
|
<a href=/people/h/hidetaka-kamigaito/>Hidetaka Kamigaito</a>
|
<a href=/people/t/tatsuya-aoki/>Tatsuya Aoki</a>
|
<a href=/people/h/hiroya-takamura/>Hiroya Takamura</a>
|
<a href=/people/m/manabu-okumura/>Manabu Okumura</a><br><a href=/volumes/2021.ranlp-1/ class=text-muted>Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2021)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--ranlp-1--180><div class="card-body p-3 small">Encoder-decoder models have been commonly used for many <a href=https://en.wikipedia.org/wiki/Task_(computing)>tasks</a> such as <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> and response generation. As previous research reported, these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> suffer from generating redundant repetition. In this research, we propose a new mechanism for encoder-decoder models that estimates the semantic difference of a source sentence before and after being fed into the encoder-decoder model to capture the consistency between two sides. This <a href=https://en.wikipedia.org/wiki/Mechanism_(engineering)>mechanism</a> helps reduce repeatedly generated tokens for a variety of <a href=https://en.wikipedia.org/wiki/Task_(computing)>tasks</a>. Evaluation results on publicly available machine translation and response generation datasets demonstrate the effectiveness of our proposal.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.125.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--125 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.125 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.eacl-main.125" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.125/>Generating Weather Comments from Meteorological Simulations</a></strong><br><a href=/people/s/soichiro-murakami/>Soichiro Murakami</a>
|
<a href=/people/s/sora-tanaka/>Sora Tanaka</a>
|
<a href=/people/m/masatsugu-hangyo/>Masatsugu Hangyo</a>
|
<a href=/people/h/hidetaka-kamigaito/>Hidetaka Kamigaito</a>
|
<a href=/people/k/kotaro-funakoshi/>Kotaro Funakoshi</a>
|
<a href=/people/h/hiroya-takamura/>Hiroya Takamura</a>
|
<a href=/people/m/manabu-okumura/>Manabu Okumura</a><br><a href=/volumes/2021.eacl-main/ class=text-muted>Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--125><div class="card-body p-3 small">The task of generating weather-forecast comments from meteorological simulations has the following requirements : (i) the changes in numerical values for various physical quantities need to be considered, (ii) the weather comments should be dependent on delivery time and area information, and (iii) the comments should provide useful information for users. To meet these requirements, we propose a data-to-text model that incorporates three types of encoders for numerical forecast maps, observation data, and meta-data. We also introduce weather labels representing <a href=https://en.wikipedia.org/wiki/Weather_forecasting>weather information</a>, such as sunny and rain, for our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> to explicitly describe useful information. We conducted automatic and human evaluations. The results indicate that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> performed best against <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a> in terms of informativeness. We make our code and data publicly available.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.296.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--296 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.296 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.296/>One-class Text Classification with Multi-modal Deep Support Vector Data Description</a></strong><br><a href=/people/c/chenlong-hu/>Chenlong Hu</a>
|
<a href=/people/y/yukun-feng/>Yukun Feng</a>
|
<a href=/people/h/hidetaka-kamigaito/>Hidetaka Kamigaito</a>
|
<a href=/people/h/hiroya-takamura/>Hiroya Takamura</a>
|
<a href=/people/m/manabu-okumura/>Manabu Okumura</a><br><a href=/volumes/2021.eacl-main/ class=text-muted>Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--296><div class="card-body p-3 small">This work presents multi-modal deep SVDD (mSVDD) for one-class text classification. By extending the uni-modal SVDD to a multiple modal one, we build mSVDD with multiple hyperspheres, that enable us to build a much better description for target one-class data. Additionally, the end-to-end architecture of mSVDD can jointly handle neural feature learning and one-class text learning. We also introduce a mechanism for incorporating negative supervision in the absence of real negative data, which can be beneficial to the mSVDD model. We conduct experiments on Reuters and 20 Newsgroup datasets, and the experimental results demonstrate that mSVDD outperforms uni-modal SVDD and mSVDD can get further improvements when negative supervision is incorporated.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-demos.27.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-demos--27 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-demos.27 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.eacl-demos.27.OptionalSupplementaryMaterial.pdf data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2021.eacl-demos.27/>A New Surprise Measure for Extracting Interesting Relationships between Persons</a></strong><br><a href=/people/h/hidetaka-kamigaito/>Hidetaka Kamigaito</a>
|
<a href=/people/j/jingun-kwon/>Jingun Kwon</a>
|
<a href=/people/y/young-in-song/>Young-In Song</a>
|
<a href=/people/m/manabu-okumura/>Manabu Okumura</a><br><a href=/volumes/2021.eacl-demos/ class=text-muted>Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: System Demonstrations</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-demos--27><div class="card-body p-3 small">One way to enhance user engagement in <a href=https://en.wikipedia.org/wiki/Web_search_engine>search engines</a> is to suggest interesting facts to the user. Although relationships between persons are important as a target for <a href=https://en.wikipedia.org/wiki/Text_mining>text mining</a>, there are few effective approaches for extracting the interesting relationships between persons. We therefore propose a method for extracting interesting relationships between persons from natural language texts by focusing on their <a href=https://en.wikipedia.org/wiki/Surprise_(emotion)>surprisingness</a>. Our method first extracts all personal relationships from dependency trees for the texts and then calculates surprise scores for distributed representations of the extracted relationships in an unsupervised manner. The unique point of our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> is that it does not require any labeled dataset with <a href=https://en.wikipedia.org/wiki/Annotation>annotation</a> for the surprising personal relationships. The results of the human evaluation show that the proposed <a href=https://en.wikipedia.org/wiki/Methodology>method</a> could extract more interesting relationships between persons from <a href=https://en.wikipedia.org/wiki/Japanese_Wikipedia>Japanese Wikipedia articles</a> than a popularity-based baseline method. We demonstrate our proposed <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>method</a> as a chrome plugin on <a href=https://en.wikipedia.org/wiki/Google_Search>google search</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.socialnlp-1.3.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--socialnlp-1--3 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.socialnlp-1.3 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.socialnlp-1.3/>A Case Study of In-House Competition for Ranking Constructive Comments in a News Service</a></strong><br><a href=/people/h/hayato-kobayashi/>Hayato Kobayashi</a>
|
<a href=/people/h/hiroaki-taguchi/>Hiroaki Taguchi</a>
|
<a href=/people/y/yoshimune-tabuchi/>Yoshimune Tabuchi</a>
|
<a href=/people/c/chahine-koleejan/>Chahine Koleejan</a>
|
<a href=/people/k/ken-kobayashi/>Ken Kobayashi</a>
|
<a href=/people/s/soichiro-fujita/>Soichiro Fujita</a>
|
<a href=/people/k/kazuma-murao/>Kazuma Murao</a>
|
<a href=/people/t/takeshi-masuyama/>Takeshi Masuyama</a>
|
<a href=/people/t/taichi-yatsuka/>Taichi Yatsuka</a>
|
<a href=/people/m/manabu-okumura/>Manabu Okumura</a>
|
<a href=/people/s/satoshi-sekine/>Satoshi Sekine</a><br><a href=/volumes/2021.socialnlp-1/ class=text-muted>Proceedings of the Ninth International Workshop on Natural Language Processing for Social Media</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--socialnlp-1--3><div class="card-body p-3 small">Ranking the user comments posted on a news article is important for online news services because comment visibility directly affects the user experience. Research on ranking comments with different <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> to measure the comment quality has shown constructiveness used in argument analysis is promising from a practical standpoint. In this paper, we report a case study in which this <a href=https://en.wikipedia.org/wiki/Constructivism_(philosophy_of_education)>constructiveness</a> is examined in the real world. Specifically, we examine an in-house competition to improve the performance of ranking constructive comments and demonstrate the effectiveness of the best obtained <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> for a <a href=https://en.wikipedia.org/wiki/Service_(economics)>commercial service</a>.</div></div><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.464.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--464 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.464 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.464/>Diverse and Non-redundant Answer Set Extraction on Community QA based on DPPs<span class=acl-fixed-case>QA</span> based on <span class=acl-fixed-case>DPP</span>s</a></strong><br><a href=/people/s/shogo-fujita/>Shogo Fujita</a>
|
<a href=/people/t/tomohide-shibata/>Tomohide Shibata</a>
|
<a href=/people/m/manabu-okumura/>Manabu Okumura</a><br><a href=/volumes/2020.coling-main/ class=text-muted>Proceedings of the 28th International Conference on Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--464><div class="card-body p-3 small">In community-based question answering (CQA) platforms, it takes time for a user to get useful information from among many answers. Although one solution is an answer ranking method, the user still needs to read through the top-ranked answers carefully. This paper proposes a new <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> of selecting a diverse and non-redundant answer set rather than ranking the answers. Our method is based on determinantal point processes (DPPs), and it calculates the answer importance and similarity between answers by using BERT. We built a <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> focusing on a Japanese CQA site, and the experiments on this <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> demonstrated that the proposed method outperformed several baseline methods.</div></div><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1587.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1587 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1587 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1587/>Split or Merge : Which is Better for Unsupervised RST Parsing?<span class=acl-fixed-case>RST</span> Parsing?</a></strong><br><a href=/people/n/naoki-kobayashi/>Naoki Kobayashi</a>
|
<a href=/people/t/tsutomu-hirao/>Tsutomu Hirao</a>
|
<a href=/people/k/kengo-nakamura/>Kengo Nakamura</a>
|
<a href=/people/h/hidetaka-kamigaito/>Hidetaka Kamigaito</a>
|
<a href=/people/m/manabu-okumura/>Manabu Okumura</a>
|
<a href=/people/m/masaaki-nagata/>Masaaki Nagata</a><br><a href=/volumes/D19-1/ class=text-muted>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1587><div class="card-body p-3 small">Rhetorical Structure Theory (RST) parsing is crucial for many downstream NLP tasks that require a discourse structure for a text. Most of the previous RST parsers have been based on supervised learning approaches. That is, they require an annotated corpus of sufficient size and quality, and heavily rely on the language and domain dependent corpus. In this paper, we present two language-independent unsupervised RST parsing methods based on <a href=https://en.wikipedia.org/wiki/Dynamic_programming>dynamic programming</a>. The first one builds the optimal tree in terms of a dissimilarity score function that is defined for splitting a text span into smaller ones. The second builds the optimal tree in terms of a similarity score function that is defined for merging two adjacent spans into a large one. Experimental results on English and German RST treebanks showed that our <a href=https://en.wikipedia.org/wiki/Parsing>parser</a> based on span merging achieved the best score, around 0.8 F_1 score, which is close to the scores of the previous supervised parsers.<tex-math>_1</tex-math> score, which is close to the scores of the previous supervised parsers.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-8641.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-8641 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-8641 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-8641/>A Large-Scale Multi-Length Headline Corpus for Analyzing Length-Constrained Headline Generation Model Evaluation</a></strong><br><a href=/people/y/yuta-hitomi/>Yuta Hitomi</a>
|
<a href=/people/y/yuya-taguchi/>Yuya Taguchi</a>
|
<a href=/people/h/hideaki-tamori/>Hideaki Tamori</a>
|
<a href=/people/k/ko-kikuta/>Ko Kikuta</a>
|
<a href=/people/j/jiro-nishitoba/>Jiro Nishitoba</a>
|
<a href=/people/n/naoaki-okazaki/>Naoaki Okazaki</a>
|
<a href=/people/k/kentaro-inui/>Kentaro Inui</a>
|
<a href=/people/m/manabu-okumura/>Manabu Okumura</a><br><a href=/volumes/W19-86/ class=text-muted>Proceedings of the 12th International Conference on Natural Language Generation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-8641><div class="card-body p-3 small">Browsing news articles on multiple devices is now possible. The lengths of news article headlines have precise upper bounds, dictated by the size of the display of the relevant device or interface. Therefore, controlling the length of headlines is essential when applying the task of headline generation to <a href=https://en.wikipedia.org/wiki/News_media>news production</a>. However, because there is no corpus of headlines of multiple lengths for a given article, previous research on controlling output length in headline generation has not discussed whether the system outputs could be adequately evaluated without multiple references of different lengths. In this paper, we introduce two <a href=https://en.wikipedia.org/wiki/Text_corpus>corpora</a>, which are Japanese News Corpus (JNC) and JApanese MUlti-Length Headline Corpus (JAMUL), to confirm the validity of previous evaluation settings. The JNC provides common supervision data for headline generation. The JAMUL is a large-scale evaluation dataset for <a href=https://en.wikipedia.org/wiki/Headline>headlines</a> of three different lengths composed by professional editors. We report new findings on these corpora ; for example, although the longest length reference summary can appropriately evaluate the existing methods controlling output length, this evaluation setting has several problems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1099.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1099 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1099 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/384475549 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1099/>Global Optimization under Length Constraint for Neural Text Summarization</a></strong><br><a href=/people/t/takuya-makino/>Takuya Makino</a>
|
<a href=/people/t/tomoya-iwakura/>Tomoya Iwakura</a>
|
<a href=/people/h/hiroya-takamura/>Hiroya Takamura</a>
|
<a href=/people/m/manabu-okumura/>Manabu Okumura</a><br><a href=/volumes/P19-1/ class=text-muted>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1099><div class="card-body p-3 small">We propose a global optimization method under length constraint (GOLC) for neural text summarization models. GOLC increases the probabilities of generating summaries that have high evaluation scores, ROUGE in this paper, within a desired length. We compared GOLC with two optimization methods, a maximum log-likelihood and a minimum risk training, on CNN / Daily Mail and a Japanese single document summarization data set of The Mainichi Shimbun Newspapers. The experimental results show that a state-of-the-art neural summarization model optimized with GOLC generates fewer overlength summaries while maintaining the fastest <a href=https://en.wikipedia.org/wiki/Time_complexity>processing speed</a> ; only 6.70 % overlength summaries on CNN / Daily and 7.8 % on long summary of Mainichi, compared to the approximately 20 % to 50 % on CNN / Daily Mail and 10 % to 30 % on Mainichi with the other optimization methods. We also demonstrate the importance of the generation of in-length summaries for <a href=https://en.wikipedia.org/wiki/Post-editing>post-editing</a> with the dataset Mainich that is created with strict length constraints. The ex- perimental results show approximately 30 % to 40 % improved post-editing time by use of in-length summaries.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K19-1086.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K19-1086 data-toggle=collapse aria-expanded=false aria-controls=abstract-K19-1086 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/K19-1086/>A Simple and Effective Method for Injecting Word-Level Information into Character-Aware Neural Language Models</a></strong><br><a href=/people/y/yukun-feng/>Yukun Feng</a>
|
<a href=/people/h/hidetaka-kamigaito/>Hidetaka Kamigaito</a>
|
<a href=/people/h/hiroya-takamura/>Hiroya Takamura</a>
|
<a href=/people/m/manabu-okumura/>Manabu Okumura</a><br><a href=/volumes/K19-1/ class=text-muted>Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K19-1086><div class="card-body p-3 small">We propose a simple and effective method to inject word-level information into character-aware neural language models. Unlike previous approaches which usually inject word-level information at the input of a long short-term memory (LSTM) network, we inject it into the <a href=https://en.wikipedia.org/wiki/Softmax_function>softmax function</a>. The resultant <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> can be seen as a combination of character-aware language model and simple word-level language model. Our <a href=https://en.wikipedia.org/wiki/Injection_(medicine)>injection method</a> can also be used together with previous methods. Through the experiments on 14 typologically diverse languages, we empirically show that our injection method, when used together with the previous methods, works better than the previous methods, including a gating mechanism, averaging, and concatenation of word vectors. We also provide a comprehensive comparison of these <a href=https://en.wikipedia.org/wiki/Injection_(medicine)>injection methods</a>.</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1274.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1274 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1274 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1274/>Neural Machine Translation Incorporating Named Entity</a></strong><br><a href=/people/a/arata-ugawa/>Arata Ugawa</a>
|
<a href=/people/a/akihiro-tamura/>Akihiro Tamura</a>
|
<a href=/people/t/takashi-ninomiya/>Takashi Ninomiya</a>
|
<a href=/people/h/hiroya-takamura/>Hiroya Takamura</a>
|
<a href=/people/m/manabu-okumura/>Manabu Okumura</a><br><a href=/volumes/C18-1/ class=text-muted>Proceedings of the 27th International Conference on Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1274><div class="card-body p-3 small">This study proposes a new neural machine translation (NMT) model based on the encoder-decoder model that incorporates named entity (NE) tags of source-language sentences. Conventional NMT models have two problems enumerated as follows : (i) they tend to have difficulty in translating words with multiple meanings because of the high ambiguity, and (ii) these models&#8217;abilitytotranslatecompoundwordsseemschallengingbecausetheencoderreceivesaword, a part of the compound word, at each time step. To alleviate these problems, the <a href=https://en.wikipedia.org/wiki/Encoder>encoder</a> of the proposed model encodes the input word on the basis of its NE tag at each time step, which could reduce the ambiguity of the input word. Furthermore, the encoder introduces a chunk-level LSTM layer over a word-level LSTM layer and hierarchically encodes a source-language sentence to capture a compound NE as a chunk on the basis of the NE tags. We evaluate the proposed model on an English-to-Japanese translation task with the ASPEC, and English-to-Bulgarian and English-to-Romanian translation tasks with the Europarl corpus. The evaluation results show that the proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieves up to 3.11 point improvement in <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6510.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6510 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6510 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6510/>Stylistically User-Specific Generation</a></strong><br><a href=/people/a/abdurrisyad-fikri/>Abdurrisyad Fikri</a>
|
<a href=/people/h/hiroya-takamura/>Hiroya Takamura</a>
|
<a href=/people/m/manabu-okumura/>Manabu Okumura</a><br><a href=/volumes/W18-65/ class=text-muted>Proceedings of the 11th International Conference on Natural Language Generation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6510><div class="card-body p-3 small">Recent neural models for response generation show good results in terms of general responses. In real conversations, however, depending on the speaker / responder, similar utterances should require different responses. In this study, we attempt to consider individual user&#8217;s information in adjusting the notable sequence-to-sequence (seq2seq) model for more diverse, user-specific responses. We assume that we need user-specific features to adjust the response and we argue that some selected representative words from the users are suitable for this task. Furthermore, we prove that even for unseen or unknown users, our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> can provide more diverse and interesting responses, while maintaining correlation with input utterances. Experimental results with human evaluation show that our model can generate more interesting responses than the popular seq2seqmodel and achieve higher relevance with input utterances than our baseline.</div></div><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-2002.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-2002 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-2002 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-2002/>Supervised Attention for Sequence-to-Sequence Constituency Parsing</a></strong><br><a href=/people/h/hidetaka-kamigaito/>Hidetaka Kamigaito</a>
|
<a href=/people/k/katsuhiko-hayashi/>Katsuhiko Hayashi</a>
|
<a href=/people/t/tsutomu-hirao/>Tsutomu Hirao</a>
|
<a href=/people/h/hiroya-takamura/>Hiroya Takamura</a>
|
<a href=/people/m/manabu-okumura/>Manabu Okumura</a>
|
<a href=/people/m/masaaki-nagata/>Masaaki Nagata</a><br><a href=/volumes/I17-2/ class=text-muted>Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-2002><div class="card-body p-3 small">The sequence-to-sequence (Seq2Seq) model has been successfully applied to <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation (MT)</a>. Recently, MT performances were improved by incorporating supervised attention into the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>. In this paper, we introduce supervised attention to constituency parsing that can be regarded as another translation task. Evaluation results on the PTB corpus showed that the bracketing F-measure was improved by supervised attention.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2044.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2044 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2044 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-2044/>Japanese Sentence Compression with a Large Training Dataset<span class=acl-fixed-case>J</span>apanese Sentence Compression with a Large Training Dataset</a></strong><br><a href=/people/s/shun-hasegawa/>Shun Hasegawa</a>
|
<a href=/people/y/yuta-kikuchi/>Yuta Kikuchi</a>
|
<a href=/people/h/hiroya-takamura/>Hiroya Takamura</a>
|
<a href=/people/m/manabu-okumura/>Manabu Okumura</a><br><a href=/volumes/P17-2/ class=text-muted>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2044><div class="card-body p-3 small">In <a href=https://en.wikipedia.org/wiki/English_language>English</a>, high-quality sentence compression models by deleting words have been trained on automatically created large training datasets. We work on Japanese sentence compression by a similar approach. To create a large Japanese training dataset, a method of creating English training dataset is modified based on the characteristics of the <a href=https://en.wikipedia.org/wiki/Japanese_language>Japanese language</a>. The created <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> is used to train Japanese sentence compression models based on the <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent neural network</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1246.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1246 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1246 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1246/>Distinguishing Japanese Non-standard Usages from Standard Ones<span class=acl-fixed-case>J</span>apanese Non-standard Usages from Standard Ones</a></strong><br><a href=/people/t/tatsuya-aoki/>Tatsuya Aoki</a>
|
<a href=/people/r/ryohei-sasano/>Ryohei Sasano</a>
|
<a href=/people/h/hiroya-takamura/>Hiroya Takamura</a>
|
<a href=/people/m/manabu-okumura/>Manabu Okumura</a><br><a href=/volumes/D17-1/ class=text-muted>Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1246><div class="card-body p-3 small">We focus on non-standard usages of common words on <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>. In the context of <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>, words sometimes have other usages that are totally different from their original. In this study, we attempt to distinguish non-standard usages on <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> from standard ones in an unsupervised manner. Our basic idea is that non-standardness can be measured by the inconsistency between the expected meaning of the target word and the given context. For this purpose, we use context embeddings derived from <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a>. Our experimental results show that the model leveraging the context embedding outperforms other methods and provide us with findings, for example, on how to construct context embeddings and which corpus to use.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Manabu+Okumura" title="Search for 'Manabu Okumura' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/h/hiroya-takamura/ class=align-middle>Hiroya Takamura</a>
<span class="badge badge-secondary align-middle ml-2">11</span></li><li class=list-group-item><a href=/people/h/hidetaka-kamigaito/ class=align-middle>Hidetaka Kamigaito</a>
<span class="badge badge-secondary align-middle ml-2">9</span></li><li class=list-group-item><a href=/people/j/jingun-kwon/ class=align-middle>Jingun Kwon</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/n/naoki-kobayashi/ class=align-middle>Naoki Kobayashi</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/t/tatsuya-aoki/ class=align-middle>Tatsuya Aoki</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/t/tsutomu-hirao/ class=align-middle>Tsutomu Hirao</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/m/masaaki-nagata/ class=align-middle>Masaaki Nagata</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/y/yukun-feng/ class=align-middle>Yukun Feng</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/t/thodsaporn-chay-intr/ class=align-middle>Thodsaporn Chay-intr</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/ying-zhang/ class=align-middle>Ying Zhang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/arata-ugawa/ class=align-middle>Arata Ugawa</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/akihiro-tamura/ class=align-middle>Akihiro Tamura</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/takashi-ninomiya/ class=align-middle>Takashi Ninomiya</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/katsuhiko-hayashi/ class=align-middle>Katsuhiko Hayashi</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/shun-hasegawa/ class=align-middle>Shun Hasegawa</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yuta-kikuchi/ class=align-middle>Yuta Kikuchi</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/soichiro-murakami/ class=align-middle>Soichiro Murakami</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/sora-tanaka/ class=align-middle>Sora Tanaka</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/masatsugu-hangyo/ class=align-middle>Masatsugu Hangyo</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/kotaro-funakoshi/ class=align-middle>Kotaro Funakoshi</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/chenlong-hu/ class=align-middle>Chenlong Hu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/young-in-song/ class=align-middle>Young-In Song</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/hayato-kobayashi/ class=align-middle>Hayato Kobayashi</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/hiroaki-taguchi/ class=align-middle>Hiroaki Taguchi</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yoshimune-tabuchi/ class=align-middle>Yoshimune Tabuchi</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/chahine-koleejan/ class=align-middle>Chahine Koleejan</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/ken-kobayashi/ class=align-middle>Ken Kobayashi</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/soichiro-fujita/ class=align-middle>Soichiro Fujita</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/kazuma-murao/ class=align-middle>Kazuma Murao</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/takeshi-masuyama/ class=align-middle>Takeshi Masuyama</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/taichi-yatsuka/ class=align-middle>Taichi Yatsuka</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/satoshi-sekine/ class=align-middle>Satoshi Sekine</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/kengo-nakamura/ class=align-middle>Kengo Nakamura</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/ryohei-sasano/ class=align-middle>Ryohei Sasano</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/abdurrisyad-fikri/ class=align-middle>Abdurrisyad Fikri</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yuta-hitomi/ class=align-middle>Yuta Hitomi</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yuya-taguchi/ class=align-middle>Yuya Taguchi</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/hideaki-tamori/ class=align-middle>Hideaki Tamori</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/ko-kikuta/ class=align-middle>Ko Kikuta</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jiro-nishitoba/ class=align-middle>Jiro Nishitoba</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/n/naoaki-okazaki/ class=align-middle>Naoaki Okazaki</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/kentaro-inui/ class=align-middle>Kentaro Inui</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/shogo-fujita/ class=align-middle>Shogo Fujita</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/tomohide-shibata/ class=align-middle>Tomohide Shibata</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/takuya-makino/ class=align-middle>Takuya Makino</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/tomoya-iwakura/ class=align-middle>Tomoya Iwakura</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/ranlp/ class=align-middle>RANLP</a><span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/venues/eacl/ class=align-middle>EACL</a><span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/venues/coling/ class=align-middle>COLING</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/acl/ class=align-middle>ACL</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/emnlp/ class=align-middle>EMNLP</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-venues aria-expanded=false aria-controls=more-venues>show all...</li><div class="collapse border-top" id=more-venues><li class=list-group-item><a href=/venues/ws/ class=align-middle>WS</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/ijcnlp/ class=align-middle>IJCNLP</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/socialnlp/ class=align-middle>SocialNLP</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/conll/ class=align-middle>CoNLL</a><span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>