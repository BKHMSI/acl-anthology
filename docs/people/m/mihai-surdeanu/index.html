<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Mihai Surdeanu - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Mihai</span> <span class=font-weight-bold>Surdeanu</span></h2><hr><div class=row><div class=col-lg-9><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.558.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--558 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.558 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.558/>Students Who Study Together Learn Better : On the Importance of Collective Knowledge Distillation for Domain Transfer in Fact Verification</a></strong><br><a href=/people/m/mitch-paul-mithun/>Mitch Paul Mithun</a>
|
<a href=/people/s/sandeep-suntwal/>Sandeep Suntwal</a>
|
<a href=/people/m/mihai-surdeanu/>Mihai Surdeanu</a><br><a href=/volumes/2021.emnlp-main/ class=text-muted>Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--558><div class="card-body p-3 small">While <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a> produce state-of-the- art performance in several NLP tasks, they generally depend heavily on lexicalized information, which transfer poorly between domains. Previous works have proposed delexicalization as a form of knowledge distillation to reduce the dependency on such lexical artifacts. However, a critical unsolved issue that remains is how much delexicalization to apply : a little helps reduce <a href=https://en.wikipedia.org/wiki/Overfitting>overfitting</a>, but too much discards useful information. We propose <a href=https://en.wikipedia.org/wiki/Group_learning>Group Learning</a>, a knowledge and model distillation approach for fact verification in which multiple student models have access to different delexicalized views of the data, but are encouraged to learn from each other through pair-wise consistency losses. In several cross-domain experiments between the FEVER and FNC fact verification datasets, we show that our approach learns the best delexicalization strategy for the given training dataset, and outperforms state-of-the-art <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifiers</a> that rely on the original data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.trustnlp-1.1.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--trustnlp-1--1 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.trustnlp-1.1 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.trustnlp-1.1.OptionalSupplementaryData.tgz data-toggle=tooltip data-placement=top title="Optional supplementary data"><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2021.trustnlp-1.1/>Interpretability Rules : Jointly Bootstrapping a Neural Relation Extractorwith an Explanation Decoder</a></strong><br><a href=/people/z/zheng-tang/>Zheng Tang</a>
|
<a href=/people/m/mihai-surdeanu/>Mihai Surdeanu</a><br><a href=/volumes/2021.trustnlp-1/ class=text-muted>Proceedings of the First Workshop on Trustworthy Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--trustnlp-1--1><div class="card-body p-3 small">We introduce a method that transforms a rule-based relation extraction (RE) classifier into a neural one such that both interpretability and performance are achieved. Our approach jointly trains a RE classifier with a decoder that generates explanations for these extractions, using as sole supervision a set of rules that match these relations. Our evaluation on the TACRED dataset shows that our neural RE classifier outperforms the rule-based one we started from by 9 F1 points ; our decoder generates explanations with a high BLEU score of over 90 % ; and, the joint learning improves the performance of both the <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifier</a> and decoder.</div></div><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.297.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--297 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.297 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.297/>An Unsupervised Method for Learning Representations of Multi-word Expressions for Semantic Classification</a></strong><br><a href=/people/r/robert-vacareanu/>Robert Vacareanu</a>
|
<a href=/people/m/marco-a-valenzuela-escarcega/>Marco A. Valenzuela-Escárcega</a>
|
<a href=/people/r/rebecca-sharp/>Rebecca Sharp</a>
|
<a href=/people/m/mihai-surdeanu/>Mihai Surdeanu</a><br><a href=/volumes/2020.coling-main/ class=text-muted>Proceedings of the 28th International Conference on Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--297><div class="card-body p-3 small">This paper explores an unsupervised approach to learning a compositional representation function for multi-word expressions (MWEs), and evaluates it on the Tratz dataset, which associates two-word expressions with the semantic relation between the compound constituents (e.g. the label employer is associated with the noun compound government agency) (Tratz, 2011). The <a href=https://en.wikipedia.org/wiki/Composition_function>composition function</a> is based on <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent neural networks</a>, and is trained using the Skip-Gram objective to predict the words in the context of MWEs. Thus our approach can naturally leverage large unlabeled text sources. Further, our method can make use of provided MWEs when available, but can also function as a completely <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised algorithm</a>, using MWE boundaries predicted by a single, domain-agnostic part-of-speech pattern. With pre-defined MWE boundaries, our method outperforms the previous state-of-the-art performance on the coarse-grained evaluation of the Tratz dataset (Tratz, 2011), with an <a href=https://en.wikipedia.org/wiki/F-number>F1 score</a> of 50.4 %. The unsupervised version of our method approaches the performance of the supervised one, and even outperforms it in some configurations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.850.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--850 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.850 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.850/>Towards the Necessity for Debiasing Natural Language Inference Datasets</a></strong><br><a href=/people/m/mithun-paul-panenghat/>Mithun Paul Panenghat</a>
|
<a href=/people/s/sandeep-suntwal/>Sandeep Suntwal</a>
|
<a href=/people/f/faiz-rafique/>Faiz Rafique</a>
|
<a href=/people/r/rebecca-sharp/>Rebecca Sharp</a>
|
<a href=/people/m/mihai-surdeanu/>Mihai Surdeanu</a><br><a href=/volumes/2020.lrec-1/ class=text-muted>Proceedings of the 12th Language Resources and Evaluation Conference</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--850><div class="card-body p-3 small">Modeling natural language inference is a challenging <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. With large annotated data sets available it has now become feasible to train complex neural network based inference methods which achieve state of the art performance. However, it has been shown that these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> also learn from the subtle biases inherent in these <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> (CITATION). In this work we explore two techniques for delexicalization that modify the <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> in such a way that we can control the importance that neural-network based methods place on <a href=https://en.wikipedia.org/wiki/Lexical_item>lexical entities</a>. We demonstrate that the proposed methods not only maintain the performance in-domain but also improve performance in some out-of-domain settings. For example, when using the delexicalized version of the FEVER dataset, the in-domain performance of a state of the art <a href=https://en.wikipedia.org/wiki/Neural_network>neural network method</a> dropped only by 1.12 % while its out-of-domain performance on the FNC dataset improved by 4.63 %. We release the delexicalized versions of three common datasets used in natural language inference. These datasets are delexicalized using two methods : one which replaces the lexical entities in an overlap-aware manner, and a second, which additionally incorporates semantic lifting of nouns and verbs to their WordNet hypernym synsets</div></div><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5300.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5300/>Proceedings of the Thirteenth Workshop on Graph-Based Methods for Natural Language Processing (TextGraphs-13)</a></strong><br><a href=/people/d/dmitry-ustalov/>Dmitry Ustalov</a>
|
<a href=/people/s/swapna-somasundaran/>Swapna Somasundaran</a>
|
<a href=/people/p/peter-jansen/>Peter Jansen</a>
|
<a href=/people/g/goran-glavas/>Goran Glavaš</a>
|
<a href=/people/m/martin-riedl/>Martin Riedl</a>
|
<a href=/people/m/mihai-surdeanu/>Mihai Surdeanu</a>
|
<a href=/people/m/michalis-vazirgiannis/>Michalis Vazirgiannis</a><br><a href=/volumes/D19-53/ class=text-muted>Proceedings of the Thirteenth Workshop on Graph-Based Methods for Natural Language Processing (TextGraphs-13)</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-6212.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-6212 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-6212 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-6212/>What does the language of foods say about us?</a></strong><br><a href=/people/h/hoang-van/>Hoang Van</a>
|
<a href=/people/a/ahmad-musa/>Ahmad Musa</a>
|
<a href=/people/h/hang-chen/>Hang Chen</a>
|
<a href=/people/s/stephen-kobourov/>Stephen Kobourov</a>
|
<a href=/people/m/mihai-surdeanu/>Mihai Surdeanu</a><br><a href=/volumes/D19-62/ class=text-muted>Proceedings of the Tenth International Workshop on Health Text Mining and Information Analysis (LOUHI 2019)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-6212><div class="card-body p-3 small">In this work we investigate the signal contained in the language of food on <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>. We experiment with a <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> of 24 million food-related tweets, and make several observations. First, thelanguageoffoodhaspredictive power. We are able to predict if states in the United States (US) are above the medianratesfortype2diabetesmellitus(T2DM), <a href=https://en.wikipedia.org/wiki/Income>income</a>, <a href=https://en.wikipedia.org/wiki/Poverty>poverty</a>, and <a href=https://en.wikipedia.org/wiki/Education>education</a> outperforming previous work by 418 %. Second, we investigate the effect of <a href=https://en.wikipedia.org/wiki/Socioeconomics>socioeconomic factors</a> (income, <a href=https://en.wikipedia.org/wiki/Poverty>poverty</a>, and education) on predicting state-level T2DM rates. Socioeconomic factors do improve T2DM prediction, with the greatestimprovementcomingfrompovertyinformation(6%),but, importantly, thelanguage of food adds distinct information that is not captured by <a href=https://en.wikipedia.org/wiki/Socioeconomics>socioeconomics</a>. Third, we analyze how the language of food has changed over a five-year period (2013 2017), which is indicative of the shift in eating habits in the US during that period. We find several food trends, and that the language of food is used differently by different groups such as differentgenders. Last, weprovideanonlinevisualization tool for real-time queries and <a href=https://en.wikipedia.org/wiki/Semantic_analysis_(linguistics)>semantic analysis</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S19-2232.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S19-2232 data-toggle=collapse aria-expanded=false aria-controls=abstract-S19-2232 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/S19-2232/>University of Arizona at SemEval-2019 Task 12 : Deep-Affix Named Entity Recognition of Geolocation Entities<span class=acl-fixed-case>U</span>niversity of <span class=acl-fixed-case>A</span>rizona at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2019 Task 12: Deep-Affix Named Entity Recognition of Geolocation Entities</a></strong><br><a href=/people/v/vikas-yadav/>Vikas Yadav</a>
|
<a href=/people/e/egoitz-laparra/>Egoitz Laparra</a>
|
<a href=/people/t/ti-tai-wang/>Ti-Tai Wang</a>
|
<a href=/people/m/mihai-surdeanu/>Mihai Surdeanu</a>
|
<a href=/people/s/steven-bethard/>Steven Bethard</a><br><a href=/volumes/S19-2/ class=text-muted>Proceedings of the 13th International Workshop on Semantic Evaluation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S19-2232><div class="card-body p-3 small">We present the Named Entity Recognition (NER) and disambiguation model used by the University of Arizona team (UArizona) for the SemEval 2019 task 12. We achieved fourth place on tasks 1 and 3. We implemented a deep-affix based LSTM-CRF NER model for task 1, which utilizes only character, word, pre- fix and suffix information for the identification of geolocation entities. Despite using just the training data provided by task organizers and not using any <a href=https://en.wikipedia.org/wiki/Lexicon>lexicon features</a>, we achieved 78.85 % strict micro F-score on task 1. We used the unsupervised population heuristics for task 3 and achieved 52.99 % strict micro-F1 score in this task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-1504.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-1504 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-1504 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-1504/>Lightly-supervised Representation Learning with Global Interpretability</a></strong><br><a href=/people/a/andrew-zupon/>Andrew Zupon</a>
|
<a href=/people/m/maria-alexeeva/>Maria Alexeeva</a>
|
<a href=/people/m/marco-valenzuela-escarcega/>Marco Valenzuela-Escárcega</a>
|
<a href=/people/a/ajay-nagesh/>Ajay Nagesh</a>
|
<a href=/people/m/mihai-surdeanu/>Mihai Surdeanu</a><br><a href=/volumes/W19-15/ class=text-muted>Proceedings of the Third Workshop on Structured Prediction for NLP</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-1504><div class="card-body p-3 small">We propose a lightly-supervised approach for <a href=https://en.wikipedia.org/wiki/Information_extraction>information extraction</a>, in particular named entity classification, which combines the benefits of traditional <a href=https://en.wikipedia.org/wiki/Bootstrapping_(statistics)>bootstrapping</a>, i.e., use of limited annotations and interpretability of extraction patterns, with the robust learning approaches proposed in <a href=https://en.wikipedia.org/wiki/Representation_learning>representation learning</a>. Our <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> iteratively learns custom embeddings for both the multi-word entities to be extracted and the patterns that match them from a few example entities per category. We demonstrate that this representation-based approach outperforms three other state-of-the-art bootstrapping approaches on two datasets : CoNLL-2003 and OntoNotes. Additionally, using these embeddings, our approach outputs a globally-interpretable model consisting of a <a href=https://en.wikipedia.org/wiki/Decision_list>decision list</a>, by ranking patterns based on their proximity to the average entity embedding in a given class. We show that this interpretable model performs close to our complete bootstrapping model, proving that <a href=https://en.wikipedia.org/wiki/Representation_learning>representation learning</a> can be used to produce interpretable models with small loss in performance. This <a href=https://en.wikipedia.org/wiki/Decision_list>decision list</a> can be edited by human experts to mitigate some of that loss and in some cases outperform the original <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-1505.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-1505 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-1505 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-1505/>Semi-Supervised Teacher-Student Architecture for Relation Extraction</a></strong><br><a href=/people/f/fan-luo/>Fan Luo</a>
|
<a href=/people/a/ajay-nagesh/>Ajay Nagesh</a>
|
<a href=/people/r/rebecca-sharp/>Rebecca Sharp</a>
|
<a href=/people/m/mihai-surdeanu/>Mihai Surdeanu</a><br><a href=/volumes/W19-15/ class=text-muted>Proceedings of the Third Workshop on Structured Prediction for NLP</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-1505><div class="card-body p-3 small">Generating a large amount of training data for information extraction (IE) is either costly (if annotations are created manually), or runs the risk of introducing noisy instances (if distant supervision is used). On the other hand, semi-supervised learning (SSL) is a cost-efficient solution to combat lack of training data. In this paper, we adapt <a href=https://en.wikipedia.org/wiki/Mean_Teacher>Mean Teacher</a> (Tarvainen and Valpola, 2017), a denoising SSL framework to extract semantic relations between pairs of entities. We explore the sweet spot of amount of supervision required for good performance on this binary relation extraction task. Additionally, different syntax representations are incorporated into our <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> to enhance the learned representation of sentences. We evaluate our approach on the Google-IISc Distant Supervision (GDS) dataset, which removes test data noise present in all previous distance supervision datasets, which makes it a reliable evaluation benchmark (Jat et al., 2017). Our results show that the SSL Mean Teacher approach nears the performance of fully-supervised approaches even with only 10 % of the labeled corpus. Further, the syntax-aware model outperforms other syntax-free approaches across all levels of <a href=https://en.wikipedia.org/wiki/Supervisor>supervision</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2603.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-2603 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-2603 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-2603/>Understanding the Polarity of Events in the Biomedical Literature : <a href=https://en.wikipedia.org/wiki/Deep_learning>Deep Learning</a> vs. <a href=https://en.wikipedia.org/wiki/Linguistic_description>Linguistically-informed Methods</a></a></strong><br><a href=/people/e/enrique-noriega-atala/>Enrique Noriega-Atala</a>
|
<a href=/people/z/zhengzhong-liang/>Zhengzhong Liang</a>
|
<a href=/people/j/john-bachman/>John Bachman</a>
|
<a href=/people/c/clayton-morrison/>Clayton Morrison</a>
|
<a href=/people/m/mihai-surdeanu/>Mihai Surdeanu</a><br><a href=/volumes/W19-26/ class=text-muted>Proceedings of the Workshop on Extracting Structured Knowledge from Scientific Publications</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-2603><div class="card-body p-3 small">An important task in the machine reading of biochemical events expressed in biomedical texts is correctly reading the <a href=https://en.wikipedia.org/wiki/Chemical_polarity>polarity</a>, i.e., attributing whether the biochemical event is a promotion or an inhibition. Here we present a novel <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> for studying polarity attribution accuracy. We use this <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> to train and evaluate several <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning models</a> for polarity identification, and compare these to a linguistically-informed model. The best performing deep learning architecture achieves 0.968 average F1 performance in a five-fold cross-validation study, a considerable improvement over the linguistically informed model average F1 of 0.862.</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1196.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1196 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1196 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1196/>An Exploration of Three Lightly-supervised Representation Learning Approaches for Named Entity Classification</a></strong><br><a href=/people/a/ajay-nagesh/>Ajay Nagesh</a>
|
<a href=/people/m/mihai-surdeanu/>Mihai Surdeanu</a><br><a href=/volumes/C18-1/ class=text-muted>Proceedings of the 27th International Conference on Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1196><div class="card-body p-3 small">Several semi-supervised representation learning methods have been proposed recently that mitigate the drawbacks of traditional <a href=https://en.wikipedia.org/wiki/Bootstrapping_(statistics)>bootstrapping</a> : they reduce the amount of <a href=https://en.wikipedia.org/wiki/Semantic_drift>semantic drift</a> introduced by iterative approaches through <a href=https://en.wikipedia.org/wiki/One-shot_learning>one-shot learning</a> ; others address the sparsity of data through the learning of custom, dense representation for the information modeled. In this work, we are the first to adapt three of these methods, most of which have been originally proposed for <a href=https://en.wikipedia.org/wiki/Digital_image_processing>image processing</a>, to an information extraction task, specifically, named entity classification. Further, we perform a rigorous comparative analysis on two distinct datasets. Our analysis yields several important observations. First, all <a href=https://en.wikipedia.org/wiki/Representation_learning>representation learning methods</a> outperform state-of-the-art <a href=https://en.wikipedia.org/wiki/Semi-supervised_learning>semi-supervised methods</a> that do not rely on <a href=https://en.wikipedia.org/wiki/Representation_learning>representation learning</a>. To the best of our knowledge, we report the latest state-of-the-art results on the semi-supervised named entity classification task. Second, one-shot learning methods clearly outperform iterative representation learning approaches. Lastly, one of the best performers relies on the mean teacher framework (Tarvainen and Valpola, 2017), a simple teacher / student approach that is independent of the underlying task-specific model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2057.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2057 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2057 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-2057/>Keep Your Bearings : Lightly-Supervised Information Extraction with <a href=https://en.wikipedia.org/wiki/Ladder_network>Ladder Networks</a> That Avoids <a href=https://en.wikipedia.org/wiki/Semantic_drift>Semantic Drift</a></a></strong><br><a href=/people/a/ajay-nagesh/>Ajay Nagesh</a>
|
<a href=/people/m/mihai-surdeanu/>Mihai Surdeanu</a><br><a href=/volumes/N18-2/ class=text-muted>Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2057><div class="card-body p-3 small">We propose a novel approach to <a href=https://en.wikipedia.org/wiki/Semi-supervised_learning>semi-supervised learning</a> for <a href=https://en.wikipedia.org/wiki/Information_extraction>information extraction</a> that uses <a href=https://en.wikipedia.org/wiki/Ladder_network>ladder networks</a> (Rasmus et al., 2015). In particular, we focus on the task of named entity classification, defined as identifying the correct label (e.g., person or organization name) of an entity mention in a given context. Our approach is simple, efficient and has the benefit of being robust to <a href=https://en.wikipedia.org/wiki/Semantic_drift>semantic drift</a>, a dominant problem in most <a href=https://en.wikipedia.org/wiki/Semi-supervised_learning>semi-supervised learning systems</a>. We empirically demonstrate the superior performance of our <a href=https://en.wikipedia.org/wiki/System>system</a> compared to the <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a> on two standard <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> for named entity classification. We obtain between 62 % and 200 % improvement over the state-of-art baseline on these two <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>.</div></div><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K17-1009.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K17-1009 data-toggle=collapse aria-expanded=false aria-controls=abstract-K17-1009 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/K17-1009/>Tell Me Why : Using Question Answering as Distant Supervision for Answer Justification</a></strong><br><a href=/people/r/rebecca-sharp/>Rebecca Sharp</a>
|
<a href=/people/m/mihai-surdeanu/>Mihai Surdeanu</a>
|
<a href=/people/p/peter-jansen/>Peter Jansen</a>
|
<a href=/people/m/marco-a-valenzuela-escarcega/>Marco A. Valenzuela-Escárcega</a>
|
<a href=/people/p/peter-clark/>Peter Clark</a>
|
<a href=/people/m/michael-hammond/>Michael Hammond</a><br><a href=/volumes/K17-1/ class=text-muted>Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K17-1009><div class="card-body p-3 small">For many applications of question answering (QA), being able to explain why a given <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> chose an answer is critical. However, the lack of labeled data for answer justifications makes learning this difficult and expensive. Here we propose an approach that uses answer ranking as distant supervision for learning how to select informative justifications, where justifications serve as inferential connections between the question and the correct answer while often containing little lexical overlap with either. We propose a neural network architecture for <a href=https://en.wikipedia.org/wiki/Quality_assurance>QA</a> that reranks answer justifications as an intermediate (and human-interpretable) step in answer selection. Our approach is informed by a set of <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> designed to combine both learned representations and explicit features to capture the connection between questions, answers, and answer justifications. We show that with this end-to-end approach we are able to significantly improve upon a strong IR baseline in both justification ranking (+9 % rated highly relevant) and answer selection (+6 % P@1).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/J17-2005.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-J17-2005 data-toggle=collapse aria-expanded=false aria-controls=abstract-J17-2005 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/J17-2005/>Framing QA as Building and Ranking Intersentence Answer Justifications<span class=acl-fixed-case>QA</span> as Building and Ranking Intersentence Answer Justifications</a></strong><br><a href=/people/p/peter-jansen/>Peter Jansen</a>
|
<a href=/people/r/rebecca-sharp/>Rebecca Sharp</a>
|
<a href=/people/m/mihai-surdeanu/>Mihai Surdeanu</a>
|
<a href=/people/p/peter-clark/>Peter Clark</a><br><a href=/volumes/J17-2/ class=text-muted>Computational Linguistics, Volume 43, Issue 2 - June 2017</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-J17-2005><div class="card-body p-3 small">We propose a question answering (QA) approach for standardized science exams that both identifies correct answers and produces compelling human-readable justifications for why those answers are correct. Our method first identifies the actual information needed in a question using psycholinguistic concreteness norms, then uses this information need to construct answer justifications by aggregating multiple sentences from different knowledge bases using syntactic and lexical information. We then jointly rank answers and their justifications using a reranking perceptron that treats <a href=https://en.wikipedia.org/wiki/Theory_of_justification>justification quality</a> as a <a href=https://en.wikipedia.org/wiki/Latent_variable>latent variable</a>. We evaluate our method on 1,000 multiple-choice questions from elementary school science exams, and empirically demonstrate that it performs better than several strong baselines, including neural network approaches. Our best configuration answers 44 % of the questions correctly, where the top justifications for 57 % of these correct answers contain a compelling human-readable justification that explains the inference required to arrive at the correct answer. We include a detailed characterization of the justification quality for both our method and a strong <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline</a>, and show that <a href=https://en.wikipedia.org/wiki/Information_aggregation>information aggregation</a> is key to addressing the information need in complex questions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1313.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1313 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1313 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1313/>Learning what to read : Focused machine reading</a></strong><br><a href=/people/e/enrique-noriega-atala/>Enrique Noriega-Atala</a>
|
<a href=/people/m/marco-a-valenzuela-escarcega/>Marco A. Valenzuela-Escárcega</a>
|
<a href=/people/c/clayton-morrison/>Clayton Morrison</a>
|
<a href=/people/m/mihai-surdeanu/>Mihai Surdeanu</a><br><a href=/volumes/D17-1/ class=text-muted>Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1313><div class="card-body p-3 small">Recent efforts in <a href=https://en.wikipedia.org/wiki/Bioinformatics>bioinformatics</a> have achieved tremendous progress in the machine reading of biomedical literature, and the assembly of the extracted biochemical interactions into large-scale models such as protein signaling pathways. However, batch machine reading of literature at today&#8217;s scale (PubMed alone indexes over 1 million papers per year) is unfeasible due to both cost and <a href=https://en.wikipedia.org/wiki/Overhead_(computing)>processing overhead</a>. In this work, we introduce a focused reading approach to guide the machine reading of biomedical literature towards what literature should be read to answer a biomedical query as efficiently as possible. We introduce a family of <a href=https://en.wikipedia.org/wiki/Algorithm>algorithms</a> for focused reading, including an intuitive, strong baseline, and a second approach which uses a reinforcement learning (RL) framework that learns when to explore (widen the search) or exploit (narrow it). We demonstrate that the RL approach is capable of answering more queries than the <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline</a>, while being more efficient, i.e., reading fewer documents.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Mihai+Surdeanu" title="Search for 'Mihai Surdeanu' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/r/rebecca-sharp/ class=align-middle>Rebecca Sharp</a>
<span class="badge badge-secondary align-middle ml-2">5</span></li><li class=list-group-item><a href=/people/a/ajay-nagesh/ class=align-middle>Ajay Nagesh</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/people/p/peter-jansen/ class=align-middle>Peter Jansen</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/m/marco-a-valenzuela-escarcega/ class=align-middle>Marco A. Valenzuela-Escárcega</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/p/peter-clark/ class=align-middle>Peter Clark</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/s/sandeep-suntwal/ class=align-middle>Sandeep Suntwal</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/e/enrique-noriega-atala/ class=align-middle>Enrique Noriega-Atala</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/c/clayton-morrison/ class=align-middle>Clayton Morrison</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/m/michael-hammond/ class=align-middle>Michael Hammond</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/mitch-paul-mithun/ class=align-middle>Mitch Paul Mithun</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/dmitry-ustalov/ class=align-middle>Dmitry Ustalov</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/swapna-somasundaran/ class=align-middle>Swapna Somasundaran</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/g/goran-glavas/ class=align-middle>Goran Glavaš</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/martin-riedl/ class=align-middle>Martin Riedl</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/michalis-vazirgiannis/ class=align-middle>Michalis Vazirgiannis</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/hoang-van/ class=align-middle>Hoang Van</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/ahmad-musa/ class=align-middle>Ahmad Musa</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/hang-chen/ class=align-middle>Hang Chen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/stephen-kobourov/ class=align-middle>Stephen Kobourov</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/v/vikas-yadav/ class=align-middle>Vikas Yadav</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/e/egoitz-laparra/ class=align-middle>Egoitz Laparra</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/ti-tai-wang/ class=align-middle>Ti-Tai Wang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/steven-bethard/ class=align-middle>Steven Bethard</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/andrew-zupon/ class=align-middle>Andrew Zupon</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/maria-alexeeva/ class=align-middle>Maria Alexeeva</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/marco-valenzuela-escarcega/ class=align-middle>Marco Valenzuela-Escárcega</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/f/fan-luo/ class=align-middle>Fan Luo</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/zhengzhong-liang/ class=align-middle>Zhengzhong Liang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/john-bachman/ class=align-middle>John Bachman</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/robert-vacareanu/ class=align-middle>Robert Vacareanu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/mithun-paul-panenghat/ class=align-middle>Mithun Paul Panenghat</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/f/faiz-rafique/ class=align-middle>Faiz Rafique</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/zheng-tang/ class=align-middle>Zheng Tang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/emnlp/ class=align-middle>EMNLP</a><span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/venues/ws/ class=align-middle>WS</a><span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/venues/coling/ class=align-middle>COLING</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/conll/ class=align-middle>CoNLL</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/cl/ class=align-middle>CL</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-venues aria-expanded=false aria-controls=more-venues>show all...</li><div class="collapse border-top" id=more-venues><li class=list-group-item><a href=/venues/semeval/ class=align-middle>SemEval</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/naacl/ class=align-middle>NAACL</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/lrec/ class=align-middle>LREC</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/trustnlp/ class=align-middle>TrustNLP</a><span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>