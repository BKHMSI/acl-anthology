<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Min-Yen Kan - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Min-Yen</span> <span class=font-weight-bold>Kan</span></h2><hr><div class=row><div class=col-lg-9><h4>2022</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.acl-long.191.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--acl-long--191 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.acl-long.191 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2022.acl-long.191.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.acl-long.191" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2022.acl-long.191/><span class=acl-fixed-case>GL</span>-<span class=acl-fixed-case>CL</span>e<span class=acl-fixed-case>F</span>: A Global–Local Contrastive Learning Framework for Cross-lingual Spoken Language Understanding</a></strong><br><a href=/people/l/libo-qin/>Libo Qin</a>
|
<a href=/people/q/qiguang-chen/>Qiguang Chen</a>
|
<a href=/people/t/tianbao-xie/>Tianbao Xie</a>
|
<a href=/people/q/qixin-li/>Qixin Li</a>
|
<a href=/people/j/jian-guang-lou/>Jian-Guang Lou</a>
|
<a href=/people/w/wanxiang-che/>Wanxiang Che</a>
|
<a href=/people/m/min-yen-kan/>Min-Yen Kan</a><br><a href=/volumes/2022.acl-long/ class=text-muted>Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--acl-long--191><div class="card-body p-3 small">Due to high data demands of current methods, attention to zero-shot cross-lingual spoken language understanding (SLU) has grown, as such approaches greatly reduce human annotation effort. However, existing models solely rely on shared parameters, which can only perform implicit alignment across languages. We present Global-Local Contrastive Learning Framework (GL-CLeF) to address this shortcoming. Specifically, we employ contrastive learning, leveraging bilingual dictionaries to construct multilingual views of the same utterance, then encourage their representations to be more similar than negative example pairs, which achieves to explicitly align representations of similar sentences across languages. In addition, a key step in GL-CLeF is a proposed Local and Global component, which achieves a fine-grained cross-lingual transfer (i.e., sentence-level Local intent transfer, token-level Local slot transfer, and semantic-level Global transfer across intent and slot). Experiments on MultiATIS++ show that GL-CLeF achieves the best performance and successfully pulls representations of similar sentences across languages closer.</div></div><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-short.61.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-short--61 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-short.61 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-short.61" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-short.61/>Zero-shot Fact Verification by Claim Generation</a></strong><br><a href=/people/l/liangming-pan/>Liangming Pan</a>
|
<a href=/people/w/wenhu-chen/>Wenhu Chen</a>
|
<a href=/people/w/wenhan-xiong/>Wenhan Xiong</a>
|
<a href=/people/m/min-yen-kan/>Min-Yen Kan</a>
|
<a href=/people/w/william-yang-wang/>William Yang Wang</a><br><a href=/volumes/2021.acl-short/ class=text-muted>Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-short--61><div class="card-body p-3 small">Neural models for automated fact verification have achieved promising results thanks to the availability of large, human-annotated datasets. However, for each new domain that requires <a href=https://en.wikipedia.org/wiki/Fact-checking>fact verification</a>, creating a <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> by manually writing claims and linking them to their supporting evidence is expensive. We develop QACG, a framework for training a robust fact verification model by using automatically generated claims that can be supported, refuted, or unverifiable from evidence from <a href=https://en.wikipedia.org/wiki/Wikipedia>Wikipedia</a>. QACG generates question-answer pairs from the evidence and then converts them into different types of claims. Experiments on the FEVER dataset show that our QACG framework significantly reduces the demand for human-annotated training data. In a zero-shot scenario, QACG improves a RoBERTa model&#8217;s F1 from 50 % to 77 %, equivalent in performance to 2K+ manually-curated examples. Our QACG code is publicly available.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.sigdial-1.14.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--sigdial-1--14 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.sigdial-1.14 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href="https://www.youtube.com/watch?v=2BkbrFFGTFA" data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.sigdial-1.14" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2021.sigdial-1.14/>Velocidapter : Task-oriented Dialogue Comprehension Modeling Pairing Synthetic Text Generation with Domain Adaptation</a></strong><br><a href=/people/i/ibrahim-taha-aksu/>Ibrahim Taha Aksu</a>
|
<a href=/people/z/zhengyuan-liu/>Zhengyuan Liu</a>
|
<a href=/people/m/min-yen-kan/>Min-Yen Kan</a>
|
<a href=/people/n/nancy-chen/>Nancy Chen</a><br><a href=/volumes/2021.sigdial-1/ class=text-muted>Proceedings of the 22nd Annual Meeting of the Special Interest Group on Discourse and Dialogue</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--sigdial-1--14><div class="card-body p-3 small">We introduce a synthetic dialogue generation framework, Velocidapter, which addresses the corpus availability problem for dialogue comprehension. Velocidapter augments datasets by simulating synthetic conversations for a task-oriented dialogue domain, requiring a small amount of bootstrapping work for each new domain. We evaluate the efficacy of our framework on a task-oriented dialogue comprehension dataset, MRCWOZ, which we curate by annotating questions for slots in the restaurant, taxi, and hotel domains of the MultiWOZ 2.2 dataset (Zang et al., 2020). We run experiments within a low-resource setting, where we pretrain a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> on SQuAD, fine-tuning it on either a small original data or on the synthetic data generated by our <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a>. Velocidapter shows significant improvements using both the transformer-based BERTBase and BiDAF as base models. We further show that the <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> is easy to use by novice users and conclude that Velocidapter can greatly help training over task-oriented dialogues, especially for low-resourced emerging domains.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.147.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--147 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.147 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.147/>Domain Divergences : A Survey and Empirical Analysis</a></strong><br><a href=/people/a/abhinav-ramesh-kashyap/>Abhinav Ramesh Kashyap</a>
|
<a href=/people/d/devamanyu-hazarika/>Devamanyu Hazarika</a>
|
<a href=/people/m/min-yen-kan/>Min-Yen Kan</a>
|
<a href=/people/r/roger-zimmermann/>Roger Zimmermann</a><br><a href=/volumes/2021.naacl-main/ class=text-muted>Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--147><div class="card-body p-3 small">Domain divergence plays a significant role in estimating the performance of a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> in <a href=https://en.wikipedia.org/wiki/Domain_of_a_function>new domains</a>. While there is a significant literature on <a href=https://en.wikipedia.org/wiki/Divergence>divergence measures</a>, researchers find it hard to choose an appropriate <a href=https://en.wikipedia.org/wiki/Divergence>divergence</a> for a given NLP application. We address this shortcoming by both surveying the literature and through an empirical study. We develop a taxonomy of divergence measures consisting of three classes Information-theoretic, Geometric, and Higher-order measures and identify the relationships between them. Further, to understand the common use-cases of these measures, we recognise three novel applications 1) Data Selection, 2) Learning Representation, and 3) Decisions in the Wild and use it to organise our literature. From this, we identify that Information-theoretic measures are prevalent for 1) and 3), and Higher-order measures are more common for 2). To further help researchers choose appropriate measures to predict drop in performance an important aspect of Decisions in the Wild, we perform <a href=https://en.wikipedia.org/wiki/Correlation_and_dependence>correlation analysis</a> spanning 130 domain adaptation scenarios, 3 varied NLP tasks and 12 divergence measures identified from our survey. To calculate these divergences, we consider the current contextual word representations (CWR) and contrast with the older distributed representations. We find that traditional <a href=https://en.wikipedia.org/wiki/Measure_(mathematics)>measures</a> over word distributions still serve as strong baselines, while higher-order measures with CWR are effective.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.469.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--469 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.469 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.naacl-main.469" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.469/>Unsupervised Multi-hop Question Answering by Question Generation</a></strong><br><a href=/people/l/liangming-pan/>Liangming Pan</a>
|
<a href=/people/w/wenhu-chen/>Wenhu Chen</a>
|
<a href=/people/w/wenhan-xiong/>Wenhan Xiong</a>
|
<a href=/people/m/min-yen-kan/>Min-Yen Kan</a>
|
<a href=/people/w/william-yang-wang/>William Yang Wang</a><br><a href=/volumes/2021.naacl-main/ class=text-muted>Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--469><div class="card-body p-3 small">Obtaining training data for multi-hop question answering (QA) is time-consuming and resource-intensive. We explore the possibility to train a well-performed multi-hop QA model without referencing any human-labeled multi-hop question-answer pairs, i.e., unsupervised multi-hop QA. We propose MQA-QG, an <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised framework</a> that can generate human-like multi-hop training data from both homogeneous and heterogeneous data sources. MQA-QG generates questions by first selecting / generating relevant information from each data source and then integrating the multiple information to form a multi-hop question. Using only generated training data, we can train a competent multi-hop QA which achieves 61 % and 83 % of the <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised learning</a> performance for the HybridQA and the HotpotQA dataset, respectively. We also show that pretraining the QA system with the generated data would greatly reduce the demand for human-annotated training data. Our codes are publicly available at https://github.com/teacherpeterpan/Unsupervised-Multi-hop-QA.</div></div><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.228.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--228 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.228 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.coling-main.228" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.228/>Exploring Question-Specific Rewards for Generating Deep Questions</a></strong><br><a href=/people/y/yuxi-xie/>Yuxi Xie</a>
|
<a href=/people/l/liangming-pan/>Liangming Pan</a>
|
<a href=/people/d/dongzhe-wang/>Dongzhe Wang</a>
|
<a href=/people/m/min-yen-kan/>Min-Yen Kan</a>
|
<a href=/people/y/yansong-feng/>Yansong Feng</a><br><a href=/volumes/2020.coling-main/ class=text-muted>Proceedings of the 28th International Conference on Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--228><div class="card-body p-3 small">Recent question generation (QG) approaches often utilize the sequence-to-sequence framework (Seq2Seq) to optimize the log likelihood of ground-truth questions using teacher forcing. However, this training objective is inconsistent with actual question quality, which is often reflected by certain global properties such as whether the question can be answered by the document. As such, we directly optimize for QG-specific objectives via <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning</a> to improve question quality. We design three different rewards that target to improve the <a href=https://en.wikipedia.org/wiki/Fluency>fluency</a>, <a href=https://en.wikipedia.org/wiki/Relevance>relevance</a>, and answerability of generated questions. We conduct both automatic and human evaluations in addition to thorough analysis to explore the effect of each QG-specific reward. We find that optimizing on question-specific rewards generally leads to better performance in automatic evaluation metrics. However, only the <a href=https://en.wikipedia.org/wiki/Motivational_salience>rewards</a> that correlate well with <a href=https://en.wikipedia.org/wiki/Judgement>human judgement</a> (e.g., relevance) lead to real improvement in question quality. Optimizing for the others, especially answerability, introduces incorrect bias to the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>, resulting in poorer question quality. The code is publicly available at https://github.com/YuxiXie/RL-for-Question-Generation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.238.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--238 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.238 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.coling-main.238" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.238/>Molweni : A Challenge Multiparty Dialogues-based Machine Reading Comprehension Dataset with Discourse Structure</a></strong><br><a href=/people/j/jiaqi-li/>Jiaqi Li</a>
|
<a href=/people/m/ming-liu/>Ming Liu</a>
|
<a href=/people/m/min-yen-kan/>Min-Yen Kan</a>
|
<a href=/people/z/zihao-zheng/>Zihao Zheng</a>
|
<a href=/people/z/zekun-wang/>Zekun Wang</a>
|
<a href=/people/w/wenqiang-lei/>Wenqiang Lei</a>
|
<a href=/people/t/ting-liu/>Ting Liu</a>
|
<a href=/people/b/bing-qin/>Bing Qin</a><br><a href=/volumes/2020.coling-main/ class=text-muted>Proceedings of the 28th International Conference on Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--238><div class="card-body p-3 small">Research into the area of multiparty dialog has grown considerably over recent years. We present the Molweni dataset, a machine reading comprehension (MRC) dataset with discourse structure built over multiparty dialog. Molweni&#8217;s source samples from the Ubuntu Chat Corpus, including 10,000 dialogs comprising 88,303 utterances. We annotate 30,066 questions on this <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a>, including both answerable and unanswerable questions. Molweni also uniquely contributes discourse dependency annotations in a modified Segmented Discourse Representation Theory (SDRT ; Asher et al., 2016) style for all of its multiparty dialogs, contributing large-scale (78,245 annotated discourse relations) data to bear on the task of multiparty dialog discourse parsing. Our experiments show that Molweni is a challenging dataset for current MRC models : BERT-wwm, a current, strong SQuAD 2.0 performer, achieves only 67.7 % F1 on Molweni&#8217;s questions, a 20+% significant drop as compared against its SQuAD 2.0 performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.513.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--513 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.513 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.coling-main.513" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.513/>Retrieving Skills from Job Descriptions : A Language Model Based Extreme Multi-label Classification Framework</a></strong><br><a href=/people/a/akshay-bhola/>Akshay Bhola</a>
|
<a href=/people/k/kishaloy-halder/>Kishaloy Halder</a>
|
<a href=/people/a/animesh-prasad/>Animesh Prasad</a>
|
<a href=/people/m/min-yen-kan/>Min-Yen Kan</a><br><a href=/volumes/2020.coling-main/ class=text-muted>Proceedings of the 28th International Conference on Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--513><div class="card-body p-3 small">We introduce a deep learning model to learn the set of enumerated job skills associated with a job description. In our analysis of a large-scale government job portal mycareersfuture.sg, we observe that as much as 65 % of <a href=https://en.wikipedia.org/wiki/Job_description>job descriptions</a> miss describing a significant number of relevant skills. Our model addresses this task from the perspective of an extreme multi-label classification (XMLC) problem, where descriptions are the evidence for the binary relevance of thousands of individual skills. Building upon the current state-of-the-art language modeling approaches such as BERT, we show our XMLC method improves on an existing baseline solution by over 9 % and 7 % absolute improvements in terms of recall and normalized discounted cumulative gain. We further show that our approach effectively addresses the missing skills problem, and helps in recovering relevant skills that were missed out in the job postings by taking into account the structured semantic representation of skills and their co-occurrences through a Correlation Aware Bootstrapping process. We further show that our approach, to ensure the BERT-XMLC model accounts for structured semantic representation of skills and their co-occurrences through a Correlation Aware Bootstrapping process, effectively addresses the missing skills problem, and helps in recovering relevant skills that were missed out in the job postings. To facilitate future research and replication of our work, we have made the dataset and the implementation of our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> publicly available.</div></div><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2604.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-2604 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-2604 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-2604/>Dataset Mention Extraction and Classification</a></strong><br><a href=/people/a/animesh-prasad/>Animesh Prasad</a>
|
<a href=/people/c/chenglei-si/>Chenglei Si</a>
|
<a href=/people/m/min-yen-kan/>Min-Yen Kan</a><br><a href=/volumes/W19-26/ class=text-muted>Proceedings of the Workshop on Extracting Structured Knowledge from Scientific Publications</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-2604><div class="card-body p-3 small">Datasets are integral artifacts of <a href=https://en.wikipedia.org/wiki/Empirical_research>empirical scientific research</a>. However, due to <a href=https://en.wikipedia.org/wiki/Variation_(linguistics)>natural language variation</a>, their recognition can be difficult and even when identified, can often be inconsistently referred across and within publications. We report our approach to the Coleridge Initiative&#8217;s Rich Context Competition, which tasks participants with identifying dataset surface forms (dataset mention extraction) and associating the extracted mention to its referred dataset (dataset classification). In this work, we propose various neural baselines and evaluate these <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> on one-plus and zero-shot classification scenarios. We further explore various joint learning approaches-exploring the synergy between the tasks-and report the issues with such techniques.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1182.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1182 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1182 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-1182/>Glocal : Incorporating Global Information in Local Convolution for Keyphrase Extraction<span class=acl-fixed-case>G</span>local: Incorporating Global Information in Local Convolution for Keyphrase Extraction</a></strong><br><a href=/people/a/animesh-prasad/>Animesh Prasad</a>
|
<a href=/people/m/min-yen-kan/>Min-Yen Kan</a><br><a href=/volumes/N19-1/ class=text-muted>Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1182><div class="card-body p-3 small">Graph Convolutional Networks (GCNs) are a class of spectral clustering techniques that leverage localized convolution filters to perform <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised classification</a> directly on <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graphical structures</a>. While such methods model nodes&#8217; local pairwise importance, they lack the capability to model global importance relative to other nodes of the <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph</a>. This causes such <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> to miss critical information in tasks where global ranking is a key component for the task, such as in <a href=https://en.wikipedia.org/wiki/Keyphrase_extraction>keyphrase extraction</a>. We address this shortcoming by allowing the proper incorporation of global information into the GCN family of models through the use of scaled node weights. In the context of <a href=https://en.wikipedia.org/wiki/Keyphrase_extraction>keyphrase extraction</a>, incorporating global random walk scores obtained from TextRank boosts performance significantly. With our proposed <a href=https://en.wikipedia.org/wiki/Methodology>method</a>, we achieve state-of-the-art results, bettering a strong <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline</a> by an absolute 2 % increase in F1 score.</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1022.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1022 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1022 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=C18-1022" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/C18-1022/>Identifying Emergent Research Trends by Key Authors and Phrases</a></strong><br><a href=/people/s/shenhao-jiang/>Shenhao Jiang</a>
|
<a href=/people/a/animesh-prasad/>Animesh Prasad</a>
|
<a href=/people/m/min-yen-kan/>Min-Yen Kan</a>
|
<a href=/people/k/kazunari-sugiyama/>Kazunari Sugiyama</a><br><a href=/volumes/C18-1/ class=text-muted>Proceedings of the 27th International Conference on Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1022><div class="card-body p-3 small">Identifying emergent research trends is a key issue for both primary researchers as well as secondary research managers. Such processes can uncover the historical development of an area, and yield insight on developing topics. We propose an embedded trend detection framework for this task which incorporates our bijunctive hypothesis that important phrases are written by important authors within a field and vice versa. By ranking both author and phrase information in a <a href=https://en.wikipedia.org/wiki/Multigraph>multigraph</a>, our method jointly determines key phrases and authoritative authors. We represent this intermediate output as phrasal embeddings, and feed this to a <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent neural network (RNN)</a> to compute trend scores that identify research trends. Over two large datasets of scientific articles, we demonstrate that our approach successfully detects past trends from the field, outperforming baselines based solely on text centrality or <a href=https://en.wikipedia.org/wiki/Citation>citation</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-2504.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-2504 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-2504 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W18-2504.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W18-2504.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/W18-2504/>The ACL Anthology : Current State and Future Directions<span class=acl-fixed-case>ACL</span> <span class=acl-fixed-case>A</span>nthology: Current State and Future Directions</a></strong><br><a href=/people/d/daniel-gildea/>Daniel Gildea</a>
|
<a href=/people/m/min-yen-kan/>Min-Yen Kan</a>
|
<a href=/people/n/nitin-madnani/>Nitin Madnani</a>
|
<a href=/people/c/christoph-teichmann/>Christoph Teichmann</a>
|
<a href=/people/m/martin-villalba/>Martín Villalba</a><br><a href=/volumes/W18-25/ class=text-muted>Proceedings of Workshop for NLP Open Source Software (NLP-OSS)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-2504><div class="card-body p-3 small">The Association of Computational Linguistic&#8217;s Anthology is the open source archive, and the main source for <a href=https://en.wikipedia.org/wiki/Computational_linguistics>computational linguistics</a> and natural language processing&#8217;s scientific literature. The <a href=https://en.wikipedia.org/wiki/ACL_Anthology>ACL Anthology</a> is currently maintained exclusively by community volunteers and has to be available and up-to-date at all times. We first discuss the current, open source approach used to achieve this, and then discuss how the planned use of <a href=https://en.wikipedia.org/wiki/Docker_(software)>Docker images</a> will improve the <a href=https://en.wikipedia.org/wiki/Anthology>Anthology</a>&#8217;s long-term stability. This change will make it easier for researchers to utilize Anthology data for experimentation. We believe the ACL community can directly benefit from the extension-friendly architecture of the <a href=https://en.wikipedia.org/wiki/Anthology>Anthology</a>. We end by issuing an open challenge of reviewer matching we encourage the community to rally towards.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3720.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3720 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3720 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W18-3720.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/W18-3720/>Countering Position Bias in Instructor Interventions in MOOC Discussion Forums<span class=acl-fixed-case>MOOC</span> Discussion Forums</a></strong><br><a href=/people/m/muthu-kumar-chandrasekaran/>Muthu Kumar Chandrasekaran</a>
|
<a href=/people/m/min-yen-kan/>Min-Yen Kan</a><br><a href=/volumes/W18-37/ class=text-muted>Proceedings of the 5th Workshop on Natural Language Processing Techniques for Educational Applications</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3720><div class="card-body p-3 small">We systematically confirm that instructors are strongly influenced by the user interface presentation of Massive Online Open Course (MOOC) discussion forums. In a large scale dataset, we conclusively show that instructor interventions exhibit strong position bias, as measured by the position where the thread appeared on the <a href=https://en.wikipedia.org/wiki/User_interface>user interface</a> at the time of intervention. We measure and remove this <a href=https://en.wikipedia.org/wiki/Bias>bias</a>, enabling unbiased statistical modelling and <a href=https://en.wikipedia.org/wiki/Evaluation>evaluation</a>. We show that our de-biased classifier improves predicting interventions over the <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a> on <a href=https://en.wikipedia.org/wiki/Course_(education)>courses</a> with sufficient number of <a href=https://en.wikipedia.org/wiki/Intervention_(counseling)>interventions</a> by 8.2 % in F1 and 24.4 % in <a href=https://en.wikipedia.org/wiki/Recall_(memory)>recall</a> on average.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1133.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1133 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1133 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1133.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-1133" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P18-1133/>Sequicity : Simplifying Task-oriented Dialogue Systems with Single Sequence-to-Sequence Architectures<span class=acl-fixed-case>S</span>equicity: Simplifying Task-oriented Dialogue Systems with Single Sequence-to-Sequence Architectures</a></strong><br><a href=/people/w/wenqiang-lei/>Wenqiang Lei</a>
|
<a href=/people/x/xisen-jin/>Xisen Jin</a>
|
<a href=/people/m/min-yen-kan/>Min-Yen Kan</a>
|
<a href=/people/z/zhaochun-ren/>Zhaochun Ren</a>
|
<a href=/people/x/xiangnan-he/>Xiangnan He</a>
|
<a href=/people/d/dawei-yin/>Dawei Yin</a><br><a href=/volumes/P18-1/ class=text-muted>Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1133><div class="card-body p-3 small">Existing solutions to task-oriented dialogue systems follow <a href=https://en.wikipedia.org/wiki/Pipeline_(software)>pipeline designs</a> which introduces <a href=https://en.wikipedia.org/wiki/Architecture>architectural complexity</a> and <a href=https://en.wikipedia.org/wiki/Fragility>fragility</a>. We propose a novel, holistic, extendable framework based on a single sequence-to-sequence (seq2seq) model which can be optimized with supervised or reinforcement learning. A key contribution is that we design text spans named belief spans to track dialogue believes, allowing task-oriented dialogue systems to be modeled in a seq2seq way. Based on this, we propose a simplistic Two Stage CopyNet instantiation which emonstrates good scalability : significantly reducing model complexity in terms of number of parameters and <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training time</a> by a magnitude. It significantly outperforms state-of-the-art pipeline-based methods on large datasets and retains a satisfactory entity match rate on out-of-vocabulary (OOV) cases where pipeline-designed competitors totally fail.</div></div><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1000.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-1000/>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></strong><br><a href=/people/r/regina-barzilay/>Regina Barzilay</a>
|
<a href=/people/m/min-yen-kan/>Min-Yen Kan</a><br><a href=/volumes/P17-1/ class=text-muted>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2000.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-2000/>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</a></strong><br><a href=/people/r/regina-barzilay/>Regina Barzilay</a>
|
<a href=/people/m/min-yen-kan/>Min-Yen Kan</a><br><a href=/volumes/P17-2/ class=text-muted>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5217.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5217 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5217 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5217/>Modeling Temporal Progression of Emotional Status in Mental Health Forum : A Recurrent Neural Net Approach</a></strong><br><a href=/people/k/kishaloy-halder/>Kishaloy Halder</a>
|
<a href=/people/l/lahari-poddar/>Lahari Poddar</a>
|
<a href=/people/m/min-yen-kan/>Min-Yen Kan</a><br><a href=/volumes/W17-52/ class=text-muted>Proceedings of the 8th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5217><div class="card-body p-3 small">Patients turn to <a href=https://en.wikipedia.org/wiki/Online_health_communities>Online Health Communities</a> not only for information on specific conditions but also for <a href=https://en.wikipedia.org/wiki/Emotional_support>emotional support</a>. Previous research has indicated that the progression of emotional status can be studied through the linguistic patterns of an individual&#8217;s posts. We analyze a real-world dataset from the Mental Health section of HealthBoards.com. Estimated from the word usages in their posts, we find that the emotional progress across patients vary widely. We study the problem of predicting a patient&#8217;s emotional status in the future from her past posts and we propose a Recurrent Neural Network (RNN) based architecture to address it. We find that the future emotional status can be predicted with reasonable accuracy given her historical posts and participation features. Our evaluation results demonstrate the efficacy of our proposed <a href=https://en.wikipedia.org/wiki/Software_architecture>architecture</a>, by outperforming <a href=https://en.wikipedia.org/wiki/Software_architecture>state-of-the-art approaches</a> with over 0.13 reduction in <a href=https://en.wikipedia.org/wiki/Mean_absolute_error>Mean Absolute Error</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S17-2170.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S17-2170 data-toggle=collapse aria-expanded=false aria-controls=abstract-S17-2170 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/S17-2170/>WING-NUS at SemEval-2017 Task 10 : Keyphrase Extraction and Classification as Joint Sequence Labeling<span class=acl-fixed-case>WING</span>-<span class=acl-fixed-case>NUS</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2017 Task 10: Keyphrase Extraction and Classification as Joint Sequence Labeling</a></strong><br><a href=/people/a/animesh-prasad/>Animesh Prasad</a>
|
<a href=/people/m/min-yen-kan/>Min-Yen Kan</a><br><a href=/volumes/S17-2/ class=text-muted>Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S17-2170><div class="card-body p-3 small">We describe an end-to-end pipeline processing approach for SemEval 2017&#8217;s Task 10 to extract keyphrases and their relations from scientific publications. We jointly identify and classify keyphrases by modeling the subtasks as sequential labeling. Our system utilizes standard, surface-level features along with the adjacent word features, and performs conditional decoding on whole text to extract <a href=https://en.wikipedia.org/wiki/Phrase>keyphrases</a>. We focus only on the identification and typing of keyphrases (Subtasks A and B, together referred as extraction), but provide an end-to-end system inclusive of keyphrase relation identification (Subtask C) for completeness. Our top performing configuration achieves an <a href=https://en.wikipedia.org/wiki/F-number>F_1</a> of 0.27 for the end-to-end keyphrase extraction and relation identification scenario on the final test data, and compares on par to other top ranked systems for <a href=https://en.wikipedia.org/wiki/Keyphrase_extraction>keyphrase extraction</a>. Our system outperforms other techniques that do not employ global decoding and hence do not account for dependencies between keyphrases. We believe this is crucial for keyphrase classification in the given context of scientific document mining.<tex-math>F_1</tex-math> of 0.27 for the end-to-end keyphrase extraction and relation identification scenario on the final test data, and compares on par to other top ranked systems for keyphrase extraction. Our system outperforms other techniques that do not employ global decoding and hence do not account for dependencies between keyphrases. We believe this is crucial for keyphrase classification in the given context of scientific document mining.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Min-Yen+Kan" title="Search for 'Min-Yen Kan' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/a/animesh-prasad/ class=align-middle>Animesh Prasad</a>
<span class="badge badge-secondary align-middle ml-2">5</span></li><li class=list-group-item><a href=/people/l/liangming-pan/ class=align-middle>Liangming Pan</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/w/wenhu-chen/ class=align-middle>Wenhu Chen</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/w/wenhan-xiong/ class=align-middle>Wenhan Xiong</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/w/william-yang-wang/ class=align-middle>William Yang Wang</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/r/regina-barzilay/ class=align-middle>Regina Barzilay</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/k/kishaloy-halder/ class=align-middle>Kishaloy Halder</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/w/wenqiang-lei/ class=align-middle>Wenqiang Lei</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/s/shenhao-jiang/ class=align-middle>Shenhao Jiang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/kazunari-sugiyama/ class=align-middle>Kazunari Sugiyama</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/i/ibrahim-taha-aksu/ class=align-middle>Ibrahim Taha Aksu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/zhengyuan-liu/ class=align-middle>Zhengyuan Liu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/n/nancy-chen/ class=align-middle>Nancy Chen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/lahari-poddar/ class=align-middle>Lahari Poddar</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/libo-qin/ class=align-middle>Libo Qin</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/q/qiguang-chen/ class=align-middle>Qiguang Chen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/tianbao-xie/ class=align-middle>Tianbao Xie</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/q/qixin-li/ class=align-middle>Qixin Li</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jian-guang-lou/ class=align-middle>Jian-Guang Lou</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/w/wanxiang-che/ class=align-middle>Wanxiang Che (车万翔)</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/abhinav-ramesh-kashyap/ class=align-middle>Abhinav Ramesh Kashyap</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/devamanyu-hazarika/ class=align-middle>Devamanyu Hazarika</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/roger-zimmermann/ class=align-middle>Roger Zimmermann</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/daniel-gildea/ class=align-middle>Daniel Gildea</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/n/nitin-madnani/ class=align-middle>Nitin Madnani</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/christoph-teichmann/ class=align-middle>Christoph Teichmann</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/martin-villalba/ class=align-middle>Martín Villalba</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/muthu-kumar-chandrasekaran/ class=align-middle>Muthu Kumar Chandrasekaran</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/chenglei-si/ class=align-middle>Chenglei Si</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yuxi-xie/ class=align-middle>Yuxi Xie</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/dongzhe-wang/ class=align-middle>Dongzhe Wang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yansong-feng/ class=align-middle>Yansong Feng</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jiaqi-li/ class=align-middle>Jiaqi Li</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/ming-liu/ class=align-middle>Ming Liu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/zihao-zheng/ class=align-middle>Zihao Zheng</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/zekun-wang/ class=align-middle>Zekun Wang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/ting-liu/ class=align-middle>Ting Liu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/b/bing-qin/ class=align-middle>Bing Qin</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/akshay-bhola/ class=align-middle>Akshay Bhola</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/x/xisen-jin/ class=align-middle>Xisen Jin</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/zhaochun-ren/ class=align-middle>Zhaochun Ren</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/x/xiangnan-he/ class=align-middle>Xiangnan He</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/dawei-yin/ class=align-middle>Dawei Yin</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/acl/ class=align-middle>ACL</a><span class="badge badge-secondary align-middle ml-2">5</span></li><li class=list-group-item><a href=/venues/coling/ class=align-middle>COLING</a><span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/venues/ws/ class=align-middle>WS</a><span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/venues/naacl/ class=align-middle>NAACL</a><span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/venues/sigdial/ class=align-middle>SIGDIAL</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-venues aria-expanded=false aria-controls=more-venues>show all...</li><div class="collapse border-top" id=more-venues><li class=list-group-item><a href=/venues/semeval/ class=align-middle>SemEval</a><span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>