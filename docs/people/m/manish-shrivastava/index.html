<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Manish Shrivastava - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Manish</span> <span class=font-weight-bold>Shrivastava</span></h2><hr><div class=row><div class=col-lg-9><h4>2022</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-acl.40.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-acl--40 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-acl.40 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.findings-acl.40/><span class=acl-fixed-case>S</span>y<span class=acl-fixed-case>MC</span>o<span class=acl-fixed-case>M</span> - Syntactic Measure of Code Mixing A Study Of <span class=acl-fixed-case>E</span>nglish-<span class=acl-fixed-case>H</span>indi Code-Mixing</a></strong><br><a href=/people/p/prashant-kodali/>Prashant Kodali</a>
|
<a href=/people/a/anmol-goel/>Anmol Goel</a>
|
<a href=/people/m/monojit-choudhury/>Monojit Choudhury</a>
|
<a href=/people/m/manish-shrivastava/>Manish Shrivastava</a>
|
<a href=/people/p/ponnurangam-kumaraguru/>Ponnurangam Kumaraguru</a><br><a href=/volumes/2022.findings-acl/ class=text-muted>Findings of the Association for Computational Linguistics: ACL 2022</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-acl--40><div class="card-body p-3 small">Code mixing is the linguistic phenomenon where bilingual speakers tend to switch between two or more languages in conversations. Recent work on code-mixing in computational settings has leveraged social media code mixed texts to train NLP models. For capturing the variety of code mixing in, and across corpus, Language ID (LID) tags based measures (CMI) have been proposed. Syntactical variety/patterns of code-mixing and their relationship vis-a-vis computational model&#8217;s performance is under explored. In this work, we investigate a collection of English(en)-Hindi(hi) code-mixed datasets from a syntactic lens to propose, <tex-math>SyMCoM</tex-math>, an indicator of syntactic variety in code-mixed text, with intuitive theoretical bounds. We train SoTA en-hi PoS tagger, accuracy of 93.4%, to reliably compute PoS tags on a corpus, and demonstrate the utility of <tex-math>SyMCoM</tex-math> by applying it on various syntactical categories on a collection of datasets, and compare datasets using the measure.</div></div><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.calcs-1.3.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--calcs-1--3 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.calcs-1.3 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.calcs-1.3" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.calcs-1.3/>Translate and Classify : Improving Sequence Level Classification for English-Hindi Code-Mixed Data<span class=acl-fixed-case>E</span>nglish-<span class=acl-fixed-case>H</span>indi Code-Mixed Data</a></strong><br><a href=/people/d/devansh-gautam/>Devansh Gautam</a>
|
<a href=/people/k/kshitij-gupta/>Kshitij Gupta</a>
|
<a href=/people/m/manish-shrivastava/>Manish Shrivastava</a><br><a href=/volumes/2021.calcs-1/ class=text-muted>Proceedings of the Fifth Workshop on Computational Approaches to Linguistic Code-Switching</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--calcs-1--3><div class="card-body p-3 small">Code-mixing is a common phenomenon in multilingual societies around the world and is especially common in social media texts. Traditional NLP systems, usually trained on monolingual corpora, do not perform well on code-mixed texts. Training specialized <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> for code-switched texts is difficult due to the lack of <a href=https://en.wikipedia.org/wiki/Data_set>large-scale datasets</a>. Translating code-mixed data into standard languages like <a href=https://en.wikipedia.org/wiki/English_language>English</a> could improve performance on various code-mixed tasks since we can use <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a> from state-of-the-art <a href=https://en.wikipedia.org/wiki/English_language>English models</a> for processing the translated data. This paper focuses on two sequence-level classification tasks for English-Hindi code mixed texts, which are part of the GLUECoS benchmark-Natural Language Inference and Sentiment Analysis. We propose using various pre-trained models that have been fine-tuned for similar English-only tasks and have shown state-of-the-art performance. We further fine-tune these models on the translated code-mixed datasets and achieve state-of-the-art performance in both tasks. To translate English-Hindi code-mixed data to English, we use mBART, a pre-trained multilingual sequence-to-sequence model that has shown competitive performance on various low-resource machine translation pairs and has also shown performance gains in languages that were not in its pre-training corpus.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.calcs-1.7.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--calcs-1--7 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.calcs-1.7 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.calcs-1.7" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.calcs-1.7/>CoMeT : Towards Code-Mixed Translation Using Parallel Monolingual Sentences<span class=acl-fixed-case>C</span>o<span class=acl-fixed-case>M</span>e<span class=acl-fixed-case>T</span>: Towards Code-Mixed Translation Using Parallel Monolingual Sentences</a></strong><br><a href=/people/d/devansh-gautam/>Devansh Gautam</a>
|
<a href=/people/p/prashant-kodali/>Prashant Kodali</a>
|
<a href=/people/k/kshitij-gupta/>Kshitij Gupta</a>
|
<a href=/people/a/anmol-goel/>Anmol Goel</a>
|
<a href=/people/m/manish-shrivastava/>Manish Shrivastava</a>
|
<a href=/people/p/ponnurangam-kumaraguru/>Ponnurangam Kumaraguru</a><br><a href=/volumes/2021.calcs-1/ class=text-muted>Proceedings of the Fifth Workshop on Computational Approaches to Linguistic Code-Switching</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--calcs-1--7><div class="card-body p-3 small">Code-mixed languages are very popular in multilingual societies around the world, yet the resources lag behind to enable robust systems on such <a href=https://en.wikipedia.org/wiki/Language>languages</a>. A major contributing factor is the informal nature of these languages which makes it difficult to collect code-mixed data. In this paper, we propose our system for Task 1 of CACLS 2021 to generate a <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation system</a> for English to Hinglish in a supervised setting. Translating in the given direction can help expand the set of resources for several tasks by translating valuable datasets from high resource languages. We propose to use mBART, a pre-trained multilingual sequence-to-sequence model, and fully utilize the pre-training of the <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> by transliterating the roman Hindi words in the code-mixed sentences to <a href=https://en.wikipedia.org/wiki/Devanagari>Devanagri script</a>. We evaluate how expanding the input by concatenating Hindi translations of the English sentences improves mBART&#8217;s performance. Our <a href=https://en.wikipedia.org/wiki/System>system</a> gives a BLEU score of 12.22 on test set. Further, we perform a detailed error analysis of our proposed systems and explore the limitations of the provided dataset and metrics.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.sdp-1.9.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--sdp-1--9 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.sdp-1.9 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.sdp-1.9/>The Effect of Pretraining on Extractive Summarization for Scientific Documents</a></strong><br><a href=/people/y/yash-gupta/>Yash Gupta</a>
|
<a href=/people/p/pawan-sasanka-ammanamanchi/>Pawan Sasanka Ammanamanchi</a>
|
<a href=/people/s/shikha-bordia/>Shikha Bordia</a>
|
<a href=/people/a/arjun-manoharan/>Arjun Manoharan</a>
|
<a href=/people/d/deepak-mittal/>Deepak Mittal</a>
|
<a href=/people/r/ramakanth-pasunuru/>Ramakanth Pasunuru</a>
|
<a href=/people/m/manish-shrivastava/>Manish Shrivastava</a>
|
<a href=/people/m/maneesh-singh/>Maneesh Singh</a>
|
<a href=/people/m/mohit-bansal/>Mohit Bansal</a>
|
<a href=/people/p/preethi-jyothi/>Preethi Jyothi</a><br><a href=/volumes/2021.sdp-1/ class=text-muted>Proceedings of the Second Workshop on Scholarly Document Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--sdp-1--9><div class="card-body p-3 small">Large pretrained models have seen enormous success in extractive summarization tasks. In this work, we investigate the influence of pretraining on a BERT-based extractive summarization system for <a href=https://en.wikipedia.org/wiki/Scientific_literature>scientific documents</a>. We derive significant performance improvements using an intermediate pretraining step that leverages existing summarization datasets and report state-of-the-art results on a recently released scientific summarization dataset, SciTLDR. We systematically analyze the intermediate pretraining step by varying the size and domain of the pretraining corpus, changing the length of the input sequence in the target task and varying target tasks. We also investigate how intermediate pretraining interacts with contextualized word embeddings trained on different domains.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.semeval-1.180.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--semeval-1--180 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.semeval-1.180 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.semeval-1.180" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.semeval-1.180/>Volta at SemEval-2021 Task 9 : Statement Verification and Evidence Finding with Tables using TAPAS and Transfer Learning<span class=acl-fixed-case>V</span>olta at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2021 Task 9: Statement Verification and Evidence Finding with Tables using <span class=acl-fixed-case>TAPAS</span> and Transfer Learning</a></strong><br><a href=/people/d/devansh-gautam/>Devansh Gautam</a>
|
<a href=/people/k/kshitij-gupta/>Kshitij Gupta</a>
|
<a href=/people/m/manish-shrivastava/>Manish Shrivastava</a><br><a href=/volumes/2021.semeval-1/ class=text-muted>Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--semeval-1--180><div class="card-body p-3 small">Tables are widely used in various kinds of documents to present information concisely. Understanding tables is a challenging problem that requires an understanding of language and table structure, along with numerical and logical reasoning. In this paper, we present our systems to solve Task 9 of SemEval-2021 : Statement Verification and Evidence Finding with Tables (SEM-TAB-FACTS). The task consists of two subtasks : (A) Given a table and a statement, predicting whether the table supports the statement and (B) Predicting which cells in the table provide evidence for / against the statement. We fine-tune TAPAS (a model which extends BERT&#8217;s architecture to capture tabular structure) for both the subtasks as it has shown state-of-the-art performance in various table understanding tasks. In subtask A, we evaluate how <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a> and standardizing tables to have a single header row improves TAPAS&#8217; performance. In subtask B, we evaluate how different fine-tuning strategies can improve TAPAS&#8217; performance. Our systems achieve an <a href=https://en.wikipedia.org/wiki/International_Federation_of_the_Phonographic_Industry>F1 score</a> of 67.34 in subtask A three-way classification, 72.89 in subtask A <a href=https://en.wikipedia.org/wiki/International_Federation_of_the_Phonographic_Industry>two-way classification</a>, and 62.95 in subtask B.</div></div><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.acl-srw.13.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--acl-srw--13 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.acl-srw.13 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://slideslive.com/38928649 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.acl-srw.13/>SCAR : Sentence Compression using Autoencoders for Reconstruction<span class=acl-fixed-case>SCAR</span>: Sentence Compression using Autoencoders for Reconstruction</a></strong><br><a href=/people/c/chanakya-malireddy/>Chanakya Malireddy</a>
|
<a href=/people/t/tirth-maniar/>Tirth Maniar</a>
|
<a href=/people/m/manish-shrivastava/>Manish Shrivastava</a><br><a href=/volumes/2020.acl-srw/ class=text-muted>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--acl-srw--13><div class="card-body p-3 small">Sentence compression is the task of shortening a sentence while retaining its meaning. Most methods proposed for this task rely on labeled or paired corpora (containing pairs of verbose and compressed sentences), which is often expensive to collect. To overcome this limitation, we present a novel unsupervised deep learning framework (SCAR) for deletion-based sentence compression. SCAR is primarily composed of two encoder-decoder pairs : a <a href=https://en.wikipedia.org/wiki/Dynamic_range_compression>compressor</a> and a <a href=https://en.wikipedia.org/wiki/Reconstructor>reconstructor</a>. The <a href=https://en.wikipedia.org/wiki/Dynamic_range_compression>compressor</a> masks the input, and the <a href=https://en.wikipedia.org/wiki/Reconstructor>reconstructor</a> tries to regenerate it. The <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> is entirely trained on unlabeled data and does not require additional inputs such as explicit syntactic information or optimal compression length. SCAR&#8217;s merit lies in the novel Linkage Loss function, which correlates the compressor and its effect on reconstruction, guiding it to drop inferable tokens. SCAR achieves higher ROUGE scores on benchmark datasets than the existing state-of-the-art methods and baselines. We also conduct a <a href=https://en.wikipedia.org/wiki/User_study>user study</a> to demonstrate the application of our <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> as a text highlighting system. Using our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> to underscore salient information facilitates speed-reading and reduces the time required to skim a document.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.157.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--157 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.157 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.157/>SIS@IIITH at SemEval-2020 Task 8 : An Overview of Simple Text Classification Methods for Meme Analysis<span class=acl-fixed-case>SIS</span>@<span class=acl-fixed-case>IIITH</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 8: An Overview of Simple Text Classification Methods for Meme Analysis</a></strong><br><a href=/people/s/sravani-boinepelli/>Sravani Boinepelli</a>
|
<a href=/people/m/manish-shrivastava/>Manish Shrivastava</a>
|
<a href=/people/v/vasudeva-varma/>Vasudeva Varma</a><br><a href=/volumes/2020.semeval-1/ class=text-muted>Proceedings of the Fourteenth Workshop on Semantic Evaluation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--157><div class="card-body p-3 small">Memes are steadily taking over the feeds of the public on <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>. There is always the threat of malicious users on the internet posting offensive content, even through <a href=https://en.wikipedia.org/wiki/Meme>memes</a>. Hence, the automatic detection of offensive images / memes is imperative along with detection of offensive text. However, this is a much more complex task as it involves both <a href=https://en.wikipedia.org/wiki/Sensory_cue>visual cues</a> as well as <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>language understanding</a> and <a href=https://en.wikipedia.org/wiki/Context_(language_use)>cultural / context knowledge</a>. This paper describes our approach to the task of SemEval-2020 Task 8 : Memotion Analysis. We chose to participate only in Task A which dealt with Sentiment Classification, which we formulated as a text classification problem. Through our experiments, we explored multiple training models to evaluate the performance of simple text classification algorithms on the raw text obtained after running <a href=https://en.wikipedia.org/wiki/Optical_character_recognition>OCR</a> on meme images. Our submitted <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieved an <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of 72.69 % and exceeded the existing baseline&#8217;s Macro F1 score by 8 % on the official test dataset. Apart from describing our official submission, we shall elucidate how different <a href=https://en.wikipedia.org/wiki/Statistical_model>classification models</a> respond to this task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.repl4nlp-1.4.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--repl4nlp-1--4 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.repl4nlp-1.4 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://slideslive.com/38929770 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.repl4nlp-1.4/>Word Embeddings as Tuples of Feature Probabilities</a></strong><br><a href=/people/s/siddharth-bhat/>Siddharth Bhat</a>
|
<a href=/people/a/alok-debnath/>Alok Debnath</a>
|
<a href=/people/s/souvik-banerjee/>Souvik Banerjee</a>
|
<a href=/people/m/manish-shrivastava/>Manish Shrivastava</a><br><a href=/volumes/2020.repl4nlp-1/ class=text-muted>Proceedings of the 5th Workshop on Representation Learning for NLP</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--repl4nlp-1--4><div class="card-body p-3 small">In this paper, we provide an alternate perspective on <a href=https://en.wikipedia.org/wiki/Word_(group_theory)>word representations</a>, by reinterpreting the dimensions of the vector space of a <a href=https://en.wikipedia.org/wiki/Word_embedding>word embedding</a> as a collection of <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a>. In this reinterpretation, every component of the word vector is normalized against all the word vectors in the vocabulary. This idea now allows us to view each vector as an n-tuple (akin to a fuzzy set), where n is the dimensionality of the word representation and each element represents the probability of the word possessing a <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>feature</a>. Indeed, this representation enables the use fuzzy set theoretic operations, such as <a href=https://en.wikipedia.org/wiki/Union_(set_theory)>union</a>, <a href=https://en.wikipedia.org/wiki/Intersection_(set_theory)>intersection</a> and <a href=https://en.wikipedia.org/wiki/Subtraction>difference</a>. Unlike previous attempts, we show that this representation of words provides a notion of similarity which is inherently asymmetric and hence closer to human similarity judgements. We compare the performance of this representation with various <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmarks</a>, and explore some of the unique properties including function word detection, detection of polysemous words, and some insight into the interpretability provided by set theoretic operations.<tex-math>n</tex-math>-tuple (akin to a fuzzy set), where <tex-math>n</tex-math> is the dimensionality of the word representation and each element represents the probability of the word possessing a feature. Indeed, this representation enables the use fuzzy set theoretic operations, such as union, intersection and difference. Unlike previous attempts, we show that this representation of words provides a notion of similarity which is inherently asymmetric and hence closer to human similarity judgements. We compare the performance of this representation with various benchmarks, and explore some of the unique properties including function word detection, detection of polysemous words, and some insight into the interpretability provided by set theoretic operations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.587.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--587 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.587 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.587/>Creation of Corpus and analysis in Code-Mixed Kannada-English Twitter data for Emotion Prediction<span class=acl-fixed-case>K</span>annada-<span class=acl-fixed-case>E</span>nglish <span class=acl-fixed-case>T</span>witter data for Emotion Prediction</a></strong><br><a href=/people/a/abhinav-reddy-appidi/>Abhinav Reddy Appidi</a>
|
<a href=/people/v/vamshi-krishna-srirangam/>Vamshi Krishna Srirangam</a>
|
<a href=/people/d/darsi-suhas/>Darsi Suhas</a>
|
<a href=/people/m/manish-shrivastava/>Manish Shrivastava</a><br><a href=/volumes/2020.coling-main/ class=text-muted>Proceedings of the 28th International Conference on Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--587><div class="card-body p-3 small">Emotion prediction is a critical task in the field of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>Natural Language Processing (NLP)</a>. There has been a significant amount of work done in emotion prediction for resource-rich languages. There has been work done on code-mixed social media corpus but not on emotion prediction of Kannada-English code-mixed Twitter data. In this paper, we analyze the problem of emotion prediction on <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> obtained from code-mixed Kannada-English extracted from <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a> annotated with their respective &#8216;Emotion&#8217; for each tweet. We experimented with machine learning prediction models using features like Character N-Grams, Word N-Grams, Repetitive characters, and others on SVM and LSTM on our corpus, which resulted in an accuracy of 30 % and 32 % respectively.</div></div><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2019.icon-1.5.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2019--icon-1--5 data-toggle=collapse aria-expanded=false aria-controls=abstract-2019.icon-1.5 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2019.icon-1.5/>Incorporating Sub-Word Level Information in Language Invariant Neural Event Detection</a></strong><br><a href=/people/s/suhan-prabhu/>Suhan Prabhu</a>
|
<a href=/people/p/pranav-goel/>Pranav Goel</a>
|
<a href=/people/a/alok-debnath/>Alok Debnath</a>
|
<a href=/people/m/manish-shrivastava/>Manish Shrivastava</a><br><a href=/volumes/2019.icon-1/ class=text-muted>Proceedings of the 16th International Conference on Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2019--icon-1--5><div class="card-body p-3 small">Detection of TimeML events in text have traditionally been done on <a href=https://en.wikipedia.org/wiki/Text_corpus>corpora</a> such as TimeBanks. However, <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning methods</a> have not been applied to these <a href=https://en.wikipedia.org/wiki/Corpus_linguistics>corpora</a>, because these <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> seldom contain more than 10,000 <a href=https://en.wikipedia.org/wiki/Event_(probability_theory)>event mentions</a>. Traditional architectures revolve around highly feature engineered, language specific statistical models. In this paper, we present a Language Invariant Neural Event Detection (ALINED) architecture. ALINED uses an aggregation of both sub-word level features as well as lexical and structural information. This is achieved by combining convolution over character embeddings, with recurrent layers over contextual word embeddings. We find that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> extracts relevant <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> for event span identification without relying on language specific features. We compare the performance of our language invariant model to the current state-of-the-art in <a href=https://en.wikipedia.org/wiki/English_language>English</a>, <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a>, <a href=https://en.wikipedia.org/wiki/Italian_language>Italian</a> and <a href=https://en.wikipedia.org/wiki/French_language>French</a>. We outperform the F1-score of the state of the art in <a href=https://en.wikipedia.org/wiki/English_language>English</a> by 1.65 points. We achieve <a href=https://en.wikipedia.org/wiki/International_Federation_of_the_Phonographic_Industry>F1-scores</a> of 84.96, 80.87 and 74.81 on <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a>, <a href=https://en.wikipedia.org/wiki/Italian_language>Italian</a> and <a href=https://en.wikipedia.org/wiki/French_language>French</a> respectively which is comparable to the current states of the art for these languages. We also introduce the automatic annotation of events in <a href=https://en.wikipedia.org/wiki/Hindi>Hindi</a>, a low resource language, with an F1-Score of 77.13.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2019.icon-1.6.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2019--icon-1--6 data-toggle=collapse aria-expanded=false aria-controls=abstract-2019.icon-1.6 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2019.icon-1.6/>Event Centric Entity Linking for Hindi News Articles : A Knowledge Graph Based Approach<span class=acl-fixed-case>H</span>indi News Articles: A Knowledge Graph Based Approach</a></strong><br><a href=/people/p/pranav-goel/>Pranav Goel</a>
|
<a href=/people/s/suhan-prabhu/>Suhan Prabhu</a>
|
<a href=/people/a/alok-debnath/>Alok Debnath</a>
|
<a href=/people/m/manish-shrivastava/>Manish Shrivastava</a><br><a href=/volumes/2019.icon-1/ class=text-muted>Proceedings of the 16th International Conference on Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2019--icon-1--6><div class="card-body p-3 small">We describe the development of a <a href=https://en.wikipedia.org/wiki/Knowledge_graph>knowledge graph</a> from an event annotated corpus by presenting a <a href=https://en.wikipedia.org/wiki/Pipeline_(software)>pipeline</a> that identifies and extracts the relations between entities and events from <a href=https://en.wikipedia.org/wiki/Hindi>Hindi news articles</a>. Due to the semantic implications of argument identification for events in <a href=https://en.wikipedia.org/wiki/Hindi>Hindi</a>, we use a combined syntactic argument and semantic role identification methodology. To the best of our knowledge, no other <a href=https://en.wikipedia.org/wiki/Architecture>architecture</a> exists for this purpose. The extracted combined role information is incorporated in a <a href=https://en.wikipedia.org/wiki/Knowledge_graph>knowledge graph</a> that can be queried via subgraph extraction for basic questions. The architectures presented in this paper can be used for participant extraction and event-entity linking in most <a href=https://en.wikipedia.org/wiki/Indo-Aryan_languages>Indo-Aryan languages</a>, due to similar syntactic and semantic properties of event arguments.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2019.icon-1.22.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2019--icon-1--22 data-toggle=collapse aria-expanded=false aria-controls=abstract-2019.icon-1.22 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2019.icon-1.22/>Kunji : A Resource Management System for Higher Productivity in Computer Aided Translation Tools</a></strong><br><a href=/people/p/priyank-gupta/>Priyank Gupta</a>
|
<a href=/people/m/manish-shrivastava/>Manish Shrivastava</a>
|
<a href=/people/d/dipti-misra-sharma/>Dipti Misra Sharma</a>
|
<a href=/people/r/rashid-ahmad/>Rashid Ahmad</a><br><a href=/volumes/2019.icon-1/ class=text-muted>Proceedings of the 16th International Conference on Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2019--icon-1--22><div class="card-body p-3 small">Complex NLP applications, such as machine translation systems, utilize various kinds of resources namely lexical, multiword, domain dictionaries, maps and rules etc. Similarly, translators working on Computer Aided Translation workbenches, also require help from various kinds of resources-glossaries, <a href=https://en.wikipedia.org/wiki/Terminology>terminologies</a>, <a href=https://en.wikipedia.org/wiki/Concordance_(publishing)>concordances</a> and <a href=https://en.wikipedia.org/wiki/Translation_memory>translation memory</a> in the workbenches in order to increase their productivity. Additionally, translators have to look away from the workbenches for linguistic resources like Named Entities, Multiwords, lexical and lexeme dictionaries in order to get help, as the available resources like concordances, terminologies and glossaries are often not enough. In this paper we present <a href=https://en.wikipedia.org/wiki/Kunji>Kunji</a>, a <a href=https://en.wikipedia.org/wiki/Resource_management_(computing)>resource management system</a> for translation workbenches and MT modules. This <a href=https://en.wikipedia.org/wiki/System>system</a> can be easily integrated in translation workbenches and can also be used as a management tool for resources for MT systems. The described <a href=https://en.wikipedia.org/wiki/Resource_management_(computing)>resource management system</a> has been integrated in a translation workbench Transzaar. We also study the impact of providing this <a href=https://en.wikipedia.org/wiki/Resource_management>resource management system</a> along with linguistic resources on the productivity of translators for English-Hindi language pair. When the linguistic resources like <a href=https://en.wikipedia.org/wiki/Lexeme>lexeme</a>, NER and MWE dictionaries were made available to translators in addition to their regular translation memories, <a href=https://en.wikipedia.org/wiki/Concordance_(publishing)>concordances</a> and <a href=https://en.wikipedia.org/wiki/Terminology>terminologies</a>, their productivity increased by 15.61 %.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5401.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5401 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5401 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5401/>Answering Naturally : Factoid to Full length Answer Generation</a></strong><br><a href=/people/v/vaishali-pal/>Vaishali Pal</a>
|
<a href=/people/m/manish-shrivastava/>Manish Shrivastava</a>
|
<a href=/people/i/irshad-bhat/>Irshad Bhat</a><br><a href=/volumes/D19-54/ class=text-muted>Proceedings of the 2nd Workshop on New Frontiers in Summarization</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5401><div class="card-body p-3 small">In recent years, the task of Question Answering over passages, also pitched as a <a href=https://en.wikipedia.org/wiki/Reading_comprehension>reading comprehension</a>, has evolved into a very active research area. A reading comprehension system extracts a span of text, comprising of <a href=https://en.wikipedia.org/wiki/Named_entity>named entities</a>, <a href=https://en.wikipedia.org/wiki/Calendar_date>dates</a>, <a href=https://en.wikipedia.org/wiki/Phrase>small phrases</a>, etc., which serve as the answer to a given question. However, these spans of text would result in an unnatural reading experience in a conversational system. Usually, <a href=https://en.wikipedia.org/wiki/Dialogue_system>dialogue systems</a> solve this issue by using template-based language generation. These <a href=https://en.wikipedia.org/wiki/System>systems</a>, though adequate for a domain specific task, are too restrictive and predefined for a domain independent system. In order to present the user with a more conversational experience, we propose a pointer generator based full-length answer generator which can be used with most QA systems. Our system generates a full length answer given a question and the extracted factoid / span answer without relying on the passage from where the answer was extracted. We also present a <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> of 315000 question, factoid answer and full length answer triples. We have evaluated our system using ROUGE-1,2,L and BLEU and achieved 74.05 BLEU score and 86.25 Rogue-L score.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5511.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5511 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5511 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5511/>Predicting Algorithm Classes for Programming Word Problems</a></strong><br><a href=/people/v/vinayak-athavale/>Vinayak Athavale</a>
|
<a href=/people/a/aayush-naik/>Aayush Naik</a>
|
<a href=/people/r/rajas-vanjape/>Rajas Vanjape</a>
|
<a href=/people/m/manish-shrivastava/>Manish Shrivastava</a><br><a href=/volumes/D19-55/ class=text-muted>Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT 2019)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5511><div class="card-body p-3 small">We introduce the task of algorithm class prediction for programming word problems. A programming word problem is a problem written in <a href=https://en.wikipedia.org/wiki/Natural_language>natural language</a>, which can be solved using an <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> or a <a href=https://en.wikipedia.org/wiki/Computer_program>program</a>. We define <a href=https://en.wikipedia.org/wiki/Class_(computer_programming)>classes</a> of various programming word problems which correspond to the class of algorithms required to solve the <a href=https://en.wikipedia.org/wiki/Problem_solving>problem</a>. We present four new <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> for this task, two multiclass datasets with 550 and 1159 problems each and two multilabel datasets having 3737 and 3960 problems each. We pose the problem as a text classification problem and train <a href=https://en.wikipedia.org/wiki/Neural_network>neural network</a> and non-neural network based models on this task. Our best performing <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifier</a> gets an <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of 62.7 percent for the multiclass case on the five class classification dataset, Codeforces Multiclass-5 (CFMC5). We also do some human-level analysis and compare human performance with that of our text classification models. Our best <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifier</a> has an <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> only 9 percent lower than that of a human on this task. To the best of our knowledge, these are the first reported results on such a <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. We make our code and datasets publicly available.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S19-2203.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S19-2203 data-toggle=collapse aria-expanded=false aria-controls=abstract-S19-2203 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/S19-2203/>Fermi at SemEval-2019 Task 8 : An elementary but effective approach to Question Discernment in Community QA Forums<span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2019 Task 8: An elementary but effective approach to Question Discernment in Community <span class=acl-fixed-case>QA</span> Forums</a></strong><br><a href=/people/b/bakhtiyar-syed/>Bakhtiyar Syed</a>
|
<a href=/people/v/vijayasaradhi-indurthi/>Vijayasaradhi Indurthi</a>
|
<a href=/people/m/manish-shrivastava/>Manish Shrivastava</a>
|
<a href=/people/m/manish-gupta/>Manish Gupta</a>
|
<a href=/people/v/vasudeva-varma/>Vasudeva Varma</a><br><a href=/volumes/S19-2/ class=text-muted>Proceedings of the 13th International Workshop on Semantic Evaluation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S19-2203><div class="card-body p-3 small">Online Community Question Answering Forums (cQA) have gained massive popularity within recent years. The rise in users for such <a href=https://en.wikipedia.org/wiki/Internet_forum>forums</a> have led to the increase in the need for automated evaluation for <a href=https://en.wikipedia.org/wiki/Sentence_processing>question comprehension</a> and <a href=https://en.wikipedia.org/wiki/Fact-checking>fact evaluation</a> of the answers provided by various participants in the forum. Our team, Fermi, participated in sub-task A of Task 8 at SemEval 2019-which tackles the first problem in the pipeline of factual evaluation in cQA forums, i.e., deciding whether a posed question asks for a factual information, an opinion / advice or is just socializing. This information is highly useful in segregating factual questions from non-factual ones which highly helps in organizing the questions into useful categories and trims down the problem space for the next task in the pipeline for fact evaluation among the available answers. Our system uses the embeddings obtained from Universal Sentence Encoder combined with <a href=https://en.wikipedia.org/wiki/XGBoost>XGBoost</a> for the classification sub-task A. We also evaluate other combinations of embeddings and off-the-shelf machine learning algorithms to demonstrate the efficacy of the various representations and their combinations. Our results across the evaluation test set gave an <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of 84 % and received the first position in the final standings judged by the organizers.<b>Fermi</b>, participated in sub-task A of Task 8 at SemEval 2019 - which tackles the first problem in the pipeline of factual evaluation in cQA forums, i.e., deciding whether a posed question asks for a factual information, an opinion/advice or is just socializing. This information is highly useful in segregating factual questions from non-factual ones which highly helps in organizing the questions into useful categories and trims down the problem space for the next task in the pipeline for fact evaluation among the available answers. Our system uses the embeddings obtained from Universal Sentence Encoder combined with XGBoost for the classification sub-task A. We also evaluate other combinations of embeddings and off-the-shelf machine learning algorithms to demonstrate the efficacy of the various representations and their combinations. Our results across the evaluation test set gave an accuracy of 84% and received the first position in the final standings judged by the organizers.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/R19-1063.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-R19-1063 data-toggle=collapse aria-expanded=false aria-controls=abstract-R19-1063 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/R19-1063/>Using Syntax to Resolve NPE in English<span class=acl-fixed-case>NPE</span> in <span class=acl-fixed-case>E</span>nglish</a></strong><br><a href=/people/p/payal-khullar/>Payal Khullar</a>
|
<a href=/people/a/allen-antony/>Allen Antony</a>
|
<a href=/people/m/manish-shrivastava/>Manish Shrivastava</a><br><a href=/volumes/R19-1/ class=text-muted>Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2019)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-R19-1063><div class="card-body p-3 small">This paper describes a novel, syntax-based system for automatic detection and resolution of Noun Phrase Ellipsis (NPE) in <a href=https://en.wikipedia.org/wiki/English_language>English</a>. The <a href=https://en.wikipedia.org/wiki/System>system</a> takes in free input English text, detects the site of nominal elision, and if present, selects potential antecedent candidates. The rules are built using the syntactic information on <a href=https://en.wikipedia.org/wiki/Ellipsis_(linguistics)>ellipsis</a> and its antecedent discussed in previous theoretical linguistics literature on NPE. Additionally, we prepare a curated dataset of 337 sentences from well-known, reliable sources, containing positive and negative samples of <a href=https://en.wikipedia.org/wiki/Positive_and_negative_predictive_values>NPE</a>. We split this <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> into two parts, and use one part to refine our rules and the other to test the performance of our final <a href=https://en.wikipedia.org/wiki/System>system</a>. We get an <a href=https://en.wikipedia.org/wiki/Fluorescence_in_situ_hybridization>F1-score</a> of 76.47 % for <a href=https://en.wikipedia.org/wiki/Fluorescence_in_situ_hybridization>detection</a> and 70.27 % for <a href=https://en.wikipedia.org/wiki/Fluorescence_in_situ_hybridization>NPE resolution</a> on the testset. To the best of our knowledge, ours is the first <a href=https://en.wikipedia.org/wiki/System>system</a> that detects and resolves <a href=https://en.wikipedia.org/wiki/Non-player_character>NPE</a> in <a href=https://en.wikipedia.org/wiki/English_language>English</a>. The curated dataset used for this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>, albeit small, covers a wide variety of NPE cases and will be made public for future work.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-2025.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-2025 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-2025 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-2025/>Corpus Creation and Analysis for Named Entity Recognition in Telugu-English Code-Mixed Social Media Data<span class=acl-fixed-case>T</span>elugu-<span class=acl-fixed-case>E</span>nglish Code-Mixed Social Media Data</a></strong><br><a href=/people/v/vamshi-krishna-srirangam/>Vamshi Krishna Srirangam</a>
|
<a href=/people/a/appidi-abhinav-reddy/>Appidi Abhinav Reddy</a>
|
<a href=/people/v/vinay-singh/>Vinay Singh</a>
|
<a href=/people/m/manish-shrivastava/>Manish Shrivastava</a><br><a href=/volumes/P19-2/ class=text-muted>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-2025><div class="card-body p-3 small">Named Entity Recognition(NER) is one of the important tasks in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>Natural Language Processing(NLP)</a> and also is a subtask of <a href=https://en.wikipedia.org/wiki/Information_extraction>Information Extraction</a>. In this paper we present our work on NER in Telugu-English code-mixed social media data. Code-Mixing, a progeny of <a href=https://en.wikipedia.org/wiki/Multilingualism>multilingualism</a> is a way in which multilingual people express themselves on <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> by using linguistics units from different languages within a sentence or speech context. Entity Extraction from social media data such as tweets(twitter) is in general difficult due to its informal nature, code-mixed data further complicates the problem due to its informal, unstructured and incomplete information. We present a Telugu-English code-mixed corpus with the corresponding named entity tags. The <a href=https://en.wikipedia.org/wiki/Named_entity>named entities</a> used to tag data are Person(&#8216;Per&#8217;), Organization(&#8216;Org&#8217;) and Location(&#8216;Loc&#8217;). We experimented with the machine learning models Conditional Random Fields(CRFs), <a href=https://en.wikipedia.org/wiki/Decision_tree_learning>Decision Trees</a> and BiLSTMs on our <a href=https://en.wikipedia.org/wiki/Corpus_linguistics>corpus</a> which resulted in a <a href=https://en.wikipedia.org/wiki/F-number>F1-score</a> of 0.96, 0.94 and 0.95 respectively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-2052.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-2052 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-2052 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-2052/>De-Mixing Sentiment from Code-Mixed Text</a></strong><br><a href=/people/y/yash-kumar-lal/>Yash Kumar Lal</a>
|
<a href=/people/v/vaibhav-kumar/>Vaibhav Kumar</a>
|
<a href=/people/m/mrinal-dhar/>Mrinal Dhar</a>
|
<a href=/people/m/manish-shrivastava/>Manish Shrivastava</a>
|
<a href=/people/p/philipp-koehn/>Philipp Koehn</a><br><a href=/volumes/P19-2/ class=text-muted>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-2052><div class="card-body p-3 small">Code-mixing is the phenomenon of mixing the vocabulary and syntax of multiple languages in the same sentence. It is an increasingly common occurrence in today&#8217;s multilingual society and poses a big challenge when encountered in different downstream tasks. In this paper, we present a hybrid architecture for the task of Sentiment Analysis of English-Hindi code-mixed data. Our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> consists of three components, each seeking to alleviate different issues. We first generate subword level representations for the sentences using a CNN architecture. The generated representations are used as inputs to a Dual Encoder Network which consists of two different BiLSTMs-the Collective and Specific Encoder. The Collective Encoder captures the overall sentiment of the sentence, while the Specific <a href=https://en.wikipedia.org/wiki/Encoder>Encoder</a> utilizes an attention mechanism in order to focus on individual sentiment-bearing sub-words. This, combined with a Feature Network consisting of orthographic features and specially trained <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a>, achieves state-of-the-art results-83.54 % <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> and 0.827 F1 score-on a <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmark dataset</a>.</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-2405.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-2405 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-2405 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-2405" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-2405/>Named Entity Recognition for Hindi-English Code-Mixed Social Media Text<span class=acl-fixed-case>H</span>indi-<span class=acl-fixed-case>E</span>nglish Code-Mixed Social Media Text</a></strong><br><a href=/people/v/vinay-singh/>Vinay Singh</a>
|
<a href=/people/d/deepanshu-vijay/>Deepanshu Vijay</a>
|
<a href=/people/s/syed-sarfaraz-akhtar/>Syed Sarfaraz Akhtar</a>
|
<a href=/people/m/manish-shrivastava/>Manish Shrivastava</a><br><a href=/volumes/W18-24/ class=text-muted>Proceedings of the Seventh Named Entities Workshop</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-2405><div class="card-body p-3 small">Named Entity Recognition (NER) is a major task in the field of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>Natural Language Processing (NLP)</a>, and also is a sub-task of <a href=https://en.wikipedia.org/wiki/Information_extraction>Information Extraction</a>. The challenge of NER for <a href=https://en.wikipedia.org/wiki/Twitter>tweets</a> lie in the insufficient information available in a tweet. There has been a significant amount of work done related to <a href=https://en.wikipedia.org/wiki/Entity_extraction>entity extraction</a>, but only for resource rich languages and domains such as <a href=https://en.wikipedia.org/wiki/News_agency>newswire</a>. Entity extraction is, in general, a challenging task for such an informal text, and code-mixed text further complicates the process with it&#8217;s unstructured and incomplete information. We propose experiments with different <a href=https://en.wikipedia.org/wiki/Statistical_classification>machine learning classification algorithms</a> with word, character and lexical features. The algorithms we experimented with are <a href=https://en.wikipedia.org/wiki/Decision_tree_learning>Decision tree</a>, <a href=https://en.wikipedia.org/wiki/Long-term_memory>Long Short-Term Memory (LSTM)</a>, and <a href=https://en.wikipedia.org/wiki/Conditional_random_field>Conditional Random Field (CRF)</a>. In this paper, we present a <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> for NER in Hindi-English Code-Mixed along with extensive experiments on our <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning models</a> which achieved the best <a href=https://en.wikipedia.org/wiki/F-number>f1-score</a> of 0.95 with both CRF and <a href=https://en.wikipedia.org/wiki/Linear_time-invariant_system>LSTM</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3205.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3205 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3205 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3205/>Transliteration Better than <a href=https://en.wikipedia.org/wiki/Translation>Translation</a>? Answering Code-mixed Questions over a Knowledge Base</a></strong><br><a href=/people/v/vishal-gupta/>Vishal Gupta</a>
|
<a href=/people/m/manoj-chinnakotla/>Manoj Chinnakotla</a>
|
<a href=/people/m/manish-shrivastava/>Manish Shrivastava</a><br><a href=/volumes/W18-32/ class=text-muted>Proceedings of the Third Workshop on Computational Approaches to Linguistic Code-Switching</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3205><div class="card-body p-3 small">Humans can learn multiple languages. If they know a fact in one language, they can answer a question in another language they understand. They can also answer Code-mix (CM) questions : questions which contain both languages. This behavior is attributed to the unique learning ability of humans. Our task aims to study if machines can achieve this. We demonstrate how effectively a <a href=https://en.wikipedia.org/wiki/Machine>machine</a> can answer CM questions. In this work, we adopt a two phase approach : candidate generation and candidate re-ranking to answer questions. We propose a Triplet-Siamese-Hybrid CNN (TSHCNN) to re-rank candidate answers. We show experiments on the SimpleQuestions dataset. Our network is trained only on English questions provided in this dataset and noisy Hindi translations of these questions and can answer English-Hindi CM questions effectively without the need of translation into English. Back-transliterated CM questions outperform their lexical and sentence level translated counterparts by 5 % & 35 % in accuracy respectively, highlighting the efficacy of our approach in a resource constrained setting.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3817.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3817 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3817 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3817/>Enabling Code-Mixed Translation : Parallel Corpus Creation and MT Augmentation Approach<span class=acl-fixed-case>MT</span> Augmentation Approach</a></strong><br><a href=/people/m/mrinal-dhar/>Mrinal Dhar</a>
|
<a href=/people/v/vaibhav-kumar/>Vaibhav Kumar</a>
|
<a href=/people/m/manish-shrivastava/>Manish Shrivastava</a><br><a href=/volumes/W18-38/ class=text-muted>Proceedings of the First Workshop on Linguistic Resources for Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3817><div class="card-body p-3 small">Code-mixing, use of two or more languages in a single sentence, is ubiquitous ; generated by multi-lingual speakers across the world. The phenomenon presents itself prominently in social media discourse. Consequently, there is a growing need for translating code-mixed hybrid language into standard languages. However, due to the lack of gold parallel data, existing <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation systems</a> fail to properly translate code-mixed text. In an effort to initiate the task of machine translation of code-mixed content, we present a newly created <a href=https://en.wikipedia.org/wiki/Parallel_text>parallel corpus</a> of code-mixed English-Hindi and <a href=https://en.wikipedia.org/wiki/English_language>English</a>. We selected previously available English-Hindi code-mixed data as a starting point for the creation of our parallel corpus. We then chose 4 human translators, fluent in both English and Hindi, for translating the 6088 code-mixed English-Hindi sentences to English. With the help of the created parallel corpus, we analyzed the structure of English-Hindi code-mixed data and present a technique to augment run-of-the-mill machine translation (MT) approaches that can help achieve superior translations without the need for specially designed translation systems. We present an augmentation pipeline for existing MT approaches, like Phrase Based MT (Moses) and Neural MT, to improve the translation of code-mixed text.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4413.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4413 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4413 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4413/>Degree based Classification of Harmful Speech using Twitter Data<span class=acl-fixed-case>T</span>witter Data</a></strong><br><a href=/people/s/sanjana-sharma/>Sanjana Sharma</a>
|
<a href=/people/s/saksham-agrawal/>Saksham Agrawal</a>
|
<a href=/people/m/manish-shrivastava/>Manish Shrivastava</a><br><a href=/volumes/W18-44/ class=text-muted>Proceedings of the First Workshop on Trolling, Aggression and Cyberbullying (TRAC-2018)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4413><div class="card-body p-3 small">Harmful speech has various forms and it has been plaguing the <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> in different ways. If we need to crackdown different degrees of <a href=https://en.wikipedia.org/wiki/Hate_speech>hate speech</a> and abusive behavior amongst it, the classification needs to be based on complex ramifications which needs to be defined and hold accountable for, other than racist, sexist or against some particular group and community. This paper primarily describes how we created an ontological classification of harmful speech based on degree of hateful intent and used it to annotate twitter data accordingly. The key contribution of this paper is the new <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> of tweets we created based on <a href=https://en.wikipedia.org/wiki/Ontology_(information_science)>ontological classes</a> and degrees of harmful speech found in the text. We also propose supervised classification system for recognizing these respective harmful speech classes in the texts hence. This serves as a preliminary work to lay down foundation on defining different classes of harmful speech and subsequent work will be done in making it&#8217;s automatic detection more robust and efficient.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5106.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5106 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5106 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-5106" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-5106/>Aggression Detection on Social Media Text Using Deep Neural Networks</a></strong><br><a href=/people/v/vinay-singh/>Vinay Singh</a>
|
<a href=/people/a/aman-varshney/>Aman Varshney</a>
|
<a href=/people/s/syed-sarfaraz-akhtar/>Syed Sarfaraz Akhtar</a>
|
<a href=/people/d/deepanshu-vijay/>Deepanshu Vijay</a>
|
<a href=/people/m/manish-shrivastava/>Manish Shrivastava</a><br><a href=/volumes/W18-51/ class=text-muted>Proceedings of the 2nd Workshop on Abusive Language Online (ALW2)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5106><div class="card-body p-3 small">In the past few years, bully and aggressive posts on <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> have grown significantly, causing serious consequences for victims / users of all demographics. Majority of the work in this field has been done for English only. In this paper, we introduce a <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning based classification system</a> for <a href=https://en.wikipedia.org/wiki/List_of_Facebook_features>Facebook posts</a> and comments of Hindi-English Code-Mixed text to detect the aggressive behaviour of / towards users. Our work focuses on text from users majorly in the Indian Subcontinent. The <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> that we used for our <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> is provided by <a href=https://en.wikipedia.org/wiki/TRAC-1>TRAC-1</a> in their shared task. Our <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification model</a> assigns each <a href=https://en.wikipedia.org/wiki/List_of_Facebook_features>Facebook post / comment</a> to one of the three predefined categories : Overtly Aggressive, Covertly Aggressive and Non-Aggressive. We experimented with 6 classification models and our CNN model on a 10 K-fold cross-validation gave the best result with the prediction accuracy of 73.2 %.<b>TRAC-1</b>in their shared task. Our classification model assigns each Facebook post/comment to one of the three predefined categories: &#8220;Overtly Aggressive&#8221;, &#8220;Covertly Aggressive&#8221; and &#8220;Non-Aggressive&#8221;. We experimented with 6 classification models and our CNN model on a 10 K-fold cross-validation gave the best result with the prediction accuracy of 73.2%.</div></div><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-2020.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-2020 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-2020 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-2020/>Injecting Word Embeddings with Another Languages Resource : An Application of Bilingual Embeddings</a></strong><br><a href=/people/p/prakhar-pandey/>Prakhar Pandey</a>
|
<a href=/people/v/vikram-pudi/>Vikram Pudi</a>
|
<a href=/people/m/manish-shrivastava/>Manish Shrivastava</a><br><a href=/volumes/I17-2/ class=text-muted>Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-2020><div class="card-body p-3 small">Word embeddings learned from <a href=https://en.wikipedia.org/wiki/Text_corpus>text corpus</a> can be improved by injecting knowledge from external resources, while at the same time also specializing them for similarity or relatedness. These knowledge resources (like WordNet, Paraphrase Database) may not exist for all languages. In this work we introduce a method to inject <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> of a language with knowledge resource of another language by leveraging bilingual embeddings. First we improve word embeddings of <a href=https://en.wikipedia.org/wiki/German_language>German</a>, <a href=https://en.wikipedia.org/wiki/Italian_language>Italian</a>, <a href=https://en.wikipedia.org/wiki/French_language>French</a> and <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a> using resources of English and test them on variety of word similarity tasks. Then we demonstrate the utility of our method by creating improved embeddings for <a href=https://en.wikipedia.org/wiki/Urdu>Urdu and Telugu languages</a> using Hindi WordNet, beating the previously established baseline for <a href=https://en.wikipedia.org/wiki/Urdu>Urdu</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-3017.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-3017 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-3017 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-3017/>Deep Neural Network based system for solving Arithmetic Word problems</a></strong><br><a href=/people/p/purvanshi-mehta/>Purvanshi Mehta</a>
|
<a href=/people/p/pruthwik-mishra/>Pruthwik Mishra</a>
|
<a href=/people/v/vinayak-athavale/>Vinayak Athavale</a>
|
<a href=/people/m/manish-shrivastava/>Manish Shrivastava</a>
|
<a href=/people/d/dipti-misra-sharma/>Dipti Sharma</a><br><a href=/volumes/I17-3/ class=text-muted>Proceedings of the IJCNLP 2017, System Demonstrations</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-3017><div class="card-body p-3 small">This paper presents DILTON a <a href=https://en.wikipedia.org/wiki/System>system</a> which solves simple arithmetic word problems. DILTON uses a Deep Neural based model to solve math word problems. DILTON divides the question into two parts-worldstate and query. The worldstate and the query are processed separately in two different <a href=https://en.wikipedia.org/wiki/Network_analysis_(electrical_circuits)>networks</a> and finally, the <a href=https://en.wikipedia.org/wiki/Network_analysis_(electrical_circuits)>networks</a> are merged to predict the final operation. We report the first deep learning approach for the prediction of operation between two numbers. DILTON learns to predict operations with 88.81 % <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> in a <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus of primary school questions</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-0811.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-0811 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-0811 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-0811/>Word Similarity Datasets for <a href=https://en.wikipedia.org/wiki/Languages_of_India>Indian Languages</a> : Annotation and Baseline Systems<span class=acl-fixed-case>I</span>ndian Languages: Annotation and Baseline Systems</a></strong><br><a href=/people/s/syed-sarfaraz-akhtar/>Syed Sarfaraz Akhtar</a>
|
<a href=/people/a/arihant-gupta/>Arihant Gupta</a>
|
<a href=/people/a/avijit-vajpayee/>Avijit Vajpayee</a>
|
<a href=/people/a/arjit-srivastava/>Arjit Srivastava</a>
|
<a href=/people/m/manish-shrivastava/>Manish Shrivastava</a><br><a href=/volumes/W17-08/ class=text-muted>Proceedings of the 11th Linguistic Annotation Workshop</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-0811><div class="card-body p-3 small">With the advent of <a href=https://en.wikipedia.org/wiki/Word_processor_(electronic_device)>word representations</a>, word similarity tasks are becoming increasing popular as an evaluation metric for the quality of the <a href=https://en.wikipedia.org/wiki/Word_processor_(electronic_device)>representations</a>. In this paper, we present manually annotated monolingual word similarity datasets of six Indian languages-Urdu, <a href=https://en.wikipedia.org/wiki/Telugu_language>Telugu</a>, <a href=https://en.wikipedia.org/wiki/Marathi_language>Marathi</a>, <a href=https://en.wikipedia.org/wiki/Punjabi_language>Punjabi</a>, <a href=https://en.wikipedia.org/wiki/Tamil_language>Tamil</a> and <a href=https://en.wikipedia.org/wiki/Gujarati_language>Gujarati</a>. These <a href=https://en.wikipedia.org/wiki/Language>languages</a> are most spoken Indian languages worldwide after <a href=https://en.wikipedia.org/wiki/Hindi>Hindi</a> and <a href=https://en.wikipedia.org/wiki/Bengali_language>Bengali</a>. For the construction of these <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>, our approach relies on translation and re-annotation of word similarity datasets of English. We also present baseline scores for word representation models using state-of-the-art techniques for <a href=https://en.wikipedia.org/wiki/Urdu>Urdu</a>, <a href=https://en.wikipedia.org/wiki/Telugu_language>Telugu</a> and <a href=https://en.wikipedia.org/wiki/Marathi_language>Marathi</a> by evaluating them on newly created word similarity datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1028.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1028 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1028 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1028/>Exploiting Morphological Regularities in Distributional Word Representations</a></strong><br><a href=/people/a/arihant-gupta/>Arihant Gupta</a>
|
<a href=/people/s/syed-sarfaraz-akhtar/>Syed Sarfaraz Akhtar</a>
|
<a href=/people/a/avijit-vajpayee/>Avijit Vajpayee</a>
|
<a href=/people/a/arjit-srivastava/>Arjit Srivastava</a>
|
<a href=/people/m/madan-gopal-jhanwar/>Madan Gopal Jhanwar</a>
|
<a href=/people/m/manish-shrivastava/>Manish Shrivastava</a><br><a href=/volumes/D17-1/ class=text-muted>Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1028><div class="card-body p-3 small">We present an unsupervised, language agnostic approach for exploiting morphological regularities present in high dimensional vector spaces. We propose a novel method for generating embeddings of words from their morphological variants using morphological transformation operators. We evaluate this approach on MSR word analogy test set with an <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of 85 % which is 12 % higher than the previous best known system.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1061.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1061 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1061 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=E17-1061" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/E17-1061/>Transition-Based Deep Input Linearization</a></strong><br><a href=/people/r/ratish-puduppully/>Ratish Puduppully</a>
|
<a href=/people/y/yue-zhang/>Yue Zhang</a>
|
<a href=/people/m/manish-shrivastava/>Manish Shrivastava</a><br><a href=/volumes/E17-1/ class=text-muted>Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1061><div class="card-body p-3 small">Traditional methods for deep NLG adopt pipeline approaches comprising stages such as constructing syntactic input, predicting function words, linearizing the syntactic input and generating the surface forms. Though easier to visualize, pipeline approaches suffer from error propagation. In addition, information available across modules can not be leveraged by all modules. We construct a transition-based model to jointly perform <a href=https://en.wikipedia.org/wiki/Linearization>linearization</a>, function word prediction and morphological generation, which considerably improves upon the accuracy compared to a pipelined baseline system. On a standard deep input linearization shared task, our <a href=https://en.wikipedia.org/wiki/System>system</a> achieves the best results reported so far.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2052.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2052 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2052 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2052/>Joining Hands : Exploiting Monolingual Treebanks for Parsing of Code-mixing Data</a></strong><br><a href=/people/i/irshad-bhat/>Irshad Bhat</a>
|
<a href=/people/r/riyaz-ahmad-bhat/>Riyaz A. Bhat</a>
|
<a href=/people/m/manish-shrivastava/>Manish Shrivastava</a>
|
<a href=/people/d/dipti-misra-sharma/>Dipti Sharma</a><br><a href=/volumes/E17-2/ class=text-muted>Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2052><div class="card-body p-3 small">In this paper, we propose efficient and less resource-intensive strategies for parsing of code-mixed data. These strategies are not constrained by in-domain annotations, rather they leverage pre-existing monolingual annotated resources for training. We show that these <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> can produce significantly better results as compared to an <a href=https://en.wikipedia.org/wiki/Baseline_(medicine)>informed baseline</a>. Due to lack of an evaluation set for code-mixed structures, we also present a data set of 450 Hindi and English code-mixed tweets of Hindi multilingual speakers for evaluation.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Manish+Shrivastava" title="Search for 'Manish Shrivastava' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/s/syed-sarfaraz-akhtar/ class=align-middle>Syed Sarfaraz Akhtar</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/people/d/dipti-misra-sharma/ class=align-middle>Dipti Misra Sharma</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/d/devansh-gautam/ class=align-middle>Devansh Gautam</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/k/kshitij-gupta/ class=align-middle>Kshitij Gupta</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/a/alok-debnath/ class=align-middle>Alok Debnath</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/v/vinay-singh/ class=align-middle>Vinay Singh</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/v/vinayak-athavale/ class=align-middle>Vinayak Athavale</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/p/prashant-kodali/ class=align-middle>Prashant Kodali</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/a/anmol-goel/ class=align-middle>Anmol Goel</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/p/ponnurangam-kumaraguru/ class=align-middle>Ponnurangam Kumaraguru</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/s/suhan-prabhu/ class=align-middle>Suhan Prabhu</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/p/pranav-goel/ class=align-middle>Pranav Goel</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/a/arihant-gupta/ class=align-middle>Arihant Gupta</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/a/avijit-vajpayee/ class=align-middle>Avijit Vajpayee</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/a/arjit-srivastava/ class=align-middle>Arjit Srivastava</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/v/vasudeva-varma/ class=align-middle>Vasudeva Varma</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/i/irshad-bhat/ class=align-middle>Irshad Bhat</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/d/deepanshu-vijay/ class=align-middle>Deepanshu Vijay</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/m/mrinal-dhar/ class=align-middle>Mrinal Dhar</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/v/vaibhav-kumar/ class=align-middle>Vaibhav Kumar</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/v/vamshi-krishna-srirangam/ class=align-middle>Vamshi Krishna Srirangam</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/p/prakhar-pandey/ class=align-middle>Prakhar Pandey</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/v/vikram-pudi/ class=align-middle>Vikram Pudi</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/p/purvanshi-mehta/ class=align-middle>Purvanshi Mehta</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/p/pruthwik-mishra/ class=align-middle>Pruthwik Mishra</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/chanakya-malireddy/ class=align-middle>Chanakya Malireddy</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/tirth-maniar/ class=align-middle>Tirth Maniar</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yash-gupta/ class=align-middle>Yash Gupta</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/p/pawan-sasanka-ammanamanchi/ class=align-middle>Pawan Sasanka Ammanamanchi</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/shikha-bordia/ class=align-middle>Shikha Bordia</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/arjun-manoharan/ class=align-middle>Arjun Manoharan</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/deepak-mittal/ class=align-middle>Deepak Mittal</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/ramakanth-pasunuru/ class=align-middle>Ramakanth Pasunuru</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/maneesh-singh/ class=align-middle>Maneesh Singh</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/mohit-bansal/ class=align-middle>Mohit Bansal</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/p/preethi-jyothi/ class=align-middle>Preethi Jyothi</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/p/priyank-gupta/ class=align-middle>Priyank Gupta</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/rashid-ahmad/ class=align-middle>Rashid Ahmad</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/sravani-boinepelli/ class=align-middle>Sravani Boinepelli</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/siddharth-bhat/ class=align-middle>Siddharth Bhat</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/souvik-banerjee/ class=align-middle>Souvik Banerjee</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/v/vaishali-pal/ class=align-middle>Vaishali Pal</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/aayush-naik/ class=align-middle>Aayush Naik</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/rajas-vanjape/ class=align-middle>Rajas Vanjape</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/monojit-choudhury/ class=align-middle>Monojit Choudhury</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/madan-gopal-jhanwar/ class=align-middle>Madan Gopal Jhanwar</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/b/bakhtiyar-syed/ class=align-middle>Bakhtiyar Syed</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/v/vijayasaradhi-indurthi/ class=align-middle>Vijayasaradhi Indurthi</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/manish-gupta/ class=align-middle>Manish Gupta</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/v/vishal-gupta/ class=align-middle>Vishal Gupta</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/manoj-chinnakotla/ class=align-middle>Manoj Chinnakotla</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/sanjana-sharma/ class=align-middle>Sanjana Sharma</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/saksham-agrawal/ class=align-middle>Saksham Agrawal</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/aman-varshney/ class=align-middle>Aman Varshney</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/abhinav-reddy-appidi/ class=align-middle>Abhinav Reddy Appidi</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/darsi-suhas/ class=align-middle>Darsi Suhas</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/p/payal-khullar/ class=align-middle>Payal Khullar</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/allen-antony/ class=align-middle>Allen Antony</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/ratish-puduppully/ class=align-middle>Ratish Puduppully</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yue-zhang/ class=align-middle>Yue Zhang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/riyaz-ahmad-bhat/ class=align-middle>Riyaz Ahmad Bhat</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/appidi-abhinav-reddy/ class=align-middle>Appidi Abhinav Reddy</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yash-kumar-lal/ class=align-middle>Yash Kumar Lal</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/p/philipp-koehn/ class=align-middle>Philipp Koehn</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/ws/ class=align-middle>WS</a><span class="badge badge-secondary align-middle ml-2">6</span></li><li class=list-group-item><a href=/venues/acl/ class=align-middle>ACL</a><span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/venues/icon/ class=align-middle>ICON</a><span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/venues/semeval/ class=align-middle>SemEval</a><span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/venues/emnlp/ class=align-middle>EMNLP</a><span class="badge badge-secondary align-middle ml-2">3</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-venues aria-expanded=false aria-controls=more-venues>show all...</li><div class="collapse border-top" id=more-venues><li class=list-group-item><a href=/venues/ijcnlp/ class=align-middle>IJCNLP</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/calcs/ class=align-middle>CALCS</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/eacl/ class=align-middle>EACL</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/sdp/ class=align-middle>sdp</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/repl4nlp/ class=align-middle>RepL4NLP</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/findings/ class=align-middle>Findings</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/coling/ class=align-middle>COLING</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/ranlp/ class=align-middle>RANLP</a><span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright &nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>