<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Ming Zhou - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Ming</span> <span class=font-weight-bold>Zhou</span></h2><hr><div class=row><div class=col-lg-9><h4>2022</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.acl-long.499.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--acl-long--499 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.acl-long.499 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.acl-long.499" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2022.acl-long.499/><span class=acl-fixed-case>U</span>ni<span class=acl-fixed-case>X</span>coder: Unified Cross-Modal Pre-training for Code Representation</a></strong><br><a href=/people/d/daya-guo/>Daya Guo</a>
|
<a href=/people/s/shuai-lu/>Shuai Lu</a>
|
<a href=/people/n/nan-duan/>Nan Duan</a>
|
<a href=/people/y/yanlin-wang/>Yanlin Wang</a>
|
<a href=/people/m/ming-zhou/>Ming Zhou</a>
|
<a href=/people/j/jian-yin/>Jian Yin</a><br><a href=/volumes/2022.acl-long/ class=text-muted>Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--acl-long--499><div class="card-body p-3 small">Pre-trained models for programming languages have recently demonstrated great success on code intelligence. To support both code-related understanding and generation tasks, recent works attempt to pre-train unified encoder-decoder models. However, such encoder-decoder framework is sub-optimal for auto-regressive tasks, especially code completion that requires a decoder-only manner for efficient inference. In this paper, we present UniXcoder, a unified cross-modal pre-trained model for programming language. The model utilizes mask attention matrices with prefix adapters to control the behavior of the model and leverages cross-modal contents like AST and code comment to enhance code representation. To encode AST that is represented as a tree in parallel, we propose a one-to-one mapping method to transform AST in a sequence structure that retains all structural information from the tree. Furthermore, we propose to utilize multi-modal contents to learn representation of code fragment with contrastive learning, and then align representations among programming languages using a cross-modal generation task. We evaluate UniXcoder on five code-related tasks over nine datasets. To further evaluate the performance of code fragment representation, we also construct a dataset for a new task, called zero-shot code-to-code search. Results show that our model achieves state-of-the-art performance on most tasks and analysis reveals that comment and AST can both enhance UniXcoder.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-acl.127.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-acl--127 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-acl.127 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2022.findings-acl.127.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.findings-acl.127" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2022.findings-acl.127/>Logic-Driven Context Extension and Data Augmentation for Logical Reasoning of Text</a></strong><br><a href=/people/s/siyuan-wang/>Siyuan Wang</a>
|
<a href=/people/w/wanjun-zhong/>Wanjun Zhong</a>
|
<a href=/people/d/duyu-tang/>Duyu Tang</a>
|
<a href=/people/z/zhongyu-wei/>Zhongyu Wei</a>
|
<a href=/people/z/zhihao-fan/>Zhihao Fan</a>
|
<a href=/people/d/daxin-jiang/>Daxin Jiang</a>
|
<a href=/people/m/ming-zhou/>Ming Zhou</a>
|
<a href=/people/n/nan-duan/>Nan Duan</a><br><a href=/volumes/2022.findings-acl/ class=text-muted>Findings of the Association for Computational Linguistics: ACL 2022</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-acl--127><div class="card-body p-3 small">Logical reasoning of text requires identifying critical logical structures in the text and performing inference over them. Existing methods for logical reasoning mainly focus on contextual semantics of text while struggling to explicitly model the logical inference process. In this paper, we not only put forward a logic-driven context extension framework but also propose a logic-driven data augmentation algorithm. The former follows a three-step reasoning paradigm, and each step is respectively to extract logical expressions as elementary reasoning units, symbolically infer the implicit expressions following equivalence laws and extend the context to validate the options. The latter augments literally similar but logically different instances and incorporates contrastive learning to better capture logical information, especially logical negative and conditional relationships. We conduct experiments on two benchmark datasets, ReClor and LogiQA. The results show that our method achieves state-of-the-art performance on both datasets, and even surpasses human performance on the ReClor dataset.</div></div><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.62.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--62 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.62 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.62" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.62/>Compare to The Knowledge : Graph Neural Fake News Detection with External Knowledge</a></strong><br><a href=/people/l/linmei-hu/>Linmei Hu</a>
|
<a href=/people/t/tianchi-yang/>Tianchi Yang</a>
|
<a href=/people/l/luhao-zhang/>Luhao Zhang</a>
|
<a href=/people/w/wanjun-zhong/>Wanjun Zhong</a>
|
<a href=/people/d/duyu-tang/>Duyu Tang</a>
|
<a href=/people/c/chuan-shi/>Chuan Shi</a>
|
<a href=/people/n/nan-duan/>Nan Duan</a>
|
<a href=/people/m/ming-zhou/>Ming Zhou</a><br><a href=/volumes/2021.acl-long/ class=text-muted>Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--62><div class="card-body p-3 small">Nowadays, fake news detection, which aims to verify whether a news document is trusted or fake, has become urgent and important. Most existing methods rely heavily on linguistic and semantic features from the news content, and fail to effectively exploit external knowledge which could help determine whether the news document is trusted. In this paper, we propose a novel end-to-end graph neural model called CompareNet, which compares the news to the knowledge base (KB) through entities for fake news detection. Considering that <a href=https://en.wikipedia.org/wiki/Fake_news>fake news detection</a> is correlated with topics, we also incorporate <a href=https://en.wikipedia.org/wiki/Topic_and_comment>topics</a> to enrich the news representation. Specifically, we first construct a directed heterogeneous document graph for each news incorporating topics and <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entities</a>. Based on the <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph</a>, we develop a heterogeneous graph attention network for learning the topic-enriched news representation as well as the contextual entity representations that encode the semantics of the news content. The contextual entity representations are then compared to the corresponding KB-based entity representations through a carefully designed entity comparison network, to capture the consistency between the news content and KB. Finally, the topic-enriched news representation combining the <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entity comparison features</a> is fed into a fake news classifier. Experimental results on two benchmark datasets demonstrate that <a href=https://en.wikipedia.org/wiki/CompareNet>CompareNet</a> significantly outperforms state-of-the-art methods.<i>directed heterogeneous document graph</i> for each news incorporating topics and entities. Based on the graph, we develop a <i>heterogeneous graph attention network</i> for learning the topic-enriched news representation as well as the contextual entity representations that encode the semantics of the news content. The contextual entity representations are then compared to the corresponding KB-based entity representations through a carefully designed <i>entity comparison network</i>, to capture the consistency between the news content and KB. Finally, the topic-enriched news representation combining the entity comparison features is fed into a fake news classifier. Experimental results on two benchmark datasets demonstrate that CompareNet significantly outperforms state-of-the-art methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.157.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--157 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.157 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.157/>Control Image Captioning Spatially and Temporally</a></strong><br><a href=/people/k/kun-yan/>Kun Yan</a>
|
<a href=/people/l/lei-ji/>Lei Ji</a>
|
<a href=/people/h/huaishao-luo/>Huaishao Luo</a>
|
<a href=/people/m/ming-zhou/>Ming Zhou</a>
|
<a href=/people/n/nan-duan/>Nan Duan</a>
|
<a href=/people/s/shuai-ma/>Shuai Ma</a><br><a href=/volumes/2021.acl-long/ class=text-muted>Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--157><div class="card-body p-3 small">Generating image captions with user intention is an emerging need. The recently published Localized Narratives dataset takes mouse traces as another input to the image captioning task, which is an intuitive and efficient way for a user to control what to describe in the image. However, how to effectively employ <a href=https://en.wikipedia.org/wiki/Trace_(disambiguation)>traces</a> to improve generation quality and <a href=https://en.wikipedia.org/wiki/Controllability>controllability</a> is still under exploration. This paper aims to solve this problem by proposing a novel model called LoopCAG, which connects Contrastive constraints and Attention Guidance in a Loop manner, engaged explicit spatial and temporal constraints to the generating process. Precisely, each generated sentence is temporally aligned to the corresponding <a href=https://en.wikipedia.org/wiki/Trace_(linear_algebra)>trace sequence</a> through a contrastive learning strategy. Besides, each generated text token is supervised to attend to the correct visual objects under heuristic spatial attention guidance. Comprehensive experimental results demonstrate that our LoopCAG model learns better correspondence among the three modalities (vision, language, and traces) and achieves SOTA performance on trace-controlled image captioning task. Moreover, the <a href=https://en.wikipedia.org/wiki/Controllability>controllability</a> and explainability of LoopCAG are validated by analyzing spatial and temporal sensitivity during the generation process.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.438.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--438 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.438 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.438" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.438/>Learning to Ask Conversational Questions by Optimizing Levenshtein Distance<span class=acl-fixed-case>L</span>evenshtein Distance</a></strong><br><a href=/people/z/zhongkun-liu/>Zhongkun Liu</a>
|
<a href=/people/p/pengjie-ren/>Pengjie Ren</a>
|
<a href=/people/z/zhumin-chen/>Zhumin Chen</a>
|
<a href=/people/z/zhaochun-ren/>Zhaochun Ren</a>
|
<a href=/people/m/maarten-de-rijke/>Maarten de Rijke</a>
|
<a href=/people/m/ming-zhou/>Ming Zhou</a><br><a href=/volumes/2021.acl-long/ class=text-muted>Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--438><div class="card-body p-3 small">Conversational Question Simplification (CQS) aims to simplify self-contained questions into conversational ones by incorporating some <a href=https://en.wikipedia.org/wiki/Interpersonal_relationship>conversational characteristics</a>, e.g., <a href=https://en.wikipedia.org/wiki/Anaphora_(rhetoric)>anaphora</a> and <a href=https://en.wikipedia.org/wiki/Ellipsis_(linguistics)>ellipsis</a>. Existing maximum likelihood estimation based methods often get trapped in easily learned tokens as all tokens are treated equally during training. In this work, we introduce a Reinforcement Iterative Sequence Editing (RISE) framework that optimizes the minimum Levenshtein distance through explicit editing actions. RISE is able to pay attention to tokens that are related to conversational characteristics. To train RISE, we devise an Iterative Reinforce Training (IRT) algorithm with a Dynamic Programming based Sampling (DPS) process to improve exploration. Experimental results on two benchmark datasets show that RISE significantly outperforms state-of-the-art methods and generalizes well on unseen data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.442.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--442 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.442 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.442" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.442/>CoSQA : 20,000 + Web Queries for Code Search and Question Answering<span class=acl-fixed-case>C</span>o<span class=acl-fixed-case>SQA</span>: 20,000+ Web Queries for Code Search and Question Answering</a></strong><br><a href=/people/j/junjie-huang/>Junjie Huang</a>
|
<a href=/people/d/duyu-tang/>Duyu Tang</a>
|
<a href=/people/l/linjun-shou/>Linjun Shou</a>
|
<a href=/people/m/ming-gong/>Ming Gong</a>
|
<a href=/people/k/ke-xu/>Ke Xu</a>
|
<a href=/people/d/daxin-jiang/>Daxin Jiang</a>
|
<a href=/people/m/ming-zhou/>Ming Zhou</a>
|
<a href=/people/n/nan-duan/>Nan Duan</a><br><a href=/volumes/2021.acl-long/ class=text-muted>Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--442><div class="card-body p-3 small">Finding codes given <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language query</a> is beneficial to the productivity of software developers. Future progress towards better <a href=https://en.wikipedia.org/wiki/Semantic_matching>semantic matching</a> between query and code requires richer supervised training resources. To remedy this, we introduce CoSQA dataset. It includes 20,604 labels for pairs of <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language queries</a> and codes, each annotated by at least 3 human annotators. We further introduce a contrastive learning method dubbed CoCLR to enhance text-code matching, which works as a data augmenter to bring more artificially generated training instances. We show that, evaluated on CodeXGLUE with the same CodeBERT model, training on CoSQA improves the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of code question answering by 5.1 % and incorporating CoCLR brings a further improvement of 10.5 %.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.771.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--771 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.771 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.771/>Jointly Learning to Repair Code and Generate Commit Message</a></strong><br><a href=/people/j/jiaqi-bai/>Jiaqi Bai</a>
|
<a href=/people/l/long-zhou/>Long Zhou</a>
|
<a href=/people/a/ambrosio-blanco/>Ambrosio Blanco</a>
|
<a href=/people/s/shujie-liu/>Shujie Liu</a>
|
<a href=/people/f/furu-wei/>Furu Wei</a>
|
<a href=/people/m/ming-zhou/>Ming Zhou</a>
|
<a href=/people/z/zhoujun-li/>Zhoujun Li</a><br><a href=/volumes/2021.emnlp-main/ class=text-muted>Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--771><div class="card-body p-3 small">We propose a novel <a href=https://en.wikipedia.org/wiki/Task_(computing)>task</a> of jointly repairing program codes and generating commit messages. Code repair and commit message generation are two essential and related tasks for <a href=https://en.wikipedia.org/wiki/Software_development>software development</a>. However, existing work usually performs the two <a href=https://en.wikipedia.org/wiki/Task_(computing)>tasks</a> independently. We construct a multilingual triple dataset including <a href=https://en.wikipedia.org/wiki/Software_bug>buggy code</a>, fixed code, and commit messages for this novel task. We first introduce a cascaded method with two models, one is to generate the fixed code first, and the other generates the commit message based on the fixed and original codes. We enhance the cascaded method with different training approaches, including the teacher-student method, the multi-task method, and the back-translation method. To deal with the error propagation problem of the cascaded method, we also propose a joint model that can both repair the <a href=https://en.wikipedia.org/wiki/Source_code>program code</a> and generate the commit message in a unified framework. Massive experiments on our constructed buggy-fixed-commit dataset reflect the challenge of this task and that the enhanced cascaded model and the proposed joint model significantly outperform baselines in both quality of code and commit messages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.280.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--280 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.280 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.naacl-main.280" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.280/>InfoXLM : An Information-Theoretic Framework for Cross-Lingual Language Model Pre-Training<span class=acl-fixed-case>I</span>nfo<span class=acl-fixed-case>XLM</span>: An Information-Theoretic Framework for Cross-Lingual Language Model Pre-Training</a></strong><br><a href=/people/z/zewen-chi/>Zewen Chi</a>
|
<a href=/people/l/li-dong/>Li Dong</a>
|
<a href=/people/f/furu-wei/>Furu Wei</a>
|
<a href=/people/n/nan-yang/>Nan Yang</a>
|
<a href=/people/s/saksham-singhal/>Saksham Singhal</a>
|
<a href=/people/w/wenhui-wang/>Wenhui Wang</a>
|
<a href=/people/x/xia-song/>Xia Song</a>
|
<a href=/people/x/xian-ling-mao/>Xian-Ling Mao</a>
|
<a href=/people/h/he-yan-huang/>Heyan Huang</a>
|
<a href=/people/m/ming-zhou/>Ming Zhou</a><br><a href=/volumes/2021.naacl-main/ class=text-muted>Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--280><div class="card-body p-3 small">In this work, we present an information-theoretic framework that formulates cross-lingual language model pre-training as maximizing <a href=https://en.wikipedia.org/wiki/Mutual_information>mutual information</a> between multilingual-multi-granularity texts. The unified view helps us to better understand the existing <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> for learning cross-lingual representations. More importantly, inspired by the <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a>, we propose a new pre-training task based on contrastive learning. Specifically, we regard a bilingual sentence pair as two views of the same meaning and encourage their encoded representations to be more similar than the negative examples. By leveraging both monolingual and parallel corpora, we jointly train the pretext tasks to improve the cross-lingual transferability of pre-trained models. Experimental results on several benchmarks show that our approach achieves considerably better performance. The <a href=https://en.wikipedia.org/wiki/Source_code>code</a> and pre-trained models are available at https://aka.ms/infoxlm.</div></div><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.193.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--193 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.193 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938858 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.193/>Neural Deepfake Detection with Factual Structure of Text</a></strong><br><a href=/people/w/wanjun-zhong/>Wanjun Zhong</a>
|
<a href=/people/d/duyu-tang/>Duyu Tang</a>
|
<a href=/people/z/zenan-xu/>Zenan Xu</a>
|
<a href=/people/r/ruize-wang/>Ruize Wang</a>
|
<a href=/people/n/nan-duan/>Nan Duan</a>
|
<a href=/people/m/ming-zhou/>Ming Zhou</a>
|
<a href=/people/j/jiahai-wang/>Jiahai Wang</a>
|
<a href=/people/j/jian-yin/>Jian Yin</a><br><a href=/volumes/2020.emnlp-main/ class=text-muted>Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--193><div class="card-body p-3 small">Deepfake detection, the task of automatically discriminating machine-generated text, is increasingly critical with recent advances in natural language generative models. Existing approaches to deepfake detection typically represent documents with coarse-grained representations. However, they struggle to capture factual structures of documents, which is a discriminative factor between machine-generated and human-written text according to our statistical analysis. To address this, we propose a graph-based model that utilizes the factual structure of a document for deepfake detection of text. Our approach represents the factual structure of a given document as an <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entity graph</a>, which is further utilized to learn <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>sentence representations</a> with a graph neural network. Sentence representations are then composed to a document representation for making predictions, where consistent relations between neighboring sentences are sequentially modeled. Results of experiments on two public deepfake datasets show that our approach significantly improves strong base models built with RoBERTa. Model analysis further indicates that our model can distinguish the difference in the factual structure between machine-generated text and <a href=https://en.wikipedia.org/wiki/Written_language>human-written text</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.acl-main.321.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--acl-main--321 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.acl-main.321 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://slideslive.com/38929020 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.acl-main.321/>A Simple and Effective Unified Encoder for Document-Level Machine Translation</a></strong><br><a href=/people/s/shuming-ma/>Shuming Ma</a>
|
<a href=/people/d/dongdong-zhang/>Dongdong Zhang</a>
|
<a href=/people/m/ming-zhou/>Ming Zhou</a><br><a href=/volumes/2020.acl-main/ class=text-muted>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--acl-main--321><div class="card-body p-3 small">Most of the existing <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> for document-level machine translation adopt dual-encoder structures. The representation of the source sentences and the document-level contexts are modeled with two separate <a href=https://en.wikipedia.org/wiki/Encoder>encoders</a>. Although these <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> can make use of the document-level contexts, they do not fully model the interaction between the contexts and the source sentences, and can not directly adapt to the recent pre-training models (e.g., BERT) which encodes multiple sentences with a single <a href=https://en.wikipedia.org/wiki/Encoder>encoder</a>. In this work, we propose a simple and effective unified encoder that can outperform the baseline models of dual-encoder models in terms of BLEU and METEOR scores. Moreover, the pre-training models can further boost the performance of our proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.acl-main.344.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--acl-main--344 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.acl-main.344 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://slideslive.com/38928753 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.acl-main.344/>Curriculum Pre-training for End-to-End Speech Translation</a></strong><br><a href=/people/c/chengyi-wang/>Chengyi Wang</a>
|
<a href=/people/y/yu-wu/>Yu Wu</a>
|
<a href=/people/s/shujie-liu/>Shujie Liu</a>
|
<a href=/people/m/ming-zhou/>Ming Zhou</a>
|
<a href=/people/z/zhenglu-yang/>Zhenglu Yang</a><br><a href=/volumes/2020.acl-main/ class=text-muted>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--acl-main--344><div class="card-body p-3 small">End-to-end speech translation poses a heavy burden on the <a href=https://en.wikipedia.org/wiki/Encoder>encoder</a> because it has to transcribe, understand, and learn cross-lingual semantics simultaneously. To obtain a powerful <a href=https://en.wikipedia.org/wiki/Encoder>encoder</a>, traditional methods pre-train it on <a href=https://en.wikipedia.org/wiki/Speech_recognition>ASR data</a> to capture <a href=https://en.wikipedia.org/wiki/Speech_recognition>speech features</a>. However, we argue that pre-training the <a href=https://en.wikipedia.org/wiki/Encoder>encoder</a> only through simple <a href=https://en.wikipedia.org/wiki/Speech_recognition>speech recognition</a> is not enough, and high-level linguistic knowledge should be considered. Inspired by this, we propose a curriculum pre-training method that includes an elementary course for transcription learning and two advanced courses for understanding the utterance and mapping words in two languages. The difficulty of these <a href=https://en.wikipedia.org/wiki/Course_(education)>courses</a> is gradually increasing. Experiments show that our curriculum pre-training method leads to significant improvements on En-De and En-Fr speech translation benchmarks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.acl-main.549.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--acl-main--549 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.acl-main.549 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://slideslive.com/38928866 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.acl-main.549/>Reasoning Over Semantic-Level Graph for Fact Checking</a></strong><br><a href=/people/w/wanjun-zhong/>Wanjun Zhong</a>
|
<a href=/people/j/jingjing-xu/>Jingjing Xu</a>
|
<a href=/people/d/duyu-tang/>Duyu Tang</a>
|
<a href=/people/z/zenan-xu/>Zenan Xu</a>
|
<a href=/people/n/nan-duan/>Nan Duan</a>
|
<a href=/people/m/ming-zhou/>Ming Zhou</a>
|
<a href=/people/j/jiahai-wang/>Jiahai Wang</a>
|
<a href=/people/j/jian-yin/>Jian Yin</a><br><a href=/volumes/2020.acl-main/ class=text-muted>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--acl-main--549><div class="card-body p-3 small">Fact checking is a challenging task because verifying the truthfulness of a claim requires reasoning about multiple retrievable evidence. In this work, we present a <a href=https://en.wikipedia.org/wiki/Methodology>method</a> suitable for reasoning about the semantic-level structure of evidence. Unlike most previous works, which typically represent evidence sentences with either string concatenation or fusing the features of isolated evidence sentences, our approach operates on rich semantic structures of evidence obtained by semantic role labeling. We propose two mechanisms to exploit the structure of evidence while leveraging the advances of pre-trained models like BERT, GPT or XLNet. Specifically, using XLNet as the backbone, we first utilize the <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph structure</a> to re-define the relative distances of words, with the intuition that semantically related words should have short distances. Then, we adopt graph convolutional network and graph attention network to propagate and aggregate information from neighboring nodes on the <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph</a>. We evaluate our system on FEVER, a benchmark dataset for <a href=https://en.wikipedia.org/wiki/Fact-checking>fact checking</a>, and find that rich structural information is helpful and both our graph-based mechanisms improve the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>. Our model is the state-of-the-art system in terms of both official evaluation metrics, namely claim verification accuracy and FEVER score.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.acl-main.599.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--acl-main--599 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.acl-main.599 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://slideslive.com/38928861 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.acl-main.599" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.acl-main.599/>Document Modeling with Graph Attention Networks for Multi-grained Machine Reading Comprehension</a></strong><br><a href=/people/b/bo-zheng/>Bo Zheng</a>
|
<a href=/people/h/haoyang-wen/>Haoyang Wen</a>
|
<a href=/people/y/yaobo-liang/>Yaobo Liang</a>
|
<a href=/people/n/nan-duan/>Nan Duan</a>
|
<a href=/people/w/wanxiang-che/>Wanxiang Che</a>
|
<a href=/people/d/daxin-jiang/>Daxin Jiang</a>
|
<a href=/people/m/ming-zhou/>Ming Zhou</a>
|
<a href=/people/t/ting-liu/>Ting Liu</a><br><a href=/volumes/2020.acl-main/ class=text-muted>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--acl-main--599><div class="card-body p-3 small">Natural Questions is a new challenging machine reading comprehension benchmark with two-grained answers, which are a long answer (typically a paragraph) and a short answer (one or more entities inside the long answer). Despite the effectiveness of existing methods on this <a href=https://en.wikipedia.org/wiki/Benchmarking>benchmark</a>, they treat these two sub-tasks individually during training while ignoring their dependencies. To address this issue, we present a novel multi-grained machine reading comprehension framework that focuses on modeling documents at their hierarchical nature, which are different levels of granularity : documents, paragraphs, sentences, and tokens. We utilize graph attention networks to obtain different levels of <a href=https://en.wikipedia.org/wiki/Representation_(mathematics)>representations</a> so that they can be learned simultaneously. The long and short answers can be extracted from paragraph-level representation and token-level representation, respectively. In this way, we can model the dependencies between the two-grained answers to provide evidence for each other. We jointly train the two sub-tasks, and our experiments show that our approach significantly outperforms previous systems at both long and short answer criteria.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.178.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--178 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.178 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.178/>Scheduled DropHead : A Regularization Method for Transformer Models<span class=acl-fixed-case>D</span>rop<span class=acl-fixed-case>H</span>ead: A Regularization Method for Transformer Models</a></strong><br><a href=/people/w/wangchunshu-zhou/>Wangchunshu Zhou</a>
|
<a href=/people/t/tao-ge/>Tao Ge</a>
|
<a href=/people/f/furu-wei/>Furu Wei</a>
|
<a href=/people/m/ming-zhou/>Ming Zhou</a>
|
<a href=/people/k/ke-xu/>Ke Xu</a><br><a href=/volumes/2020.findings-emnlp/ class=text-muted>Findings of the Association for Computational Linguistics: EMNLP 2020</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--178><div class="card-body p-3 small">We introduce DropHead, a structured dropout method specifically designed for regularizing the multi-head attention mechanism which is a key component of <a href=https://en.wikipedia.org/wiki/Transformer>transformer</a>. In contrast to the conventional dropout mechanism which randomly drops units or connections, DropHead drops entire <a href=https://en.wikipedia.org/wiki/Attentional_control>attention heads</a> during training to prevent the multi-head attention model from being dominated by a small portion of <a href=https://en.wikipedia.org/wiki/Attentional_control>attention heads</a>. It can help reduce the risk of <a href=https://en.wikipedia.org/wiki/Overfitting>overfitting</a> and allow the <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> to better benefit from the multi-head attention. Given the interaction between <a href=https://en.wikipedia.org/wiki/Multi-headedness>multi-headedness</a> and training dynamics, we further propose a novel dropout rate scheduler to adjust the dropout rate of DropHead throughout training, which results in a better regularization effect. Experimental results demonstrate that our proposed approach can improve transformer models by 0.9 BLEU score on WMT14 En-De translation task and around 1.0 <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> for various text classification tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.217.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--217 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.217 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.217/>ProphetNet : Predicting Future N-gram for Sequence-to-SequencePre-training<span class=acl-fixed-case>P</span>rophet<span class=acl-fixed-case>N</span>et: Predicting Future N-gram for Sequence-to-<span class=acl-fixed-case>S</span>equence<span class=acl-fixed-case>P</span>re-training</a></strong><br><a href=/people/w/weizhen-qi/>Weizhen Qi</a>
|
<a href=/people/y/yu-yan/>Yu Yan</a>
|
<a href=/people/y/yeyun-gong/>Yeyun Gong</a>
|
<a href=/people/d/dayiheng-liu/>Dayiheng Liu</a>
|
<a href=/people/n/nan-duan/>Nan Duan</a>
|
<a href=/people/j/jiusheng-chen/>Jiusheng Chen</a>
|
<a href=/people/r/ruofei-zhang/>Ruofei Zhang</a>
|
<a href=/people/m/ming-zhou/>Ming Zhou</a><br><a href=/volumes/2020.findings-emnlp/ class=text-muted>Findings of the Association for Computational Linguistics: EMNLP 2020</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--217><div class="card-body p-3 small">This paper presents a new sequence-to-sequence pre-training model called ProphetNet, which introduces a novel self-supervised objective named future n-gram prediction and the proposed n-stream self-attention mechanism. Instead of optimizing one-step-ahead prediction in the traditional sequence-to-sequence model, the ProphetNet is optimized by n-step ahead prediction that predicts the next n tokens simultaneously based on previous context tokens at each time step. The future n-gram prediction explicitly encourages the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> to plan for the future tokens and prevent <a href=https://en.wikipedia.org/wiki/Overfitting>overfitting</a> on strong local correlations. We pre-train ProphetNet using a base scale dataset (16 GB) and a large-scale dataset (160 GB), respectively. Then we conduct experiments on CNN / DailyMail, Gigaword, and SQuAD 1.1 benchmarks for abstractive summarization and question generation tasks. Experimental results show that ProphetNet achieves new state-of-the-art results on all these <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> compared to the <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> using the same scale pre-training corpus.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.82.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--82 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.82 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.coling-main.82" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.82/>DocBank : A Benchmark Dataset for <a href=https://en.wikipedia.org/wiki/Document_layout_analysis>Document Layout Analysis</a><span class=acl-fixed-case>D</span>oc<span class=acl-fixed-case>B</span>ank: A Benchmark Dataset for Document Layout Analysis</a></strong><br><a href=/people/m/minghao-li/>Minghao Li</a>
|
<a href=/people/y/yiheng-xu/>Yiheng Xu</a>
|
<a href=/people/l/lei-cui/>Lei Cui</a>
|
<a href=/people/s/shaohan-huang/>Shaohan Huang</a>
|
<a href=/people/f/furu-wei/>Furu Wei</a>
|
<a href=/people/z/zhoujun-li/>Zhoujun Li</a>
|
<a href=/people/m/ming-zhou/>Ming Zhou</a><br><a href=/volumes/2020.coling-main/ class=text-muted>Proceedings of the 28th International Conference on Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--82><div class="card-body p-3 small">Document layout analysis usually relies on <a href=https://en.wikipedia.org/wiki/Computer_vision>computer vision models</a> to understand documents while ignoring <a href=https://en.wikipedia.org/wiki/Text_(literary_theory)>textual information</a> that is vital to capture. Meanwhile, high quality labeled datasets with both visual and textual information are still insufficient. In this paper, we present DocBank, a benchmark dataset that contains 500 K document pages with fine-grained token-level annotations for <a href=https://en.wikipedia.org/wiki/Document_layout_analysis>document layout analysis</a>. DocBank is constructed using a simple yet effective way with weak supervision from the LaTeX documents available on the arXiv.com. With DocBank, models from different modalities can be compared fairly and multi-modal approaches will be further investigated and boost the performance of <a href=https://en.wikipedia.org/wiki/Document_layout_analysis>document layout analysis</a>. We build several strong baselines and manually split train / dev / test sets for evaluation. Experiment results show that <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> trained on DocBank accurately recognize the layout information for a variety of documents. The DocBank dataset is publicly available at https://github.com/doc-analysis/DocBank.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.236.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--236 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.236 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.lrec-1.236" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.236/>TableBank : Table Benchmark for Image-based Table Detection and Recognition<span class=acl-fixed-case>T</span>able<span class=acl-fixed-case>B</span>ank: Table Benchmark for Image-based Table Detection and Recognition</a></strong><br><a href=/people/m/minghao-li/>Minghao Li</a>
|
<a href=/people/l/lei-cui/>Lei Cui</a>
|
<a href=/people/s/shaohan-huang/>Shaohan Huang</a>
|
<a href=/people/f/furu-wei/>Furu Wei</a>
|
<a href=/people/m/ming-zhou/>Ming Zhou</a>
|
<a href=/people/z/zhoujun-li/>Zhoujun Li</a><br><a href=/volumes/2020.lrec-1/ class=text-muted>Proceedings of the 12th Language Resources and Evaluation Conference</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--236><div class="card-body p-3 small">We present TableBank, a new image-based table detection and recognition dataset built with novel weak supervision from Word and Latex documents on the internet. Existing research for image-based table detection and recognition usually fine-tunes pre-trained models on out-of-domain data with a few thousand human-labeled examples, which is difficult to generalize on real-world applications. With TableBank that contains 417 K high quality labeled tables, we build several strong baselines using state-of-the-art models with <a href=https://en.wikipedia.org/wiki/Deep_learning>deep neural networks</a>. We make TableBank publicly available and hope it will empower more deep learning approaches in the table detection and recognition task. The dataset and models can be downloaded from https://github.com/doc-analysis/TableBank.</div></div><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1071.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1071 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1071 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1071/>Explicit Cross-lingual Pre-training for Unsupervised Machine Translation</a></strong><br><a href=/people/s/shuo-ren/>Shuo Ren</a>
|
<a href=/people/y/yu-wu/>Yu Wu</a>
|
<a href=/people/s/shujie-liu/>Shujie Liu</a>
|
<a href=/people/m/ming-zhou/>Ming Zhou</a>
|
<a href=/people/s/shuai-ma/>Shuai Ma</a><br><a href=/volumes/D19-1/ class=text-muted>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1071><div class="card-body p-3 small">Pre-training has proven to be effective in unsupervised machine translation due to its ability to model deep context information in cross-lingual scenarios. However, the cross-lingual information obtained from shared BPE spaces is inexplicit and limited. In this paper, we propose a novel cross-lingual pre-training method for unsupervised machine translation by incorporating explicit cross-lingual training signals. Specifically, we first calculate cross-lingual n-gram embeddings and infer an n-gram translation table from them. With those n-gram translation pairs, we propose a new pre-training model called Cross-lingual Masked Language Model (CMLM), which randomly chooses source n-grams in the input text stream and predicts their translation candidates at each time step. Experiments show that our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> can incorporate beneficial cross-lingual information into pre-trained models. Taking pre-trained CMLM models as the encoder and decoder, we significantly improve the performance of unsupervised machine translation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5802.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5802 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5802 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5802/>Inspecting Unification of Encoding and Matching with Transformer : A Case Study of Machine Reading Comprehension</a></strong><br><a href=/people/h/hangbo-bao/>Hangbo Bao</a>
|
<a href=/people/l/li-dong/>Li Dong</a>
|
<a href=/people/f/furu-wei/>Furu Wei</a>
|
<a href=/people/w/wenhui-wang/>Wenhui Wang</a>
|
<a href=/people/n/nan-yang/>Nan Yang</a>
|
<a href=/people/l/lei-cui/>Lei Cui</a>
|
<a href=/people/s/songhao-piao/>Songhao Piao</a>
|
<a href=/people/m/ming-zhou/>Ming Zhou</a><br><a href=/volumes/D19-58/ class=text-muted>Proceedings of the 2nd Workshop on Machine Reading for Question Answering</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5802><div class="card-body p-3 small">Most machine reading comprehension (MRC) models separately handle encoding and matching with different <a href=https://en.wikipedia.org/wiki/Network_architecture>network architectures</a>. In contrast, pretrained language models with Transformer layers, such as GPT (Radford et al., 2018) and BERT (Devlin et al., 2018), have achieved competitive performance on MRC. A research question that naturally arises is : apart from the benefits of pre-training, how many performance gain comes from the unified network architecture. In this work, we evaluate and analyze unifying encoding and matching components with Transformer for the MRC task. Experimental results on SQuAD show that the <a href=https://en.wikipedia.org/wiki/Unified_Model>unified model</a> outperforms previous networks that separately treat <a href=https://en.wikipedia.org/wiki/Code>encoding</a> and <a href=https://en.wikipedia.org/wiki/Matching_(graph_theory)>matching</a>. We also introduce a <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metric</a> to inspect whether a Transformer layer tends to perform <a href=https://en.wikipedia.org/wiki/Code>encoding</a> or matching. The analysis results show that the <a href=https://en.wikipedia.org/wiki/Unified_model>unified model</a> learns different <a href=https://en.wikipedia.org/wiki/Mathematical_model>modeling strategies</a> compared with previous manually-designed models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/J19-1005.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-J19-1005 data-toggle=collapse aria-expanded=false aria-controls=abstract-J19-1005 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/J19-1005/>A Sequential Matching Framework for Multi-Turn Response Selection in Retrieval-Based Chatbots</a></strong><br><a href=/people/y/yu-wu/>Yu Wu</a>
|
<a href=/people/w/wei-wu/>Wei Wu</a>
|
<a href=/people/c/chen-xing/>Chen Xing</a>
|
<a href=/people/c/can-xu/>Can Xu</a>
|
<a href=/people/z/zhoujun-li/>Zhoujun Li</a>
|
<a href=/people/m/ming-zhou/>Ming Zhou</a><br><a href=/volumes/J19-1/ class=text-muted>Computational Linguistics, Volume 45, Issue 1 - March 2019</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-J19-1005><div class="card-body p-3 small">We study the problem of response selection for multi-turn conversation in retrieval-based chatbots. The task involves matching a response candidate with a conversation context, the challenges for which include how to recognize important parts of the context, and how to model the relationships among utterances in the context. Existing <a href=https://en.wikipedia.org/wiki/Matching_(graph_theory)>matching methods</a> may lose important information in contexts as we can interpret them with a unified framework in which contexts are transformed to fixed-length vectors without any interaction with responses before <a href=https://en.wikipedia.org/wiki/Matching_(graph_theory)>matching</a>. This motivates us to propose a new matching framework that can sufficiently carry important information in contexts to matching and model relationships among utterances at the same time. The new framework, which we call a sequential matching framework (SMF), lets each utterance in a context interact with a response candidate at the first step and transforms the pair to a matching vector. The matching vectors are then accumulated following the order of the utterances in the context with a <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent neural network (RNN)</a> that models relationships among utterances. Context-response matching is then calculated with the hidden states of the <a href=https://en.wikipedia.org/wiki/Radio-frequency_identification>RNN</a>. Under SMF, we propose a sequential convolutional network and sequential attention network and conduct experiments on two public data sets to test their performance. Experiment results show that both <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> can significantly outperform state-of-the-art matching methods. We also show that the <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> are interpretable with visualizations that provide us insights on how they capture and leverage important information in contexts for <a href=https://en.wikipedia.org/wiki/Matching_(statistics)>matching</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1328.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1328 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1328 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P19-1328.Supplementary.pdf data-toggle=tooltip data-placement=top title=Supplementary><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1328/>BERT-based Lexical Substitution<span class=acl-fixed-case>BERT</span>-based Lexical Substitution</a></strong><br><a href=/people/w/wangchunshu-zhou/>Wangchunshu Zhou</a>
|
<a href=/people/t/tao-ge/>Tao Ge</a>
|
<a href=/people/k/ke-xu/>Ke Xu</a>
|
<a href=/people/f/furu-wei/>Furu Wei</a>
|
<a href=/people/m/ming-zhou/>Ming Zhou</a><br><a href=/volumes/P19-1/ class=text-muted>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1328><div class="card-body p-3 small">Previous studies on <a href=https://en.wikipedia.org/wiki/Lexical_substitution>lexical substitution</a> tend to obtain substitute candidates by finding the target word&#8217;s synonyms from lexical resources (e.g., WordNet) and then rank the candidates based on its contexts. These approaches have two limitations : (1) They are likely to overlook good substitute candidates that are not the synonyms of the target words in the lexical resources ; (2) They fail to take into account the substitution&#8217;s influence on the global context of the sentence. To address these issues, we propose an end-to-end BERT-based lexical substitution approach which can propose and validate substitute candidates without using any annotated data or manually curated resources. Our approach first applies dropout to the target word&#8217;s embedding for partially masking the word, allowing BERT to take balanced consideration of the target word&#8217;s semantics and contexts for proposing substitute candidates, and then validates the candidates based on their substitution&#8217;s influence on the global contextualized representation of the sentence. Experiments show our approach performs well in both proposing and ranking substitute candidates, achieving the state-of-the-art results in both LS07 and LS14 benchmarks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1641.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1641 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1641 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-1641/>Dense Procedure Captioning in Narrated Instructional Videos</a></strong><br><a href=/people/b/botian-shi/>Botian Shi</a>
|
<a href=/people/l/lei-ji/>Lei Ji</a>
|
<a href=/people/y/yaobo-liang/>Yaobo Liang</a>
|
<a href=/people/n/nan-duan/>Nan Duan</a>
|
<a href=/people/p/peng-chen/>Peng Chen</a>
|
<a href=/people/z/zhendong-niu/>Zhendong Niu</a>
|
<a href=/people/m/ming-zhou/>Ming Zhou</a><br><a href=/volumes/P19-1/ class=text-muted>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1641><div class="card-body p-3 small">Understanding narrated instructional videos is important for both research and real-world web applications. Motivated by video dense captioning, we propose a model to generate procedure captions from narrated instructional videos which are a sequence of step-wise clips with description. Previous works on video dense captioning learn video segments and generate <a href=https://en.wikipedia.org/wiki/Closed_captioning>captions</a> without considering <a href=https://en.wikipedia.org/wiki/Transcript_(law)>transcripts</a>. We argue that transcripts in narrated instructional videos can enhance video representation by providing fine-grained complimentary and semantic textual information. In this paper, we introduce a framework to (1) extract procedures by a cross-modality module, which fuses video content with the entire transcript ; and (2) generate captions by encoding video frames as well as a snippet of transcripts within each extracted procedure. Experiments show that our model can achieve state-of-the-art performance in procedure extraction and captioning, and the ablation studies demonstrate that both the video frames and the transcripts are important for the task.</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1088.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1088 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1088 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-1088/>Neural Latent Extractive Document Summarization</a></strong><br><a href=/people/x/xingxing-zhang/>Xingxing Zhang</a>
|
<a href=/people/m/mirella-lapata/>Mirella Lapata</a>
|
<a href=/people/f/furu-wei/>Furu Wei</a>
|
<a href=/people/m/ming-zhou/>Ming Zhou</a><br><a href=/volumes/D18-1/ class=text-muted>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1088><div class="card-body p-3 small">Extractive summarization models need sentence level labels, which are usually created with rule-based methods since most summarization datasets only have document summary pairs. These labels might be suboptimal. We propose a latent variable extractive model, where sentences are viewed as <a href=https://en.wikipedia.org/wiki/Latent_variable>latent variables</a> and sentences with activated variables are used to infer gold summaries. During <a href=https://en.wikipedia.org/wiki/Training>training</a>, the loss can come directly from gold summaries. Experiments on CNN / Dailymail dataset show our latent extractive model outperforms a strong extractive baseline trained on rule-based labels and also performs competitively with several recent models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1188.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1188 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1188 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-1188/>Question Generation from SQL Queries Improves Neural Semantic Parsing<span class=acl-fixed-case>SQL</span> Queries Improves Neural Semantic Parsing</a></strong><br><a href=/people/d/daya-guo/>Daya Guo</a>
|
<a href=/people/y/yibo-sun/>Yibo Sun</a>
|
<a href=/people/d/duyu-tang/>Duyu Tang</a>
|
<a href=/people/n/nan-duan/>Nan Duan</a>
|
<a href=/people/j/jian-yin/>Jian Yin</a>
|
<a href=/people/h/hong-chi/>Hong Chi</a>
|
<a href=/people/j/james-cao/>James Cao</a>
|
<a href=/people/p/peng-chen/>Peng Chen</a>
|
<a href=/people/m/ming-zhou/>Ming Zhou</a><br><a href=/volumes/D18-1/ class=text-muted>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1188><div class="card-body p-3 small">In this paper, we study how to learn a <a href=https://en.wikipedia.org/wiki/Semantic_parser>semantic parser</a> of state-of-the-art <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> with less supervised training data. We conduct our study on WikiSQL, the largest hand-annotated semantic parsing dataset to date. First, we demonstrate that question generation is an effective method that empowers us to learn a state-of-the-art neural network based semantic parser with thirty percent of the supervised training data. Second, we show that applying question generation to the full supervised training data further improves the state-of-the-art model. In addition, we observe that there is a logarithmic relationship between the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of a <a href=https://en.wikipedia.org/wiki/Semantic_parser>semantic parser</a> and the amount of training data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1141.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1141 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1141 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-1141/>Learning to Collaborate for Question Answering and Asking</a></strong><br><a href=/people/d/duyu-tang/>Duyu Tang</a>
|
<a href=/people/n/nan-duan/>Nan Duan</a>
|
<a href=/people/z/zhao-yan/>Zhao Yan</a>
|
<a href=/people/z/zhirui-zhang/>Zhirui Zhang</a>
|
<a href=/people/y/yibo-sun/>Yibo Sun</a>
|
<a href=/people/s/shujie-liu/>Shujie Liu</a>
|
<a href=/people/y/yuanhua-lv/>Yuanhua Lv</a>
|
<a href=/people/m/ming-zhou/>Ming Zhou</a><br><a href=/volumes/N18-1/ class=text-muted>Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1141><div class="card-body p-3 small">Question answering (QA) and question generation (QG) are closely related <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> that could improve each other ; however, the connection of these two <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> is not well explored in literature. In this paper, we give a systematic study that seeks to leverage the connection to improve both QA and QG. We present a training algorithm that generalizes both Generative Adversarial Network (GAN) and Generative Domain-Adaptive Nets (GDAN) under the question answering scenario. The two key ideas are improving the QG model with QA through incorporating additional QA-specific signal as the <a href=https://en.wikipedia.org/wiki/Loss_function>loss function</a>, and improving the QA model with QG through adding artificially generated training instances. We conduct experiments on both document based and knowledge based question answering tasks. We have two main findings. Firstly, the performance of a QG model (e.g in terms of BLEU score) could be easily improved by a QA model via policy gradient. Secondly, directly applying GAN that regards all the generated questions as negative instances could not improve the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of the QA model. Learning when to regard generated questions as positive instances could bring performance boost.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1006.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1006 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1006 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285807823 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-1006/>Triangular Architecture for Rare Language Translation</a></strong><br><a href=/people/s/shuo-ren/>Shuo Ren</a>
|
<a href=/people/w/wenhu-chen/>Wenhu Chen</a>
|
<a href=/people/s/shujie-liu/>Shujie Liu</a>
|
<a href=/people/m/mu-li/>Mu Li</a>
|
<a href=/people/m/ming-zhou/>Ming Zhou</a>
|
<a href=/people/s/shuai-ma/>Shuai Ma</a><br><a href=/volumes/P18-1/ class=text-muted>Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1006><div class="card-body p-3 small">Neural Machine Translation (NMT) performs poor on the low-resource language pair (X, Z), especially when Z is a rare language. By introducing another rich language Y, we propose a novel triangular training architecture (TA-NMT) to leverage bilingual data (Y, Z) (may be small) and (X, Y) (can be rich) to improve the translation performance of low-resource pairs. In this triangular architecture, Z is taken as the intermediate latent variable, and translation models of Z are jointly optimized with an unified bidirectional EM algorithm under the goal of maximizing the translation likelihood of (X, Y). Empirical results demonstrate that our method significantly improves the translation quality of rare languages on MultiUN and IWSLT2012 datasets, and achieves even better performance combining back-translation methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1034.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1034 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1034 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P18-1034/>Semantic Parsing with Syntax- and Table-Aware SQL Generation<span class=acl-fixed-case>SQL</span> Generation</a></strong><br><a href=/people/y/yibo-sun/>Yibo Sun</a>
|
<a href=/people/d/duyu-tang/>Duyu Tang</a>
|
<a href=/people/n/nan-duan/>Nan Duan</a>
|
<a href=/people/j/jianshu-ji/>Jianshu Ji</a>
|
<a href=/people/g/guihong-cao/>Guihong Cao</a>
|
<a href=/people/x/xiaocheng-feng/>Xiaocheng Feng</a>
|
<a href=/people/b/bing-qin/>Bing Qin</a>
|
<a href=/people/t/ting-liu/>Ting Liu</a>
|
<a href=/people/m/ming-zhou/>Ming Zhou</a><br><a href=/volumes/P18-1/ class=text-muted>Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1034><div class="card-body p-3 small">We present a <a href=https://en.wikipedia.org/wiki/Generative_model>generative model</a> to map <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language questions</a> into <a href=https://en.wikipedia.org/wiki/SQL>SQL queries</a>. Existing neural network based approaches typically generate a SQL query word-by-word, however, a large portion of the generated results is incorrect or not executable due to the mismatch between question words and table contents. Our approach addresses this problem by considering the <a href=https://en.wikipedia.org/wiki/Table_(database)>structure of table</a> and the <a href=https://en.wikipedia.org/wiki/SQL>syntax of SQL language</a>. The quality of the generated SQL query is significantly improved through (1) learning to replicate content from column names, cells or SQL keywords ; and (2) improving the generation of WHERE clause by leveraging the column-cell relation. Experiments are conducted on WikiSQL, a recently released dataset with the largest question- SQL pairs. Our <a href=https://en.wikipedia.org/wiki/Scientific_method>approach</a> significantly improves the state-of-the-art <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>execution accuracy</a> from 69.0 % to 74.4 %.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2065.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2065 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2065 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285804053 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-2065/>Neural Open Information Extraction</a></strong><br><a href=/people/l/lei-cui/>Lei Cui</a>
|
<a href=/people/f/furu-wei/>Furu Wei</a>
|
<a href=/people/m/ming-zhou/>Ming Zhou</a><br><a href=/volumes/P18-2/ class=text-muted>Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2065><div class="card-body p-3 small">Conventional Open Information Extraction (Open IE) systems are usually built on hand-crafted patterns from other NLP tools such as <a href=https://en.wikipedia.org/wiki/Syntactic_parsing>syntactic parsing</a>, yet they face problems of <a href=https://en.wikipedia.org/wiki/Error_propagation>error propagation</a>. In this paper, we propose a neural Open IE approach with an encoder-decoder framework. Distinct from existing methods, the neural Open IE approach learns highly confident arguments and relation tuples bootstrapped from a state-of-the-art Open IE system. An empirical study on a large benchmark dataset shows that the neural Open IE system significantly outperforms several <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a>, while maintaining comparable computational efficiency.</div></div><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1018.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1018 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1018 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-1018/>Gated Self-Matching Networks for Reading Comprehension and Question Answering</a></strong><br><a href=/people/w/wenhui-wang/>Wenhui Wang</a>
|
<a href=/people/n/nan-yang/>Nan Yang</a>
|
<a href=/people/f/furu-wei/>Furu Wei</a>
|
<a href=/people/b/baobao-chang/>Baobao Chang</a>
|
<a href=/people/m/ming-zhou/>Ming Zhou</a><br><a href=/volumes/P17-1/ class=text-muted>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1018><div class="card-body p-3 small">In this paper, we present the gated self-matching networks for reading comprehension style question answering, which aims to answer questions from a given passage. We first match the question and passage with gated attention-based recurrent networks to obtain the question-aware passage representation. Then we propose a self-matching attention mechanism to refine the <a href=https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning>representation</a> by matching the passage against itself, which effectively encodes information from the whole passage. We finally employ the pointer networks to locate the positions of answers from the passages. We conduct extensive experiments on the SQuAD dataset. The <a href=https://en.wikipedia.org/wiki/Statistical_model>single model</a> achieves 71.3 % on the evaluation metrics of exact match on the hidden test set, while the <a href=https://en.wikipedia.org/wiki/Statistical_ensemble_(mathematical_physics)>ensemble model</a> further boosts the results to 75.9 %. At the time of submission of the paper, our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> holds the first place on the SQuAD leaderboard for both single and ensemble model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1046.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1046 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1046 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234953894 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P17-1046" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-1046/>Sequential Matching Network : A New Architecture for Multi-turn Response Selection in Retrieval-Based Chatbots</a></strong><br><a href=/people/y/yu-wu/>Yu Wu</a>
|
<a href=/people/w/wei-wu/>Wei Wu</a>
|
<a href=/people/c/chen-xing/>Chen Xing</a>
|
<a href=/people/m/ming-zhou/>Ming Zhou</a>
|
<a href=/people/z/zhoujun-li/>Zhoujun Li</a><br><a href=/volumes/P17-1/ class=text-muted>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1046><div class="card-body p-3 small">We study response selection for multi-turn conversation in retrieval based chatbots. Existing work either concatenates utterances in context or matches a response with a highly abstract context vector finally, which may lose relationships among the utterances or important information in the context. We propose a sequential matching network (SMN) to address both problems. SMN first matches a response with each utterance in the context on multiple levels of granularity, and distills important matching information from each pair as a vector with convolution and pooling operations. The vectors are then accumulated in a chronological order through a <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent neural network (RNN)</a> which models relationships among the utterances. The final matching score is calculated with the hidden states of the <a href=https://en.wikipedia.org/wiki/Random-access_memory>RNN</a>. Empirical study on two public data sets shows that SMN can significantly outperform state-of-the-art methods for response selection in multi-turn conversation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1101.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1101 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1101 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234956352 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P17-1101" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-1101/>Selective Encoding for Abstractive Sentence Summarization</a></strong><br><a href=/people/q/qingyu-zhou/>Qingyu Zhou</a>
|
<a href=/people/n/nan-yang/>Nan Yang</a>
|
<a href=/people/f/furu-wei/>Furu Wei</a>
|
<a href=/people/m/ming-zhou/>Ming Zhou</a><br><a href=/volumes/P17-1/ class=text-muted>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1101><div class="card-body p-3 small">We propose a selective encoding model to extend the sequence-to-sequence framework for abstractive sentence summarization. It consists of a sentence encoder, a selective gate network, and an attention equipped decoder. The sentence encoder and decoder are built with <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent neural networks</a>. The selective gate network constructs a second level sentence representation by controlling the <a href=https://en.wikipedia.org/wiki/Information_flow>information flow</a> from <a href=https://en.wikipedia.org/wiki/Encoder>encoder</a> to decoder. The second level representation is tailored for sentence summarization task, which leads to better performance. We evaluate our model on the English Gigaword, DUC 2004 and MSR abstractive sentence summarization datasets. The experimental results show that the proposed selective encoding model outperforms the state-of-the-art baseline models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1174.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1174 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1174 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-1174/>Chunk-based Decoder for <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a></a></strong><br><a href=/people/s/shonosuke-ishiwatari/>Shonosuke Ishiwatari</a>
|
<a href=/people/j/jingtao-yao/>Jingtao Yao</a>
|
<a href=/people/s/shujie-liu/>Shujie Liu</a>
|
<a href=/people/m/mu-li/>Mu Li</a>
|
<a href=/people/m/ming-zhou/>Ming Zhou</a>
|
<a href=/people/n/naoki-yoshinaga/>Naoki Yoshinaga</a>
|
<a href=/people/m/masaru-kitsuregawa/>Masaru Kitsuregawa</a>
|
<a href=/people/w/weijia-jia/>Weijia Jia</a><br><a href=/volumes/P17-1/ class=text-muted>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1174><div class="card-body p-3 small">Chunks (or phrases) once played a pivotal role in <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>. By using a chunk rather than a word as the basic translation unit, local (intra-chunk) and global (inter-chunk) word orders and dependencies can be easily modeled. The chunk structure, despite its importance, has not been considered in the <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>decoders</a> used for <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation (NMT)</a>. In this paper, we propose chunk-based decoders for (NMT), each of which consists of a chunk-level decoder and a word-level decoder. The chunk-level decoder models global dependencies while the word-level decoder decides the local word order in a chunk. To output a target sentence, the chunk-level decoder generates a chunk representation containing global information, which the word-level decoder then uses as a basis to predict the words inside the chunk. Experimental results show that our proposed decoders can significantly improve <a href=https://en.wikipedia.org/wiki/Translation>translation</a> performance in a WAT &#8216;16 English-to-Japanese translation task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S17-2045.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S17-2045 data-toggle=collapse aria-expanded=false aria-controls=abstract-S17-2045 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/S17-2045/>Beihang-MSRA at SemEval-2017 Task 3 : A Ranking System with Neural Matching Features for Community Question Answering<span class=acl-fixed-case>MSRA</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2017 Task 3: A Ranking System with Neural Matching Features for Community Question Answering</a></strong><br><a href=/people/w/wenzheng-feng/>Wenzheng Feng</a>
|
<a href=/people/y/yu-wu/>Yu Wu</a>
|
<a href=/people/w/wei-wu/>Wei Wu</a>
|
<a href=/people/z/zhoujun-li/>Zhoujun Li</a>
|
<a href=/people/m/ming-zhou/>Ming Zhou</a><br><a href=/volumes/S17-2/ class=text-muted>Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S17-2045><div class="card-body p-3 small">This paper presents the <a href=https://en.wikipedia.org/wiki/System>system</a> in SemEval-2017 Task 3, Community Question Answering (CQA). We develop a <a href=https://en.wikipedia.org/wiki/Ranking>ranking system</a> that is capable of capturing <a href=https://en.wikipedia.org/wiki/Semantics>semantic relations</a> between text pairs with little word overlap. In addition to traditional NLP features, we introduce several neural network based matching features which enable our system to measure text similarity beyond <a href=https://en.wikipedia.org/wiki/Lexicon>lexicons</a>. Our system significantly outperforms baseline methods and holds the second place in Subtask A and the fifth place in Subtask B, which demonstrates its efficacy on answer selection and question retrieval.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1007.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1007 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1007 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/238228743 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D17-1007/>Entity Linking for Queries by Searching Wikipedia Sentences<span class=acl-fixed-case>W</span>ikipedia Sentences</a></strong><br><a href=/people/c/chuanqi-tan/>Chuanqi Tan</a>
|
<a href=/people/f/furu-wei/>Furu Wei</a>
|
<a href=/people/p/pengjie-ren/>Pengjie Ren</a>
|
<a href=/people/w/weifeng-lv/>Weifeng Lv</a>
|
<a href=/people/m/ming-zhou/>Ming Zhou</a><br><a href=/volumes/D17-1/ class=text-muted>Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1007><div class="card-body p-3 small">We present a simple yet effective approach for linking entities in queries. The key idea is to search sentences similar to a query from Wikipedia articles and directly use the human-annotated entities in the similar sentences as candidate entities for the query. Then, we employ a rich set of features, such as link-probability, context-matching, word embeddings, and relatedness among candidate entities as well as their related entities, to rank the candidates under a regression based framework. The advantages of our <a href=https://en.wikipedia.org/wiki/Software_development_process>approach</a> lie in two aspects, which contribute to the ranking process and final linking result. First, it can greatly reduce the number of candidate entities by filtering out irrelevant entities with the words in the query. Second, we can obtain the query sensitive prior probability in addition to the static link-probability derived from all Wikipedia articles. We conduct experiments on two benchmark datasets on <a href=https://en.wikipedia.org/wiki/Entity_linking>entity linking</a> for queries, namely the ERD14 dataset and the GERDAQ dataset. Experimental results show that our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> outperforms <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art systems</a> and yields 75.0 % in F1 on the ERD14 dataset and 56.9 % on the GERDAQ dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1090.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1090 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1090 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1090/>Question Generation for Question Answering</a></strong><br><a href=/people/n/nan-duan/>Nan Duan</a>
|
<a href=/people/d/duyu-tang/>Duyu Tang</a>
|
<a href=/people/p/peng-chen/>Peng Chen</a>
|
<a href=/people/m/ming-zhou/>Ming Zhou</a><br><a href=/volumes/D17-1/ class=text-muted>Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1090><div class="card-body p-3 small">This paper presents how to generate questions from given passages using <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a>, where large scale QA pairs are automatically crawled and processed from Community-QA website, and used as training data. The contribution of the paper is 2-fold : First, two types of question generation approaches are proposed, one is a retrieval-based method using convolution neural network (CNN), the other is a generation-based method using recurrent neural network (RNN) ; Second, we show how to leverage the generated questions to improve existing question answering systems. We evaluate our question generation method for the answer sentence selection task on three benchmark datasets, including SQuAD, MS MARCO, and WikiQA. Experimental results show that, by using generated questions as an extra signal, significant <a href=https://en.wikipedia.org/wiki/Quality_assurance>QA</a> improvement can be achieved.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1175.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1175 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1175 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1175/>Stack-based Multi-layer Attention for Transition-based Dependency Parsing</a></strong><br><a href=/people/z/zhirui-zhang/>Zhirui Zhang</a>
|
<a href=/people/s/shujie-liu/>Shujie Liu</a>
|
<a href=/people/m/mu-li/>Mu Li</a>
|
<a href=/people/m/ming-zhou/>Ming Zhou</a>
|
<a href=/people/e/enhong-chen/>Enhong Chen</a><br><a href=/volumes/D17-1/ class=text-muted>Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1175><div class="card-body p-3 small">Although sequence-to-sequence (seq2seq) network has achieved significant success in many NLP tasks such as <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> and <a href=https://en.wikipedia.org/wiki/Automatic_summarization>text summarization</a>, simply applying this approach to transition-based dependency parsing can not yield a comparable performance gain as in other state-of-the-art methods, such as stack-LSTM and head selection. In this paper, we propose a stack-based multi-layer attention model for seq2seq learning to better leverage structural linguistics information. In our method, two binary vectors are used to track the decoding stack in transition-based parsing, and multi-layer attention is introduced to capture multiple word dependencies in partial trees. We conduct experiments on PTB and CTB datasets, and the results show that our proposed model achieves state-of-the-art accuracy and significant improvement in labeled precision with respect to the baseline seq2seq model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1059.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1059 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1059 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1059/>Learning to Generate Product Reviews from Attributes</a></strong><br><a href=/people/l/li-dong/>Li Dong</a>
|
<a href=/people/s/shaohan-huang/>Shaohan Huang</a>
|
<a href=/people/f/furu-wei/>Furu Wei</a>
|
<a href=/people/m/mirella-lapata/>Mirella Lapata</a>
|
<a href=/people/m/ming-zhou/>Ming Zhou</a>
|
<a href=/people/k/ke-xu/>Ke Xu</a><br><a href=/volumes/E17-1/ class=text-muted>Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1059><div class="card-body p-3 small">Automatically generating product reviews is a meaningful, yet not well-studied task in <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a>. Traditional natural language generation methods rely extensively on hand-crafted rules and predefined templates. This paper presents an attention-enhanced attribute-to-sequence model to generate product reviews for given attribute information, such as <a href=https://en.wikipedia.org/wiki/User_(computing)>user</a>, product, and rating. The attribute encoder learns to represent input attributes as vectors. Then, the sequence decoder generates reviews by conditioning its output on these vectors. We also introduce an <a href=https://en.wikipedia.org/wiki/Attentional_control>attention mechanism</a> to jointly generate <a href=https://en.wikipedia.org/wiki/Review_article>reviews</a> and align words with input attributes. The proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is trained end-to-end to maximize the likelihood of target product reviews given the <a href=https://en.wikipedia.org/wiki/Variable_and_attribute_(research)>attributes</a>. We build a publicly available dataset for the review generation task by leveraging the Amazon book reviews and their metadata. Experiments on the <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> show that our approach outperforms baseline methods and the <a href=https://en.wikipedia.org/wiki/Attentional_control>attention mechanism</a> significantly improves the performance of our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Ming+Zhou" title="Search for 'Ming Zhou' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/n/nan-duan/ class=align-middle>Nan Duan</a>
<span class="badge badge-secondary align-middle ml-2">14</span></li><li class=list-group-item><a href=/people/f/furu-wei/ class=align-middle>Furu Wei</a>
<span class="badge badge-secondary align-middle ml-2">13</span></li><li class=list-group-item><a href=/people/d/duyu-tang/ class=align-middle>Duyu Tang</a>
<span class="badge badge-secondary align-middle ml-2">9</span></li><li class=list-group-item><a href=/people/s/shujie-liu/ class=align-middle>Shujie Liu</a>
<span class="badge badge-secondary align-middle ml-2">7</span></li><li class=list-group-item><a href=/people/z/zhoujun-li/ class=align-middle>Zhoujun Li</a>
<span class="badge badge-secondary align-middle ml-2">6</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/y/yu-wu/ class=align-middle>Yu Wu</a>
<span class="badge badge-secondary align-middle ml-2">5</span></li><li class=list-group-item><a href=/people/w/wanjun-zhong/ class=align-middle>Wanjun Zhong</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/people/k/ke-xu/ class=align-middle>Ke Xu</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/people/j/jian-yin/ class=align-middle>Jian Yin</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/people/n/nan-yang/ class=align-middle>Nan Yang</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/people/l/lei-cui/ class=align-middle>Lei Cui</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/people/s/shuai-ma/ class=align-middle>Shuai Ma</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/d/daxin-jiang/ class=align-middle>Daxin Jiang</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/w/wenhui-wang/ class=align-middle>Wenhui Wang</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/w/wei-wu/ class=align-middle>Wei Wu</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/m/mu-li/ class=align-middle>Mu Li</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/y/yibo-sun/ class=align-middle>Yibo Sun</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/p/peng-chen/ class=align-middle>Peng Chen</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/l/li-dong/ class=align-middle>Li Dong</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/s/shaohan-huang/ class=align-middle>Shaohan Huang</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/l/lei-ji/ class=align-middle>Lei Ji</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/p/pengjie-ren/ class=align-middle>Pengjie Ren</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/z/zenan-xu/ class=align-middle>Zenan Xu</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/j/jiahai-wang/ class=align-middle>Jiahai Wang</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/y/yaobo-liang/ class=align-middle>Yaobo Liang</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/t/ting-liu/ class=align-middle>Ting Liu</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/c/chen-xing/ class=align-middle>Chen Xing</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/d/daya-guo/ class=align-middle>Daya Guo</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/m/mirella-lapata/ class=align-middle>Mirella Lapata</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/s/shuo-ren/ class=align-middle>Shuo Ren</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/z/zhirui-zhang/ class=align-middle>Zhirui Zhang</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/w/wangchunshu-zhou/ class=align-middle>Wangchunshu Zhou</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/t/tao-ge/ class=align-middle>Tao Ge</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/m/minghao-li/ class=align-middle>Minghao Li</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/l/linmei-hu/ class=align-middle>Linmei Hu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/tianchi-yang/ class=align-middle>Tianchi Yang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/luhao-zhang/ class=align-middle>Luhao Zhang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/chuan-shi/ class=align-middle>Chuan Shi</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/kun-yan/ class=align-middle>Kun Yan</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/huaishao-luo/ class=align-middle>Huaishao Luo</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/zhongkun-liu/ class=align-middle>Zhongkun Liu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/zhumin-chen/ class=align-middle>Zhumin Chen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/zhaochun-ren/ class=align-middle>Zhaochun Ren</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/maarten-de-rijke/ class=align-middle>Maarten de Rijke</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/junjie-huang/ class=align-middle>Junjie Huang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/linjun-shou/ class=align-middle>Linjun Shou</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/ming-gong/ class=align-middle>Ming Gong</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/ruize-wang/ class=align-middle>Ruize Wang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/shuming-ma/ class=align-middle>Shuming Ma</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/dongdong-zhang/ class=align-middle>Dongdong Zhang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/chengyi-wang/ class=align-middle>Chengyi Wang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/zhenglu-yang/ class=align-middle>Zhenglu Yang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jingjing-xu/ class=align-middle>Jingjing Xu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/b/bo-zheng/ class=align-middle>Bo Zheng</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/haoyang-wen/ class=align-middle>Haoyang Wen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/w/wanxiang-che/ class=align-middle>Wanxiang Che ()</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/b/baobao-chang/ class=align-middle>Baobao Chang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/q/qingyu-zhou/ class=align-middle>Qingyu Zhou</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/shonosuke-ishiwatari/ class=align-middle>Shonosuke Ishiwatari</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jingtao-yao/ class=align-middle>Jingtao Yao</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/n/naoki-yoshinaga/ class=align-middle>Naoki Yoshinaga</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/masaru-kitsuregawa/ class=align-middle>Masaru Kitsuregawa</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/w/weijia-jia/ class=align-middle>Weijia Jia</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/shuai-lu/ class=align-middle>Shuai Lu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yanlin-wang/ class=align-middle>Yanlin Wang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/x/xingxing-zhang/ class=align-middle>Xingxing Zhang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/hong-chi/ class=align-middle>Hong Chi</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/james-cao/ class=align-middle>James Cao</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jiaqi-bai/ class=align-middle>Jiaqi Bai</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/long-zhou/ class=align-middle>Long Zhou</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/ambrosio-blanco/ class=align-middle>Ambrosio Blanco</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/hangbo-bao/ class=align-middle>Hangbo Bao</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/songhao-piao/ class=align-middle>Songhao Piao</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/w/wenzheng-feng/ class=align-middle>Wenzheng Feng</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/siyuan-wang/ class=align-middle>Siyuan Wang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/zhongyu-wei/ class=align-middle>Zhongyu Wei</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/zhihao-fan/ class=align-middle>Zhihao Fan</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/chuanqi-tan/ class=align-middle>Chuanqi Tan</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/w/weifeng-lv/ class=align-middle>Weifeng Lv</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/e/enhong-chen/ class=align-middle>Enhong Chen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/can-xu/ class=align-middle>Can Xu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/zewen-chi/ class=align-middle>Zewen Chi</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/saksham-singhal/ class=align-middle>Saksham Singhal</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/x/xia-song/ class=align-middle>Xia Song</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/x/xian-ling-mao/ class=align-middle>Xian-Ling Mao</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/he-yan-huang/ class=align-middle>He-Yan Huang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/w/weizhen-qi/ class=align-middle>Weizhen Qi</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yu-yan/ class=align-middle>Yu Yan</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yeyun-gong/ class=align-middle>Yeyun Gong</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/dayiheng-liu/ class=align-middle>Dayiheng Liu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jiusheng-chen/ class=align-middle>Jiusheng Chen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/ruofei-zhang/ class=align-middle>Ruofei Zhang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yiheng-xu/ class=align-middle>Yiheng Xu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/zhao-yan/ class=align-middle>Zhao Yan</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yuanhua-lv/ class=align-middle>Yuanhua Lv</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/w/wenhu-chen/ class=align-middle>Wenhu Chen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jianshu-ji/ class=align-middle>Jianshu Ji</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/g/guihong-cao/ class=align-middle>Guihong Cao</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/x/xiaocheng-feng/ class=align-middle>Xiaocheng Feng</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/b/bing-qin/ class=align-middle>Bing Qin</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/b/botian-shi/ class=align-middle>Botian Shi</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/zhendong-niu/ class=align-middle>Zhendong Niu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/acl/ class=align-middle>ACL</a><span class="badge badge-secondary align-middle ml-2">18</span></li><li class=list-group-item><a href=/venues/emnlp/ class=align-middle>EMNLP</a><span class="badge badge-secondary align-middle ml-2">9</span></li><li class=list-group-item><a href=/venues/findings/ class=align-middle>Findings</a><span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/venues/naacl/ class=align-middle>NAACL</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/semeval/ class=align-middle>SemEval</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-venues aria-expanded=false aria-controls=more-venues>show all...</li><div class="collapse border-top" id=more-venues><li class=list-group-item><a href=/venues/cl/ class=align-middle>CL</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/coling/ class=align-middle>COLING</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/lrec/ class=align-middle>LREC</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/eacl/ class=align-middle>EACL</a><span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright &nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>