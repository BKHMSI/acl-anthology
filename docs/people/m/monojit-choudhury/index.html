<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Monojit Choudhury - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Monojit</span> <span class=font-weight-bold>Choudhury</span></h2><hr><div class=row><div class=col-lg-9><h4>2022</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-acl.40.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-acl--40 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-acl.40 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.findings-acl.40/><span class=acl-fixed-case>S</span>y<span class=acl-fixed-case>MC</span>o<span class=acl-fixed-case>M</span> - Syntactic Measure of Code Mixing A Study Of <span class=acl-fixed-case>E</span>nglish-<span class=acl-fixed-case>H</span>indi Code-Mixing</a></strong><br><a href=/people/p/prashant-kodali/>Prashant Kodali</a>
|
<a href=/people/a/anmol-goel/>Anmol Goel</a>
|
<a href=/people/m/monojit-choudhury/>Monojit Choudhury</a>
|
<a href=/people/m/manish-shrivastava/>Manish Shrivastava</a>
|
<a href=/people/p/ponnurangam-kumaraguru/>Ponnurangam Kumaraguru</a><br><a href=/volumes/2022.findings-acl/ class=text-muted>Findings of the Association for Computational Linguistics: ACL 2022</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-acl--40><div class="card-body p-3 small">Code mixing is the linguistic phenomenon where bilingual speakers tend to switch between two or more languages in conversations. Recent work on code-mixing in computational settings has leveraged social media code mixed texts to train NLP models. For capturing the variety of code mixing in, and across corpus, Language ID (LID) tags based measures (CMI) have been proposed. Syntactical variety/patterns of code-mixing and their relationship vis-a-vis computational model&#8217;s performance is under explored. In this work, we investigate a collection of English(en)-Hindi(hi) code-mixed datasets from a syntactic lens to propose, <tex-math>SyMCoM</tex-math>, an indicator of syntactic variety in code-mixed text, with intuitive theoretical bounds. We train SoTA en-hi PoS tagger, accuracy of 93.4%, to reliably compute PoS tags on a corpus, and demonstrate the utility of <tex-math>SyMCoM</tex-math> by applying it on various syntactical categories on a collection of datasets, and compare datasets using the measure.</div></div><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.sigmorphon-1.7.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--sigmorphon-1--7 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.sigmorphon-1.7 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.sigmorphon-1.7" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.sigmorphon-1.7/>Sample-efficient Linguistic Generalizations through <a href=https://en.wikipedia.org/wiki/Program_synthesis>Program Synthesis</a> : Experiments with Phonology Problems</a></strong><br><a href=/people/s/saujas-vaduguru/>Saujas Vaduguru</a>
|
<a href=/people/a/aalok-sathe/>Aalok Sathe</a>
|
<a href=/people/m/monojit-choudhury/>Monojit Choudhury</a>
|
<a href=/people/d/dipti-misra-sharma/>Dipti Sharma</a><br><a href=/volumes/2021.sigmorphon-1/ class=text-muted>Proceedings of the 18th SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--sigmorphon-1--7><div class="card-body p-3 small">Neural models excel at extracting statistical patterns from large amounts of data, but struggle to learn <a href=https://en.wikipedia.org/wiki/Pattern>patterns</a> or reason about <a href=https://en.wikipedia.org/wiki/Language>language</a> from only a few examples. In this paper, we ask : Can we learn explicit <a href=https://en.wikipedia.org/wiki/Rule_of_inference>rules</a> that generalize well from only a few examples? We explore this question using <a href=https://en.wikipedia.org/wiki/Program_synthesis>program synthesis</a>. We develop a synthesis model to learn <a href=https://en.wikipedia.org/wiki/Phonology>phonology rules</a> as <a href=https://en.wikipedia.org/wiki/Computer_program>programs</a> in a <a href=https://en.wikipedia.org/wiki/Domain-specific_language>domain-specific language</a>. We test the ability of our models to generalize from few training examples using our new dataset of problems from the <a href=https://en.wikipedia.org/wiki/International_Linguistic_Olympiad>Linguistics Olympiad</a>, a challenging set of tasks that require strong linguistic reasoning ability. In addition to being highly sample-efficient, our approach generates <a href=https://en.wikipedia.org/wiki/Human-readable_medium>human-readable programs</a>, and allows control over the generalizability of the learnt programs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.mrl-1.8.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--mrl-1--8 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.mrl-1.8 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.mrl-1.8" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.mrl-1.8/>Analyzing the Effects of Reasoning Types on Cross-Lingual Transfer Performance</a></strong><br><a href=/people/k/karthikeyan-k/>Karthikeyan K</a>
|
<a href=/people/a/aalok-sathe/>Aalok Sathe</a>
|
<a href=/people/s/somak-aditya/>Somak Aditya</a>
|
<a href=/people/m/monojit-choudhury/>Monojit Choudhury</a><br><a href=/volumes/2021.mrl-1/ class=text-muted>Proceedings of the 1st Workshop on Multilingual Representation Learning</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--mrl-1--8><div class="card-body p-3 small">Multilingual language models achieve impressive zero-shot accuracies in many languages in complex tasks such as Natural Language Inference (NLI). Examples in NLI (and equivalent complex tasks) often pertain to various types of sub-tasks, requiring different kinds of <a href=https://en.wikipedia.org/wiki/Reason>reasoning</a>. Certain types of <a href=https://en.wikipedia.org/wiki/Reason>reasoning</a> have proven to be more difficult to learn in a monolingual context, and in the crosslingual context, similar observations may shed light on zero-shot transfer efficiency and few-shot sample selection. Hence, to investigate the effects of types of reasoning on transfer performance, we propose a category-annotated multilingual NLI dataset and discuss the challenges to scale monolingual annotations to multiple languages. We statistically observe interesting effects that the confluence of reasoning types and language similarities have on transfer performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.adaptnlp-1.12.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--adaptnlp-1--12 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.adaptnlp-1.12 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.adaptnlp-1.12/>BERTologiCoMix : How does Code-Mixing interact with Multilingual BERT?<span class=acl-fixed-case>BERT</span>ologi<span class=acl-fixed-case>C</span>o<span class=acl-fixed-case>M</span>ix: How does Code-Mixing interact with Multilingual <span class=acl-fixed-case>BERT</span>?</a></strong><br><a href=/people/s/sebastin-santy/>Sebastin Santy</a>
|
<a href=/people/a/anirudh-srinivasan/>Anirudh Srinivasan</a>
|
<a href=/people/m/monojit-choudhury/>Monojit Choudhury</a><br><a href=/volumes/2021.adaptnlp-1/ class=text-muted>Proceedings of the Second Workshop on Domain Adaptation for NLP</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--adaptnlp-1--12><div class="card-body p-3 small">Models such as mBERT and XLMR have shown success in solving Code-Mixed NLP tasks even though they were not exposed to such text during pretraining. Code-Mixed NLP models have relied on using <a href=https://en.wikipedia.org/wiki/Synthetic_genomics>synthetically generated data</a> along with <a href=https://en.wikipedia.org/wiki/Natural_language_processing>naturally occurring data</a> to improve their performance. Finetuning mBERT on such <a href=https://en.wikipedia.org/wiki/Data_(computing)>data</a> improves it&#8217;s code-mixed performance, but the benefits of using the different types of Code-Mixed data are n&#8217;t clear. In this paper, we study the impact of <a href=https://en.wikipedia.org/wiki/Finetuning>finetuning</a> with different types of code-mixed data and outline the changes that occur to the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> during such <a href=https://en.wikipedia.org/wiki/Finetuning>finetuning</a>. Our findings suggest that using naturally occurring code-mixed data brings in the best performance improvement after <a href=https://en.wikipedia.org/wiki/Finetuning>finetuning</a> and that <a href=https://en.wikipedia.org/wiki/Finetuning>finetuning</a> with any type of code-mixed text improves the responsivity of it&#8217;s attention heads to code-mixed text inputs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wnut-1.18.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wnut-1--18 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wnut-1.18 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wnut-1.18/>Comparing Grammatical Theories of Code-Mixing</a></strong><br><a href=/people/a/adithya-pratapa/>Adithya Pratapa</a>
|
<a href=/people/m/monojit-choudhury/>Monojit Choudhury</a><br><a href=/volumes/2021.wnut-1/ class=text-muted>Proceedings of the Seventh Workshop on Noisy User-generated Text (W-NUT 2021)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wnut-1--18><div class="card-body p-3 small">Code-mixed text generation systems have found applications in many downstream tasks, including <a href=https://en.wikipedia.org/wiki/Speech_recognition>speech recognition</a>, <a href=https://en.wikipedia.org/wiki/Translation>translation</a> and <a href=https://en.wikipedia.org/wiki/Dialogue>dialogue</a>. A paradigm of these generation systems relies on well-defined grammatical theories of code-mixing, and there is a lack of comparison of these <a href=https://en.wikipedia.org/wiki/Theory>theories</a>. We present a large-scale human evaluation of two popular grammatical theories, Matrix-Embedded Language (ML) and Equivalence Constraint (EC). We compare them against three heuristic-based models and quantitatively demonstrate the effectiveness of the two <a href=https://en.wikipedia.org/wiki/Grammatical_theory>grammatical theories</a>.</div></div><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.acl-main.560.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--acl-main--560 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.acl-main.560 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://slideslive.com/38929069 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.acl-main.560/>The State and Fate of <a href=https://en.wikipedia.org/wiki/Linguistic_diversity>Linguistic Diversity</a> and Inclusion in the NLP World<span class=acl-fixed-case>NLP</span> World</a></strong><br><a href=/people/p/pratik-joshi/>Pratik Joshi</a>
|
<a href=/people/s/sebastin-santy/>Sebastin Santy</a>
|
<a href=/people/a/amar-budhiraja/>Amar Budhiraja</a>
|
<a href=/people/k/kalika-bali/>Kalika Bali</a>
|
<a href=/people/m/monojit-choudhury/>Monojit Choudhury</a><br><a href=/volumes/2020.acl-main/ class=text-muted>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--acl-main--560><div class="card-body p-3 small">Language technologies contribute to promoting <a href=https://en.wikipedia.org/wiki/Multilingualism>multilingualism</a> and <a href=https://en.wikipedia.org/wiki/Language>linguistic diversity</a> around the world. However, only a very small number of the over 7000 languages of the world are represented in the rapidly evolving <a href=https://en.wikipedia.org/wiki/Language_technology>language technologies</a> and <a href=https://en.wikipedia.org/wiki/Application_software>applications</a>. In this paper we look at the relation between the types of languages, resources, and their representation in NLP conferences to understand the trajectory that different languages have followed over time. Our quantitative investigation underlines the disparity between languages, especially in terms of their resources, and calls into question the language agnostic status of current <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> and <a href=https://en.wikipedia.org/wiki/System>systems</a>. Through this paper, we attempt to convince the ACL community to prioritise the resolution of the predicaments highlighted here, so that no language is left behind.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.calcs-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.calcs-1.0/>Proceedings of the The 4th Workshop on Computational Approaches to Code Switching</a></strong><br><a href=/people/t/thamar-solorio/>Thamar Solorio</a>
|
<a href=/people/m/monojit-choudhury/>Monojit Choudhury</a>
|
<a href=/people/k/kalika-bali/>Kalika Bali</a>
|
<a href=/people/s/sunayana-sitaram/>Sunayana Sitaram</a>
|
<a href=/people/a/amitava-das/>Amitava Das</a>
|
<a href=/people/m/mona-diab/>Mona Diab</a><br><a href=/volumes/2020.calcs-1/ class=text-muted>Proceedings of the The 4th Workshop on Computational Approaches to Code Switching</a></span></p><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2019.icon-1.25.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2019--icon-1--25 data-toggle=collapse aria-expanded=false aria-controls=abstract-2019.icon-1.25 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2019.icon-1.25/>Unsung Challenges of Building and Deploying Language Technologies for Low Resource Language Communities</a></strong><br><a href=/people/p/pratik-joshi/>Pratik Joshi</a>
|
<a href=/people/c/christain-barnes/>Christain Barnes</a>
|
<a href=/people/s/sebastin-santy/>Sebastin Santy</a>
|
<a href=/people/s/simran-khanuja/>Simran Khanuja</a>
|
<a href=/people/s/sanket-shah/>Sanket Shah</a>
|
<a href=/people/a/anirudh-srinivasan/>Anirudh Srinivasan</a>
|
<a href=/people/s/satwik-bhattamishra/>Satwik Bhattamishra</a>
|
<a href=/people/s/sunayana-sitaram/>Sunayana Sitaram</a>
|
<a href=/people/m/monojit-choudhury/>Monojit Choudhury</a>
|
<a href=/people/k/kalika-bali/>Kalika Bali</a><br><a href=/volumes/2019.icon-1/ class=text-muted>Proceedings of the 16th International Conference on Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2019--icon-1--25><div class="card-body p-3 small">In this paper, we examine and analyze the challenges associated with developing and introducing <a href=https://en.wikipedia.org/wiki/Language_technology>language technologies</a> to low-resource language communities. While doing so we bring to light the successes and failures of past work in this area, challenges being faced in doing so, and what have they achieved. Throughout this paper, we take a problem-facing approach and describe essential factors which the success of such <a href=https://en.wikipedia.org/wiki/Technology>technologies</a> hinges upon. We present the various aspects in a manner which clarify and lay out the different tasks involved, which can aid organizations looking to make an impact in this area. We take the example of <a href=https://en.wikipedia.org/wiki/Gondi_language>Gondi</a>, an extremely-low resource Indian language, to reinforce and complement our discussion.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-info align-middle mr-1" href=#abstract-D19-2002 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-2002 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-2002/>Processing and Understanding Mixed Language Data</a></strong><br><a href=/people/m/monojit-choudhury/>Monojit Choudhury</a>
|
<a href=/people/a/anirudh-srinivasan/>Anirudh Srinivasan</a>
|
<a href=/people/s/sandipan-dandapat/>Sandipan Dandapat</a><br><a href=/volumes/D19-2/ class=text-muted>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP): Tutorial Abstracts</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-2002><div class="card-body p-3 small">Multilingual communities exhibit code-mixing, that is, mixing of two or more socially stable languages in a single conversation, sometimes even in a single utterance. This phenomenon has been widely studied by linguists and interaction scientists in the spoken language of such communities. However, with the prevalence of social media and other informal interactive platforms, code-switching is now also ubiquitously observed in user-generated text. As multilingual communities are more the norm from a global perspective, it becomes essential that code-switched text and speech are adequately handled by language technologies and NUIs.\n\nCode-mixing is extremely prevalent in all multilingual societies. Current studies have shown that as much as 20% of user generated content from some geographies, like South Asia, parts of Europe, and Singapore, are code-mixed. Thus, it is very important to handle code-mixed content as a part of NLP systems and applications for these geographies.\n\nIn the past 5 years, there has been an active interest in computational models for code-mixing with a substantive research outcome in terms of publications, datasets and systems. However, it is not easy to find a single point of access for a complete and coherent overview of the research. This tutorial is expecting to fill this gap and provide new researchers in the area with a foundation in both linguistic and computational aspects of code-mixing. We hope that this then becomes a starting point for those who wish to pursue research, design, development and deployment of code-mixed systems in multilingual societies.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-3018.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-3018 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-3018 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-3018" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-3018/>INMT : Interactive Neural Machine Translation Prediction<span class=acl-fixed-case>INMT</span>: Interactive Neural Machine Translation Prediction</a></strong><br><a href=/people/s/sebastin-santy/>Sebastin Santy</a>
|
<a href=/people/s/sandipan-dandapat/>Sandipan Dandapat</a>
|
<a href=/people/m/monojit-choudhury/>Monojit Choudhury</a>
|
<a href=/people/k/kalika-bali/>Kalika Bali</a><br><a href=/volumes/D19-3/ class=text-muted>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP): System Demonstrations</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-3018><div class="card-body p-3 small">In this paper, we demonstrate an Interactive Machine Translation interface, that assists human translators with on-the-fly hints and suggestions. This makes the end-to-end translation process faster, more efficient and creates high-quality translations. We augment the OpenNMT backend with a mechanism to accept the user input and generate conditioned translations.</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1344.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1344 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1344 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-1344/>Word Embeddings for Code-Mixed Language Processing</a></strong><br><a href=/people/a/adithya-pratapa/>Adithya Pratapa</a>
|
<a href=/people/m/monojit-choudhury/>Monojit Choudhury</a>
|
<a href=/people/s/sunayana-sitaram/>Sunayana Sitaram</a><br><a href=/volumes/D18-1/ class=text-muted>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1344><div class="card-body p-3 small">We compare three existing bilingual word embedding approaches, and a novel approach of training skip-grams on synthetic code-mixed text generated through linguistic models of code-mixing, on two tasks-sentiment analysis and POS tagging for code-mixed text. Our results show that while CVM and CCA based embeddings perform as well as the proposed embedding technique on semantic and syntactic tasks respectively, the proposed approach provides the best performance for both tasks overall. Thus, this study demonstrates that existing bilingual embedding techniques are not ideal for code-mixed text processing and there is a need for learning multilingual word embedding from the code-mixed text.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3202.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3202 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3202 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3202/>Phone Merging For Code-Switched Speech Recognition</a></strong><br><a href=/people/s/sunit-sivasankaran/>Sunit Sivasankaran</a>
|
<a href=/people/b/brij-mohan-lal-srivastava/>Brij Mohan Lal Srivastava</a>
|
<a href=/people/s/sunayana-sitaram/>Sunayana Sitaram</a>
|
<a href=/people/k/kalika-bali/>Kalika Bali</a>
|
<a href=/people/m/monojit-choudhury/>Monojit Choudhury</a><br><a href=/volumes/W18-32/ class=text-muted>Proceedings of the Third Workshop on Computational Approaches to Linguistic Code-Switching</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3202><div class="card-body p-3 small">Speakers in multilingual communities often switch between or mix multiple languages in the same conversation. Automatic Speech Recognition (ASR) of code-switched speech faces many challenges including the influence of phones of different languages on each other. This paper shows evidence that phone sharing between languages improves the Acoustic Model performance for Hindi-English code-switched speech. We compare baseline system built with separate phones for <a href=https://en.wikipedia.org/wiki/Hindi>Hindi</a> and <a href=https://en.wikipedia.org/wiki/English_language>English</a> with systems where the phones were manually merged based on linguistic knowledge. Encouraged by the improved ASR performance after manually merging the phones, we further investigate multiple data-driven methods to identify phones to be merged across the languages. We show detailed analysis of automatic phone merging in this language pair and the impact it has on individual phone accuracies and WER. Though the best performance gain of 1.2 % <a href=https://en.wikipedia.org/wiki/Effective_radiated_power>WER</a> was observed with manually merged phones, we show experimentally that the manual phone merge is not optimal.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3210.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3210 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3210 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3210/>Accommodation of Conversational Code-Choice</a></strong><br><a href=/people/a/anshul-bawa/>Anshul Bawa</a>
|
<a href=/people/m/monojit-choudhury/>Monojit Choudhury</a>
|
<a href=/people/k/kalika-bali/>Kalika Bali</a><br><a href=/volumes/W18-32/ class=text-muted>Proceedings of the Third Workshop on Computational Approaches to Linguistic Code-Switching</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3210><div class="card-body p-3 small">Bilingual speakers often freely mix languages. However, in such bilingual conversations, are the language choices of the speakers coordinated? How much does one speaker&#8217;s choice of language affect other speakers? In this paper, we formulate code-choice as a <a href=https://en.wikipedia.org/wiki/Style_(sociolinguistics)>linguistic style</a>, and show that speakers are indeed sensitive to and accommodating of each other&#8217;s code-choice. We find that the saliency or markedness of a language in context directly affects the degree of accommodation observed. More importantly, we discover that accommodation of code-choices persists over several conversational turns. We also propose an alternative interpretation of conversational accommodation as a retrieval problem, and show that the differences in accommodation characteristics of code-choices are based on their markedness in context.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1143.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1143 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1143 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1143.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-1143/>Language Modeling for <a href=https://en.wikipedia.org/wiki/Code_mixing>Code-Mixing</a> : The Role of <a href=https://en.wikipedia.org/wiki/Linguistic_theory>Linguistic Theory</a> based Synthetic Data</a></strong><br><a href=/people/a/adithya-pratapa/>Adithya Pratapa</a>
|
<a href=/people/g/gayatri-bhat/>Gayatri Bhat</a>
|
<a href=/people/m/monojit-choudhury/>Monojit Choudhury</a>
|
<a href=/people/s/sunayana-sitaram/>Sunayana Sitaram</a>
|
<a href=/people/s/sandipan-dandapat/>Sandipan Dandapat</a>
|
<a href=/people/k/kalika-bali/>Kalika Bali</a><br><a href=/volumes/P18-1/ class=text-muted>Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1143><div class="card-body p-3 small">Training language models for Code-mixed (CM) language is known to be a difficult problem because of lack of data compounded by the increased confusability due to the presence of more than one language. We present a computational technique for creation of grammatically valid artificial CM data based on the Equivalence Constraint Theory. We show that when training examples are sampled appropriately from this synthetic data and presented in certain order (aka training curriculum) along with monolingual and real CM data, it can significantly reduce the perplexity of an RNN-based language model. We also show that randomly generated CM data does not help in decreasing the perplexity of the LMs.</div></div><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1180.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1180 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1180 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P17-1180.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-1180/>Estimating Code-Switching on <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a> with a Novel Generalized Word-Level Language Detection Technique<span class=acl-fixed-case>T</span>witter with a Novel Generalized Word-Level Language Detection Technique</a></strong><br><a href=/people/s/shruti-rijhwani/>Shruti Rijhwani</a>
|
<a href=/people/r/royal-sequiera/>Royal Sequiera</a>
|
<a href=/people/m/monojit-choudhury/>Monojit Choudhury</a>
|
<a href=/people/k/kalika-bali/>Kalika Bali</a>
|
<a href=/people/c/chandra-shekhar-maddila/>Chandra Shekhar Maddila</a><br><a href=/volumes/P17-1/ class=text-muted>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1180><div class="card-body p-3 small">Word-level language detection is necessary for analyzing code-switched text, where multiple languages could be mixed within a sentence. Existing models are restricted to code-switching between two specific languages and fail in real-world scenarios as text input rarely has a priori information on the languages used. We present a novel unsupervised word-level language detection technique for code-switched text for an arbitrarily large number of languages, which does not require any manually annotated training data. Our experiments with <a href=https://en.wikipedia.org/wiki/Twitter>tweets</a> in seven languages show a 74 % relative error reduction in word-level labeling with respect to competitive baselines. We then use this system to conduct a large-scale quantitative analysis of code-switching patterns on <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a>, both global as well as region-specific, with 58 M tweets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1240.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1240 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1240 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D17-1240.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D17-1240/>All that is English may be Hindi : Enhancing <a href=https://en.wikipedia.org/wiki/Language_identification>language identification</a> through automatic ranking of the likeliness of <a href=https://en.wikipedia.org/wiki/Loanword>word borrowing</a> in <a href=https://en.wikipedia.org/wiki/Social_media>social media</a><span class=acl-fixed-case>E</span>nglish may be <span class=acl-fixed-case>H</span>indi: Enhancing language identification through automatic ranking of the likeliness of word borrowing in social media</a></strong><br><a href=/people/j/jasabanta-patro/>Jasabanta Patro</a>
|
<a href=/people/b/bidisha-samanta/>Bidisha Samanta</a>
|
<a href=/people/s/saurabh-singh/>Saurabh Singh</a>
|
<a href=/people/a/abhipsa-basu/>Abhipsa Basu</a>
|
<a href=/people/p/prithwish-mukherjee/>Prithwish Mukherjee</a>
|
<a href=/people/m/monojit-choudhury/>Monojit Choudhury</a>
|
<a href=/people/a/animesh-mukherjee/>Animesh Mukherjee</a><br><a href=/volumes/D17-1/ class=text-muted>Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1240><div class="card-body p-3 small">n this paper, we present a set of computational methods to identify the likeliness of a word being borrowed, based on the signals from <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>. In terms of Spearman&#8217;s correlation values, our methods perform more than two times better (0.62) in predicting the borrowing likeliness compared to the best performing baseline (0.26) reported in literature. Based on this likeliness estimate we asked annotators to re-annotate the language tags of foreign words in predominantly native contexts. In 88 % of cases the annotators felt that the foreign language tag should be replaced by native language tag, thus indicating a huge scope for improvement of automatic language identification systems.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Monojit+Choudhury" title="Search for 'Monojit Choudhury' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/k/kalika-bali/ class=align-middle>Kalika Bali</a>
<span class="badge badge-secondary align-middle ml-2">8</span></li><li class=list-group-item><a href=/people/s/sunayana-sitaram/ class=align-middle>Sunayana Sitaram</a>
<span class="badge badge-secondary align-middle ml-2">5</span></li><li class=list-group-item><a href=/people/s/sebastin-santy/ class=align-middle>Sebastin Santy</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/people/a/anirudh-srinivasan/ class=align-middle>Anirudh Srinivasan</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/a/adithya-pratapa/ class=align-middle>Adithya Pratapa</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/s/sandipan-dandapat/ class=align-middle>Sandipan Dandapat</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/p/pratik-joshi/ class=align-middle>Pratik Joshi</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/a/aalok-sathe/ class=align-middle>Aalok Sathe</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/a/amar-budhiraja/ class=align-middle>Amar Budhiraja</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/shruti-rijhwani/ class=align-middle>Shruti Rijhwani</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/royal-sequiera/ class=align-middle>Royal Sequiera</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/chandra-shekhar-maddila/ class=align-middle>Chandra Shekhar Maddila</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/christain-barnes/ class=align-middle>Christain Barnes</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/simran-khanuja/ class=align-middle>Simran Khanuja</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/sanket-shah/ class=align-middle>Sanket Shah</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/satwik-bhattamishra/ class=align-middle>Satwik Bhattamishra</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/saujas-vaduguru/ class=align-middle>Saujas Vaduguru</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/dipti-misra-sharma/ class=align-middle>Dipti Misra Sharma</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/thamar-solorio/ class=align-middle>Thamar Solorio</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/amitava-das/ class=align-middle>Amitava Das</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/mona-diab/ class=align-middle>Mona Diab</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/karthikeyan-k/ class=align-middle>Karthikeyan K</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/somak-aditya/ class=align-middle>Somak Aditya</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/p/prashant-kodali/ class=align-middle>Prashant Kodali</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/anmol-goel/ class=align-middle>Anmol Goel</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/manish-shrivastava/ class=align-middle>Manish Shrivastava</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/p/ponnurangam-kumaraguru/ class=align-middle>Ponnurangam Kumaraguru</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jasabanta-patro/ class=align-middle>Jasabanta Patro</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/b/bidisha-samanta/ class=align-middle>Bidisha Samanta</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/saurabh-singh/ class=align-middle>Saurabh Singh</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/abhipsa-basu/ class=align-middle>Abhipsa Basu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/p/prithwish-mukherjee/ class=align-middle>Prithwish Mukherjee</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/animesh-mukherjee/ class=align-middle>Animesh Mukherjee</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/sunit-sivasankaran/ class=align-middle>Sunit Sivasankaran</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/b/brij-mohan-lal-srivastava/ class=align-middle>Brij Mohan Lal Srivastava</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/anshul-bawa/ class=align-middle>Anshul Bawa</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/g/gayatri-bhat/ class=align-middle>Gayatri Bhat</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/emnlp/ class=align-middle>EMNLP</a><span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/venues/acl/ class=align-middle>ACL</a><span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/venues/ws/ class=align-middle>WS</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/icon/ class=align-middle>ICON</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/sigmorphon/ class=align-middle>SIGMORPHON</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-venues aria-expanded=false aria-controls=more-venues>show all...</li><div class="collapse border-top" id=more-venues><li class=list-group-item><a href=/venues/calcs/ class=align-middle>CALCS</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/mrl/ class=align-middle>MRL</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/findings/ class=align-middle>Findings</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/adaptnlp/ class=align-middle>AdaptNLP</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/wnut/ class=align-middle>WNUT</a><span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>