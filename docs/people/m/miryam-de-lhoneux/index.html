<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Miryam de Lhoneux - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Miryam</span> <span class=font-weight-bold>de Lhoneux</span></h2><hr><div class=row><div class=col-lg-9><h4>2022</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.acl-long.482.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--acl-long--482 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.acl-long.482 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.acl-long.482/>Challenges and Strategies in Cross-Cultural <span class=acl-fixed-case>NLP</span></a></strong><br><a href=/people/d/daniel-hershcovich/>Daniel Hershcovich</a>
|
<a href=/people/s/stella-frank/>Stella Frank</a>
|
<a href=/people/h/heather-lent/>Heather Lent</a>
|
<a href=/people/m/miryam-de-lhoneux/>Miryam de Lhoneux</a>
|
<a href=/people/m/mostafa-abdou/>Mostafa Abdou</a>
|
<a href=/people/s/stephanie-brandl/>Stephanie Brandl</a>
|
<a href=/people/e/emanuele-bugliarello/>Emanuele Bugliarello</a>
|
<a href=/people/l/laura-cabello-piqueras/>Laura Cabello Piqueras</a>
|
<a href=/people/i/ilias-chalkidis/>Ilias Chalkidis</a>
|
<a href=/people/r/ruixiang-cui/>Ruixiang Cui</a>
|
<a href=/people/c/constanza-fierro/>Constanza Fierro</a>
|
<a href=/people/k/katerina-margatina/>Katerina Margatina</a>
|
<a href=/people/p/phillip-rust/>Phillip Rust</a>
|
<a href=/people/a/anders-sogaard/>Anders Søgaard</a><br><a href=/volumes/2022.acl-long/ class=text-muted>Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--acl-long--482><div class="card-body p-3 small">Various efforts in the Natural Language Processing (NLP) community have been made to accommodate linguistic diversity and serve speakers of many different languages. However, it is important to acknowledge that speakers and the content they produce and require, vary not just by language, but also by culture. Although language and culture are tightly linked, there are important differences. Analogous to cross-lingual and multilingual NLP, cross-cultural and multicultural NLP considers these differences in order to better serve users of NLP systems. We propose a principled framework to frame these efforts, and survey existing and potential strategies.</div></div><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.conll-1.5.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--conll-1--5 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.conll-1.5 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.conll-1.5" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.conll-1.5/>On <a href=https://en.wikipedia.org/wiki/Language_model>Language Models</a> for Creoles</a></strong><br><a href=/people/h/heather-lent/>Heather Lent</a>
|
<a href=/people/e/emanuele-bugliarello/>Emanuele Bugliarello</a>
|
<a href=/people/m/miryam-de-lhoneux/>Miryam de Lhoneux</a>
|
<a href=/people/c/chen-qiu/>Chen Qiu</a>
|
<a href=/people/a/anders-sogaard/>Anders Søgaard</a><br><a href=/volumes/2021.conll-1/ class=text-muted>Proceedings of the 25th Conference on Computational Natural Language Learning</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--conll-1--5><div class="card-body p-3 small">Creole languages such as <a href=https://en.wikipedia.org/wiki/Nigerian_Pidgin_English>Nigerian Pidgin English</a> and <a href=https://en.wikipedia.org/wiki/Haitian_Creole>Haitian Creole</a> are under-resourced and largely ignored in the NLP literature. Creoles typically result from the fusion of a foreign language with multiple local languages, and what grammatical and lexical features are transferred to the <a href=https://en.wikipedia.org/wiki/Creole_language>creole</a> is a complex process. While <a href=https://en.wikipedia.org/wiki/Creole_language>creoles</a> are generally stable, the prominence of some features may be much stronger with certain demographics or in some linguistic situations. This paper makes several contributions : We collect existing corpora and release models for <a href=https://en.wikipedia.org/wiki/Haitian_Creole>Haitian Creole</a>, <a href=https://en.wikipedia.org/wiki/Nigerian_Pidgin_English>Nigerian Pidgin English</a>, and <a href=https://en.wikipedia.org/wiki/Singaporean_English>Singaporean Colloquial English</a>. We evaluate these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> on intrinsic and extrinsic tasks. Motivated by the above literature, we compare standard language models with distributionally robust ones and find that, somewhat surprisingly, the standard language models are superior to the distributionally robust ones. We investigate whether this is an effect of <a href=https://en.wikipedia.org/wiki/Parameterized_complexity>over-parameterization</a> or relative distributional stability, and find that the difference persists in the absence of <a href=https://en.wikipedia.org/wiki/Parameterized_complexity>over-parameterization</a>, and that drift is limited, confirming the relative stability of <a href=https://en.wikipedia.org/wiki/Creole_language>creole languages</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.conll-1.19.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--conll-1--19 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.conll-1.19 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.conll-1.19" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.conll-1.19/>A Multilingual Benchmark for Probing Negation-Awareness with Minimal Pairs</a></strong><br><a href=/people/m/mareike-hartmann/>Mareike Hartmann</a>
|
<a href=/people/m/miryam-de-lhoneux/>Miryam de Lhoneux</a>
|
<a href=/people/d/daniel-hershcovich/>Daniel Hershcovich</a>
|
<a href=/people/y/yova-kementchedjhieva/>Yova Kementchedjhieva</a>
|
<a href=/people/l/lukas-nielsen/>Lukas Nielsen</a>
|
<a href=/people/c/chen-qiu/>Chen Qiu</a>
|
<a href=/people/a/anders-sogaard/>Anders Søgaard</a><br><a href=/volumes/2021.conll-1/ class=text-muted>Proceedings of the 25th Conference on Computational Natural Language Learning</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--conll-1--19><div class="card-body p-3 small">Negation is one of the most fundamental concepts in human cognition and language, and several natural language inference (NLI) probes have been designed to investigate pretrained language models&#8217; ability to detect and reason with <a href=https://en.wikipedia.org/wiki/Affirmation_and_negation>negation</a>. However, the existing probing datasets are limited to English only, and do not enable controlled probing of performance in the absence or presence of <a href=https://en.wikipedia.org/wiki/Negation>negation</a>. In response, we present a multilingual (English, Bulgarian, German, French and Chinese) benchmark collection of NLI examples that are grammatical and correctly labeled, as a result of manual inspection and reformulation. We use the benchmark to probe the negation-awareness of multilingual language models and find that models that correctly predict examples with negation cues, often fail to correctly predict their counter-examples without negation cues, even when the cues are irrelevant for semantic inference.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wat-1.22.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wat-1--22 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wat-1.22 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wat-1.22/>Itihasa : A <a href=https://en.wikipedia.org/wiki/Text_corpus>large-scale corpus</a> for Sanskrit to English translation<span class=acl-fixed-case>S</span>anskrit to <span class=acl-fixed-case>E</span>nglish translation</a></strong><br><a href=/people/r/rahul-aralikatte/>Rahul Aralikatte</a>
|
<a href=/people/m/miryam-de-lhoneux/>Miryam de Lhoneux</a>
|
<a href=/people/a/anoop-kunchukuttan/>Anoop Kunchukuttan</a>
|
<a href=/people/a/anders-sogaard/>Anders Søgaard</a><br><a href=/volumes/2021.wat-1/ class=text-muted>Proceedings of the 8th Workshop on Asian Translation (WAT2021)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wat-1--22><div class="card-body p-3 small">This work introduces <a href=https://en.wikipedia.org/wiki/Itihasa>Itihasa</a>, a large-scale translation dataset containing 93,000 pairs of Sanskrit shlokas and their English translations. The <a href=https://en.wikipedia.org/wiki/Shloka>shlokas</a> are extracted from two <a href=https://en.wikipedia.org/wiki/Indian_epic_poetry>Indian epics</a> viz., The <a href=https://en.wikipedia.org/wiki/Ramayana>Ramayana</a> and The <a href=https://en.wikipedia.org/wiki/Mahabharata>Mahabharata</a>. We first describe the motivation behind the curation of such a <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> and follow up with empirical analysis to bring out its nuances. We then benchmark the performance of standard translation models on this <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> and show that even state-of-the-art transformer architectures perform poorly, emphasizing the complexity of the dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wat-1.24.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wat-1--24 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wat-1.24 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wat-1.24/>How far can we get with one GPU in 100 hours? CoAStaL at MultiIndicMT Shared Task<span class=acl-fixed-case>GPU</span> in 100 hours? <span class=acl-fixed-case>C</span>o<span class=acl-fixed-case>AS</span>ta<span class=acl-fixed-case>L</span> at <span class=acl-fixed-case>M</span>ulti<span class=acl-fixed-case>I</span>ndic<span class=acl-fixed-case>MT</span> Shared Task</a></strong><br><a href=/people/r/rahul-aralikatte/>Rahul Aralikatte</a>
|
<a href=/people/h/hector-ricardo-murrieta-bello/>Héctor Ricardo Murrieta Bello</a>
|
<a href=/people/m/miryam-de-lhoneux/>Miryam de Lhoneux</a>
|
<a href=/people/d/daniel-hershcovich/>Daniel Hershcovich</a>
|
<a href=/people/m/marcel-bollmann/>Marcel Bollmann</a>
|
<a href=/people/a/anders-sogaard/>Anders Søgaard</a><br><a href=/volumes/2021.wat-1/ class=text-muted>Proceedings of the 8th Workshop on Asian Translation (WAT2021)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wat-1--24><div class="card-body p-3 small">This work shows that competitive <a href=https://en.wikipedia.org/wiki/Translation_(geometry)>translation</a> results can be obtained in a constrained setting by incorporating the latest advances in memory and compute optimization. We train and evaluate large multilingual translation models using a single <a href=https://en.wikipedia.org/wiki/Graphics_processing_unit>GPU</a> for a maximum of 100 hours and get within 4-5 BLEU points of the top submission on the leaderboard. We also benchmark standard baselines on the PMI corpus and re-discover well-known shortcomings of translation systems and <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a>.</div></div><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.udw-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.udw-1.0/>Proceedings of the Fourth Workshop on Universal Dependencies (UDW 2020)</a></strong><br><a href=/people/m/marie-catherine-de-marneffe/>Marie-Catherine de Marneffe</a>
|
<a href=/people/m/miryam-de-lhoneux/>Miryam de Lhoneux</a>
|
<a href=/people/j/joakim-nivre/>Joakim Nivre</a>
|
<a href=/people/s/sebastian-schuster/>Sebastian Schuster</a><br><a href=/volumes/2020.udw-1/ class=text-muted>Proceedings of the Fourth Workshop on Universal Dependencies (UDW 2020)</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.264.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--264 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.264 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.coling-main.264" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.264/>Comparison by Conversion : Reverse-Engineering UCCA from Syntax and Lexical Semantics<span class=acl-fixed-case>UCCA</span> from Syntax and Lexical Semantics</a></strong><br><a href=/people/d/daniel-hershcovich/>Daniel Hershcovich</a>
|
<a href=/people/n/nathan-schneider/>Nathan Schneider</a>
|
<a href=/people/d/dotan-dvir/>Dotan Dvir</a>
|
<a href=/people/j/jakob-prange/>Jakob Prange</a>
|
<a href=/people/m/miryam-de-lhoneux/>Miryam de Lhoneux</a>
|
<a href=/people/o/omri-abend/>Omri Abend</a><br><a href=/volumes/2020.coling-main/ class=text-muted>Proceedings of the 28th International Conference on Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--264><div class="card-body p-3 small">Building robust <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language understanding systems</a> will require a clear characterization of whether and how various linguistic meaning representations complement each other. To perform a systematic comparative analysis, we evaluate the mapping between meaning representations from different frameworks using two complementary methods : (i) a rule-based converter, and (ii) a supervised delexicalized parser that parses to one framework using only information from the other as features. We apply these methods to convert the STREUSLE corpus (with syntactic and lexical semantic annotations) to UCCA (a graph-structured full-sentence meaning representation). Both methods yield surprisingly accurate target representations, close to fully supervised UCCA parser qualityindicating that UCCA annotations are partially redundant with STREUSLE annotations. Despite this substantial convergence between <a href=https://en.wikipedia.org/wiki/Conceptual_framework>frameworks</a>, we find several important areas of divergence.</div></div><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1159.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1159 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1159 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N19-1159.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/364704101 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1159" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1159/>Recursive Subtree Composition in LSTM-Based Dependency Parsing<span class=acl-fixed-case>LSTM</span>-Based Dependency Parsing</a></strong><br><a href=/people/m/miryam-de-lhoneux/>Miryam de Lhoneux</a>
|
<a href=/people/m/miguel-ballesteros/>Miguel Ballesteros</a>
|
<a href=/people/j/joakim-nivre/>Joakim Nivre</a><br><a href=/volumes/N19-1/ class=text-muted>Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1159><div class="card-body p-3 small">The need for tree structure modelling on top of sequence modelling is an open issue in neural dependency parsing. We investigate the impact of adding a <a href=https://en.wikipedia.org/wiki/Tree_layer>tree layer</a> on top of a <a href=https://en.wikipedia.org/wiki/Sequential_model>sequential model</a> by recursively composing subtree representations (composition) in a transition-based parser that uses features extracted by a BiLSTM. Composition seems superfluous with such a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>, suggesting that BiLSTMs capture information about <a href=https://en.wikipedia.org/wiki/Tree_(graph_theory)>subtrees</a>. We perform model ablations to tease out the conditions under which <a href=https://en.wikipedia.org/wiki/Composition_(music)>composition</a> helps. When ablating the backward LSTM, performance drops and <a href=https://en.wikipedia.org/wiki/Musical_composition>composition</a> does not recover much of the gap. When ablating the forward LSTM, performance drops less dramatically and <a href=https://en.wikipedia.org/wiki/Composition_(music)>composition</a> recovers a substantial part of the gap, indicating that a forward LSTM and <a href=https://en.wikipedia.org/wiki/Composition_(music)>composition</a> capture similar information. We take the backward LSTM to be related to lookahead features and the forward LSTM to the rich history-based features both crucial for transition-based parsers. To capture history-based information, composition is better than a forward LSTM on its own, but it is even better to have a forward LSTM as part of a BiLSTM. We correlate results with language properties, showing that the improved <a href=https://en.wikipedia.org/wiki/Ahead-of-time_compilation>lookahead</a> of a backward LSTM is especially important for head-final languages.</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1543.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1543 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1543 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D18-1543.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D18-1543.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1543" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D18-1543/>Parameter sharing between dependency parsers for related languages</a></strong><br><a href=/people/m/miryam-de-lhoneux/>Miryam de Lhoneux</a>
|
<a href=/people/j/johannes-bjerva/>Johannes Bjerva</a>
|
<a href=/people/i/isabelle-augenstein/>Isabelle Augenstein</a>
|
<a href=/people/a/anders-sogaard/>Anders Søgaard</a><br><a href=/volumes/D18-1/ class=text-muted>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1543><div class="card-body p-3 small">Previous work has suggested that parameter sharing between transition-based neural dependency parsers for related languages can lead to better performance, but there is no consensus on what parameters to share. We present an evaluation of 27 different parameter sharing strategies across 10 languages, representing five pairs of related languages, each pair from a different <a href=https://en.wikipedia.org/wiki/Language_family>language family</a>. We find that sharing transition classifier parameters always helps, whereas the usefulness of sharing word and/or character LSTM parameters varies. Based on this result, we propose an architecture where the transition classifier is shared, and the sharing of word and character parameters is controlled by a parameter that can be tuned on validation data. This <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> is linguistically motivated and obtains significant improvements over a monolingually trained baseline. We also find that sharing transition classifier parameters helps when training a <a href=https://en.wikipedia.org/wiki/Parsing>parser</a> on unrelated language pairs, but we find that, in the case of unrelated languages, sharing too many parameters does not help.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K18-2011.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K18-2011 data-toggle=collapse aria-expanded=false aria-controls=abstract-K18-2011 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/K18-2011/>82 Treebanks, 34 Models : Universal Dependency Parsing with Multi-Treebank Models<span class=acl-fixed-case>U</span>niversal <span class=acl-fixed-case>D</span>ependency Parsing with Multi-Treebank Models</a></strong><br><a href=/people/a/aaron-smith/>Aaron Smith</a>
|
<a href=/people/b/bernd-bohnet/>Bernd Bohnet</a>
|
<a href=/people/m/miryam-de-lhoneux/>Miryam de Lhoneux</a>
|
<a href=/people/j/joakim-nivre/>Joakim Nivre</a>
|
<a href=/people/y/yan-shao/>Yan Shao</a>
|
<a href=/people/s/sara-stymne/>Sara Stymne</a><br><a href=/volumes/K18-2/ class=text-muted>Proceedings of the CoNLL 2018 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K18-2011><div class="card-body p-3 small">We present the Uppsala system for the CoNLL 2018 Shared Task on universal dependency parsing. Our system is a pipeline consisting of three components : the first performs joint word and sentence segmentation ; the second predicts <a href=https://en.wikipedia.org/wiki/Part-of-speech_tagging>part-of-speech tags</a> and <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphological features</a> ; the third predicts dependency trees from words and tags. Instead of training a single parsing model for each <a href=https://en.wikipedia.org/wiki/Treebank>treebank</a>, we trained <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> with multiple treebanks for one language or closely related languages, greatly reducing the number of <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a>. On the official test run, we ranked 7th of 27 teams for the LAS and MLAS metrics. Our system obtained the best scores overall for <a href=https://en.wikipedia.org/wiki/Word_segmentation>word segmentation</a>, universal POS tagging, and <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphological features</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2098.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2098 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2098 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2098.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-2098" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P18-2098/>Parser Training with Heterogeneous Treebanks</a></strong><br><a href=/people/s/sara-stymne/>Sara Stymne</a>
|
<a href=/people/m/miryam-de-lhoneux/>Miryam de Lhoneux</a>
|
<a href=/people/a/aaron-smith/>Aaron Smith</a>
|
<a href=/people/j/joakim-nivre/>Joakim Nivre</a><br><a href=/volumes/P18-2/ class=text-muted>Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2098><div class="card-body p-3 small">How to make the most of multiple heterogeneous treebanks when training a monolingual dependency parser is an open question. We start by investigating previously suggested, but little evaluated, strategies for exploiting multiple treebanks based on concatenating training sets, with or without <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a>. We go on to propose a new <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>method</a> based on treebank embeddings. We perform experiments for several languages and show that in many cases <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a> and treebank embeddings lead to substantial improvements over single treebanks or <a href=https://en.wikipedia.org/wiki/Concatenation>concatenation</a>, with average gains of 2.03.5 LAS points. We argue that treebank embeddings should be preferred due to their conceptual simplicity, flexibility and extensibility.</div></div><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-6314.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-6314 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-6314 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W17-6314.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W17-6314" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W17-6314/>Arc-Hybrid Non-Projective Dependency Parsing with a Static-Dynamic Oracle</a></strong><br><a href=/people/m/miryam-de-lhoneux/>Miryam de Lhoneux</a>
|
<a href=/people/s/sara-stymne/>Sara Stymne</a>
|
<a href=/people/j/joakim-nivre/>Joakim Nivre</a><br><a href=/volumes/W17-63/ class=text-muted>Proceedings of the 15th International Conference on Parsing Technologies</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-6314><div class="card-body p-3 small">In this paper, we extend the arc-hybrid system for transition-based parsing with a swap transition that enables reordering of the words and construction of non-projective trees. Although this extension breaks the arc-decomposability of the transition system, we show how the existing dynamic oracle for this <a href=https://en.wikipedia.org/wiki/System>system</a> can be modified and combined with a static oracle only for the swap transition. Experiments on 5 languages show that the new <a href=https://en.wikipedia.org/wiki/System>system</a> gives competitive accuracy and is significantly better than a <a href=https://en.wikipedia.org/wiki/System>system</a> trained with a purely static oracle.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Miryam+de+Lhoneux" title="Search for 'Miryam de Lhoneux' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/a/anders-sogaard/ class=align-middle>Anders Søgaard</a>
<span class="badge badge-secondary align-middle ml-2">6</span></li><li class=list-group-item><a href=/people/j/joakim-nivre/ class=align-middle>Joakim Nivre</a>
<span class="badge badge-secondary align-middle ml-2">5</span></li><li class=list-group-item><a href=/people/d/daniel-hershcovich/ class=align-middle>Daniel Hershcovich</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/people/s/sara-stymne/ class=align-middle>Sara Stymne</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/h/heather-lent/ class=align-middle>Heather Lent</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/e/emanuele-bugliarello/ class=align-middle>Emanuele Bugliarello</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/c/chen-qiu/ class=align-middle>Chen Qiu</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/r/rahul-aralikatte/ class=align-middle>Rahul Aralikatte</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/a/aaron-smith/ class=align-middle>Aaron Smith</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/m/marie-catherine-de-marneffe/ class=align-middle>Marie-Catherine de Marneffe</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/sebastian-schuster/ class=align-middle>Sebastian Schuster</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/mareike-hartmann/ class=align-middle>Mareike Hartmann</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yova-kementchedjhieva/ class=align-middle>Yova Kementchedjhieva</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/lukas-nielsen/ class=align-middle>Lukas Nielsen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/stella-frank/ class=align-middle>Stella Frank</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/mostafa-abdou/ class=align-middle>Mostafa Abdou</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/stephanie-brandl/ class=align-middle>Stephanie Brandl</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/laura-cabello-piqueras/ class=align-middle>Laura Cabello Piqueras</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/i/ilias-chalkidis/ class=align-middle>Ilias Chalkidis</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/ruixiang-cui/ class=align-middle>Ruixiang Cui</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/constanza-fierro/ class=align-middle>Constanza Fierro</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/katerina-margatina/ class=align-middle>Katerina Margatina</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/p/phillip-rust/ class=align-middle>Phillip Rust</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/johannes-bjerva/ class=align-middle>Johannes Bjerva</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/i/isabelle-augenstein/ class=align-middle>Isabelle Augenstein</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/anoop-kunchukuttan/ class=align-middle>Anoop Kunchukuttan</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/hector-ricardo-murrieta-bello/ class=align-middle>Héctor Ricardo Murrieta Bello</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/marcel-bollmann/ class=align-middle>Marcel Bollmann</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/n/nathan-schneider/ class=align-middle>Nathan Schneider</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/dotan-dvir/ class=align-middle>Dotan Dvir</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jakob-prange/ class=align-middle>Jakob Prange</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/o/omri-abend/ class=align-middle>Omri Abend</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/miguel-ballesteros/ class=align-middle>Miguel Ballesteros</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/b/bernd-bohnet/ class=align-middle>Bernd Bohnet</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yan-shao/ class=align-middle>Yan Shao</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/conll/ class=align-middle>CoNLL</a><span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/venues/acl/ class=align-middle>ACL</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/wat/ class=align-middle>WAT</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/udw/ class=align-middle>UDW</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/ws/ class=align-middle>WS</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-venues aria-expanded=false aria-controls=more-venues>show all...</li><div class="collapse border-top" id=more-venues><li class=list-group-item><a href=/venues/emnlp/ class=align-middle>EMNLP</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/coling/ class=align-middle>COLING</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/naacl/ class=align-middle>NAACL</a><span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>