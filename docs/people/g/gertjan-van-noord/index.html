<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Gertjan van Noord - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Gertjan</span> <span class=font-weight-bold>van Noord</span></h2><hr><div class=row><div class=col-lg-9><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wat-1.21.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wat-1--21 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wat-1.21 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wat-1.21/>Optimal Word Segmentation for <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a> into Dravidian Languages<span class=acl-fixed-case>D</span>ravidian Languages</a></strong><br><a href=/people/p/prajit-dhar/>Prajit Dhar</a>
|
<a href=/people/a/arianna-bisazza/>Arianna Bisazza</a>
|
<a href=/people/g/gertjan-van-noord/>Gertjan van Noord</a><br><a href=/volumes/2021.wat-1/ class=text-muted>Proceedings of the 8th Workshop on Asian Translation (WAT2021)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wat-1--21><div class="card-body p-3 small">Dravidian languages, such as <a href=https://en.wikipedia.org/wiki/Kannada>Kannada</a> and <a href=https://en.wikipedia.org/wiki/Tamil_language>Tamil</a>, are notoriously difficult to translate by state-of-the-art neural models. This stems from the fact that these <a href=https://en.wikipedia.org/wiki/Language>languages</a> are morphologically very rich as well as being low-resourced. In this paper, we focus on subword segmentation and evaluate Linguistically Motivated Vocabulary Reduction (LMVR) against the more commonly used SentencePiece (SP) for the task of translating from <a href=https://en.wikipedia.org/wiki/English_language>English</a> into four different <a href=https://en.wikipedia.org/wiki/Dravidian_languages>Dravidian languages</a>. Additionally we investigate the optimal subword vocabulary size for each language. We find that SP is the overall best choice for segmentation, and that larger dictionary sizes lead to higher translation quality.</div></div><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/R19-1140.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-R19-1140 data-toggle=collapse aria-expanded=false aria-controls=abstract-R19-1140 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/R19-1140/>Cross-Lingual Word Embeddings for Morphologically Rich Languages</a></strong><br><a href=/people/a/ahmet-ustun/>Ahmet Üstün</a>
|
<a href=/people/g/gosse-bouma/>Gosse Bouma</a>
|
<a href=/people/g/gertjan-van-noord/>Gertjan van Noord</a><br><a href=/volumes/R19-1/ class=text-muted>Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2019)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-R19-1140><div class="card-body p-3 small">Cross-lingual word embedding models learn a shared vector space for two or more languages so that words with similar meaning are represented by similar vectors regardless of their language. Although the existing models achieve high performance on pairs of morphologically simple languages, they perform very poorly on morphologically rich languages such as <a href=https://en.wikipedia.org/wiki/Turkish_language>Turkish</a> and <a href=https://en.wikipedia.org/wiki/Finnish_language>Finnish</a>. In this paper, we propose a morpheme-based model in order to increase the performance of cross-lingual word embeddings on morphologically rich languages. Our <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> includes a simple extension which enables us to exploit <a href=https://en.wikipedia.org/wiki/Morpheme>morphemes</a> for cross-lingual mapping. We applied our <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> for the <a href=https://en.wikipedia.org/wiki/Finnish_language>Turkish-Finnish language pair</a> on the bilingual word translation task. Results show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms the baseline models by 2 % in the <a href=https://en.wikipedia.org/wiki/Nearest_neighbour_search>nearest neighbour ranking</a>.</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2018.gwc-1.30.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2018--gwc-1--30 data-toggle=collapse aria-expanded=false aria-controls=abstract-2018.gwc-1.30 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2018.gwc-1.30/>Simple Embedding-Based Word Sense Disambiguation</a></strong><br><a href=/people/d/dieke-oele/>Dieke Oele</a>
|
<a href=/people/g/gertjan-van-noord/>Gertjan van Noord</a><br><a href=/volumes/2018.gwc-1/ class=text-muted>Proceedings of the 9th Global Wordnet Conference</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2018--gwc-1--30><div class="card-body p-3 small">We present a simple knowledge-based WSD method that uses word and sense embeddings to compute the similarity between the gloss of a sense and the context of the word. Our method is inspired by the <a href=https://en.wikipedia.org/wiki/Lesk_algorithm>Lesk algorithm</a> as it exploits both the context of the words and the definitions of the senses. It only requires large unlabeled corpora and a sense inventory such as <a href=https://en.wikipedia.org/wiki/WordNet>WordNet</a>, and therefore does not rely on annotated data. We explore whether additional extensions to Lesk are compatible with our <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>method</a>. The results of our experiments show that by lexically extending the amount of words in the gloss and context, although it works well for other implementations of Lesk, harms our method. Using a lexical selection method on the <a href=https://en.wikipedia.org/wiki/Context_(language_use)>context words</a>, on the other hand, improves it. The combination of our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> with lexical selection enables our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> to outperform state-of the art <a href=https://en.wikipedia.org/wiki/Knowledge-based_systems>knowledge-based systems</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1542.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1542 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1542 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D18-1542.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1542" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D18-1542/>Modeling Input Uncertainty in Neural Network Dependency Parsing</a></strong><br><a href=/people/r/rob-van-der-goot/>Rob van der Goot</a>
|
<a href=/people/g/gertjan-van-noord/>Gertjan van Noord</a><br><a href=/volumes/D18-1/ class=text-muted>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1542><div class="card-body p-3 small">Recently introduced neural network parsers allow for new approaches to circumvent data sparsity issues by modeling character level information and by exploiting raw data in a semi-supervised setting. Data sparsity is especially prevailing when transferring to <a href=https://en.wikipedia.org/wiki/Standardization>non-standard domains</a>. In this setting, lexical normalization has often been used in the past to circumvent data sparsity. In this paper, we investigate whether these new neural approaches provide similar functionality as lexical normalization, or whether they are complementary. We provide experimental results which show that a separate normalization component improves performance of a neural network parser even if it has access to character level information as well as external word embeddings. Further improvements are obtained by a straightforward but novel approach in which the top-N best candidates provided by the <a href=https://en.wikipedia.org/wiki/Parsing>normalization component</a> are available to the <a href=https://en.wikipedia.org/wiki/Parsing>parser</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/J18-4003.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-J18-4003 data-toggle=collapse aria-expanded=false aria-controls=abstract-J18-4003 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/J18-4003/>Squib : Reproducibility in Computational Linguistics : Are We Willing to Share?<span class=acl-fixed-case>S</span>quib: Reproducibility in Computational Linguistics: Are We Willing to Share?</a></strong><br><a href=/people/m/martijn-wieling/>Martijn Wieling</a>
|
<a href=/people/j/josine-rawee/>Josine Rawee</a>
|
<a href=/people/g/gertjan-van-noord/>Gertjan van Noord</a><br><a href=/volumes/J18-4/ class=text-muted>Computational Linguistics, Volume 44, Issue 4 - December 2018</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-J18-4003><div class="card-body p-3 small">This study focuses on an essential precondition for reproducibility in <a href=https://en.wikipedia.org/wiki/Computational_linguistics>computational linguistics</a> : the willingness of authors to share relevant source code and data. Ten years after Ted Pedersen&#8217;s influential Last Words contribution in <a href=https://en.wikipedia.org/wiki/Computational_linguistics>Computational Linguistics</a>, we investigate to what extent researchers in <a href=https://en.wikipedia.org/wiki/Computational_linguistics>computational linguistics</a> are willing and able to share their data and code. We surveyed all 395 full papers presented at the 2011 and 2016 ACL Annual Meetings, and identified whether links to data and code were provided. If working links were not provided, authors were requested to provide this information. Although data were often available, code was shared less often. When working links to code or data were not provided in the paper, authors provided the <a href=https://en.wikipedia.org/wiki/Source_code>code</a> in about one third of cases. For a selection of ten papers, we attempted to reproduce the results using the provided data and code. We were able to reproduce the results approximately for six papers. For only a single paper did we obtain the exact same results. Our findings show that even though the situation appears to have improved comparing 2016 to 2011, <a href=https://en.wikipedia.org/wiki/Empiricism>empiricism</a> in <a href=https://en.wikipedia.org/wiki/Computational_linguistics>computational linguistics</a> still largely remains a matter of faith. Nevertheless, we are somewhat optimistic about the future. Ensuring reproducibility is not only important for the field as a whole, but also seems worthwhile for individual researchers : The median citation count for studies with working links to the source code is higher.</div></div><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2078.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2078 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2078 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P17-2078.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P17-2078.Software.tgz data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-2078/>Parser Adaptation for <a href=https://en.wikipedia.org/wiki/Social_media>Social Media</a> by Integrating Normalization</a></strong><br><a href=/people/r/rob-van-der-goot/>Rob van der Goot</a>
|
<a href=/people/g/gertjan-van-noord/>Gertjan van Noord</a><br><a href=/volumes/P17-2/ class=text-muted>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2078><div class="card-body p-3 small">This work explores different approaches of using normalization for parser adaptation. Traditionally, <a href=https://en.wikipedia.org/wiki/Normalization_(image_processing)>normalization</a> is used as separate pre-processing step. We show that integrating the <a href=https://en.wikipedia.org/wiki/Normalization_(statistics)>normalization model</a> into the <a href=https://en.wikipedia.org/wiki/Parsing>parsing algorithm</a> is more beneficial. This way, multiple normalization candidates can be leveraged, which improves <a href=https://en.wikipedia.org/wiki/Parsing>parsing</a> performance on <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>. We test this hypothesis by modifying the Berkeley parser ; out-of-the-box it achieves an <a href=https://en.wikipedia.org/wiki/Feasible_region>F1 score</a> of 66.52. Our integrated approach reaches a significant improvement with an <a href=https://en.wikipedia.org/wiki/F-number>F1 score</a> of 67.36, while using the best <a href=https://en.wikipedia.org/wiki/Normalization_(statistics)>normalization sequence</a> results in an <a href=https://en.wikipedia.org/wiki/F-number>F1 score</a> of only 66.94.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5043.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5043 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5043 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5043/>The Power of Character N-grams in Native Language Identification</a></strong><br><a href=/people/a/artur-kulmizev/>Artur Kulmizev</a>
|
<a href=/people/b/bo-blankers/>Bo Blankers</a>
|
<a href=/people/j/johannes-bjerva/>Johannes Bjerva</a>
|
<a href=/people/m/malvina-nissim/>Malvina Nissim</a>
|
<a href=/people/g/gertjan-van-noord/>Gertjan van Noord</a>
|
<a href=/people/b/barbara-plank/>Barbara Plank</a>
|
<a href=/people/m/martijn-wieling/>Martijn Wieling</a><br><a href=/volumes/W17-50/ class=text-muted>Proceedings of the 12th Workshop on Innovative Use of NLP for Building Educational Applications</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5043><div class="card-body p-3 small">In this paper, we explore the performance of a linear SVM trained on language independent character features for the NLI Shared Task 2017. Our basic <a href=https://en.wikipedia.org/wiki/System>system</a> (GRONINGEN) achieves the best performance (87.56 F1-score) on the evaluation set using only 1-9 character n-grams as <a href=https://en.wikipedia.org/wiki/Feature_(computer_vision)>features</a>. We compare this against several ensemble and meta-classifiers in order to examine how the <a href=https://en.wikipedia.org/wiki/Linear_system>linear system</a> fares when combined with other, especially non-linear classifiers. Special emphasis is placed on the topic bias that exists by virtue of the assessment essay prompt distribution.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Gertjan+van+Noord" title="Search for 'Gertjan van Noord' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/r/rob-van-der-goot/ class=align-middle>Rob van der Goot</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/m/martijn-wieling/ class=align-middle>Martijn Wieling</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/a/artur-kulmizev/ class=align-middle>Artur Kulmizev</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/b/bo-blankers/ class=align-middle>Bo Blankers</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/johannes-bjerva/ class=align-middle>Johannes Bjerva</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/m/malvina-nissim/ class=align-middle>Malvina Nissim</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/b/barbara-plank/ class=align-middle>Barbara Plank</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/dieke-oele/ class=align-middle>Dieke Oele</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/josine-rawee/ class=align-middle>Josine Rawee</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/p/prajit-dhar/ class=align-middle>Prajit Dhar</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/arianna-bisazza/ class=align-middle>Arianna Bisazza</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/ahmet-ustun/ class=align-middle>Ahmet Üstün</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/g/gosse-bouma/ class=align-middle>Gosse Bouma</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/acl/ class=align-middle>ACL</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/ws/ class=align-middle>WS</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/gwc/ class=align-middle>GWC</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/emnlp/ class=align-middle>EMNLP</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/cl/ class=align-middle>CL</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-venues aria-expanded=false aria-controls=more-venues>show all...</li><div class="collapse border-top" id=more-venues><li class=list-group-item><a href=/venues/wat/ class=align-middle>WAT</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/ranlp/ class=align-middle>RANLP</a><span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>