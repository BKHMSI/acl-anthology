<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Giuseppe Carenini - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Giuseppe</span> <span class=font-weight-bold>Carenini</span></h2><hr><div class=row><div class=col-lg-9><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.sigdial-1.18.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--sigdial-1--18 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.sigdial-1.18 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href="https://www.youtube.com/watch?v=04Urc5LRBlk" data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.sigdial-1.18" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2021.sigdial-1.18/>Improving Unsupervised Dialogue Topic Segmentation with Utterance-Pair Coherence Scoring</a></strong><br><a href=/people/l/linzi-xing/>Linzi Xing</a>
|
<a href=/people/g/giuseppe-carenini/>Giuseppe Carenini</a><br><a href=/volumes/2021.sigdial-1/ class=text-muted>Proceedings of the 22nd Annual Meeting of the Special Interest Group on Discourse and Dialogue</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--sigdial-1--18><div class="card-body p-3 small">Dialogue topic segmentation is critical in several dialogue modeling problems. However, popular <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised approaches</a> only exploit surface features in assessing topical coherence among utterances. In this work, we address this limitation by leveraging supervisory signals from the utterance-pair coherence scoring task. First, we present a simple yet effective strategy to generate a <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training corpus</a> for utterance-pair coherence scoring. Then, we train a BERT-based neural utterance-pair coherence model with the obtained <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training corpus</a>. Finally, such <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> is used to measure the topical relevance between utterances, acting as the basis of the segmentation inference. Experiments on three public datasets in <a href=https://en.wikipedia.org/wiki/English_language>English</a> and <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a> demonstrate that our proposal outperforms the state-of-the-art baselines.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-demo.26.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-demo--26 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-demo.26 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-demo.26" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-demo.26/>T3-Vis : visual analytic for Training and fine-Tuning Transformers in NLP<span class=acl-fixed-case>NLP</span></a></strong><br><a href=/people/r/raymond-li/>Raymond Li</a>
|
<a href=/people/w/wen-xiao/>Wen Xiao</a>
|
<a href=/people/l/lanjun-wang/>Lanjun Wang</a>
|
<a href=/people/h/hyeju-jang/>Hyeju Jang</a>
|
<a href=/people/g/giuseppe-carenini/>Giuseppe Carenini</a><br><a href=/volumes/2021.emnlp-demo/ class=text-muted>Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-demo--26><div class="card-body p-3 small">Transformers are the dominant architecture in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a>, but their training and fine-tuning is still very challenging. In this paper, we present the design and implementation of a visual analytic framework for assisting researchers in such process, by providing them with valuable insights about the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a>&#8217;s intrinsic properties and behaviours. Our <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> offers an intuitive overview that allows the user to explore different facets of the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> (e.g., hidden states, attention) through <a href=https://en.wikipedia.org/wiki/Interactive_visualization>interactive visualization</a>, and allows a suite of built-in algorithms that compute the importance of model components and different parts of the input sequence. Case studies and feedback from a user focus group indicate that the <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> is useful, and suggest several improvements. Our <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> is available at : https://github.com/raymondzmc/T3-Vis.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.326.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--326 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.326 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.naacl-main.326" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.326/>Predicting Discourse Trees from Transformer-based Neural Summarizers</a></strong><br><a href=/people/w/wen-xiao/>Wen Xiao</a>
|
<a href=/people/p/patrick-huber/>Patrick Huber</a>
|
<a href=/people/g/giuseppe-carenini/>Giuseppe Carenini</a><br><a href=/volumes/2021.naacl-main/ class=text-muted>Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--326><div class="card-body p-3 small">Previous work indicates that <a href=https://en.wikipedia.org/wiki/Discourse_analysis>discourse information</a> benefits <a href=https://en.wikipedia.org/wiki/Automatic_summarization>summarization</a>. In this paper, we explore whether this synergy between <a href=https://en.wikipedia.org/wiki/Discourse>discourse</a> and <a href=https://en.wikipedia.org/wiki/Automatic_summarization>summarization</a> is bidirectional, by inferring document-level discourse trees from pre-trained neural summarizers. In particular, we generate unlabeled RST-style discourse trees from the self-attention matrices of the transformer model. Experiments across models and datasets reveal that the summarizer learns both, dependency- and constituency-style discourse information, which is typically encoded in a single head, covering long- and short-distance discourse dependencies. Overall, the experimental results suggest that the learned <a href=https://en.wikipedia.org/wiki/Discourse_analysis>discourse information</a> is general and transferable inter-domain.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.newsum-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.newsum-1.0/>Proceedings of the Third Workshop on New Frontiers in Summarization</a></strong><br><a href=/people/g/giuseppe-carenini/>Giuseppe Carenini</a>
|
<a href=/people/j/jackie-chi-kit-cheung/>Jackie Chi Kit Cheung</a>
|
<a href=/people/y/yue-dong/>Yue Dong</a>
|
<a href=/people/f/fei-liu-utdallas/>Fei Liu</a>
|
<a href=/people/l/lu-wang/>Lu Wang</a><br><a href=/volumes/2021.newsum-1/ class=text-muted>Proceedings of the Third Workshop on New Frontiers in Summarization</a></span></p><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.aacl-main.63.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--aacl-main--63 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.aacl-main.63 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.aacl-main.63/>Improving <a href=https://en.wikipedia.org/wiki/Context_model>Context Modeling</a> in Neural Topic Segmentation</a></strong><br><a href=/people/l/linzi-xing/>Linzi Xing</a>
|
<a href=/people/b/brad-hackinen/>Brad Hackinen</a>
|
<a href=/people/g/giuseppe-carenini/>Giuseppe Carenini</a>
|
<a href=/people/f/francesco-trebbi/>Francesco Trebbi</a><br><a href=/volumes/2020.aacl-main/ class=text-muted>Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--aacl-main--63><div class="card-body p-3 small">Topic segmentation is critical in key NLP tasks and recent works favor highly effective neural supervised approaches. However, current neural solutions are arguably limited in how they model context. In this paper, we enhance a segmenter based on a hierarchical attention BiLSTM network to better model context, by adding a coherence-related auxiliary task and restricted self-attention. Our optimized segmenter outperforms SOTA approaches when trained and tested on three datasets. We also the robustness of our proposed <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> in domain transfer setting by training a <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> on a large-scale dataset and testing it on four challenging real-world benchmarks. Furthermore, we apply our proposed <a href=https://en.wikipedia.org/wiki/Strategy>strategy</a> to two other languages (German and Chinese), and show its effectiveness in multilingual scenarios.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.281.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--281 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.281 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.281/>Towards Domain-Independent Text Structuring Trainable on Large Discourse Treebanks</a></strong><br><a href=/people/g/grigorii-guz/>Grigorii Guz</a>
|
<a href=/people/g/giuseppe-carenini/>Giuseppe Carenini</a><br><a href=/volumes/2020.findings-emnlp/ class=text-muted>Findings of the Association for Computational Linguistics: EMNLP 2020</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--281><div class="card-body p-3 small">Text structuring is a fundamental step in NLG, especially when generating multi-sentential text. With the goal of fostering more general and data-driven approaches to text structuring, we propose the new and domain-independent NLG task of structuring and ordering a (possibly large) set of EDUs. We then present a solution for this task that combines neural dependency tree induction with pointer networks, and can be trained on large discourse treebanks that have only recently become available. Further, we propose a new evaluation metric that is arguably more suitable for our new <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> compared to existing content ordering metrics. Finally, we empirically show that our approach outperforms competitive alternatives on the proposed measure and is equivalent in performance with respect to previously established measures.</div></div><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1235.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1235 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1235 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1235/>Predicting Discourse Structure using Distant Supervision from Sentiment</a></strong><br><a href=/people/p/patrick-huber/>Patrick Huber</a>
|
<a href=/people/g/giuseppe-carenini/>Giuseppe Carenini</a><br><a href=/volumes/D19-1/ class=text-muted>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1235><div class="card-body p-3 small">Discourse parsing could not yet take full advantage of the neural NLP revolution, mostly due to the lack of annotated datasets. We propose a novel approach that uses distant supervision on an auxiliary task (sentiment classification), to generate abundant data for RST-style discourse structure prediction. Our approach combines a neural variant of multiple-instance learning, using document-level supervision, with an optimal CKY-style tree generation algorithm. In a series of experiments, we train a discourse parser (for only structure prediction) on our automatically generated dataset and compare it with parsers trained on human-annotated corpora (news domain RST-DT and Instructional domain). Results indicate that while our <a href=https://en.wikipedia.org/wiki/Parsing>parser</a> does not yet match the performance of a <a href=https://en.wikipedia.org/wiki/Parsing>parser</a> trained and tested on the same dataset (intra-domain), it does perform remarkably well on the much more difficult and arguably more useful task of inter-domain discourse structure prediction, where the <a href=https://en.wikipedia.org/wiki/Parsing>parser</a> is trained on one domain and tested / applied on another one.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5400.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5400/>Proceedings of the 2nd Workshop on New Frontiers in Summarization</a></strong><br><a href=/people/l/lu-wang/>Lu Wang</a>
|
<a href=/people/j/jackie-chi-kit-cheung/>Jackie Chi Kit Cheung</a>
|
<a href=/people/g/giuseppe-carenini/>Giuseppe Carenini</a>
|
<a href=/people/f/fei-liu-utdallas/>Fei Liu</a><br><a href=/volumes/D19-54/ class=text-muted>Proceedings of the 2nd Workshop on New Frontiers in Summarization</a></span></p><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1062.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1062 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1062 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/I17-1062.Datasets.tgz data-toggle=tooltip data-placement=top title=Dataset><i class="fas fa-file-archive"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/I17-1062/>Chat Disentanglement : Identifying Semantic Reply Relationships with Random Forests and <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>Recurrent Neural Networks</a></a></strong><br><a href=/people/s/shikib-mehri/>Shikib Mehri</a>
|
<a href=/people/g/giuseppe-carenini/>Giuseppe Carenini</a><br><a href=/volumes/I17-1/ class=text-muted>Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1062><div class="card-body p-3 small">Thread disentanglement is a precursor to any high-level analysis of multiparticipant chats. Existing research approaches the problem by calculating the likelihood of two messages belonging in the same thread. Our approach leverages a newly annotated dataset to identify reply relationships. Furthermore, we explore the usage of an RNN, along with large quantities of unlabeled data, to learn semantic relationships between messages. Our proposed <a href=https://en.wikipedia.org/wiki/Pipeline_(software)>pipeline</a>, which utilizes a reply classifier and an RNN to generate a set of disentangled threads, is novel and performs well against previous work.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2329.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2329 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2329 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W17-2329" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W17-2329/>Detecting Dementia through Retrospective Analysis of Routine Blog Posts by Bloggers with Dementia</a></strong><br><a href=/people/v/vaden-masrani/>Vaden Masrani</a>
|
<a href=/people/g/gabriel-murray/>Gabriel Murray</a>
|
<a href=/people/t/thalia-field/>Thalia Field</a>
|
<a href=/people/g/giuseppe-carenini/>Giuseppe Carenini</a><br><a href=/volumes/W17-23/ class=text-muted>BioNLP 2017</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2329><div class="card-body p-3 small">We investigate if writers with dementia can be automatically distinguished from those without by analyzing <a href=https://en.wikipedia.org/wiki/Marker_(linguistics)>linguistic markers</a> in <a href=https://en.wikipedia.org/wiki/Writing>written text</a>, in the form of <a href=https://en.wikipedia.org/wiki/Blog>blog posts</a>. We have built a <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> of several thousand <a href=https://en.wikipedia.org/wiki/Blog>blog posts</a>, some by people with dementia and others by people with loved ones with dementia. We use this <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> to train and test several <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning methods</a>, and achieve <a href=https://en.wikipedia.org/wiki/Prediction>prediction</a> performance at a level far above the <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4500.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4500/>Proceedings of the Workshop on New Frontiers in Summarization</a></strong><br><a href=/people/l/lu-wang/>Lu Wang</a>
|
<a href=/people/j/jackie-chi-kit-cheung/>Jackie Chi Kit Cheung</a>
|
<a href=/people/g/giuseppe-carenini/>Giuseppe Carenini</a>
|
<a href=/people/f/fei-liu-utdallas/>Fei Liu</a><br><a href=/volumes/W17-45/ class=text-muted>Proceedings of the Workshop on New Frontiers in Summarization</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4502.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4502 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4502 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4502/>Multimedia Summary Generation from Online Conversations : Current Approaches and Future Directions</a></strong><br><a href=/people/e/enamul-hoque/>Enamul Hoque</a>
|
<a href=/people/g/giuseppe-carenini/>Giuseppe Carenini</a><br><a href=/volumes/W17-45/ class=text-muted>Proceedings of the Workshop on New Frontiers in Summarization</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4502><div class="card-body p-3 small">With the proliferation of Web-based social media, asynchronous conversations have become very common for supporting <a href=https://en.wikipedia.org/wiki/Online_communication>online communication</a> and <a href=https://en.wikipedia.org/wiki/Collaboration>collaboration</a>. Yet the increasing volume and complexity of conversational data often make it very difficult to get insights about the discussions. We consider combining textual summary with visual representation of conversational data as a promising way of supporting the user in exploring conversations. In this paper, we report our current work on developing visual interfaces that present multimedia summary combining text and visualization for online conversations and how our solutions have been tailored for a variety of domain problems. We then discuss the key challenges and opportunities for future work in this research space.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4506.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4506 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4506 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4506/>Automatic Community Creation for Abstractive Spoken Conversations Summarization</a></strong><br><a href=/people/k/karan-singla/>Karan Singla</a>
|
<a href=/people/e/evgeny-stepanov/>Evgeny Stepanov</a>
|
<a href=/people/a/ali-orkan-bayer/>Ali Orkan Bayer</a>
|
<a href=/people/g/giuseppe-carenini/>Giuseppe Carenini</a>
|
<a href=/people/g/giuseppe-riccardi/>Giuseppe Riccardi</a><br><a href=/volumes/W17-45/ class=text-muted>Proceedings of the Workshop on New Frontiers in Summarization</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4506><div class="card-body p-3 small">Summarization of spoken conversations is a challenging task, since it requires deep understanding of dialogs. Abstractive summarization techniques rely on linking the summary sentences to sets of original conversation sentences, i.e. communities. Unfortunately, such linking information is rarely available or requires trained annotators. We propose and experiment automatic community creation using <a href=https://en.wikipedia.org/wiki/Cosine_similarity>cosine similarity</a> on different levels of representation : raw text, WordNet SynSet IDs, and <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a>. We show that the abstractive summarization systems with automatic communities significantly outperform previously published results on both English and Italian corpora.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5532.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5532 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5532 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5532/>Generating and Evaluating Summaries for Partial Email Threads : Conversational Bayesian Surprise and Silver Standards<span class=acl-fixed-case>B</span>ayesian Surprise and Silver Standards</a></strong><br><a href=/people/j/jordon-johnson/>Jordon Johnson</a>
|
<a href=/people/v/vaden-masrani/>Vaden Masrani</a>
|
<a href=/people/g/giuseppe-carenini/>Giuseppe Carenini</a>
|
<a href=/people/r/raymond-ng/>Raymond Ng</a><br><a href=/volumes/W17-55/ class=text-muted>Proceedings of the 18th Annual SIGdial Meeting on Discourse and Dialogue</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5532><div class="card-body p-3 small">We define and motivate the problem of summarizing partial email threads. This problem introduces the challenge of generating reference summaries for partial threads when human annotation is only available for the threads as a whole, particularly when the human-selected sentences are not uniformly distributed within the threads. We propose an oracular algorithm for generating these reference summaries with arbitrary length, and we are making the resulting <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> publicly available. In addition, we apply a recent <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised method</a> based on Bayesian Surprise that incorporates background knowledge into partial thread summarization, extend it with conversational features, and modify the mechanism by which it handles redundancy. Experiments with our method indicate improved performance over the baseline for shorter partial threads ; and our results suggest that the potential benefits of background knowledge to partial thread summarization should be further investigated with larger datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5535.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5535 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5535 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5535/>Exploring Joint Neural Model for Sentence Level Discourse Parsing and <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>Sentiment Analysis</a></a></strong><br><a href=/people/b/bita-nejat/>Bita Nejat</a>
|
<a href=/people/g/giuseppe-carenini/>Giuseppe Carenini</a>
|
<a href=/people/r/raymond-ng/>Raymond Ng</a><br><a href=/volumes/W17-55/ class=text-muted>Proceedings of the 18th Annual SIGdial Meeting on Discourse and Dialogue</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5535><div class="card-body p-3 small">Discourse Parsing and <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>Sentiment Analysis</a> are two fundamental tasks in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>Natural Language Processing</a> that have been shown to be mutually beneficial. In this work, we design and compare two Neural Based models for jointly learning both <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a>. In the proposed approach, we first create a <a href=https://en.wikipedia.org/wiki/Vector_graphics>vector representation</a> for all the <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>text segments</a> in the input sentence. Next, we apply three different Recursive Neural Net models : one for discourse structure prediction, one for discourse relation prediction and one for <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a>. Finally, we combine these Neural Nets in two different joint models : <a href=https://en.wikipedia.org/wiki/Computer_multitasking>Multi-tasking</a> and Pre-training. Our results on two standard corpora indicate that both methods result in improvements in each task but <a href=https://en.wikipedia.org/wiki/Multi-tasking>Multi-tasking</a> has a bigger impact than Pre-training. Specifically for Discourse Parsing, we see improvements in the prediction of the set of contrastive relations.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Giuseppe+Carenini" title="Search for 'Giuseppe Carenini' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/l/lu-wang/ class=align-middle>Lu Wang</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/j/jackie-chi-kit-cheung/ class=align-middle>Jackie Chi Kit Cheung</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/f/fei-liu-utdallas/ class=align-middle>Fei Liu</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/l/linzi-xing/ class=align-middle>Linzi Xing</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/v/vaden-masrani/ class=align-middle>Vaden Masrani</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/r/raymond-ng/ class=align-middle>Raymond Ng</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/w/wen-xiao/ class=align-middle>Wen Xiao</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/p/patrick-huber/ class=align-middle>Patrick Huber</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/s/shikib-mehri/ class=align-middle>Shikib Mehri</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/g/gabriel-murray/ class=align-middle>Gabriel Murray</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/thalia-field/ class=align-middle>Thalia Field</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/e/enamul-hoque/ class=align-middle>Enamul Hoque</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/karan-singla/ class=align-middle>Karan Singla</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/e/evgeny-stepanov/ class=align-middle>Evgeny Stepanov</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/ali-orkan-bayer/ class=align-middle>Ali Orkan Bayer</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/g/giuseppe-riccardi/ class=align-middle>Giuseppe Riccardi</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jordon-johnson/ class=align-middle>Jordon Johnson</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/b/bita-nejat/ class=align-middle>Bita Nejat</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/b/brad-hackinen/ class=align-middle>Brad Hackinen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/f/francesco-trebbi/ class=align-middle>Francesco Trebbi</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/raymond-li/ class=align-middle>Raymond Li</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/lanjun-wang/ class=align-middle>Lanjun Wang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/hyeju-jang/ class=align-middle>Hyeju Jang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/g/grigorii-guz/ class=align-middle>Grigorii Guz</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yue-dong/ class=align-middle>Yue Dong</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/ws/ class=align-middle>WS</a><span class="badge badge-secondary align-middle ml-2">6</span></li><li class=list-group-item><a href=/venues/emnlp/ class=align-middle>EMNLP</a><span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/venues/ijcnlp/ class=align-middle>IJCNLP</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/sigdial/ class=align-middle>SIGDIAL</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/aacl/ class=align-middle>AACL</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-venues aria-expanded=false aria-controls=more-venues>show all...</li><div class="collapse border-top" id=more-venues><li class=list-group-item><a href=/venues/naacl/ class=align-middle>NAACL</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/findings/ class=align-middle>Findings</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/newsum/ class=align-middle>newsum</a><span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>