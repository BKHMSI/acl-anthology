<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Gerhard Weikum - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Gerhard</span> <span class=font-weight-bold>Weikum</span></h2><hr><div class=row><div class=col-lg-9><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-demo.5.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-demo--5 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-demo.5 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-demo.5/>Inside ASCENT : Exploring a Deep Commonsense Knowledge Base and its Usage in Question Answering<span class=acl-fixed-case>ASCENT</span>: Exploring a Deep Commonsense Knowledge Base and its Usage in Question Answering</a></strong><br><a href=/people/t/tuan-phong-nguyen/>Tuan-Phong Nguyen</a>
|
<a href=/people/s/simon-razniewski/>Simon Razniewski</a>
|
<a href=/people/g/gerhard-weikum/>Gerhard Weikum</a><br><a href=/volumes/2021.acl-demo/ class=text-muted>Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-demo--5><div class="card-body p-3 small">ASCENT is a fully automated methodology for extracting and consolidating commonsense assertions from web contents (Nguyen et al., 2021). It advances traditional triple-based commonsense knowledge representation by capturing semantic facets like locations and purposes, and <a href=https://en.wikipedia.org/wiki/Compound_(linguistics)>composite concepts</a>, i.e., subgroups and related aspects of subjects. In this demo, we present a <a href=https://en.wikipedia.org/wiki/Web_portal>web portal</a> that allows users to understand its construction process, explore its content, and observe its impact in the use case of <a href=https://en.wikipedia.org/wiki/Question_answering>question answering</a>. The demo website (https://ascent.mpi-inf.mpg.de) and an introductory video (https://youtu.be/qMkJXqu_Yd4) are both available online.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.85.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--85 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.85 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.85/>SANDI : Story-and-Images Alignment<span class=acl-fixed-case>SANDI</span>: Story-and-Images Alignment</a></strong><br><a href=/people/s/sreyasi-nag-chowdhury/>Sreyasi Nag Chowdhury</a>
|
<a href=/people/s/simon-razniewski/>Simon Razniewski</a>
|
<a href=/people/g/gerhard-weikum/>Gerhard Weikum</a><br><a href=/volumes/2021.eacl-main/ class=text-muted>Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--85><div class="card-body p-3 small">The <a href=https://en.wikipedia.org/wiki/Internet>Internet</a> contains a multitude of social media posts and other of stories where <a href=https://en.wikipedia.org/wiki/Text_(literary_theory)>text</a> is interspersed with <a href=https://en.wikipedia.org/wiki/Image>images</a>. In these contexts, <a href=https://en.wikipedia.org/wiki/Image>images</a> are not simply used for general illustration, but are judiciously placed in certain spots of a story for multimodal descriptions and narration. In this work we analyze the problem of text-image alignment, and present SANDI, a methodology for automatically selecting images from an image collection and aligning them with text paragraphs of a story. SANDI combines visual tags, user-provided tags and background knowledge, and uses an Integer Linear Program to compute alignments that are semantically meaningful. Experiments show that SANDI can select and align images with texts with high quality of semantic fit.</div></div><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.434.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--434 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.434 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.emnlp-main.434.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939312 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.434/>CHARM : Inferring Personal Attributes from Conversations<span class=acl-fixed-case>CHARM</span>: Inferring Personal Attributes from Conversations</a></strong><br><a href=/people/a/anna-tigunova/>Anna Tigunova</a>
|
<a href=/people/a/andrew-yates/>Andrew Yates</a>
|
<a href=/people/p/paramita-mirza/>Paramita Mirza</a>
|
<a href=/people/g/gerhard-weikum/>Gerhard Weikum</a><br><a href=/volumes/2020.emnlp-main/ class=text-muted>Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--434><div class="card-body p-3 small">Personal knowledge about users&#8217; professions, hobbies, favorite food, and travel preferences, among others, is a valuable asset for individualized AI, such as <a href=https://en.wikipedia.org/wiki/Recommender_system>recommenders</a> or <a href=https://en.wikipedia.org/wiki/Chatbot>chatbots</a>. Conversations in <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>, such as <a href=https://en.wikipedia.org/wiki/Reddit>Reddit</a>, are a rich source of data for inferring personal facts. Prior work developed supervised methods to extract this knowledge, but these approaches can not generalize beyond attribute values with ample labeled training samples. This paper overcomes this limitation by devising CHARM : a zero-shot learning method that creatively leverages keyword extraction and document retrieval in order to predict attribute values that were never seen during training. Experiments with large datasets from <a href=https://en.wikipedia.org/wiki/Reddit>Reddit</a> show the viability of CHARM for open-ended attributes, such as <a href=https://en.wikipedia.org/wiki/Profession>professions</a> and <a href=https://en.wikipedia.org/wiki/Hobby>hobbies</a>.</div></div><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1675.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1675 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1675 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1675/>STANCY : Stance Classification Based on Consistency Cues<span class=acl-fixed-case>STANCY</span>: Stance Classification Based on Consistency Cues</a></strong><br><a href=/people/k/kashyap-popat/>Kashyap Popat</a>
|
<a href=/people/s/subhabrata-mukherjee/>Subhabrata Mukherjee</a>
|
<a href=/people/a/andrew-yates/>Andrew Yates</a>
|
<a href=/people/g/gerhard-weikum/>Gerhard Weikum</a><br><a href=/volumes/D19-1/ class=text-muted>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1675><div class="card-body p-3 small">Controversial claims are abundant in <a href=https://en.wikipedia.org/wiki/Mass_media>online media</a> and <a href=https://en.wikipedia.org/wiki/Internet_forum>discussion forums</a>. A better understanding of such claims requires analyzing them from different perspectives. Stance classification is a necessary step for inferring these <a href=https://en.wikipedia.org/wiki/Point_of_view_(philosophy)>perspectives</a> in terms of supporting or opposing the claim. In this work, we present a <a href=https://en.wikipedia.org/wiki/Neural_network>neural network model</a> for stance classification leveraging BERT representations and augmenting them with a novel consistency constraint. Experiments on the Perspectrum dataset, consisting of claims and users&#8217; perspectives from various debate websites, demonstrate the effectiveness of our approach over state-of-the-art baselines.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1027.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1027 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1027 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-1027/>ComQA : A Community-sourced Dataset for Complex Factoid Question Answering with Paraphrase Clusters<span class=acl-fixed-case>C</span>om<span class=acl-fixed-case>QA</span>: A Community-sourced Dataset for Complex Factoid Question Answering with Paraphrase Clusters</a></strong><br><a href=/people/a/abdalghani-abujabal/>Abdalghani Abujabal</a>
|
<a href=/people/r/rishiraj-saha-roy/>Rishiraj Saha Roy</a>
|
<a href=/people/m/mohamed-yahya/>Mohamed Yahya</a>
|
<a href=/people/g/gerhard-weikum/>Gerhard Weikum</a><br><a href=/volumes/N19-1/ class=text-muted>Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1027><div class="card-body p-3 small">To bridge the gap between the capabilities of the state-of-the-art in factoid question answering (QA) and what users ask, we need large datasets of real user questions that capture the various question phenomena users are interested in, and the diverse ways in which these <a href=https://en.wikipedia.org/wiki/Questionnaire>questions</a> are formulated. We introduce ComQA, a large dataset of real user questions that exhibit different challenging aspects such as <a href=https://en.wikipedia.org/wiki/Compositionality>compositionality</a>, temporal reasoning, and comparisons. ComQA questions come from the WikiAnswers community QA platform, which typically contains questions that are not satisfactorily answerable by existing search engine technology. Through a large crowdsourcing effort, we clean the question dataset, group questions into paraphrase clusters, and annotate <a href=https://en.wikipedia.org/wiki/Cluster_analysis>clusters</a> with their answers. ComQA contains 11,214 questions grouped into 4,834 paraphrase clusters. We detail the process of constructing ComQA, including the measures taken to ensure its high quality while making effective use of <a href=https://en.wikipedia.org/wiki/Crowdsourcing>crowdsourcing</a>. We also present an extensive analysis of the <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> and the results achieved by state-of-the-art systems on ComQA, demonstrating that our <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> can be a driver of future research on <a href=https://en.wikipedia.org/wiki/Quality_assurance>QA</a>.</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1003.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1003 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1003 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/305203523 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1003" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1003/>DeClarE : Debunking Fake News and False Claims using Evidence-Aware Deep Learning<span class=acl-fixed-case>D</span>e<span class=acl-fixed-case>C</span>lar<span class=acl-fixed-case>E</span>: Debunking Fake News and False Claims using Evidence-Aware Deep Learning</a></strong><br><a href=/people/k/kashyap-popat/>Kashyap Popat</a>
|
<a href=/people/s/subhabrata-mukherjee/>Subhabrata Mukherjee</a>
|
<a href=/people/a/andrew-yates/>Andrew Yates</a>
|
<a href=/people/g/gerhard-weikum/>Gerhard Weikum</a><br><a href=/volumes/D18-1/ class=text-muted>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1003><div class="card-body p-3 small">Misinformation such as <a href=https://en.wikipedia.org/wiki/Fake_news>fake news</a> is one of the big challenges of our society. Research on automated fact-checking has proposed methods based on <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised learning</a>, but these approaches do not consider external evidence apart from labeled training instances. Recent approaches counter this deficit by considering <a href=https://en.wikipedia.org/wiki/Source_text>external sources</a> related to a claim. However, these <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> require substantial <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>feature modeling</a> and rich lexicons. This paper overcomes these limitations of prior work with an end-to-end model for evidence-aware credibility assessment of arbitrary textual claims, without any human intervention. It presents a neural network model that judiciously aggregates signals from external evidence articles, the language of these articles and the trustworthiness of their sources. It also derives informative features for generating user-comprehensible explanations that makes the neural network predictions transparent to the end-user. Experiments with four datasets and ablation studies show the strength of our <a href=https://en.wikipedia.org/wiki/Methodology>method</a>.</div></div><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-2038.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-2038 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-2038 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-2038/>Efficiency-aware Answering of Compositional Questions using Answer Type Prediction</a></strong><br><a href=/people/d/david-ziegler/>David Ziegler</a>
|
<a href=/people/a/abdalghani-abujabal/>Abdalghani Abujabal</a>
|
<a href=/people/r/rishiraj-saha-roy/>Rishiraj Saha Roy</a>
|
<a href=/people/g/gerhard-weikum/>Gerhard Weikum</a><br><a href=/volumes/I17-2/ class=text-muted>Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-2038><div class="card-body p-3 small">This paper investigates the problem of answering compositional factoid questions over knowledge bases (KB) under efficiency constraints. The method, called TIPI, (i) decomposes compositional questions, (ii) predicts answer types for individual sub-questions, (iii) reasons over the compatibility of joint types, and finally, (iv) formulates compositional SPARQL queries respecting type constraints. TIPI&#8217;s answer type predictor is trained using distant supervision, and exploits lexical, syntactic and embedding-based features to compute context- and hierarchy-aware candidate answer types for an input question. Experiments on a recent <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmark</a> show that TIPI results in state-of-the-art performance under the real-world assumption that only a single SPARQL query can be executed over the <a href=https://en.wikipedia.org/wiki/Kilobyte>KB</a>, and substantial reduction in the number of queries in the more general case.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2055.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2055 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2055 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P17-2055.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P17-2055.Datasets.zip data-toggle=tooltip data-placement=top title=Dataset><i class="fas fa-file-archive"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-2055/>Cardinal Virtues : Extracting Relation Cardinalities from Text</a></strong><br><a href=/people/p/paramita-mirza/>Paramita Mirza</a>
|
<a href=/people/s/simon-razniewski/>Simon Razniewski</a>
|
<a href=/people/f/fariz-darari/>Fariz Darari</a>
|
<a href=/people/g/gerhard-weikum/>Gerhard Weikum</a><br><a href=/volumes/P17-2/ class=text-muted>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2055><div class="card-body p-3 small">Information extraction (IE) from text has largely focused on relations between individual entities, such as who has won which award. However, some facts are never fully mentioned, and no IE method has perfect <a href=https://en.wikipedia.org/wiki/Recall_(memory)>recall</a>. Thus, it is beneficial to also tap contents about the cardinalities of these relations, for example, how many awards someone has won. We introduce this novel <a href=https://en.wikipedia.org/wiki/Problem_solving>problem</a> of extracting cardinalities and discusses the specific challenges that set it apart from standard IE. We present a distant supervision method using <a href=https://en.wikipedia.org/wiki/Conditional_random_field>conditional random fields</a>. A preliminary evaluation results in <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>precision</a> between 3 % and 55 %, depending on the difficulty of relations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-2011.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-2011 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-2011 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-2011/>QUINT : Interpretable Question Answering over Knowledge Bases<span class=acl-fixed-case>QUINT</span>: Interpretable Question Answering over Knowledge Bases</a></strong><br><a href=/people/a/abdalghani-abujabal/>Abdalghani Abujabal</a>
|
<a href=/people/r/rishiraj-saha-roy/>Rishiraj Saha Roy</a>
|
<a href=/people/m/mohamed-yahya/>Mohamed Yahya</a>
|
<a href=/people/g/gerhard-weikum/>Gerhard Weikum</a><br><a href=/volumes/D17-2/ class=text-muted>Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-2011><div class="card-body p-3 small">We present QUINT, a live system for <a href=https://en.wikipedia.org/wiki/Question_answering>question answering</a> over <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge bases</a>. QUINT automatically learns role-aligned utterance-query templates from user questions paired with their answers. When QUINT answers a question, it visualizes the complete derivation sequence from the natural language utterance to the final answer. The derivation provides an explanation of how the <a href=https://en.wikipedia.org/wiki/Syntax_(programming_languages)>syntactic structure</a> of the question was used to derive the structure of a SPARQL query, and how the phrases in the question were used to instantiate different parts of the query. When an answer seems unsatisfactory, the <a href=https://en.wikipedia.org/wiki/Derivation_(logic)>derivation</a> provides valuable insights towards reformulating the question.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Gerhard+Weikum" title="Search for 'Gerhard Weikum' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/s/simon-razniewski/ class=align-middle>Simon Razniewski</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/a/abdalghani-abujabal/ class=align-middle>Abdalghani Abujabal</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/r/rishiraj-saha-roy/ class=align-middle>Rishiraj Saha Roy</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/a/andrew-yates/ class=align-middle>Andrew Yates</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/p/paramita-mirza/ class=align-middle>Paramita Mirza</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/k/kashyap-popat/ class=align-middle>Kashyap Popat</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/s/subhabrata-mukherjee/ class=align-middle>Subhabrata Mukherjee</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/m/mohamed-yahya/ class=align-middle>Mohamed Yahya</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/t/tuan-phong-nguyen/ class=align-middle>Tuan-Phong Nguyen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/david-ziegler/ class=align-middle>David Ziegler</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/anna-tigunova/ class=align-middle>Anna Tigunova</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/f/fariz-darari/ class=align-middle>Fariz Darari</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/sreyasi-nag-chowdhury/ class=align-middle>Sreyasi Nag Chowdhury</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/emnlp/ class=align-middle>EMNLP</a><span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/venues/acl/ class=align-middle>ACL</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/ijcnlp/ class=align-middle>IJCNLP</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/eacl/ class=align-middle>EACL</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/naacl/ class=align-middle>NAACL</a><span class="badge badge-secondary align-middle ml-2">1</span></li></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>