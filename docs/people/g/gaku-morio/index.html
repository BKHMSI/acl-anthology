<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Gaku Morio - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Gaku</span> <span class=font-weight-bold>Morio</span></h2><hr><div class=row><div class=col-lg-9><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.149.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--149 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.149 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.149/>Hitachi at SemEval-2020 Task 8 : Simple but Effective Modality Ensemble for Meme Emotion Recognition<span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 8: Simple but Effective Modality Ensemble for Meme Emotion Recognition</a></strong><br><a href=/people/t/terufumi-morishita/>Terufumi Morishita</a>
|
<a href=/people/g/gaku-morio/>Gaku Morio</a>
|
<a href=/people/s/shota-horiguchi/>Shota Horiguchi</a>
|
<a href=/people/h/hiroaki-ozaki/>Hiroaki Ozaki</a>
|
<a href=/people/t/toshinori-miyoshi/>Toshinori Miyoshi</a><br><a href=/volumes/2020.semeval-1/ class=text-muted>Proceedings of the Fourteenth Workshop on Semantic Evaluation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--149><div class="card-body p-3 small">Users of <a href=https://en.wikipedia.org/wiki/Social_networking_service>social networking services</a> often share their emotions via multi-modal content, usually <a href=https://en.wikipedia.org/wiki/Image>images</a> paired with text embedded in them. SemEval-2020 task 8, Memotion Analysis, aims at automatically recognizing these <a href=https://en.wikipedia.org/wiki/Emotion>emotions</a> of so-called <a href=https://en.wikipedia.org/wiki/Internet_meme>internet memes</a>. In this paper, we propose a simple but effective Modality Ensemble that incorporates visual and textual deep-learning models, which are independently trained, rather than providing a single multi-modal joint network. To this end, we first fine-tune four pre-trained <a href=https://en.wikipedia.org/wiki/Visual_system>visual models</a> (i.e., Inception-ResNet, PolyNet, SENet, and PNASNet) and four textual models (i.e., <a href=https://en.wikipedia.org/wiki/BERT>BERT</a>, GPT-2, Transformer-XL, and XLNet). Then, we fuse their predictions with <a href=https://en.wikipedia.org/wiki/Ensemble_learning>ensemble methods</a> to effectively capture cross-modal correlations. The experiments performed on dev-set show that both visual and textual features aided each other, especially in subtask-C, and consequently, our system ranked 2nd on subtask-C.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.conll-shared.4.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--conll-shared--4 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.conll-shared.4 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.conll-shared.4/>Hitachi at MRP 2020 : Text-to-Graph-Notation Transducer<span class=acl-fixed-case>MRP</span> 2020: Text-to-Graph-Notation Transducer</a></strong><br><a href=/people/h/hiroaki-ozaki/>Hiroaki Ozaki</a>
|
<a href=/people/g/gaku-morio/>Gaku Morio</a>
|
<a href=/people/y/yuta-koreeda/>Yuta Koreeda</a>
|
<a href=/people/t/terufumi-morishita/>Terufumi Morishita</a>
|
<a href=/people/t/toshinori-miyoshi/>Toshinori Miyoshi</a><br><a href=/volumes/2020.conll-shared/ class=text-muted>Proceedings of the CoNLL 2020 Shared Task: Cross-Framework Meaning Representation Parsing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--conll-shared--4><div class="card-body p-3 small">This paper presents our proposed <a href=https://en.wikipedia.org/wiki/Parsing>parser</a> for the shared task on Meaning Representation Parsing (MRP 2020) at CoNLL, where participant systems were required to parse five types of <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graphs</a> in different languages. We propose to unify these tasks as a text-to-graph-notation transduction in which we convert an input text into a <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph notation</a>. To this end, we designed a novel Plain Graph Notation (PGN) that handles various <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graphs</a> universally. Then, our <a href=https://en.wikipedia.org/wiki/Parsing>parser</a> predicts a PGN-based sequence by leveraging Transformers and biaffine attentions. Notably, our <a href=https://en.wikipedia.org/wiki/Parsing>parser</a> can handle any PGN-formatted graphs with fewer framework-specific modifications. As a result, ensemble versions of the <a href=https://en.wikipedia.org/wiki/Parsing>parser</a> tied for 1st place in both cross-framework and cross-lingual tracks.</div></div><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1653.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1653 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1653 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1653/>Revealing and Predicting Online Persuasion Strategy with Elementary Units</a></strong><br><a href=/people/g/gaku-morio/>Gaku Morio</a>
|
<a href=/people/r/ryo-egawa/>Ryo Egawa</a>
|
<a href=/people/k/katsuhide-fujita/>Katsuhide Fujita</a><br><a href=/volumes/D19-1/ class=text-muted>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1653><div class="card-body p-3 small">In online arguments, identifying how users construct their arguments to persuade others is important in order to understand a persuasive strategy directly. However, existing research lacks empirical investigations on highly semantic aspects of elementary units (EUs), such as propositions for a persuasive online argument. Therefore, this paper focuses on a pilot study, revealing a <a href=https://en.wikipedia.org/wiki/Persuasion>persuasion strategy</a> using <a href=https://en.wikipedia.org/wiki/European_Union>EUs</a>. Our contributions are as follows : (1) annotating five types of EUs in a persuasive forum, the so-called ChangeMyView, (2) revealing both intuitive and non-intuitive strategic insights for the persuasion by analyzing 4612 annotated EUs, and (3) proposing baseline neural models that identify the EU boundary and type. Our observations imply that <a href=https://en.wikipedia.org/wiki/European_Union>EUs</a> definitively characterize online persuasion strategies.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-2059.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-2059 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-2059 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-2059/>Annotating and Analyzing Semantic Role of Elementary Units and Relations in Online Persuasive Arguments</a></strong><br><a href=/people/r/ryo-egawa/>Ryo Egawa</a>
|
<a href=/people/g/gaku-morio/>Gaku Morio</a>
|
<a href=/people/k/katsuhide-fujita/>Katsuhide Fujita</a><br><a href=/volumes/P19-2/ class=text-muted>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-2059><div class="card-body p-3 small">For analyzing online persuasions, one of the important goals is to semantically understand how people construct comments to persuade others. However, analyzing the semantic role of arguments for online persuasion has been less emphasized. Therefore, in this study, we propose a novel annotation scheme that captures the semantic role of arguments in a popular online persuasion forum, so-called ChangeMyView. Through this study, we have made the following contributions : (i) proposing a <a href=https://en.wikipedia.org/wiki/Scheme_(mathematics)>scheme</a> that includes five types of elementary units (EUs) and two types of <a href=https://en.wikipedia.org/wiki/Binary_relation>relations</a>. (ii) annotating ChangeMyView which results in 4612 EUs and 2713 relations in 345 posts. (iii) analyzing the semantic role of persuasive arguments. Our analyses captured certain characteristic phenomena for <a href=https://en.wikipedia.org/wiki/Persuasion>online persuasion</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K19-2011.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K19-2011 data-toggle=collapse aria-expanded=false aria-controls=abstract-K19-2011 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/K19-2011/>Hitachi at MRP 2019 : Unified Encoder-to-Biaffine Network for Cross-Framework Meaning Representation Parsing<span class=acl-fixed-case>MRP</span> 2019: Unified Encoder-to-Biaffine Network for Cross-Framework Meaning Representation Parsing</a></strong><br><a href=/people/y/yuta-koreeda/>Yuta Koreeda</a>
|
<a href=/people/g/gaku-morio/>Gaku Morio</a>
|
<a href=/people/t/terufumi-morishita/>Terufumi Morishita</a>
|
<a href=/people/h/hiroaki-ozaki/>Hiroaki Ozaki</a>
|
<a href=/people/k/kohsuke-yanai/>Kohsuke Yanai</a><br><a href=/volumes/K19-2/ class=text-muted>Proceedings of the Shared Task on Cross-Framework Meaning Representation Parsing at the 2019 Conference on Natural Language Learning</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K19-2011><div class="card-body p-3 small">This paper describes the proposed system of the Hitachi team for the Cross-Framework Meaning Representation Parsing (MRP 2019) shared task. In this shared task, the participating systems were asked to predict <a href=https://en.wikipedia.org/wiki/Vertex_(graph_theory)>nodes</a>, <a href=https://en.wikipedia.org/wiki/Glossary_of_graph_theory_terms>edges</a> and their attributes for five <a href=https://en.wikipedia.org/wiki/Software_framework>frameworks</a>, each with different order of abstraction from input tokens. We proposed a unified encoder-to-biaffine network for all five frameworks, which effectively incorporates a shared encoder to extract rich input features, decoder networks to generate anchorless nodes in UCCA and AMR, and biaffine networks to predict edges. Our system was ranked fifth with the macro-averaged MRP F1 score of 0.7604, and outperformed the baseline unified transition-based MRP. Furthermore, post-evaluation experiments showed that we can boost the performance of the proposed <a href=https://en.wikipedia.org/wiki/System>system</a> by incorporating <a href=https://en.wikipedia.org/wiki/Multi-task_learning>multi-task learning</a>, whereas the <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline</a> could not. These imply efficacy of incorporating the biaffine network to the shared architecture for MRP and that learning heterogeneous meaning representations at once can boost the system performance.</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5202.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5202 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5202 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5202/>End-to-End Argument Mining for Discussion Threads Based on Parallel Constrained Pointer Architecture</a></strong><br><a href=/people/g/gaku-morio/>Gaku Morio</a>
|
<a href=/people/k/katsuhide-fujita/>Katsuhide Fujita</a><br><a href=/volumes/W18-52/ class=text-muted>Proceedings of the 5th Workshop on Argument Mining</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5202><div class="card-body p-3 small">Argument Mining (AM) is a relatively recent discipline, which concentrates on extracting claims or premises from discourses, and inferring their structures. However, many existing works do not consider micro-level AM studies on <a href=https://en.wikipedia.org/wiki/Conversation_threading>discussion threads</a> sufficiently. In this paper, we tackle <a href=https://en.wikipedia.org/wiki/Amplitude_modulation>AM</a> for <a href=https://en.wikipedia.org/wiki/Internet_forum>discussion threads</a>. Our main contributions are follows : (1) A novel combination scheme focusing on micro-level inner- and inter- post schemes for a discussion thread. (2) Annotation of large-scale civic discussion threads with the <a href=https://en.wikipedia.org/wiki/Scheme_(mathematics)>scheme</a>. (3) Parallel constrained pointer architecture (PCPA), a novel end-to-end technique to discriminate sentence types, inner-post relations, and inter-post interactions simultaneously. The experimental results demonstrate that our proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> shows better <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> in terms of relations extraction, in comparison to existing state-of-the-art models.<i>Parallel constrained pointer architecture</i> (PCPA), a novel end-to-end technique to discriminate sentence types, inner-post relations, and inter-post interactions simultaneously. The experimental results demonstrate that our proposed model shows better accuracy in terms of relations extraction, in comparison to existing state-of-the-art models.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Gaku+Morio" title="Search for 'Gaku Morio' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/t/terufumi-morishita/ class=align-middle>Terufumi Morishita</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/h/hiroaki-ozaki/ class=align-middle>Hiroaki Ozaki</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/k/katsuhide-fujita/ class=align-middle>Katsuhide Fujita</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/t/toshinori-miyoshi/ class=align-middle>Toshinori Miyoshi</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/y/yuta-koreeda/ class=align-middle>Yuta Koreeda</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/r/ryo-egawa/ class=align-middle>Ryo Egawa</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/s/shota-horiguchi/ class=align-middle>Shota Horiguchi</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/kohsuke-yanai/ class=align-middle>Kohsuke Yanai</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/conll/ class=align-middle>CoNLL</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/semeval/ class=align-middle>SemEval</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/emnlp/ class=align-middle>EMNLP</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/ws/ class=align-middle>WS</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/acl/ class=align-middle>ACL</a><span class="badge badge-secondary align-middle ml-2">1</span></li></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>