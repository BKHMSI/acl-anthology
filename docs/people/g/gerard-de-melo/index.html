<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Gerard de Melo - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Gerard</span> <span class=font-weight-bold>de Melo</span></h2><hr><div class=row><div class=col-lg-9><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.379.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--379 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.379 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.379" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.379/>R2D2 : Recursive Transformer based on Differentiable Tree for Interpretable Hierarchical Language Modeling<span class=acl-fixed-case>R</span>2<span class=acl-fixed-case>D</span>2: Recursive Transformer based on Differentiable Tree for Interpretable Hierarchical Language Modeling</a></strong><br><a href=/people/x/xiang-hu/>Xiang Hu</a>
|
<a href=/people/h/haitao-mi/>Haitao Mi</a>
|
<a href=/people/z/zujie-wen/>Zujie Wen</a>
|
<a href=/people/y/yafang-wang/>Yafang Wang</a>
|
<a href=/people/y/yi-su/>Yi Su</a>
|
<a href=/people/j/jing-zheng/>Jing Zheng</a>
|
<a href=/people/g/gerard-de-melo/>Gerard de Melo</a><br><a href=/volumes/2021.acl-long/ class=text-muted>Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--379><div class="card-body p-3 small">Human language understanding operates at multiple levels of granularity (e.g., words, phrases, and sentences) with increasing levels of <a href=https://en.wikipedia.org/wiki/Abstraction>abstraction</a> that can be hierarchically combined. However, existing <a href=https://en.wikipedia.org/wiki/Deep_learning>deep models</a> with stacked layers do not explicitly model any sort of hierarchical process. In this paper, we propose a recursive Transformer model based on differentiable CKY style binary trees to emulate this composition process, and we extend the bidirectional language model pre-training objective to this architecture, attempting to predict each word given its left and right abstraction nodes. To scale up our approach, we also introduce an efficient pruning and growing algorithm to reduce the <a href=https://en.wikipedia.org/wiki/Time_complexity>time complexity</a> and enable <a href=https://en.wikipedia.org/wiki/Code>encoding</a> in <a href=https://en.wikipedia.org/wiki/Time_complexity>linear time</a>. Experimental results on <a href=https://en.wikipedia.org/wiki/Language_model>language modeling</a> and unsupervised parsing show the effectiveness of our approach.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.781.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--781 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.781 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.781/>Guilt by Association : Emotion Intensities in Lexical Representations</a></strong><br><a href=/people/s/shahab-raji/>Shahab Raji</a>
|
<a href=/people/g/gerard-de-melo/>Gerard de Melo</a><br><a href=/volumes/2021.emnlp-main/ class=text-muted>Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--781><div class="card-body p-3 small">What do linguistic models reveal about the <a href=https://en.wikipedia.org/wiki/Emotion>emotions</a> associated with words? In this study, we consider the task of estimating word-level emotion intensity scores for specific emotions, exploring unsupervised, supervised, and finally a self-supervised method of extracting emotional associations from pretrained vectors and models. Overall, we find that linguistic models carry substantial potential for inducing fine-grained emotion intensity scores, showing a far higher correlation with human ground truth ratings than state-of-the-art emotion lexicons based on labeled data.</div></div><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.300.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--300 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.300 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.300/>Sentence Analogies : Linguistic Regularities in Sentence Embeddings</a></strong><br><a href=/people/x/xunjie-zhu/>Xunjie Zhu</a>
|
<a href=/people/g/gerard-de-melo/>Gerard de Melo</a><br><a href=/volumes/2020.coling-main/ class=text-muted>Proceedings of the 28th International Conference on Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--300><div class="card-body p-3 small">While important properties of word vector representations have been studied extensively, far less is known about the properties of sentence vector representations. Word vectors are often evaluated by assessing to what degree they exhibit regularities with regard to relationships of the sort considered in word analogies. In this paper, we investigate to what extent commonly used sentence vector representation spaces as well reflect certain kinds of regularities. We propose a number of schemes to induce evaluation data, based on lexical analogy data as well as semantic relationships between sentences. Our experiments consider a wide range of sentence embedding methods, including ones based on BERT-style contextual embeddings. We find that different <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> differ substantially in their ability to reflect such regularities.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-industry.4.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-industry--4 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-industry.4 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-industry.4/>Query Distillation : BERT-based Distillation for Ensemble Ranking<span class=acl-fixed-case>BERT</span>-based Distillation for Ensemble Ranking</a></strong><br><a href=/people/w/wangshu-zhang/>Wangshu Zhang</a>
|
<a href=/people/j/junhong-liu/>Junhong Liu</a>
|
<a href=/people/z/zujie-wen/>Zujie Wen</a>
|
<a href=/people/y/yafang-wang/>Yafang Wang</a>
|
<a href=/people/g/gerard-de-melo/>Gerard de Melo</a><br><a href=/volumes/2020.coling-industry/ class=text-muted>Proceedings of the 28th International Conference on Computational Linguistics: Industry Track</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-industry--4><div class="card-body p-3 small">Recent years have witnessed substantial progress in the development of neural ranking networks, but also an increasingly heavy computational burden due to growing numbers of parameters and the adoption of model ensembles. Knowledge Distillation (KD) is a common solution to balance the effectiveness and efficiency. However, it is not straightforward to apply KD to ranking problems. Ranking Distillation (RD) has been proposed to address this issue, but only shows effectiveness on recommendation tasks. We present a novel two-stage distillation method for ranking problems that allows a smaller student model to be trained while benefitting from the better performance of the teacher model, providing better control of the inference latency and computational burden. We design a novel BERT-based ranking model structure for list-wise ranking to serve as our student model. All ranking candidates are fed to the BERT model simultaneously, such that the self-attention mechanism can enable joint inference to rank the document list. Our experiments confirm the advantages of our method, not just with regard to the inference latency but also in terms of higher-quality rankings compared to the original teacher model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-industry.8.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-industry--8 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-industry.8 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-industry.8/>Interactive Question Clarification in Dialogue via <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>Reinforcement Learning</a></a></strong><br><a href=/people/x/xiang-hu/>Xiang Hu</a>
|
<a href=/people/z/zujie-wen/>Zujie Wen</a>
|
<a href=/people/y/yafang-wang/>Yafang Wang</a>
|
<a href=/people/x/xiaolong-li/>Xiaolong Li</a>
|
<a href=/people/g/gerard-de-melo/>Gerard de Melo</a><br><a href=/volumes/2020.coling-industry/ class=text-muted>Proceedings of the 28th International Conference on Computational Linguistics: Industry Track</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-industry--8><div class="card-body p-3 small">Coping with ambiguous questions has been a perennial problem in real-world dialogue systems. Although clarification by asking questions is a common form of <a href=https://en.wikipedia.org/wiki/Human&#8211;computer_interaction>human interaction</a>, it is hard to define appropriate questions to elicit more specific intents from a user. In this work, we propose a <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement model</a> to clarify ambiguous questions by suggesting refinements of the original query. We first formulate a collection partitioning problem to select a set of labels enabling us to distinguish potential unambiguous intents. We list the chosen labels as intent phrases to the user for further confirmation. The selected label along with the original user query then serves as a refined query, for which a suitable response can more easily be identified. The <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is trained using <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning</a> with a <a href=https://en.wikipedia.org/wiki/Deep_learning>deep policy network</a>. We evaluate our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> based on real-world user clicks and demonstrate significant improvements across several different experiments.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.382.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--382 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.382 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.382/>Inducing Universal Semantic Tag Vectors</a></strong><br><a href=/people/d/da-huo/>Da Huo</a>
|
<a href=/people/g/gerard-de-melo/>Gerard de Melo</a><br><a href=/volumes/2020.lrec-1/ class=text-muted>Proceedings of the 12th Language Resources and Evaluation Conference</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--382><div class="card-body p-3 small">Given the well-established usefulness of part-of-speech tag annotations in many syntactically oriented downstream NLP tasks, the recently proposed notion of semantic tagging (Bjerva et al. 2016) aims at tagging words with tags informed by semantic distinctions, which are likely to be useful across a range of semantic tasks. To this end, their annotation scheme distinguishes, for instance, privative attributes from subsective ones. While annotated corpora exist, their size is limited and thus many words are out-of-vocabulary words. In this paper, we study to what extent we can automatically predict the <a href=https://en.wikipedia.org/wiki/Tag_(metadata)>tags</a> associated with unseen words. We draw on large-scale word representation data to derive a large new Semantic Tag lexicon. Our experiments show that we can infer semantic tags for words with high <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> both monolingually and cross-lingually.</div></div><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1658.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1658 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1658 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1658/>A Robust Self-Learning Framework for Cross-Lingual Text Classification</a></strong><br><a href=/people/x/xin-luna-dong/>Xin Dong</a>
|
<a href=/people/g/gerard-de-melo/>Gerard de Melo</a><br><a href=/volumes/D19-1/ class=text-muted>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1658><div class="card-body p-3 small">Based on massive amounts of data, recent pretrained contextual representation models have made significant strides in advancing a number of different English NLP tasks. However, for other languages, relevant training data may be lacking, while state-of-the-art deep learning methods are known to be data-hungry. In this paper, we present an elegantly simple robust self-learning framework to include unlabeled non-English samples in the fine-tuning process of pretrained multilingual representation models. We leverage a multilingual model&#8217;s own predictions on unlabeled non-English data in order to obtain additional information that can be used during further <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a>. Compared with original multilingual models and other cross-lingual classification models, we observe significant gains in effectiveness on document and sentiment classification for a range of diverse languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-0421.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-0421 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-0421 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-0421" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-0421/>Using Multi-Sense Vector Embeddings for Reverse Dictionaries</a></strong><br><a href=/people/m/michael-a-hedderich/>Michael A. Hedderich</a>
|
<a href=/people/a/andrew-yates/>Andrew Yates</a>
|
<a href=/people/d/dietrich-klakow/>Dietrich Klakow</a>
|
<a href=/people/g/gerard-de-melo/>Gerard de Melo</a><br><a href=/volumes/W19-04/ class=text-muted>Proceedings of the 13th International Conference on Computational Semantics - Long Papers</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-0421><div class="card-body p-3 small">Popular word embedding methods such as <a href=https://en.wikipedia.org/wiki/Word2vec>word2vec</a> and GloVe assign a single vector representation to each word, even if a word has multiple distinct meanings. Multi-sense embeddings instead provide different vectors for each sense of a word. However, they typically can not serve as a drop-in replacement for conventional single-sense embeddings, because the correct sense vector needs to be selected for each word. In this work, we study the effect of multi-sense embeddings on the task of <a href=https://en.wikipedia.org/wiki/Reverse_dictionary>reverse dictionaries</a>. We propose a <a href=https://en.wikipedia.org/wiki/Scientific_technique>technique</a> to easily integrate them into an existing <a href=https://en.wikipedia.org/wiki/Neural_network>neural network architecture</a> using an <a href=https://en.wikipedia.org/wiki/Attentional_control>attention mechanism</a>. Our experiments demonstrate that large improvements can be obtained when employing multi-sense embeddings both in the input sequence as well as for the target representation. An analysis of the sense distributions and of the learned attention is provided as well.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1192.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1192 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1192 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1192" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-1192/>Rhetorically Controlled Encoder-Decoder for Modern Chinese Poetry Generation<span class=acl-fixed-case>M</span>odern <span class=acl-fixed-case>C</span>hinese Poetry Generation</a></strong><br><a href=/people/z/zhiqiang-liu/>Zhiqiang Liu</a>
|
<a href=/people/z/zuohui-fu/>Zuohui Fu</a>
|
<a href=/people/j/jie-cao/>Jie Cao</a>
|
<a href=/people/g/gerard-de-melo/>Gerard de Melo</a>
|
<a href=/people/y/yik-cheung-tam/>Yik-Cheung Tam</a>
|
<a href=/people/c/cheng-niu/>Cheng Niu</a>
|
<a href=/people/j/jie-zhou/>Jie Zhou</a><br><a href=/volumes/P19-1/ class=text-muted>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1192><div class="card-body p-3 small">Rhetoric is a vital element in modern <a href=https://en.wikipedia.org/wiki/Poetry>poetry</a>, and plays an essential role in improving its <a href=https://en.wikipedia.org/wiki/Aesthetics>aesthetics</a>. However, to date, it has not been considered in research on automatic poetry generation. In this paper, we propose a rhetorically controlled encoder-decoder for modern Chinese poetry generation. Our model relies on a <a href=https://en.wikipedia.org/wiki/Latent_variable_model>continuous latent variable</a> as a rhetoric controller to capture various rhetorical patterns in an <a href=https://en.wikipedia.org/wiki/Encoder>encoder</a>, and then incorporates rhetoric-based mixtures while generating <a href=https://en.wikipedia.org/wiki/Modern_Chinese_poetry>modern Chinese poetry</a>. For metaphor and personification, an automated evaluation shows that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms state-of-the-art baselines by a substantial margin, while human evaluation shows that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> generates better poems than baseline methods in terms of fluency, coherence, meaningfulness, and rhetorical aesthetics.</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/Q18-1013.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-Q18-1013 data-toggle=collapse aria-expanded=false aria-controls=abstract-Q18-1013 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/Q18-1013/>Video Captioning with Multi-Faceted Attention</a></strong><br><a href=/people/x/xiang-long/>Xiang Long</a>
|
<a href=/people/c/chuang-gan/>Chuang Gan</a>
|
<a href=/people/g/gerard-de-melo/>Gerard de Melo</a><br><a href=/volumes/Q18-1/ class=text-muted>Transactions of the Association for Computational Linguistics, Volume 6</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-Q18-1013><div class="card-body p-3 small">Video captioning has attracted an increasing amount of interest, due in part to its potential for improved <a href=https://en.wikipedia.org/wiki/Accessibility>accessibility</a> and <a href=https://en.wikipedia.org/wiki/Information_retrieval>information retrieval</a>. While existing methods rely on different kinds of <a href=https://en.wikipedia.org/wiki/Visual_system>visual features</a> and model architectures, they do not make full use of pertinent semantic cues. We present a unified and extensible framework to jointly leverage multiple sorts of <a href=https://en.wikipedia.org/wiki/Feature_(computer_vision)>visual features</a> and <a href=https://en.wikipedia.org/wiki/Semantic_Web>semantic attributes</a>. Our novel <a href=https://en.wikipedia.org/wiki/Architecture>architecture</a> builds on LSTMs with two multi-faceted attention layers. These first learn to automatically select the most salient visual features or semantic attributes, and then yield overall representations for the input and output of the sentence generation component via custom feature scaling operations. Experimental results on the challenging MSVD and MSR-VTT datasets show that our framework outperforms previous work and performs robustly even in the presence of added noise to the features and attributes.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1235.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1235 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1235 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P18-1235/>A Helping Hand : <a href=https://en.wikipedia.org/wiki/Transfer_learning>Transfer Learning</a> for Deep Sentiment Analysis</a></strong><br><a href=/people/x/xin-luna-dong/>Xin Dong</a>
|
<a href=/people/g/gerard-de-melo/>Gerard de Melo</a><br><a href=/volumes/P18-1/ class=text-muted>Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1235><div class="card-body p-3 small">Deep convolutional neural networks excel at sentiment polarity classification, but tend to require substantial amounts of training data, which moreover differs quite significantly between domains. In this work, we present an approach to feed generic cues into the training process of such <a href=https://en.wikipedia.org/wiki/Neural_network>networks</a>, leading to better generalization abilities given limited training data. We propose to induce sentiment embeddings via supervision on extrinsic data, which are then fed into the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> via a dedicated memory-based component. We observe significant gains in effectiveness on a range of different <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> in seven different languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2100.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2100 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2100 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2100.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285805830 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-2100/>Exploring Semantic Properties of Sentence Embeddings</a></strong><br><a href=/people/x/xunjie-zhu/>Xunjie Zhu</a>
|
<a href=/people/t/tingfeng-li/>Tingfeng Li</a>
|
<a href=/people/g/gerard-de-melo/>Gerard de Melo</a><br><a href=/volumes/P18-2/ class=text-muted>Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2100><div class="card-body p-3 small">Neural vector representations are ubiquitous throughout all subfields of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a>. While word vectors have been studied in much detail, thus far only little light has been shed on the properties of <a href=https://en.wikipedia.org/wiki/Sentence_embedding>sentence embeddings</a>. In this paper, we assess to what extent prominent sentence embedding methods exhibit select semantic properties. We propose a framework that generate triplets of sentences to explore how changes in the <a href=https://en.wikipedia.org/wiki/Syntax>syntactic structure</a> or <a href=https://en.wikipedia.org/wiki/Semantics>semantics</a> of a given sentence affect the similarities obtained between their sentence embeddings.</div></div><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-5002.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-5002 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-5002 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-5002/>Multilingual Vector Representations of Words, Sentences, and Documents</a></strong><br><a href=/people/g/gerard-de-melo/>Gerard de Melo</a><br><a href=/volumes/I17-5/ class=text-muted>Proceedings of the IJCNLP 2017, Tutorial Abstracts</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-5002><div class="card-body p-3 small">Neural vector representations are now ubiquitous in all subfields of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a> and <a href=https://en.wikipedia.org/wiki/Text_mining>text mining</a>. While methods such as <a href=https://en.wikipedia.org/wiki/Word2vec>word2vec</a> and GloVe are well-known, this tutorial focuses on multilingual and cross-lingual vector representations, of words, but also of sentences and documents as well.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1110.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1110 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1110 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/238235171 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D17-1110" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/D17-1110/>PACRR : A Position-Aware Neural IR Model for Relevance Matching<span class=acl-fixed-case>PACRR</span>: A Position-Aware Neural <span class=acl-fixed-case>IR</span> Model for Relevance Matching</a></strong><br><a href=/people/k/kai-hui/>Kai Hui</a>
|
<a href=/people/a/andrew-yates/>Andrew Yates</a>
|
<a href=/people/k/klaus-berberich/>Klaus Berberich</a>
|
<a href=/people/g/gerard-de-melo/>Gerard de Melo</a><br><a href=/volumes/D17-1/ class=text-muted>Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1110><div class="card-body p-3 small">In order to adopt <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning</a> for <a href=https://en.wikipedia.org/wiki/Information_retrieval>information retrieval</a>, <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> are needed that can capture all relevant information required to assess the relevance of a document to a given user query. While previous works have successfully captured unigram term matches, how to fully employ position-dependent information such as proximity and term dependencies has been insufficiently explored. In this work, we propose a novel neural IR model named PACRR aiming at better modeling position-dependent interactions between a query and a document. Extensive experiments on six years&#8217; TREC Web Track data confirm that the proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> yields better results under multiple benchmarks.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Gerard+de+Melo" title="Search for 'Gerard de Melo' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/z/zujie-wen/ class=align-middle>Zujie Wen</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/y/yafang-wang/ class=align-middle>Yafang Wang</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/x/xiang-hu/ class=align-middle>Xiang Hu</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/x/xin-luna-dong/ class=align-middle>Xin Luna Dong</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/a/andrew-yates/ class=align-middle>Andrew Yates</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/x/xunjie-zhu/ class=align-middle>Xunjie Zhu</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/h/haitao-mi/ class=align-middle>Haitao Mi</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yi-su/ class=align-middle>Yi Su</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jing-zheng/ class=align-middle>Jing Zheng</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/shahab-raji/ class=align-middle>Shahab Raji</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/x/xiang-long/ class=align-middle>Xiang Long</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/chuang-gan/ class=align-middle>Chuang Gan</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/kai-hui/ class=align-middle>Kai Hui</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/klaus-berberich/ class=align-middle>Klaus Berberich</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/michael-a-hedderich/ class=align-middle>Michael A. Hedderich</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/dietrich-klakow/ class=align-middle>Dietrich Klakow</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/w/wangshu-zhang/ class=align-middle>Wangshu Zhang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/junhong-liu/ class=align-middle>Junhong Liu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/x/xiaolong-li/ class=align-middle>Xiaolong Li</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/da-huo/ class=align-middle>Da Huo</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/tingfeng-li/ class=align-middle>Tingfeng Li</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/zhiqiang-liu/ class=align-middle>Zhiqiang Liu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/zuohui-fu/ class=align-middle>Zuohui Fu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jie-cao/ class=align-middle>Jie Cao</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yik-cheung-tam/ class=align-middle>Yik-Cheung Tam</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/cheng-niu/ class=align-middle>Cheng Niu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jie-zhou/ class=align-middle>Jie Zhou</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/acl/ class=align-middle>ACL</a><span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/venues/emnlp/ class=align-middle>EMNLP</a><span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/venues/coling/ class=align-middle>COLING</a><span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/venues/ijcnlp/ class=align-middle>IJCNLP</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/tacl/ class=align-middle>TACL</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-venues aria-expanded=false aria-controls=more-venues>show all...</li><div class="collapse border-top" id=more-venues><li class=list-group-item><a href=/venues/ws/ class=align-middle>WS</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/lrec/ class=align-middle>LREC</a><span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>