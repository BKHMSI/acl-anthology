<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Gregor Wiedemann - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Gregor</span> <span class=font-weight-bold>Wiedemann</span></h2><hr><div class=row><div class=col-lg-9><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-demos.8.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-demos--8 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-demos.8 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-demos.8/>Forum 4.0 : An Open-Source User Comment Analysis Framework</a></strong><br><a href=/people/m/marlo-haering/>Marlo Haering</a>
|
<a href=/people/j/jakob-smedegaard-andersen/>Jakob Smedegaard Andersen</a>
|
<a href=/people/c/chris-biemann/>Chris Biemann</a>
|
<a href=/people/w/wiebke-loosen/>Wiebke Loosen</a>
|
<a href=/people/b/benjamin-milde/>Benjamin Milde</a>
|
<a href=/people/t/tim-pietz/>Tim Pietz</a>
|
<a href=/people/c/christian-stocker/>Christian Stöcker</a>
|
<a href=/people/g/gregor-wiedemann/>Gregor Wiedemann</a>
|
<a href=/people/o/olaf-zukunft/>Olaf Zukunft</a>
|
<a href=/people/w/walid-maalej/>Walid Maalej</a><br><a href=/volumes/2021.eacl-demos/ class=text-muted>Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: System Demonstrations</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-demos--8><div class="card-body p-3 small">With the increasing number of user comments in diverse domains, including comments on <a href=https://en.wikipedia.org/wiki/Digital_journalism>online journalism</a> and <a href=https://en.wikipedia.org/wiki/E-commerce>e-commerce websites</a>, the manual content analysis of these comments becomes time-consuming and challenging. However, research showed that user comments contain useful information for different domain experts, which is thus worth finding and utilizing. This paper introduces Forum 4.0, an open-source framework to semi-automatically analyze, aggregate, and visualize user comments based on labels defined by domain experts. We demonstrate the applicability of Forum 4.0 with comments analytics scenarios within the domains of <a href=https://en.wikipedia.org/wiki/Digital_journalism>online journalism</a> and <a href=https://en.wikipedia.org/wiki/App_store>app stores</a>. We outline the underlying container architecture, including the <a href=https://en.wikipedia.org/wiki/Web_interface>web-based user interface</a>, the <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning component</a>, and the <a href=https://en.wikipedia.org/wiki/Task_manager>task manager</a> for time-consuming tasks. We finally conduct <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning</a> experiments with simulated annotations and different <a href=https://en.wikipedia.org/wiki/Sampling_(statistics)>sampling strategies</a> on existing <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> from both domains to evaluate Forum 4.0&#8217;s performance. Forum 4.0 achieves promising classification results (ROC-AUC 0.9 with 100 annotated samples), utilizing transformer-based embeddings with a lightweight <a href=https://en.wikipedia.org/wiki/Logistic_regression>logistic regression model</a>. We explain how Forum 4.0&#8217;s architecture is applicable for millions of user comments in real-time, yet at feasible training and classification costs.</div></div><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.213.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--213 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.213 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.213/>UHH-LT at SemEval-2020 Task 12 : Fine-Tuning of Pre-Trained Transformer Networks for Offensive Language Detection<span class=acl-fixed-case>UHH</span>-<span class=acl-fixed-case>LT</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 12: Fine-Tuning of Pre-Trained Transformer Networks for Offensive Language Detection</a></strong><br><a href=/people/g/gregor-wiedemann/>Gregor Wiedemann</a>
|
<a href=/people/s/seid-muhie-yimam/>Seid Muhie Yimam</a>
|
<a href=/people/c/chris-biemann/>Chris Biemann</a><br><a href=/volumes/2020.semeval-1/ class=text-muted>Proceedings of the Fourteenth Workshop on Semantic Evaluation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--213><div class="card-body p-3 small">Fine-tuning of pre-trained transformer networks such as BERT yield state-of-the-art results for text classification tasks. Typically, <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a> is performed on <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>task-specific training datasets</a> in a supervised manner. One can also fine-tune in unsupervised manner beforehand by further pre-training the masked language modeling (MLM) task. Hereby, in-domain data for <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised MLM</a> resembling the actual classification target dataset allows for domain adaptation of the <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a>. In this paper, we compare current pre-trained transformer networks with and without MLM fine-tuning on their performance for offensive language detection. Our MLM fine-tuned RoBERTa-based classifier officially ranks 1st in the SemEval 2020 Shared Task 12 for the <a href=https://en.wikipedia.org/wiki/English_language>English language</a>. Further experiments with the ALBERT model even surpass this result.</div></div><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S19-2137.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S19-2137 data-toggle=collapse aria-expanded=false aria-controls=abstract-S19-2137 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/S19-2137/>UHH-LT at SemEval-2019 Task 6 : Supervised vs. Unsupervised Transfer Learning for Offensive Language Detection<span class=acl-fixed-case>UHH</span>-<span class=acl-fixed-case>LT</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2019 Task 6: Supervised vs. Unsupervised Transfer Learning for Offensive Language Detection</a></strong><br><a href=/people/g/gregor-wiedemann/>Gregor Wiedemann</a>
|
<a href=/people/e/eugen-ruppert/>Eugen Ruppert</a>
|
<a href=/people/c/chris-biemann/>Chris Biemann</a><br><a href=/volumes/S19-2/ class=text-muted>Proceedings of the 13th International Workshop on Semantic Evaluation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S19-2137><div class="card-body p-3 small">We present a neural network based approach of <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a> for offensive language detection. For our <a href=https://en.wikipedia.org/wiki/System>system</a>, we compare two types of <a href=https://en.wikipedia.org/wiki/Knowledge_transfer>knowledge transfer</a> : supervised and unsupervised pre-training. Supervised pre-training of our bidirectional GRU-3-CNN architecture is performed as multi-task learning of parallel training of five different tasks. The selected tasks are supervised classification problems from public NLP resources with some overlap to offensive language such as sentiment detection, emoji classification, and aggressive language classification. Unsupervised transfer learning is performed with a thematic clustering of 40 M unlabeled tweets via <a href=https://en.wikipedia.org/wiki/Linear_predictive_coding>LDA</a>. Based on this <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>, pre-training is performed by predicting the main topic of a tweet. Results indicate that unsupervised transfer from large datasets performs slightly better than <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised training</a> on small &#8216;near target category&#8217; datasets. In the SemEval Task, our <a href=https://en.wikipedia.org/wiki/System>system</a> ranks 14 out of 103 participants.</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-2014.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-2014 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-2014 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-2014/>A Multilingual Information Extraction Pipeline for Investigative Journalism</a></strong><br><a href=/people/g/gregor-wiedemann/>Gregor Wiedemann</a>
|
<a href=/people/s/seid-muhie-yimam/>Seid Muhie Yimam</a>
|
<a href=/people/c/chris-biemann/>Chris Biemann</a><br><a href=/volumes/D18-2/ class=text-muted>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-2014><div class="card-body p-3 small">We introduce an advanced information extraction pipeline to automatically process very large collections of <a href=https://en.wikipedia.org/wiki/Unstructured_data>unstructured textual data</a> for the purpose of <a href=https://en.wikipedia.org/wiki/Investigative_journalism>investigative journalism</a>. The <a href=https://en.wikipedia.org/wiki/Pipeline_(computing)>pipeline</a> serves as a new input processor for the upcoming major release of our New / s / leak 2.0 software, which we develop in cooperation with a large German news organization. The use case is that journalists receive a large collection of files up to several Gigabytes containing unknown contents. Collections may originate either from official disclosures of documents, e.g. Freedom of Information Act requests, or unofficial data leaks.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Gregor+Wiedemann" title="Search for 'Gregor Wiedemann' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/c/chris-biemann/ class=align-middle>Chris Biemann</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/people/s/seid-muhie-yimam/ class=align-middle>Seid Muhie Yimam</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/m/marlo-haering/ class=align-middle>Marlo Haering</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jakob-smedegaard-andersen/ class=align-middle>Jakob Smedegaard Andersen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/w/wiebke-loosen/ class=align-middle>Wiebke Loosen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/b/benjamin-milde/ class=align-middle>Benjamin Milde</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/tim-pietz/ class=align-middle>Tim Pietz</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/christian-stocker/ class=align-middle>Christian Stöcker</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/o/olaf-zukunft/ class=align-middle>Olaf Zukunft</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/w/walid-maalej/ class=align-middle>Walid Maalej</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/e/eugen-ruppert/ class=align-middle>Eugen Ruppert</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/semeval/ class=align-middle>SemEval</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/eacl/ class=align-middle>EACL</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/emnlp/ class=align-middle>EMNLP</a><span class="badge badge-secondary align-middle ml-2">1</span></li></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>