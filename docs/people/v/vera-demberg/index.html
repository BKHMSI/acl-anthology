<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Vera Demberg - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Vera</span> <span class=font-weight-bold>Demberg</span></h2><hr><div class=row><div class=col-lg-9><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.64.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--64 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.64 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.64/>Neural Data-to-Text Generation with LM-based Text Augmentation<span class=acl-fixed-case>LM</span>-based Text Augmentation</a></strong><br><a href=/people/e/ernie-chang/>Ernie Chang</a>
|
<a href=/people/x/xiaoyu-shen/>Xiaoyu Shen</a>
|
<a href=/people/d/dawei-zhu/>Dawei Zhu</a>
|
<a href=/people/v/vera-demberg/>Vera Demberg</a>
|
<a href=/people/h/hui-su/>Hui Su</a><br><a href=/volumes/2021.eacl-main/ class=text-muted>Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--64><div class="card-body p-3 small">For many new application domains for data-to-text generation, the main obstacle in training neural models consists of a lack of training data. While usually large numbers of instances are available on the data side, often only very few text samples are available. To address this problem, we here propose a novel few-shot approach for this setting. Our approach automatically augments the <a href=https://en.wikipedia.org/wiki/Data>data</a> available for training by (i) generating new text samples based on replacing specific values by alternative ones from the same category, (ii) generating new text samples based on GPT-2, and (iii) proposing an automatic method for pairing the new text samples with <a href=https://en.wikipedia.org/wiki/Data>data samples</a>. As the text augmentation can introduce noise to the training data, we use cycle consistency as an objective, in order to make sure that a given data sample can be correctly reconstructed after having been formulated as text (and that text samples can be reconstructed from data). On both the E2E and WebNLG benchmarks, we show that this weakly supervised training paradigm is able to outperform fully supervised sequence-to-sequence models with less than 10 % of the training set. By utilizing all annotated data, our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> can boost the performance of a standard sequence-to-sequence model by over 5 BLEU points, establishing a new state-of-the-art on both datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.lchange-1.1.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--lchange-1--1 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.lchange-1.1 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.lchange-1.1" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.lchange-1.1/>Time-Aware Ancient Chinese Text Translation and Inference<span class=acl-fixed-case>A</span>ncient <span class=acl-fixed-case>C</span>hinese Text Translation and Inference</a></strong><br><a href=/people/e/ernie-chang/>Ernie Chang</a>
|
<a href=/people/y/yow-ting-shiue/>Yow-Ting Shiue</a>
|
<a href=/people/h/hui-syuan-yeh/>Hui-Syuan Yeh</a>
|
<a href=/people/v/vera-demberg/>Vera Demberg</a><br><a href=/volumes/2021.lchange-1/ class=text-muted>Proceedings of the 2nd International Workshop on Computational Approaches to Historical Language Change 2021</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--lchange-1--1><div class="card-body p-3 small">In this paper, we aim to address the challenges surrounding the translation of ancient Chinese text : (1) The linguistic gap due to the difference in eras results in translations that are poor in quality, and (2) most translations are missing the contextual information that is often very crucial to understanding the text. To this end, we improve upon past translation techniques by proposing the following : We reframe the task as a multi-label prediction task where the model predicts both the translation and its particular era. We observe that this helps to bridge the linguistic gap as <a href=https://en.wikipedia.org/wiki/Context_(language_use)>chronological context</a> is also used as auxiliary information. We validate our framework on a <a href=https://en.wikipedia.org/wiki/Parallel_text>parallel corpus</a> annotated with chronology information and show experimentally its efficacy in producing quality translation outputs. We release both the code and the data for future research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.codi-main.9.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--codi-main--9 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.codi-main.9 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.codi-main.9/>Comparison of methods for explicit discourse connective identification across various domains</a></strong><br><a href=/people/m/merel-scholman/>Merel Scholman</a>
|
<a href=/people/t/tianai-dong/>Tianai Dong</a>
|
<a href=/people/f/frances-yung/>Frances Yung</a>
|
<a href=/people/v/vera-demberg/>Vera Demberg</a><br><a href=/volumes/2021.codi-main/ class=text-muted>Proceedings of the 2nd Workshop on Computational Approaches to Discourse</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--codi-main--9><div class="card-body p-3 small">Existing parse methods use varying approaches to identify explicit discourse connectives, but their performance has not been consistently evaluated in comparison to each other, nor have they been evaluated consistently on text other than newspaper articles. We here assess the performance on explicit connective identification of three parse methods (PDTB e2e, Lin et al., 2014 ; the winner of CONLL2015, Wang et al., 2015 ; and DisSent, Nie et al., 2019), along with a simple heuristic. We also examine how well these systems generalize to different datasets, namely written newspaper text (PDTB), written scientific text (BioDRB), prepared spoken text (TED-MDB) and spontaneous spoken text (Disco-SPICE). The results show that the e2e parser outperforms the other <a href=https://en.wikipedia.org/wiki/Parsing>parse methods</a> in all <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>. However, performance drops significantly from the <a href=https://en.wikipedia.org/wiki/PDTB>PDTB</a> to all other <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>. We provide a more fine-grained analysis of domain differences and connectives that prove difficult to parse, in order to highlight the areas where gains can be made.</div></div><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-0400.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-0400/>Proceedings of the 13th International Conference on Computational Semantics - Long Papers</a></strong><br><a href=/people/s/simon-dobnik/>Simon Dobnik</a>
|
<a href=/people/s/stergios-chatzikyriakidis/>Stergios Chatzikyriakidis</a>
|
<a href=/people/v/vera-demberg/>Vera Demberg</a><br><a href=/volumes/W19-04/ class=text-muted>Proceedings of the 13th International Conference on Computational Semantics - Long Papers</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-0416.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-0416 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-0416 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-0416/>Learning to Explicitate Connectives with Seq2Seq Network for Implicit Discourse Relation Classification<span class=acl-fixed-case>S</span>eq2<span class=acl-fixed-case>S</span>eq Network for Implicit Discourse Relation Classification</a></strong><br><a href=/people/w/wei-shi/>Wei Shi</a>
|
<a href=/people/v/vera-demberg/>Vera Demberg</a><br><a href=/volumes/W19-04/ class=text-muted>Proceedings of the 13th International Conference on Computational Semantics - Long Papers</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-0416><div class="card-body p-3 small">Implicit discourse relation classification is one of the most difficult steps in discourse parsing. The difficulty stems from the fact that the coherence relation must be inferred based on the content of the discourse relational arguments. Therefore, an effective encoding of the relational arguments is of crucial importance. We here propose a new model for implicit discourse relation classification, which consists of a <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifier</a>, and a sequence-to-sequence model which is trained to generate a representation of the discourse relational arguments by trying to predict the relational arguments including a suitable implicit connective. Training is possible because such implicit connectives have been annotated as part of the PDTB corpus. Along with a memory network, our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> could generate more refined representations for the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. And on the now standard 11-way classification, our method outperforms the previous state of the art systems on the PDTB benchmark on multiple settings including cross validation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-0500.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-0500/>Proceedings of the 13th International Conference on Computational Semantics - Short Papers</a></strong><br><a href=/people/s/simon-dobnik/>Simon Dobnik</a>
|
<a href=/people/s/stergios-chatzikyriakidis/>Stergios Chatzikyriakidis</a>
|
<a href=/people/v/vera-demberg/>Vera Demberg</a><br><a href=/volumes/W19-05/ class=text-muted>Proceedings of the 13th International Conference on Computational Semantics - Short Papers</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-0600.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-0600/>Proceedings of the 13th International Conference on Computational Semantics - Student Papers</a></strong><br><a href=/people/s/simon-dobnik/>Simon Dobnik</a>
|
<a href=/people/s/stergios-chatzikyriakidis/>Stergios Chatzikyriakidis</a>
|
<a href=/people/v/vera-demberg/>Vera Demberg</a>
|
<a href=/people/k/kathrein-abu-kwaik/>Kathrein Abu Kwaik</a>
|
<a href=/people/v/vladislav-maraev/>Vladislav Maraev</a><br><a href=/volumes/W19-06/ class=text-muted>Proceedings of the 13th International Conference on Computational Semantics - Student Papers</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2915.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-2915 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-2915 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-2915/>Verb-Second Effect on Quantifier Scope Interpretation</a></strong><br><a href=/people/a/asad-sayeed/>Asad Sayeed</a>
|
<a href=/people/m/matthias-lindemann/>Matthias Lindemann</a>
|
<a href=/people/v/vera-demberg/>Vera Demberg</a><br><a href=/volumes/W19-29/ class=text-muted>Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-2915><div class="card-body p-3 small">Sentences like Every child climbed a tree have at least two interpretations depending on the precedence order of the <a href=https://en.wikipedia.org/wiki/Universal_quantifier>universal quantifier</a> and the indefinite. Previous experimental work explores the role that different <a href=https://en.wikipedia.org/wiki/Mechanism_(sociology)>mechanisms</a> such as semantic reanalysis and <a href=https://en.wikipedia.org/wiki/World_knowledge>world knowledge</a> may have in enabling each interpretation. This paper discusses a web-based task that uses the verb-second characteristic of German main clauses to estimate the influence of word order variation over <a href=https://en.wikipedia.org/wiki/World_knowledge>world knowledge</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3404.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3404 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3404 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3404/>A Hybrid Model for Globally Coherent Story Generation</a></strong><br><a href=/people/f/fangzhou-zhai/>Fangzhou Zhai</a>
|
<a href=/people/v/vera-demberg/>Vera Demberg</a>
|
<a href=/people/p/pavel-shkadzko/>Pavel Shkadzko</a>
|
<a href=/people/w/wei-shi/>Wei Shi</a>
|
<a href=/people/a/asad-sayeed/>Asad Sayeed</a><br><a href=/volumes/W19-34/ class=text-muted>Proceedings of the Second Workshop on Storytelling</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3404><div class="card-body p-3 small">Automatically generating globally coherent stories is a challenging problem. Neural text generation models have been shown to perform well at generating fluent sentences from data, but they usually fail to keep track of the overall coherence of the story after a couple of sentences. Existing work that incorporates a text planning module succeeded in generating <a href=https://en.wikipedia.org/wiki/Recipe>recipes</a> and <a href=https://en.wikipedia.org/wiki/Dialogue>dialogues</a>, but appears quite data-demanding. We propose a novel story generation approach that generates globally coherent stories from a fairly small corpus. The model exploits a symbolic text planning module to produce text plans, thus reducing the demand of data ; a neural surface realization module then generates fluent text conditioned on the text plan. Human evaluation showed that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms various baselines by a wide margin and generates stories which are fluent as well as globally coherent.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4003.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4003 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4003 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4003/>Crowdsourcing Discourse Relation Annotations by a Two-Step Connective Insertion Task</a></strong><br><a href=/people/f/frances-yung/>Frances Yung</a>
|
<a href=/people/v/vera-demberg/>Vera Demberg</a>
|
<a href=/people/m/merel-scholman/>Merel Scholman</a><br><a href=/volumes/W19-40/ class=text-muted>Proceedings of the 13th Linguistic Annotation Workshop</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4003><div class="card-body p-3 small">The perspective of being able to crowd-source coherence relations bears the promise of acquiring <a href=https://en.wikipedia.org/wiki/Annotation>annotations</a> for new texts quickly, which could then increase the size and variety of discourse-annotated corpora. It would also open the avenue to answering new research questions : Collecting annotations from a larger number of individuals per instance would allow to investigate the distribution of inferred relations, and to study individual differences in coherence relation interpretation. However, annotating <a href=https://en.wikipedia.org/wiki/Coherence_(linguistics)>coherence relations</a> with untrained workers is not trivial. We here propose a novel two-step annotation procedure, which extends an earlier method by Scholman and Demberg (2017a). In our approach, coherence relation labels are inferred from <a href=https://en.wikipedia.org/wiki/Logical_connective>connectives</a> that workers insert into the text. We show that the proposed method leads to replicable coherence annotations, and analyse the agreement between the obtained relation labels and <a href=https://en.wikipedia.org/wiki/Annotation>annotations</a> from PDTB and RSTDT on the same texts.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-8100.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-8100/>Proceedings of the 1st Workshop on Discourse Structure in Neural NLG</a></strong><br><a href=/people/a/anusha-balakrishnan/>Anusha Balakrishnan</a>
|
<a href=/people/v/vera-demberg/>Vera Demberg</a>
|
<a href=/people/c/chandra-khatri/>Chandra Khatri</a>
|
<a href=/people/a/abhinav-rastogi/>Abhinav Rastogi</a>
|
<a href=/people/d/donia-scott/>Donia Scott</a>
|
<a href=/people/m/marilyn-walker/>Marilyn Walker</a>
|
<a href=/people/m/michael-white/>Michael White</a><br><a href=/volumes/W19-81/ class=text-muted>Proceedings of the 1st Workshop on Discourse Structure in Neural NLG</a></span></p><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S18-2002.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S18-2002 data-toggle=collapse aria-expanded=false aria-controls=abstract-S18-2002 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/S18-2002/>Learning distributed event representations with a multi-task approach</a></strong><br><a href=/people/x/xudong-hong/>Xudong Hong</a>
|
<a href=/people/a/asad-sayeed/>Asad Sayeed</a>
|
<a href=/people/v/vera-demberg/>Vera Demberg</a><br><a href=/volumes/S18-2/ class=text-muted>Proceedings of the Seventh Joint Conference on Lexical and Computational Semantics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S18-2002><div class="card-body p-3 small">Human world knowledge contains information about prototypical events and their participants and locations. In this paper, we train the first models using <a href=https://en.wikipedia.org/wiki/Multi-task_learning>multi-task learning</a> that can both predict missing event participants and also perform semantic role classification based on semantic plausibility. Our best-performing <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is an improvement over the previous <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a> on thematic fit modelling tasks. The event embeddings learned by the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> can additionally be used effectively in an event similarity task, also outperforming the <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6002.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6002 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6002 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6002/>Using Universal Dependencies in cross-linguistic complexity research<span class=acl-fixed-case>U</span>niversal <span class=acl-fixed-case>D</span>ependencies in cross-linguistic complexity research</a></strong><br><a href=/people/a/aleksandrs-berdicevskis/>Aleksandrs Berdicevskis</a>
|
<a href=/people/c/cagri-coltekin/>Çağrı Çöltekin</a>
|
<a href=/people/k/katharina-ehret/>Katharina Ehret</a>
|
<a href=/people/k/kilu-von-prince/>Kilu von Prince</a>
|
<a href=/people/d/daniel-ross/>Daniel Ross</a>
|
<a href=/people/b/bill-thompson/>Bill Thompson</a>
|
<a href=/people/c/chunxiao-yan/>Chunxiao Yan</a>
|
<a href=/people/v/vera-demberg/>Vera Demberg</a>
|
<a href=/people/g/gary-lupyan/>Gary Lupyan</a>
|
<a href=/people/t/taraka-rama/>Taraka Rama</a>
|
<a href=/people/c/christian-bentz/>Christian Bentz</a><br><a href=/volumes/W18-60/ class=text-muted>Proceedings of the Second Workshop on Universal Dependencies (UDW 2018)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6002><div class="card-body p-3 small">We evaluate corpus-based measures of linguistic complexity obtained using Universal Dependencies (UD) treebanks. We propose a method of estimating <a href=https://en.wikipedia.org/wiki/Robust_statistics>robustness</a> of the <a href=https://en.wikipedia.org/wiki/Computational_complexity_theory>complexity</a> values obtained using a given <a href=https://en.wikipedia.org/wiki/Measure_(mathematics)>measure</a> and a given <a href=https://en.wikipedia.org/wiki/Treebank>treebank</a>. The results indicate that measures of syntactic complexity might be on average less robust than those of morphological complexity. We also estimate the validity of <a href=https://en.wikipedia.org/wiki/Complexity_measure>complexity measures</a> by comparing the results for very similar languages and checking for unexpected differences. We show that some of those differences that arise can be diminished by using parallel treebanks and, more importantly from the practical point of view, by harmonizing the language-specific solutions in the UD annotation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6546.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6546 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6546 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6546/>Toward Bayesian Synchronous Tree Substitution Grammars for Sentence Planning<span class=acl-fixed-case>B</span>ayesian Synchronous Tree Substitution Grammars for Sentence Planning</a></strong><br><a href=/people/d/david-m-howcroft/>David M. Howcroft</a>
|
<a href=/people/d/dietrich-klakow/>Dietrich Klakow</a>
|
<a href=/people/v/vera-demberg/>Vera Demberg</a><br><a href=/volumes/W18-65/ class=text-muted>Proceedings of the 11th International Conference on Natural Language Generation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6546><div class="card-body p-3 small">Developing conventional natural language generation systems requires extensive attention from human experts in order to craft complex sets of sentence planning rules. We propose a Bayesian nonparametric approach to learn sentence planning rules by inducing synchronous tree substitution grammars for pairs of text plans and morphosyntactically-specified dependency trees. Our system is able to learn rules which can be used to generate novel texts after training on small datasets.</div></div><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1049.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1049 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1049 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-1049/>Using Explicit Discourse Connectives in <a href=https://en.wikipedia.org/wiki/Translation>Translation</a> for Implicit Discourse Relation Classification</a></strong><br><a href=/people/w/wei-shi/>Wei Shi</a>
|
<a href=/people/f/frances-yung/>Frances Yung</a>
|
<a href=/people/r/raphael-rubino/>Raphael Rubino</a>
|
<a href=/people/v/vera-demberg/>Vera Demberg</a><br><a href=/volumes/I17-1/ class=text-muted>Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1049><div class="card-body p-3 small">Implicit discourse relation recognition is an extremely challenging <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> due to the lack of indicative connectives. Various neural network architectures have been proposed for this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> recently, but most of them suffer from the shortage of <a href=https://en.wikipedia.org/wiki/Labeled_data>labeled data</a>. In this paper, we address this problem by procuring additional training data from parallel corpora : When humans translate a text, they sometimes add connectives (a process known as explicitation). We automatically back-translate it into an English connective and use <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> to infer a label with high confidence. We show that a training set several times larger than the original training set can be generated this way. With the extra labeled instances, we show that even a simple bidirectional Long Short-Term Memory Network can outperform the current <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a>.<i>explicitation</i>). We automatically back-translate it into an English connective and use it to infer a label with high confidence. We show that a training set several times larger than the original training set can be generated this way. With the extra labeled instances, we show that even a simple bidirectional Long Short-Term Memory Network can outperform the current state-of-the-art.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-0803.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-0803 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-0803 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-0803/>Crowdsourcing discourse interpretations : On the influence of context and the reliability of a connective insertion task</a></strong><br><a href=/people/m/merel-scholman/>Merel Scholman</a>
|
<a href=/people/v/vera-demberg/>Vera Demberg</a><br><a href=/volumes/W17-08/ class=text-muted>Proceedings of the 11th Linguistic Annotation Workshop</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-0803><div class="card-body p-3 small">Traditional discourse annotation tasks are considered costly and time-consuming, and the <a href=https://en.wikipedia.org/wiki/Reliability_(statistics)>reliability</a> and <a href=https://en.wikipedia.org/wiki/Validity_(statistics)>validity</a> of these tasks is in question. In this paper, we investigate whether <a href=https://en.wikipedia.org/wiki/Crowdsourcing>crowdsourcing</a> can be used to obtain reliable discourse relation annotations. We also examine the influence of <a href=https://en.wikipedia.org/wiki/Context_(language_use)>context</a> on the <a href=https://en.wikipedia.org/wiki/Reliability_(statistics)>reliability</a> of the <a href=https://en.wikipedia.org/wiki/Data>data</a>. The results of a crowdsourced connective insertion task showed that the method can be used to obtain reliable annotations : The majority of the inserted connectives converged with the original label. Further, the <a href=https://en.wikipedia.org/wiki/Methodology>method</a> is sensitive to the fact that multiple senses can often be inferred for a single relation. Regarding the presence of <a href=https://en.wikipedia.org/wiki/Context_(language_use)>context</a>, the results show no significant difference in distributions of insertions between conditions overall. However, a by-item comparison revealed several characteristics of segments that determine whether the presence of context makes a difference in annotations. The findings discussed in this paper can be taken as evidence that <a href=https://en.wikipedia.org/wiki/Crowdsourcing>crowdsourcing</a> can be used as a valuable method to obtain insights into the sense(s) of relations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-3522.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-3522 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-3522 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-3522/>G-TUNA : a corpus of referring expressions in <a href=https://en.wikipedia.org/wiki/German_language>German</a>, including duration information<span class=acl-fixed-case>G</span>-<span class=acl-fixed-case>TUNA</span>: a corpus of referring expressions in <span class=acl-fixed-case>G</span>erman, including duration information</a></strong><br><a href=/people/d/david-m-howcroft/>David Howcroft</a>
|
<a href=/people/j/jorrig-vogels/>Jorrig Vogels</a>
|
<a href=/people/v/vera-demberg/>Vera Demberg</a><br><a href=/volumes/W17-35/ class=text-muted>Proceedings of the 10th International Conference on Natural Language Generation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-3522><div class="card-body p-3 small">Corpora of referring expressions elicited from human participants in a controlled environment are an important resource for research on automatic referring expression generation. We here present G-TUNA, a new corpus of referring expressions for <a href=https://en.wikipedia.org/wiki/German_language>German</a>. Using the furniture stimuli set developed for the TUNA and D-TUNA corpora, our corpus extends on these <a href=https://en.wikipedia.org/wiki/Corpus_linguistics>corpora</a> by providing data collected in a simulated driving dual-task setting, and additionally provides exact duration annotations for the spoken referring expressions. This <a href=https://en.wikipedia.org/wiki/Speech_corpus>corpus</a> will hence allow researchers to analyze the interaction between referring expression length and speech rate, under conditions where the listener is under high vs. low cognitive load.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/Q17-1003.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-Q17-1003 data-toggle=collapse aria-expanded=false aria-controls=abstract-Q17-1003 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234958123 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/Q17-1003/>Modeling Semantic Expectation : Using Script Knowledge for Referent Prediction</a></strong><br><a href=/people/a/ashutosh-modi/>Ashutosh Modi</a>
|
<a href=/people/i/ivan-titov/>Ivan Titov</a>
|
<a href=/people/v/vera-demberg/>Vera Demberg</a>
|
<a href=/people/a/asad-sayeed/>Asad Sayeed</a>
|
<a href=/people/m/manfred-pinkal/>Manfred Pinkal</a><br><a href=/volumes/Q17-1/ class=text-muted>Transactions of the Association for Computational Linguistics, Volume 5</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-Q17-1003><div class="card-body p-3 small">Recent research in <a href=https://en.wikipedia.org/wiki/Psycholinguistics>psycholinguistics</a> has provided increasing evidence that humans predict upcoming content. Prediction also affects <a href=https://en.wikipedia.org/wiki/Perception>perception</a> and might be a key to robustness in <a href=https://en.wikipedia.org/wiki/Language_processing_in_the_brain>human language processing</a>. In this paper, we investigate the factors that affect human prediction by building a <a href=https://en.wikipedia.org/wiki/Computational_model>computational model</a> that can predict upcoming discourse referents based on linguistic knowledge alone vs. linguistic knowledge jointly with common-sense knowledge in the form of scripts. We find that script knowledge significantly improves model estimates of human predictions. In a second study, we test the highly controversial hypothesis that <a href=https://en.wikipedia.org/wiki/Predictability>predictability</a> influences referring expression type but do not find evidence for such an effect.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1027.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1027 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1027 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1027/>A Systematic Study of Neural Discourse Models for Implicit Discourse Relation</a></strong><br><a href=/people/a/attapol-rutherford/>Attapol Rutherford</a>
|
<a href=/people/v/vera-demberg/>Vera Demberg</a>
|
<a href=/people/n/nianwen-xue/>Nianwen Xue</a><br><a href=/volumes/E17-1/ class=text-muted>Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1027><div class="card-body p-3 small">Inferring implicit discourse relations in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language text</a> is the most difficult subtask in discourse parsing. Many neural network models have been proposed to tackle this problem. However, the comparison for this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> is not unified, so we could hardly draw clear conclusions about the effectiveness of various <a href=https://en.wikipedia.org/wiki/Computer_architecture>architectures</a>. Here, we propose neural network models that are based on feedforward and long-short term memory architecture and systematically study the effects of varying structures. To our surprise, the best-configured feedforward architecture outperforms LSTM-based model in most cases despite thorough tuning. Further, we compare our best feedforward system with competitive convolutional and recurrent networks and find that feedforward can actually be more effective. For the first time for this task, we compile and publish outputs from previous neural and non-neural systems to establish the standard for further comparison.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1090.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1090 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1090 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1090/>Psycholinguistic Models of Sentence Processing Improve Sentence Readability Ranking</a></strong><br><a href=/people/d/david-m-howcroft/>David M. Howcroft</a>
|
<a href=/people/v/vera-demberg/>Vera Demberg</a><br><a href=/volumes/E17-1/ class=text-muted>Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1090><div class="card-body p-3 small">While previous research on <a href=https://en.wikipedia.org/wiki/Readability>readability</a> has typically focused on document-level measures, recent work in areas such as <a href=https://en.wikipedia.org/wiki/Natural-language_generation>natural language generation</a> has pointed out the need of sentence-level readability measures. Much of <a href=https://en.wikipedia.org/wiki/Psycholinguistics>psycholinguistics</a> has focused for many years on processing measures that provide difficulty estimates on a word-by-word basis. However, these psycholinguistic measures have not yet been tested on sentence readability ranking tasks. In this paper, we use four psycholinguistic measures : idea density, surprisal, integration cost, and embedding depth to test whether these features are predictive of readability levels. We find that psycholinguistic features significantly improve performance by up to 3 percentage points over a standard document-level readability metric baseline.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2024.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2024 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2024 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2024/>On the Need of <a href=https://en.wikipedia.org/wiki/Cross-validation_(statistics)>Cross Validation</a> for Discourse Relation Classification</a></strong><br><a href=/people/w/wei-shi/>Wei Shi</a>
|
<a href=/people/v/vera-demberg/>Vera Demberg</a><br><a href=/volumes/E17-2/ class=text-muted>Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2024><div class="card-body p-3 small">The task of implicit discourse relation classification has received increased attention in recent years, including two CoNNL shared tasks on the topic. Existing machine learning models for the task train on sections 2-21 of the PDTB and test on section 23, which includes a total of 761 implicit discourse relations. In this paper, we&#8217;d like to make a methodological point, arguing that the standard test set is too small to draw conclusions about whether the inclusion of certain features constitute a genuine improvement, or whether one got lucky with some properties of the <a href=https://en.wikipedia.org/wiki/Test_set>test set</a>, and argue for the adoption of <a href=https://en.wikipedia.org/wiki/Cross-validation_(statistics)>cross validation</a> for the discourse relation classification task by the community.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Vera+Demberg" title="Search for 'Vera Demberg' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/w/wei-shi/ class=align-middle>Wei Shi</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/people/a/asad-sayeed/ class=align-middle>Asad Sayeed</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/people/f/frances-yung/ class=align-middle>Frances Yung</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/m/merel-scholman/ class=align-middle>Merel Scholman</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/d/david-m-howcroft/ class=align-middle>David M. Howcroft</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/s/simon-dobnik/ class=align-middle>Simon Dobnik</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/s/stergios-chatzikyriakidis/ class=align-middle>Stergios Chatzikyriakidis</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/e/ernie-chang/ class=align-middle>Ernie Chang</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/r/raphael-rubino/ class=align-middle>Raphael Rubino</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/x/xiaoyu-shen/ class=align-middle>Xiaoyu Shen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/dawei-zhu/ class=align-middle>Dawei Zhu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/hui-su/ class=align-middle>Hui Su</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jorrig-vogels/ class=align-middle>Jorrig Vogels</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/ashutosh-modi/ class=align-middle>Ashutosh Modi</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/i/ivan-titov/ class=align-middle>Ivan Titov</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/manfred-pinkal/ class=align-middle>Manfred Pinkal</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yow-ting-shiue/ class=align-middle>Yow-Ting Shiue</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/hui-syuan-yeh/ class=align-middle>Hui-Syuan Yeh</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/x/xudong-hong/ class=align-middle>Xudong Hong</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/aleksandrs-berdicevskis/ class=align-middle>Aleksandrs Berdicevskis</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/cagri-coltekin/ class=align-middle>Çağrı Çöltekin</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/katharina-ehret/ class=align-middle>Katharina Ehret</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/kilu-von-prince/ class=align-middle>Kilu von Prince</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/daniel-ross/ class=align-middle>Daniel Ross</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/b/bill-thompson/ class=align-middle>Bill Thompson</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/chunxiao-yan/ class=align-middle>Chunxiao Yan</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/g/gary-lupyan/ class=align-middle>Gary Lupyan</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/taraka-rama/ class=align-middle>Taraka Rama</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/christian-bentz/ class=align-middle>Christian Bentz</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/dietrich-klakow/ class=align-middle>Dietrich Klakow</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/kathrein-abu-kwaik/ class=align-middle>Kathrein Abu Kwaik</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/v/vladislav-maraev/ class=align-middle>Vladislav Maraev</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/matthias-lindemann/ class=align-middle>Matthias Lindemann</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/f/fangzhou-zhai/ class=align-middle>Fangzhou Zhai</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/p/pavel-shkadzko/ class=align-middle>Pavel Shkadzko</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/anusha-balakrishnan/ class=align-middle>Anusha Balakrishnan</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/chandra-khatri/ class=align-middle>Chandra Khatri</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/abhinav-rastogi/ class=align-middle>Abhinav Rastogi</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/donia-scott/ class=align-middle>Donia Scott</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/marilyn-walker/ class=align-middle>Marilyn Walker</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/michael-white/ class=align-middle>Michael White</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/attapol-rutherford/ class=align-middle>Attapol Rutherford</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/n/nianwen-xue/ class=align-middle>Nianwen Xue</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/tianai-dong/ class=align-middle>Tianai Dong</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/ws/ class=align-middle>WS</a><span class="badge badge-secondary align-middle ml-2">12</span></li><li class=list-group-item><a href=/venues/eacl/ class=align-middle>EACL</a><span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/venues/ijcnlp/ class=align-middle>IJCNLP</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/tacl/ class=align-middle>TACL</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/lchange/ class=align-middle>LChange</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-venues aria-expanded=false aria-controls=more-venues>show all...</li><div class="collapse border-top" id=more-venues><li class=list-group-item><a href=/venues/semeval/ class=align-middle>SemEval</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/codi/ class=align-middle>CODI</a><span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>