<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Viktor Hangya - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Viktor</span> <span class=font-weight-bold>Hangya</span></h2><hr><div class=row><div class=col-lg-9><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.mrl-1.4.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--mrl-1--4 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.mrl-1.4 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.mrl-1.4/>Do not neglect related languages : The case of low-resource Occitan cross-lingual word embeddings<span class=acl-fixed-case>O</span>ccitan cross-lingual word embeddings</a></strong><br><a href=/people/l/lisa-woller/>Lisa Woller</a>
|
<a href=/people/v/viktor-hangya/>Viktor Hangya</a>
|
<a href=/people/a/alexander-fraser/>Alexander Fraser</a><br><a href=/volumes/2021.mrl-1/ class=text-muted>Proceedings of the 1st Workshop on Multilingual Representation Learning</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--mrl-1--4><div class="card-body p-3 small">Cross-lingual word embeddings (CLWEs) have proven indispensable for various <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing tasks</a>, e.g., bilingual lexicon induction (BLI). However, the lack of data often impairs the quality of <a href=https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning>representations</a>. Various approaches requiring only weak cross-lingual supervision were proposed, but current methods still fail to learn good CLWEs for languages with only a small monolingual corpus. We therefore claim that it is necessary to explore further <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> to improve CLWEs in low-resource setups. In this paper we propose to incorporate data of related high-resource languages. In contrast to previous approaches which leverage independently pre-trained embeddings of languages, we (i) train CLWEs for the low-resource and a related language jointly and (ii) map them to the target language to build the final multilingual space. In our experiments we focus on <a href=https://en.wikipedia.org/wiki/Occitan_language>Occitan</a>, a low-resource Romance language which is often neglected due to lack of resources. We leverage data from <a href=https://en.wikipedia.org/wiki/French_language>French</a>, <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a> and <a href=https://en.wikipedia.org/wiki/Catalan_language>Catalan</a> for training and evaluate on the Occitan-English BLI task. By incorporating supporting languages our <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>method</a> outperforms previous approaches by a large margin. Furthermore, our analysis shows that the degree of relatedness between an incorporated language and the low-resource language is critically important.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wmt-1.66.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wmt-1--66 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wmt-1.66 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wmt-1.66/>Improving <a href=https://en.wikipedia.org/wiki/Machine_translation>Machine Translation</a> of Rare and Unseen Word Senses</a></strong><br><a href=/people/v/viktor-hangya/>Viktor Hangya</a>
|
<a href=/people/q/qianchu-liu/>Qianchu Liu</a>
|
<a href=/people/d/dario-stojanovski/>Dario Stojanovski</a>
|
<a href=/people/a/alexander-fraser/>Alexander Fraser</a>
|
<a href=/people/a/anna-korhonen/>Anna Korhonen</a><br><a href=/volumes/2021.wmt-1/ class=text-muted>Proceedings of the Sixth Conference on Machine Translation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wmt-1--66><div class="card-body p-3 small">The performance of NMT systems has improved drastically in the past few years but the translation of multi-sense words still poses a challenge. Since word senses are not represented uniformly in the <a href=https://en.wikipedia.org/wiki/Parallel_text>parallel corpora</a> used for training, there is an excessive use of the most frequent sense in MT output. In this work, we propose CmBT (Contextually-mined Back-Translation), an approach for improving multi-sense word translation leveraging pre-trained cross-lingual contextual word representations (CCWRs). Because of their contextual sensitivity and their large <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>pre-training data</a>, CCWRs can easily capture <a href=https://en.wikipedia.org/wiki/Word_sense>word senses</a> that are missing or very rare in <a href=https://en.wikipedia.org/wiki/Parallel_text>parallel corpora</a> used to train MT. Specifically, CmBT applies bilingual lexicon induction on CCWRs to mine sense-specific target sentences from a monolingual dataset, and then back-translates these sentences to generate a pseudo parallel corpus as additional training data for an MT system. We test the translation quality of ambiguous words on the MuCoW test suite, which was built to test the <a href=https://en.wikipedia.org/wiki/Word-sense_disambiguation>word sense disambiguation</a> effectiveness of MT systems. We show that our <a href=https://en.wikipedia.org/wiki/System>system</a> improves on the translation of difficult unseen and low frequency word senses.</div></div><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.313.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--313 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.313 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.313/>Exploring Bilingual Word Embeddings for <a href=https://en.wikipedia.org/wiki/Hiligaynon_language>Hiligaynon</a>, a Low-Resource Language<span class=acl-fixed-case>H</span>iligaynon, a Low-Resource Language</a></strong><br><a href=/people/l/leah-michel/>Leah Michel</a>
|
<a href=/people/v/viktor-hangya/>Viktor Hangya</a>
|
<a href=/people/a/alexander-fraser/>Alexander Fraser</a><br><a href=/volumes/2020.lrec-1/ class=text-muted>Proceedings of the 12th Language Resources and Evaluation Conference</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--313><div class="card-body p-3 small">This paper investigates the use of bilingual word embeddings for mining Hiligaynon translations of English words. There is very little research on <a href=https://en.wikipedia.org/wiki/Hiligaynon_language>Hiligaynon</a>, an extremely low-resource language of Malayo-Polynesian origin with over 9 million speakers in the Philippines (we found just one paper). We use a publicly available Hiligaynon corpus with only 300 K words, and match it with a comparable corpus in English. As there are no bilingual resources available, we manually develop a English-Hiligaynon lexicon and use this to train bilingual word embeddings. But we fail to mine accurate translations due to the small amount of data. To find out if the same holds true for a related language pair, we simulate the same low-resource setup on <a href=https://en.wikipedia.org/wiki/English_language>English</a> to <a href=https://en.wikipedia.org/wiki/German_language>German</a> and arrive at similar results. We then vary the size of the comparable English and German corpora to determine the minimum corpus size necessary to achieve competitive results. Further, we investigate the role of the seed lexicon. We show that with the same corpus size but with a smaller seed lexicon, performance can surpass results of previous studies. We release the lexicon of 1,200 English-Hiligaynon word pairs we created to encourage further investigation.</div></div><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5344.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5344 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5344 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5344/>The LMU Munich Unsupervised Machine Translation System for WMT19<span class=acl-fixed-case>LMU</span> <span class=acl-fixed-case>M</span>unich Unsupervised Machine Translation System for <span class=acl-fixed-case>WMT</span>19</a></strong><br><a href=/people/d/dario-stojanovski/>Dario Stojanovski</a>
|
<a href=/people/v/viktor-hangya/>Viktor Hangya</a>
|
<a href=/people/m/matthias-huck/>Matthias Huck</a>
|
<a href=/people/a/alexander-fraser/>Alexander Fraser</a><br><a href=/volumes/W19-53/ class=text-muted>Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5344><div class="card-body p-3 small">We describe LMU Munich&#8217;s machine translation system for GermanCzech translation which was used to participate in the WMT19 shared task on unsupervised news translation. We train our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> using <a href=https://en.wikipedia.org/wiki/Monolingualism>monolingual data</a> only from both languages. The final model is an <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised neural model</a> using established techniques for unsupervised translation such as denoising autoencoding and online back-translation. We bootstrap the model with masked language model pretraining and enhance it with back-translations from an unsupervised phrase-based system which is itself bootstrapped using unsupervised bilingual word embeddings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1581.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1581 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1581 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/385434714 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1581/>Better OOV Translation with Bilingual Terminology Mining<span class=acl-fixed-case>OOV</span> Translation with Bilingual Terminology Mining</a></strong><br><a href=/people/m/matthias-huck/>Matthias Huck</a>
|
<a href=/people/v/viktor-hangya/>Viktor Hangya</a>
|
<a href=/people/a/alexander-fraser/>Alexander Fraser</a><br><a href=/volumes/P19-1/ class=text-muted>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1581><div class="card-body p-3 small">Unseen words, also called out-of-vocabulary words (OOVs), are difficult for <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>. In <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a>, byte-pair encoding can be used to represent OOVs, but they are still often incorrectly translated. We improve the translation of OOVs in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NMT</a> using easy-to-obtain monolingual data. We look for OOVs in the text to be translated and translate them using simple-to-construct bilingual word embeddings (BWEs). In our MT experiments we take the 5-best candidates, which is motivated by intrinsic mining experiments. Using all five of the proposed target language words as queries we mine target-language sentences. We then back-translate, forcing the <a href=https://en.wikipedia.org/wiki/Back-translation>back-translation</a> of each of the five proposed target-language OOV-translation-candidates to be the original source-language OOV. We show that by using this synthetic data to fine-tune our <a href=https://en.wikipedia.org/wiki/System>system</a> the translation of OOVs can be dramatically improved. In our experiments we use a system trained on <a href=https://en.wikipedia.org/wiki/Europarl>Europarl</a> and mine sentences containing <a href=https://en.wikipedia.org/wiki/Medical_terminology>medical terms</a> from <a href=https://en.wikipedia.org/wiki/Monolingualism>monolingual data</a>.</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6428.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6428 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6428 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6428/>The LMU Munich Unsupervised Machine Translation Systems<span class=acl-fixed-case>LMU</span> <span class=acl-fixed-case>M</span>unich Unsupervised Machine Translation Systems</a></strong><br><a href=/people/d/dario-stojanovski/>Dario Stojanovski</a>
|
<a href=/people/v/viktor-hangya/>Viktor Hangya</a>
|
<a href=/people/m/matthias-huck/>Matthias Huck</a>
|
<a href=/people/a/alexander-fraser/>Alexander Fraser</a><br><a href=/volumes/W18-64/ class=text-muted>Proceedings of the Third Conference on Machine Translation: Shared Task Papers</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6428><div class="card-body p-3 small">We describe LMU Munich&#8217;s unsupervised machine translation systems for EnglishGerman translation. These systems were used to participate in the WMT18 news translation shared task and more specifically, for the unsupervised learning sub-track. The systems are trained on English and German monolingual data only and exploit and combine previously proposed techniques such as using word-by-word translated data based on bilingual word embeddings, denoising and on-the-fly backtranslation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6446.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6446 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6446 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6446/>LMU Munich’s Neural Machine Translation Systems at WMT 2018<span class=acl-fixed-case>LMU</span> <span class=acl-fixed-case>M</span>unich’s Neural Machine Translation Systems at <span class=acl-fixed-case>WMT</span> 2018</a></strong><br><a href=/people/m/matthias-huck/>Matthias Huck</a>
|
<a href=/people/d/dario-stojanovski/>Dario Stojanovski</a>
|
<a href=/people/v/viktor-hangya/>Viktor Hangya</a>
|
<a href=/people/a/alexander-fraser/>Alexander Fraser</a><br><a href=/volumes/W18-64/ class=text-muted>Proceedings of the Third Conference on Machine Translation: Shared Task Papers</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6446><div class="card-body p-3 small">We present the LMU Munich machine translation systems for the EnglishGerman language pair. We have built neural machine translation systems for both translation directions (EnglishGerman and GermanEnglish) and for two different domains (the biomedical domain and the news domain). The systems were used for our participation in the WMT18 biomedical translation task and in the shared task on machine translation of news. The main focus of our recent system development efforts has been on achieving improvements in the biomedical domain over last year&#8217;s strong biomedical translation engine for EnglishGerman (Huck et al., 2017a). Considerable progress has been made in the latter <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>, which we report on in this paper.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6477.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6477 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6477 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6477/>An Unsupervised System for Parallel Corpus Filtering</a></strong><br><a href=/people/v/viktor-hangya/>Viktor Hangya</a>
|
<a href=/people/a/alexander-fraser/>Alexander Fraser</a><br><a href=/volumes/W18-64/ class=text-muted>Proceedings of the Third Conference on Machine Translation: Shared Task Papers</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6477><div class="card-body p-3 small">In this paper we describe LMU Munich&#8217;s submission for the WMT 2018 Parallel Corpus Filtering shared task which addresses the problem of cleaning noisy parallel corpora. The task of mining and cleaning parallel sentences is important for improving the quality of <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation systems</a>, especially for low-resource languages. We tackle this problem in a fully unsupervised fashion relying on bilingual word embeddings created without any bilingual signal. After pre-filtering noisy data we rank sentence pairs by calculating bilingual sentence-level similarities and then remove redundant data by employing monolingual similarity as well. Our <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised system</a> achieved good performance during the official evaluation of the shared task, scoring only a few BLEU points behind the best systems, while not requiring any parallel training data.<i>WMT 2018 Parallel Corpus Filtering</i> shared task which addresses the problem of cleaning noisy parallel corpora. The task of mining and cleaning parallel sentences is important for improving the quality of machine translation systems, especially for low-resource languages. We tackle this problem in a fully unsupervised fashion relying on bilingual word embeddings created without any bilingual signal. After pre-filtering noisy data we rank sentence pairs by calculating bilingual sentence-level similarities and then remove redundant data by employing monolingual similarity as well. Our unsupervised system achieved good performance during the official evaluation of the shared task, scoring only a few BLEU points behind the best systems, while not requiring any parallel training data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2030.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2030 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2030 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N18-2030" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N18-2030/>Evaluating bilingual word embeddings on the long tail</a></strong><br><a href=/people/f/fabienne-braune/>Fabienne Braune</a>
|
<a href=/people/v/viktor-hangya/>Viktor Hangya</a>
|
<a href=/people/t/tobias-eder/>Tobias Eder</a>
|
<a href=/people/a/alexander-fraser/>Alexander Fraser</a><br><a href=/volumes/N18-2/ class=text-muted>Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2030><div class="card-body p-3 small">Bilingual word embeddings are useful for bilingual lexicon induction, the task of mining translations of given words. Many studies have shown that bilingual word embeddings perform well for bilingual lexicon induction but they focused on frequent words in general domains. For many applications, bilingual lexicon induction of rare and domain-specific words is of critical importance. Therefore, we design a new task to evaluate bilingual word embeddings on rare words in different domains. We show that state-of-the-art approaches fail on this task and present simple new techniques to improve bilingual word embeddings for mining rare words. We release new gold standard datasets and code to stimulate research on this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2018.iwslt-1.2.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2018--iwslt-1--2 data-toggle=collapse aria-expanded=false aria-controls=abstract-2018.iwslt-1.2 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2018.iwslt-1.2/>Unsupervised Parallel Sentence Extraction from Comparable Corpora</a></strong><br><a href=/people/v/viktor-hangya/>Viktor Hangya</a>
|
<a href=/people/f/fabienne-braune/>Fabienne Braune</a>
|
<a href=/people/y/yuliya-kalasouskaya/>Yuliya Kalasouskaya</a>
|
<a href=/people/a/alexander-fraser/>Alexander Fraser</a><br><a href=/volumes/2018.iwslt-1/ class=text-muted>Proceedings of the 15th International Conference on Spoken Language Translation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2018--iwslt-1--2><div class="card-body p-3 small">Mining parallel sentences from comparable corpora is of great interest for many downstream tasks. In the BUCC 2017 shared task, <a href=https://en.wikipedia.org/wiki/System>systems</a> performed well by training on gold standard parallel sentences. However, we often want to mine <a href=https://en.wikipedia.org/wiki/Parallelism_(grammar)>parallel sentences</a> without bilingual supervision. We present a simple approach relying on bilingual word embeddings trained in an unsupervised fashion. We incorporate orthographic similarity in order to handle words with similar surface forms. In addition, we propose a dynamic threshold method to decide if a candidate sentence-pair is parallel which eliminates the need to fine tune a static value for different datasets. Since we do not employ any language specific engineering our approach is highly generic. We show that our approach is effective, on three language-pairs, without the use of any bilingual signal which is important because parallel sentence mining is most useful in low resource scenarios.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Viktor+Hangya" title="Search for 'Viktor Hangya' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/a/alexander-fraser/ class=align-middle>Alexander Fraser</a>
<span class="badge badge-secondary align-middle ml-2">10</span></li><li class=list-group-item><a href=/people/d/dario-stojanovski/ class=align-middle>Dario Stojanovski</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/people/m/matthias-huck/ class=align-middle>Matthias Huck</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/people/f/fabienne-braune/ class=align-middle>Fabienne Braune</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/l/lisa-woller/ class=align-middle>Lisa Woller</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/q/qianchu-liu/ class=align-middle>Qianchu Liu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/anna-korhonen/ class=align-middle>Anna Korhonen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/tobias-eder/ class=align-middle>Tobias Eder</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/leah-michel/ class=align-middle>Leah Michel</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yuliya-kalasouskaya/ class=align-middle>Yuliya Kalasouskaya</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/ws/ class=align-middle>WS</a><span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/venues/mrl/ class=align-middle>MRL</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/wmt/ class=align-middle>WMT</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/naacl/ class=align-middle>NAACL</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/lrec/ class=align-middle>LREC</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-venues aria-expanded=false aria-controls=more-venues>show all...</li><div class="collapse border-top" id=more-venues><li class=list-group-item><a href=/venues/acl/ class=align-middle>ACL</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/iwslt/ class=align-middle>IWSLT</a><span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>