<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Dragomir Radev - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Dragomir</span> <span class=font-weight-bold>Radev</span></h2><hr><div class=row><div class=col-lg-9><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eval4nlp-1.8.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eval4nlp-1--8 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eval4nlp-1.8 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.eval4nlp-1.8" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.eval4nlp-1.8/>Testing Cross-Database Semantic Parsers With Canonical Utterances</a></strong><br><a href=/people/h/heather-lent/>Heather Lent</a>
|
<a href=/people/s/semih-yavuz/>Semih Yavuz</a>
|
<a href=/people/t/tao-yu/>Tao Yu</a>
|
<a href=/people/t/tong-niu/>Tong Niu</a>
|
<a href=/people/y/yingbo-zhou/>Yingbo Zhou</a>
|
<a href=/people/d/dragomir-radev/>Dragomir Radev</a>
|
<a href=/people/x/xi-victoria-lin/>Xi Victoria Lin</a><br><a href=/volumes/2021.eval4nlp-1/ class=text-muted>Proceedings of the 2nd Workshop on Evaluation and Comparison of NLP Systems</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eval4nlp-1--8><div class="card-body p-3 small">The benchmark performance of cross-database semantic parsing has climbed steadily in recent years, catalyzed by the wide adoption of pre-trained language models. Yet existing work have shown that state-of-the-art cross-database semantic parsers struggle to generalize to novel user utterances, databases and query structures. To obtain transparent details on the strengths and limitation of these models, we propose a diagnostic testing approach based on controlled synthesis of canonical natural language and SQL pairs. Inspired by the CheckList, we characterize a set of essential capabilities for cross-database semantic parsing models, and detailed the method for synthesizing the corresponding test data. We evaluated a variety of high performing <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> using the proposed approach, and identified several non-obvious weaknesses across <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> (e.g. unable to correctly select many columns). Our dataset and code are released as a test suite at http://github.com/hclent/BehaviorCheckingSemPar.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.repl4nlp-1.1.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--repl4nlp-1--1 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.repl4nlp-1.1 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.repl4nlp-1.1/>Improving Cross-lingual Text Classification with Zero-shot Instance-Weighting</a></strong><br><a href=/people/i/irene-li/>Irene Li</a>
|
<a href=/people/p/prithviraj-sen/>Prithviraj Sen</a>
|
<a href=/people/h/huaiyu-zhu/>Huaiyu Zhu</a>
|
<a href=/people/y/yunyao-li/>Yunyao Li</a>
|
<a href=/people/d/dragomir-radev/>Dragomir Radev</a><br><a href=/volumes/2021.repl4nlp-1/ class=text-muted>Proceedings of the 6th Workshop on Representation Learning for NLP (RepL4NLP-2021)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--repl4nlp-1--1><div class="card-body p-3 small">Cross-lingual text classification (CLTC) is a challenging task made even harder still due to the lack of labeled data in low-resource languages. In this paper, we propose zero-shot instance-weighting, a general model-agnostic zero-shot learning framework for improving CLTC by leveraging source instance weighting. It adds a module on top of pre-trained language models for similarity computation of instance weights, thus aligning each source instance to the target language. During <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training</a>, the framework utilizes <a href=https://en.wikipedia.org/wiki/Gradient_descent>gradient descent</a> that is weighted by instance weights to update parameters. We evaluate this <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> over seven target languages on three fundamental tasks and show its effectiveness and extensibility, by improving on F1 score up to 4 % in single-source transfer and 8 % in multi-source transfer. To the best of our knowledge, our method is the first to apply instance weighting in zero-shot CLTC. It is simple yet effective and easily extensible into multi-source transfer.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.37.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--37 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.37 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.naacl-main.37" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.37/>DART : Open-Domain Structured Data Record to Text Generation<span class=acl-fixed-case>DART</span>: Open-Domain Structured Data Record to Text Generation</a></strong><br><a href=/people/l/linyong-nan/>Linyong Nan</a>
|
<a href=/people/d/dragomir-radev/>Dragomir Radev</a>
|
<a href=/people/r/rui-zhang/>Rui Zhang</a>
|
<a href=/people/a/amrit-rau/>Amrit Rau</a>
|
<a href=/people/a/abhinand-sivaprasad/>Abhinand Sivaprasad</a>
|
<a href=/people/c/chiachun-hsieh/>Chiachun Hsieh</a>
|
<a href=/people/x/xiangru-tang/>Xiangru Tang</a>
|
<a href=/people/a/aadit-vyas/>Aadit Vyas</a>
|
<a href=/people/n/neha-verma/>Neha Verma</a>
|
<a href=/people/p/pranav-krishna/>Pranav Krishna</a>
|
<a href=/people/y/yangxiaokang-liu/>Yangxiaokang Liu</a>
|
<a href=/people/n/nadia-irwanto/>Nadia Irwanto</a>
|
<a href=/people/j/jessica-pan/>Jessica Pan</a>
|
<a href=/people/f/faiaz-rahman/>Faiaz Rahman</a>
|
<a href=/people/a/ahmad-zaidi/>Ahmad Zaidi</a>
|
<a href=/people/m/mutethia-mutuma/>Mutethia Mutuma</a>
|
<a href=/people/y/yasin-tarabar/>Yasin Tarabar</a>
|
<a href=/people/a/ankit-gupta/>Ankit Gupta</a>
|
<a href=/people/t/tao-yu/>Tao Yu</a>
|
<a href=/people/y/yi-chern-tan/>Yi Chern Tan</a>
|
<a href=/people/x/xi-victoria-lin/>Xi Victoria Lin</a>
|
<a href=/people/c/caiming-xiong/>Caiming Xiong</a>
|
<a href=/people/r/richard-socher/>Richard Socher</a>
|
<a href=/people/n/nazneen-fatema-rajani/>Nazneen Fatema Rajani</a><br><a href=/volumes/2021.naacl-main/ class=text-muted>Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--37><div class="card-body p-3 small">We present DART, an open domain structured DAta Record to Text generation dataset with over 82k instances (DARTs). Data-to-text annotations can be a costly process, especially when dealing with tables which are the major source of structured data and contain nontrivial structures. To this end, we propose a procedure of extracting semantic triples from tables that encodes their structures by exploiting the semantic dependencies among table headers and the table title. Our dataset construction framework effectively merged heterogeneous sources from open domain semantic parsing and spoken dialogue systems by utilizing techniques including tree ontology annotation, question-answer pair to declarative sentence conversion, and predicate unification, all with minimum post-editing. We present systematic evaluation on DART as well as new state-of-the-art results on WebNLG 2017 to show that DART (1) poses new challenges to existing data-to-text datasets and (2) facilitates out-of-domain generalization. Our data and code can be found at https://github.com/Yale-LILY/dart.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.57.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--57 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.57 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.57/>Improving Zero and Few-Shot Abstractive Summarization with Intermediate Fine-tuning and Data Augmentation</a></strong><br><a href=/people/a/alexander-richard-fabbri/>Alexander Fabbri</a>
|
<a href=/people/s/simeng-han/>Simeng Han</a>
|
<a href=/people/h/haoyuan-li/>Haoyuan Li</a>
|
<a href=/people/h/haoran-li/>Haoran Li</a>
|
<a href=/people/m/marjan-ghazvininejad/>Marjan Ghazvininejad</a>
|
<a href=/people/s/shafiq-joty/>Shafiq Joty</a>
|
<a href=/people/d/dragomir-radev/>Dragomir Radev</a>
|
<a href=/people/y/yashar-mehdad/>Yashar Mehdad</a><br><a href=/volumes/2021.naacl-main/ class=text-muted>Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--57><div class="card-body p-3 small">Models pretrained with self-supervised objectives on large text corpora achieve state-of-the-art performance on English text summarization tasks. However, these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> are typically fine-tuned on hundreds of thousands of data points, an infeasible requirement when applying <a href=https://en.wikipedia.org/wiki/Automatic_summarization>summarization</a> to new, niche domains. In this work, we introduce a novel and generalizable method, called WikiTransfer, for fine-tuning pretrained models for <a href=https://en.wikipedia.org/wiki/Automatic_summarization>summarization</a> in an unsupervised, dataset-specific manner. WikiTransfer fine-tunes pretrained models on pseudo-summaries, produced from generic Wikipedia data, which contain characteristics of the target dataset, such as the length and level of abstraction of the desired summaries. WikiTransfer models achieve state-of-the-art, zero-shot abstractive summarization performance on the CNN-DailyMail dataset and demonstrate the effectiveness of our approach on three additional diverse datasets. These models are more robust to noisy data and also achieve better or comparable few-shot performance using 10 and 100 training examples when compared to few-shot transfer from other summarization datasets. To further boost performance, we employ <a href=https://en.wikipedia.org/wiki/Data_augmentation>data augmentation</a> via <a href=https://en.wikipedia.org/wiki/Round-trip_translation>round-trip translation</a> as well as introduce a regularization term for improved few-shot transfer. To understand the role of dataset aspects in transfer performance and the quality of the resulting output summaries, we further study the effect of the components of our unsupervised fine-tuning data and analyze few-shot performance using both automatic and human evaluation.</div></div><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.intexsempar-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.intexsempar-1.0/>Proceedings of the First Workshop on Interactive and Executable Semantic Parsing</a></strong><br><a href=/people/b/ben-bogin/>Ben Bogin</a>
|
<a href=/people/s/srinivasan-iyer/>Srinivasan Iyer</a>
|
<a href=/people/x/xi-victoria-lin/>Victoria Lin</a>
|
<a href=/people/d/dragomir-radev/>Dragomir Radev</a>
|
<a href=/people/a/alane-suhr/>Alane Suhr</a>
|
<a href=/people/p/panupong/>Panupong</a>
|
<a href=/people/c/caiming-xiong/>Caiming Xiong</a>
|
<a href=/people/p/pengcheng-yin/>Pengcheng Yin</a>
|
<a href=/people/t/tao-yu/>Tao Yu</a>
|
<a href=/people/r/rui-zhang/>Rui Zhang</a>
|
<a href=/people/v/victor-zhong/>Victor Zhong</a><br><a href=/volumes/2020.intexsempar-1/ class=text-muted>Proceedings of the First Workshop on Interactive and Executable Semantic Parsing</a></span></p><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1537.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1537 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1537 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1537" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1537/>Editing-Based SQL Query Generation for Cross-Domain Context-Dependent Questions<span class=acl-fixed-case>SQL</span> Query Generation for Cross-Domain Context-Dependent Questions</a></strong><br><a href=/people/r/rui-zhang/>Rui Zhang</a>
|
<a href=/people/t/tao-yu/>Tao Yu</a>
|
<a href=/people/h/heyang-er/>Heyang Er</a>
|
<a href=/people/s/sungrok-shim/>Sungrok Shim</a>
|
<a href=/people/e/eric-xue/>Eric Xue</a>
|
<a href=/people/x/xi-victoria-lin/>Xi Victoria Lin</a>
|
<a href=/people/t/tianze-shi/>Tianze Shi</a>
|
<a href=/people/c/caiming-xiong/>Caiming Xiong</a>
|
<a href=/people/r/richard-socher/>Richard Socher</a>
|
<a href=/people/d/dragomir-radev/>Dragomir Radev</a><br><a href=/volumes/D19-1/ class=text-muted>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1537><div class="card-body p-3 small">We focus on the cross-domain context-dependent text-to-SQL generation task. Based on the observation that adjacent natural language questions are often linguistically dependent and their corresponding SQL queries tend to overlap, we utilize the interaction history by editing the previous predicted query to improve the generation quality. Our editing mechanism views SQL as sequences and reuses generation results at the token level in a simple manner. It is flexible to change individual tokens and robust to <a href=https://en.wikipedia.org/wiki/Propagation_of_uncertainty>error propagation</a>. Furthermore, to deal with complex table structures in different domains, we employ an utterance-table encoder and a table-aware decoder to incorporate the context of the user utterance and the table schema. We evaluate our approach on the SParC dataset and demonstrate the benefit of editing compared with the state-of-the-art baselines which generate SQL from scratch. Our code is available at.<url>https://github.com/ryanzhumich/sparc_atis_pytorch</url>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1102.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1102 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1102 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P19-1102.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/384478403 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1102" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1102/>Multi-News : A Large-Scale Multi-Document Summarization Dataset and Abstractive Hierarchical Model</a></strong><br><a href=/people/a/alexander-richard-fabbri/>Alexander Fabbri</a>
|
<a href=/people/i/irene-li/>Irene Li</a>
|
<a href=/people/t/tianwei-she/>Tianwei She</a>
|
<a href=/people/s/suyi-li/>Suyi Li</a>
|
<a href=/people/d/dragomir-radev/>Dragomir Radev</a><br><a href=/volumes/P19-1/ class=text-muted>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1102><div class="card-body p-3 small">Automatic generation of summaries from multiple <a href=https://en.wikipedia.org/wiki/Article_(publishing)>news articles</a> is a valuable tool as the number of <a href=https://en.wikipedia.org/wiki/Online_newspaper>online publications</a> grows rapidly. Single document summarization (SDS) systems have benefited from advances in neural encoder-decoder model thanks to the availability of large datasets. However, multi-document summarization (MDS) of <a href=https://en.wikipedia.org/wiki/Article_(publishing)>news articles</a> has been limited to <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> of a couple of hundred examples. In this paper, we introduce Multi-News, the first large-scale MDS news dataset. Additionally, we propose an end-to-end model which incorporates a traditional extractive summarization model with a standard SDS model and achieves competitive results on MDS datasets. We benchmark several methods on Multi-News and hope that this work will promote advances in summarization in the multi-document setting.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1306.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1306 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1306 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-1306/>Improving Low-Resource Cross-lingual Document Retrieval by Reranking with Deep Bilingual Representations</a></strong><br><a href=/people/r/rui-zhang/>Rui Zhang</a>
|
<a href=/people/c/caitlin-westerfield/>Caitlin Westerfield</a>
|
<a href=/people/s/sungrok-shim/>Sungrok Shim</a>
|
<a href=/people/g/garrett-bingham/>Garrett Bingham</a>
|
<a href=/people/a/alexander-richard-fabbri/>Alexander Fabbri</a>
|
<a href=/people/w/william-hu/>William Hu</a>
|
<a href=/people/n/neha-verma/>Neha Verma</a>
|
<a href=/people/d/dragomir-radev/>Dragomir Radev</a><br><a href=/volumes/P19-1/ class=text-muted>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1306><div class="card-body p-3 small">In this paper, we propose to boost low-resource cross-lingual document retrieval performance with deep bilingual query-document representations. We match queries and documents in both source and target languages with four components, each of which is implemented as a term interaction-based deep neural network with cross-lingual word embeddings as input. By including query likelihood scores as extra <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a>, our model effectively learns to rerank the retrieved documents by using a small number of relevance labels for low-resource language pairs. Due to the shared cross-lingual word embedding space, the <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> can also be directly applied to another language pair without any <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training label</a>. Experimental results on the Material dataset show that our model outperforms the competitive translation-based baselines on English-Swahili, English-Tagalog, and English-Somali cross-lingual information retrieval tasks.</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1193.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1193 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1193 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-1193/>SyntaxSQLNet : Syntax Tree Networks for Complex and Cross-Domain Text-to-SQL Task<span class=acl-fixed-case>S</span>yntax<span class=acl-fixed-case>SQLN</span>et: Syntax Tree Networks for Complex and Cross-Domain Text-to-<span class=acl-fixed-case>SQL</span> Task</a></strong><br><a href=/people/t/tao-yu/>Tao Yu</a>
|
<a href=/people/m/michihiro-yasunaga/>Michihiro Yasunaga</a>
|
<a href=/people/k/kai-yang/>Kai Yang</a>
|
<a href=/people/r/rui-zhang/>Rui Zhang</a>
|
<a href=/people/d/dongxu-wang/>Dongxu Wang</a>
|
<a href=/people/z/zifan-li/>Zifan Li</a>
|
<a href=/people/d/dragomir-radev/>Dragomir Radev</a><br><a href=/volumes/D18-1/ class=text-muted>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1193><div class="card-body p-3 small">Most existing studies in text-to-SQL tasks do not require generating complex SQL queries with multiple clauses or sub-queries, and generalizing to new, unseen databases. In this paper we propose SyntaxSQLNet, a syntax tree network to address the complex and cross-domain text-to-SQL generation task. SyntaxSQLNet employs a SQL specific syntax tree-based decoder with SQL generation path history and table-aware column attention encoders. We evaluate SyntaxSQLNet on a new large-scale text-to-SQL corpus containing databases with multiple tables and complex SQL queries containing multiple SQL clauses and nested queries. We use a database split setting where databases in the test set are unseen during training. Experimental results show that SyntaxSQLNet can handle a significantly greater number of complex SQL examples than prior work, outperforming the previous state-of-the-art <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> by 9.5 % in exact matching accuracy. To our knowledge, we are the first to study this complex text-to-SQL task. Our <a href=https://en.wikipedia.org/wiki/Task_(computing)>task</a> and models with the latest updates are available at.<url>https://yale-lily.github.io/seq2sql/spider</url>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1089.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1089 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1089 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N18-1089" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N18-1089/>Robust Multilingual Part-of-Speech Tagging via Adversarial Training</a></strong><br><a href=/people/m/michihiro-yasunaga/>Michihiro Yasunaga</a>
|
<a href=/people/j/jungo-kasai/>Jungo Kasai</a>
|
<a href=/people/d/dragomir-radev/>Dragomir Radev</a><br><a href=/volumes/N18-1/ class=text-muted>Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1089><div class="card-body p-3 small">Adversarial training (AT) is a powerful <a href=https://en.wikipedia.org/wiki/Regularization_(mathematics)>regularization method</a> for <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a>, aiming to achieve <a href=https://en.wikipedia.org/wiki/Robustness_(computer_science)>robustness</a> to input perturbations. Yet, the specific effects of the <a href=https://en.wikipedia.org/wiki/Robustness_(computer_science)>robustness</a> obtained from AT are still unclear in the context of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a>. In this paper, we propose and analyze a neural POS tagging model that exploits AT. In our experiments on the Penn Treebank WSJ corpus and the Universal Dependencies (UD) dataset (27 languages), we find that AT not only improves the overall tagging accuracy, but also 1) prevents over-fitting well in low resource languages and 2) boosts tagging accuracy for rare / unseen words. We also demonstrate that 3) the improved <a href=https://en.wikipedia.org/wiki/Tag_(metadata)>tagging</a> performance by AT contributes to the downstream task of dependency parsing, and that 4) AT helps the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> to learn cleaner word representations. 5) The proposed AT model is generally effective in different sequence labeling tasks. These positive results motivate further use of AT for <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language tasks</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1033.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1033 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1033 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1033.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-1033" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P18-1033/>Improving Text-to-SQL Evaluation Methodology<span class=acl-fixed-case>SQL</span> Evaluation Methodology</a></strong><br><a href=/people/c/catherine-finegan-dollak/>Catherine Finegan-Dollak</a>
|
<a href=/people/j/jonathan-k-kummerfeld/>Jonathan K. Kummerfeld</a>
|
<a href=/people/l/li-zhang/>Li Zhang</a>
|
<a href=/people/k/karthik-ramanathan/>Karthik Ramanathan</a>
|
<a href=/people/s/sesh-sadasivam/>Sesh Sadasivam</a>
|
<a href=/people/r/rui-zhang/>Rui Zhang</a>
|
<a href=/people/d/dragomir-radev/>Dragomir Radev</a><br><a href=/volumes/P18-1/ class=text-muted>Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1033><div class="card-body p-3 small">To be informative, an evaluation must measure how well systems generalize to realistic unseen data. We identify limitations of and propose improvements to current evaluations of text-to-SQL systems. First, we compare human-generated and automatically generated questions, characterizing properties of queries necessary for real-world applications. To facilitate evaluation on multiple datasets, we release standardized and improved versions of seven existing <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> and one new text-to-SQL dataset. Second, we show that the current division of data into training and test sets measures robustness to variations in the way questions are asked, but only partially tests how well systems generalize to new queries ; therefore, we propose a complementary dataset split for evaluation of future work. Finally, we demonstrate how the common practice of anonymizing variables during <a href=https://en.wikipedia.org/wiki/Evaluation>evaluation</a> removes an important challenge of the task. Our observations highlight key difficulties, and our <a href=https://en.wikipedia.org/wiki/Methodology>methodology</a> enables effective measurement of future development.</div></div><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K17-1045.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K17-1045 data-toggle=collapse aria-expanded=false aria-controls=abstract-K17-1045 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/K17-1045/>Graph-based Neural Multi-Document Summarization</a></strong><br><a href=/people/m/michihiro-yasunaga/>Michihiro Yasunaga</a>
|
<a href=/people/r/rui-zhang/>Rui Zhang</a>
|
<a href=/people/k/kshitijh-meelu/>Kshitijh Meelu</a>
|
<a href=/people/a/ayush-pareek/>Ayush Pareek</a>
|
<a href=/people/k/krishnan-srinivasan/>Krishnan Srinivasan</a>
|
<a href=/people/d/dragomir-radev/>Dragomir Radev</a><br><a href=/volumes/K17-1/ class=text-muted>Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K17-1045><div class="card-body p-3 small">We propose a neural multi-document summarization system that incorporates sentence relation graphs. We employ a Graph Convolutional Network (GCN) on the relation graphs, with sentence embeddings obtained from <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>Recurrent Neural Networks</a> as input node features. Through multiple layer-wise propagation, the GCN generates high-level hidden sentence features for salience estimation. We then use a <a href=https://en.wikipedia.org/wiki/Greedy_heuristic>greedy heuristic</a> to extract salient sentences that avoid <a href=https://en.wikipedia.org/wiki/Redundancy_(information_theory)>redundancy</a>. In our experiments on DUC 2004, we consider three types of sentence relation graphs and demonstrate the advantage of combining sentence relations in <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graphs</a> with the representation power of <a href=https://en.wikipedia.org/wiki/Deep_learning>deep neural networks</a>. Our model improves upon other traditional graph-based extractive approaches and the vanilla GRU sequence model with no graph, and it achieves competitive results against other state-of-the-art multi-document summarization systems.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Dragomir+Radev" title="Search for 'Dragomir Radev' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/r/rui-zhang/ class=align-middle>Rui Zhang</a>
<span class="badge badge-secondary align-middle ml-2">7</span></li><li class=list-group-item><a href=/people/t/tao-yu/ class=align-middle>Tao Yu</a>
<span class="badge badge-secondary align-middle ml-2">5</span></li><li class=list-group-item><a href=/people/x/xi-victoria-lin/ class=align-middle>Xi Victoria Lin</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/people/m/michihiro-yasunaga/ class=align-middle>Michihiro Yasunaga</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/c/caiming-xiong/ class=align-middle>Caiming Xiong</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/a/alexander-richard-fabbri/ class=align-middle>Alexander Richard Fabbri</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/i/irene-li/ class=align-middle>Irene Li</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/s/sungrok-shim/ class=align-middle>Sungrok Shim</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/r/richard-socher/ class=align-middle>Richard Socher</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/n/neha-verma/ class=align-middle>Neha Verma</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/h/heather-lent/ class=align-middle>Heather Lent</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/semih-yavuz/ class=align-middle>Semih Yavuz</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/tong-niu/ class=align-middle>Tong Niu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yingbo-zhou/ class=align-middle>Yingbo Zhou</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/kshitijh-meelu/ class=align-middle>Kshitijh Meelu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/ayush-pareek/ class=align-middle>Ayush Pareek</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/krishnan-srinivasan/ class=align-middle>Krishnan Srinivasan</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/p/prithviraj-sen/ class=align-middle>Prithviraj Sen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/huaiyu-zhu/ class=align-middle>Huaiyu Zhu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yunyao-li/ class=align-middle>Yunyao Li</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/b/ben-bogin/ class=align-middle>Ben Bogin</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/srinivasan-iyer/ class=align-middle>Srinivasan Iyer</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/alane-suhr/ class=align-middle>Alane Suhr</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/p/panupong/ class=align-middle>Panupong</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/p/pengcheng-yin/ class=align-middle>Pengcheng Yin</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/v/victor-zhong/ class=align-middle>Victor Zhong</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/kai-yang/ class=align-middle>Kai Yang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/dongxu-wang/ class=align-middle>Dongxu Wang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/zifan-li/ class=align-middle>Zifan Li</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/heyang-er/ class=align-middle>Heyang Er</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/e/eric-xue/ class=align-middle>Eric Xue</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/tianze-shi/ class=align-middle>Tianze Shi</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/linyong-nan/ class=align-middle>Linyong Nan</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/amrit-rau/ class=align-middle>Amrit Rau</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/abhinand-sivaprasad/ class=align-middle>Abhinand Sivaprasad</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/chiachun-hsieh/ class=align-middle>Chiachun Hsieh</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/x/xiangru-tang/ class=align-middle>Xiangru Tang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/aadit-vyas/ class=align-middle>Aadit Vyas</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/p/pranav-krishna/ class=align-middle>Pranav Krishna</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yangxiaokang-liu/ class=align-middle>Yangxiaokang Liu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/n/nadia-irwanto/ class=align-middle>Nadia Irwanto</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jessica-pan/ class=align-middle>Jessica Pan</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/f/faiaz-rahman/ class=align-middle>Faiaz Rahman</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/ahmad-zaidi/ class=align-middle>Ahmad Zaidi</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/mutethia-mutuma/ class=align-middle>Mutethia Mutuma</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yasin-tarabar/ class=align-middle>Yasin Tarabar</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/ankit-gupta/ class=align-middle>Ankit Gupta</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yi-chern-tan/ class=align-middle>Yi Chern Tan</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/n/nazneen-fatema-rajani/ class=align-middle>Nazneen Fatema Rajani</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/simeng-han/ class=align-middle>Simeng Han</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/haoyuan-li/ class=align-middle>Haoyuan Li</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/haoran-li/ class=align-middle>Haoran Li</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/marjan-ghazvininejad/ class=align-middle>Marjan Ghazvininejad</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/shafiq-joty/ class=align-middle>Shafiq Joty</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yashar-mehdad/ class=align-middle>Yashar Mehdad</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jungo-kasai/ class=align-middle>Jungo Kasai</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/catherine-finegan-dollak/ class=align-middle>Catherine Finegan-Dollak</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jonathan-k-kummerfeld/ class=align-middle>Jonathan K. Kummerfeld</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/li-zhang/ class=align-middle>Li Zhang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/karthik-ramanathan/ class=align-middle>Karthik Ramanathan</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/sesh-sadasivam/ class=align-middle>Sesh Sadasivam</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/tianwei-she/ class=align-middle>Tianwei She</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/suyi-li/ class=align-middle>Suyi Li</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/caitlin-westerfield/ class=align-middle>Caitlin Westerfield</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/g/garrett-bingham/ class=align-middle>Garrett Bingham</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/w/william-hu/ class=align-middle>William Hu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/naacl/ class=align-middle>NAACL</a><span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/venues/acl/ class=align-middle>ACL</a><span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/venues/emnlp/ class=align-middle>EMNLP</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/eval4nlp/ class=align-middle>Eval4NLP</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/conll/ class=align-middle>CoNLL</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-venues aria-expanded=false aria-controls=more-venues>show all...</li><div class="collapse border-top" id=more-venues><li class=list-group-item><a href=/venues/repl4nlp/ class=align-middle>RepL4NLP</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/intexsempar/ class=align-middle>intexsempar</a><span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>