<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Dat Quoc Nguyen - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Dat Quoc</span> <span class=font-weight-bold>Nguyen</span></h2><hr><div class=row><div class=col-lg-9><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.369.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--369 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.369 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.369" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.369/>PhoMT : A High-Quality and Large-Scale Benchmark Dataset for Vietnamese-English Machine Translation<span class=acl-fixed-case>P</span>ho<span class=acl-fixed-case>MT</span>: A High-Quality and Large-Scale Benchmark Dataset for <span class=acl-fixed-case>V</span>ietnamese-<span class=acl-fixed-case>E</span>nglish Machine Translation</a></strong><br><a href=/people/l/long-doan/>Long Doan</a>
|
<a href=/people/l/linh-the-nguyen/>Linh The Nguyen</a>
|
<a href=/people/n/nguyen-luong-tran/>Nguyen Luong Tran</a>
|
<a href=/people/t/thai-hoang/>Thai Hoang</a>
|
<a href=/people/d/dat-quoc-nguyen/>Dat Quoc Nguyen</a><br><a href=/volumes/2021.emnlp-main/ class=text-muted>Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--369><div class="card-body p-3 small">We introduce a high-quality and large-scale Vietnamese-English parallel dataset of 3.02 M sentence pairs, which is 2.9 M pairs larger than the benchmark Vietnamese-English machine translation corpus IWSLT15. We conduct experiments comparing strong neural baselines and well-known automatic translation engines on our dataset and find that in both automatic and human evaluations : the best performance is obtained by fine-tuning the pre-trained sequence-to-sequence denoising auto-encoder mBART. To our best knowledge, this is the first large-scale Vietnamese-English machine translation study. We hope our publicly available dataset and study can serve as a starting point for future research and applications on Vietnamese-English machine translation. We release our dataset at : https://github.com/VinAIResearch/PhoMT</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-demos.1.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-demos--1 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-demos.1 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.naacl-demos.1" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-demos.1/>PhoNLP : A joint multi-task learning model for Vietnamese part-of-speech tagging, named entity recognition and dependency parsing<span class=acl-fixed-case>P</span>ho<span class=acl-fixed-case>NLP</span>: A joint multi-task learning model for <span class=acl-fixed-case>V</span>ietnamese part-of-speech tagging, named entity recognition and dependency parsing</a></strong><br><a href=/people/l/linh-the-nguyen/>Linh The Nguyen</a>
|
<a href=/people/d/dat-quoc-nguyen/>Dat Quoc Nguyen</a><br><a href=/volumes/2021.naacl-demos/ class=text-muted>Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Demonstrations</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-demos--1><div class="card-body p-3 small">We present the first multi-task learning model named PhoNLP for joint Vietnamese part-of-speech (POS) tagging, named entity recognition (NER) and dependency parsing. Experiments on Vietnamese benchmark datasets show that PhoNLP produces state-of-the-art results, outperforming a single-task learning approach that fine-tunes the pre-trained Vietnamese language model PhoBERT (Nguyen and Nguyen, 2020) for each task independently. We publicly release PhoNLP as an open-source toolkit under the <a href=https://en.wikipedia.org/wiki/Apache_License>Apache License 2.0</a>. Although we specify PhoNLP for <a href=https://en.wikipedia.org/wiki/Vietnamese_language>Vietnamese</a>, our PhoNLP training and evaluation command scripts in fact can directly work for other languages that have a pre-trained BERT-based language model and gold annotated corpora available for the three tasks of POS tagging, NER and dependency parsing. We hope that PhoNLP can serve as a strong baseline and useful toolkit for future NLP research and applications to not only <a href=https://en.wikipedia.org/wiki/Vietnamese_language>Vietnamese</a> but also the other languages. Our PhoNLP is available at https://github.com/VinAIResearch/PhoNLP</div></div><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.textgraphs-1.1.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--textgraphs-1--1 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.textgraphs-1.1 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.textgraphs-1.1.OptionalSupplementaryMaterial.pdf data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.textgraphs-1.1" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.textgraphs-1.1/>A survey of embedding models of entities and relationships for knowledge graph completion</a></strong><br><a href=/people/d/dat-quoc-nguyen/>Dat Quoc Nguyen</a><br><a href=/volumes/2020.textgraphs-1/ class=text-muted>Proceedings of the Graph-based Methods for Natural Language Processing (TextGraphs)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--textgraphs-1--1><div class="card-body p-3 small">Knowledge graphs (KGs) of real-world facts about entities and their relationships are useful resources for a variety of natural language processing tasks. However, because <a href=https://en.wikipedia.org/wiki/Knowledge_graph>knowledge graphs</a> are typically incomplete, it is useful to perform knowledge graph completion or link prediction, i.e. predict whether a relationship not in the <a href=https://en.wikipedia.org/wiki/Knowledge_graph>knowledge graph</a> is likely to be true. This paper serves as a comprehensive survey of embedding models of entities and relationships for knowledge graph completion, summarizing up-to-date experimental results on standard benchmark datasets and pointing out potential future research directions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-demos.2.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-demos--2 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-demos.2 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-demos.2" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.emnlp-demos.2/>BERTweet : A pre-trained language model for English Tweets<span class=acl-fixed-case>BERT</span>weet: A pre-trained language model for <span class=acl-fixed-case>E</span>nglish Tweets</a></strong><br><a href=/people/d/dat-quoc-nguyen/>Dat Quoc Nguyen</a>
|
<a href=/people/t/thanh-vu/>Thanh Vu</a>
|
<a href=/people/a/anh-tuan-nguyen/>Anh Tuan Nguyen</a><br><a href=/volumes/2020.emnlp-demos/ class=text-muted>Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-demos--2><div class="card-body p-3 small">We present BERTweet, the first public large-scale pre-trained language model for English Tweets. Our BERTweet, having the same architecture as BERT-base (Devlin et al., 2019), is trained using the RoBERTa pre-training procedure (Liu et al., 2019). Experiments show that BERTweet outperforms strong baselines RoBERTa-base and XLM-R-base (Conneau et al., 2020), producing better performance results than the previous state-of-the-art models on three Tweet NLP tasks : <a href=https://en.wikipedia.org/wiki/Part-of-speech_tagging>Part-of-speech tagging</a>, <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>Named-entity recognition</a> and text classification. We release BERTweet under the MIT License to facilitate future research and applications on Tweet data. Our BERTweet is available at https://github.com/VinAIResearch/BERTweet</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wnut-1.41.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wnut-1--41 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wnut-1.41 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.wnut-1.41/>WNUT-2020 Task 2 : Identification of Informative COVID-19 English Tweets<span class=acl-fixed-case>WNUT</span>-2020 Task 2: Identification of Informative <span class=acl-fixed-case>COVID</span>-19 <span class=acl-fixed-case>E</span>nglish Tweets</a></strong><br><a href=/people/d/dat-quoc-nguyen/>Dat Quoc Nguyen</a>
|
<a href=/people/t/thanh-vu/>Thanh Vu</a>
|
<a href=/people/a/afshin-rahimi/>Afshin Rahimi</a>
|
<a href=/people/m/mai-hoang-dao/>Mai Hoang Dao</a>
|
<a href=/people/l/linh-the-nguyen/>Linh The Nguyen</a>
|
<a href=/people/l/long-doan/>Long Doan</a><br><a href=/volumes/2020.wnut-1/ class=text-muted>Proceedings of the Sixth Workshop on Noisy User-generated Text (W-NUT 2020)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wnut-1--41><div class="card-body p-3 small">In this paper, we provide an overview of the WNUT-2020 shared task on the identification of informative COVID-19 English Tweets. We describe how we construct a <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus of 10 K Tweets</a> and organize the development and evaluation phases for this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. In addition, we also present a brief summary of results obtained from the final system evaluation submissions of 55 teams, finding that (i) many systems obtain very high performance, up to 0.91 F1 score, (ii) the majority of the submissions achieve substantially higher results than the baseline <a href=https://en.wikipedia.org/wiki/FastText>fastText</a> (Joulin et al., 2017), and (iii) fine-tuning pre-trained language models on relevant language data followed by <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised training</a> performs well in this task.</div></div><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/U19-1014.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-U19-1014 data-toggle=collapse aria-expanded=false aria-controls=abstract-U19-1014 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/U19-1014/>Detecting Chemical Reactions in Patents</a></strong><br><a href=/people/h/hiyori-yoshikawa/>Hiyori Yoshikawa</a>
|
<a href=/people/d/dat-quoc-nguyen/>Dat Quoc Nguyen</a>
|
<a href=/people/z/zenan-zhai/>Zenan Zhai</a>
|
<a href=/people/c/christian-druckenbrodt/>Christian Druckenbrodt</a>
|
<a href=/people/c/camilo-thorne/>Camilo Thorne</a>
|
<a href=/people/s/saber-a-akhondi/>Saber A. Akhondi</a>
|
<a href=/people/t/timothy-baldwin/>Timothy Baldwin</a>
|
<a href=/people/k/karin-verspoor/>Karin Verspoor</a><br><a href=/volumes/U19-1/ class=text-muted>Proceedings of the The 17th Annual Workshop of the Australasian Language Technology Association</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-U19-1014><div class="card-body p-3 small">Extracting <a href=https://en.wikipedia.org/wiki/Chemical_reaction>chemical reactions</a> from <a href=https://en.wikipedia.org/wiki/Patent>patents</a> is a crucial task for chemists working on <a href=https://en.wikipedia.org/wiki/Chemical_engineering>chemical exploration</a>. In this paper we introduce the novel task of detecting the textual spans that describe or refer to <a href=https://en.wikipedia.org/wiki/Chemical_reaction>chemical reactions</a> within patents. We formulate this task as a paragraph-level sequence tagging problem, where the system is required to return a sequence of paragraphs which contain a description of a reaction. To address this new task, we construct an annotated dataset from an existing proprietary database of chemical reactions manually extracted from <a href=https://en.wikipedia.org/wiki/Patent>patents</a>. We introduce several baseline methods for the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> and evaluate them over our <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>. Through error analysis, we discuss what makes the task complex and challenging, and suggest possible directions for future research.</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S18-1085.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S18-1085 data-toggle=collapse aria-expanded=false aria-controls=abstract-S18-1085 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=S18-1085" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/S18-1085/>NIHRIO at SemEval-2018 Task 3 : A Simple and Accurate Neural Network Model for Irony Detection in Twitter<span class=acl-fixed-case>NIHRIO</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2018 Task 3: A Simple and Accurate Neural Network Model for Irony Detection in <span class=acl-fixed-case>T</span>witter</a></strong><br><a href=/people/t/thanh-vu/>Thanh Vu</a>
|
<a href=/people/d/dat-quoc-nguyen/>Dat Quoc Nguyen</a>
|
<a href=/people/x/xuan-son-vu/>Xuan-Son Vu</a>
|
<a href=/people/d/dai-quoc-nguyen/>Dai Quoc Nguyen</a>
|
<a href=/people/m/michael-catt/>Michael Catt</a>
|
<a href=/people/m/michael-trenell/>Michael Trenell</a><br><a href=/volumes/S18-1/ class=text-muted>Proceedings of The 12th International Workshop on Semantic Evaluation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S18-1085><div class="card-body p-3 small">This paper describes our NIHRIO system for SemEval-2018 Task 3 Irony detection in English tweets. We propose to use a simple neural network architecture of Multilayer Perceptron with various types of input features including : lexical, syntactic, semantic and polarity features. Our system achieves very high performance in both subtasks of binary and multi-class irony detection in <a href=https://en.wikipedia.org/wiki/Twitter>tweets</a>. In particular, we rank at least fourth using the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy metric</a> and sixth using the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>F1 metric</a>. Our code is available at :<url>https://github.com/NIHRIO/IronyDetectionInTwitter</url>\n</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5605.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5605 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5605 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5605/>Comparing CNN and LSTM character-level embeddings in BiLSTM-CRF models for chemical and disease named entity recognition<span class=acl-fixed-case>CNN</span> and <span class=acl-fixed-case>LSTM</span> character-level embeddings in <span class=acl-fixed-case>B</span>i<span class=acl-fixed-case>LSTM</span>-<span class=acl-fixed-case>CRF</span> models for chemical and disease named entity recognition</a></strong><br><a href=/people/z/zenan-zhai/>Zenan Zhai</a>
|
<a href=/people/d/dat-quoc-nguyen/>Dat Quoc Nguyen</a>
|
<a href=/people/k/karin-verspoor/>Karin Verspoor</a><br><a href=/volumes/W18-56/ class=text-muted>Proceedings of the Ninth International Workshop on Health Text Mining and Information Analysis</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5605><div class="card-body p-3 small">We compare the use of LSTM-based and CNN-based character-level word embeddings in BiLSTM-CRF models to approach chemical and disease named entity recognition (NER) tasks. Empirical results over the BioCreative V CDR corpus show that the use of either type of character-level word embeddings in conjunction with the BiLSTM-CRF models leads to comparable state-of-the-art performance. However, the models using CNN-based character-level word embeddings have a computational performance advantage, increasing <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training time</a> over word-based models by 25 % while the LSTM-based character-level word embeddings more than double the required <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training time</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2053.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2053 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2053 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N18-2053" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N18-2053/>A Novel Embedding Model for Knowledge Base Completion Based on <a href=https://en.wikipedia.org/wiki/Convolutional_neural_network>Convolutional Neural Network</a></a></strong><br><a href=/people/d/dai-quoc-nguyen/>Dai Quoc Nguyen</a>
|
<a href=/people/t/tu-dinh-nguyen/>Tu Dinh Nguyen</a>
|
<a href=/people/d/dat-quoc-nguyen/>Dat Quoc Nguyen</a>
|
<a href=/people/d/dinh-phung/>Dinh Phung</a><br><a href=/volumes/N18-2/ class=text-muted>Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2053><div class="card-body p-3 small">In this paper, we propose a novel <a href=https://en.wikipedia.org/wiki/Embedding>embedding model</a>, named ConvKB, for knowledge base completion. Our model ConvKB advances state-of-the-art models by employing a <a href=https://en.wikipedia.org/wiki/Convolutional_neural_network>convolutional neural network</a>, so that it can capture global relationships and transitional characteristics between entities and relations in <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge bases</a>. In ConvKB, each triple (head entity, relation, tail entity) is represented as a 3-column matrix where each column vector represents a triple element. This 3-column matrix is then fed to a convolution layer where multiple <a href=https://en.wikipedia.org/wiki/Filter_(signal_processing)>filters</a> are operated on the <a href=https://en.wikipedia.org/wiki/Matrix_(mathematics)>matrix</a> to generate different <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>feature maps</a>. These <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>feature maps</a> are then concatenated into a single <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>feature vector</a> representing the input triple. The <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>feature vector</a> is multiplied with a <a href=https://en.wikipedia.org/wiki/Weight_vector>weight vector</a> via a <a href=https://en.wikipedia.org/wiki/Dot_product>dot product</a> to return a score. This <a href=https://en.wikipedia.org/wiki/Score_(game)>score</a> is then used to predict whether the triple is valid or not. Experiments show that ConvKB achieves better link prediction performance than previous state-of-the-art embedding models on two benchmark datasets WN18RR and FB15k-237.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-5012.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-5012 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-5012 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N18-5012" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N18-5012/>VnCoreNLP : A Vietnamese Natural Language Processing Toolkit<span class=acl-fixed-case>V</span>n<span class=acl-fixed-case>C</span>ore<span class=acl-fixed-case>NLP</span>: A <span class=acl-fixed-case>V</span>ietnamese Natural Language Processing Toolkit</a></strong><br><a href=/people/t/thanh-vu/>Thanh Vu</a>
|
<a href=/people/d/dat-quoc-nguyen/>Dat Quoc Nguyen</a>
|
<a href=/people/d/dai-quoc-nguyen/>Dai Quoc Nguyen</a>
|
<a href=/people/m/mark-dras/>Mark Dras</a>
|
<a href=/people/m/mark-johnson/>Mark Johnson</a><br><a href=/volumes/N18-5/ class=text-muted>Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Demonstrations</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-5012><div class="card-body p-3 small">We present an easy-to-use and fast toolkit, namely VnCoreNLPa Java NLP annotation pipeline for <a href=https://en.wikipedia.org/wiki/Vietnamese_language>Vietnamese</a>. Our VnCoreNLP supports key natural language processing (NLP) tasks including word segmentation, part-of-speech (POS) tagging, named entity recognition (NER) and dependency parsing, and obtains state-of-the-art (SOTA) results for these tasks. We release VnCoreNLP to provide rich linguistic annotations to facilitate research work on Vietnamese NLP. Our VnCoreNLP is open-source and available at :<url>https://github.com/vncorenlp/VnCoreNLP</url>\n</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K18-2008.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K18-2008 data-toggle=collapse aria-expanded=false aria-controls=abstract-K18-2008 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=K18-2008" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/K18-2008/>An Improved Neural Network Model for Joint POS Tagging and Dependency Parsing<span class=acl-fixed-case>POS</span> Tagging and Dependency Parsing</a></strong><br><a href=/people/d/dat-quoc-nguyen/>Dat Quoc Nguyen</a>
|
<a href=/people/k/karin-verspoor/>Karin Verspoor</a><br><a href=/volumes/K18-2/ class=text-muted>Proceedings of the CoNLL 2018 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K18-2008><div class="card-body p-3 small">We propose a novel neural network model for joint part-of-speech (POS) tagging and dependency parsing. Our model extends the well-known BIST graph-based dependency parser (Kiperwasser and Goldberg, 2016) by incorporating a BiLSTM-based tagging component to produce automatically predicted POS tags for the <a href=https://en.wikipedia.org/wiki/Parsing>parser</a>. On the benchmark English Penn treebank, our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> obtains strong UAS and LAS scores at 94.51 % and 92.87 %, respectively, producing 1.5+% absolute improvements to the BIST graph-based parser, and also obtaining a state-of-the-art POS tagging accuracy at 97.97 %. Furthermore, experimental results on parsing 61 big Universal Dependencies treebanks from raw texts show that our model outperforms the baseline UDPipe (Straka and Strakova, 2017) with 0.8 % higher average POS tagging score and 3.6 % higher average LAS score. In addition, with our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>, we also obtain state-of-the-art downstream task scores for biomedical event extraction and opinion analysis applications. Our code is available together with all pre-trained models at :<url>https://github.com/datquocnguyen/jPTDP</url>\n</div></div><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K17-3014.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K17-3014 data-toggle=collapse aria-expanded=false aria-controls=abstract-K17-3014 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=K17-3014" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/K17-3014/>A Novel Neural Network Model for Joint POS Tagging and Graph-based Dependency Parsing<span class=acl-fixed-case>POS</span> Tagging and Graph-based Dependency Parsing</a></strong><br><a href=/people/d/dat-quoc-nguyen/>Dat Quoc Nguyen</a>
|
<a href=/people/m/mark-dras/>Mark Dras</a>
|
<a href=/people/m/mark-johnson/>Mark Johnson</a><br><a href=/volumes/K17-3/ class=text-muted>Proceedings of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K17-3014><div class="card-body p-3 small">We present a novel <a href=https://en.wikipedia.org/wiki/Neural_network>neural network model</a> that learns <a href=https://en.wikipedia.org/wiki/Tag_(metadata)>POS tagging</a> and graph-based dependency parsing jointly. Our model uses bidirectional LSTMs to learn feature representations shared for both POS tagging and dependency parsing tasks, thus handling the feature-engineering problem. Our extensive experiments, on 19 languages from the Universal Dependencies project, show that our model outperforms the state-of-the-art neural network-based Stack-propagation model for joint POS tagging and transition-based dependency parsing, resulting in a new state of the art. Our code is open-source and available together with pre-trained models at :<url>https://github.com/datquocnguyen/jPTDP</url>\n</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S17-1015.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S17-1015 data-toggle=collapse aria-expanded=false aria-controls=abstract-S17-1015 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/S17-1015/>A <a href=https://en.wikipedia.org/wiki/Mixture_model>Mixture Model</a> for Learning Multi-Sense Word Embeddings</a></strong><br><a href=/people/d/dai-quoc-nguyen/>Dai Quoc Nguyen</a>
|
<a href=/people/d/dat-quoc-nguyen/>Dat Quoc Nguyen</a>
|
<a href=/people/a/ashutosh-modi/>Ashutosh Modi</a>
|
<a href=/people/s/stefan-thater/>Stefan Thater</a>
|
<a href=/people/m/manfred-pinkal/>Manfred Pinkal</a><br><a href=/volumes/S17-1/ class=text-muted>Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (*SEM 2017)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S17-1015><div class="card-body p-3 small">Word embeddings are now a standard technique for inducing <a href=https://en.wikipedia.org/wiki/Meaning_(linguistics)>meaning representations</a> for words. For getting good <a href=https://en.wikipedia.org/wiki/Representation_(arts)>representations</a>, it is important to take into account different senses of a word. In this paper, we propose a <a href=https://en.wikipedia.org/wiki/Mixture_model>mixture model</a> for learning multi-sense word embeddings. Our <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> generalizes the previous works in that it allows to induce different weights of different senses of a word. The experimental results show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms previous <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> on standard evaluation tasks.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Dat+Quoc+Nguyen" title="Search for 'Dat Quoc Nguyen' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/t/thanh-vu/ class=align-middle>Thanh Vu</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/people/d/dai-quoc-nguyen/ class=align-middle>Dai Quoc Nguyen</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/people/l/linh-the-nguyen/ class=align-middle>Linh The Nguyen</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/k/karin-verspoor/ class=align-middle>Karin Verspoor</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/m/mark-dras/ class=align-middle>Mark Dras</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/m/mark-johnson/ class=align-middle>Mark Johnson</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/l/long-doan/ class=align-middle>Long Doan</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/z/zenan-zhai/ class=align-middle>Zenan Zhai</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/a/anh-tuan-nguyen/ class=align-middle>Anh Tuan Nguyen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/n/nguyen-luong-tran/ class=align-middle>Nguyen Luong Tran</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/thai-hoang/ class=align-middle>Thai Hoang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/ashutosh-modi/ class=align-middle>Ashutosh Modi</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/stefan-thater/ class=align-middle>Stefan Thater</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/manfred-pinkal/ class=align-middle>Manfred Pinkal</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/x/xuan-son-vu/ class=align-middle>Xuan-Son Vu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/michael-catt/ class=align-middle>Michael Catt</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/michael-trenell/ class=align-middle>Michael Trenell</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/hiyori-yoshikawa/ class=align-middle>Hiyori Yoshikawa</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/christian-druckenbrodt/ class=align-middle>Christian Druckenbrodt</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/camilo-thorne/ class=align-middle>Camilo Thorne</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/saber-a-akhondi/ class=align-middle>Saber A. Akhondi</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/timothy-baldwin/ class=align-middle>Timothy Baldwin</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/tu-dinh-nguyen/ class=align-middle>Tu Dinh Nguyen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/dinh-phung/ class=align-middle>Dinh Phung</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/afshin-rahimi/ class=align-middle>Afshin Rahimi</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/mai-hoang-dao/ class=align-middle>Mai Hoang Dao</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/naacl/ class=align-middle>NAACL</a><span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/venues/emnlp/ class=align-middle>EMNLP</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/conll/ class=align-middle>CoNLL</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/semeval/ class=align-middle>SemEval</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/textgraphs/ class=align-middle>TextGraphs</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-venues aria-expanded=false aria-controls=more-venues>show all...</li><div class="collapse border-top" id=more-venues><li class=list-group-item><a href=/venues/ws/ class=align-middle>WS</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/alta/ class=align-middle>ALTA</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/wnut/ class=align-middle>WNUT</a><span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright Â©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>