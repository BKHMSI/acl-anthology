<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Daisuke Kawahara - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Daisuke</span> <span class=font-weight-bold>Kawahara</span></h2><hr><div class=row><div class=col-lg-9><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-srw.15.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-srw--15 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-srw.15 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-srw.15/>Multi-Task Learning of Generation and Classification for Emotion-Aware Dialogue Response Generation</a></strong><br><a href=/people/t/tatsuya-ide/>Tatsuya Ide</a>
|
<a href=/people/d/daisuke-kawahara/>Daisuke Kawahara</a><br><a href=/volumes/2021.naacl-srw/ class=text-muted>Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Student Research Workshop</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-srw--15><div class="card-body p-3 small">For a computer to naturally interact with a human, it needs to be human-like. In this paper, we propose a neural response generation model with multi-task learning of generation and classification, focusing on <a href=https://en.wikipedia.org/wiki/Emotion>emotion</a>. Our <a href=https://en.wikipedia.org/wiki/Computer_simulation>model</a> based on BART (Lewis et al., 2020), a pre-trained transformer encoder-decoder model, is trained to generate responses and recognize <a href=https://en.wikipedia.org/wiki/Emotion>emotions</a> simultaneously. Furthermore, we weight the losses for the <a href=https://en.wikipedia.org/wiki/Task_(computing)>tasks</a> to control the update of parameters. Automatic evaluations and crowdsourced manual evaluations show that the proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> makes generated responses more emotionally aware.</div></div><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.114.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--114 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.114 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.coling-main.114" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.114/>BERT-based Cohesion Analysis of <a href=https://en.wikipedia.org/wiki/Japanese_language>Japanese Texts</a><span class=acl-fixed-case>BERT</span>-based Cohesion Analysis of <span class=acl-fixed-case>J</span>apanese Texts</a></strong><br><a href=/people/n/nobuhiro-ueda/>Nobuhiro Ueda</a>
|
<a href=/people/d/daisuke-kawahara/>Daisuke Kawahara</a>
|
<a href=/people/s/sadao-kurohashi/>Sadao Kurohashi</a><br><a href=/volumes/2020.coling-main/ class=text-muted>Proceedings of the 28th International Conference on Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--114><div class="card-body p-3 small">The meaning of natural language text is supported by <a href=https://en.wikipedia.org/wiki/Cohesion_(linguistics)>cohesion</a> among various kinds of entities, including <a href=https://en.wikipedia.org/wiki/Coreference>coreference relations</a>, predicate-argument structures, and <a href=https://en.wikipedia.org/wiki/Anaphora_(linguistics)>bridging anaphora relations</a>. However, predicate-argument structures for nominal predicates and bridging anaphora relations have not been studied well, and their analyses have been still very difficult. Recent advances in neural networks, in particular, self training-based language models including <a href=https://en.wikipedia.org/wiki/BERT>BERT</a> (Devlin et al., 2019), have significantly improved many natural language processing tasks, making it possible to dive into the study on analysis of cohesion in the whole text. In this study, we tackle an integrated analysis of cohesion in <a href=https://en.wikipedia.org/wiki/Japanese_literature>Japanese texts</a>. Our results significantly outperformed existing studies in each task, especially about 10 to 20 point improvement both for zero anaphora and coreference resolution. Furthermore, we also showed that <a href=https://en.wikipedia.org/wiki/Coreference_resolution>coreference resolution</a> is different in nature from the other <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> and should be treated specially.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.379.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--379 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.379 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.379/>Development of a Japanese Personality Dictionary based on Psychological Methods<span class=acl-fixed-case>J</span>apanese Personality Dictionary based on Psychological Methods</a></strong><br><a href=/people/r/ritsuko-iwai/>Ritsuko Iwai</a>
|
<a href=/people/d/daisuke-kawahara/>Daisuke Kawahara</a>
|
<a href=/people/t/takatsune-kumada/>Takatsune Kumada</a>
|
<a href=/people/s/sadao-kurohashi/>Sadao Kurohashi</a><br><a href=/volumes/2020.lrec-1/ class=text-muted>Proceedings of the 12th Language Resources and Evaluation Conference</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--379><div class="card-body p-3 small">We propose a new approach to constructing a personality dictionary with psychological evidence. In this study, we collect personality words, using <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a>, and construct a personality dictionary with weights for <a href=https://en.wikipedia.org/wiki/Big_Five_personality_traits>Big Five traits</a>. The weights are calculated based on the responses of the large sample (N=1,938, female = 1,004, M=49.8years old:20-78, SD=16.3). All the respondents answered a 20-item personality questionnaire and 537 personality items derived from <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a>. We present the <a href=https://en.wikipedia.org/wiki/Procedure_(term)>procedures</a> to examine the qualities of responses with <a href=https://en.wikipedia.org/wiki/Psychology>psychological methods</a> and to calculate the weights. These result in a personality dictionary with two sub-dictionaries. We also discuss an application of the acquired resources.</div></div><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5814.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5814 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5814 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5814/>Machine Comprehension Improves Domain-Specific Japanese Predicate-Argument Structure Analysis<span class=acl-fixed-case>J</span>apanese Predicate-Argument Structure Analysis</a></strong><br><a href=/people/n/norio-takahashi/>Norio Takahashi</a>
|
<a href=/people/t/tomohide-shibata/>Tomohide Shibata</a>
|
<a href=/people/d/daisuke-kawahara/>Daisuke Kawahara</a>
|
<a href=/people/s/sadao-kurohashi/>Sadao Kurohashi</a><br><a href=/volumes/D19-58/ class=text-muted>Proceedings of the 2nd Workshop on Machine Reading for Question Answering</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5814><div class="card-body p-3 small">To improve the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of predicate-argument structure (PAS) analysis, large-scale training data and knowledge for PAS analysis are indispensable. We focus on a specific domain, specifically Japanese blogs on driving, and construct two wide-coverage datasets as a form of QA using <a href=https://en.wikipedia.org/wiki/Crowdsourcing>crowdsourcing</a> : a PAS-QA dataset and a reading comprehension QA (RC-QA) dataset. We train a machine comprehension (MC) model based on these <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> to perform PAS analysis. Our experiments show that a stepwise training method is the most effective, which pre-trains an MC model based on the RC-QA dataset to acquire <a href=https://en.wikipedia.org/wiki/Domain_knowledge>domain knowledge</a> and then fine-tunes based on the PAS-QA dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-6014.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-6014 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-6014 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-6014/>Diversity-aware Event Prediction based on a Conditional Variational Autoencoder with Reconstruction</a></strong><br><a href=/people/h/hirokazu-kiyomaru/>Hirokazu Kiyomaru</a>
|
<a href=/people/k/kazumasa-omura/>Kazumasa Omura</a>
|
<a href=/people/y/yugo-murawaki/>Yugo Murawaki</a>
|
<a href=/people/d/daisuke-kawahara/>Daisuke Kawahara</a>
|
<a href=/people/s/sadao-kurohashi/>Sadao Kurohashi</a><br><a href=/volumes/D19-60/ class=text-muted>Proceedings of the First Workshop on Commonsense Inference in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-6014><div class="card-body p-3 small">Typical event sequences are an important class of <a href=https://en.wikipedia.org/wiki/Commonsense_knowledge>commonsense knowledge</a>. Formalizing the task as the generation of a next event conditioned on a current event, previous work in event prediction employs sequence-to-sequence (seq2seq) models. However, what can happen after a given event is usually diverse, a fact that can hardly be captured by deterministic models. In this paper, we propose to incorporate a conditional variational autoencoder (CVAE) into seq2seq for its ability to represent diverse next events as a probabilistic distribution. We further extend the CVAE-based seq2seq with a reconstruction mechanism to prevent the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> from concentrating on highly typical events. To facilitate fair and systematic evaluation of the diversity-aware models, we also extend existing evaluation datasets by tying each current event to multiple next events. Experiments show that the CVAE-based models drastically outperform deterministic models in terms of <a href=https://en.wikipedia.org/wiki/Precision_(statistics)>precision</a> and that the reconstruction mechanism improves the <a href=https://en.wikipedia.org/wiki/Precision_(statistics)>recall</a> of CVAE-based models without sacrificing <a href=https://en.wikipedia.org/wiki/Precision_(statistics)>precision</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1281.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1281 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1281 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-1281/>Shrinking Japanese Morphological Analyzers With <a href=https://en.wikipedia.org/wiki/Neural_network>Neural Networks</a> and <a href=https://en.wikipedia.org/wiki/Semi-supervised_learning>Semi-supervised Learning</a><span class=acl-fixed-case>J</span>apanese Morphological Analyzers With Neural Networks and Semi-supervised Learning</a></strong><br><a href=/people/a/arseny-tolmachev/>Arseny Tolmachev</a>
|
<a href=/people/d/daisuke-kawahara/>Daisuke Kawahara</a>
|
<a href=/people/s/sadao-kurohashi/>Sadao Kurohashi</a><br><a href=/volumes/N19-1/ class=text-muted>Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1281><div class="card-body p-3 small">For languages without natural word boundaries, like <a href=https://en.wikipedia.org/wiki/Japanese_language>Japanese</a> and <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a>, <a href=https://en.wikipedia.org/wiki/Word_segmentation>word segmentation</a> is a prerequisite for downstream analysis. For <a href=https://en.wikipedia.org/wiki/Japanese_language>Japanese</a>, segmentation is often done jointly with <a href=https://en.wikipedia.org/wiki/Part-of-speech_tagging>part of speech tagging</a>, and this process is usually referred to as <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphological analysis</a>. Morphological analyzers are trained on data hand-annotated with segmentation boundaries and <a href=https://en.wikipedia.org/wiki/Tag_(metadata)>part of speech tags</a>. A segmentation dictionary or character n-gram information is also provided as additional inputs to the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>. Incorporating this extra information makes <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> large. Modern neural morphological analyzers can consume gigabytes of <a href=https://en.wikipedia.org/wiki/Computer_memory>memory</a>. We propose a compact alternative to these cumbersome approaches which do not rely on any externally provided n-gram or word representations. The model uses only unigram character embeddings, encodes them using either stacked bi-LSTM or a self-attention network, and independently infers both segmentation and part of speech information. The <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is trained in an end-to-end and semi-supervised fashion, on labels produced by a state-of-the-art analyzer. We demonstrate that the proposed technique rivals performance of a previous dictionary-based state-of-the-art approach and can even surpass it when training with the combination of human-annotated and automatically-annotated data. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> itself is significantly smaller than the dictionary-based one : it uses less than 15 megabytes of space.</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1128.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1128 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1128 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=C18-1128" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/C18-1128/>Cross-lingual Knowledge Projection Using <a href=https://en.wikipedia.org/wiki/Machine_translation>Machine Translation</a> and Target-side Knowledge Base Completion</a></strong><br><a href=/people/n/naoki-otani/>Naoki Otani</a>
|
<a href=/people/h/hirokazu-kiyomaru/>Hirokazu Kiyomaru</a>
|
<a href=/people/d/daisuke-kawahara/>Daisuke Kawahara</a>
|
<a href=/people/s/sadao-kurohashi/>Sadao Kurohashi</a><br><a href=/volumes/C18-1/ class=text-muted>Proceedings of the 27th International Conference on Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1128><div class="card-body p-3 small">Considerable effort has been devoted to building <a href=https://en.wikipedia.org/wiki/Commonsense_knowledge_base>commonsense knowledge bases</a>. However, <a href=https://en.wikipedia.org/wiki/Copula_(linguistics)>they</a> are not available in many languages because the <a href=https://en.wikipedia.org/wiki/Copula_(linguistics)>construction of KBs</a> is expensive. To bridge the gap between languages, this paper addresses the problem of projecting the knowledge in <a href=https://en.wikipedia.org/wiki/English_language>English</a>, a resource-rich language, into other languages, where the main challenge lies in projection ambiguity. This <a href=https://en.wikipedia.org/wiki/Ambiguity>ambiguity</a> is partially solved by <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> and target-side knowledge base completion, but neither of them is adequately reliable by itself. We show their combination can project English commonsense knowledge into <a href=https://en.wikipedia.org/wiki/Japanese_language>Japanese</a> and <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a> with high precision. Our method also achieves a top-10 accuracy of 90 % on the crowdsourced EnglishJapanese benchmark. Furthermore, we use our method to obtain 18,747 facts of accurate Japanese commonsense within a very short period.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-2010.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-2010 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-2010 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-2010" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D18-2010/>Juman++ : A Morphological Analysis Toolkit for Scriptio Continua<span class=acl-fixed-case>J</span>uman++: A Morphological Analysis Toolkit for Scriptio Continua</a></strong><br><a href=/people/a/arseny-tolmachev/>Arseny Tolmachev</a>
|
<a href=/people/d/daisuke-kawahara/>Daisuke Kawahara</a>
|
<a href=/people/s/sadao-kurohashi/>Sadao Kurohashi</a><br><a href=/volumes/D18-2/ class=text-muted>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-2010><div class="card-body p-3 small">We present a three-part toolkit for developing <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphological analyzers</a> for languages without natural word boundaries. The first part is a C++11/14 lattice-based morphological analysis library that uses a combination of linear and recurrent neural net language models for analysis. The other parts are a tool for exposing problems in the trained <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> and a partial annotation tool. Our morphological analyzer of <a href=https://en.wikipedia.org/wiki/Japanese_language>Japanese</a> achieves new SOTA on Jumandic-based corpora while being 250 times faster than the previous one. We also perform a small experiment and quantitive analysis and experience of using <a href=https://en.wikipedia.org/wiki/Programming_tool>development tools</a>. All <a href=https://en.wikipedia.org/wiki/Component-based_software_engineering>components</a> of the <a href=https://en.wikipedia.org/wiki/List_of_toolkits>toolkit</a> is open source and available under a permissive Apache 2 License.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2041.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2041 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2041 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-2041/>Knowledge-Enriched Two-Layered Attention Network for <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>Sentiment Analysis</a></a></strong><br><a href=/people/a/abhishek-kumar/>Abhishek Kumar</a>
|
<a href=/people/d/daisuke-kawahara/>Daisuke Kawahara</a>
|
<a href=/people/s/sadao-kurohashi/>Sadao Kurohashi</a><br><a href=/volumes/N18-2/ class=text-muted>Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2041><div class="card-body p-3 small">We propose a novel two-layered attention network based on Bidirectional Long Short-Term Memory for <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a>. The novel two-layered attention network takes advantage of the <a href=https://en.wikipedia.org/wiki/Knowledge_base>external knowledge bases</a> to improve the <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment prediction</a>. It uses the Knowledge Graph Embedding generated using the <a href=https://en.wikipedia.org/wiki/WordNet>WordNet</a>. We build our model by combining the two-layered attention network with the supervised model based on Support Vector Regression using a Multilayer Perceptron network for <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a>. We evaluate our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> on the benchmark dataset of SemEval 2017 Task 5. Experimental results show that the proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> surpasses the top system of SemEval 2017 Task 5. The <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> performs significantly better by improving the state-of-the-art system at SemEval 2017 Task 5 by 1.7 and 3.7 points for sub-tracks 1 and 2 respectively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1044.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1044 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1044 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1044.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-1044/>Neural Adversarial Training for Semi-supervised Japanese Predicate-argument Structure Analysis<span class=acl-fixed-case>J</span>apanese Predicate-argument Structure Analysis</a></strong><br><a href=/people/s/shuhei-kurita/>Shuhei Kurita</a>
|
<a href=/people/d/daisuke-kawahara/>Daisuke Kawahara</a>
|
<a href=/people/s/sadao-kurohashi/>Sadao Kurohashi</a><br><a href=/volumes/P18-1/ class=text-muted>Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1044><div class="card-body p-3 small">Japanese predicate-argument structure (PAS) analysis involves zero anaphora resolution, which is notoriously difficult. To improve the performance of Japanese PAS analysis, it is straightforward to increase the size of corpora annotated with <a href=https://en.wikipedia.org/wiki/Partially_ordered_set>PAS</a>. However, since <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> is prohibitively expensive, it is promising to take advantage of a large amount of raw corpora. In this paper, we propose a novel Japanese PAS analysis model based on semi-supervised adversarial training with a <a href=https://en.wikipedia.org/wiki/Text_corpus>raw corpus</a>. In our experiments, our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms existing state-of-the-art models for Japanese PAS analysis.</div></div><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-6301.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-6301 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-6301 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-6301/>Automatically Acquired Lexical Knowledge Improves Japanese Joint Morphological and Dependency Analysis<span class=acl-fixed-case>J</span>apanese Joint Morphological and Dependency Analysis</a></strong><br><a href=/people/d/daisuke-kawahara/>Daisuke Kawahara</a>
|
<a href=/people/y/yuta-hayashibe/>Yuta Hayashibe</a>
|
<a href=/people/h/hajime-morita/>Hajime Morita</a>
|
<a href=/people/s/sadao-kurohashi/>Sadao Kurohashi</a><br><a href=/volumes/W17-63/ class=text-muted>Proceedings of the 15th International Conference on Parsing Technologies</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-6301><div class="card-body p-3 small">This paper presents a joint model for morphological and dependency analysis based on automatically acquired lexical knowledge. This model takes advantage of rich lexical knowledge to simultaneously resolve <a href=https://en.wikipedia.org/wiki/Word_segmentation>word segmentation</a>, POS, and dependency ambiguities. In our experiments on <a href=https://en.wikipedia.org/wiki/Japanese_language>Japanese</a>, we show the effectiveness of our joint model over conventional pipeline models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1054.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1054 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1054 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1054/>Improving Chinese Semantic Role Labeling using High-quality Surface and Deep Case Frames<span class=acl-fixed-case>C</span>hinese Semantic Role Labeling using High-quality Surface and Deep Case Frames</a></strong><br><a href=/people/g/gongye-jin/>Gongye Jin</a>
|
<a href=/people/d/daisuke-kawahara/>Daisuke Kawahara</a>
|
<a href=/people/s/sadao-kurohashi/>Sadao Kurohashi</a><br><a href=/volumes/E17-1/ class=text-muted>Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1054><div class="card-body p-3 small">This paper presents a method for applying automatically acquired knowledge to semantic role labeling (SRL). We use a large amount of automatically extracted knowledge to improve the performance of SRL. We present two varieties of <a href=https://en.wikipedia.org/wiki/Knowledge>knowledge</a>, which we call surface case frames and deep case frames. Although the surface case frames are compiled from syntactic parses and can be used as rich syntactic knowledge, they have limited capability for resolving semantic ambiguity. To compensate the deficiency of the <a href=https://en.wikipedia.org/wiki/Frame_(artificial_intelligence)>surface case frames</a>, we compile <a href=https://en.wikipedia.org/wiki/Frame_(artificial_intelligence)>deep case frames</a> from automatic semantic roles. We also consider <a href=https://en.wikipedia.org/wiki/Quality_management>quality management</a> for both types of <a href=https://en.wikipedia.org/wiki/Knowledge>knowledge</a> in order to get rid of the noise brought from the automatic analyses. The experimental results show that Chinese SRL can be improved using automatically acquired knowledge and the <a href=https://en.wikipedia.org/wiki/Quality_management>quality management</a> shows a positive effect on this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Daisuke+Kawahara" title="Search for 'Daisuke Kawahara' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/s/sadao-kurohashi/ class=align-middle>Sadao Kurohashi</a>
<span class="badge badge-secondary align-middle ml-2">11</span></li><li class=list-group-item><a href=/people/h/hirokazu-kiyomaru/ class=align-middle>Hirokazu Kiyomaru</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/a/arseny-tolmachev/ class=align-middle>Arseny Tolmachev</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/n/naoki-otani/ class=align-middle>Naoki Otani</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yuta-hayashibe/ class=align-middle>Yuta Hayashibe</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/h/hajime-morita/ class=align-middle>Hajime Morita</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/n/norio-takahashi/ class=align-middle>Norio Takahashi</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/tomohide-shibata/ class=align-middle>Tomohide Shibata</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/kazumasa-omura/ class=align-middle>Kazumasa Omura</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yugo-murawaki/ class=align-middle>Yugo Murawaki</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/tatsuya-ide/ class=align-middle>Tatsuya Ide</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/n/nobuhiro-ueda/ class=align-middle>Nobuhiro Ueda</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/abhishek-kumar/ class=align-middle>Abhishek Kumar</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/ritsuko-iwai/ class=align-middle>Ritsuko Iwai</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/takatsune-kumada/ class=align-middle>Takatsune Kumada</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/g/gongye-jin/ class=align-middle>Gongye Jin</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/shuhei-kurita/ class=align-middle>Shuhei Kurita</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/emnlp/ class=align-middle>EMNLP</a><span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/venues/naacl/ class=align-middle>NAACL</a><span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/venues/coling/ class=align-middle>COLING</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/ws/ class=align-middle>WS</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/lrec/ class=align-middle>LREC</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-venues aria-expanded=false aria-controls=more-venues>show all...</li><div class="collapse border-top" id=more-venues><li class=list-group-item><a href=/venues/eacl/ class=align-middle>EACL</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/acl/ class=align-middle>ACL</a><span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright Â©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>