<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Dan Klein - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Dan</span> <span class=font-weight-bold>Klein</span></h2><hr><div class=row><div class=col-lg-9><h4>2022</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.acl-long.220.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--acl-long--220 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.acl-long.220 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><span class="align-middle mr-1" data-toggle=tooltip data-placement=bottom title="Best Paper"><i class="fas fa-award"></i></span><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.acl-long.220" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.acl-long.220/>Learned Incremental Representations for <a href=https://en.wikipedia.org/wiki/Parsing>Parsing</a></a></strong><br><a href=/people/n/nikita-kitaev/>Nikita Kitaev</a>
|
<a href=/people/t/thomas-lu/>Thomas Lu</a>
|
<a href=/people/d/dan-klein/>Dan Klein</a><br><a href=/volumes/2022.acl-long/ class=text-muted>Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--acl-long--220><div class="card-body p-3 small">We present an incremental syntactic representation that consists of assigning a single discrete label to each word in a sentence where the label is predicted using strictly incremental processing of a prefix of the sentence and the sequence of labels for a sentence fully determines a <a href=https://en.wikipedia.org/wiki/Parse_tree>parse tree</a> Our goal is to induce a syntactic representation that commits to syntactic choices only as they are incrementally revealed by the input in contrast with standard representations that must make output choices such as attachments speculatively and later throw out conflicting analyses Our learned <a href=https://en.wikipedia.org/wiki/Representation_(mathematics)>representations</a> achieve 93.72 F1 on the <a href=https://en.wikipedia.org/wiki/Penn_Treebank>Penn Treebank</a> with as few as bits per word and at bits per word they achieve 94.97 F1 which is comparable with other state of the art parsing models when using the same pre trained embeddings We also provide an analysis of the representations learned by our <a href=https://en.wikipedia.org/wiki/System>system</a> investigating properties such as the interpretable syntactic features captured by the <a href=https://en.wikipedia.org/wiki/System>system</a> and mechanisms for deferred resolution of syntactic ambiguities</div></div><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.284.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--284 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.284 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.284/>Value-Agnostic Conversational Semantic Parsing</a></strong><br><a href=/people/e/emmanouil-antonios-platanios/>Emmanouil Antonios Platanios</a>
|
<a href=/people/a/adam-pauls/>Adam Pauls</a>
|
<a href=/people/s/subhro-roy/>Subhro Roy</a>
|
<a href=/people/y/yuchen-zhang/>Yuchen Zhang</a>
|
<a href=/people/a/alexander-kyte/>Alexander Kyte</a>
|
<a href=/people/a/alan-guo/>Alan Guo</a>
|
<a href=/people/s/sam-thomson/>Sam Thomson</a>
|
<a href=/people/j/jayant-krishnamurthy/>Jayant Krishnamurthy</a>
|
<a href=/people/j/jason-wolfe/>Jason Wolfe</a>
|
<a href=/people/j/jacob-andreas/>Jacob Andreas</a>
|
<a href=/people/d/dan-klein/>Dan Klein</a><br><a href=/volumes/2021.acl-long/ class=text-muted>Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--284><div class="card-body p-3 small">Conversational semantic parsers map user utterances to executable programs given dialogue histories composed of previous utterances, <a href=https://en.wikipedia.org/wiki/Computer_program>programs</a>, and system responses. Existing <a href=https://en.wikipedia.org/wiki/Parsing>parsers</a> typically condition on rich representations of history that include the complete set of values and computations previously discussed. We propose a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> that abstracts over values to focus <a href=https://en.wikipedia.org/wiki/Prediction>prediction</a> on type- and function-level context. This approach provides a compact encoding of dialogue histories and predicted programs, improving <a href=https://en.wikipedia.org/wiki/Generalization>generalization</a> and computational efficiency. Our model incorporates several other components, including an atomic span copy operation and structural enforcement of well-formedness constraints on predicted programs, that are particularly advantageous in the low-data regime. Trained on the SMCalFlow and TreeDST datasets, our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms prior <a href=https://en.wikipedia.org/wiki/Work_(physics)>work</a> by 7.3 % and 10.6 % respectively in terms of <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>absolute accuracy</a>. Trained on only a thousand examples from each <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>, <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> outperforms strong <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a> by 12.4 % and 6.4 %. These results indicate that simple <a href=https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning>representations</a> are key to effective <a href=https://en.wikipedia.org/wiki/Generalization>generalization</a> in conversational semantic parsing.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.163.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--163 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.163 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.163" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.163/>Reference-Centric Models for Grounded Collaborative Dialogue</a></strong><br><a href=/people/d/daniel-fried/>Daniel Fried</a>
|
<a href=/people/j/justin-chiu/>Justin Chiu</a>
|
<a href=/people/d/dan-klein/>Dan Klein</a><br><a href=/volumes/2021.emnlp-main/ class=text-muted>Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--163><div class="card-body p-3 small">We present a grounded neural dialogue model that successfully collaborates with people in a partially-observable reference game. We focus on a setting where two agents each observe an overlapping part of a world context and need to identify and agree on some object they share. Therefore, the agents should pool their information and communicate pragmatically to solve the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. Our dialogue agent accurately grounds referents from the partner&#8217;s utterances using a structured reference resolver, conditions on these referents using a recurrent memory, and uses a pragmatic generation procedure to ensure the partner can resolve the references the agent produces. We evaluate on the OneCommon spatial grounding dialogue task (Udagawa and Aizawa 2019), involving a number of dots arranged on a board with continuously varying positions, sizes, and shades. Our agent substantially outperforms the previous <a href=https://en.wikipedia.org/wiki/State_of_the_art>state of the art</a> for the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>, obtaining a 20 % relative improvement in successful task completion in self-play evaluations and a 50 % relative improvement in success in human evaluations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.190.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--190 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.190 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.naacl-main.190" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.190/>Detoxifying Language Models Risks Marginalizing Minority Voices</a></strong><br><a href=/people/a/albert-xu/>Albert Xu</a>
|
<a href=/people/e/eshaan-pathak/>Eshaan Pathak</a>
|
<a href=/people/e/eric-wallace/>Eric Wallace</a>
|
<a href=/people/s/suchin-gururangan/>Suchin Gururangan</a>
|
<a href=/people/m/maarten-sap/>Maarten Sap</a>
|
<a href=/people/d/dan-klein/>Dan Klein</a><br><a href=/volumes/2021.naacl-main/ class=text-muted>Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--190><div class="card-body p-3 small">Language models (LMs) must be both safe and equitable to be responsibly deployed in practice. With safety in mind, numerous detoxification techniques (e.g., Dathathri et al. 2020 ; Krause et al. 2020) have been proposed to mitigate toxic LM generations. In this work, we show that these detoxification techniques hurt equity : they decrease the utility of LMs on language used by marginalized groups (e.g., African-American English and minority identity mentions). In particular, we perform automatic and human evaluations of text generation quality when LMs are conditioned on inputs with different <a href=https://en.wikipedia.org/wiki/Dialect>dialects</a> and group identifiers. We find that detoxification makes LMs more brittle to distribution shift, especially on language used by <a href=https://en.wikipedia.org/wiki/Social_exclusion>marginalized groups</a>. We identify that these failures stem from <a href=https://en.wikipedia.org/wiki/Detoxification_(alternative_medicine)>detoxification methods</a> exploiting spurious correlations in toxicity datasets. Overall, our results highlight the tension between the <a href=https://en.wikipedia.org/wiki/Controllability>controllability</a> and distributional robustness of LMs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.373.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--373 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.373 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.373/>Constructing Taxonomies from Pretrained Language Models</a></strong><br><a href=/people/c/catherine-chen/>Catherine Chen</a>
|
<a href=/people/k/kevin-lin/>Kevin Lin</a>
|
<a href=/people/d/dan-klein/>Dan Klein</a><br><a href=/volumes/2021.naacl-main/ class=text-muted>Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--373><div class="card-body p-3 small">We present a <a href=https://en.wikipedia.org/wiki/Methodology>method</a> for constructing <a href=https://en.wikipedia.org/wiki/Taxonomy_(biology)>taxonomic trees</a> (e.g., WordNet) using pretrained language models. Our approach is composed of two <a href=https://en.wikipedia.org/wiki/Modular_programming>modules</a>, one that predicts parenthood relations and another that reconciles those pairwise predictions into <a href=https://en.wikipedia.org/wiki/Tree_(data_structure)>trees</a>. The parenthood prediction module produces likelihood scores for each potential parent-child pair, creating a graph of parent-child relation scores. The tree reconciliation module treats the task as a <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph optimization problem</a> and outputs the maximum spanning tree of this <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph</a>. We train our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> on subtrees sampled from <a href=https://en.wikipedia.org/wiki/WordNet>WordNet</a>, and test on nonoverlapping WordNet subtrees. We show that incorporating web-retrieved glosses can further improve performance. On the task of constructing subtrees of <a href=https://en.wikipedia.org/wiki/WordNet>English WordNet</a>, the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieves 66.7 ancestor F1, a 20.0 % relative increase over the previous best published result on this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. In addition, we convert the original English dataset into nine other languages using Open Multilingual WordNet and extend our results across these languages.</div></div><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.366.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--366 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.366 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939191 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.366" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.366/>A Streaming Approach For Efficient Batched Beam Search</a></strong><br><a href=/people/k/kevin-yang/>Kevin Yang</a>
|
<a href=/people/v/violet-yao/>Violet Yao</a>
|
<a href=/people/j/john-denero/>John DeNero</a>
|
<a href=/people/d/dan-klein/>Dan Klein</a><br><a href=/volumes/2020.emnlp-main/ class=text-muted>Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--366><div class="card-body p-3 small">We propose an efficient <a href=https://en.wikipedia.org/wiki/Batch_processing>batching strategy</a> for variable-length decoding on <a href=https://en.wikipedia.org/wiki/Graphics_processing_unit>GPU architectures</a>. During decoding, when candidates terminate or are pruned according to <a href=https://en.wikipedia.org/wiki/Heuristic>heuristics</a>, our streaming approach periodically refills the batch before proceeding with a selected subset of candidates. We apply our method to variable-width beam search on a state-of-the-art machine translation model. Our method decreases <a href=https://en.wikipedia.org/wiki/Run_time_(program_lifecycle_phase)>runtime</a> by up to 71 % compared to a fixed-width beam search baseline and 17 % compared to a variable-width baseline, while matching baselines&#8217; BLEU. Finally, experiments show that our <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>method</a> can speed up decoding in other domains, such as <a href=https://en.wikipedia.org/wiki/Semantic_parsing>semantic and syntactic parsing</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.389.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--389 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.389 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938920 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.389/>Unsupervised Parsing via Constituency Tests</a></strong><br><a href=/people/s/steven-cao/>Steven Cao</a>
|
<a href=/people/n/nikita-kitaev/>Nikita Kitaev</a>
|
<a href=/people/d/dan-klein/>Dan Klein</a><br><a href=/volumes/2020.emnlp-main/ class=text-muted>Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--389><div class="card-body p-3 small">We propose a method for unsupervised parsing based on the linguistic notion of a constituency test. One type of constituency test involves modifying the sentence via some <a href=https://en.wikipedia.org/wiki/Transformation_(function)>transformation</a> (e.g. replacing the span with a pronoun) and then judging the result (e.g. checking if it is grammatical). Motivated by this idea, we design an unsupervised parser by specifying a set of <a href=https://en.wikipedia.org/wiki/Transformation_(function)>transformations</a> and using an unsupervised neural acceptability model to make grammaticality decisions. To produce a <a href=https://en.wikipedia.org/wiki/Tree_(graph_theory)>tree</a> given a sentence, we score each span by aggregating its constituency test judgments, and we choose the binary tree with the highest total score. While this approach already achieves performance in the range of current methods, we further improve accuracy by fine-tuning the grammaticality model through a refinement procedure, where we alternate between improving the estimated trees and improving the grammaticality model. The refined <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieves 62.8 F1 on the Penn Treebank test set, an absolute improvement of 7.6 points over the previously best published result.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.acl-main.208.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--acl-main--208 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.acl-main.208 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.acl-main.208.Software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=http://slideslive.com/38929435 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.acl-main.208" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.acl-main.208/>Semantic Scaffolds for Pseudocode-to-Code Generation</a></strong><br><a href=/people/r/ruiqi-zhong/>Ruiqi Zhong</a>
|
<a href=/people/m/mitchell-stern/>Mitchell Stern</a>
|
<a href=/people/d/dan-klein/>Dan Klein</a><br><a href=/volumes/2020.acl-main/ class=text-muted>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--acl-main--208><div class="card-body p-3 small">We propose a method for program generation based on semantic scaffolds, lightweight structures representing the high-level semantic and syntactic composition of a program. By first searching over plausible scaffolds then using these as constraints for a beam search over programs, we achieve better coverage of the search space when compared with existing techniques. We apply our hierarchical search method to the SPoC dataset for pseudocode-to-code generation, in which we are given line-level natural language pseudocode annotations and aim to produce a program satisfying execution-based test cases. By using semantic scaffolds during <a href=https://en.wikipedia.org/wiki/Inference>inference</a>, we achieve a 10 % absolute improvement in top-100 accuracy over the previous <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a>. Additionally, we require only 11 candidates to reach the top-3000 performance of the previous best approach when tested against unseen problems, demonstrating a substantial improvement in efficiency.</div></div><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1225.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1225 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1225 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1225" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1225/>A Deep Factorization of Style and Structure in Fonts</a></strong><br><a href=/people/n/nikita-srivatsan/>Nikita Srivatsan</a>
|
<a href=/people/j/jonathan-barron/>Jonathan Barron</a>
|
<a href=/people/d/dan-klein/>Dan Klein</a>
|
<a href=/people/t/taylor-berg-kirkpatrick/>Taylor Berg-Kirkpatrick</a><br><a href=/volumes/D19-1/ class=text-muted>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1225><div class="card-body p-3 small">We propose a deep factorization model for typographic analysis that disentangles content from style. Specifically, a variational inference procedure factors each training glyph into the combination of a character-specific content embedding and a latent font-specific style variable. The underlying <a href=https://en.wikipedia.org/wiki/Generative_model>generative model</a> combines these factors through an asymmetric transpose convolutional process to generate the image of the glyph itself. When trained on corpora of fonts, our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> learns a manifold over font styles that can be used to analyze or reconstruct new, unseen fonts. On the task of reconstructing missing glyphs from an unknown font given only a small number of observations, our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms both a strong nearest neighbors baseline and a state-of-the-art discriminative model from prior work.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1031.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1031 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1031 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/385244938 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1031" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1031/>Cross-Domain Generalization of Neural Constituency Parsers</a></strong><br><a href=/people/d/daniel-fried/>Daniel Fried</a>
|
<a href=/people/n/nikita-kitaev/>Nikita Kitaev</a>
|
<a href=/people/d/dan-klein/>Dan Klein</a><br><a href=/volumes/P19-1/ class=text-muted>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1031><div class="card-body p-3 small">Neural parsers obtain state-of-the-art results on benchmark treebanks for constituency parsingbut to what degree do they generalize to other domains? We present three results about the generalization of neural parsers in a zero-shot setting : training on trees from one corpus and evaluating on out-of-domain corpora. First, neural and non-neural parsers generalize comparably to new domains. Second, incorporating pre-trained encoder representations into neural parsers substantially improves their performance across all domains, but does not give a larger relative improvement for out-of-domain treebanks. Finally, despite the rich input representations they learn, neural parsers still benefit from structured output prediction of output trees, yielding higher exact match accuracy and stronger generalization both to larger text spans and to out-of-domain corpora. We analyze generalization on English and Chinese corpora, and in the process obtain state-of-the-art parsing results for the Brown, Genia, and English Web treebanks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1340.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1340 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1340 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/384782901 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1340" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1340/>Multilingual Constituency Parsing with Self-Attention and Pre-Training</a></strong><br><a href=/people/n/nikita-kitaev/>Nikita Kitaev</a>
|
<a href=/people/s/steven-cao/>Steven Cao</a>
|
<a href=/people/d/dan-klein/>Dan Klein</a><br><a href=/volumes/P19-1/ class=text-muted>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1340><div class="card-body p-3 small">We show that constituency parsing benefits from unsupervised pre-training across a variety of languages and a range of pre-training conditions. We first compare the benefits of no pre-training, <a href=https://en.wikipedia.org/wiki/FastText>fastText</a>, ELMo, and BERT for English and find that BERT outperforms ELMo, in large part due to increased model capacity, whereas ELMo in turn outperforms the non-contextual fastText embeddings. We also find that pre-training is beneficial across all 11 languages tested ; however, large model sizes (more than 100 million parameters) make it computationally expensive to train separate models for each language. To address this shortcoming, we show that joint multilingual pre-training and fine-tuning allows sharing all but a small number of parameters between ten languages in the final <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a>. The 10x reduction in model size compared to fine-tuning one model per language causes only a 3.2 % relative error increase in aggregate. We further explore the idea of joint fine-tuning and show that it gives low-resource languages a way to benefit from the larger datasets of other languages. Finally, we demonstrate new state-of-the-art results for 11 <a href=https://en.wikipedia.org/wiki/Language>languages</a>, including <a href=https://en.wikipedia.org/wiki/English_language>English</a> (95.8 F1) and <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a> (91.8 F1).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1655.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1655 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1655 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P19-1655.Supplementary.pdf data-toggle=tooltip data-placement=top title=Supplementary><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1655/>Are You Looking? Grounding to Multiple Modalities in Vision-and-Language Navigation</a></strong><br><a href=/people/r/ronghang-hu/>Ronghang Hu</a>
|
<a href=/people/d/daniel-fried/>Daniel Fried</a>
|
<a href=/people/a/anna-rohrbach/>Anna Rohrbach</a>
|
<a href=/people/d/dan-klein/>Dan Klein</a>
|
<a href=/people/t/trevor-darrell/>Trevor Darrell</a>
|
<a href=/people/k/kate-saenko/>Kate Saenko</a><br><a href=/volumes/P19-1/ class=text-muted>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1655><div class="card-body p-3 small">Vision-and-Language Navigation (VLN) requires grounding instructions, such as turn right and stop at the door, to routes in a visual environment. The actual grounding can connect language to the environment through multiple <a href=https://en.wikipedia.org/wiki/Linguistic_modality>modalities</a>, e.g. stop at the door might ground into visual objects, while turn right might rely only on the geometric structure of a route. We investigate where the <a href=https://en.wikipedia.org/wiki/Natural_language>natural language</a> empirically grounds under two recent state-of-the-art VLN models. Surprisingly, we discover that visual features may actually hurt these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> : <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> which only use route structure, ablating visual features, outperform their visual counterparts in unseen new environments on the benchmark Room-to-Room dataset. To better use all the available modalities, we propose to decompose the grounding procedure into a set of expert models with access to different modalities (including object detections) and ensemble them at prediction time, improving the performance of state-of-the-art models on the VLN task.</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1177.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1177 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1177 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://vimeo.com/277672896 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N18-1177" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/N18-1177/>Unified Pragmatic Models for Generating and Following Instructions</a></strong><br><a href=/people/d/daniel-fried/>Daniel Fried</a>
|
<a href=/people/j/jacob-andreas/>Jacob Andreas</a>
|
<a href=/people/d/dan-klein/>Dan Klein</a><br><a href=/volumes/N18-1/ class=text-muted>Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1177><div class="card-body p-3 small">We show that explicit pragmatic inference aids in correctly generating and following <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language instructions</a> for complex, sequential tasks. Our pragmatics-enabled models reason about why speakers produce certain instructions, and about how listeners will react upon hearing them. Like previous pragmatic models, we use learned base listener and speaker models to build a pragmatic speaker that uses the base listener to simulate the interpretation of candidate descriptions, and a pragmatic listener that reasons counterfactually about alternative descriptions. We extend these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> to tasks with <a href=https://en.wikipedia.org/wiki/Sequential_analysis>sequential structure</a>. Evaluation of <a href=https://en.wikipedia.org/wiki/Language_generation>language generation</a> and <a href=https://en.wikipedia.org/wiki/Language_interpretation>interpretation</a> shows that pragmatic inference improves state-of-the-art listener models (at correctly interpreting human instructions) and speaker models (at producing instructions correctly interpreted by humans) in diverse settings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2075.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2075 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2075 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2075.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2075.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285804205 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-2075/>Policy Gradient as a Proxy for Dynamic Oracles in Constituency Parsing</a></strong><br><a href=/people/d/daniel-fried/>Daniel Fried</a>
|
<a href=/people/d/dan-klein/>Dan Klein</a><br><a href=/volumes/P18-2/ class=text-muted>Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2075><div class="card-body p-3 small">Dynamic oracles provide strong supervision for training constituency parsers with exploration, but must be custom defined for a given parser&#8217;s transition system. We explore using a <a href=https://en.wikipedia.org/wiki/Policy_gradient_method>policy gradient method</a> as a parser-agnostic alternative. In addition to directly optimizing for a tree-level metric such as F1, policy gradient has the potential to reduce <a href=https://en.wikipedia.org/wiki/Exposure_bias>exposure bias</a> by allowing exploration during training ; moreover, it does not require a dynamic oracle for supervision. On four constituency parsers in three languages, the <a href=https://en.wikipedia.org/wiki/Numerical_methods_for_ordinary_differential_equations>method</a> substantially outperforms static oracle likelihood training in almost all settings. For parsers where a dynamic oracle is available (including a novel <a href=https://en.wikipedia.org/wiki/Oracle_machine>oracle</a> which we define for the transition system of Dyer et al., 2016), policy gradient typically recaptures a substantial fraction of the performance gain afforded by the dynamic oracle.</div></div><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1022.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1022 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1022 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P17-1022.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234954361 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P17-1022" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-1022/>Translating Neuralese</a></strong><br><a href=/people/j/jacob-andreas/>Jacob Andreas</a>
|
<a href=/people/a/anca-dragan/>Anca Dragan</a>
|
<a href=/people/d/dan-klein/>Dan Klein</a><br><a href=/volumes/P17-1/ class=text-muted>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1022><div class="card-body p-3 small">Several approaches have recently been proposed for learning decentralized deep multiagent policies that coordinate via a differentiable communication channel. While these <a href=https://en.wikipedia.org/wiki/Policy>policies</a> are effective for many <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a>, interpretation of their induced communication strategies has remained a challenge. Here we propose to interpret agents&#8217; messages by translating them. Unlike in typical machine translation problems, we have no parallel data to learn from. Instead we develop a translation model based on the insight that <a href=https://en.wikipedia.org/wiki/Agent-based_model>agent messages</a> and <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language strings</a> mean the same thing if they induce the same belief about the world in a listener. We present theoretical guarantees and empirical evidence that our approach preserves both the <a href=https://en.wikipedia.org/wiki/Semantics>semantics</a> and pragmatics of messages by ensuring that players communicating through a translation layer do not suffer a substantial loss in reward relative to players with a common language.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1076.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1076 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1076 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234957467 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-1076/>A Minimal Span-Based Neural Constituency Parser</a></strong><br><a href=/people/m/mitchell-stern/>Mitchell Stern</a>
|
<a href=/people/j/jacob-andreas/>Jacob Andreas</a>
|
<a href=/people/d/dan-klein/>Dan Klein</a><br><a href=/volumes/P17-1/ class=text-muted>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1076><div class="card-body p-3 small">In this work, we present a minimal neural model for constituency parsing based on independent scoring of labels and spans. We show that this model is not only compatible with classical <a href=https://en.wikipedia.org/wiki/Dynamic_programming>dynamic programming techniques</a>, but also admits a novel greedy top-down inference algorithm based on recursive partitioning of the input. We demonstrate empirically that both prediction schemes are competitive with recent work, and when combined with basic extensions to the scoring model are capable of achieving state-of-the-art single-model performance on the Penn Treebank (91.79 F1) and strong performance on the French Treebank (82.23 F1).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2025.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2025 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2025 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234957130 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-2025/>Improving Neural Parsing by Disentangling Model Combination and Reranking Effects</a></strong><br><a href=/people/d/daniel-fried/>Daniel Fried</a>
|
<a href=/people/m/mitchell-stern/>Mitchell Stern</a>
|
<a href=/people/d/dan-klein/>Dan Klein</a><br><a href=/volumes/P17-2/ class=text-muted>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2025><div class="card-body p-3 small">Recent work has proposed several generative neural models for constituency parsing that achieve state-of-the-art results. Since direct search in these generative models is difficult, they have primarily been used to rescore candidate outputs from base parsers in which decoding is more straightforward. We first present an <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> for direct search in these <a href=https://en.wikipedia.org/wiki/Generative_model>generative models</a>. We then demonstrate that the rescoring results are at least partly due to implicit model combination rather than reranking effects. Finally, we show that explicit model combination can improve performance even further, resulting in new state-of-the-art numbers on the PTB of 94.25 F1 when training only on gold data and 94.66 F1 when using external data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2052.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2052 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2052 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-2052/>Fine-Grained Entity Typing with High-Multiplicity Assignments</a></strong><br><a href=/people/m/maxim-rabinovich/>Maxim Rabinovich</a>
|
<a href=/people/d/dan-klein/>Dan Klein</a><br><a href=/volumes/P17-2/ class=text-muted>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2052><div class="card-body p-3 small">As entity type systems become richer and more fine-grained, we expect the number of types assigned to a given entity to increase. However, most fine-grained typing work has focused on datasets that exhibit a low degree of type multiplicity. In this paper, we consider the high-multiplicity regime inherent in data sources such as <a href=https://en.wikipedia.org/wiki/Wikipedia>Wikipedia</a> that have semi-open type systems. We introduce a set-prediction approach to this problem and show that our model outperforms unstructured baselines on a new Wikipedia-based fine-grained typing corpus.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1015.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1015 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1015 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/238230308 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D17-1015" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/D17-1015/>Where is Misty? Interpreting Spatial Descriptors by Modeling Regions in Space</a></strong><br><a href=/people/n/nikita-kitaev/>Nikita Kitaev</a>
|
<a href=/people/d/dan-klein/>Dan Klein</a><br><a href=/volumes/D17-1/ class=text-muted>Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1015><div class="card-body p-3 small">We present a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> for locating regions in space based on <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language descriptions</a>. Starting with a 3D scene and a sentence, our model is able to associate words in the sentence with regions in the scene, interpret relations such as &#8216;on top of&#8217; or &#8216;next to,&#8217; and finally locate the region described in the sentence. All <a href=https://en.wikipedia.org/wiki/Component-based_software_engineering>components</a> form a single <a href=https://en.wikipedia.org/wiki/Neural_network>neural network</a> that is trained end-to-end without prior knowledge of object segmentation. To evaluate our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>, we construct and release a new <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> consisting of Minecraft scenes with <a href=https://en.wikipedia.org/wiki/Crowdsourcing>crowdsourced natural language descriptions</a>. We achieve a 32 % relative error reduction compared to a strong neural baseline.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1178.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1178 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1178 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1178/>Effective Inference for Generative Neural Parsing</a></strong><br><a href=/people/m/mitchell-stern/>Mitchell Stern</a>
|
<a href=/people/d/daniel-fried/>Daniel Fried</a>
|
<a href=/people/d/dan-klein/>Dan Klein</a><br><a href=/volumes/D17-1/ class=text-muted>Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1178><div class="card-body p-3 small">Generative neural models have recently achieved state-of-the-art results for constituency parsing. However, without a feasible search procedure, their use has so far been limited to reranking the output of external parsers in which decoding is more tractable. We describe an alternative to the conventional action-level beam search used for discriminative neural models that enables us to decode directly in these <a href=https://en.wikipedia.org/wiki/Generative_model>generative models</a>. We then show that by improving our basic candidate selection strategy and using a coarse pruning function, we can improve <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> while exploring significantly less of the <a href=https://en.wikipedia.org/wiki/Feasible_region>search space</a>. Applied to the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> of Choe and Charniak (2016), our <a href=https://en.wikipedia.org/wiki/Statistical_inference>inference procedure</a> obtains 92.56 F1 on section 23 of the <a href=https://en.wikipedia.org/wiki/Penn_Treebank>Penn Treebank</a>, surpassing prior state-of-the-art results for single-model systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1311.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1311 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1311 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/238231647 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D17-1311" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/D17-1311/>Analogs of Linguistic Structure in Deep Representations</a></strong><br><a href=/people/j/jacob-andreas/>Jacob Andreas</a>
|
<a href=/people/d/dan-klein/>Dan Klein</a><br><a href=/volumes/D17-1/ class=text-muted>Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1311><div class="card-body p-3 small">We investigate the compositional structure of message vectors computed by a <a href=https://en.wikipedia.org/wiki/Deep_learning>deep network</a> trained on a communication game. By comparing truth-conditional representations of encoder-produced message vectors to human-produced referring expressions, we are able to identify aligned (vector, utterance) pairs with the same meaning. We then search for structured relationships among these aligned pairs to discover simple vector space transformations corresponding to <a href=https://en.wikipedia.org/wiki/Negation>negation</a>, <a href=https://en.wikipedia.org/wiki/Logical_conjunction>conjunction</a>, and <a href=https://en.wikipedia.org/wiki/Logical_disjunction>disjunction</a>. Our results suggest that neural representations are capable of spontaneously developing a <a href=https://en.wikipedia.org/wiki/Syntax>syntax</a> with functional analogues to qualitative properties of <a href=https://en.wikipedia.org/wiki/Natural_language>natural language</a>.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Dan+Klein" title="Search for 'Dan Klein' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/d/daniel-fried/ class=align-middle>Daniel Fried</a>
<span class="badge badge-secondary align-middle ml-2">7</span></li><li class=list-group-item><a href=/people/j/jacob-andreas/ class=align-middle>Jacob Andreas</a>
<span class="badge badge-secondary align-middle ml-2">5</span></li><li class=list-group-item><a href=/people/n/nikita-kitaev/ class=align-middle>Nikita Kitaev</a>
<span class="badge badge-secondary align-middle ml-2">5</span></li><li class=list-group-item><a href=/people/m/mitchell-stern/ class=align-middle>Mitchell Stern</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/people/s/steven-cao/ class=align-middle>Steven Cao</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/e/emmanouil-antonios-platanios/ class=align-middle>Emmanouil Antonios Platanios</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/adam-pauls/ class=align-middle>Adam Pauls</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/subhro-roy/ class=align-middle>Subhro Roy</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yuchen-zhang/ class=align-middle>Yuchen Zhang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/alexander-kyte/ class=align-middle>Alexander Kyte</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/alan-guo/ class=align-middle>Alan Guo</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/sam-thomson/ class=align-middle>Sam Thomson</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jayant-krishnamurthy/ class=align-middle>Jayant Krishnamurthy</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jason-wolfe/ class=align-middle>Jason Wolfe</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/kevin-yang/ class=align-middle>Kevin Yang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/v/violet-yao/ class=align-middle>Violet Yao</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/john-denero/ class=align-middle>John DeNero</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/ruiqi-zhong/ class=align-middle>Ruiqi Zhong</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/anca-dragan/ class=align-middle>Anca Dragan</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/maxim-rabinovich/ class=align-middle>Maxim Rabinovich</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/thomas-lu/ class=align-middle>Thomas Lu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/justin-chiu/ class=align-middle>Justin Chiu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/n/nikita-srivatsan/ class=align-middle>Nikita Srivatsan</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jonathan-barron/ class=align-middle>Jonathan Barron</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/taylor-berg-kirkpatrick/ class=align-middle>Taylor Berg-Kirkpatrick</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/albert-xu/ class=align-middle>Albert Xu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/e/eshaan-pathak/ class=align-middle>Eshaan Pathak</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/e/eric-wallace/ class=align-middle>Eric Wallace</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/suchin-gururangan/ class=align-middle>Suchin Gururangan</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/maarten-sap/ class=align-middle>Maarten Sap</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/catherine-chen/ class=align-middle>Catherine Chen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/kevin-lin/ class=align-middle>Kevin Lin</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/ronghang-hu/ class=align-middle>Ronghang Hu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/anna-rohrbach/ class=align-middle>Anna Rohrbach</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/trevor-darrell/ class=align-middle>Trevor Darrell</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/kate-saenko/ class=align-middle>Kate Saenko</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/acl/ class=align-middle>ACL</a><span class="badge badge-secondary align-middle ml-2">11</span></li><li class=list-group-item><a href=/venues/emnlp/ class=align-middle>EMNLP</a><span class="badge badge-secondary align-middle ml-2">7</span></li><li class=list-group-item><a href=/venues/naacl/ class=align-middle>NAACL</a><span class="badge badge-secondary align-middle ml-2">3</span></li></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright &nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>