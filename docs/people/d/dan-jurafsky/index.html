<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Dan Jurafsky - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Dan</span> <span class=font-weight-bold>Jurafsky</span></h2><hr><div class=row><div class=col-lg-9><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.416.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--416 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.416 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.naacl-main.416" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.416/>Improving Factual Completeness and Consistency of Image-to-Text Radiology Report Generation</a></strong><br><a href=/people/y/yasuhide-miura/>Yasuhide Miura</a>
|
<a href=/people/y/yuhao-zhang/>Yuhao Zhang</a>
|
<a href=/people/e/emily-tsai/>Emily Tsai</a>
|
<a href=/people/c/curtis-langlotz/>Curtis Langlotz</a>
|
<a href=/people/d/dan-jurafsky/>Dan Jurafsky</a><br><a href=/volumes/2021.naacl-main/ class=text-muted>Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--416><div class="card-body p-3 small">Neural image-to-text radiology report generation systems offer the potential to improve radiology reporting by reducing the repetitive process of report drafting and identifying possible <a href=https://en.wikipedia.org/wiki/Medical_error>medical errors</a>. However, existing report generation systems, despite achieving high performances on natural language generation metrics such as CIDEr or <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a>, still suffer from incomplete and inconsistent generations. Here we introduce two new simple rewards to encourage the generation of factually complete and consistent radiology reports : one that encourages the system to generate radiology domain entities consistent with the reference, and one that uses natural language inference to encourage these entities to be described in inferentially consistent ways. We combine these with the novel use of an existing semantic equivalence metric (BERTScore). We further propose a report generation system that optimizes these <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>rewards</a> via <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning</a>. On two open radiology report datasets, our <a href=https://en.wikipedia.org/wiki/System>system</a> substantially improved the F1 score of a clinical information extraction performance by +22.1 (Delta +63.9 %). We further show via a <a href=https://en.wikipedia.org/wiki/Evaluation>human evaluation</a> and a <a href=https://en.wikipedia.org/wiki/Qualitative_property>qualitative analysis</a> that our <a href=https://en.wikipedia.org/wiki/System>system</a> leads to generations that are more factually complete and consistent compared to the <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a>.</div></div><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.393.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--393 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.393 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938914 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.393/>Utility is in the Eye of the User : A Critique of NLP Leaderboards<span class=acl-fixed-case>NLP</span> Leaderboards</a></strong><br><a href=/people/k/kawin-ethayarajh/>Kawin Ethayarajh</a>
|
<a href=/people/d/dan-jurafsky/>Dan Jurafsky</a><br><a href=/volumes/2020.emnlp-main/ class=text-muted>Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--393><div class="card-body p-3 small">Benchmarks such as GLUE have helped drive advances in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> by incentivizing the creation of more accurate models. While this leaderboard paradigm has been remarkably successful, a historical focus on performance-based evaluation has been at the expense of other qualities that the NLP community values in models, such as compactness, fairness, and <a href=https://en.wikipedia.org/wiki/Efficient_energy_use>energy efficiency</a>. In this opinion paper, we study the divergence between what is incentivized by leaderboards and what is useful in practice through the lens of <a href=https://en.wikipedia.org/wiki/Microeconomics>microeconomic theory</a>. We frame both the leaderboard and NLP practitioners as consumers and the benefit they get from a <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> as its utility to them. With this framing, we formalize how leaderboards in their current form can be poor proxies for the NLP community at large. For example, a highly inefficient model would provide less utility to practitioners but not to a <a href=https://en.wikipedia.org/wiki/Glossary_of_economics>leaderboard</a>, since it is a cost that only the former must bear. To allow practitioners to better estimate a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>&#8217;s utility to them, we advocate for more transparency on leaderboards, such as the reporting of statistics that are of practical concern (e.g., <a href=https://en.wikipedia.org/wiki/Mathematical_model>model size</a>, <a href=https://en.wikipedia.org/wiki/Efficient_energy_use>energy efficiency</a>, and inference latency).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.acl-main.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.acl-main.0/>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</a></strong><br><a href=/people/d/dan-jurafsky/>Dan Jurafsky</a>
|
<a href=/people/j/joyce-chai/>Joyce Chai</a>
|
<a href=/people/n/natalie-schluter/>Natalie Schluter</a>
|
<a href=/people/j/joel-tetreault/>Joel Tetreault</a><br><a href=/volumes/2020.acl-main/ class=text-muted>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.acl-main.439.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--acl-main--439 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.acl-main.439 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://slideslive.com/38928972 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.acl-main.439" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.acl-main.439/>Pretraining with Contrastive Sentence Objectives Improves <a href=https://en.wikipedia.org/wiki/Discourse>Discourse</a> Performance of Language Models</a></strong><br><a href=/people/d/dan-iter/>Dan Iter</a>
|
<a href=/people/k/kelvin-guu/>Kelvin Guu</a>
|
<a href=/people/l/larry-lansing/>Larry Lansing</a>
|
<a href=/people/d/dan-jurafsky/>Dan Jurafsky</a><br><a href=/volumes/2020.acl-main/ class=text-muted>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--acl-main--439><div class="card-body p-3 small">Recent models for unsupervised representation learning of text have employed a number of techniques to improve contextual word representations but have put little focus on discourse-level representations. We propose Conpono, an inter-sentence objective for pretraining language models that models discourse coherence and the distance between sentences. Given an anchor sentence, our model is trained to predict the text k sentences away using a sampled-softmax objective where the candidates consist of neighboring sentences and sentences randomly sampled from the corpus. On the discourse representation benchmark DiscoEval, our model improves over the previous <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a> by up to 13 % and on average 4 % absolute across 7 tasks. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is the same size as BERT-Base, but outperforms the much larger BERT-Large model and other more recent approaches that incorporate <a href=https://en.wikipedia.org/wiki/Discourse>discourse</a>. We also show that Conpono yields gains of 2%-6 % absolute even for tasks that do not explicitly evaluate discourse : textual entailment (RTE), common sense reasoning (COPA) and reading comprehension (ReCoRD).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.acl-main.486.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--acl-main--486 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.acl-main.486 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.acl-main.486.Dataset.tgz data-toggle=tooltip data-placement=top title=Dataset><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=http://slideslive.com/38928840 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.acl-main.486/>Social Bias Frames : Reasoning about Social and Power Implications of Language</a></strong><br><a href=/people/m/maarten-sap/>Maarten Sap</a>
|
<a href=/people/s/saadia-gabriel/>Saadia Gabriel</a>
|
<a href=/people/l/lianhui-qin/>Lianhui Qin</a>
|
<a href=/people/d/dan-jurafsky/>Dan Jurafsky</a>
|
<a href=/people/n/noah-a-smith/>Noah A. Smith</a>
|
<a href=/people/y/yejin-choi/>Yejin Choi</a><br><a href=/volumes/2020.acl-main/ class=text-muted>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--acl-main--486><div class="card-body p-3 small">Warning : this paper contains content that may be offensive or upsetting. Language has the power to reinforce <a href=https://en.wikipedia.org/wiki/Stereotype>stereotypes</a> and project <a href=https://en.wikipedia.org/wiki/Bias>social biases</a> onto others. At the core of the challenge is that it is rarely what is stated explicitly, but rather the implied meanings, that frame people&#8217;s judgments about others. For example, given a statement that we should n&#8217;t lower our standards to hire more women, most listeners will infer the implicature intended by the speaker-that women (candidates) are less qualified. Most semantic formalisms, to date, do not capture such pragmatic implications in which people express social biases and power differentials in language. We introduce Social Bias Frames, a new conceptual formalism that aims to model the pragmatic frames in which people project social biases and stereotypes onto others. In addition, we introduce the Social Bias Inference Corpus to support large-scale modelling and evaluation with 150k structured annotations of social media posts, covering over 34k implications about a thousand demographic groups. We then establish baseline approaches that learn to recover Social Bias Frames from <a href=https://en.wikipedia.org/wiki/Unstructured_data>unstructured text</a>. We find that while state-of-the-art neural models are effective at high-level categorization of whether a given statement projects unwanted social bias (80 % F1), they are not effective at spelling out more detailed explanations in terms of Social Bias Frames. Our study motivates future work that combines structured pragmatic inference with <a href=https://en.wikipedia.org/wiki/Commonsense_reasoning>commonsense reasoning</a> on social implications.</div></div><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1304.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1304 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1304 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/359689303 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1304" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1304/>Analyzing Polarization in <a href=https://en.wikipedia.org/wiki/Social_media>Social Media</a> : Method and Application to <a href=https://en.wikipedia.org/wiki/Twitter>Tweets</a> on 21 Mass Shootings</a></strong><br><a href=/people/d/dorottya-demszky/>Dorottya Demszky</a>
|
<a href=/people/n/nikhil-garg/>Nikhil Garg</a>
|
<a href=/people/r/rob-voigt/>Rob Voigt</a>
|
<a href=/people/j/james-zou/>James Zou</a>
|
<a href=/people/j/jesse-shapiro/>Jesse Shapiro</a>
|
<a href=/people/m/matthew-gentzkow/>Matthew Gentzkow</a>
|
<a href=/people/d/dan-jurafsky/>Dan Jurafsky</a><br><a href=/volumes/N19-1/ class=text-muted>Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1304><div class="card-body p-3 small">We provide an NLP framework to uncover four linguistic dimensions of <a href=https://en.wikipedia.org/wiki/Political_polarization>political polarization</a> in <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> : topic choice, <a href=https://en.wikipedia.org/wiki/Framing_(social_sciences)>framing</a>, <a href=https://en.wikipedia.org/wiki/Affect_(psychology)>affect</a> and <a href=https://en.wikipedia.org/wiki/Illocutionary_force>illocutionary force</a>. We quantify these aspects with existing lexical methods, and propose clustering of tweet embeddings as a means to identify salient topics for analysis across events ; human evaluations show that our approach generates more cohesive topics than traditional LDA-based models. We apply our <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> to study 4.4 M tweets on 21 <a href=https://en.wikipedia.org/wiki/Mass_shooting>mass shootings</a>. We provide evidence that the discussion of these events is highly polarized politically and that this polarization is primarily driven by partisan differences in <a href=https://en.wikipedia.org/wiki/Framing_(social_sciences)>framing</a> rather than topic choice. We identify <a href=https://en.wikipedia.org/wiki/Framing_(social_sciences)>framing devices</a>, such as grounding and the contrasting use of the terms terrorist and crazy, that contribute to <a href=https://en.wikipedia.org/wiki/Political_polarization>polarization</a>. Results pertaining to topic choice, <a href=https://en.wikipedia.org/wiki/Affect_(psychology)>affect</a> and illocutionary force suggest that Republicans focus more on the shooter and event-specific facts (news) while Democrats focus more on the victims and call for policy changes. Our work contributes to a deeper understanding of the way group divisions manifest in language and to <a href=https://en.wikipedia.org/wiki/Computational_linguistics>computational methods</a> for studying them.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1364.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1364 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1364 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/356153695 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1364/>Let’s Make Your Request More Persuasive : Modeling Persuasive Strategies via Semi-Supervised Neural Nets on Crowdfunding Platforms</a></strong><br><a href=/people/d/diyi-yang/>Diyi Yang</a>
|
<a href=/people/j/jiaao-chen/>Jiaao Chen</a>
|
<a href=/people/z/zichao-yang/>Zichao Yang</a>
|
<a href=/people/d/dan-jurafsky/>Dan Jurafsky</a>
|
<a href=/people/e/eduard-hovy/>Eduard Hovy</a><br><a href=/volumes/N19-1/ class=text-muted>Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1364><div class="card-body p-3 small">Modeling what makes a request persuasive-eliciting the desired response from a reader-is critical to the study of <a href=https://en.wikipedia.org/wiki/Propaganda>propaganda</a>, <a href=https://en.wikipedia.org/wiki/Behavioral_economics>behavioral economics</a>, and <a href=https://en.wikipedia.org/wiki/Advertising>advertising</a>. Yet current <a href=https://en.wikipedia.org/wiki/Scientific_modelling>models</a> ca n&#8217;t quantify the persuasiveness of requests or extract successful persuasive strategies. Building on theories of <a href=https://en.wikipedia.org/wiki/Persuasion>persuasion</a>, we propose a <a href=https://en.wikipedia.org/wiki/Neural_network>neural network</a> to quantify persuasiveness and identify the persuasive strategies in advocacy requests. Our semi-supervised hierarchical neural network model is supervised by the number of people persuaded to take actions and partially supervised at the sentence level with human-labeled rhetorical strategies. Our method outperforms several baselines, uncovers persuasive strategies-offering increased interpretability of persuasive speech-and has applications for other situations with document-level supervision but only partial sentence supervision.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1365.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1365 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1365 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/356167288 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1365/>Recursive Routing Networks : Learning to Compose Modules for Language Understanding</a></strong><br><a href=/people/i/ignacio-cases/>Ignacio Cases</a>
|
<a href=/people/c/clemens-rosenbaum/>Clemens Rosenbaum</a>
|
<a href=/people/m/matthew-riemer/>Matthew Riemer</a>
|
<a href=/people/a/atticus-geiger/>Atticus Geiger</a>
|
<a href=/people/t/tim-klinger/>Tim Klinger</a>
|
<a href=/people/a/alex-tamkin/>Alex Tamkin</a>
|
<a href=/people/o/olivia-li/>Olivia Li</a>
|
<a href=/people/s/sandhini-agarwal/>Sandhini Agarwal</a>
|
<a href=/people/j/joshua-d-greene/>Joshua D. Greene</a>
|
<a href=/people/d/dan-jurafsky/>Dan Jurafsky</a>
|
<a href=/people/c/christopher-potts/>Christopher Potts</a>
|
<a href=/people/l/lauri-karttunen/>Lauri Karttunen</a><br><a href=/volumes/N19-1/ class=text-muted>Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1365><div class="card-body p-3 small">We introduce Recursive Routing Networks (RRNs), which are modular, adaptable models that learn effectively in diverse environments. RRNs consist of a set of <a href=https://en.wikipedia.org/wiki/Subroutine>functions</a>, typically organized into a <a href=https://en.wikipedia.org/wiki/Grid_(spatial_index)>grid</a>, and a meta-learner decision-making component called the <a href=https://en.wikipedia.org/wiki/Router_(computing)>router</a>. The <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> jointly optimizes the parameters of the <a href=https://en.wikipedia.org/wiki/Function_(mathematics)>functions</a> and the meta-learner&#8217;s policy for routing inputs through those <a href=https://en.wikipedia.org/wiki/Function_(mathematics)>functions</a>. RRNs can be incorporated into existing architectures in a number of ways ; we explore adding them to word representation layers, recurrent network hidden layers, and classifier layers. Our evaluation task is natural language inference (NLI). Using the MultiNLI corpus, we show that an RRN&#8217;s routing decisions reflect the high-level genre structure of that <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a>. To show that RRNs can learn to specialize to more fine-grained semantic distinctions, we introduce a new corpus of NLI examples involving implicative predicates, and show that the model components become fine-tuned to the inferential signatures that are characteristic of these <a href=https://en.wikipedia.org/wiki/Predicate_(grammar)>predicates</a>.</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1393.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1393 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1393 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D18-1393.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1393/>Framing and Agenda-setting in Russian News : a Computational Analysis of Intricate Political Strategies<span class=acl-fixed-case>R</span>ussian News: a Computational Analysis of Intricate Political Strategies</a></strong><br><a href=/people/a/anjalie-field/>Anjalie Field</a>
|
<a href=/people/d/doron-kliger/>Doron Kliger</a>
|
<a href=/people/s/shuly-wintner/>Shuly Wintner</a>
|
<a href=/people/j/jennifer-pan/>Jennifer Pan</a>
|
<a href=/people/d/dan-jurafsky/>Dan Jurafsky</a>
|
<a href=/people/y/yulia-tsvetkov/>Yulia Tsvetkov</a><br><a href=/volumes/D18-1/ class=text-muted>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1393><div class="card-body p-3 small">Amidst growing concern over <a href=https://en.wikipedia.org/wiki/Media_manipulation>media manipulation</a>, NLP attention has focused on overt strategies like <a href=https://en.wikipedia.org/wiki/Censorship>censorship</a> and <a href=https://en.wikipedia.org/wiki/Fake_news>fake news</a>. Here, we draw on two concepts from political science literature to explore subtler strategies for government media manipulation : <a href=https://en.wikipedia.org/wiki/Agenda-setting_theory>agenda-setting</a> (selecting what topics to cover) and <a href=https://en.wikipedia.org/wiki/Framing_(social_sciences)>framing</a> (deciding how topics are covered). We analyze 13 years (100 K articles) of the <a href=https://en.wikipedia.org/wiki/List_of_newspapers_in_Russia>Russian newspaper</a> Izvestia and identify a strategy of distraction : articles mention the U.S. more frequently in the month directly following an economic downturn in Russia. We introduce embedding-based methods for cross-lingually projecting English frames to <a href=https://en.wikipedia.org/wiki/Russian_language>Russian</a>, and discover that these articles emphasize U.S. moral failings and threats to the U.S. Our work offers new ways to identify subtle media manipulation strategies at the intersection of agenda-setting and framing.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/Q18-1033.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-Q18-1033 data-toggle=collapse aria-expanded=false aria-controls=abstract-Q18-1033 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285803587 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/Q18-1033/>Detecting Institutional Dialog Acts in Police Traffic Stops</a></strong><br><a href=/people/v/vinodkumar-prabhakaran/>Vinodkumar Prabhakaran</a>
|
<a href=/people/c/camilla-griffiths/>Camilla Griffiths</a>
|
<a href=/people/h/hang-su/>Hang Su</a>
|
<a href=/people/p/prateek-verma/>Prateek Verma</a>
|
<a href=/people/n/nelson-morgan/>Nelson Morgan</a>
|
<a href=/people/j/jennifer-l-eberhardt/>Jennifer L. Eberhardt</a>
|
<a href=/people/d/dan-jurafsky/>Dan Jurafsky</a><br><a href=/volumes/Q18-1/ class=text-muted>Transactions of the Association for Computational Linguistics, Volume 6</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-Q18-1033><div class="card-body p-3 small">We apply computational dialog methods to police body-worn camera footage to model conversations between police officers and community members in <a href=https://en.wikipedia.org/wiki/Traffic_stop>traffic stops</a>. Relying on the theory of institutional talk, we develop a labeling scheme for police speech during traffic stops, and a tagger to detect institutional dialog acts (Reasons, Searches, Offering Help) from transcribed text at the turn (78 % F-score) and stop (89 % F-score) level. We then develop speech recognition and segmentation algorithms to detect these acts at the stop level from raw camera audio (81 % F-score, with even higher accuracy for crucial acts like conveying the reason for the stop). We demonstrate that the dialog structures produced by our tagger could reveal whether officers follow law enforcement norms like introducing themselves, explaining the reason for the stop, and asking permission for searches. This work may therefore inform and aid efforts to ensure the procedural justice of police-community interactions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0615.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0615 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0615 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0615/>Automatic Detection of Incoherent Speech for Diagnosing Schizophrenia</a></strong><br><a href=/people/d/dan-iter/>Dan Iter</a>
|
<a href=/people/j/jong-yoon/>Jong Yoon</a>
|
<a href=/people/d/dan-jurafsky/>Dan Jurafsky</a><br><a href=/volumes/W18-06/ class=text-muted>Proceedings of the Fifth Workshop on Computational Linguistics and Clinical Psychology: From Keyboard to Clinic</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0615><div class="card-body p-3 small">Schizophrenia is a mental disorder which afflicts an estimated 0.7 % of adults world wide. It affects many areas of <a href=https://en.wikipedia.org/wiki/Cognition>mental function</a>, often evident from <a href=https://en.wikipedia.org/wiki/Speech_disorder>incoherent speech</a>. Diagnosing schizophrenia relies on <a href=https://en.wikipedia.org/wiki/Subjectivity>subjective judgments</a> resulting in disagreements even among trained clinicians. Recent studies have proposed the use of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a> for diagnosis by drawing on automatically-extracted linguistic features like discourse coherence and <a href=https://en.wikipedia.org/wiki/Lexicon>lexicon</a>. Here, we present the first benchmark comparison of previously proposed coherence models for detecting symptoms of schizophrenia and evaluate their performance on a new dataset of recorded interviews between subjects and clinicians. We also present two alternative coherence metrics based on modern sentence embedding techniques that outperform the previous methods on our dataset. Lastly, we propose a novel <a href=https://en.wikipedia.org/wiki/Computational_model>computational model</a> for reference incoherence based on ambiguous pronoun usage and show that it is a highly predictive feature on our <a href=https://en.wikipedia.org/wiki/Data>data</a>. While the number of subjects is limited in this pilot study, our results suggest new directions for diagnosing common symptoms of schizophrenia.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1057.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1057 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1057 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://vimeo.com/276371501 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N18-1057/>Noising and Denoising Natural Language : Diverse Backtranslation for Grammar Correction</a></strong><br><a href=/people/z/ziang-xie/>Ziang Xie</a>
|
<a href=/people/g/guillaume-genthial/>Guillaume Genthial</a>
|
<a href=/people/s/stanley-xie/>Stanley Xie</a>
|
<a href=/people/a/andrew-y-ng/>Andrew Ng</a>
|
<a href=/people/d/dan-jurafsky/>Dan Jurafsky</a><br><a href=/volumes/N18-1/ class=text-muted>Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1057><div class="card-body p-3 small">Translation-based methods for grammar correction that directly map noisy, ungrammatical text to their clean counterparts are able to correct a broad range of errors ; however, such techniques are bottlenecked by the need for a large parallel corpus of noisy and clean sentence pairs. In this paper, we consider synthesizing <a href=https://en.wikipedia.org/wiki/Parallel_computing>parallel data</a> by noising a clean monolingual corpus. While most previous approaches introduce perturbations using <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> computed from local context windows, we instead develop error generation processes using a neural sequence transduction model trained to translate clean examples to their noisy counterparts. Given a corpus of clean examples, we propose beam search noising procedures to synthesize additional noisy examples that human evaluators were nearly unable to discriminate from nonsynthesized examples. Surprisingly, when trained on additional <a href=https://en.wikipedia.org/wiki/Data>data</a> synthesized using our best-performing noising scheme, our model approaches the same performance as when trained on additional nonsynthesized data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1146.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1146 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1146 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-1146/>Deconfounded Lexicon Induction for Interpretable Social Science</a></strong><br><a href=/people/r/reid-pryzant/>Reid Pryzant</a>
|
<a href=/people/k/kelly-shen/>Kelly Shen</a>
|
<a href=/people/d/dan-jurafsky/>Dan Jurafsky</a>
|
<a href=/people/s/stefan-wagner/>Stefan Wagner</a><br><a href=/volumes/N18-1/ class=text-muted>Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1146><div class="card-body p-3 small">NLP algorithms are increasingly used in <a href=https://en.wikipedia.org/wiki/Computational_social_science>computational social science</a> to take <a href=https://en.wikipedia.org/wiki/Linguistic_description>linguistic observations</a> and predict outcomes like <a href=https://en.wikipedia.org/wiki/Preference>human preferences</a> or actions. Making these <a href=https://en.wikipedia.org/wiki/Social_model>social models</a> transparent and interpretable often requires identifying <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> in the input that predict outcomes while also controlling for potential <a href=https://en.wikipedia.org/wiki/Confounding>confounds</a>. We formalize this need as a new task : inducing a lexicon that is predictive of a set of target variables yet uncorrelated to a set of confounding variables. We introduce two <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning algorithms</a> for the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. The first uses a bifurcated architecture to separate the explanatory power of the text and <a href=https://en.wikipedia.org/wiki/Confounding>confounds</a>. The second uses an adversarial discriminator to force confound-invariant text encodings. Both elicit <a href=https://en.wikipedia.org/wiki/Lexicon>lexicons</a> from learned weights and <a href=https://en.wikipedia.org/wiki/Attentional_control>attentional scores</a>. We use them to induce lexicons that are predictive of timely responses to consumer complaints (controlling for product), enrollment from course descriptions (controlling for subject), and sales from product descriptions (controlling for seller). In each domain our algorithms pick words that are associated with narrative persuasion ; more predictive and less confound-related than those of standard feature weighting and lexicon induction techniques like <a href=https://en.wikipedia.org/wiki/Regression_analysis>regression</a> and log odds.<i>narrative persuasion</i>; more\n predictive and less confound-related than those of standard\n feature weighting and lexicon induction techniques like\n regression and log odds.\n</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1027.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1027 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1027 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1027.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1027.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-1027" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P18-1027/>Sharp Nearby, Fuzzy Far Away : How Neural Language Models Use Context</a></strong><br><a href=/people/u/urvashi-khandelwal/>Urvashi Khandelwal</a>
|
<a href=/people/h/he-he/>He He</a>
|
<a href=/people/p/peng-qi/>Peng Qi</a>
|
<a href=/people/d/dan-jurafsky/>Dan Jurafsky</a><br><a href=/volumes/P18-1/ class=text-muted>Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1027><div class="card-body p-3 small">We know very little about how neural language models (LM) use <a href=https://en.wikipedia.org/wiki/Context_(language_use)>prior linguistic context</a>. In this paper, we investigate the role of <a href=https://en.wikipedia.org/wiki/Context_(language_use)>context</a> in an LSTM LM, through <a href=https://en.wikipedia.org/wiki/Ablation>ablation studies</a>. Specifically, we analyze the increase in <a href=https://en.wikipedia.org/wiki/Perplexity>perplexity</a> when prior context words are shuffled, replaced, or dropped. On two standard datasets, <a href=https://en.wikipedia.org/wiki/Penn_Treebank>Penn Treebank</a> and WikiText-2, we find that the model is capable of using about 200 tokens of context on average, but sharply distinguishes nearby context (recent 50 tokens) from the distant history. The model is highly sensitive to the order of words within the most recent sentence, but ignores word order in the long-range context (beyond 50 tokens), suggesting the distant past is modeled only as a rough semantic field or topic. We further find that the neural caching model (Grave et al., 2017b) especially helps the LSTM to copy words from within this distant context. Overall, our analysis not only provides a better understanding of how neural LMs use their context, but also sheds light on recent success from cache-based models.</div></div><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2009.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2009 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2009 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P17-2009.Notes.zip data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234952193 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-2009/>Incorporating Dialectal Variability for Socially Equitable Language Identification</a></strong><br><a href=/people/d/david-jurgens/>David Jurgens</a>
|
<a href=/people/y/yulia-tsvetkov/>Yulia Tsvetkov</a>
|
<a href=/people/d/dan-jurafsky/>Dan Jurafsky</a><br><a href=/volumes/P17-2/ class=text-muted>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2009><div class="card-body p-3 small">Language identification (LID) is a critical first step for processing <a href=https://en.wikipedia.org/wiki/Multilingualism>multilingual text</a>. Yet most LID systems are not designed to handle the linguistic diversity of global platforms like <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a>, where local dialects and rampant code-switching lead language classifiers to systematically miss minority dialect speakers and multilingual speakers. We propose a new dataset and a character-based sequence-to-sequence model for LID designed to support dialectal and multilingual language varieties. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieves state-of-the-art performance on multiple LID benchmarks. Furthermore, in a case study using <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a> for health tracking, our method substantially increases the availability of texts written by underrepresented populations, enabling the development of socially inclusive NLP tools.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1019.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1019 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1019 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/238234840 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D17-1019/>Neural Net Models of Open-domain Discourse Coherence</a></strong><br><a href=/people/j/jiwei-li/>Jiwei Li</a>
|
<a href=/people/d/dan-jurafsky/>Dan Jurafsky</a><br><a href=/volumes/D17-1/ class=text-muted>Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1019><div class="card-body p-3 small">Discourse coherence is strongly associated with text quality, making it important to <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language generation and understanding</a>. Yet existing models of coherence focus on measuring individual aspects of <a href=https://en.wikipedia.org/wiki/Coherence_(linguistics)>coherence</a> (lexical overlap, rhetorical structure, entity centering) in narrow domains. In this paper, we describe domain-independent neural models of discourse coherence that are capable of measuring multiple aspects of <a href=https://en.wikipedia.org/wiki/Coherence_(linguistics)>coherence</a> in existing sentences and can maintain <a href=https://en.wikipedia.org/wiki/Coherence_(linguistics)>coherence</a> while generating new sentences. We study both discriminative models that learn to distinguish coherent from incoherent discourse, and generative models that produce coherent text, including a novel neural latent-variable Markovian generative model that captures the latent discourse dependencies between sentences in a text. Our work achieves state-of-the-art performance on multiple coherence evaluations, and marks an initial step in generating coherent texts given discourse contexts.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1230.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1230 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1230 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D17-1230" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D17-1230/>Adversarial Learning for Neural Dialogue Generation</a></strong><br><a href=/people/j/jiwei-li/>Jiwei Li</a>
|
<a href=/people/w/will-monroe/>Will Monroe</a>
|
<a href=/people/t/tianlin-shi/>Tianlin Shi</a>
|
<a href=/people/s/sebastien-jean/>Sébastien Jean</a>
|
<a href=/people/a/alan-ritter/>Alan Ritter</a>
|
<a href=/people/d/dan-jurafsky/>Dan Jurafsky</a><br><a href=/volumes/D17-1/ class=text-muted>Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1230><div class="card-body p-3 small">We apply adversarial training to open-domain dialogue generation, training a system to produce sequences that are indistinguishable from human-generated dialogue utterances. We cast the task as a reinforcement learning problem where we jointly train two systems : a <a href=https://en.wikipedia.org/wiki/Generative_model>generative model</a> to produce response sequences, and a discriminatoranalagous to the human evaluator in the <a href=https://en.wikipedia.org/wiki/Turing_test>Turing test</a> to distinguish between the human-generated dialogues and the machine-generated ones. In this generative adversarial network approach, the outputs from the <a href=https://en.wikipedia.org/wiki/Discriminator>discriminator</a> are used to encourage the <a href=https://en.wikipedia.org/wiki/System>system</a> towards more human-like dialogue. Further, we investigate models for adversarial evaluation that uses success in fooling an adversary as a dialogue evaluation metric, while avoiding a number of potential pitfalls. Experimental results on several metrics, including adversarial evaluation, demonstrate that the adversarially-trained system generates higher-quality responses than previous baselines</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1044.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1044 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1044 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1044/>A Two-stage Sieve Approach for Quote Attribution</a></strong><br><a href=/people/g/grace-muzny/>Grace Muzny</a>
|
<a href=/people/m/michael-fang/>Michael Fang</a>
|
<a href=/people/a/angel-chang/>Angel Chang</a>
|
<a href=/people/d/dan-jurafsky/>Dan Jurafsky</a><br><a href=/volumes/E17-1/ class=text-muted>Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1044><div class="card-body p-3 small">We present a deterministic sieve-based system for attributing quotations in literary text and a new dataset : QuoteLi3. Quote attribution, determining who said what in a given text, is important for tasks like creating dialogue systems, and in newer areas like computational literary studies, where it creates opportunities to analyze novels at scale rather than only a few at a time. We release QuoteLi3, which contains more than 6,000 annotations linking quotes to speaker mentions and quotes to speaker entities, and introduce a new <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> for quote attribution. Our two-stage algorithm first links quotes to mentions, then mentions to entities. Using two stages encapsulates difficult sub-problems and improves system performance. The modular design allows us to tune for overall performance or higher <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>precision</a>, which is useful for many real-world use cases. Our <a href=https://en.wikipedia.org/wiki/System>system</a> achieves an average <a href=https://en.wikipedia.org/wiki/F-score>F-score</a> of 87.5 across three novels, outperforming previous systems, and can be tuned for <a href=https://en.wikipedia.org/wiki/Precision_(computer_science)>precision</a> of 90.4 at a <a href=https://en.wikipedia.org/wiki/Precision_(computer_science)>recall</a> of 65.1.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Dan+Jurafsky" title="Search for 'Dan Jurafsky' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/d/dan-iter/ class=align-middle>Dan Iter</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/y/yulia-tsvetkov/ class=align-middle>Yulia Tsvetkov</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/j/jiwei-li/ class=align-middle>Jiwei Li</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/k/kawin-ethayarajh/ class=align-middle>Kawin Ethayarajh</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/joyce-chai/ class=align-middle>Joyce Chai</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/n/natalie-schluter/ class=align-middle>Natalie Schluter</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/joel-tetreault/ class=align-middle>Joel Tetreault</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/kelvin-guu/ class=align-middle>Kelvin Guu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/larry-lansing/ class=align-middle>Larry Lansing</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/maarten-sap/ class=align-middle>Maarten Sap</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/saadia-gabriel/ class=align-middle>Saadia Gabriel</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/lianhui-qin/ class=align-middle>Lianhui Qin</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/n/noah-a-smith/ class=align-middle>Noah A. Smith</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yejin-choi/ class=align-middle>Yejin Choi</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/david-jurgens/ class=align-middle>David Jurgens</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/anjalie-field/ class=align-middle>Anjalie Field</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/doron-kliger/ class=align-middle>Doron Kliger</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/shuly-wintner/ class=align-middle>Shuly Wintner</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jennifer-pan/ class=align-middle>Jennifer Pan</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/v/vinodkumar-prabhakaran/ class=align-middle>Vinodkumar Prabhakaran</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/camilla-griffiths/ class=align-middle>Camilla Griffiths</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/hang-su/ class=align-middle>Hang Su</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/p/prateek-verma/ class=align-middle>Prateek Verma</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/n/nelson-morgan/ class=align-middle>Nelson Morgan</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jennifer-l-eberhardt/ class=align-middle>Jennifer L. Eberhardt</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/w/will-monroe/ class=align-middle>Will Monroe</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/tianlin-shi/ class=align-middle>Tianlin Shi</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/sebastien-jean/ class=align-middle>Sébastien Jean</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/alan-ritter/ class=align-middle>Alan Ritter</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yasuhide-miura/ class=align-middle>Yasuhide Miura</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yuhao-zhang/ class=align-middle>Yuhao Zhang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/e/emily-tsai/ class=align-middle>Emily Tsai</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/curtis-langlotz/ class=align-middle>Curtis Langlotz</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jong-yoon/ class=align-middle>Jong Yoon</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/dorottya-demszky/ class=align-middle>Dorottya Demszky</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/n/nikhil-garg/ class=align-middle>Nikhil Garg</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/rob-voigt/ class=align-middle>Rob Voigt</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/james-zou/ class=align-middle>James Zou</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jesse-shapiro/ class=align-middle>Jesse Shapiro</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/matthew-gentzkow/ class=align-middle>Matthew Gentzkow</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/diyi-yang/ class=align-middle>Diyi Yang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jiaao-chen/ class=align-middle>Jiaao Chen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/zichao-yang/ class=align-middle>Zichao Yang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/e/eduard-hovy/ class=align-middle>Eduard Hovy</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/i/ignacio-cases/ class=align-middle>Ignacio Cases</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/clemens-rosenbaum/ class=align-middle>Clemens Rosenbaum</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/matthew-riemer/ class=align-middle>Matthew Riemer</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/atticus-geiger/ class=align-middle>Atticus Geiger</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/tim-klinger/ class=align-middle>Tim Klinger</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/alex-tamkin/ class=align-middle>Alex Tamkin</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/o/olivia-li/ class=align-middle>Olivia Li</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/sandhini-agarwal/ class=align-middle>Sandhini Agarwal</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/joshua-d-greene/ class=align-middle>Joshua D. Greene</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/christopher-potts/ class=align-middle>Christopher Potts</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/lauri-karttunen/ class=align-middle>Lauri Karttunen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/ziang-xie/ class=align-middle>Ziang Xie</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/g/guillaume-genthial/ class=align-middle>Guillaume Genthial</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/stanley-xie/ class=align-middle>Stanley Xie</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/andrew-y-ng/ class=align-middle>Andrew Y. Ng</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/reid-pryzant/ class=align-middle>Reid Pryzant</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/kelly-shen/ class=align-middle>Kelly Shen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/stefan-wagner/ class=align-middle>Stefan Wagner</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/g/grace-muzny/ class=align-middle>Grace Muzny</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/michael-fang/ class=align-middle>Michael Fang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/angel-chang/ class=align-middle>Angel Chang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/u/urvashi-khandelwal/ class=align-middle>Urvashi Khandelwal</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/he-he/ class=align-middle>He He</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/p/peng-qi/ class=align-middle>Peng Qi</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/naacl/ class=align-middle>NAACL</a><span class="badge badge-secondary align-middle ml-2">6</span></li><li class=list-group-item><a href=/venues/acl/ class=align-middle>ACL</a><span class="badge badge-secondary align-middle ml-2">5</span></li><li class=list-group-item><a href=/venues/emnlp/ class=align-middle>EMNLP</a><span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/venues/tacl/ class=align-middle>TACL</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/ws/ class=align-middle>WS</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-venues aria-expanded=false aria-controls=more-venues>show all...</li><div class="collapse border-top" id=more-venues><li class=list-group-item><a href=/venues/eacl/ class=align-middle>EACL</a><span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>