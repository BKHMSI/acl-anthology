<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Di Jin - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Di</span> <span class=font-weight-bold>Jin</span></h2><hr><div class=row><div class=col-lg-9><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.nlp4convai-1.27.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--nlp4convai-1--27 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.nlp4convai-1.27 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.nlp4convai-1.27" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.nlp4convai-1.27/>Towards Zero and Few-shot Knowledge-seeking Turn Detection in Task-orientated Dialogue Systems</a></strong><br><a href=/people/d/di-jin/>Di Jin</a>
|
<a href=/people/s/shuyang-gao/>Shuyang Gao</a>
|
<a href=/people/s/seokhwan-kim/>Seokhwan Kim</a>
|
<a href=/people/y/yang-liu-icsi/>Yang Liu</a>
|
<a href=/people/d/dilek-hakkani-tur/>Dilek Hakkani-Tur</a><br><a href=/volumes/2021.nlp4convai-1/ class=text-muted>Proceedings of the 3rd Workshop on Natural Language Processing for Conversational AI</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--nlp4convai-1--27><div class="card-body p-3 small">Most prior work on task-oriented dialogue systems is restricted to supporting domain APIs. However, users may have requests that are out of the scope of these <a href=https://en.wikipedia.org/wiki/Application_programming_interface>APIs</a>. This work focuses on identifying such <a href=https://en.wikipedia.org/wiki/User_(computing)>user requests</a>. Existing methods for this task mainly rely on fine-tuning pre-trained models on large annotated data. We propose a novel method, REDE, based on adaptive representation learning and <a href=https://en.wikipedia.org/wiki/Density_estimation>density estimation</a>. REDE can be applied to zero-shot cases, and quickly learns a high-performing detector with only a few shots by updating less than 3 K parameters. We demonstrate <a href=https://en.wikipedia.org/wiki/Rede>REDE</a>&#8217;s competitive performance on DSTC9 data and our newly collected test set.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.metanlp-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.metanlp-1.0/>Proceedings of the 1st Workshop on Meta Learning and Its Applications to Natural Language Processing</a></strong><br><a href=/people/h/hung-yi-lee/>Hung-Yi Lee</a>
|
<a href=/people/m/mitra-mohtarami/>Mitra Mohtarami</a>
|
<a href=/people/s/shang-wen-li/>Shang-Wen Li</a>
|
<a href=/people/d/di-jin/>Di Jin</a>
|
<a href=/people/m/mandy-korpusik/>Mandy Korpusik</a>
|
<a href=/people/s/shuyan-dong/>Shuyan Dong</a>
|
<a href=/people/n/ngoc-thang-vu/>Ngoc Thang Vu</a>
|
<a href=/people/d/dilek-hakkani-tur/>Dilek Hakkani-Tur</a><br><a href=/volumes/2021.metanlp-1/ class=text-muted>Proceedings of the 1st Workshop on Meta Learning and Its Applications to Natural Language Processing</a></span></p><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.acl-main.654.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--acl-main--654 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.acl-main.654 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://slideslive.com/38929127 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.acl-main.654/>Multi-source Meta Transfer for Low Resource Multiple-Choice Question Answering</a></strong><br><a href=/people/m/ming-yan/>Ming Yan</a>
|
<a href=/people/h/hao-zhang/>Hao Zhang</a>
|
<a href=/people/d/di-jin/>Di Jin</a>
|
<a href=/people/j/joey-tianyi-zhou/>Joey Tianyi Zhou</a><br><a href=/volumes/2020.acl-main/ class=text-muted>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--acl-main--654><div class="card-body p-3 small">Multiple-choice question answering (MCQA) is one of the most challenging tasks in machine reading comprehension since it requires more advanced reading comprehension skills such as <a href=https://en.wikipedia.org/wiki/Logical_reasoning>logical reasoning</a>, <a href=https://en.wikipedia.org/wiki/Automatic_summarization>summarization</a>, and <a href=https://en.wikipedia.org/wiki/Arithmetic>arithmetic operations</a>. Unfortunately, most existing MCQA datasets are small in size, which increases the difficulty of <a href=https://en.wikipedia.org/wiki/Machine_learning>model learning</a> and <a href=https://en.wikipedia.org/wiki/Generalization>generalization</a>. To address this challenge, we propose a multi-source meta transfer (MMT) for low-resource MCQA. In this <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a>, we first extend <a href=https://en.wikipedia.org/wiki/Meta_learning_(computer_science)>meta learning</a> by incorporating multiple training sources to learn a generalized feature representation across domains. To bridge the distribution gap between training sources and the target, we further introduce the meta transfer that can be integrated into the multi-source meta training. More importantly, the proposed <a href=https://en.wikipedia.org/wiki/Model-driven_architecture>MMT</a> is independent of <a href=https://en.wikipedia.org/wiki/Backbone_network>backbone language models</a>. Extensive experiments demonstrate the superiority of MMT over <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-arts</a>, and continuous improvements can be achieved on different backbone networks on both supervised and unsupervised domain adaptation settings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.611.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--611 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.611 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.611/>Augmenting NLP models using Latent Feature Interpolations<span class=acl-fixed-case>NLP</span> models using Latent Feature Interpolations</a></strong><br><a href=/people/a/amit-jindal/>Amit Jindal</a>
|
<a href=/people/a/arijit-ghosh-chowdhury/>Arijit Ghosh Chowdhury</a>
|
<a href=/people/a/aniket-didolkar/>Aniket Didolkar</a>
|
<a href=/people/d/di-jin/>Di Jin</a>
|
<a href=/people/r/ramit-sawhney/>Ramit Sawhney</a>
|
<a href=/people/r/rajiv-shah/>Rajiv Ratn Shah</a><br><a href=/volumes/2020.coling-main/ class=text-muted>Proceedings of the 28th International Conference on Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--611><div class="card-body p-3 small">Models with a large number of parameters are prone to <a href=https://en.wikipedia.org/wiki/Overfitting>over-fitting</a> and often fail to capture the underlying input distribution. We introduce Emix, a data augmentation method that uses interpolations of word embeddings and hidden layer representations to construct virtual examples. We show that Emix shows significant improvements over previously used <a href=https://en.wikipedia.org/wiki/Regularization_(mathematics)>interpolation based regularizers</a> and <a href=https://en.wikipedia.org/wiki/Data_augmentation>data augmentation techniques</a>. We also demonstrate how our proposed <a href=https://en.wikipedia.org/wiki/Methodology>method</a> is more robust to sparsification. We highlight the merits of our proposed <a href=https://en.wikipedia.org/wiki/Methodology>methodology</a> by performing thorough quantitative and qualitative assessments.</div></div><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1336.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1336 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1336 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/384782139 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1336/>Dual Adversarial Neural Transfer for Low-Resource Named Entity Recognition</a></strong><br><a href=/people/j/joey-tianyi-zhou/>Joey Tianyi Zhou</a>
|
<a href=/people/h/hao-zhang/>Hao Zhang</a>
|
<a href=/people/d/di-jin/>Di Jin</a>
|
<a href=/people/h/hongyuan-zhu/>Hongyuan Zhu</a>
|
<a href=/people/m/meng-fang/>Meng Fang</a>
|
<a href=/people/r/rick-siow-mong-goh/>Rick Siow Mong Goh</a>
|
<a href=/people/k/kenneth-kwok/>Kenneth Kwok</a><br><a href=/volumes/P19-1/ class=text-muted>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1336><div class="card-body p-3 small">We propose a new neural transfer method termed Dual Adversarial Transfer Network (DATNet) for addressing low-resource Named Entity Recognition (NER). Specifically, two variants of DATNet, i.e., DATNet-F and DATNet-P, are investigated to explore effective feature fusion between high and low resource. To address the noisy and imbalanced training data, we propose a novel Generalized Resource-Adversarial Discriminator (GRAD). Additionally, adversarial training is adopted to boost model generalization. In experiments, we examine the effects of different components in DATNet across domains and languages and show that significant improvement can be obtained especially for low-resource data, without augmenting any additional hand-crafted features and pre-trained language model.</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1046.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1046 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1046 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1046/>Implicit Discourse Relation Recognition using Neural Tensor Network with Interactive Attention and Sparse Learning</a></strong><br><a href=/people/f/fengyu-guo/>Fengyu Guo</a>
|
<a href=/people/r/ruifang-he/>Ruifang He</a>
|
<a href=/people/d/di-jin/>Di Jin</a>
|
<a href=/people/j/jianwu-dang/>Jianwu Dang</a>
|
<a href=/people/l/longbiao-wang/>Longbiao Wang</a>
|
<a href=/people/x/xiangang-li/>Xiangang Li</a><br><a href=/volumes/C18-1/ class=text-muted>Proceedings of the 27th International Conference on Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1046><div class="card-body p-3 small">Implicit discourse relation recognition aims to understand and annotate the latent relations between two discourse arguments, such as temporal, comparison, etc. Most previous methods encode two discourse arguments separately, the ones considering pair specific clues ignore the bidirectional interactions between two arguments and the sparsity of pair patterns. In this paper, we propose a novel neural Tensor network framework with Interactive Attention and Sparse Learning (TIASL) for implicit discourse relation recognition. (1) We mine the most correlated word pairs from two discourse arguments to model pair specific clues, and integrate them as interactive attention into argument representations produced by the bidirectional long short-term memory network. Meanwhile, (2) the neural tensor network with sparse constraint is proposed to explore the deeper and the more important pair patterns so as to fully recognize discourse relations. The experimental results on PDTB show that our proposed TIASL framework is effective.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1118.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1118 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1118 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1118/>Interaction-Aware Topic Model for Microblog Conversations through Network Embedding and User Attention</a></strong><br><a href=/people/r/ruifang-he/>Ruifang He</a>
|
<a href=/people/x/xuefei-zhang/>Xuefei Zhang</a>
|
<a href=/people/d/di-jin/>Di Jin</a>
|
<a href=/people/l/longbiao-wang/>Longbiao Wang</a>
|
<a href=/people/j/jianwu-dang/>Jianwu Dang</a>
|
<a href=/people/x/xiangang-li/>Xiangang Li</a><br><a href=/volumes/C18-1/ class=text-muted>Proceedings of the 27th International Conference on Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1118><div class="card-body p-3 small">Traditional <a href=https://en.wikipedia.org/wiki/Topic_model>topic models</a> are insufficient for topic extraction in <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>. The existing methods only consider text information or simultaneously model the posts and the static characteristics of <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>. They ignore that one discusses diverse topics when dynamically interacting with different people. Moreover, people who talk about the same topic have different effects on the topic. In this paper, we propose an Interaction-Aware Topic Model (IATM) for <a href=https://en.wikipedia.org/wiki/Microblogging>microblog conversations</a> by integrating network embedding and user attention. A conversation network linking users based on reposting and replying relationship is constructed to mine the dynamic user behaviours. We model dynamic interactions and user attention so as to learn interaction-aware edge embeddings with <a href=https://en.wikipedia.org/wiki/Social_environment>social context</a>. Then they are incorporated into neural variational inference for generating the more consistent topics. The experiments on three real-world datasets show that our proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is effective.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1349.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1349 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1349 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/305946571 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1349" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1349/>Hierarchical Neural Networks for Sequential Sentence Classification in Medical Scientific Abstracts</a></strong><br><a href=/people/d/di-jin/>Di Jin</a>
|
<a href=/people/p/peter-szolovits/>Peter Szolovits</a><br><a href=/volumes/D18-1/ class=text-muted>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1349><div class="card-body p-3 small">Prevalent models based on artificial neural network (ANN) for sentence classification often classify sentences in isolation without considering the context in which sentences appear. This hampers the traditional sentence classification approaches to the problem of sequential sentence classification, where <a href=https://en.wikipedia.org/wiki/Structured_prediction>structured prediction</a> is needed for better overall classification performance. In this work, we present a hierarchical sequential labeling network to make use of the <a href=https://en.wikipedia.org/wiki/Context_(language_use)>contextual information</a> within surrounding sentences to help classify the current sentence. Our model outperforms the state-of-the-art results by 2%-3 % on two benchmarking datasets for sequential sentence classification in medical scientific abstracts.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S18-1127.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S18-1127 data-toggle=collapse aria-expanded=false aria-controls=abstract-S18-1127 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/S18-1127/>MIT-MEDG at SemEval-2018 Task 7 : Semantic Relation Classification via Convolution Neural Network<span class=acl-fixed-case>MIT</span>-<span class=acl-fixed-case>MEDG</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2018 Task 7: Semantic Relation Classification via Convolution Neural Network</a></strong><br><a href=/people/d/di-jin/>Di Jin</a>
|
<a href=/people/f/franck-dernoncourt/>Franck Dernoncourt</a>
|
<a href=/people/e/elena-sergeeva/>Elena Sergeeva</a>
|
<a href=/people/m/matthew-mcdermott/>Matthew McDermott</a>
|
<a href=/people/g/geeticka-chauhan/>Geeticka Chauhan</a><br><a href=/volumes/S18-1/ class=text-muted>Proceedings of The 12th International Workshop on Semantic Evaluation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S18-1127><div class="card-body p-3 small">SemEval 2018 Task 7 tasked participants to build a <a href=https://en.wikipedia.org/wiki/System>system</a> to classify two entities within a sentence into one of the 6 possible relation types. We tested 3 classes of <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> : Linear classifiers, Long Short-Term Memory (LSTM) models, and Convolutional Neural Network (CNN) models. Ultimately, the CNN model class proved most performant, so we specialized to this <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> for our final submissions. We improved performance beyond a vanilla CNN by including a variant of negative sampling, using custom <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> learned over a corpus of ACL articles, training over corpora of both <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> 1.1 and 1.2, using reversed feature, using part of context words beyond the entity pairs and using <a href=https://en.wikipedia.org/wiki/Statistical_ensemble_(mathematical_physics)>ensemble methods</a> to improve our final predictions. We also tested attention based pooling, up-sampling, and <a href=https://en.wikipedia.org/wiki/Data_augmentation>data augmentation</a>, but none improved performance. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieved rank 6 out of 28 (macro-averaged F1-score : 72.7) in subtask 1.1, and rank 4 out of 20 (macro F1 : 80.6) in subtask 1.2.</div></div><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1069.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1069 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1069 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234955861 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-1069/>Leveraging Behavioral and Social Information for Weakly Supervised Collective Classification of Political Discourse on Twitter<span class=acl-fixed-case>T</span>witter</a></strong><br><a href=/people/k/kristen-johnson/>Kristen Johnson</a>
|
<a href=/people/d/di-jin/>Di Jin</a>
|
<a href=/people/d/dan-goldwasser/>Dan Goldwasser</a><br><a href=/volumes/P17-1/ class=text-muted>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1069><div class="card-body p-3 small">Framing is a political strategy in which politicians carefully word their statements in order to control public perception of issues. Previous works exploring <a href=https://en.wikipedia.org/wiki/Political_framing>political framing</a> typically analyze frame usage in longer texts, such as <a href=https://en.wikipedia.org/wiki/Public_speaking>congressional speeches</a>. We present a collection of weakly supervised models which harness collective classification to predict the frames used in <a href=https://en.wikipedia.org/wiki/Discourse_analysis>political discourse</a> on the <a href=https://en.wikipedia.org/wiki/Microblogging>microblogging platform</a>, <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a>. Our global probabilistic models show that by combining both lexical features of <a href=https://en.wikipedia.org/wiki/Twitter>tweets</a> and network-based behavioral features of <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a>, we are able to increase the average, <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised F1 score</a> by 21.52 points over a lexical baseline alone.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S17-2029.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S17-2029 data-toggle=collapse aria-expanded=false aria-controls=abstract-S17-2029 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/S17-2029/>PurdueNLP at SemEval-2017 Task 1 : Predicting Semantic Textual Similarity with Paraphrase and Event Embeddings<span class=acl-fixed-case>P</span>urdue<span class=acl-fixed-case>NLP</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2017 Task 1: Predicting Semantic Textual Similarity with Paraphrase and Event Embeddings</a></strong><br><a href=/people/i/i-ta-lee/>I-Ta Lee</a>
|
<a href=/people/m/mahak-goindani/>Mahak Goindani</a>
|
<a href=/people/c/chang-li/>Chang Li</a>
|
<a href=/people/d/di-jin/>Di Jin</a>
|
<a href=/people/k/kristen-johnson/>Kristen Marie Johnson</a>
|
<a href=/people/x/xiao-zhang/>Xiao Zhang</a>
|
<a href=/people/m/maria-leonor-pacheco/>Maria Leonor Pacheco</a>
|
<a href=/people/d/dan-goldwasser/>Dan Goldwasser</a><br><a href=/volumes/S17-2/ class=text-muted>Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S17-2029><div class="card-body p-3 small">This paper describes our proposed solution for SemEval 2017 Task 1 : <a href=https://en.wikipedia.org/wiki/Semantic_similarity>Semantic Textual Similarity</a> (Daniel Cer and Specia, 2017). The <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> aims at measuring the degree of equivalence between sentences given in English. Performance is evaluated by computing <a href=https://en.wikipedia.org/wiki/Pearson_correlation_coefficient>Pearson Correlation scores</a> between the predicted scores and <a href=https://en.wikipedia.org/wiki/Judgement>human judgements</a>. Our proposed <a href=https://en.wikipedia.org/wiki/System>system</a> consists of two <a href=https://en.wikipedia.org/wiki/System>subsystems</a> and one <a href=https://en.wikipedia.org/wiki/Regression_analysis>regression model</a> for predicting STS scores. The two subsystems are designed to learn Paraphrase and Event Embeddings that can take the consideration of paraphrasing characteristics and <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>sentence structures</a> into our <a href=https://en.wikipedia.org/wiki/System>system</a>. The <a href=https://en.wikipedia.org/wiki/Regression_analysis>regression model</a> associates these <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a> to make the final predictions. The experimental result shows that our <a href=https://en.wikipedia.org/wiki/Tensor_(intrinsic_definition)>system</a> acquires 0.8 of <a href=https://en.wikipedia.org/wiki/Pearson_correlation_coefficient>Pearson Correlation Scores</a> in this task.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Di+Jin" title="Search for 'Di Jin' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/r/ruifang-he/ class=align-middle>Ruifang He</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/j/jianwu-dang/ class=align-middle>Jianwu Dang</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/l/longbiao-wang/ class=align-middle>Longbiao Wang</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/x/xiangang-li/ class=align-middle>Xiangang Li</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/h/hao-zhang/ class=align-middle>Hao Zhang</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/j/joey-tianyi-zhou/ class=align-middle>Joey Tianyi Zhou</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/k/kristen-johnson/ class=align-middle>Kristen Johnson</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/d/dan-goldwasser/ class=align-middle>Dan Goldwasser</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/d/dilek-hakkani-tur/ class=align-middle>Dilek Hakkani-Tur</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/f/fengyu-guo/ class=align-middle>Fengyu Guo</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/x/xuefei-zhang/ class=align-middle>Xuefei Zhang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/ming-yan/ class=align-middle>Ming Yan</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/shuyang-gao/ class=align-middle>Shuyang Gao</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/seokhwan-kim/ class=align-middle>Seokhwan Kim</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yang-liu-icsi/ class=align-middle>Yang Liu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/p/peter-szolovits/ class=align-middle>Peter Szolovits</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/i/i-ta-lee/ class=align-middle>I-Ta Lee</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/mahak-goindani/ class=align-middle>Mahak Goindani</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/chang-li/ class=align-middle>Chang Li</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/x/xiao-zhang/ class=align-middle>Xiao Zhang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/maria-leonor-pacheco/ class=align-middle>María Leonor Pacheco</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/f/franck-dernoncourt/ class=align-middle>Franck Dernoncourt</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/e/elena-sergeeva/ class=align-middle>Elena Sergeeva</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/matthew-mcdermott/ class=align-middle>Matthew McDermott</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/g/geeticka-chauhan/ class=align-middle>Geeticka Chauhan</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/amit-jindal/ class=align-middle>Amit Jindal</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/arijit-ghosh-chowdhury/ class=align-middle>Arijit Ghosh Chowdhury</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/aniket-didolkar/ class=align-middle>Aniket Didolkar</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/ramit-sawhney/ class=align-middle>Ramit Sawhney</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/rajiv-shah/ class=align-middle>Rajiv Shah</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/hongyuan-zhu/ class=align-middle>Hongyuan Zhu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/meng-fang/ class=align-middle>Meng Fang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/rick-siow-mong-goh/ class=align-middle>Rick Siow Mong Goh</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/kenneth-kwok/ class=align-middle>Kenneth Kwok</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/hung-yi-lee/ class=align-middle>Hung-Yi Lee</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/mitra-mohtarami/ class=align-middle>Mitra Mohtarami</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/shang-wen-li/ class=align-middle>Shang-Wen Li</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/mandy-korpusik/ class=align-middle>Mandy Korpusik</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/shuyan-dong/ class=align-middle>Shuyan Dong</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/n/ngoc-thang-vu/ class=align-middle>Ngoc Thang Vu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/coling/ class=align-middle>COLING</a><span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/venues/acl/ class=align-middle>ACL</a><span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/venues/semeval/ class=align-middle>SemEval</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/nlp4convai/ class=align-middle>NLP4ConvAI</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/emnlp/ class=align-middle>EMNLP</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-venues aria-expanded=false aria-controls=more-venues>show all...</li><div class="collapse border-top" id=more-venues><li class=list-group-item><a href=/venues/metanlp/ class=align-middle>MetaNLP</a><span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>