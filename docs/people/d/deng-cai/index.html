<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Deng Cai - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Deng</span> <span class=font-weight-bold>Cai</span></h2><hr><div class=row><div class=col-lg-9><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.137.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--137 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.137 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.137" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.137/>Dialogue Response Selection with Hierarchical Curriculum Learning</a></strong><br><a href=/people/y/yixuan-su/>Yixuan Su</a>
|
<a href=/people/d/deng-cai/>Deng Cai</a>
|
<a href=/people/q/qingyu-zhou/>Qingyu Zhou</a>
|
<a href=/people/z/zibo-lin/>Zibo Lin</a>
|
<a href=/people/s/simon-baker/>Simon Baker</a>
|
<a href=/people/y/yunbo-cao/>Yunbo Cao</a>
|
<a href=/people/s/shuming-shi/>Shuming Shi</a>
|
<a href=/people/n/nigel-collier/>Nigel Collier</a>
|
<a href=/people/y/yan-wang/>Yan Wang</a><br><a href=/volumes/2021.acl-long/ class=text-muted>Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--137><div class="card-body p-3 small">We study the learning of a <a href=https://en.wikipedia.org/wiki/Matching_model>matching model</a> for dialogue response selection. Motivated by the recent finding that models trained with random negative samples are not ideal in real-world scenarios, we propose a hierarchical curriculum learning framework that trains the matching model in an easy-to-difficult scheme. Our learning framework consists of two complementary curricula : (1) corpus-level curriculum (CC) ; and (2) instance-level curriculum (IC). In CC, the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> gradually increases its ability in finding the matching clues between the <a href=https://en.wikipedia.org/wiki/Context_(language_use)>dialogue context</a> and a response candidate. As for IC, it progressively strengthens the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a>&#8217;s ability in identifying the mismatching information between the <a href=https://en.wikipedia.org/wiki/Context_(language_use)>dialogue context</a> and a response candidate. Empirical studies on three benchmark datasets with three state-of-the-art matching models demonstrate that the proposed learning framework significantly improves the model performance across various evaluation metrics.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.567.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--567 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.567 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><span class="align-middle mr-1" data-toggle=tooltip data-placement=bottom title="Outstanding Paper"><i class="fas fa-award"></i></span><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.567" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2021.acl-long.567/>Neural Machine Translation with Monolingual Translation Memory</a></strong><br><a href=/people/d/deng-cai/>Deng Cai</a>
|
<a href=/people/y/yan-wang/>Yan Wang</a>
|
<a href=/people/h/huayang-li/>Huayang Li</a>
|
<a href=/people/w/wai-lam/>Wai Lam</a>
|
<a href=/people/l/lemao-liu/>Lemao Liu</a><br><a href=/volumes/2021.acl-long/ class=text-muted>Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--567><div class="card-body p-3 small">Prior work has proved that Translation Memory (TM) can boost the performance of Neural Machine Translation (NMT). In contrast to existing work that uses bilingual corpus as TM and employs source-side similarity search for <a href=https://en.wikipedia.org/wiki/Memory_retrieval>memory retrieval</a>, we propose a new framework that uses monolingual memory and performs learnable memory retrieval in a cross-lingual manner. Our <a href=https://en.wikipedia.org/wiki/Conceptual_framework>framework</a> has unique advantages. First, the cross-lingual memory retriever allows abundant monolingual data to be TM. Second, the <a href=https://en.wikipedia.org/wiki/Memory_retrieval>memory retriever</a> and NMT model can be jointly optimized for the ultimate translation goal. Experiments show that the proposed <a href=https://en.wikipedia.org/wiki/Scientific_method>method</a> obtains substantial improvements. Remarkably, it even outperforms strong TM-augmented NMT baselines using bilingual TM. Owning to the ability to leverage monolingual data, our model also demonstrates effectiveness in low-resource and domain adaptation scenarios.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.18.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--18 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.18 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.eacl-main.18" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.18/>Non-Autoregressive Text Generation with Pre-trained Language Models</a></strong><br><a href=/people/y/yixuan-su/>Yixuan Su</a>
|
<a href=/people/d/deng-cai/>Deng Cai</a>
|
<a href=/people/y/yan-wang/>Yan Wang</a>
|
<a href=/people/d/david-vandyke/>David Vandyke</a>
|
<a href=/people/s/simon-baker/>Simon Baker</a>
|
<a href=/people/p/piji-li/>Piji Li</a>
|
<a href=/people/n/nigel-collier/>Nigel Collier</a><br><a href=/volumes/2021.eacl-main/ class=text-muted>Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--18><div class="card-body p-3 small">Non-autoregressive generation (NAG) has recently attracted great attention due to its fast inference speed. However, the generation quality of existing NAG models still lags behind their autoregressive counterparts. In this work, we show that BERT can be employed as the backbone of a <a href=https://en.wikipedia.org/wiki/Non-blocking_algorithm>NAG model</a> for a greatly improved performance. Additionally, we devise two mechanisms to alleviate the two common problems of vanilla NAG models : the inflexibility of prefixed output length and the conditional independence of individual token predictions. To further strengthen the speed advantage of the proposed model, we propose a new decoding strategy, ratio-first, for applications where the output lengths can be approximately estimated beforehand. For a comprehensive evaluation, we test the proposed model on three text generation tasks, including <a href=https://en.wikipedia.org/wiki/Automatic_summarization>text summarization</a>, sentence compression and <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>. Experimental results show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> significantly outperforms existing non-autoregressive baselines and achieves competitive performance with many strong autoregressive models. In addition, we also conduct extensive analysis experiments to reveal the effect of each proposed component.</div></div><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.741.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--741 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.741 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938681 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.741/>The World is Not Binary : Learning to Rank with <a href=https://en.wikipedia.org/wiki/Grayscale>Grayscale Data</a> for Dialogue Response Selection</a></strong><br><a href=/people/z/zibo-lin/>Zibo Lin</a>
|
<a href=/people/d/deng-cai/>Deng Cai</a>
|
<a href=/people/y/yan-wang/>Yan Wang</a>
|
<a href=/people/x/xiaojiang-liu/>Xiaojiang Liu</a>
|
<a href=/people/h/haitao-zheng/>Haitao Zheng</a>
|
<a href=/people/s/shuming-shi/>Shuming Shi</a><br><a href=/volumes/2020.emnlp-main/ class=text-muted>Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--741><div class="card-body p-3 small">Response selection plays a vital role in building retrieval-based conversation systems. Despite that response selection is naturally a learning-to-rank problem, most prior works take a point-wise view and train binary classifiers for this task : each response candidate is labeled either relevant (one) or irrelevant (zero). On the one hand, this <a href=https://en.wikipedia.org/wiki/Formal_system>formalization</a> can be sub-optimal due to its ignorance of the diversity of response quality. On the other hand, annotating <a href=https://en.wikipedia.org/wiki/Grayscale>grayscale data</a> for learning-to-rank can be prohibitively expensive and challenging. In this work, we show that <a href=https://en.wikipedia.org/wiki/Grayscale>grayscale data</a> can be automatically constructed without human effort. Our method employs off-the-shelf response retrieval models and response generation models as automatic grayscale data generators. With the constructed <a href=https://en.wikipedia.org/wiki/Grayscale>grayscale data</a>, we propose multi-level ranking objectives for training, which can (1) teach a matching model to capture more fine-grained context-response relevance difference and (2) reduce the train-test discrepancy in terms of distractor strength. Our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> is simple, effective, and universal. Experiments on three <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmark datasets</a> and four state-of-the-art matching models show that the proposed approach brings significant and consistent performance improvements.</div></div><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1195.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1195 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1195 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1195/>Retrieval-guided Dialogue Response Generation via a Matching-to-Generation Framework</a></strong><br><a href=/people/d/deng-cai/>Deng Cai</a>
|
<a href=/people/y/yan-wang/>Yan Wang</a>
|
<a href=/people/w/wei-bi/>Wei Bi</a>
|
<a href=/people/z/zhaopeng-tu/>Zhaopeng Tu</a>
|
<a href=/people/x/xiaojiang-liu/>Xiaojiang Liu</a>
|
<a href=/people/s/shuming-shi/>Shuming Shi</a><br><a href=/volumes/D19-1/ class=text-muted>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1195><div class="card-body p-3 small">End-to-end sequence generation is a popular technique for developing open domain dialogue systems, though they suffer from the safe response problem. Researchers have attempted to tackle this problem by incorporating <a href=https://en.wikipedia.org/wiki/Generative_model>generative models</a> with the returns of <a href=https://en.wikipedia.org/wiki/Information_retrieval>retrieval systems</a>. Recently, a skeleton-then-response framework has been shown promising results for this <a href=https://en.wikipedia.org/wiki/Task_(computing)>task</a>. Nevertheless, how to precisely extract a <a href=https://en.wikipedia.org/wiki/Skeleton>skeleton</a> and how to effectively train a retrieval-guided response generator are still challenging. This paper presents a novel framework in which the skeleton extraction is made by an interpretable matching model and the following skeleton-guided response generation is accomplished by a separately trained generator. Extensive experiments demonstrate the effectiveness of our model designs.<i>safe response problem</i>. Researchers have attempted to tackle this problem by incorporating generative models with the returns of retrieval systems. Recently, a skeleton-then-response framework has been shown promising results for this task. Nevertheless, how to precisely extract a skeleton and how to effectively train a retrieval-guided response generator are still challenging. This paper presents a novel framework in which the skeleton extraction is made by an interpretable matching model and the following skeleton-guided response generation is accomplished by a separately trained generator. Extensive experiments demonstrate the effectiveness of our model designs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1667.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1667 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1667 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1667/>Charge-Based Prison Term Prediction with Deep Gating Network</a></strong><br><a href=/people/h/huajie-chen/>Huajie Chen</a>
|
<a href=/people/d/deng-cai/>Deng Cai</a>
|
<a href=/people/w/wei-dai/>Wei Dai</a>
|
<a href=/people/z/zehui-dai/>Zehui Dai</a>
|
<a href=/people/y/yadong-ding/>Yadong Ding</a><br><a href=/volumes/D19-1/ class=text-muted>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1667><div class="card-body p-3 small">Judgment prediction for <a href=https://en.wikipedia.org/wiki/Legal_case>legal cases</a> has attracted much research efforts for its practice use, of which the ultimate goal is prison term prediction. While existing work merely predicts the total prison term, in reality a defendant is often charged with multiple crimes. In this paper, we argue that charge-based prison term prediction (CPTP) not only better fits realistic needs, but also makes the total prison term prediction more accurate and interpretable. We collect the first large-scale structured data for <a href=https://en.wikipedia.org/wiki/CPTP>CPTP</a> and evaluate several competitive <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a>. Based on the observation that fine-grained feature selection is the key to achieving good performance, we propose the Deep Gating Network (DGN) for charge-specific feature selection and aggregation. Experiments show that DGN achieves the state-of-the-art performance.</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1132.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1132 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1132 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-1132/>Translating a Math Word Problem to a <a href=https://en.wikipedia.org/wiki/Expression_tree>Expression Tree</a></a></strong><br><a href=/people/l/lei-wang/>Lei Wang</a>
|
<a href=/people/y/yan-wang/>Yan Wang</a>
|
<a href=/people/d/deng-cai/>Deng Cai</a>
|
<a href=/people/d/dongxiang-zhang/>Dongxiang Zhang</a>
|
<a href=/people/x/xiaojiang-liu/>Xiaojiang Liu</a><br><a href=/volumes/D18-1/ class=text-muted>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1132><div class="card-body p-3 small">Sequence-to-sequence (SEQ2SEQ) models have been successfully applied to automatic math word problem solving. Despite its simplicity, a drawback still remains : a math word problem can be correctly solved by more than one equations. This non-deterministic transduction harms the performance of <a href=https://en.wikipedia.org/wiki/Maximum_likelihood_estimation>maximum likelihood estimation</a>. In this paper, by considering the uniqueness of <a href=https://en.wikipedia.org/wiki/Expression_tree>expression tree</a>, we propose an equation normalization method to normalize the duplicated equations. Moreover, we analyze the performance of three popular SEQ2SEQ models on the math word problem solving. We find that each <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> has its own specialty in solving problems, consequently an ensemble model is then proposed to combine their advantages. Experiments on dataset Math23 K show that the <a href=https://en.wikipedia.org/wiki/Ensemble_learning>ensemble model</a> with equation normalization significantly outperforms the previous state-of-the-art methods.</div></div><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2096.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2096 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2096 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P17-2096" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P17-2096/>Fast and Accurate Neural Word Segmentation for Chinese<span class=acl-fixed-case>C</span>hinese</a></strong><br><a href=/people/d/deng-cai/>Deng Cai</a>
|
<a href=/people/h/hai-zhao/>Hai Zhao</a>
|
<a href=/people/z/zhisong-zhang/>Zhisong Zhang</a>
|
<a href=/people/y/yuan-xin/>Yuan Xin</a>
|
<a href=/people/y/yongjian-wu/>Yongjian Wu</a>
|
<a href=/people/f/feiyue-huang/>Feiyue Huang</a><br><a href=/volumes/P17-2/ class=text-muted>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2096><div class="card-body p-3 small">Neural models with minimal feature engineering have achieved competitive performance against traditional methods for the task of <a href=https://en.wikipedia.org/wiki/Chinese_word_segmentation>Chinese word segmentation</a>. However, both training and working procedures of the current neural models are computationally inefficient. In this paper, we propose a greedy neural word segmenter with balanced word and character embedding inputs to alleviate the existing drawbacks. Our segmenter is truly end-to-end, capable of performing segmentation much faster and even more accurate than state-of-the-art neural models on Chinese benchmark datasets.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Deng+Cai" title="Search for 'Deng Cai' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/y/yan-wang/ class=align-middle>Yan Wang</a>
<span class="badge badge-secondary align-middle ml-2">6</span></li><li class=list-group-item><a href=/people/s/shuming-shi/ class=align-middle>Shuming Shi</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/x/xiaojiang-liu/ class=align-middle>Xiaojiang Liu</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/y/yixuan-su/ class=align-middle>Yixuan Su</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/z/zibo-lin/ class=align-middle>Zibo Lin</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/s/simon-baker/ class=align-middle>Simon Baker</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/n/nigel-collier/ class=align-middle>Nigel Collier</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/q/qingyu-zhou/ class=align-middle>Qingyu Zhou</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yunbo-cao/ class=align-middle>Yunbo Cao</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/huayang-li/ class=align-middle>Huayang Li</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/w/wai-lam/ class=align-middle>Wai Lam</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/lemao-liu/ class=align-middle>Lemao Liu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/haitao-zheng/ class=align-middle>Haitao Zheng</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/hai-zhao/ class=align-middle>Hai Zhao</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/zhisong-zhang/ class=align-middle>Zhisong Zhang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yuan-xin/ class=align-middle>Yuan Xin</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yongjian-wu/ class=align-middle>Yongjian Wu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/f/feiyue-huang/ class=align-middle>Feiyue Huang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/david-vandyke/ class=align-middle>David Vandyke</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/p/piji-li/ class=align-middle>Piji Li</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/lei-wang/ class=align-middle>Lei Wang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/dongxiang-zhang/ class=align-middle>Dongxiang Zhang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/w/wei-bi/ class=align-middle>Wei Bi</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/zhaopeng-tu/ class=align-middle>Zhaopeng Tu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/huajie-chen/ class=align-middle>Huajie Chen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/w/wei-dai/ class=align-middle>Wei Dai</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/zehui-dai/ class=align-middle>Zehui Dai</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yadong-ding/ class=align-middle>Yadong Ding</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/emnlp/ class=align-middle>EMNLP</a><span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/venues/acl/ class=align-middle>ACL</a><span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/venues/eacl/ class=align-middle>EACL</a><span class="badge badge-secondary align-middle ml-2">1</span></li></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright Â©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>