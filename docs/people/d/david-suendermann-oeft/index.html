<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>David Suendermann-Oeft - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>David</span> <span class=font-weight-bold>Suendermann-Oeft</span></h2><hr><div class=row><div class=col-lg-9><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.nlpmc-1.7.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--nlpmc-1--7 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.nlpmc-1.7 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://slideslive.com/38929892 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.nlpmc-1.7/>On the Utility of Audiovisual Dialog Technologies and Signal Analytics for Real-time Remote Monitoring of Depression Biomarkers</a></strong><br><a href=/people/m/michael-neumann/>Michael Neumann</a>
|
<a href=/people/o/oliver-roessler/>Oliver Roessler</a>
|
<a href=/people/d/david-suendermann-oeft/>David Suendermann-Oeft</a>
|
<a href=/people/v/vikram-ramanarayanan/>Vikram Ramanarayanan</a><br><a href=/volumes/2020.nlpmc-1/ class=text-muted>Proceedings of the First Workshop on Natural Language Processing for Medical Conversations</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--nlpmc-1--7><div class="card-body p-3 small">We investigate the utility of audiovisual dialog systems combined with speech and video analytics for real-time remote monitoring of depression at scale in uncontrolled environment settings. We collected audiovisual conversational data from participants who interacted with a cloud-based multimodal dialog system, and automatically extracted a large set of speech and vision metrics based on the rich existing literature of laboratory studies. We report on the efficacy of various audio and video metrics in differentiating people with mild, moderate and severe depression, and discuss the implications of these results for the deployment of such technologies in real-world neurological diagnosis and monitoring applications.</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5029.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5029 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5029 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5029/>Leveraging Multimodal Dialog Technology for the Design of Automated and Interactive Student Agents for Teacher Training</a></strong><br><a href=/people/d/david-pautler/>David Pautler</a>
|
<a href=/people/v/vikram-ramanarayanan/>Vikram Ramanarayanan</a>
|
<a href=/people/k/kirby-cofino/>Kirby Cofino</a>
|
<a href=/people/p/patrick-l-lange/>Patrick Lange</a>
|
<a href=/people/d/david-suendermann-oeft/>David Suendermann-Oeft</a><br><a href=/volumes/W18-50/ class=text-muted>Proceedings of the 19th Annual SIGdial Meeting on Discourse and Dialogue</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5029><div class="card-body p-3 small">We present a paradigm for interactive teacher training that leverages multimodal dialog technology to puppeteer custom-designed embodied conversational agents (ECAs) in student roles. We used the open-source multimodal dialog system HALEF to implement a small-group classroom math discussion involving <a href=https://en.wikipedia.org/wiki/Venn_diagram>Venn diagrams</a> where a human teacher candidate has to interact with two student ECAs whose actions are controlled by the dialog system. Such an automated paradigm has the potential to be extended and scaled to a wide range of interactive simulation scenarios in <a href=https://en.wikipedia.org/wiki/Education>education</a>, <a href=https://en.wikipedia.org/wiki/Medicine>medicine</a>, and <a href=https://en.wikipedia.org/wiki/Business>business</a> where group interaction training is essential.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-3015.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-3015 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-3015 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://vimeo.com/277631480 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N18-3015/>From dictations to clinical reports using <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a></a></strong><br><a href=/people/g/gregory-finley/>Gregory Finley</a>
|
<a href=/people/w/wael-salloum/>Wael Salloum</a>
|
<a href=/people/n/najmeh-sadoughi/>Najmeh Sadoughi</a>
|
<a href=/people/e/erik-edwards/>Erik Edwards</a>
|
<a href=/people/a/amanda-robinson/>Amanda Robinson</a>
|
<a href=/people/n/nico-axtmann/>Nico Axtmann</a>
|
<a href=/people/m/michael-brenndoerfer/>Michael Brenndoerfer</a>
|
<a href=/people/m/mark-miller/>Mark Miller</a>
|
<a href=/people/d/david-suendermann-oeft/>David Suendermann-Oeft</a><br><a href=/volumes/N18-3/ class=text-muted>Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 3 (Industry Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-3015><div class="card-body p-3 small">A typical <a href=https://en.wikipedia.org/wiki/Workflow>workflow</a> to document clinical encounters entails dictating a summary, running <a href=https://en.wikipedia.org/wiki/Speech_recognition>speech recognition</a>, and post-processing the resulting text into a formatted letter. Post-processing entails a host of transformations including punctuation restoration, <a href=https://en.wikipedia.org/wiki/Truecasing>truecasing</a>, marking sections and headers, converting dates and numerical expressions, parsing lists, etc. In conventional implementations, most of these <a href=https://en.wikipedia.org/wiki/Task_(computing)>tasks</a> are accomplished by individual <a href=https://en.wikipedia.org/wiki/Modular_programming>modules</a>. We introduce a novel holistic approach to <a href=https://en.wikipedia.org/wiki/Post-processing>post-processing</a> that relies on machine callytranslation. We show how this technique outperforms an alternative conventional systemeven learning to correct speech recognition errors during post-processingwhile being much simpler to maintain.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-5003.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-5003 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-5003 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-5003/>An automated medical scribe for documenting clinical encounters</a></strong><br><a href=/people/g/gregory-finley/>Gregory Finley</a>
|
<a href=/people/e/erik-edwards/>Erik Edwards</a>
|
<a href=/people/a/amanda-robinson/>Amanda Robinson</a>
|
<a href=/people/m/michael-brenndoerfer/>Michael Brenndoerfer</a>
|
<a href=/people/n/najmeh-sadoughi/>Najmeh Sadoughi</a>
|
<a href=/people/j/james-fone/>James Fone</a>
|
<a href=/people/n/nico-axtmann/>Nico Axtmann</a>
|
<a href=/people/m/mark-miller/>Mark Miller</a>
|
<a href=/people/d/david-suendermann-oeft/>David Suendermann-Oeft</a><br><a href=/volumes/N18-5/ class=text-muted>Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Demonstrations</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-5003><div class="card-body p-3 small">A <a href=https://en.wikipedia.org/wiki/Medical_scribe>medical scribe</a> is a clinical professional who charts patientphysician encounters in real time, relieving physicians of most of their administrative burden and substantially increasing productivity and job satisfaction. We present a complete implementation of an automated medical scribe. Our system can serve either as a scalable, standardized, and economical alternative to human scribes ; or as an assistive tool for them, providing a first draft of a report along with a convenient means to modify it. This solution is, to our knowledge, the first automated scribe ever presented and relies upon multiple speech and language technologies, including <a href=https://en.wikipedia.org/wiki/Speaker_diarization>speaker diarization</a>, <a href=https://en.wikipedia.org/wiki/Speech_recognition>medical speech recognition</a>, <a href=https://en.wikipedia.org/wiki/Knowledge_extraction>knowledge extraction</a>, and <a href=https://en.wikipedia.org/wiki/Natural-language_generation>natural language generation</a>.</div></div><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2319.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2319 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2319 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-2319/>Deep Learning for Punctuation Restoration in Medical Reports</a></strong><br><a href=/people/w/wael-salloum/>Wael Salloum</a>
|
<a href=/people/g/gregory-finley/>Greg Finley</a>
|
<a href=/people/e/erik-edwards/>Erik Edwards</a>
|
<a href=/people/m/mark-miller/>Mark Miller</a>
|
<a href=/people/d/david-suendermann-oeft/>David Suendermann-Oeft</a><br><a href=/volumes/W17-23/ class=text-muted>BioNLP 2017</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2319><div class="card-body p-3 small">In clinical dictation, speakers try to be as concise as possible to save time, often resulting in utterances without explicit punctuation commands. Since the end product of a dictated report, e.g. an out-patient letter, does require correct <a href=https://en.wikipedia.org/wiki/Orthography>orthography</a>, including exact <a href=https://en.wikipedia.org/wiki/Punctuation>punctuation</a>, the latter need to be restored, preferably by automated means. This paper describes a method for punctuation restoration based on a state-of-the-art stack of NLP and machine learning techniques including B-RNNs with an attention mechanism and late fusion, as well as a feature extraction technique tailored to the processing of medical terminology using a novel vocabulary reduction model. To the best of our knowledge, the resulting performance is superior to that reported in prior art on similar <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2336.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2336 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2336 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-2336/>Automated Preamble Detection in Dictated Medical Reports</a></strong><br><a href=/people/w/wael-salloum/>Wael Salloum</a>
|
<a href=/people/g/gregory-finley/>Greg Finley</a>
|
<a href=/people/e/erik-edwards/>Erik Edwards</a>
|
<a href=/people/m/mark-miller/>Mark Miller</a>
|
<a href=/people/d/david-suendermann-oeft/>David Suendermann-Oeft</a><br><a href=/volumes/W17-23/ class=text-muted>BioNLP 2017</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2336><div class="card-body p-3 small">Dictated medical reports very often feature a <a href=https://en.wikipedia.org/wiki/Preamble>preamble</a> containing <a href=https://en.wikipedia.org/wiki/Metainformation>metainformation</a> about the report such as patient and physician names, location and name of the clinic, date of procedure, and so on. In the medical transcription process, the <a href=https://en.wikipedia.org/wiki/Preamble>preamble</a> is usually omitted from the final report, as it contains information already available in the <a href=https://en.wikipedia.org/wiki/Electronic_health_record>electronic medical record</a>. We present a <a href=https://en.wikipedia.org/wiki/Scientific_method>method</a> which is able to automatically identify <a href=https://en.wikipedia.org/wiki/Preamble>preambles</a> in <a href=https://en.wikipedia.org/wiki/Dictation_(exercise)>medical dictations</a>. The method makes use of state-of-the-art NLP techniques including <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> and Bi-LSTMs and achieves preamble detection performance superior to humans.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=David+Suendermann-Oeft" title="Search for 'David Suendermann-Oeft' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/g/gregory-finley/ class=align-middle>Gregory Finley</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/people/e/erik-edwards/ class=align-middle>Erik Edwards</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/people/m/mark-miller/ class=align-middle>Mark Miller</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/people/w/wael-salloum/ class=align-middle>Wael Salloum</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/v/vikram-ramanarayanan/ class=align-middle>Vikram Ramanarayanan</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/n/najmeh-sadoughi/ class=align-middle>Najmeh Sadoughi</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/a/amanda-robinson/ class=align-middle>Amanda Robinson</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/n/nico-axtmann/ class=align-middle>Nico Axtmann</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/m/michael-brenndoerfer/ class=align-middle>Michael Brenndoerfer</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/d/david-pautler/ class=align-middle>David Pautler</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/kirby-cofino/ class=align-middle>Kirby Cofino</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/p/patrick-l-lange/ class=align-middle>Patrick L. Lange</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/michael-neumann/ class=align-middle>Michael Neumann</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/o/oliver-roessler/ class=align-middle>Oliver Roessler</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/james-fone/ class=align-middle>James Fone</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/ws/ class=align-middle>WS</a><span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/venues/naacl/ class=align-middle>NAACL</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/nlpmc/ class=align-middle>NLPMC</a><span class="badge badge-secondary align-middle ml-2">1</span></li></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>