<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>David Chiang - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>David</span> <span class=font-weight-bold>Chiang</span></h2><hr><div class=row><div class=col-lg-9><h4>2022</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.acl-long.527.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--acl-long--527 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.acl-long.527 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.acl-long.527" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2022.acl-long.527/>Overcoming a Theoretical Limitation of Self-Attention</a></strong><br><a href=/people/d/david-chiang/>David Chiang</a>
|
<a href=/people/p/peter-cholak/>Peter Cholak</a><br><a href=/volumes/2022.acl-long/ class=text-muted>Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--acl-long--527><div class="card-body p-3 small">Although transformers are remarkably effective for many tasks, there are some surprisingly easy-looking regular languages that they struggle with. Hahn shows that for languages where acceptance depends on a single input symbol, a transformer&#8217;s classification decisions get closer and closer to random guessing (that is, a cross-entropy of 1) as input strings get longer and longer. We examine this limitation using two languages: PARITY, the language of bit strings with an odd number of 1s, and FIRST, the language of bit strings starting with a 1. We demonstrate three ways of overcoming the limitation implied by Hahn&#8217;s lemma. First, we settle an open question by constructing a transformer that recognizes PARITY with perfect accuracy, and similarly for FIRST. Second, we use layer normalization to bring the cross-entropy of both models arbitrarily close to zero. Third, when transformers need to focus on a single position, as for FIRST, we find that they can fail to generalize to longer strings; we offer a simple remedy to this problem that also improves length generalization in machine translation.</div></div><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-tutorials.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-tutorials.0/>Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: Tutorial Abstracts</a></strong><br><a href=/people/d/david-chiang/>David Chiang</a>
|
<a href=/people/m/min-zhang/>Min Zhang</a><br><a href=/volumes/2021.acl-tutorials/ class=text-muted>Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: Tutorial Abstracts</a></span></p><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5625.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5625 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5625 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-5625" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-5625/>Auto-Sizing the Transformer Network : Improving Speed, Efficiency, and Performance for Low-Resource Machine Translation</a></strong><br><a href=/people/k/kenton-murray/>Kenton Murray</a>
|
<a href=/people/j/jeffery-kinnison/>Jeffery Kinnison</a>
|
<a href=/people/t/toan-q-nguyen/>Toan Q. Nguyen</a>
|
<a href=/people/w/walter-scheirer/>Walter Scheirer</a>
|
<a href=/people/d/david-chiang/>David Chiang</a><br><a href=/volumes/D19-56/ class=text-muted>Proceedings of the 3rd Workshop on Neural Generation and Translation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5625><div class="card-body p-3 small">Neural sequence-to-sequence models, particularly the Transformer, are the state of the art in <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>. Yet these <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a> are very sensitive to <a href=https://en.wikipedia.org/wiki/Network_architecture>architecture</a> and hyperparameter settings. Optimizing these settings by grid or random search is computationally expensive because it requires many training runs. In this paper, we incorporate <a href=https://en.wikipedia.org/wiki/Architecture_search>architecture search</a> into a single training run through auto-sizing, which uses <a href=https://en.wikipedia.org/wiki/Regularization_(mathematics)>regularization</a> to delete <a href=https://en.wikipedia.org/wiki/Neuron>neurons</a> in a <a href=https://en.wikipedia.org/wiki/Computer_network>network</a> over the course of training. On very low-resource language pairs, we show that auto-sizing can improve BLEU scores by up to 3.9 points while removing one-third of the <a href=https://en.wikipedia.org/wiki/Parameter_(computer_programming)>parameters</a> from the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>.</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1000.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-1000/>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</a></strong><br><a href=/people/e/ellen-riloff/>Ellen Riloff</a>
|
<a href=/people/d/david-chiang/>David Chiang</a>
|
<a href=/people/j/julia-hockenmaier/>Julia Hockenmaier</a>
|
<a href=/people/j/junichi-tsujii/>Jun’ichi Tsujii</a><br><a href=/volumes/D18-1/ class=text-muted>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/J18-1005.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-J18-1005 data-toggle=collapse aria-expanded=false aria-controls=abstract-J18-1005 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/J18-1005/>Weighted DAG Automata for Semantic Graphs<span class=acl-fixed-case>DAG</span> Automata for Semantic Graphs</a></strong><br><a href=/people/d/david-chiang/>David Chiang</a>
|
<a href=/people/f/frank-drewes/>Frank Drewes</a>
|
<a href=/people/d/daniel-gildea/>Daniel Gildea</a>
|
<a href=/people/a/adam-lopez/>Adam Lopez</a>
|
<a href=/people/g/giorgio-satta/>Giorgio Satta</a><br><a href=/volumes/J18-1/ class=text-muted>Computational Linguistics, Volume 44, Issue 1 - April 2018</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-J18-1005><div class="card-body p-3 small">Graphs have a variety of uses in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a>, particularly as representations of linguistic meaning. A deficit in this area of research is a formal framework for creating, combining, and using models involving graphs that parallels the frameworks of <a href=https://en.wikipedia.org/wiki/Finite-state_machine>finite automata</a> for <a href=https://en.wikipedia.org/wiki/String_(computer_science)>strings</a> and <a href=https://en.wikipedia.org/wiki/Finite-state_machine>finite tree automata</a> for <a href=https://en.wikipedia.org/wiki/Tree_(graph_theory)>trees</a>. A possible starting point for such a framework is the formalism of directed acyclic graph (DAG) automata, defined by Kamimura and Slutzki and extended by Quernheim and Knight. In this article, we study the latter in depth, demonstrating several new results, including a practical recognition algorithm that can be used for <a href=https://en.wikipedia.org/wiki/Inference>inference</a> and <a href=https://en.wikipedia.org/wiki/Machine_learning>learning</a> with <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> defined on DAG automata. We also propose an extension to <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graphs</a> with unbounded node degree and show that our results carry over to the extended <a href=https://en.wikipedia.org/wiki/Formalism_(philosophy_of_mathematics)>formalism</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1008.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1008 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1008 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://vimeo.com/276441141 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N18-1008/>Tied Multitask Learning for Neural Speech Translation</a></strong><br><a href=/people/a/antonios-anastasopoulos/>Antonios Anastasopoulos</a>
|
<a href=/people/d/david-chiang/>David Chiang</a><br><a href=/volumes/N18-1/ class=text-muted>Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1008><div class="card-body p-3 small">We explore multitask models for neural translation of speech, augmenting them in order to reflect two intuitive notions. First, we introduce a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> where the second task decoder receives information from the decoder of the first <a href=https://en.wikipedia.org/wiki/Task_(computing)>task</a>, since higher-level intermediate representations should provide useful information. Second, we apply <a href=https://en.wikipedia.org/wiki/Regularization_(mathematics)>regularization</a> that encourages transitivity and <a href=https://en.wikipedia.org/wiki/Inverse_problem>invertibility</a>. We show that the application of these notions on jointly trained models improves performance on the tasks of low-resource speech transcription and <a href=https://en.wikipedia.org/wiki/Translation>translation</a>. It also leads to better performance when using <a href=https://en.wikipedia.org/wiki/Attentional_control>attention information</a> for word discovery over unsegmented input.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1116.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1116 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1116 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-1116/>Combining Character and Word Information in <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a> Using a Multi-Level Attention</a></strong><br><a href=/people/h/huadong-chen/>Huadong Chen</a>
|
<a href=/people/s/shujian-huang/>Shujian Huang</a>
|
<a href=/people/d/david-chiang/>David Chiang</a>
|
<a href=/people/x/xinyu-dai/>Xinyu Dai</a>
|
<a href=/people/j/jiajun-chen/>Jiajun Chen</a><br><a href=/volumes/N18-1/ class=text-muted>Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1116><div class="card-body p-3 small">Natural language sentences, being hierarchical, can be represented at different levels of <a href=https://en.wikipedia.org/wiki/Granularity>granularity</a>, like <a href=https://en.wikipedia.org/wiki/Word>words</a>, <a href=https://en.wikipedia.org/wiki/Subword>subwords</a>, or <a href=https://en.wikipedia.org/wiki/Character_(symbol)>characters</a>. But most neural machine translation systems require the sentence to be represented as a sequence at a single level of granularity. It can be difficult to determine which <a href=https://en.wikipedia.org/wiki/Granularity>granularity</a> is better for a particular translation task. In this paper, we improve the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> by incorporating multiple levels of granularity. Specifically, we propose (1) an encoder with character attention which augments the (sub)word-level representation with character-level information ; (2) a decoder with multiple attentions that enable the representations from different levels of granularity to control the translation cooperatively. Experiments on three translation tasks demonstrate that our proposed models outperform the standard word-based model, the subword-based model, and a strong character-based model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1251.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1251 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1251 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1251.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-1251/>Composing <a href=https://en.wikipedia.org/wiki/Finite-state_transducer>Finite State Transducers</a> on GPUs<span class=acl-fixed-case>GPU</span>s</a></strong><br><a href=/people/a/arturo-argueta/>Arturo Argueta</a>
|
<a href=/people/d/david-chiang/>David Chiang</a><br><a href=/volumes/P18-1/ class=text-muted>Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1251><div class="card-body p-3 small">Weighted finite state transducers (FSTs) are frequently used in <a href=https://en.wikipedia.org/wiki/Language_processing>language processing</a> to handle tasks such as <a href=https://en.wikipedia.org/wiki/Part-of-speech_tagging>part-of-speech tagging</a> and <a href=https://en.wikipedia.org/wiki/Speech_recognition>speech recognition</a>. There has been previous work using multiple <a href=https://en.wikipedia.org/wiki/Central_processing_unit>CPU cores</a> to accelerate <a href=https://en.wikipedia.org/wiki/Finite-state_machine>finite state algorithms</a>, but limited attention has been given to parallel graphics processing unit (GPU) implementations. In this paper, we introduce the first (to our knowledge) GPU implementation of the FST composition operation, and we also discuss the optimizations used to achieve the best performance on this <a href=https://en.wikipedia.org/wiki/Computer_architecture>architecture</a>. We show that our approach obtains speedups of up to 6 times over our serial implementation and 4.5 times over OpenFST.</div></div><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K17-1011.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K17-1011 data-toggle=collapse aria-expanded=false aria-controls=abstract-K17-1011 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/K17-1011/>Top-Rank Enhanced Listwise Optimization for <a href=https://en.wikipedia.org/wiki/Statistical_machine_translation>Statistical Machine Translation</a></a></strong><br><a href=/people/h/huadong-chen/>Huadong Chen</a>
|
<a href=/people/s/shujian-huang/>Shujian Huang</a>
|
<a href=/people/d/david-chiang/>David Chiang</a>
|
<a href=/people/x/xinyu-dai/>Xinyu Dai</a>
|
<a href=/people/j/jiajun-chen/>Jiajun Chen</a><br><a href=/volumes/K17-1/ class=text-muted>Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K17-1011><div class="card-body p-3 small">Pairwise ranking methods are the most widely used discriminative training approaches for structure prediction problems in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing (NLP)</a>. Decomposing the problem of ranking hypotheses into <a href=https://en.wikipedia.org/wiki/Pairwise_comparisons>pairwise comparisons</a> enables simple and efficient solutions. However, neglecting the global ordering of the hypothesis list may hinder <a href=https://en.wikipedia.org/wiki/Learning>learning</a>. We propose a listwise learning framework for structure prediction problems such as <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>. Our framework directly models the entire translation list&#8217;s ordering to learn parameters which may better fit the given listwise samples. Furthermore, we propose top-rank enhanced loss functions, which are more sensitive to ranking errors at higher positions. Experiments on a large-scale Chinese-English translation task show that both our listwise learning framework and top-rank enhanced listwise losses lead to significant improvements in translation quality.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1177.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1177 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1177 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P17-1177" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P17-1177/>Improved <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a> with a Syntax-Aware Encoder and Decoder</a></strong><br><a href=/people/h/huadong-chen/>Huadong Chen</a>
|
<a href=/people/s/shujian-huang/>Shujian Huang</a>
|
<a href=/people/d/david-chiang/>David Chiang</a>
|
<a href=/people/j/jiajun-chen/>Jiajun Chen</a><br><a href=/volumes/P17-1/ class=text-muted>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1177><div class="card-body p-3 small">Most neural machine translation (NMT) models are based on the sequential encoder-decoder framework, which makes no use of <a href=https://en.wikipedia.org/wiki/Syntax>syntactic information</a>. In this paper, we improve this <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> by explicitly incorporating source-side syntactic trees. More specifically, we propose (1) a bidirectional tree encoder which learns both sequential and tree structured representations ; (2) a tree-coverage model that lets the attention depend on the source-side syntax. Experiments on Chinese-English translation demonstrate that our proposed models outperform the sequential attentional model as well as a stronger baseline with a bottom-up tree encoder and word coverage.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4607.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4607 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4607 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4607/>Spoken Term Discovery for <a href=https://en.wikipedia.org/wiki/Language_documentation>Language Documentation</a> using Translations</a></strong><br><a href=/people/a/antonios-anastasopoulos/>Antonios Anastasopoulos</a>
|
<a href=/people/s/sameer-bansal/>Sameer Bansal</a>
|
<a href=/people/d/david-chiang/>David Chiang</a>
|
<a href=/people/s/sharon-goldwater/>Sharon Goldwater</a>
|
<a href=/people/a/adam-lopez/>Adam Lopez</a><br><a href=/volumes/W17-46/ class=text-muted>Proceedings of the Workshop on Speech-Centric Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4607><div class="card-body p-3 small">Vast amounts of speech data collected for <a href=https://en.wikipedia.org/wiki/Language_documentation>language documentation</a> and research remain untranscribed and unsearchable, but often a small amount of speech may have text translations available. We present a <a href=https://en.wikipedia.org/wiki/Methodology>method</a> for partially labeling additional speech with translations in this <a href=https://en.wikipedia.org/wiki/Scenario>scenario</a>. We modify an unsupervised speech-to-translation alignment model and obtain prototype speech segments that match the translation words, which are in turn used to discover terms in the unlabelled data. We evaluate our method on a Spanish-English speech translation corpus and on two corpora of endangered languages, <a href=https://en.wikipedia.org/wiki/Arapaho_language>Arapaho</a> and Ainu, demonstrating its appropriateness and applicability in an actual very-low-resource scenario.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1098.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1098 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1098 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1098/>Decoding with <a href=https://en.wikipedia.org/wiki/Finite-state_transducer>Finite-State Transducers</a> on GPUs<span class=acl-fixed-case>GPU</span>s</a></strong><br><a href=/people/a/arturo-argueta/>Arturo Argueta</a>
|
<a href=/people/d/david-chiang/>David Chiang</a><br><a href=/volumes/E17-1/ class=text-muted>Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1098><div class="card-body p-3 small">Weighted finite automata and transducers (including hidden Markov models and conditional random fields) are widely used in natural language processing (NLP) to perform tasks such as morphological analysis, <a href=https://en.wikipedia.org/wiki/Part-of-speech_tagging>part-of-speech tagging</a>, chunking, <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>named entity recognition</a>, <a href=https://en.wikipedia.org/wiki/Speech_recognition>speech recognition</a>, and others. Parallelizing finite state algorithms on graphics processing units (GPUs) would benefit many areas of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a>. Although researchers have implemented GPU versions of basic graph algorithms, no work, to our knowledge, has been done on GPU algorithms for weighted finite automata. We introduce a GPU implementation of the Viterbi and forward-backward algorithm, achieving speedups of up to 4x over our serial implementations running on different computer architectures and 3335x over widely used tools such as OpenFST.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=David+Chiang" title="Search for 'David Chiang' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/h/huadong-chen/ class=align-middle>Huadong Chen</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/s/shujian-huang/ class=align-middle>Shujian Huang</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/j/jiajun-chen/ class=align-middle>Jiajun Chen</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/x/xinyu-dai/ class=align-middle>Xinyu Dai</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/a/antonios-anastasopoulos/ class=align-middle>Antonios Anastasopoulos</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/a/adam-lopez/ class=align-middle>Adam Lopez</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/a/arturo-argueta/ class=align-middle>Arturo Argueta</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/m/min-zhang/ class=align-middle>Min Zhang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/sameer-bansal/ class=align-middle>Sameer Bansal</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/sharon-goldwater/ class=align-middle>Sharon Goldwater</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/p/peter-cholak/ class=align-middle>Peter Cholak</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/e/ellen-riloff/ class=align-middle>Ellen Riloff</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/julia-hockenmaier/ class=align-middle>Julia Hockenmaier</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/junichi-tsujii/ class=align-middle>Jun’ichi Tsujii</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/kenton-murray/ class=align-middle>Kenton Murray</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jeffery-kinnison/ class=align-middle>Jeffery Kinnison</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/toan-q-nguyen/ class=align-middle>Toan Q. Nguyen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/w/walter-scheirer/ class=align-middle>Walter Scheirer</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/f/frank-drewes/ class=align-middle>Frank Drewes</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/daniel-gildea/ class=align-middle>Daniel Gildea</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/g/giorgio-satta/ class=align-middle>Giorgio Satta</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/acl/ class=align-middle>ACL</a><span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/venues/emnlp/ class=align-middle>EMNLP</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/naacl/ class=align-middle>NAACL</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/conll/ class=align-middle>CoNLL</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/ws/ class=align-middle>WS</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-venues aria-expanded=false aria-controls=more-venues>show all...</li><div class="collapse border-top" id=more-venues><li class=list-group-item><a href=/venues/cl/ class=align-middle>CL</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/eacl/ class=align-middle>EACL</a><span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>