<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Dong Yu - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Dong</span> <span class=font-weight-bold>Yu</span></h2><hr><div class=row><div class=col-lg-9><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.445.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--445 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.445 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.445" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.445/>Importance-based Neuron Allocation for Multilingual Neural Machine Translation</a></strong><br><a href=/people/w/wanying-xie/>Wanying Xie</a>
|
<a href=/people/y/yang-feng/>Yang Feng</a>
|
<a href=/people/s/shuhao-gu/>Shuhao Gu</a>
|
<a href=/people/d/dong-yu/>Dong Yu</a><br><a href=/volumes/2021.acl-long/ class=text-muted>Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--445><div class="card-body p-3 small">Multilingual neural machine translation with a single <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> has drawn much attention due to its capability to deal with multiple languages. However, the current multilingual translation paradigm often makes the model tend to preserve the <a href=https://en.wikipedia.org/wiki/General_knowledge>general knowledge</a>, but ignore the language-specific knowledge. Some previous works try to solve this problem by adding various kinds of language-specific modules to the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>, but they suffer from the parameter explosion problem and require specialized manual design. To solve these problems, we propose to divide the model neurons into general and language-specific parts based on their importance across languages. The general part is responsible for preserving the <a href=https://en.wikipedia.org/wiki/General_knowledge>general knowledge</a> and participating in the <a href=https://en.wikipedia.org/wiki/Translation>translation</a> of all the languages, while the language-specific part is responsible for preserving the language-specific knowledge and participating in the <a href=https://en.wikipedia.org/wiki/Translation>translation</a> of some specific languages. Experimental results on several language pairs, covering IWSLT and Europarl corpus datasets, demonstrate the effectiveness and universality of the proposed method.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.402.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--402 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.402 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.402/>RAST : Domain-Robust Dialogue Rewriting as Sequence Tagging<span class=acl-fixed-case>RAST</span>: Domain-Robust Dialogue Rewriting as Sequence Tagging</a></strong><br><a href=/people/j/jie-hao/>Jie Hao</a>
|
<a href=/people/l/linfeng-song/>Linfeng Song</a>
|
<a href=/people/l/liwei-wang/>Liwei Wang</a>
|
<a href=/people/k/kun-xu/>Kun Xu</a>
|
<a href=/people/z/zhaopeng-tu/>Zhaopeng Tu</a>
|
<a href=/people/d/dong-yu/>Dong Yu</a><br><a href=/volumes/2021.emnlp-main/ class=text-muted>Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--402><div class="card-body p-3 small">The task of dialogue rewriting aims to reconstruct the latest dialogue utterance by copying the missing content from the dialogue context. Until now, the existing <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> for this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> suffer from the robustness issue, i.e., performances drop dramatically when testing on a different dataset. We address this robustness issue by proposing a novel sequence-tagging-based model so that the <a href=https://en.wikipedia.org/wiki/Feasible_region>search space</a> is significantly reduced, yet the core of this task is still well covered. As a common issue of most tagging models for text generation, the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a>&#8217;s outputs may lack fluency. To alleviate this issue, we inject the loss signal from <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a> or GPT-2 under a REINFORCE framework. Experiments show huge improvements of our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> over the current state-of-the-art <a href=https://en.wikipedia.org/wiki/System>systems</a> when transferring to another <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>.</div></div><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.acl-main.482.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--acl-main--482 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.acl-main.482 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://slideslive.com/38928741 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.acl-main.482/>ZPR2 : Joint Zero Pronoun Recovery and Resolution using Multi-Task Learning and BERT<span class=acl-fixed-case>ZPR</span>2: Joint Zero Pronoun Recovery and Resolution using Multi-Task Learning and <span class=acl-fixed-case>BERT</span></a></strong><br><a href=/people/l/linfeng-song/>Linfeng Song</a>
|
<a href=/people/k/kun-xu/>Kun Xu</a>
|
<a href=/people/y/yue-zhang/>Yue Zhang</a>
|
<a href=/people/j/jianshu-chen/>Jianshu Chen</a>
|
<a href=/people/d/dong-yu/>Dong Yu</a><br><a href=/volumes/2020.acl-main/ class=text-muted>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--acl-main--482><div class="card-body p-3 small">Zero pronoun recovery and resolution aim at recovering the dropped pronoun and pointing out its <a href=https://en.wikipedia.org/wiki/Anaphora_(linguistics)>anaphoric mentions</a>, respectively. We propose to better explore their interaction by solving both <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> together, while the previous work treats them separately. For zero pronoun resolution, we study this task in a more realistic setting, where no <a href=https://en.wikipedia.org/wiki/Parsing_tree>parsing trees</a> or only <a href=https://en.wikipedia.org/wiki/Tree_(data_structure)>automatic trees</a> are available, while most previous work assumes gold trees. Experiments on two <a href=https://en.wikipedia.org/wiki/Benchmarking>benchmarks</a> show that joint modeling significantly outperforms our <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline</a> that already beats the previous state of the arts.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.acl-main.712.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--acl-main--712 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.acl-main.712 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://slideslive.com/38928740 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.acl-main.712" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.acl-main.712/>Structural Information Preserving for Graph-to-Text Generation</a></strong><br><a href=/people/l/linfeng-song/>Linfeng Song</a>
|
<a href=/people/a/ante-wang/>Ante Wang</a>
|
<a href=/people/j/jinsong-su/>Jinsong Su</a>
|
<a href=/people/y/yue-zhang/>Yue Zhang</a>
|
<a href=/people/k/kun-xu/>Kun Xu</a>
|
<a href=/people/y/yubin-ge/>Yubin Ge</a>
|
<a href=/people/d/dong-yu/>Dong Yu</a><br><a href=/volumes/2020.acl-main/ class=text-muted>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--acl-main--712><div class="card-body p-3 small">The task of graph-to-text generation aims at producing sentences that preserve the meaning of input graphs. As a crucial defect, the current state-of-the-art models may mess up or even drop the core structural information of input graphs when generating outputs. We propose to tackle this problem by leveraging richer training signals that can guide our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> for preserving input information. In particular, we introduce two types of autoencoding losses, each individually focusing on different aspects (a.k.a. views) of input graphs. The losses are then back-propagated to better calibrate our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> via multi-task training. Experiments on two <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmarks</a> for graph-to-text generation show the effectiveness of our approach over a state-of-the-art baseline.</div></div><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-6012.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-6012 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-6012 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-6012/>BLCU-NLP at COIN-Shared Task1 : Stagewise Fine-tuning BERT for Commonsense Inference in Everyday Narrations<span class=acl-fixed-case>BLCU</span>-<span class=acl-fixed-case>NLP</span> at <span class=acl-fixed-case>COIN</span>-Shared Task1: Stagewise Fine-tuning <span class=acl-fixed-case>BERT</span> for Commonsense Inference in Everyday Narrations</a></strong><br><a href=/people/c/chunhua-liu/>Chunhua Liu</a>
|
<a href=/people/d/dong-yu/>Dong Yu</a><br><a href=/volumes/D19-60/ class=text-muted>Proceedings of the First Workshop on Commonsense Inference in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-6012><div class="card-body p-3 small">This paper describes our <a href=https://en.wikipedia.org/wiki/System>system</a> for COIN Shared Task 1 : Commonsense Inference in Everyday Narrations. To inject more external knowledge to better reason over the narrative passage, question and answer, the <a href=https://en.wikipedia.org/wiki/System>system</a> adopts a stagewise fine-tuning method based on pre-trained BERT model. More specifically, the first stage is to fine-tune on addi- tional machine reading comprehension dataset to learn more <a href=https://en.wikipedia.org/wiki/Commonsense_knowledge>commonsense knowledge</a>. The second stage is to fine-tune on target-task (MCScript2.0) with MCScript (2018) dataset assisted. Experimental results show that our <a href=https://en.wikipedia.org/wiki/System>system</a> achieves significant improvements over the <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline systems</a> with 84.2 % <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> on the official test dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/Q19-1014.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-Q19-1014 data-toggle=collapse aria-expanded=false aria-controls=abstract-Q19-1014 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/Q19-1014/>DREAM : A Challenge Data Set and Models for Dialogue-Based Reading Comprehension<span class=acl-fixed-case>DREAM</span>: A Challenge Data Set and Models for Dialogue-Based Reading Comprehension</a></strong><br><a href=/people/k/kai-sun/>Kai Sun</a>
|
<a href=/people/d/dian-yu/>Dian Yu</a>
|
<a href=/people/j/jianshu-chen/>Jianshu Chen</a>
|
<a href=/people/d/dong-yu/>Dong Yu</a>
|
<a href=/people/y/yejin-choi/>Yejin Choi</a>
|
<a href=/people/c/claire-cardie/>Claire Cardie</a><br><a href=/volumes/Q19-1/ class=text-muted>Transactions of the Association for Computational Linguistics, Volume 7</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-Q19-1014><div class="card-body p-3 small">We present <a href=https://en.wikipedia.org/wiki/DREAM>DREAM</a>, the first dialogue-based multiple-choice reading comprehension data set. Collected from English as a Foreign Language examinations designed by human experts to evaluate the comprehension level of Chinese learners of English, our <a href=https://en.wikipedia.org/wiki/Data_set>data set</a> contains 10,197 <a href=https://en.wikipedia.org/wiki/Multiple_choice>multiple-choice questions</a> for 6,444 dialogues. In contrast to existing reading comprehension data sets, DREAM is the first to focus on in-depth multi-turn multi-party dialogue understanding. DREAM is likely to present significant challenges for existing reading comprehension systems : 84 % of answers are non-extractive, 85 % of questions require reasoning beyond a single sentence, and 34 % of questions also involve commonsense knowledge. We apply several popular neural reading comprehension models that primarily exploit surface information within the text and find them to, at best, just barely outperform a rule-based approach. We next investigate the effects of incorporating dialogue structure and different kinds of general world knowledge into both rule-based and (neural and non-neural) machine learning-based reading comprehension models. Experimental results on the DREAM data set show the effectiveness of dialogue structure and <a href=https://en.wikipedia.org/wiki/General_knowledge>general world knowledge</a>. DREAM is available at https://dataset.org/dream/.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S19-2191.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S19-2191 data-toggle=collapse aria-expanded=false aria-controls=abstract-S19-2191 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/S19-2191/>BLCU_NLP at SemEval-2019 Task 7 : An Inference Chain-based GPT Model for Rumour Evaluation<span class=acl-fixed-case>BLCU</span>_<span class=acl-fixed-case>NLP</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2019 Task 7: An Inference Chain-based <span class=acl-fixed-case>GPT</span> Model for Rumour Evaluation</a></strong><br><a href=/people/r/ruoyao-yang/>Ruoyao Yang</a>
|
<a href=/people/w/wanying-xie/>Wanying Xie</a>
|
<a href=/people/c/chunhua-liu/>Chunhua Liu</a>
|
<a href=/people/d/dong-yu/>Dong Yu</a><br><a href=/volumes/S19-2/ class=text-muted>Proceedings of the 13th International Workshop on Semantic Evaluation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S19-2191><div class="card-body p-3 small">Researchers have been paying increasing attention to rumour evaluation due to the rapid spread of unsubstantiated rumours on <a href=https://en.wikipedia.org/wiki/Social_media>social media platforms</a>, including SemEval 2019 task 7. However, labelled data for learning rumour veracity is scarce, and labels in rumour stance data are highly disproportionate, making it challenging for a model to perform <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised-learning</a> adequately. We propose an inference chain-based system, which fully utilizes conversation structure-based knowledge in the limited data and expand the training data in minority categories to alleviate <a href=https://en.wikipedia.org/wiki/Social_class>class imbalance</a>. Our <a href=https://en.wikipedia.org/wiki/Software_development_process>approach</a> obtains 12.6 % improvement upon the baseline system for subtask A, ranks 1st among 21 systems in subtask A, and ranks 4th among 12 systems in subtask B.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1270.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1270 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1270 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/364746823 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1270" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1270/>Improving Machine Reading Comprehension with General Reading Strategies</a></strong><br><a href=/people/k/kai-sun/>Kai Sun</a>
|
<a href=/people/d/dian-yu/>Dian Yu</a>
|
<a href=/people/d/dong-yu/>Dong Yu</a>
|
<a href=/people/c/claire-cardie/>Claire Cardie</a><br><a href=/volumes/N19-1/ class=text-muted>Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1270><div class="card-body p-3 small">Reading strategies have been shown to improve <a href=https://en.wikipedia.org/wiki/Sentence_processing>comprehension levels</a>, especially for readers lacking adequate prior knowledge. Just as the process of knowledge accumulation is time-consuming for human readers, it is resource-demanding to impart rich general domain knowledge into a <a href=https://en.wikipedia.org/wiki/Deep_learning>deep language model</a> via pre-training. Inspired by reading strategies identified in <a href=https://en.wikipedia.org/wiki/Cognitive_science>cognitive science</a>, and given limited computational resources-just a pre-trained model and a fixed number of training instances-we propose three general strategies aimed to improve non-extractive machine reading comprehension (MRC): (i) BACK AND FORTH READING that considers both the original and reverse order of an input sequence, (ii) HIGHLIGHTING, which adds a trainable embedding to the text embedding of tokens that are relevant to the question and candidate answers, and (iii) SELF-ASSESSMENT that generates practice questions and candidate answers directly from the text in an unsupervised manner. By fine-tuning a pre-trained language model (Radford et al., 2018) with our proposed strategies on the largest general domain multiple-choice MRC dataset RACE, we obtain a 5.8 % absolute increase in accuracy over the previous best result achieved by the same pre-trained model fine-tuned on RACE without the use of strategies.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K19-1030.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K19-1030 data-toggle=collapse aria-expanded=false aria-controls=abstract-K19-1030 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/K19-1030/>Improving Pre-Trained Multilingual Model with Vocabulary Expansion</a></strong><br><a href=/people/h/hai-wang/>Hai Wang</a>
|
<a href=/people/d/dian-yu/>Dian Yu</a>
|
<a href=/people/k/kai-sun/>Kai Sun</a>
|
<a href=/people/j/jianshu-chen/>Jianshu Chen</a>
|
<a href=/people/d/dong-yu/>Dong Yu</a><br><a href=/volumes/K19-1/ class=text-muted>Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K19-1030><div class="card-body p-3 small">Recently, pre-trained language models have achieved remarkable success in a broad range of natural language processing tasks. However, in multilingual setting, it is extremely resource-consuming to pre-train a deep language model over large-scale corpora for each language. Instead of exhaustively pre-training monolingual language models independently, an alternative solution is to pre-train a powerful multilingual deep language model over large-scale corpora in hundreds of languages. However, the <a href=https://en.wikipedia.org/wiki/Vocabulary_size>vocabulary size</a> for each language in such a <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> is relatively small, especially for low-resource languages. This limitation inevitably hinders the performance of these multilingual models on tasks such as <a href=https://en.wikipedia.org/wiki/Sequence_labeling>sequence labeling</a>, wherein in-depth token-level or sentence-level understanding is essential. In this paper, inspired by previous <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> designed for monolingual settings, we investigate two <a href=https://en.wikipedia.org/wiki/Methodology>approaches</a> (i.e., joint mapping and mixture mapping) based on a pre-trained multilingual model BERT for addressing the out-of-vocabulary (OOV) problem on a variety of tasks, including <a href=https://en.wikipedia.org/wiki/Part-of-speech_tagging>part-of-speech tagging</a>, <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>named entity recognition</a>, machine translation quality estimation, and machine reading comprehension. Experimental results show that using mixture mapping is more promising. To the best of our knowledge, this is the first work that attempts to address and discuss the OOV issue in multilingual settings.</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1038.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1038 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1038 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D18-1038.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1038" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D18-1038/>XL-NBT : A Cross-lingual Neural Belief Tracking Framework<span class=acl-fixed-case>XL</span>-<span class=acl-fixed-case>NBT</span>: A Cross-lingual Neural Belief Tracking Framework</a></strong><br><a href=/people/w/wenhu-chen/>Wenhu Chen</a>
|
<a href=/people/j/jianshu-chen/>Jianshu Chen</a>
|
<a href=/people/y/yu-su/>Yu Su</a>
|
<a href=/people/x/xin-wang/>Xin Wang</a>
|
<a href=/people/d/dong-yu/>Dong Yu</a>
|
<a href=/people/x/xifeng-yan/>Xifeng Yan</a>
|
<a href=/people/w/william-yang-wang/>William Yang Wang</a><br><a href=/volumes/D18-1/ class=text-muted>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1038><div class="card-body p-3 small">Task-oriented dialog systems are becoming pervasive, and many companies heavily rely on them to complement <a href=https://en.wikipedia.org/wiki/Intelligent_agent>human agents</a> for customer service in call centers. With globalization, the need for providing cross-lingual customer support becomes more urgent than ever. However, cross-lingual support poses great challengesit requires a large amount of additional annotated data from <a href=https://en.wikipedia.org/wiki/First_language>native speakers</a>. In order to bypass the expensive human annotation and achieve the first step towards the ultimate goal of building a universal dialog system, we set out to build a cross-lingual state tracking framework. Specifically, we assume that there exists a source language with dialog belief tracking annotations while the target languages have no annotated dialog data of any form. Then, we pre-train a state tracker for the source language as a teacher, which is able to exploit easy-to-access parallel data. We then distill and transfer its own knowledge to the student state tracker in target languages. We specifically discuss two types of common parallel resources : <a href=https://en.wikipedia.org/wiki/Bilingual_corpus>bilingual corpus</a> and <a href=https://en.wikipedia.org/wiki/Bilingual_dictionary>bilingual dictionary</a>, and design different transfer learning strategies accordingly. Experimentally, we successfully use English state tracker as the teacher to transfer its knowledge to both Italian and German trackers and achieve promising results.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S18-1186.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S18-1186 data-toggle=collapse aria-expanded=false aria-controls=abstract-S18-1186 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/S18-1186/>BLCU_NLP at SemEval-2018 Task 12 : An Ensemble Model for Argument Reasoning Based on Hierarchical Attention<span class=acl-fixed-case>BLCU</span>_<span class=acl-fixed-case>NLP</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2018 Task 12: An Ensemble Model for Argument Reasoning Based on Hierarchical Attention</a></strong><br><a href=/people/m/meiqian-zhao/>Meiqian Zhao</a>
|
<a href=/people/c/chunhua-liu/>Chunhua Liu</a>
|
<a href=/people/l/lu-liu/>Lu Liu</a>
|
<a href=/people/y/yan-zhao/>Yan Zhao</a>
|
<a href=/people/d/dong-yu/>Dong Yu</a><br><a href=/volumes/S18-1/ class=text-muted>Proceedings of The 12th International Workshop on Semantic Evaluation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S18-1186><div class="card-body p-3 small">To comprehend an argument and fill the gap between claims and reasons, it is vital to find the implicit supporting warrants behind. In this paper, we propose a hierarchical attention model to identify the right warrant which explains why the reason stands for the claim. Our model focuses not only on the similar part between warrants and other information but also on the contradictory part between two opposing warrants. In addition, we use the ensemble method for different <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieves an <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of 61 %, ranking second in this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. Experimental results demonstrate that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is effective to make correct choices.</div></div><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S17-1010.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S17-1010 data-toggle=collapse aria-expanded=false aria-controls=abstract-S17-1010 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/S17-1010/>Semantic Frame Labeling with Target-based Neural Model</a></strong><br><a href=/people/y/yukun-feng/>Yukun Feng</a>
|
<a href=/people/d/dong-yu/>Dong Yu</a>
|
<a href=/people/j/jian-xu/>Jian Xu</a>
|
<a href=/people/c/chunhua-liu/>Chunhua Liu</a><br><a href=/volumes/S17-1/ class=text-muted>Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (*SEM 2017)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S17-1010><div class="card-body p-3 small">This paper explores the automatic learning of <a href=https://en.wikipedia.org/wiki/Distributed_representation>distributed representations</a> of the target&#8217;s context for semantic frame labeling with target-based neural model. We constrain the whole sentence as the <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a>&#8217;s input without <a href=https://en.wikipedia.org/wiki/Feature_extraction>feature extraction</a> from the sentence. This is different from many previous works in which local feature extraction of the targets is widely used. This constraint makes the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> harder, especially with long sentences, but also makes our <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> easily applicable to a range of resources and other similar tasks. We evaluate our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> on several resources and get the state-of-the-art result on subtask 2 of SemEval 2015 task 15. Finally, we extend the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> to word-sense disambiguation task and we also achieve a strong result in comparison to state-of-the-art work.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Dong+Yu" title="Search for 'Dong Yu' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/j/jianshu-chen/ class=align-middle>Jianshu Chen</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/people/c/chunhua-liu/ class=align-middle>Chunhua Liu</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/people/l/linfeng-song/ class=align-middle>Linfeng Song</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/k/kun-xu/ class=align-middle>Kun Xu</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/k/kai-sun/ class=align-middle>Kai Sun</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/d/dian-yu/ class=align-middle>Dian Yu</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/w/wanying-xie/ class=align-middle>Wanying Xie</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/y/yue-zhang/ class=align-middle>Yue Zhang</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/c/claire-cardie/ class=align-middle>Claire Cardie</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/y/yang-feng/ class=align-middle>Yang Feng</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/shuhao-gu/ class=align-middle>Shuhao Gu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/ante-wang/ class=align-middle>Ante Wang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jinsong-su/ class=align-middle>Jinsong Su</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yubin-ge/ class=align-middle>Yubin Ge</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/w/wenhu-chen/ class=align-middle>Wenhu Chen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yu-su/ class=align-middle>Yu Su</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/x/xin-wang/ class=align-middle>Xin Wang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/x/xifeng-yan/ class=align-middle>Xifeng Yan</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/w/william-yang-wang/ class=align-middle>William Yang Wang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jie-hao/ class=align-middle>Jie Hao</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/liwei-wang/ class=align-middle>Liwei Wang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/zhaopeng-tu/ class=align-middle>Zhaopeng Tu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yukun-feng/ class=align-middle>Yukun Feng</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jian-xu/ class=align-middle>Jian Xu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yejin-choi/ class=align-middle>Yejin Choi</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/ruoyao-yang/ class=align-middle>Ruoyao Yang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/meiqian-zhao/ class=align-middle>Meiqian Zhao</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/lu-liu/ class=align-middle>Lu Liu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yan-zhao/ class=align-middle>Yan Zhao</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/hai-wang/ class=align-middle>Hai Wang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/acl/ class=align-middle>ACL</a><span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/venues/emnlp/ class=align-middle>EMNLP</a><span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/venues/semeval/ class=align-middle>SemEval</a><span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/venues/tacl/ class=align-middle>TACL</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/naacl/ class=align-middle>NAACL</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-venues aria-expanded=false aria-controls=more-venues>show all...</li><div class="collapse border-top" id=more-venues><li class=list-group-item><a href=/venues/conll/ class=align-middle>CoNLL</a><span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>