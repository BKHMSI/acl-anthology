<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Didier Schwab - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Didier</span> <span class=font-weight-bold>Schwab</span></h2><hr><div class=row><div class=col-lg-9><h4>2022</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-acl.197.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-acl--197 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-acl.197 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.findings-acl.197" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2022.findings-acl.197/>Automatic Speech Recognition and Query By Example for Creole Languages Documentation</a></strong><br><a href=/people/c/cecile-macaire/>Cécile Macaire</a>
|
<a href=/people/d/didier-schwab/>Didier Schwab</a>
|
<a href=/people/b/benjamin-lecouteux/>Benjamin Lecouteux</a>
|
<a href=/people/e/emmanuel-schang/>Emmanuel Schang</a><br><a href=/volumes/2022.findings-acl/ class=text-muted>Findings of the Association for Computational Linguistics: ACL 2022</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-acl--197><div class="card-body p-3 small">We investigate the exploitation of self supervised models for two <a href=https://en.wikipedia.org/wiki/Creole_language>Creole languages</a> with few resources Gwadloupyen and Morisien Automatic language processing tools are almost non existent for these two languages We propose to use about one hour of annotated data to design an automatic speech recognition system for each language We evaluate how much data is needed to obtain a query by example system that is usable by linguists Moreover our experiments show that multilingual self supervised models are not necessarily the most efficient for <a href=https://en.wikipedia.org/wiki/Creole_language>Creole languages</a></div></div><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.314.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--314 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.314 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.coling-main.314" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.314/>Dual-decoder Transformer for Joint Automatic Speech Recognition and Multilingual Speech Translation</a></strong><br><a href=/people/h/hang-le/>Hang Le</a>
|
<a href=/people/j/juan-pino/>Juan Pino</a>
|
<a href=/people/c/changhan-wang/>Changhan Wang</a>
|
<a href=/people/j/jiatao-gu/>Jiatao Gu</a>
|
<a href=/people/d/didier-schwab/>Didier Schwab</a>
|
<a href=/people/l/laurent-besacier/>Laurent Besacier</a><br><a href=/volumes/2020.coling-main/ class=text-muted>Proceedings of the 28th International Conference on Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--314><div class="card-body p-3 small">We introduce dual-decoder Transformer, a new model architecture that jointly performs automatic speech recognition (ASR) and multilingual speech translation (ST). Our models are based on the original Transformer architecture (Vaswani et al., 2017) but consist of two decoders, each responsible for one task (ASR or ST). Our major contribution lies in how these <a href=https://en.wikipedia.org/wiki/Code>decoders</a> interact with each other : one decoder can attend to different information sources from the other via a dual-attention mechanism. We propose two variants of these architectures corresponding to two different levels of dependencies between the <a href=https://en.wikipedia.org/wiki/Code>decoders</a>, called the parallel and cross dual-decoder Transformers, respectively. Extensive experiments on the MuST-C dataset show that our models outperform the previously-reported highest <a href=https://en.wikipedia.org/wiki/Translation>translation</a> performance in the multilingual settings, and outperform as well bilingual one-to-one results. Furthermore, our parallel models demonstrate no trade-off between ASR and ST compared to the vanilla multi-task architecture. Our code and pre-trained models are available at https://github.com/formiel/speech-translation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.237.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--237 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.237 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.lrec-1.237" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.237/>WIKIR : A Python Toolkit for Building a Large-scale Wikipedia-based English Information Retrieval Dataset<span class=acl-fixed-case>WIKIR</span>: A Python Toolkit for Building a Large-scale <span class=acl-fixed-case>W</span>ikipedia-based <span class=acl-fixed-case>E</span>nglish Information Retrieval Dataset</a></strong><br><a href=/people/j/jibril-frej/>Jibril Frej</a>
|
<a href=/people/d/didier-schwab/>Didier Schwab</a>
|
<a href=/people/j/jean-pierre-chevallet/>Jean-Pierre Chevallet</a><br><a href=/volumes/2020.lrec-1/ class=text-muted>Proceedings of the 12th Language Resources and Evaluation Conference</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--237><div class="card-body p-3 small">Over the past years, <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning methods</a> allowed for new state-of-the-art results in ad-hoc information retrieval. However such <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> usually require large amounts of annotated data to be effective. Since most standard ad-hoc information retrieval datasets publicly available for academic research (e.g. Robust04, ClueWeb09) have at most 250 annotated queries, the recent <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning models</a> for <a href=https://en.wikipedia.org/wiki/Information_retrieval>information retrieval</a> perform poorly on these datasets. These <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> (e.g. DUET, Conv-KNRM) are trained and evaluated on data collected from commercial search engines not publicly available for academic research which is a problem for reproducibility and the advancement of research. In this paper, we propose WIKIR : an <a href=https://en.wikipedia.org/wiki/Open-source_software>open-source toolkit</a> to automatically build large-scale English information retrieval datasets based on <a href=https://en.wikipedia.org/wiki/Wikipedia>Wikipedia</a>. WIKIR is publicly available on GitHub. We also provide wikIR59k : a large-scale publicly available dataset that contains 59,252 queries and 2,617,003 (query, relevant documents) pairs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.302.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--302 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.302 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.lrec-1.302" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.302/>FlauBERT : Unsupervised Language Model Pre-training for <a href=https://en.wikipedia.org/wiki/French_language>French</a><span class=acl-fixed-case>F</span>lau<span class=acl-fixed-case>BERT</span>: Unsupervised Language Model Pre-training for <span class=acl-fixed-case>F</span>rench</a></strong><br><a href=/people/h/hang-le/>Hang Le</a>
|
<a href=/people/l/loic-vial/>Loïc Vial</a>
|
<a href=/people/j/jibril-frej/>Jibril Frej</a>
|
<a href=/people/v/vincent-segonne/>Vincent Segonne</a>
|
<a href=/people/m/maximin-coavoux/>Maximin Coavoux</a>
|
<a href=/people/b/benjamin-lecouteux/>Benjamin Lecouteux</a>
|
<a href=/people/a/alexandre-allauzen/>Alexandre Allauzen</a>
|
<a href=/people/b/benoit-crabbe/>Benoit Crabbé</a>
|
<a href=/people/l/laurent-besacier/>Laurent Besacier</a>
|
<a href=/people/d/didier-schwab/>Didier Schwab</a><br><a href=/volumes/2020.lrec-1/ class=text-muted>Proceedings of the 12th Language Resources and Evaluation Conference</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--302><div class="card-body p-3 small">Language models have become a key step to achieve state-of-the art results in many different Natural Language Processing (NLP) tasks. Leveraging the huge amount of unlabeled texts nowadays available, they provide an efficient way to pre-train continuous word representations that can be fine-tuned for a downstream task, along with their <a href=https://en.wikipedia.org/wiki/Context_(language_use)>contextualization</a> at the sentence level. This has been widely demonstrated for <a href=https://en.wikipedia.org/wiki/English_language>English</a> using contextualized representations (Dai and Le, 2015 ; Peters et al., 2018 ; Howard and Ruder, 2018 ; Radford et al., 2018 ; Devlin et al., 2019 ; Yang et al., 2019b). In this paper, we introduce and share FlauBERT, a <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> learned on a very large and heterogeneous French corpus. Models of different sizes are trained using the new CNRS (French National Centre for Scientific Research) Jean Zay supercomputer. We apply our French language models to diverse NLP tasks (text classification, paraphrasing, natural language inference, parsing, word sense disambiguation) and show that most of the time they outperform other pre-training approaches. Different versions of FlauBERT as well as a unified evaluation protocol for the downstream tasks, called FLUE (French Language Understanding Evaluation), are shared to the research community for further reproducible experiments in French NLP.</div></div><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2019.gwc-1.14.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2019--gwc-1--14 data-toggle=collapse aria-expanded=false aria-controls=abstract-2019.gwc-1.14 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2019.gwc-1.14" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2019.gwc-1.14/>Sense Vocabulary Compression through the Semantic Knowledge of <a href=https://en.wikipedia.org/wiki/WordNet>WordNet</a> for Neural Word Sense Disambiguation<span class=acl-fixed-case>W</span>ord<span class=acl-fixed-case>N</span>et for Neural Word Sense Disambiguation</a></strong><br><a href=/people/l/loic-vial/>Loïc Vial</a>
|
<a href=/people/b/benjamin-lecouteux/>Benjamin Lecouteux</a>
|
<a href=/people/d/didier-schwab/>Didier Schwab</a><br><a href=/volumes/2019.gwc-1/ class=text-muted>Proceedings of the 10th Global Wordnet Conference</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2019--gwc-1--14><div class="card-body p-3 small">In this article, we tackle the issue of the limited quantity of manually sense annotated corpora for the task of <a href=https://en.wikipedia.org/wiki/Word_sense_disambiguation>word sense disambiguation</a>, by exploiting the semantic relationships between senses such as <a href=https://en.wikipedia.org/wiki/Synonym>synonymy</a>, <a href=https://en.wikipedia.org/wiki/Hypernymy>hypernymy</a> and <a href=https://en.wikipedia.org/wiki/Hyponymy_and_hypernymy>hyponymy</a>, in order to compress the sense vocabulary of <a href=https://en.wikipedia.org/wiki/Princeton_WordNet>Princeton WordNet</a>, and thus reduce the number of different sense tags that must be observed to disambiguate all words of the lexical database. We propose two different methods that greatly reduce the size of neural WSD models, with the benefit of improving their coverage without additional <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training data</a>, and without impacting their <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>precision</a>. In addition to our methods, we present a WSD system which relies on pre-trained BERT word vectors in order to achieve results that significantly outperforms the state of the art on all WSD evaluation tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2019.iwslt-1.11.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2019--iwslt-1--11 data-toggle=collapse aria-expanded=false aria-controls=abstract-2019.iwslt-1.11 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2019.iwslt-1.11/>The LIG system for the English-Czech Text Translation Task of IWSLT 2019<span class=acl-fixed-case>LIG</span> system for the <span class=acl-fixed-case>E</span>nglish-<span class=acl-fixed-case>C</span>zech Text Translation Task of <span class=acl-fixed-case>IWSLT</span> 2019</a></strong><br><a href=/people/l/loic-vial/>Loïc Vial</a>
|
<a href=/people/b/benjamin-lecouteux/>Benjamin Lecouteux</a>
|
<a href=/people/d/didier-schwab/>Didier Schwab</a>
|
<a href=/people/h/hang-le/>Hang Le</a>
|
<a href=/people/l/laurent-besacier/>Laurent Besacier</a><br><a href=/volumes/2019.iwslt-1/ class=text-muted>Proceedings of the 16th International Conference on Spoken Language Translation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2019--iwslt-1--11><div class="card-body p-3 small">In this paper, we present our submission for the English to Czech Text Translation Task of IWSLT 2019. Our system aims to study how pre-trained language models, used as input embeddings, can improve a specialized machine translation system trained on few data. Therefore, we implemented a Transformer-based encoder-decoder neural system which is able to use the output of a pre-trained language model as input embeddings, and we compared its performance under three configurations : 1) without any pre-trained language model (constrained), 2) using a <a href=https://en.wikipedia.org/wiki/Language_model>language model</a> trained on the monolingual parts of the allowed English-Czech data (constrained), and 3) using a <a href=https://en.wikipedia.org/wiki/Language_model>language model</a> trained on a large quantity of external monolingual data (unconstrained). We used BERT as external pre-trained language model (configuration 3), and BERT architecture for training our own <a href=https://en.wikipedia.org/wiki/Language_model>language model</a> (configuration 2). Regarding the training data, we trained our MT system on a small quantity of parallel text : one set only consists of the provided MuST-C corpus, and the other set consists of the MuST-C corpus and the News Commentary corpus from WMT. We observed that using the external pre-trained BERT improves the scores of our <a href=https://en.wikipedia.org/wiki/System>system</a> by +0.8 to +1.5 of BLEU on our development set, and +0.97 to +1.94 of BLEU on the test set. However, using our own <a href=https://en.wikipedia.org/wiki/Language_model>language model</a> trained only on the allowed parallel data seems to improve the <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> performances only when the system is trained on the smallest dataset.</div></div><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1303.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1303 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1303 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1303/>Semantic Similarity of Arabic Sentences with Word Embeddings<span class=acl-fixed-case>A</span>rabic Sentences with Word Embeddings</a></strong><br><a href=/people/e/el-moatez-billah-nagoudi/>El Moatez Billah Nagoudi</a>
|
<a href=/people/d/didier-schwab/>Didier Schwab</a><br><a href=/volumes/W17-13/ class=text-muted>Proceedings of the Third Arabic Natural Language Processing Workshop</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1303><div class="card-body p-3 small">Semantic textual similarity is the basis of countless applications and plays an important role in diverse areas, such as <a href=https://en.wikipedia.org/wiki/Information_retrieval>information retrieval</a>, <a href=https://en.wikipedia.org/wiki/Plagiarism_detection>plagiarism detection</a>, <a href=https://en.wikipedia.org/wiki/Information_extraction>information extraction</a> and <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>. This article proposes an innovative word embedding-based system devoted to calculate the <a href=https://en.wikipedia.org/wiki/Semantic_similarity>semantic similarity</a> in <a href=https://en.wikipedia.org/wiki/Arabic_grammar>Arabic sentences</a>. The main idea is to exploit <a href=https://en.wikipedia.org/wiki/Vector_space>vectors</a> as word representations in a <a href=https://en.wikipedia.org/wiki/Dimension_(vector_space)>multidimensional space</a> in order to capture the semantic and syntactic properties of words. IDF weighting and Part-of-Speech tagging are applied on the examined sentences to support the identification of words that are highly descriptive in each sentence. The performance of our proposed <a href=https://en.wikipedia.org/wiki/System>system</a> is confirmed through the <a href=https://en.wikipedia.org/wiki/Pearson_correlation_coefficient>Pearson correlation</a> between our assigned semantic similarity scores and <a href=https://en.wikipedia.org/wiki/Judgement>human judgments</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2502.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2502 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2502 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W17-2502.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W17-2502" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W17-2502/>Deep Investigation of Cross-Language Plagiarism Detection Methods</a></strong><br><a href=/people/j/jeremy-ferrero/>Jérémy Ferrero</a>
|
<a href=/people/l/laurent-besacier/>Laurent Besacier</a>
|
<a href=/people/d/didier-schwab/>Didier Schwab</a>
|
<a href=/people/f/frederic-agnes/>Frédéric Agnès</a><br><a href=/volumes/W17-25/ class=text-muted>Proceedings of the 10th Workshop on Building and Using Comparable Corpora</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2502><div class="card-body p-3 small">This paper is a deep investigation of cross-language plagiarism detection methods on a new recently introduced <a href=https://en.wikipedia.org/wiki/Open_data>open dataset</a>, which contains parallel and comparable collections of documents with multiple characteristics (different genres, languages and sizes of texts). We investigate cross-language plagiarism detection methods for 6 language pairs on 2 granularities of text units in order to draw robust conclusions on the best methods while deeply analyzing correlations across document styles and languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S17-2012.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S17-2012 data-toggle=collapse aria-expanded=false aria-controls=abstract-S17-2012 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/S17-2012.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/S17-2012/>CompiLIG at SemEval-2017 Task 1 : Cross-Language Plagiarism Detection Methods for <a href=https://en.wikipedia.org/wiki/Semantic_similarity>Semantic Textual Similarity</a><span class=acl-fixed-case>C</span>ompi<span class=acl-fixed-case>LIG</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2017 Task 1: Cross-Language Plagiarism Detection Methods for Semantic Textual Similarity</a></strong><br><a href=/people/j/jeremy-ferrero/>Jérémy Ferrero</a>
|
<a href=/people/l/laurent-besacier/>Laurent Besacier</a>
|
<a href=/people/d/didier-schwab/>Didier Schwab</a>
|
<a href=/people/f/frederic-agnes/>Frédéric Agnès</a><br><a href=/volumes/S17-2/ class=text-muted>Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S17-2012><div class="card-body p-3 small">We present our submitted <a href=https://en.wikipedia.org/wiki/System>systems</a> for Semantic Textual Similarity (STS) Track 4 at SemEval-2017. Given a pair of Spanish-English sentences, each <a href=https://en.wikipedia.org/wiki/System>system</a> must estimate their <a href=https://en.wikipedia.org/wiki/Semantic_similarity>semantic similarity</a> by a score between 0 and 5. In our submission, we use syntax-based, dictionary-based, context-based, and MT-based methods. We also combine these <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> in unsupervised and supervised way. Our best run ranked 1st on track 4a with a correlation of 83.02 % with human annotations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S17-2017.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S17-2017 data-toggle=collapse aria-expanded=false aria-controls=abstract-S17-2017 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/S17-2017/>LIM-LIG at SemEval-2017 Task1 : Enhancing the Semantic Similarity for Arabic Sentences with Vectors Weighting<span class=acl-fixed-case>LIM</span>-<span class=acl-fixed-case>LIG</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2017 Task1: Enhancing the Semantic Similarity for <span class=acl-fixed-case>A</span>rabic Sentences with Vectors Weighting</a></strong><br><a href=/people/e/el-moatez-billah-nagoudi/>El Moatez Billah Nagoudi</a>
|
<a href=/people/j/jeremy-ferrero/>Jérémy Ferrero</a>
|
<a href=/people/d/didier-schwab/>Didier Schwab</a><br><a href=/volumes/S17-2/ class=text-muted>Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S17-2017><div class="card-body p-3 small">This article describes our proposed <a href=https://en.wikipedia.org/wiki/System>system</a> named LIM-LIG. This system is designed for SemEval 2017 Task1 : Semantic Textual Similarity (Track1). LIM-LIG proposes an innovative enhancement to word embedding-based model devoted to measure the <a href=https://en.wikipedia.org/wiki/Semantic_similarity>semantic similarity</a> in Arabic sentences. The main idea is to exploit the word representations as vectors in a <a href=https://en.wikipedia.org/wiki/Dimension_(vector_space)>multidimensional space</a> to capture the semantic and syntactic properties of words. IDF weighting and Part-of-Speech tagging are applied on the examined sentences to support the identification of words that are highly descriptive in each sentence. LIM-LIG system achieves a <a href=https://en.wikipedia.org/wiki/Pearson_correlation_coefficient>Pearson&#8217;s correlation</a> of 0.74633, ranking 2nd among all participants in the Arabic monolingual pairs STS task organized within the SemEval 2017 evaluation campaign</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2066.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2066 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2066 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/E17-2066.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/E17-2066/>Using Word Embedding for Cross-Language Plagiarism Detection</a></strong><br><a href=/people/j/jeremy-ferrero/>Jérémy Ferrero</a>
|
<a href=/people/l/laurent-besacier/>Laurent Besacier</a>
|
<a href=/people/d/didier-schwab/>Didier Schwab</a>
|
<a href=/people/f/frederic-agnes/>Frédéric Agnès</a><br><a href=/volumes/E17-2/ class=text-muted>Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2066><div class="card-body p-3 small">This paper proposes to use distributed representation of words (word embeddings) in cross-language textual similarity detection. The main contributions of this paper are the following : (a) we introduce new cross-language similarity detection methods based on distributed representation of words ; (b) we combine the different methods proposed to verify their complementarity and finally obtain an overall F1 score of 89.15 % for English-French similarity detection at chunk level (88.5 % at sentence level) on a very challenging corpus.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Didier+Schwab" title="Search for 'Didier Schwab' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/l/laurent-besacier/ class=align-middle>Laurent Besacier</a>
<span class="badge badge-secondary align-middle ml-2">6</span></li><li class=list-group-item><a href=/people/j/jeremy-ferrero/ class=align-middle>Jérémy Ferrero</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/people/b/benjamin-lecouteux/ class=align-middle>Benjamin Lecouteux</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/people/f/frederic-agnes/ class=align-middle>Frédéric Agnès</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/l/loic-vial/ class=align-middle>Loïc Vial</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/h/hang-le/ class=align-middle>Hang Le</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/e/el-moatez-billah-nagoudi/ class=align-middle>El Moatez Billah Nagoudi</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/j/jibril-frej/ class=align-middle>Jibril Frej</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/c/cecile-macaire/ class=align-middle>Cécile Macaire</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/e/emmanuel-schang/ class=align-middle>Emmanuel Schang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/juan-pino/ class=align-middle>Juan Pino</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/changhan-wang/ class=align-middle>Changhan Wang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jiatao-gu/ class=align-middle>Jiatao Gu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jean-pierre-chevallet/ class=align-middle>Jean-Pierre Chevallet</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/v/vincent-segonne/ class=align-middle>Vincent Segonne</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/maximin-coavoux/ class=align-middle>Maximin Coavoux</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/alexandre-allauzen/ class=align-middle>Alexandre Allauzen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/b/benoit-crabbe/ class=align-middle>Benoit Crabbé</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/ws/ class=align-middle>WS</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/semeval/ class=align-middle>SemEval</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/lrec/ class=align-middle>LREC</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/gwc/ class=align-middle>GWC</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/findings/ class=align-middle>Findings</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-venues aria-expanded=false aria-controls=more-venues>show all...</li><div class="collapse border-top" id=more-venues><li class=list-group-item><a href=/venues/coling/ class=align-middle>COLING</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/iwslt/ class=align-middle>IWSLT</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/eacl/ class=align-middle>EACL</a><span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>