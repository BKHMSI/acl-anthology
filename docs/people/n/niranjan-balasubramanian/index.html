<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Niranjan Balasubramanian - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Niranjan</span> <span class=font-weight-bold>Balasubramanian</span></h2><hr><div class=row><div class=col-lg-9><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-short.76.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-short--76 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-short.76 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-short.76/>Do n’t Let Discourse Confine Your Model : Sequence Perturbations for Improved Event Language Models</a></strong><br><a href=/people/m/mahnaz-koupaee/>Mahnaz Koupaee</a>
|
<a href=/people/g/greg-durrett/>Greg Durrett</a>
|
<a href=/people/n/nathanael-chambers/>Nathanael Chambers</a>
|
<a href=/people/n/niranjan-balasubramanian/>Niranjan Balasubramanian</a><br><a href=/volumes/2021.acl-short/ class=text-muted>Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-short--76><div class="card-body p-3 small">Event language models represent plausible sequences of events. Most existing approaches train <a href=https://en.wikipedia.org/wiki/Autoregressive_model>autoregressive models</a> on text, which successfully capture <a href=https://en.wikipedia.org/wiki/Co-occurrence>event co-occurrence</a> but unfortunately constrain the <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> to follow the <a href=https://en.wikipedia.org/wiki/Discourse_analysis>discourse order</a> in which events are presented. Other domains may employ different discourse orders, and for many applications, we may care about different notions of <a href=https://en.wikipedia.org/wiki/Partially_ordered_set>ordering</a> (e.g., temporal) or not care about <a href=https://en.wikipedia.org/wiki/Partially_ordered_set>ordering</a> at all (e.g., when predicting related events in a schema). We propose a simple yet surprisingly effective strategy for improving event language models by perturbing event sequences so we can relax model dependence on text order. Despite generating completely synthetic event orderings, we show that this technique improves the performance of the event language models on both <a href=https://en.wikipedia.org/wiki/Application_software>applications</a> and out-of-domain events data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.starsem-1.15.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--starsem-1--15 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.starsem-1.15 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.starsem-1.15/>Toward Diverse Precondition Generation</a></strong><br><a href=/people/h/heeyoung-kwon/>Heeyoung Kwon</a>
|
<a href=/people/n/nathanael-chambers/>Nathanael Chambers</a>
|
<a href=/people/n/niranjan-balasubramanian/>Niranjan Balasubramanian</a><br><a href=/volumes/2021.starsem-1/ class=text-muted>Proceedings of *SEM 2021: The Tenth Joint Conference on Lexical and Computational Semantics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--starsem-1--15><div class="card-body p-3 small">A typical goal for <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>language understanding</a> is to logically connect the events of a discourse, but often connective events are not described due to their commonsense nature. In order to address this deficit, we focus here on generating precondition events. Precondition generation can be framed as a sequence-to-sequence problem : given a target event, generate a possible precondition. However, in most real-world scenarios, an event can have several preconditions, which is not always suitable for standard seq2seq frameworks. We propose DiP, the Diverse Precondition generation system that can generate unique and diverse preconditions. DiP consists of three stages of the generative process an event sampler, a candidate generator, and a post-processor. The event sampler provides control codes (precondition triggers) which the candidate generator uses to focus its generation. Post-processing further improves the results through <a href=https://en.wikipedia.org/wiki/Ranking>re-ranking</a> and <a href=https://en.wikipedia.org/wiki/Filter_(software)>filtering</a>. Unlike other conditional generation systems, DiP automatically generates <a href=https://en.wikipedia.org/wiki/Control_code>control codes</a> without training on diverse examples. Analysis reveals that DiP improves the diversity of preconditions significantly compared to a beam search baseline. Also, manual evaluation shows that DiP generates more <a href=https://en.wikipedia.org/wiki/Precursor_(chemistry)>preconditions</a> than a strong nucleus sampling baseline.</div></div><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.acl-main.411.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--acl-main--411 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.acl-main.411 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://slideslive.com/38929429 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.acl-main.411" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.acl-main.411/>DeFormer : Decomposing Pre-trained Transformers for Faster Question Answering<span class=acl-fixed-case>D</span>e<span class=acl-fixed-case>F</span>ormer: Decomposing Pre-trained Transformers for Faster Question Answering</a></strong><br><a href=/people/q/qingqing-cao/>Qingqing Cao</a>
|
<a href=/people/h/harsh-trivedi/>Harsh Trivedi</a>
|
<a href=/people/a/aruna-balasubramanian/>Aruna Balasubramanian</a>
|
<a href=/people/n/niranjan-balasubramanian/>Niranjan Balasubramanian</a><br><a href=/volumes/2020.acl-main/ class=text-muted>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--acl-main--411><div class="card-body p-3 small">Transformer-based QA models use input-wide self-attention i.e. across both the question and the input passage at all layers, causing them to be slow and memory-intensive. It turns out that we can get by without input-wide self-attention at all layers, especially in the lower layers. We introduce DeFormer, a decomposed transformer, which substitutes the full self-attention with question-wide and passage-wide self-attentions in the lower layers. This allows for question-independent processing of the input text representations, which in turn enables pre-computing passage representations reducing runtime compute drastically. Furthermore, because DeFormer is largely similar to the original model, we can initialize DeFormer with the pre-training weights of a standard transformer, and directly fine-tune on the target QA dataset. We show DeFormer versions of <a href=https://en.wikipedia.org/wiki/BERT>BERT</a> and XLNet can be used to speed up <a href=https://en.wikipedia.org/wiki/Quality_assurance>QA</a> by over 4.3x and with simple distillation-based losses they incur only a 1 % drop in <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>. We open source the code at https://github.com/StonyBrookNLP/deformer.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.sustainlp-1.19.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--sustainlp-1--19 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.sustainlp-1.19 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939441 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.sustainlp-1.19" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.sustainlp-1.19/>Towards Accurate and Reliable Energy Measurement of NLP Models<span class=acl-fixed-case>NLP</span> Models</a></strong><br><a href=/people/q/qingqing-cao/>Qingqing Cao</a>
|
<a href=/people/a/aruna-balasubramanian/>Aruna Balasubramanian</a>
|
<a href=/people/n/niranjan-balasubramanian/>Niranjan Balasubramanian</a><br><a href=/volumes/2020.sustainlp-1/ class=text-muted>Proceedings of SustaiNLP: Workshop on Simple and Efficient Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--sustainlp-1--19><div class="card-body p-3 small">Accurate and reliable measurement of energy consumption is critical for making well-informed design choices when choosing and training large scale NLP models. In this work, we show that existing software-based energy estimations are not accurate because they do not take into account hardware differences and how resource utilization affects <a href=https://en.wikipedia.org/wiki/Energy_consumption>energy consumption</a>. We conduct energy measurement experiments with four different <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> for a <a href=https://en.wikipedia.org/wiki/Question_answering>question answering task</a>. We quantify the error of existing software-based energy estimations by using a hardware power meter that provides highly accurate energy measurements. Our key takeaway is the need for a more accurate energy estimation model that takes into account hardware variabilities and the non-linear relationship between resource utilization and <a href=https://en.wikipedia.org/wiki/Energy_consumption>energy consumption</a>. We release the code and data at https://github.com/csarron/sustainlp2020-energy.</div></div><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1072.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1072 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1072 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1072.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D19-1072/>Latent Part-of-Speech Sequences for Neural Machine Translation</a></strong><br><a href=/people/x/xuewen-yang/>Xuewen Yang</a>
|
<a href=/people/y/yingru-liu/>Yingru Liu</a>
|
<a href=/people/d/dongliang-xie/>Dongliang Xie</a>
|
<a href=/people/x/xin-wang/>Xin Wang</a>
|
<a href=/people/n/niranjan-balasubramanian/>Niranjan Balasubramanian</a><br><a href=/volumes/D19-1/ class=text-muted>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1072><div class="card-body p-3 small">Learning target side syntactic structure has been shown to improve Neural Machine Translation (NMT). However, incorporating <a href=https://en.wikipedia.org/wiki/Syntax>syntax</a> through <a href=https://en.wikipedia.org/wiki/Latent_variable>latent variables</a> introduces additional complexity in <a href=https://en.wikipedia.org/wiki/Statistical_inference>inference</a>, as the models need to marginalize over the latent syntactic structures. To avoid this, <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> often resort to <a href=https://en.wikipedia.org/wiki/Greedy_search>greedy search</a> which only allows them to explore a limited portion of the latent space. In this work, we introduce a new <a href=https://en.wikipedia.org/wiki/Latent_variable_model>latent variable model</a>, LaSyn, that captures the co-dependence between <a href=https://en.wikipedia.org/wiki/Syntax>syntax</a> and <a href=https://en.wikipedia.org/wiki/Semantics>semantics</a>, while allowing for effective and efficient inference over the latent space. LaSyn decouples direct dependence between successive latent variables, which allows its decoder to exhaustively search through the latent syntactic choices, while keeping decoding speed proportional to the size of the latent variable vocabulary. We implement LaSyn by modifying a transformer-based NMT system and design a neural expectation maximization algorithm that we regularize with part-of-speech information as the latent sequences. Evaluations on four different MT tasks show that incorporating target side syntax with LaSyn improves both translation quality, and also provides an opportunity to improve diversity.</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-1004.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-1004 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-1004 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-1004" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-1004/>The Fine Line between Linguistic Generalization and Failure in Seq2Seq-Attention Models<span class=acl-fixed-case>S</span>eq2<span class=acl-fixed-case>S</span>eq-Attention Models</a></strong><br><a href=/people/n/noah-weber/>Noah Weber</a>
|
<a href=/people/l/leena-shekhar/>Leena Shekhar</a>
|
<a href=/people/n/niranjan-balasubramanian/>Niranjan Balasubramanian</a><br><a href=/volumes/W18-10/ class=text-muted>Proceedings of the Workshop on Generalization in the Age of Deep Learning</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-1004><div class="card-body p-3 small">Seq2Seq based neural architectures have become the go-to architecture to apply to sequence to sequence language tasks. Despite their excellent performance on these tasks, recent work has noted that these models typically do not fully capture the linguistic structure required to generalize beyond the dense sections of the data distribution (Ettinger et al., 2017), and as such, are likely to fail on examples from the tail end of the distribution (such as inputs that are noisy (Belinkov and Bisk, 2018), or of different length (Bentivogli et al., 2016)). In this paper we look at a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>&#8217;s ability to generalize on a simple symbol rewriting task with a clearly defined structure. We find that the <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a>&#8217;s ability to generalize this structure beyond the <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training distribution</a> depends greatly on the chosen random seed, even when performance on the test set remains the same. This finding suggests that <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>&#8217;s ability to capture generalizable structure is highly sensitive, and more so, this sensitivity may not be apparent when evaluating the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> on standard test sets.</div></div><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1119.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1119 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1119 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/238233454 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D17-1119/>Human Centered NLP with User-Factor Adaptation<span class=acl-fixed-case>NLP</span> with User-Factor Adaptation</a></strong><br><a href=/people/v/veronica-lynn/>Veronica Lynn</a>
|
<a href=/people/y/youngseo-son/>Youngseo Son</a>
|
<a href=/people/v/vivek-kulkarni/>Vivek Kulkarni</a>
|
<a href=/people/n/niranjan-balasubramanian/>Niranjan Balasubramanian</a>
|
<a href=/people/h/h-andrew-schwartz/>H. Andrew Schwartz</a><br><a href=/volumes/D17-1/ class=text-muted>Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1119><div class="card-body p-3 small">We pose the general task of user-factor adaptation adapting supervised learning models to real-valued user factors inferred from a background of their language, reflecting the idea that a piece of text should be understood within the context of the user that wrote it. We introduce a continuous adaptation technique, suited for real-valued user factors that are common in <a href=https://en.wikipedia.org/wiki/Social_science>social science</a> and bringing us closer to personalized NLP, adapting to each user uniquely. We apply this technique with known user factors including age, gender, and personality traits, as well as latent factors, evaluating over five tasks : POS tagging, PP-attachment, <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a>, sarcasm detection, and stance detection. Adaptation provides statistically significant benefits for 3 of the 5 tasks : up to +1.2 points for PP-attachment, +3.4 points for <a href=https://en.wikipedia.org/wiki/Sarcasm>sarcasm</a>, and +3.0 points for <a href=https://en.wikipedia.org/wiki/List_of_human_positions>stance</a>.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Niranjan+Balasubramanian" title="Search for 'Niranjan Balasubramanian' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/n/nathanael-chambers/ class=align-middle>Nathanael Chambers</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/q/qingqing-cao/ class=align-middle>Qingqing Cao</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/a/aruna-balasubramanian/ class=align-middle>Aruna Balasubramanian</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/m/mahnaz-koupaee/ class=align-middle>Mahnaz Koupaee</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/g/greg-durrett/ class=align-middle>Greg Durrett</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/h/harsh-trivedi/ class=align-middle>Harsh Trivedi</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/x/xuewen-yang/ class=align-middle>Xuewen Yang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yingru-liu/ class=align-middle>Yingru Liu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/dongliang-xie/ class=align-middle>Dongliang Xie</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/x/xin-wang/ class=align-middle>Xin Wang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/v/veronica-lynn/ class=align-middle>Veronica Lynn</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/youngseo-son/ class=align-middle>Youngseo Son</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/v/vivek-kulkarni/ class=align-middle>Vivek Kulkarni</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/h-andrew-schwartz/ class=align-middle>H. Andrew Schwartz</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/heeyoung-kwon/ class=align-middle>Heeyoung Kwon</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/n/noah-weber/ class=align-middle>Noah Weber</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/leena-shekhar/ class=align-middle>Leena Shekhar</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/acl/ class=align-middle>ACL</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/emnlp/ class=align-middle>EMNLP</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/starsem/ class=align-middle>*SEM</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/ws/ class=align-middle>WS</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/sustainlp/ class=align-middle>sustainlp</a><span class="badge badge-secondary align-middle ml-2">1</span></li></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>