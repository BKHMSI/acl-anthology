<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Nikola Mrkšić - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Nikola</span> <span class=font-weight-bold>Mrkšić</span></h2><hr><div class=row><div class=col-lg-9><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.88.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--88 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.88 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.88/>ConvFiT : Conversational Fine-Tuning of Pretrained Language Models<span class=acl-fixed-case>ConvFiT:</span> <span class=acl-fixed-case>C</span>onversational Fine-Tuning of Pretrained Language Models</a></strong><br><a href=/people/i/ivan-vulic/>Ivan Vulić</a>
|
<a href=/people/p/pei-hao-su/>Pei-Hao Su</a>
|
<a href=/people/s/samuel-coope/>Samuel Coope</a>
|
<a href=/people/d/daniela-gerz/>Daniela Gerz</a>
|
<a href=/people/p/pawel-budzianowski/>Paweł Budzianowski</a>
|
<a href=/people/i/inigo-casanueva/>Iñigo Casanueva</a>
|
<a href=/people/n/nikola-mrksic/>Nikola Mrkšić</a>
|
<a href=/people/t/tsung-hsien-wen/>Tsung-Hsien Wen</a><br><a href=/volumes/2021.emnlp-main/ class=text-muted>Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--88><div class="card-body p-3 small">Transformer-based language models (LMs) pretrained on large text collections are proven to store a wealth of semantic knowledge. However, 1) they are not effective as sentence encoders when used off-the-shelf, and 2) thus typically lag behind conversationally pretrained (e.g., via response selection) encoders on conversational tasks such as intent detection (ID). In this work, we propose ConvFiT, a simple and efficient two-stage procedure which turns any pretrained LM into a universal conversational encoder (after Stage 1 ConvFiT-ing) and task-specialised sentence encoder (after Stage 2). We demonstrate that 1) full-blown conversational pretraining is not required, and that LMs can be quickly transformed into effective conversational encoders with much smaller amounts of unannotated data ; 2) pretrained LMs can be fine-tuned into task-specialised sentence encoders, optimised for the fine-grained semantics of a particular task. Consequently, such specialised sentence encoders allow for treating ID as a simple semantic similarity task based on interpretable nearest neighbours retrieval. We validate the robustness and versatility of the ConvFiT framework with such similarity-based inference on the standard ID evaluation sets : ConvFiT-ed LMs achieve state-of-the-art ID performance across the board, with particular gains in the most challenging, few-shot setups.</div></div><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-3031.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-3031 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-3031 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-3031/>PolyResponse : A Rank-based Approach to Task-Oriented Dialogue with Application in Restaurant Search and Booking<span class=acl-fixed-case>P</span>oly<span class=acl-fixed-case>R</span>esponse: A Rank-based Approach to Task-Oriented Dialogue with Application in Restaurant Search and Booking</a></strong><br><a href=/people/m/matthew-henderson/>Matthew Henderson</a>
|
<a href=/people/i/ivan-vulic/>Ivan Vulić</a>
|
<a href=/people/i/inigo-casanueva/>Iñigo Casanueva</a>
|
<a href=/people/p/pawel-budzianowski/>Paweł Budzianowski</a>
|
<a href=/people/d/daniela-gerz/>Daniela Gerz</a>
|
<a href=/people/s/sam-coope/>Sam Coope</a>
|
<a href=/people/g/georgios-spithourakis/>Georgios Spithourakis</a>
|
<a href=/people/t/tsung-hsien-wen/>Tsung-Hsien Wen</a>
|
<a href=/people/n/nikola-mrksic/>Nikola Mrkšić</a>
|
<a href=/people/p/pei-hao-su/>Pei-Hao Su</a><br><a href=/volumes/D19-3/ class=text-muted>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP): System Demonstrations</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-3031><div class="card-body p-3 small">We present PolyResponse, a conversational search engine that supports task-oriented dialogue. It is a retrieval-based approach that bypasses the complex multi-component design of traditional task-oriented dialogue systems and the use of explicit semantics in the form of task-specific ontologies. The PolyResponse engine is trained on hundreds of millions of examples extracted from real conversations : it learns what responses are appropriate in different conversational contexts. It then ranks a large index of text and visual responses according to their similarity to the given context, and narrows down the list of relevant entities during the multi-turn conversation. We introduce a restaurant search and booking system powered by the PolyResponse engine, currently available in 8 different languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4101.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4101 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4101 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-4101" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-4101/>A Repository of Conversational Datasets</a></strong><br><a href=/people/m/matthew-henderson/>Matthew Henderson</a>
|
<a href=/people/p/pawel-budzianowski/>Paweł Budzianowski</a>
|
<a href=/people/i/inigo-casanueva/>Iñigo Casanueva</a>
|
<a href=/people/s/sam-coope/>Sam Coope</a>
|
<a href=/people/d/daniela-gerz/>Daniela Gerz</a>
|
<a href=/people/g/girish-kumar/>Girish Kumar</a>
|
<a href=/people/n/nikola-mrksic/>Nikola Mrkšić</a>
|
<a href=/people/g/georgios-spithourakis/>Georgios Spithourakis</a>
|
<a href=/people/p/pei-hao-su/>Pei-Hao Su</a>
|
<a href=/people/i/ivan-vulic/>Ivan Vulić</a>
|
<a href=/people/t/tsung-hsien-wen/>Tsung-Hsien Wen</a><br><a href=/volumes/W19-41/ class=text-muted>Proceedings of the First Workshop on NLP for Conversational AI</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4101><div class="card-body p-3 small">Progress in <a href=https://en.wikipedia.org/wiki/Machine_learning>Machine Learning</a> is often driven by the availability of large datasets, and consistent evaluation metrics for comparing modeling approaches. To this end, we present a repository of conversational datasets consisting of hundreds of millions of examples, and a standardised evaluation procedure for conversational response selection models using 1-of-100 accuracy. The <a href=https://en.wikipedia.org/wiki/Disciplinary_repository>repository</a> contains scripts that allow researchers to reproduce the standard <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>, or to adapt the pre-processing and data filtering steps to their needs. We introduce and evaluate several competitive baselines for conversational response selection, whose implementations are shared in the repository, as well as a neural encoder model that is trained on the entire training set.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1536.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1536 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1536 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-1536/>Training Neural Response Selection for Task-Oriented Dialogue Systems</a></strong><br><a href=/people/m/matthew-henderson/>Matthew Henderson</a>
|
<a href=/people/i/ivan-vulic/>Ivan Vulić</a>
|
<a href=/people/d/daniela-gerz/>Daniela Gerz</a>
|
<a href=/people/i/inigo-casanueva/>Iñigo Casanueva</a>
|
<a href=/people/p/pawel-budzianowski/>Paweł Budzianowski</a>
|
<a href=/people/s/sam-coope/>Sam Coope</a>
|
<a href=/people/g/georgios-spithourakis/>Georgios Spithourakis</a>
|
<a href=/people/t/tsung-hsien-wen/>Tsung-Hsien Wen</a>
|
<a href=/people/n/nikola-mrksic/>Nikola Mrkšić</a>
|
<a href=/people/p/pei-hao-su/>Pei-Hao Su</a><br><a href=/volumes/P19-1/ class=text-muted>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1536><div class="card-body p-3 small">Despite their popularity in the chatbot literature, retrieval-based models have had modest impact on task-oriented dialogue systems, with the main obstacle to their application being the low-data regime of most task-oriented dialogue tasks. Inspired by the recent success of pretraining in language modelling, we propose an effective method for deploying response selection in task-oriented dialogue. To train response selection models for task-oriented dialogue tasks, we propose a novel method which : 1) pretrains the response selection model on large general-domain conversational corpora ; and then 2) fine-tunes the pretrained model for the target dialogue domain, relying only on the small in-domain dataset to capture the nuances of the given dialogue domain. Our evaluation on five diverse application domains, ranging from <a href=https://en.wikipedia.org/wiki/E-commerce>e-commerce</a> to <a href=https://en.wikipedia.org/wiki/Bank>banking</a>, demonstrates the effectiveness of the proposed training method.</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1026.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1026 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1026 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1026" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D18-1026/>Adversarial Propagation and Zero-Shot Cross-Lingual Transfer of Word Vector Specialization</a></strong><br><a href=/people/e/edoardo-maria-ponti/>Edoardo Maria Ponti</a>
|
<a href=/people/i/ivan-vulic/>Ivan Vulić</a>
|
<a href=/people/g/goran-glavas/>Goran Glavaš</a>
|
<a href=/people/n/nikola-mrksic/>Nikola Mrkšić</a>
|
<a href=/people/a/anna-korhonen/>Anna Korhonen</a><br><a href=/volumes/D18-1/ class=text-muted>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1026><div class="card-body p-3 small">Semantic specialization is a process of fine-tuning pre-trained <a href=https://en.wikipedia.org/wiki/Distribution_(mathematics)>distributional word vectors</a> using <a href=https://en.wikipedia.org/wiki/Lexical_analysis>external lexical knowledge</a> (e.g., WordNet) to accentuate a particular <a href=https://en.wikipedia.org/wiki/Semantic_relation>semantic relation</a> in the specialized vector space. While post-processing specialization methods are applicable to arbitrary distributional vectors, they are limited to updating only the vectors of words occurring in external lexicons (i.e., seen words), leaving the vectors of all other words unchanged. We propose a novel <a href=https://en.wikipedia.org/wiki/Scientific_method>approach</a> to specializing the full distributional vocabulary. Our adversarial post-specialization method propagates the external lexical knowledge to the full distributional space. We exploit words seen in the resources as training examples for learning a global specialization function. This function is learned by combining a standard L2-distance loss with a adversarial loss : the adversarial component produces more realistic output vectors. We show the effectiveness and robustness of the proposed method across three languages and on three tasks : <a href=https://en.wikipedia.org/wiki/Similarity_measure>word similarity</a>, dialog state tracking, and lexical simplification. We report consistent improvements over distributional word vectors and vectors specialized by other state-of-the-art specialization frameworks. Finally, we also propose a cross-lingual transfer method for zero-shot specialization which successfully specializes a full target distributional space without any lexical knowledge in the target language and without any bilingual data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1048.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1048 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1048 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N18-1048" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N18-1048/>Post-Specialisation : Retrofitting Vectors of Words Unseen in Lexical Resources</a></strong><br><a href=/people/i/ivan-vulic/>Ivan Vulić</a>
|
<a href=/people/g/goran-glavas/>Goran Glavaš</a>
|
<a href=/people/n/nikola-mrksic/>Nikola Mrkšić</a>
|
<a href=/people/a/anna-korhonen/>Anna Korhonen</a><br><a href=/volumes/N18-1/ class=text-muted>Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1048><div class="card-body p-3 small">Word vector specialisation (also known as retrofitting) is a portable, light-weight approach to fine-tuning arbitrary distributional word vector spaces by injecting external knowledge from rich lexical resources such as <a href=https://en.wikipedia.org/wiki/WordNet>WordNet</a>. By design, these post-processing methods only update the vectors of words occurring in external lexicons, leaving the representations of all unseen words intact. In this paper, we show that constraint-driven vector space specialisation can be extended to unseen words. We propose a novel post-specialisation method that : a) preserves the useful linguistic knowledge for seen words ; while b) propagating this external signal to unseen words in order to improve their vector representations as well. Our post-specialisation approach explicits a non-linear specialisation function in the form of a <a href=https://en.wikipedia.org/wiki/Deep_learning>deep neural network</a> by learning to predict specialised vectors from their original distributional counterparts. The learned <a href=https://en.wikipedia.org/wiki/Function_(mathematics)>function</a> is then used to specialise vectors of unseen words. This approach, applicable to any post-processing model, yields considerable gains over the initial specialisation models both in intrinsic word similarity tasks, and in two downstream tasks : dialogue state tracking and lexical text simplification. The positive effects persist across three languages, demonstrating the importance of specialising the full vocabulary of distributional word vector spaces.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1103.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1103 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1103 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://vimeo.com/282333968 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N18-1103" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/N18-1103/>Specialising <a href=https://en.wikipedia.org/wiki/Lexical_analysis>Word Vectors</a> for Lexical Entailment</a></strong><br><a href=/people/i/ivan-vulic/>Ivan Vulić</a>
|
<a href=/people/n/nikola-mrksic/>Nikola Mrkšić</a><br><a href=/volumes/N18-1/ class=text-muted>Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1103><div class="card-body p-3 small">We present LEAR (Lexical Entailment Attract-Repel), a novel post-processing method that transforms any input word vector space to emphasise the asymmetric relation of lexical entailment (LE), also known as the IS-A or hyponymy-hypernymy relation. By injecting external linguistic constraints (e.g., WordNet links) into the initial <a href=https://en.wikipedia.org/wiki/Vector_space>vector space</a>, the LE specialisation procedure brings true <a href=https://en.wikipedia.org/wiki/Hyponymy_and_hypernymy>hyponymy-hypernymy pairs</a> closer together in the transformed <a href=https://en.wikipedia.org/wiki/Euclidean_space>Euclidean space</a>. The proposed asymmetric distance measure adjusts the norms of word vectors to reflect the actual WordNet-style hierarchy of concepts. Simultaneously, a joint objective enforces <a href=https://en.wikipedia.org/wiki/Semantic_similarity>semantic similarity</a> using the symmetric cosine distance, yielding a <a href=https://en.wikipedia.org/wiki/Vector_space>vector space</a> specialised for both lexical relations at once. LEAR specialisation achieves state-of-the-art performance in the tasks of hypernymy directionality, hypernymy detection, and graded lexical entailment, demonstrating the effectiveness and robustness of the proposed asymmetric specialisation model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-6006.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-6006 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-6006 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/279154253 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N18-6006/>Deep Learning for Conversational AI<span class=acl-fixed-case>AI</span></a></strong><br><a href=/people/p/pei-hao-su/>Pei-Hao Su</a>
|
<a href=/people/n/nikola-mrksic/>Nikola Mrkšić</a>
|
<a href=/people/i/inigo-casanueva/>Iñigo Casanueva</a>
|
<a href=/people/i/ivan-vulic/>Ivan Vulić</a><br><a href=/volumes/N18-6/ class=text-muted>Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Tutorial Abstracts</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-6006><div class="card-body p-3 small">Spoken Dialogue Systems (SDS) have great commercial potential as they promise to revolutionise the way in which humans interact with machines. The advent of <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning</a> led to substantial developments in this area of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP research</a>, and the goal of this tutorial is to familiarise the research community with the recent advances in what some call the most difficult problem in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a>. From a research perspective, the design of spoken dialogue systems provides a number of significant challenges, as these <a href=https://en.wikipedia.org/wiki/System>systems</a> depend on : a) solving several difficult NLP and decision-making tasks ; and b) combining these into a functional dialogue system pipeline. A key long-term goal of dialogue system research is to enable open-domain systems that can converse about arbitrary topics and assist humans with completing a wide range of tasks. Furthermore, such systems need to autonomously learn on-line to improve their performance and recover from errors using both signals from their environment and from implicit and explicit user feedback. While the design of such systems has traditionally been modular, domain and language-specific, advances in <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning</a> have alleviated many of the design problems. The main purpose of this tutorial is to encourage dialogue research in the NLP community by providing the research background, a survey of available resources, and giving key insights to application of state-of-the-art SDS methodology into industry-scale conversational AI systems. We plan to introduce researchers to the pipeline framework for modelling goal-oriented dialogue systems, which includes three key components : 1) <a href=https://en.wikipedia.org/wiki/Language_understanding>Language Understanding</a> ; 2) Dialogue Management ; and 3) Language Generation. The differences between goal-oriented dialogue systems and chat-bot style conversational agents will be explained in order to show the motivation behind the design of both, with the main focus on the pipeline SDS framework. For each key component, we will define the research problem, provide a brief literature review and introduce the current state-of-the-art approaches. Complementary resources (e.g. available datasets and toolkits) will also be discussed. Finally, future work, outstanding challenges, and current industry practices will be presented. All of the presented material will be made available online for future reference.</div></div><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1006.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1006 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1006 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P17-1006.Notes.zip data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234954143 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-1006/>Morph-fitting : Fine-Tuning Word Vector Spaces with Simple Language-Specific Rules</a></strong><br><a href=/people/i/ivan-vulic/>Ivan Vulić</a>
|
<a href=/people/n/nikola-mrksic/>Nikola Mrkšić</a>
|
<a href=/people/r/roi-reichart/>Roi Reichart</a>
|
<a href=/people/d/diarmuid-o-seaghdha/>Diarmuid Ó Séaghdha</a>
|
<a href=/people/s/steve-young/>Steve Young</a>
|
<a href=/people/a/anna-korhonen/>Anna Korhonen</a><br><a href=/volumes/P17-1/ class=text-muted>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1006><div class="card-body p-3 small">Morphologically rich languages accentuate two properties of distributional vector space models : 1) the difficulty of inducing accurate representations for low-frequency word forms ; and 2) insensitivity to distinct lexical relations that have similar distributional signatures. These effects are detrimental for <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>language understanding systems</a>, which may infer that &#8216;inexpensive&#8217; is a rephrasing for &#8216;expensive&#8217; or may not associate &#8216;acquire&#8217; with &#8216;acquires&#8217;. In this work, we propose a novel morph-fitting procedure which moves past the use of curated semantic lexicons for improving distributional vector spaces. Instead, our method injects <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphological constraints</a> generated using simple language-specific rules, pulling <a href=https://en.wikipedia.org/wiki/Inflection>inflectional forms</a> of the same word close together and pushing <a href=https://en.wikipedia.org/wiki/Morphological_derivation>derivational antonyms</a> far apart. In intrinsic evaluation over four languages, we show that our approach : 1) improves low-frequency word estimates ; and 2) boosts the semantic quality of the entire word vector collection. Finally, we show that morph-fitted vectors yield large gains in the downstream task of dialogue state tracking, highlighting the importance of <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphology</a> for tackling long-tail phenomena in language understanding tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1163.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1163 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1163 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-1163/>Neural Belief Tracker : Data-Driven Dialogue State Tracking</a></strong><br><a href=/people/n/nikola-mrksic/>Nikola Mrkšić</a>
|
<a href=/people/d/diarmuid-o-seaghdha/>Diarmuid Ó Séaghdha</a>
|
<a href=/people/t/tsung-hsien-wen/>Tsung-Hsien Wen</a>
|
<a href=/people/b/blaise-thomson/>Blaise Thomson</a>
|
<a href=/people/s/steve-young/>Steve Young</a><br><a href=/volumes/P17-1/ class=text-muted>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1163><div class="card-body p-3 small">One of the core components of modern spoken dialogue systems is the belief tracker, which estimates the user&#8217;s goal at every step of the dialogue. However, most current approaches have difficulty scaling to larger, more complex dialogue domains. This is due to their dependency on either : a) Spoken Language Understanding models that require large amounts of annotated training data ; or b) hand-crafted lexicons for capturing some of the linguistic variation in users&#8217; language. We propose a novel Neural Belief Tracking (NBT) framework which overcomes these problems by building on recent advances in <a href=https://en.wikipedia.org/wiki/Representation_learning>representation learning</a>. NBT models reason over pre-trained word vectors, learning to compose them into distributed representations of user utterances and dialogue context. Our evaluation on two datasets shows that this approach surpasses past limitations, matching the performance of state-of-the-art models which rely on hand-crafted semantic lexicons and outperforming them when such lexicons are not provided.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5509.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5509 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5509 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5509/>Reward-Balancing for Statistical Spoken Dialogue Systems using Multi-objective Reinforcement Learning</a></strong><br><a href=/people/s/stefan-ultes/>Stefan Ultes</a>
|
<a href=/people/p/pawel-budzianowski/>Paweł Budzianowski</a>
|
<a href=/people/i/inigo-casanueva/>Iñigo Casanueva</a>
|
<a href=/people/n/nikola-mrksic/>Nikola Mrkšić</a>
|
<a href=/people/l/lina-m-rojas-barahona/>Lina M. Rojas-Barahona</a>
|
<a href=/people/p/pei-hao-su/>Pei-Hao Su</a>
|
<a href=/people/t/tsung-hsien-wen/>Tsung-Hsien Wen</a>
|
<a href=/people/m/milica-gasic/>Milica Gašić</a>
|
<a href=/people/s/steve-young/>Steve Young</a><br><a href=/volumes/W17-55/ class=text-muted>Proceedings of the 18th Annual SIGdial Meeting on Discourse and Dialogue</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5509><div class="card-body p-3 small">Reinforcement learning is widely used for dialogue policy optimization where the <a href=https://en.wikipedia.org/wiki/Reward_system>reward function</a> often consists of more than one <a href=https://en.wikipedia.org/wiki/Function_(mathematics)>component</a>, e.g., the dialogue success and the dialogue length. In this work, we propose a structured method for finding a good balance between these components by searching for the optimal reward component weighting. To render this search feasible, we use multi-objective reinforcement learning to significantly reduce the number of training dialogues required. We apply our proposed method to find optimized component weights for six domains and compare them to a default baseline.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5512.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5512 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5512 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5512/>Sub-domain Modelling for <a href=https://en.wikipedia.org/wiki/Dialogue_management>Dialogue Management</a> with Hierarchical Reinforcement Learning</a></strong><br><a href=/people/p/pawel-budzianowski/>Paweł Budzianowski</a>
|
<a href=/people/s/stefan-ultes/>Stefan Ultes</a>
|
<a href=/people/p/pei-hao-su/>Pei-Hao Su</a>
|
<a href=/people/n/nikola-mrksic/>Nikola Mrkšić</a>
|
<a href=/people/t/tsung-hsien-wen/>Tsung-Hsien Wen</a>
|
<a href=/people/i/inigo-casanueva/>Iñigo Casanueva</a>
|
<a href=/people/l/lina-m-rojas-barahona/>Lina M. Rojas-Barahona</a>
|
<a href=/people/m/milica-gasic/>Milica Gašić</a><br><a href=/volumes/W17-55/ class=text-muted>Proceedings of the 18th Annual SIGdial Meeting on Discourse and Dialogue</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5512><div class="card-body p-3 small">Human conversation is inherently complex, often spanning many different topics / domains. This makes <a href=https://en.wikipedia.org/wiki/Policy_learning>policy learning</a> for <a href=https://en.wikipedia.org/wiki/Dialogue_system>dialogue systems</a> very challenging. Standard flat reinforcement learning methods do not provide an efficient <a href=https://en.wikipedia.org/wiki/Conceptual_framework>framework</a> for modelling such <a href=https://en.wikipedia.org/wiki/Dialogue>dialogues</a>. In this paper, we focus on the under-explored problem of multi-domain dialogue management. First, we propose a new <a href=https://en.wikipedia.org/wiki/Methodology>method</a> for hierarchical reinforcement learning using the option framework. Next, we show that the proposed <a href=https://en.wikipedia.org/wiki/Architecture>architecture</a> learns faster and arrives at a better <a href=https://en.wikipedia.org/wiki/Policy>policy</a> than the existing flat ones do. Moreover, we show how pretrained policies can be adapted to more complex systems with an additional set of new actions. In doing that, we show that our approach has the potential to facilitate policy optimisation for more sophisticated multi-domain dialogue systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/Q17-1022.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-Q17-1022 data-toggle=collapse aria-expanded=false aria-controls=abstract-Q17-1022 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/Q17-1022/>Semantic Specialization of Distributional Word Vector Spaces using Monolingual and Cross-Lingual Constraints</a></strong><br><a href=/people/n/nikola-mrksic/>Nikola Mrkšić</a>
|
<a href=/people/i/ivan-vulic/>Ivan Vulić</a>
|
<a href=/people/d/diarmuid-o-seaghdha/>Diarmuid Ó Séaghdha</a>
|
<a href=/people/i/ira-leviant/>Ira Leviant</a>
|
<a href=/people/r/roi-reichart/>Roi Reichart</a>
|
<a href=/people/m/milica-gasic/>Milica Gašić</a>
|
<a href=/people/a/anna-korhonen/>Anna Korhonen</a>
|
<a href=/people/s/steve-young/>Steve Young</a><br><a href=/volumes/Q17-1/ class=text-muted>Transactions of the Association for Computational Linguistics, Volume 5</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-Q17-1022><div class="card-body p-3 small">We present Attract-Repel, an <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> for improving the semantic quality of word vectors by injecting constraints extracted from lexical resources. Attract-Repel facilitates the use of constraints from mono- and cross-lingual resources, yielding semantically specialized cross-lingual vector spaces. Our evaluation shows that the method can make use of existing cross-lingual lexicons to construct high-quality vector spaces for a plethora of different languages, facilitating semantic transfer from high- to lower-resource ones. The effectiveness of our approach is demonstrated with state-of-the-art results on semantic similarity datasets in six languages. We next show that Attract-Repel-specialized vectors boost performance in the downstream task of dialogue state tracking (DST) across multiple languages. Finally, we show that cross-lingual vector spaces produced by our <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> facilitate the training of multilingual DST models, which brings further performance improvements.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1270.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1270 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1270 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D17-1270.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D17-1270/>Cross-Lingual Induction and Transfer of Verb Classes Based on Word Vector Space Specialisation</a></strong><br><a href=/people/i/ivan-vulic/>Ivan Vulić</a>
|
<a href=/people/n/nikola-mrksic/>Nikola Mrkšić</a>
|
<a href=/people/a/anna-korhonen/>Anna Korhonen</a><br><a href=/volumes/D17-1/ class=text-muted>Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1270><div class="card-body p-3 small">Existing approaches to automatic VerbNet-style verb classification are heavily dependent on <a href=https://en.wikipedia.org/wiki/Feature_engineering>feature engineering</a> and therefore limited to languages with mature <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP pipelines</a>. In this work, we propose a novel cross-lingual transfer method for inducing VerbNets for multiple languages. To the best of our knowledge, this is the first study which demonstrates how the architectures for learning <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> can be applied to this challenging syntactic-semantic task. Our method uses cross-lingual translation pairs to tie each of the six target languages into a bilingual vector space with English, jointly specialising the representations to encode the relational information from English VerbNet. A standard clustering algorithm is then run on top of the VerbNet-specialised representations, using <a href=https://en.wikipedia.org/wiki/Dimension_(vector_space)>vector dimensions</a> as <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> for learning verb classes. Our results show that the proposed cross-lingual transfer approach sets new state-of-the-art verb classification performance across all six target languages explored in this work.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1042.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1042 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1042 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1042/>A Network-based End-to-End Trainable Task-oriented Dialogue System</a></strong><br><a href=/people/t/tsung-hsien-wen/>Tsung-Hsien Wen</a>
|
<a href=/people/d/david-vandyke/>David Vandyke</a>
|
<a href=/people/n/nikola-mrksic/>Nikola Mrkšić</a>
|
<a href=/people/m/milica-gasic/>Milica Gašić</a>
|
<a href=/people/l/lina-m-rojas-barahona/>Lina M. Rojas-Barahona</a>
|
<a href=/people/p/pei-hao-su/>Pei-Hao Su</a>
|
<a href=/people/s/stefan-ultes/>Stefan Ultes</a>
|
<a href=/people/s/steve-young/>Steve Young</a><br><a href=/volumes/E17-1/ class=text-muted>Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1042><div class="card-body p-3 small">Teaching machines to accomplish tasks by conversing naturally with humans is challenging. Currently, developing task-oriented dialogue systems requires creating multiple components and typically this involves either a large amount of <a href=https://en.wikipedia.org/wiki/Handicraft>handcrafting</a>, or acquiring costly labelled datasets to solve a statistical learning problem for each component. In this work we introduce a neural network-based text-in, text-out end-to-end trainable goal-oriented dialogue system along with a new way of collecting dialogue data based on a novel pipe-lined Wizard-of-Oz framework. This approach allows us to develop <a href=https://en.wikipedia.org/wiki/Dialogue_system>dialogue systems</a> easily and without making too many assumptions about the task at hand. The results show that the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> can converse with human subjects naturally whilst helping them to accomplish tasks in a restaurant search domain.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Nikola+Mrk%C5%A1i%C4%87" title="Search for 'Nikola Mrkšić' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/i/ivan-vulic/ class=align-middle>Ivan Vulić</a>
<span class="badge badge-secondary align-middle ml-2">11</span></li><li class=list-group-item><a href=/people/t/tsung-hsien-wen/ class=align-middle>Tsung-Hsien Wen</a>
<span class="badge badge-secondary align-middle ml-2">8</span></li><li class=list-group-item><a href=/people/p/pei-hao-su/ class=align-middle>Pei-Hao Su</a>
<span class="badge badge-secondary align-middle ml-2">8</span></li><li class=list-group-item><a href=/people/i/inigo-casanueva/ class=align-middle>Iñigo Casanueva</a>
<span class="badge badge-secondary align-middle ml-2">7</span></li><li class=list-group-item><a href=/people/p/pawel-budzianowski/ class=align-middle>Paweł Budzianowski</a>
<span class="badge badge-secondary align-middle ml-2">6</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/s/steve-young/ class=align-middle>Steve Young</a>
<span class="badge badge-secondary align-middle ml-2">5</span></li><li class=list-group-item><a href=/people/a/anna-korhonen/ class=align-middle>Anna Korhonen</a>
<span class="badge badge-secondary align-middle ml-2">5</span></li><li class=list-group-item><a href=/people/m/milica-gasic/ class=align-middle>Milica Gasic</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/people/d/daniela-gerz/ class=align-middle>Daniela Gerz</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/people/d/diarmuid-o-seaghdha/ class=align-middle>Diarmuid Ó Séaghdha</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/s/stefan-ultes/ class=align-middle>Stefan Ultes</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/l/lina-m-rojas-barahona/ class=align-middle>Lina M. Rojas Barahona</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/m/matthew-henderson/ class=align-middle>Matthew Henderson</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/s/sam-coope/ class=align-middle>Sam Coope</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/g/georgios-spithourakis/ class=align-middle>Georgios Spithourakis</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/r/roi-reichart/ class=align-middle>Roi Reichart</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/g/goran-glavas/ class=align-middle>Goran Glavaš</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/b/blaise-thomson/ class=align-middle>Blaise Thomson</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/i/ira-leviant/ class=align-middle>Ira Leviant</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/e/edoardo-maria-ponti/ class=align-middle>Edoardo Maria Ponti</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/samuel-coope/ class=align-middle>Samuel Coope</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/g/girish-kumar/ class=align-middle>Girish Kumar</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/david-vandyke/ class=align-middle>David Vandyke</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/emnlp/ class=align-middle>EMNLP</a><span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/venues/acl/ class=align-middle>ACL</a><span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/venues/ws/ class=align-middle>WS</a><span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/venues/naacl/ class=align-middle>NAACL</a><span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/venues/tacl/ class=align-middle>TACL</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-venues aria-expanded=false aria-controls=more-venues>show all...</li><div class="collapse border-top" id=more-venues><li class=list-group-item><a href=/venues/eacl/ class=align-middle>EACL</a><span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>