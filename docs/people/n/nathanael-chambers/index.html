<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Nathanael Chambers - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Nathanael</span> <span class=font-weight-bold>Chambers</span></h2><hr><div class=row><div class=col-lg-9><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.555.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--555 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.555 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.555/>Conditional Generation of Temporally-ordered Event Sequences</a></strong><br><a href=/people/s/shih-ting-lin/>Shih-Ting Lin</a>
|
<a href=/people/n/nathanael-chambers/>Nathanael Chambers</a>
|
<a href=/people/g/greg-durrett/>Greg Durrett</a><br><a href=/volumes/2021.acl-long/ class=text-muted>Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--555><div class="card-body p-3 small">Models of narrative schema knowledge have proven useful for a range of event-related tasks, but they typically do not capture the temporal relationships between events. We propose a single model that addresses both temporal ordering, sorting given events into the order they occurred, and event infilling, predicting new events which fit into an existing temporally-ordered sequence. We use a BART-based conditional generation model that can capture both temporality and common event co-occurrence, meaning it can be flexibly applied to different tasks in this space. Our model is trained as a denoising autoencoder : we take temporally-ordered event sequences, shuffle them, delete some events, and then attempt to recover the original event sequence. This task teaches the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> to make inferences given incomplete knowledge about the events in an underlying scenario. On the temporal ordering task, we show that our model is able to unscramble event sequences from existing datasets without access to explicitly labeled temporal training data, outperforming both a BERT-based pairwise model and a BERT-based pointer network. On event infilling, human evaluation shows that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is able to generate events that fit better temporally into the input events when compared to GPT-2 story completion models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-short.76.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-short--76 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-short.76 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-short.76/>Do nâ€™t Let Discourse Confine Your Model : Sequence Perturbations for Improved Event Language Models</a></strong><br><a href=/people/m/mahnaz-koupaee/>Mahnaz Koupaee</a>
|
<a href=/people/g/greg-durrett/>Greg Durrett</a>
|
<a href=/people/n/nathanael-chambers/>Nathanael Chambers</a>
|
<a href=/people/n/niranjan-balasubramanian/>Niranjan Balasubramanian</a><br><a href=/volumes/2021.acl-short/ class=text-muted>Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-short--76><div class="card-body p-3 small">Event language models represent plausible sequences of events. Most existing approaches train <a href=https://en.wikipedia.org/wiki/Autoregressive_model>autoregressive models</a> on text, which successfully capture <a href=https://en.wikipedia.org/wiki/Co-occurrence>event co-occurrence</a> but unfortunately constrain the <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> to follow the <a href=https://en.wikipedia.org/wiki/Discourse_analysis>discourse order</a> in which events are presented. Other domains may employ different discourse orders, and for many applications, we may care about different notions of <a href=https://en.wikipedia.org/wiki/Partially_ordered_set>ordering</a> (e.g., temporal) or not care about <a href=https://en.wikipedia.org/wiki/Partially_ordered_set>ordering</a> at all (e.g., when predicting related events in a schema). We propose a simple yet surprisingly effective strategy for improving event language models by perturbing event sequences so we can relax model dependence on text order. Despite generating completely synthetic event orderings, we show that this technique improves the performance of the event language models on both <a href=https://en.wikipedia.org/wiki/Application_software>applications</a> and out-of-domain events data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.starsem-1.15.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--starsem-1--15 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.starsem-1.15 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.starsem-1.15/>Toward Diverse Precondition Generation</a></strong><br><a href=/people/h/heeyoung-kwon/>Heeyoung Kwon</a>
|
<a href=/people/n/nathanael-chambers/>Nathanael Chambers</a>
|
<a href=/people/n/niranjan-balasubramanian/>Niranjan Balasubramanian</a><br><a href=/volumes/2021.starsem-1/ class=text-muted>Proceedings of *SEM 2021: The Tenth Joint Conference on Lexical and Computational Semantics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--starsem-1--15><div class="card-body p-3 small">A typical goal for <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>language understanding</a> is to logically connect the events of a discourse, but often connective events are not described due to their commonsense nature. In order to address this deficit, we focus here on generating precondition events. Precondition generation can be framed as a sequence-to-sequence problem : given a target event, generate a possible precondition. However, in most real-world scenarios, an event can have several preconditions, which is not always suitable for standard seq2seq frameworks. We propose DiP, the Diverse Precondition generation system that can generate unique and diverse preconditions. DiP consists of three stages of the generative process an event sampler, a candidate generator, and a post-processor. The event sampler provides control codes (precondition triggers) which the candidate generator uses to focus its generation. Post-processing further improves the results through <a href=https://en.wikipedia.org/wiki/Ranking>re-ranking</a> and <a href=https://en.wikipedia.org/wiki/Filter_(software)>filtering</a>. Unlike other conditional generation systems, DiP automatically generates <a href=https://en.wikipedia.org/wiki/Control_code>control codes</a> without training on diverse examples. Analysis reveals that DiP improves the diversity of preconditions significantly compared to a beam search baseline. Also, manual evaluation shows that DiP generates more <a href=https://en.wikipedia.org/wiki/Precursor_(chemistry)>preconditions</a> than a strong nucleus sampling baseline.</div></div><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.50.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--50 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.50 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938669 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.50/>Connecting the Dots : Event Graph Schema Induction with Path Language Modeling</a></strong><br><a href=/people/m/manling-li/>Manling Li</a>
|
<a href=/people/q/qi-zeng/>Qi Zeng</a>
|
<a href=/people/y/ying-lin/>Ying Lin</a>
|
<a href=/people/k/kyunghyun-cho/>Kyunghyun Cho</a>
|
<a href=/people/h/heng-ji/>Heng Ji</a>
|
<a href=/people/j/jonathan-may/>Jonathan May</a>
|
<a href=/people/n/nathanael-chambers/>Nathanael Chambers</a>
|
<a href=/people/c/clare-voss/>Clare Voss</a><br><a href=/volumes/2020.emnlp-main/ class=text-muted>Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--50><div class="card-body p-3 small">Event schemas can guide our understanding and ability to make predictions with respect to what might happen next. We propose a new Event Graph Schema, where two event types are connected through multiple paths involving entities that fill important roles in a coherent story. We then introduce Path Language Model, an auto-regressive language model trained on event-event paths, and select salient and coherent paths to probabilistically construct these graph schemas. We design two evaluation metrics, instance coverage and instance coherence, to evaluate the quality of graph schema induction, by checking when coherent event instances are covered by the schema graph. Intrinsic evaluations show that our approach is highly effective at inducing salient and coherent schemas. Extrinsic evaluations show the induced schema repository provides significant improvement to downstream end-to-end Information Extraction over a state-of-the-art joint neural extraction model, when used as additional global features to unfold instance graphs.</div></div><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1085.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1085 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1085 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-1085/>Event Ordering with a Generalized Model for Sieve Prediction Ranking</a></strong><br><a href=/people/b/bill-mcdowell/>Bill McDowell</a>
|
<a href=/people/n/nathanael-chambers/>Nathanael Chambers</a>
|
<a href=/people/a/alexander-ororbia-ii/>Alexander Ororbia II</a>
|
<a href=/people/d/david-reitter/>David Reitter</a><br><a href=/volumes/I17-1/ class=text-muted>Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1085><div class="card-body p-3 small">This paper improves on several aspects of a sieve-based event ordering architecture, CAEVO (Chambers et al., 2014), which creates globally consistent temporal relations between events and time expressions. First, we examine the usage of <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> and semantic role features. With the incorporation of these new features, we demonstrate a 5 % relative F1 gain over our replicated version of CAEVO. Second, we reformulate the architecture&#8217;s sieve-based inference algorithm as a prediction reranking method that approximately optimizes a scoring function computed using classifier precisions. Within this prediction reranking framework, we propose an alternative scoring function, showing an 8.8 % relative gain over the original CAEVO. We further include an in-depth analysis of one of the main datasets that is used to evaluate temporal classifiers, and we show how despite using the densest corpus, there is still a danger of <a href=https://en.wikipedia.org/wiki/Overfitting>overfitting</a>. While this paper focuses on temporal ordering, its results are applicable to other areas that use sieve-based architectures.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-0900.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-0900/>Proceedings of the 2nd Workshop on Linking Models of Lexical, Sentential and Discourse-level Semantics</a></strong><br><a href=/people/m/michael-roth/>Michael Roth</a>
|
<a href=/people/n/nasrin-mostafazadeh/>Nasrin Mostafazadeh</a>
|
<a href=/people/n/nathanael-chambers/>Nathanael Chambers</a>
|
<a href=/people/a/annie-louis/>Annie Louis</a><br><a href=/volumes/W17-09/ class=text-muted>Proceedings of the 2nd Workshop on Linking Models of Lexical, Sentential and Discourse-level Semantics</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-0905.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-0905 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-0905 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-0905/>Behind the Scenes of an Evolving Event Cloze Test</a></strong><br><a href=/people/n/nathanael-chambers/>Nathanael Chambers</a><br><a href=/volumes/W17-09/ class=text-muted>Proceedings of the 2nd Workshop on Linking Models of Lexical, Sentential and Discourse-level Semantics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-0905><div class="card-body p-3 small">This paper analyzes the narrative event cloze test and its recent evolution. The <a href=https://en.wikipedia.org/wiki/Software_testing>test</a> removes one event from a document&#8217;s chain of events, and systems predict the missing event. Originally proposed to evaluate learned knowledge of <a href=https://en.wikipedia.org/wiki/Event_(computing)>event scenarios</a> (e.g., <a href=https://en.wikipedia.org/wiki/Scripting_language>scripts</a> and frames), most recent work now builds ngram-like language models (LM) to beat the test. This paper argues that the <a href=https://en.wikipedia.org/wiki/Test_(assessment)>test</a> has slowly / unknowingly been altered to accommodate LMs.5 Most notably, tests are auto-generated rather than by hand, and no effort is taken to include core script events. Recent work is not clear on evaluation goals and contains contradictory results. We implement several <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>, and show that the test&#8217;s bias to high-frequency events explains the inconsistencies. We conclude with recommendations on how to return to the test&#8217;s original intent, and offer brief suggestions on a path forward.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-0906.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-0906 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-0906 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-0906/>LSDSem 2017 Shared Task : The Story Cloze Test<span class=acl-fixed-case>LSDS</span>em 2017 Shared Task: The Story Cloze Test</a></strong><br><a href=/people/n/nasrin-mostafazadeh/>Nasrin Mostafazadeh</a>
|
<a href=/people/m/michael-roth/>Michael Roth</a>
|
<a href=/people/a/annie-louis/>Annie Louis</a>
|
<a href=/people/n/nathanael-chambers/>Nathanael Chambers</a>
|
<a href=/people/j/james-allen/>James Allen</a><br><a href=/volumes/W17-09/ class=text-muted>Proceedings of the 2nd Workshop on Linking Models of Lexical, Sentential and Discourse-level Semantics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-0906><div class="card-body p-3 small">The LSDSem&#8217;17 shared task is the Story Cloze Test, a new evaluation for story understanding and script learning. This test provides a <a href=https://en.wikipedia.org/wiki/System>system</a> with a four-sentence story and two possible endings, and the system must choose the correct ending to the story. Successful narrative understanding (getting closer to human performance of 100 %) requires systems to link various levels of <a href=https://en.wikipedia.org/wiki/Semantics>semantics</a> to <a href=https://en.wikipedia.org/wiki/Commonsense_knowledge>commonsense knowledge</a>. A total of eight systems participated in the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>shared task</a>, with a variety of <a href=https://en.wikipedia.org/wiki/Software_development_process>approaches</a> including.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1104.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1104 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1104 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1104/>Aligning Entity Names with Online Aliases on Twitter<span class=acl-fixed-case>T</span>witter</a></strong><br><a href=/people/k/kevin-mckelvey/>Kevin McKelvey</a>
|
<a href=/people/p/peter-goutzounis/>Peter Goutzounis</a>
|
<a href=/people/s/stephen-da-cruz/>Stephen da Cruz</a>
|
<a href=/people/n/nathanael-chambers/>Nathanael Chambers</a><br><a href=/volumes/W17-11/ class=text-muted>Proceedings of the Fifth International Workshop on Natural Language Processing for Social Media</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1104><div class="card-body p-3 small">This paper presents new <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> that automatically align <a href=https://en.wikipedia.org/wiki/Pseudonym>online aliases</a> with their real entity names. Many research applications rely on identifying entity names in text, but people often refer to entities with unexpected nicknames and <a href=https://en.wikipedia.org/wiki/Pseudonym>aliases</a>. For example, The King and King James are <a href=https://en.wikipedia.org/wiki/Pseudonym>aliases</a> for Lebron James, a professional basketball player. Recent work on <a href=https://en.wikipedia.org/wiki/Entity_linking>entity linking</a> attempts to resolve mentions to knowledge base entries, like a wikipedia page, but linking is unfortunately limited to well-known entities with pre-built pages. This paper asks a more basic question : can <a href=https://en.wikipedia.org/wiki/Pseudonym>aliases</a> be aligned without background knowledge of the entity? Further, can the <a href=https://en.wikipedia.org/wiki/Semantics>semantics</a> surrounding alias mentions be used to inform alignments? We describe <a href=https://en.wikipedia.org/wiki/Statistical_model>statistical models</a> that make decisions based on the lexicographic properties of the <a href=https://en.wikipedia.org/wiki/Pseudonym>aliases</a> with their semantic context in a large corpus of tweets. We experiment on a database of Twitter users and their <a href=https://en.wikipedia.org/wiki/User_(computing)>usernames</a>, and present the first human evaluation for this task. Alignment accuracy approaches human performance at 81 %, and we show that while lexicographic features are most important, the semantic context of an alias further improves classification accuracy.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Nathanael+Chambers" title="Search for 'Nathanael Chambers' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/g/greg-durrett/ class=align-middle>Greg Durrett</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/n/niranjan-balasubramanian/ class=align-middle>Niranjan Balasubramanian</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/m/michael-roth/ class=align-middle>Michael Roth</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/n/nasrin-mostafazadeh/ class=align-middle>Nasrin Mostafazadeh</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/a/annie-louis/ class=align-middle>Annie Louis</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/s/shih-ting-lin/ class=align-middle>Shih-Ting Lin</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/mahnaz-koupaee/ class=align-middle>Mahnaz Koupaee</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/b/bill-mcdowell/ class=align-middle>Bill McDowell</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/alexander-ororbia-ii/ class=align-middle>Alexander Ororbia II</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/david-reitter/ class=align-middle>David Reitter</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/manling-li/ class=align-middle>Manling Li</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/q/qi-zeng/ class=align-middle>Qi Zeng</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/ying-lin/ class=align-middle>Ying Lin</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/kyunghyun-cho/ class=align-middle>Kyunghyun Cho</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/heng-ji/ class=align-middle>Heng Ji</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jonathan-may/ class=align-middle>Jonathan May</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/clare-voss/ class=align-middle>Clare Voss</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/james-allen/ class=align-middle>James Allen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/kevin-mckelvey/ class=align-middle>Kevin McKelvey</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/p/peter-goutzounis/ class=align-middle>Peter Goutzounis</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/stephen-da-cruz/ class=align-middle>Stephen da Cruz</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/heeyoung-kwon/ class=align-middle>Heeyoung Kwon</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/ws/ class=align-middle>WS</a><span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/venues/acl/ class=align-middle>ACL</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/ijcnlp/ class=align-middle>IJCNLP</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/emnlp/ class=align-middle>EMNLP</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/starsem/ class=align-middle>*SEM</a><span class="badge badge-secondary align-middle ml-2">1</span></li></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright Â©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>