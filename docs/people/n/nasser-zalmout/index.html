<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Nasser Zalmout - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Nasser</span> <span class=font-weight-bold>Zalmout</span></h2><hr><div class=row><div class=col-lg-9><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.362.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--362 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.362 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.362/>AdaTag : Multi-Attribute Value Extraction from Product Profiles with Adaptive Decoding<span class=acl-fixed-case>A</span>da<span class=acl-fixed-case>T</span>ag: Multi-Attribute Value Extraction from Product Profiles with Adaptive Decoding</a></strong><br><a href=/people/j/jun-yan/>Jun Yan</a>
|
<a href=/people/n/nasser-zalmout/>Nasser Zalmout</a>
|
<a href=/people/y/yan-liang/>Yan Liang</a>
|
<a href=/people/c/christan-grant/>Christan Grant</a>
|
<a href=/people/x/xiang-ren/>Xiang Ren</a>
|
<a href=/people/x/xin-luna-dong/>Xin Luna Dong</a><br><a href=/volumes/2021.acl-long/ class=text-muted>Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--362><div class="card-body p-3 small">Automatic extraction of product attribute values is an important enabling technology in <a href=https://en.wikipedia.org/wiki/E-commerce>e-Commerce platforms</a>. This <a href=https://en.wikipedia.org/wiki/Task_(computing)>task</a> is usually modeled using sequence labeling architectures, with several extensions to handle multi-attribute extraction. One line of previous work constructs attribute-specific models, through separate <a href=https://en.wikipedia.org/wiki/Code>decoders</a> or entirely separate models. However, this approach constrains <a href=https://en.wikipedia.org/wiki/Knowledge_sharing>knowledge sharing</a> across different attributes. Other contributions use a single multi-attribute model, with different techniques to embed <a href=https://en.wikipedia.org/wiki/Attribute_(computing)>attribute information</a>. But sharing the entire network parameters across all attributes can limit the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>&#8217;s capacity to capture attribute-specific characteristics. In this paper we present AdaTag, which uses adaptive decoding to handle <a href=https://en.wikipedia.org/wiki/Data_extraction>extraction</a>. We parameterize the decoder with pretrained attribute embeddings, through a hypernetwork and a Mixture-of-Experts (MoE) module. This allows for separate, but semantically correlated, <a href=https://en.wikipedia.org/wiki/Code>decoders</a> to be generated on the fly for different attributes. This approach facilitates <a href=https://en.wikipedia.org/wiki/Knowledge_sharing>knowledge sharing</a>, while maintaining the specificity of each attribute. Our experiments on a real-world e-Commerce dataset show marked improvements over previous methods.</div></div><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.480.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--480 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.480 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.480/>Morphological Analysis and Disambiguation for <a href=https://en.wikipedia.org/wiki/Gulf_Arabic>Gulf Arabic</a> : The Interplay between Resources and Methods<span class=acl-fixed-case>G</span>ulf <span class=acl-fixed-case>A</span>rabic: The Interplay between Resources and Methods</a></strong><br><a href=/people/s/salam-khalifa/>Salam Khalifa</a>
|
<a href=/people/n/nasser-zalmout/>Nasser Zalmout</a>
|
<a href=/people/n/nizar-habash/>Nizar Habash</a><br><a href=/volumes/2020.lrec-1/ class=text-muted>Proceedings of the 12th Language Resources and Evaluation Conference</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--480><div class="card-body p-3 small">In this paper we present the first full morphological analysis and disambiguation system for <a href=https://en.wikipedia.org/wiki/Gulf_Arabic>Gulf Arabic</a>. We use an existing state-of-the-art morphological disambiguation system to investigate the effects of different data sizes and different combinations of morphological analyzers for <a href=https://en.wikipedia.org/wiki/Modern_Standard_Arabic>Modern Standard Arabic</a>, <a href=https://en.wikipedia.org/wiki/Egyptian_Arabic>Egyptian Arabic</a>, and <a href=https://en.wikipedia.org/wiki/Gulf_Arabic>Gulf Arabic</a>. We find that in very low settings, morphological analyzers help boost the performance of the full morphological disambiguation task. However, as the size of resources increase, the value of the <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphological analyzers</a> decreases.</div></div><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5555.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5555 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5555 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5555/>Unsupervised Neologism Normalization Using Embedding Space Mapping</a></strong><br><a href=/people/n/nasser-zalmout/>Nasser Zalmout</a>
|
<a href=/people/k/kapil-thadani/>Kapil Thadani</a>
|
<a href=/people/a/aasish-pappu/>Aasish Pappu</a><br><a href=/volumes/D19-55/ class=text-muted>Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT 2019)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5555><div class="card-body p-3 small">This paper presents an approach for detecting and normalizing neologisms in <a href=https://en.wikipedia.org/wiki/Social_media>social media content</a>. Neologisms refer to recent expressions that are specific to certain entities or events and are being increasingly used by the public, but have not yet been accepted in mainstream language. Automated methods for handling <a href=https://en.wikipedia.org/wiki/Neologism>neologisms</a> are important for <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language understanding</a> and normalization, especially for informal genres with <a href=https://en.wikipedia.org/wiki/User-generated_content>user generated content</a>. We present an <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised approach</a> for detecting <a href=https://en.wikipedia.org/wiki/Neologism>neologisms</a> and then normalizing them to canonical words without relying on parallel training data. Our approach builds on the text normalization literature and introduces <a href=https://en.wikipedia.org/wiki/Adaptation>adaptations</a> to fit the specificities of this task, including <a href=https://en.wikipedia.org/wiki/Phonetic_transcription>phonetic and etymological considerations</a>. We evaluate the proposed techniques on a dataset of Reddit comments, with detected neologisms and corresponding <a href=https://en.wikipedia.org/wiki/Normalization_(statistics)>normalizations</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1173.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1173 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1173 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/384512599 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1173/>Adversarial Multitask Learning for Joint Multi-Feature and Multi-Dialect Morphological Modeling</a></strong><br><a href=/people/n/nasser-zalmout/>Nasser Zalmout</a>
|
<a href=/people/n/nizar-habash/>Nizar Habash</a><br><a href=/volumes/P19-1/ class=text-muted>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1173><div class="card-body p-3 small">Morphological tagging is challenging for morphologically rich languages due to the large target space and the need for more <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training data</a> to minimize model sparsity. Dialectal variants of morphologically rich languages suffer more as they tend to be more noisy and have less resources. In this paper we explore the use of <a href=https://en.wikipedia.org/wiki/Multitask_learning>multitask learning</a> and adversarial training to address morphological richness and dialectal variations in the context of full morphological tagging. We use <a href=https://en.wikipedia.org/wiki/Multitask_learning>multitask learning</a> for joint morphological modeling for the <a href=https://en.wikipedia.org/wiki/Feature_(linguistics)>features</a> within two dialects, and as a <a href=https://en.wikipedia.org/wiki/Knowledge_transfer>knowledge-transfer scheme</a> for cross-dialectal modeling. We use adversarial training to learn dialect invariant features that can help the knowledge-transfer scheme from the high to low-resource variants. We work with two dialectal variants : Modern Standard Arabic (high-resource dialect&#8217;) and Egyptian Arabic (low-resource dialect) as a case study. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> achieve state-of-the-art results for both. Furthermore, adversarial training provides more significant improvement when using smaller training datasets in particular.</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1097.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1097 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1097 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-1097/>Utilizing Character and Word Embeddings for Text Normalization with Sequence-to-Sequence Models</a></strong><br><a href=/people/d/daniel-watson/>Daniel Watson</a>
|
<a href=/people/n/nasser-zalmout/>Nasser Zalmout</a>
|
<a href=/people/n/nizar-habash/>Nizar Habash</a><br><a href=/volumes/D18-1/ class=text-muted>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1097><div class="card-body p-3 small">Text normalization is an important enabling technology for several NLP tasks. Recently, neural-network-based approaches have outperformed well-established <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> in this task. However, in languages other than English, there has been little exploration in this direction. Both the scarcity of annotated data and the complexity of the language increase the difficulty of the problem. To address these challenges, we use a sequence-to-sequence model with character-based attention, which in addition to its self-learned character embeddings, uses word embeddings pre-trained with an approach that also models subword information. This provides the neural model with access to more linguistic information especially suitable for <a href=https://en.wikipedia.org/wiki/Text_normalization>text normalization</a>, without large parallel corpora. We show that providing the model with word-level features bridges the gap for the neural network approach to achieve a state-of-the-art F1 score on a standard Arabic language correction shared task dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1087.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1087 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1087 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-1087/>Noise-Robust Morphological Disambiguation for Dialectal Arabic<span class=acl-fixed-case>A</span>rabic</a></strong><br><a href=/people/n/nasser-zalmout/>Nasser Zalmout</a>
|
<a href=/people/a/alexander-erdmann/>Alexander Erdmann</a>
|
<a href=/people/n/nizar-habash/>Nizar Habash</a><br><a href=/volumes/N18-1/ class=text-muted>Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1087><div class="card-body p-3 small">User-generated text tends to be noisy with many lexical and orthographic inconsistencies, making natural language processing (NLP) tasks more challenging. The challenging nature of noisy text processing is exacerbated for dialectal content, where in addition to spelling and lexical differences, dialectal text is characterized with morpho-syntactic and phonetic variations. These issues increase sparsity in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP models</a> and reduce <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>. We present a neural morphological tagging and disambiguation model for <a href=https://en.wikipedia.org/wiki/Egyptian_Arabic>Egyptian Arabic</a>, with various extensions to handle noisy and inconsistent content. Our models achieve about 5 % <a href=https://en.wikipedia.org/wiki/Errors-in-variables_models>relative error reduction</a> (1.1 % absolute improvement) for full morphological analysis, and around 22 % <a href=https://en.wikipedia.org/wiki/Errors-in-variables_models>relative error reduction</a> (1.8 % absolute improvement) for <a href=https://en.wikipedia.org/wiki/Part-of-speech_tagging>part-of-speech tagging</a>, over a state-of-the-art baseline.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2089.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2089 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2089 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2089.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-2089/>Addressing Noise in Multidialectal Word Embeddings</a></strong><br><a href=/people/a/alexander-erdmann/>Alexander Erdmann</a>
|
<a href=/people/n/nasser-zalmout/>Nasser Zalmout</a>
|
<a href=/people/n/nizar-habash/>Nizar Habash</a><br><a href=/volumes/P18-2/ class=text-muted>Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2089><div class="card-body p-3 small">Word embeddings are crucial to many <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing tasks</a>. The quality of <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a> relies on large non-noisy corpora. Arabic dialects lack <a href=https://en.wikipedia.org/wiki/Varieties_of_Arabic>large corpora</a> and are noisy, being linguistically disparate with no <a href=https://en.wikipedia.org/wiki/Standard_language>standardized spelling</a>. We make three contributions to address this noise. First, we describe simple but effective adaptations to word embedding tools to maximize the informative content leveraged in each training sentence. Second, we analyze methods for representing disparate dialects in one embedding space, either by mapping individual dialects into a shared space or learning a joint model of all dialects. Finally, we evaluate via dictionary induction, showing that two metrics not typically reported in the task enable us to analyze our contributions&#8217; effects on low and high frequency words. In addition to boosting performance between 2-53 %, we specifically improve on noisy, low frequency forms without compromising <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> on high frequency forms.</div></div><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1073.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1073 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1073 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1073/>Do n’t Throw Those Morphological Analyzers Away Just Yet : Neural Morphological Disambiguation for <a href=https://en.wikipedia.org/wiki/Arabic>Arabic</a><span class=acl-fixed-case>A</span>rabic</a></strong><br><a href=/people/n/nasser-zalmout/>Nasser Zalmout</a>
|
<a href=/people/n/nizar-habash/>Nizar Habash</a><br><a href=/volumes/D17-1/ class=text-muted>Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1073><div class="card-body p-3 small">This paper presents a <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> for Arabic morphological disambiguation based on Recurrent Neural Networks (RNN). We train Long Short-Term Memory (LSTM) cells in several configurations and embedding levels to model the various morphological features. Our experiments show that these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> outperform state-of-the-art <a href=https://en.wikipedia.org/wiki/System>systems</a> without explicit use of <a href=https://en.wikipedia.org/wiki/Feature_engineering>feature engineering</a>. However, adding learning features from a <a href=https://en.wikipedia.org/wiki/Morphological_analysis>morphological analyzer</a> to model the space of possible analyses provides additional improvement. We make use of the resulting <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphological models</a> for scoring and ranking the analyses of the morphological analyzer for morphological disambiguation. The results show significant gains in <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> across several evaluation metrics. Our <a href=https://en.wikipedia.org/wiki/System>system</a> results in 4.4 % absolute increase over the <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a> in full morphological analysis accuracy (30.6 % relative error reduction), and 10.6 % (31.5 % relative error reduction) for out-of-vocabulary words.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Nasser+Zalmout" title="Search for 'Nasser Zalmout' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/n/nizar-habash/ class=align-middle>Nizar Habash</a>
<span class="badge badge-secondary align-middle ml-2">6</span></li><li class=list-group-item><a href=/people/a/alexander-erdmann/ class=align-middle>Alexander Erdmann</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/j/jun-yan/ class=align-middle>Jun Yan</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yan-liang/ class=align-middle>Yan Liang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/christan-grant/ class=align-middle>Christan Grant</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/x/xiang-ren/ class=align-middle>Xiang Ren</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/x/xin-luna-dong/ class=align-middle>Xin Luna Dong</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/daniel-watson/ class=align-middle>Daniel Watson</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/kapil-thadani/ class=align-middle>Kapil Thadani</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/aasish-pappu/ class=align-middle>Aasish Pappu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/salam-khalifa/ class=align-middle>Salam Khalifa</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/acl/ class=align-middle>ACL</a><span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/venues/emnlp/ class=align-middle>EMNLP</a><span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/venues/naacl/ class=align-middle>NAACL</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/lrec/ class=align-middle>LREC</a><span class="badge badge-secondary align-middle ml-2">1</span></li></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>