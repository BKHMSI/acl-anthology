<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Kenneth Heafield - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Kenneth</span> <span class=font-weight-bold>Heafield</span></h2><hr><div class=row><div class=col-lg-9><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wmt-1.74.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wmt-1--74 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wmt-1.74 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wmt-1.74/>Efficient <a href=https://en.wikipedia.org/wiki/Machine_translation>Machine Translation</a> with Model Pruning and Quantization</a></strong><br><a href=/people/m/maximiliana-behnke/>Maximiliana Behnke</a>
|
<a href=/people/n/nikolay-bogoychev/>Nikolay Bogoychev</a>
|
<a href=/people/a/alham-fikri-aji/>Alham Fikri Aji</a>
|
<a href=/people/k/kenneth-heafield/>Kenneth Heafield</a>
|
<a href=/people/g/graeme-nail/>Graeme Nail</a>
|
<a href=/people/q/qianqian-zhu/>Qianqian Zhu</a>
|
<a href=/people/s/svetlana-tchistiakova/>Svetlana Tchistiakova</a>
|
<a href=/people/j/jelmer-van-der-linde/>Jelmer van der Linde</a>
|
<a href=/people/p/pinzhen-chen/>Pinzhen Chen</a>
|
<a href=/people/s/sidharth-kashyap/>Sidharth Kashyap</a>
|
<a href=/people/r/roman-grundkiewicz/>Roman Grundkiewicz</a><br><a href=/volumes/2021.wmt-1/ class=text-muted>Proceedings of the Sixth Conference on Machine Translation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wmt-1--74><div class="card-body p-3 small">We participated in all tracks of the WMT 2021 efficient machine translation task : <a href=https://en.wikipedia.org/wiki/Single-core>single-core CPU</a>, <a href=https://en.wikipedia.org/wiki/Multi-core_processor>multi-core CPU</a>, and <a href=https://en.wikipedia.org/wiki/Graphics_processing_unit>GPU hardware</a> with throughput and latency conditions. Our submissions combine several efficiency strategies : knowledge distillation, a simpler simple recurrent unit (SSRU) decoder with one or two layers, lexical shortlists, smaller numerical formats, and pruning. For the CPU track, we used <a href=https://en.wikipedia.org/wiki/Quantization_(signal_processing)>quantized 8-bit models</a>. For the GPU track, we experimented with <a href=https://en.wikipedia.org/wiki/FP16>FP16</a> and 8-bit integers in tensorcores. Some of our submissions optimize for size via 4-bit log quantization and omitting a <a href=https://en.wikipedia.org/wiki/Lexical_analysis>lexical shortlist</a>. We have extended pruning to more parts of the network, emphasizing component- and block-level pruning that actually improves speed unlike coefficient-wise pruning.</div></div><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.211.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--211 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.211 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938739 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.211/>Losing Heads in the Lottery : Pruning Transformer Attention in Neural Machine Translation</a></strong><br><a href=/people/m/maximiliana-behnke/>Maximiliana Behnke</a>
|
<a href=/people/k/kenneth-heafield/>Kenneth Heafield</a><br><a href=/volumes/2020.emnlp-main/ class=text-muted>Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--211><div class="card-body p-3 small">The attention mechanism is the crucial component of the transformer architecture. Recent research shows that most <a href=https://en.wikipedia.org/wiki/Attentional_control>attention heads</a> are not confident in their decisions and can be pruned. However, removing them before training a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> results in lower quality. In this paper, we apply the lottery ticket hypothesis to prune heads in the early stages of training. Our experiments on <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> show that it is possible to remove up to three-quarters of attention heads from transformer-big during early training with an average -0.1 change in <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a> for <a href=https://en.wikipedia.org/wiki/Turkish_language>TurkishEnglish</a>. The pruned model is 1.5 times as fast at <a href=https://en.wikipedia.org/wiki/Statistical_inference>inference</a>, albeit at the cost of longer training. Our method is complementary to other approaches, such as teacher-student, with EnglishGerman student model gaining an additional 10 % speed-up with 75 % encoder attention removed and 0.2 BLEU loss.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.acl-main.152.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--acl-main--152 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.acl-main.152 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://slideslive.com/38929223 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.acl-main.152" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.acl-main.152/>Parallel Sentence Mining by Constrained Decoding</a></strong><br><a href=/people/p/pinzhen-chen/>Pinzhen Chen</a>
|
<a href=/people/n/nikolay-bogoychev/>Nikolay Bogoychev</a>
|
<a href=/people/k/kenneth-heafield/>Kenneth Heafield</a>
|
<a href=/people/f/faheem-kirefu/>Faheem Kirefu</a><br><a href=/volumes/2020.acl-main/ class=text-muted>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--acl-main--152><div class="card-body p-3 small">We present a novel method to extract <a href=https://en.wikipedia.org/wiki/Parallel_text>parallel sentences</a> from two monolingual corpora, using <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a>. Our method relies on translating sentences in one corpus, but constraining the decoding by a <a href=https://en.wikipedia.org/wiki/Prefix_tree>prefix tree</a> built on the other corpus. We argue that a neural machine translation system by itself can be a sentence similarity scorer and it efficiently approximates <a href=https://en.wikipedia.org/wiki/Pairwise_comparison>pairwise comparison</a> with a modified <a href=https://en.wikipedia.org/wiki/Beam_search>beam search</a>. When benchmarked on the BUCC shared task, our <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>method</a> achieves results comparable to other submissions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.acl-main.417.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--acl-main--417 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.acl-main.417 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://slideslive.com/38928733 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.acl-main.417" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.acl-main.417/>ParaCrawl : Web-Scale Acquisition of Parallel Corpora<span class=acl-fixed-case>P</span>ara<span class=acl-fixed-case>C</span>rawl: Web-Scale Acquisition of Parallel Corpora</a></strong><br><a href=/people/m/marta-banon/>Marta Bañón</a>
|
<a href=/people/p/pinzhen-chen/>Pinzhen Chen</a>
|
<a href=/people/b/barry-haddow/>Barry Haddow</a>
|
<a href=/people/k/kenneth-heafield/>Kenneth Heafield</a>
|
<a href=/people/h/hieu-hoang/>Hieu Hoang</a>
|
<a href=/people/m/miquel-espla-gomis/>Miquel Esplà-Gomis</a>
|
<a href=/people/m/mikel-l-forcada/>Mikel L. Forcada</a>
|
<a href=/people/a/amir-kamran/>Amir Kamran</a>
|
<a href=/people/f/faheem-kirefu/>Faheem Kirefu</a>
|
<a href=/people/p/philipp-koehn/>Philipp Koehn</a>
|
<a href=/people/s/sergio-ortiz-rojas/>Sergio Ortiz Rojas</a>
|
<a href=/people/l/leopoldo-pla-sempere/>Leopoldo Pla Sempere</a>
|
<a href=/people/g/gema-ramirez-sanchez/>Gema Ramírez-Sánchez</a>
|
<a href=/people/e/elsa-sarrias/>Elsa Sarrías</a>
|
<a href=/people/m/marek-strelec/>Marek Strelec</a>
|
<a href=/people/b/brian-thompson/>Brian Thompson</a>
|
<a href=/people/w/william-waites/>William Waites</a>
|
<a href=/people/d/dion-wiggins/>Dion Wiggins</a>
|
<a href=/people/j/jaume-zaragoza/>Jaume Zaragoza</a><br><a href=/volumes/2020.acl-main/ class=text-muted>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--acl-main--417><div class="card-body p-3 small">We report on methods to create the largest publicly available parallel corpora by crawling the <a href=https://en.wikipedia.org/wiki/World_Wide_Web>web</a>, using <a href=https://en.wikipedia.org/wiki/Open-source_software>open source software</a>. We empirically compare alternative methods and publish benchmark data sets for sentence alignment and sentence pair filtering. We also describe the parallel corpora released and evaluate their quality and their usefulness to create machine translation systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.acl-main.688.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--acl-main--688 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.acl-main.688 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://slideslive.com/38929410 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.acl-main.688/>In <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a>, What Does Transfer Learning Transfer?</a></strong><br><a href=/people/a/alham-fikri-aji/>Alham Fikri Aji</a>
|
<a href=/people/n/nikolay-bogoychev/>Nikolay Bogoychev</a>
|
<a href=/people/k/kenneth-heafield/>Kenneth Heafield</a>
|
<a href=/people/r/rico-sennrich/>Rico Sennrich</a><br><a href=/volumes/2020.acl-main/ class=text-muted>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--acl-main--688><div class="card-body p-3 small">Transfer learning improves <a href=https://en.wikipedia.org/wiki/Quality_(business)>quality</a> for low-resource machine translation, but it is unclear what exactly it transfers. We perform several <a href=https://en.wikipedia.org/wiki/Ablation>ablation studies</a> that limit <a href=https://en.wikipedia.org/wiki/Information_transfer>information transfer</a>, then measure the quality impact across three language pairs to gain a black-box understanding of <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a>. Word embeddings play an important role in <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a>, particularly if they are properly aligned. Although <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a> can be performed without <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a>, results are sub-optimal. In contrast, transferring only the <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a> but nothing else yields catastrophic results. We then investigate diagonal alignments with <a href=https://en.wikipedia.org/wiki/Auto-encoder>auto-encoders</a> over real languages and <a href=https://en.wikipedia.org/wiki/Random_sequence>randomly generated sequences</a>, finding even <a href=https://en.wikipedia.org/wiki/Random_sequence>randomly generated sequences</a> as parents yield noticeable but smaller gains. Finally, <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a> can eliminate the need for a warm-up phase when training transformer models in high resource language pairs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wmt-1.17.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wmt-1--17 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wmt-1.17 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939661 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.wmt-1.17/>Speed-optimized, Compact Student Models that Distill Knowledge from a Larger Teacher Model : the UEDIN-CUNI Submission to the WMT 2020 News Translation Task<span class=acl-fixed-case>UEDIN</span>-<span class=acl-fixed-case>CUNI</span> Submission to the <span class=acl-fixed-case>WMT</span> 2020 News Translation Task</a></strong><br><a href=/people/u/ulrich-germann/>Ulrich Germann</a>
|
<a href=/people/r/roman-grundkiewicz/>Roman Grundkiewicz</a>
|
<a href=/people/m/martin-popel/>Martin Popel</a>
|
<a href=/people/r/radina-dobreva/>Radina Dobreva</a>
|
<a href=/people/n/nikolay-bogoychev/>Nikolay Bogoychev</a>
|
<a href=/people/k/kenneth-heafield/>Kenneth Heafield</a><br><a href=/volumes/2020.wmt-1/ class=text-muted>Proceedings of the Fifth Conference on Machine Translation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wmt-1--17><div class="card-body p-3 small">We describe the joint submission of the University of Edinburgh and Charles University, Prague, to the Czech / English track in the WMT 2020 Shared Task on News Translation. Our fast and compact student models distill knowledge from a larger, slower teacher. They are designed to offer a good trade-off between translation quality and <a href=https://en.wikipedia.org/wiki/Statistical_inference>inference efficiency</a>. On the WMT 2020 Czech English test sets, they achieve translation speeds of over 700 whitespace-delimited source words per second on a single <a href=https://en.wikipedia.org/wiki/Thread_(computing)>CPU thread</a>, thus making neural translation feasible on consumer hardware without a <a href=https://en.wikipedia.org/wiki/Graphics_processing_unit>GPU</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.ngt-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.ngt-1.0/>Proceedings of the Fourth Workshop on Neural Generation and Translation</a></strong><br><a href=/people/a/alexandra-birch/>Alexandra Birch</a>
|
<a href=/people/a/andrew-finch/>Andrew Finch</a>
|
<a href=/people/h/hiroaki-hayashi/>Hiroaki Hayashi</a>
|
<a href=/people/k/kenneth-heafield/>Kenneth Heafield</a>
|
<a href=/people/m/marcin-junczys-dowmunt/>Marcin Junczys-Dowmunt</a>
|
<a href=/people/i/ioannis-konstas/>Ioannis Konstas</a>
|
<a href=/people/x/xian-li/>Xian Li</a>
|
<a href=/people/g/graham-neubig/>Graham Neubig</a>
|
<a href=/people/y/yusuke-oda/>Yusuke Oda</a><br><a href=/volumes/2020.ngt-1/ class=text-muted>Proceedings of the Fourth Workshop on Neural Generation and Translation</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.ngt-1.26.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--ngt-1--26 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.ngt-1.26 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.ngt-1.26.Dataset.txt data-toggle=tooltip data-placement=top title=Dataset><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=http://slideslive.com/38929840 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.ngt-1.26/>Edinburgh’s Submissions to the 2020 Machine Translation Efficiency Task<span class=acl-fixed-case>E</span>dinburgh’s Submissions to the 2020 Machine Translation Efficiency Task</a></strong><br><a href=/people/n/nikolay-bogoychev/>Nikolay Bogoychev</a>
|
<a href=/people/r/roman-grundkiewicz/>Roman Grundkiewicz</a>
|
<a href=/people/a/alham-fikri-aji/>Alham Fikri Aji</a>
|
<a href=/people/m/maximiliana-behnke/>Maximiliana Behnke</a>
|
<a href=/people/k/kenneth-heafield/>Kenneth Heafield</a>
|
<a href=/people/s/sidharth-kashyap/>Sidharth Kashyap</a>
|
<a href=/people/e/emmanouil-ioannis-farsarakis/>Emmanouil-Ioannis Farsarakis</a>
|
<a href=/people/m/mateusz-chudyk/>Mateusz Chudyk</a><br><a href=/volumes/2020.ngt-1/ class=text-muted>Proceedings of the Fourth Workshop on Neural Generation and Translation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--ngt-1--26><div class="card-body p-3 small">We participated in all tracks of the Workshop on Neural Generation and Translation 2020 Efficiency Shared Task : <a href=https://en.wikipedia.org/wiki/Single-core>single-core CPU</a>, <a href=https://en.wikipedia.org/wiki/Multi-core_processor>multi-core CPU</a>, and <a href=https://en.wikipedia.org/wiki/Graphics_processing_unit>GPU</a>. At the model level, we use teacher-student training with a variety of student sizes, tie embeddings and sometimes layers, use the Simpler Simple Recurrent Unit, and introduce head pruning. On <a href=https://en.wikipedia.org/wiki/Graphics_processing_unit>GPUs</a>, we used 16-bit floating-point tensor cores. On <a href=https://en.wikipedia.org/wiki/Central_processing_unit>CPUs</a>, we customized 8-bit quantization and <a href=https://en.wikipedia.org/wiki/Multiprocessing>multiple processes</a> with affinity for the <a href=https://en.wikipedia.org/wiki/Multi-core_processor>multi-core setting</a>. To reduce model size, we experimented with 4-bit log quantization but use floats at runtime. In the shared task, most of our submissions were Pareto optimal with respect the trade-off between time and quality.</div></div><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1373.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1373 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1373 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1373/>Combining Global Sparse Gradients with Local Gradients in Distributed Neural Network Training</a></strong><br><a href=/people/a/alham-fikri-aji/>Alham Fikri Aji</a>
|
<a href=/people/k/kenneth-heafield/>Kenneth Heafield</a>
|
<a href=/people/n/nikolay-bogoychev/>Nikolay Bogoychev</a><br><a href=/volumes/D19-1/ class=text-muted>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1373><div class="card-body p-3 small">One way to reduce network traffic in multi-node data-parallel stochastic gradient descent is to only exchange the largest gradients. However, doing so damages the <a href=https://en.wikipedia.org/wiki/Gradient>gradient</a> and degrades the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>&#8217;s performance. Transformer models degrade dramatically while the impact on <a href=https://en.wikipedia.org/wiki/Radio-frequency_identification>RNNs</a> is smaller. We restore gradient quality by combining the compressed global gradient with the node&#8217;s locally computed uncompressed gradient. Neural machine translation experiments show that Transformer convergence is restored while <a href=https://en.wikipedia.org/wiki/Random-access_memory>RNNs</a> converge faster. With our method, training on 4 nodes converges up to 1.5x as fast as with uncompressed gradients and scales 3.5x relative to single-node training.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5608.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5608 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5608 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5608/>Making Asynchronous Stochastic Gradient Descent Work for Transformers</a></strong><br><a href=/people/a/alham-fikri-aji/>Alham Fikri Aji</a>
|
<a href=/people/k/kenneth-heafield/>Kenneth Heafield</a><br><a href=/volumes/D19-56/ class=text-muted>Proceedings of the 3rd Workshop on Neural Generation and Translation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5608><div class="card-body p-3 small">Asynchronous stochastic gradient descent (SGD) converges poorly for Transformer models, so synchronous SGD has become the norm for Transformer training. This is unfortunate because asynchronous SGD is faster at raw training speed since it avoids waiting for <a href=https://en.wikipedia.org/wiki/Synchronization_(computer_science)>synchronization</a>. Moreover, the Transformer model is the basis for state-of-the-art models for several tasks, including <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>, so training speed matters. To understand why asynchronous SGD under-performs, we blur the lines between asynchronous and synchronous methods. We find that summing several asynchronous updates, rather than applying them immediately, restores convergence behavior. With this <a href=https://en.wikipedia.org/wiki/Methodology>method</a>, the Transformer attains the same BLEU score 1.36 times as fast.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5203.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5203 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5203 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5203/>Incorporating <a href=https://en.wikipedia.org/wiki/Source_code>Source Syntax</a> into Transformer-Based Neural Machine Translation</a></strong><br><a href=/people/a/anna-currey/>Anna Currey</a>
|
<a href=/people/k/kenneth-heafield/>Kenneth Heafield</a><br><a href=/volumes/W19-52/ class=text-muted>Proceedings of the Fourth Conference on Machine Translation (Volume 1: Research Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5203><div class="card-body p-3 small">Transformer-based neural machine translation (NMT) has recently achieved state-of-the-art performance on many machine translation tasks. However, recent work (Raganato and Tiedemann, 2018 ; Tang et al., 2018 ; Tran et al., 2018) has indicated that Transformer models may not learn syntactic structures as well as their recurrent neural network-based counterparts, particularly in low-resource cases. In this paper, we incorporate constituency parse information into a Transformer NMT model. We leverage linearized parses of the source training sentences in order to inject <a href=https://en.wikipedia.org/wiki/Syntax>syntax</a> into the Transformer architecture without modifying it. We introduce two methods : a multi-task machine translation and parsing model with a single encoder and decoder, and a mixed encoder model that learns to translate directly from parsed and unparsed source sentences. We evaluate our methods on low-resource translation from <a href=https://en.wikipedia.org/wiki/English_language>English</a> into twenty target languages, showing consistent improvements of 1.3 <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a> on average across diverse target languages for the multi-task technique. We further evaluate the models on full-scale WMT tasks, finding that the multi-task model aids low- and medium-resource NMT but degenerates high-resource English-German translation.</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1327.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1327 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1327 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-1327/>Multi-Source Syntactic Neural Machine Translation</a></strong><br><a href=/people/a/anna-currey/>Anna Currey</a>
|
<a href=/people/k/kenneth-heafield/>Kenneth Heafield</a><br><a href=/volumes/D18-1/ class=text-muted>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1327><div class="card-body p-3 small">We introduce a novel multi-source technique for incorporating <a href=https://en.wikipedia.org/wiki/Syntax_(programming_languages)>source syntax</a> into <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a> using linearized parses. This is achieved by employing separate <a href=https://en.wikipedia.org/wiki/Encoder>encoders</a> for the sequential and parsed versions of the same source sentence ; the resulting representations are then combined using a hierarchical attention mechanism. The proposed model improves over both seq2seq and parsed baselines by over 1 BLEU on the WMT17 English-German task. Further analysis shows that our multi-source syntactic model is able to translate successfully without any parsed input, unlike standard parsed methods. In addition, performance does not deteriorate as much on long sentences as for the <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>baselines</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-2902.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-2902 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-2902 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-2902/>Unsupervised Source Hierarchies for Low-Resource Neural Machine Translation</a></strong><br><a href=/people/a/anna-currey/>Anna Currey</a>
|
<a href=/people/k/kenneth-heafield/>Kenneth Heafield</a><br><a href=/volumes/W18-29/ class=text-muted>Proceedings of the Workshop on the Relevance of Linguistic Structure in Neural Architectures for NLP</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-2902><div class="card-body p-3 small">Incorporating source syntactic information into neural machine translation (NMT) has recently proven successful (Eriguchi et al., 2016 ; Luong et al., 2016). However, this is generally done using an outside <a href=https://en.wikipedia.org/wiki/Parsing>parser</a> to syntactically annotate the training data, making this technique difficult to use for languages or domains for which a reliable <a href=https://en.wikipedia.org/wiki/Parsing>parser</a> is not available. In this paper, we introduce an unsupervised tree-to-sequence (tree2seq) model for <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a> ; this model is able to induce an unsupervised hierarchical structure on the source sentence based on the downstream task of <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a>. We adapt the Gumbel tree-LSTM of Choi et al. (2018) to NMT in order to create the <a href=https://en.wikipedia.org/wiki/Encoder>encoder</a>. We evaluate our <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> against sequential and supervised parsing baselines on three low- and medium-resource language pairs. For low-resource cases, the unsupervised tree2seq encoder significantly outperforms the baselines ; no improvements are seen for medium-resource translation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6412.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6412 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6412 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6412/>The University of Edinburgh’s Submissions to the WMT18 News Translation Task<span class=acl-fixed-case>U</span>niversity of <span class=acl-fixed-case>E</span>dinburgh’s Submissions to the <span class=acl-fixed-case>WMT</span>18 News Translation Task</a></strong><br><a href=/people/b/barry-haddow/>Barry Haddow</a>
|
<a href=/people/n/nikolay-bogoychev/>Nikolay Bogoychev</a>
|
<a href=/people/d/denis-emelin/>Denis Emelin</a>
|
<a href=/people/u/ulrich-germann/>Ulrich Germann</a>
|
<a href=/people/r/roman-grundkiewicz/>Roman Grundkiewicz</a>
|
<a href=/people/k/kenneth-heafield/>Kenneth Heafield</a>
|
<a href=/people/a/antonio-valerio-miceli-barone/>Antonio Valerio Miceli Barone</a>
|
<a href=/people/r/rico-sennrich/>Rico Sennrich</a><br><a href=/volumes/W18-64/ class=text-muted>Proceedings of the Third Conference on Machine Translation: Shared Task Papers</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6412><div class="card-body p-3 small">The University of Edinburgh made submissions to all 14 language pairs in the news translation task, with strong performances in most pairs. We introduce new RNN-variant, mixed RNN / Transformer ensembles, data selection and weighting, and extensions to back-translation.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Kenneth+Heafield" title="Search for 'Kenneth Heafield' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/n/nikolay-bogoychev/ class=align-middle>Nikolay Bogoychev</a>
<span class="badge badge-secondary align-middle ml-2">7</span></li><li class=list-group-item><a href=/people/a/alham-fikri-aji/ class=align-middle>Alham Fikri Aji</a>
<span class="badge badge-secondary align-middle ml-2">5</span></li><li class=list-group-item><a href=/people/r/roman-grundkiewicz/ class=align-middle>Roman Grundkiewicz</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/people/m/maximiliana-behnke/ class=align-middle>Maximiliana Behnke</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/p/pinzhen-chen/ class=align-middle>Pinzhen Chen</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/a/anna-currey/ class=align-middle>Anna Currey</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/f/faheem-kirefu/ class=align-middle>Faheem Kirefu</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/b/barry-haddow/ class=align-middle>Barry Haddow</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/r/rico-sennrich/ class=align-middle>Rico Sennrich</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/s/sidharth-kashyap/ class=align-middle>Sidharth Kashyap</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/u/ulrich-germann/ class=align-middle>Ulrich Germann</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/m/marta-banon/ class=align-middle>Marta Bañón</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/hieu-hoang/ class=align-middle>Hieu Hoang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/miquel-espla-gomis/ class=align-middle>Miquel Esplà-Gomis</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/mikel-l-forcada/ class=align-middle>Mikel L. Forcada</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/amir-kamran/ class=align-middle>Amir Kamran</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/p/philipp-koehn/ class=align-middle>Philipp Koehn</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/sergio-ortiz-rojas/ class=align-middle>Sergio Ortiz Rojas</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/leopoldo-pla-sempere/ class=align-middle>Leopoldo Pla Sempere</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/g/gema-ramirez-sanchez/ class=align-middle>Gema Ramírez-Sánchez</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/e/elsa-sarrias/ class=align-middle>Elsa Sarrías</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/marek-strelec/ class=align-middle>Marek Strelec</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/b/brian-thompson/ class=align-middle>Brian Thompson</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/w/william-waites/ class=align-middle>William Waites</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/dion-wiggins/ class=align-middle>Dion Wiggins</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jaume-zaragoza/ class=align-middle>Jaume Zaragoza</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/g/graeme-nail/ class=align-middle>Graeme Nail</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/q/qianqian-zhu/ class=align-middle>Qianqian Zhu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/svetlana-tchistiakova/ class=align-middle>Svetlana Tchistiakova</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jelmer-van-der-linde/ class=align-middle>Jelmer van der Linde</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/denis-emelin/ class=align-middle>Denis Emelin</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/antonio-valerio-miceli-barone/ class=align-middle>Antonio Valerio Miceli-Barone</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/martin-popel/ class=align-middle>Martin Popel</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/radina-dobreva/ class=align-middle>Radina Dobreva</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/alexandra-birch/ class=align-middle>Alexandra Birch</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/andrew-finch/ class=align-middle>Andrew Finch</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/hiroaki-hayashi/ class=align-middle>Hiroaki Hayashi</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/marcin-junczys-dowmunt/ class=align-middle>Marcin Junczys-Dowmunt</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/i/ioannis-konstas/ class=align-middle>Ioannis Konstas</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/x/xian-li/ class=align-middle>Xian Li</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/g/graham-neubig/ class=align-middle>Graham Neubig</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yusuke-oda/ class=align-middle>Yusuke Oda</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/e/emmanouil-ioannis-farsarakis/ class=align-middle>Emmanouil-Ioannis Farsarakis</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/mateusz-chudyk/ class=align-middle>Mateusz Chudyk</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/emnlp/ class=align-middle>EMNLP</a><span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/venues/acl/ class=align-middle>ACL</a><span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/venues/ws/ class=align-middle>WS</a><span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/venues/wmt/ class=align-middle>WMT</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/ngt/ class=align-middle>NGT</a><span class="badge badge-secondary align-middle ml-2">2</span></li></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>