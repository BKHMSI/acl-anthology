<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Kang Liu (刘康) - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Kang</span> <span class=font-weight-bold>Liu</span></h2><hr><div class=row><div class=col-lg-9><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.ccl-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.ccl-1.0/>Proceedings of the 20th Chinese National Conference on Computational Linguistics</a></strong><br><a href=/people/s/sheng-li/>Sheng Li (李生)</a>
|
<a href=/people/m/maosong-sun/>Maosong Sun (孙茂松)</a>
|
<a href=/people/y/yang-liu-ict/>Yang Liu (刘洋)</a>
|
<a href=/people/h/hua-wu/>Hua Wu (吴华)</a>
|
<a href=/people/k/kang-liu/>Kang Liu (刘康)</a>
|
<a href=/people/w/wanxiang-che/>Wanxiang Che (车万翔)</a>
|
<a href=/people/s/shizhu-he/>Shizhu He (何世柱)</a>
|
<a href=/people/g/gaoqi-rao/>Gaoqi Rao (饶高琦)</a><br><a href=/volumes/2021.ccl-1/ class=text-muted>Proceedings of the 20th Chinese National Conference on Computational Linguistics</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.376.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--376 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.376 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.376/>Knowledge-Enriched Event Causality Identification via Latent Structure Induction Networks</a></strong><br><a href=/people/p/pengfei-cao/>Pengfei Cao</a>
|
<a href=/people/x/xinyu-zuo/>Xinyu Zuo</a>
|
<a href=/people/y/yubo-chen/>Yubo Chen</a>
|
<a href=/people/k/kang-liu/>Kang Liu</a>
|
<a href=/people/j/jun-zhao/>Jun Zhao</a>
|
<a href=/people/y/yuguang-chen/>Yuguang Chen</a>
|
<a href=/people/w/weihua-peng/>Weihua Peng</a><br><a href=/volumes/2021.acl-long/ class=text-muted>Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--376><div class="card-body p-3 small">Identifying causal relations of events is an important task in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing area</a>. However, the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> is very challenging, because <a href=https://en.wikipedia.org/wiki/Causality>event causality</a> is usually expressed in diverse forms that often lack explicit causal clues. Existing methods can not handle well the <a href=https://en.wikipedia.org/wiki/Problem_solving>problem</a>, especially in the condition of lacking training data. Nonetheless, humans can make a correct judgement based on their background knowledge, including <a href=https://en.wikipedia.org/wiki/Descriptive_knowledge>descriptive knowledge</a> and relational knowledge. Inspired by it, we propose a novel Latent Structure Induction Network (LSIN) to incorporate the external structural knowledge into this task. Specifically, to make use of the <a href=https://en.wikipedia.org/wiki/Descriptive_knowledge>descriptive knowledge</a>, we devise a Descriptive Graph Induction module to obtain and encode the graph-structured descriptive knowledge. To leverage the relational knowledge, we propose a Relational Graph Induction module which is able to automatically learn a reasoning structure for event causality reasoning. Experimental results on two widely used datasets indicate that our approach significantly outperforms previous state-of-the-art methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.417.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--417 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.417 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.417" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.417/>Alignment Rationale for Natural Language Inference</a></strong><br><a href=/people/z/zhongtao-jiang/>Zhongtao Jiang</a>
|
<a href=/people/y/yuanzhe-zhang/>Yuanzhe Zhang</a>
|
<a href=/people/z/zhao-yang/>Zhao Yang</a>
|
<a href=/people/j/jun-zhao/>Jun Zhao</a>
|
<a href=/people/k/kang-liu/>Kang Liu</a><br><a href=/volumes/2021.acl-long/ class=text-muted>Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--417><div class="card-body p-3 small">Deep learning models have achieved great success on the task of Natural Language Inference (NLI), though only a few attempts try to explain their behaviors. Existing explanation methods usually pick prominent features such as words or phrases from the input text. However, for NLI, alignments among words or phrases are more enlightening clues to explain the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>. To this end, this paper presents AREC, a post-hoc approach to generate alignment rationale explanations for co-attention based models in NLI. The explanation is based on <a href=https://en.wikipedia.org/wiki/Feature_selection>feature selection</a>, which keeps few but sufficient alignments while maintaining the same prediction of the target model. Experimental results show that our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> is more faithful and human-readable compared with many existing approaches. We further study and re-evaluate three typical <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> through our explanation beyond accuracy, and propose a simple method that greatly improves the model robustness.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.175.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--175 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.175 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.eacl-main.175.Dataset.zip data-toggle=tooltip data-placement=top title=Dataset><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.eacl-main.175.Software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2021.eacl-main.175/>Probing into the Root : A Dataset for Reason Extraction of Structural Events from Financial Documents</a></strong><br><a href=/people/p/pei-chen/>Pei Chen</a>
|
<a href=/people/k/kang-liu/>Kang Liu</a>
|
<a href=/people/y/yubo-chen/>Yubo Chen</a>
|
<a href=/people/t/taifeng-wang/>Taifeng Wang</a>
|
<a href=/people/j/jun-zhao/>Jun Zhao</a><br><a href=/volumes/2021.eacl-main/ class=text-muted>Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--175><div class="card-body p-3 small">This paper proposes a new <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> regarding event reason extraction from document-level texts. Unlike the previous causality detection task, we do not assign target events in the text, but only provide structural event descriptions, and such settings accord more with practice scenarios. Moreover, we annotate a large dataset FinReason for evaluation, which provides Reasons annotation for Financial events in company announcements. This task is challenging because the cases of multiple-events, multiple-reasons, and implicit-reasons are included. In total, FinReason contains 8,794 documents, 12,861 <a href=https://en.wikipedia.org/wiki/Financial_crisis>financial events</a> and 11,006 reason spans. We also provide the performance of existing canonical methods in <a href=https://en.wikipedia.org/wiki/Event_(computing)>event extraction</a> and machine reading comprehension on this task. The results show a 7 percentage point F1 score gap between the best <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> and human performance, and existing methods are far from resolving this problem.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.176.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--176 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.176 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.176" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.176/>Domain-Lifelong Learning for Dialogue State Tracking via Knowledge Preservation Networks</a></strong><br><a href=/people/q/qingbin-liu/>Qingbin Liu</a>
|
<a href=/people/p/pengfei-cao/>Pengfei Cao</a>
|
<a href=/people/c/cao-liu/>Cao Liu</a>
|
<a href=/people/j/jiansong-chen/>Jiansong Chen</a>
|
<a href=/people/x/xunliang-cai/>Xunliang Cai</a>
|
<a href=/people/f/fan-yang/>Fan Yang</a>
|
<a href=/people/s/shizhu-he/>Shizhu He</a>
|
<a href=/people/k/kang-liu/>Kang Liu</a>
|
<a href=/people/j/jun-zhao/>Jun Zhao</a><br><a href=/volumes/2021.emnlp-main/ class=text-muted>Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--176><div class="card-body p-3 small">Dialogue state tracking (DST), which estimates user goals given a dialogue context, is an essential component of task-oriented dialogue systems. Conventional DST models are usually trained offline, which requires a fixed dataset prepared in advance. This <a href=https://en.wikipedia.org/wiki/Paradigm>paradigm</a> is often impractical in real-world applications since online dialogue systems usually involve continually emerging new data and domains. Therefore, this paper explores Domain-Lifelong Learning for Dialogue State Tracking (DLL-DST), which aims to continually train a DST model on new data to learn incessantly emerging new domains while avoiding catastrophically forgetting old learned domains. To this end, we propose a novel domain-lifelong learning method, called Knowledge Preservation Networks (KPN), which consists of multi-prototype enhanced retrospection and multi-strategy knowledge distillation, to solve the problems of expression diversity and combinatorial explosion in the DLL-DST task. Experimental results show that <a href=https://en.wikipedia.org/wiki/KPN>KPN</a> effectively alleviates catastrophic forgetting and outperforms previous state-of-the-art lifelong learning methods by 4.25 % and 8.27 % of whole joint goal accuracy on the MultiWOZ benchmark and the SGD benchmark, respectively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.284.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--284 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.284 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.284" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.284/>Biomedical Concept Normalization by Leveraging Hypernyms</a></strong><br><a href=/people/c/cheng-yan/>Cheng Yan</a>
|
<a href=/people/y/yuanzhe-zhang/>Yuanzhe Zhang</a>
|
<a href=/people/k/kang-liu/>Kang Liu</a>
|
<a href=/people/j/jun-zhao/>Jun Zhao</a>
|
<a href=/people/y/yafei-shi/>Yafei Shi</a>
|
<a href=/people/s/shengping-liu/>Shengping Liu</a><br><a href=/volumes/2021.emnlp-main/ class=text-muted>Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--284><div class="card-body p-3 small">Biomedical Concept Normalization (BCN) is widely used in biomedical text processing as a fundamental module. Owing to numerous surface variants of biomedical concepts, BCN still remains challenging and unsolved. In this paper, we exploit biomedical concept hypernyms to facilitate BCN. We propose Biomedical Concept Normalizer with Hypernyms (BCNH), a novel framework that adopts list-wise training to make use of both hypernyms and synonyms, and also employs norm constraint on the representation of hypernym-hyponym entity pairs. The experimental results show that BCNH outperforms the previous state-of-the-art <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> on the NCBI dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.295.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--295 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.295 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.295/>Enhancing Multiple-choice Machine Reading Comprehension by Punishing Illogical Interpretations</a></strong><br><a href=/people/y/yiming-ju/>Yiming Ju</a>
|
<a href=/people/y/yuanzhe-zhang/>Yuanzhe Zhang</a>
|
<a href=/people/z/zhixing-tian/>Zhixing Tian</a>
|
<a href=/people/k/kang-liu/>Kang Liu</a>
|
<a href=/people/x/xiaohuan-cao/>Xiaohuan Cao</a>
|
<a href=/people/w/wenting-zhao/>Wenting Zhao</a>
|
<a href=/people/j/jinlong-li/>Jinlong Li</a>
|
<a href=/people/j/jun-zhao/>Jun Zhao</a><br><a href=/volumes/2021.emnlp-main/ class=text-muted>Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--295><div class="card-body p-3 small">Machine Reading Comprehension (MRC), which requires a machine to answer questions given the relevant documents, is an important way to test machines&#8217; ability to understand <a href=https://en.wikipedia.org/wiki/Human_language>human language</a>. Multiple-choice MRC is one of the most studied tasks in <a href=https://en.wikipedia.org/wiki/Medical_Subject_Headings>MRC</a> due to the convenience of evaluation and the flexibility of answer format. Post-hoc interpretation aims to explain a trained <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> and reveal how the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> arrives at the prediction. One of the most important interpretation forms is to attribute model decisions to input <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a>. Based on post-hoc interpretation methods, we assess attributions of paragraphs in multiple-choice MRC and improve the model by punishing the illogical attributions. Our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> can improve model performance without any external information and model structure change. Furthermore, we also analyze how and why such a self-training method works.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.760.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--760 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.760 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.760/>Set Generation Networks for End-to-End Knowledge Base Population</a></strong><br><a href=/people/d/dianbo-sui/>Dianbo Sui</a>
|
<a href=/people/c/chenhao-wang/>Chenhao Wang</a>
|
<a href=/people/y/yubo-chen/>Yubo Chen</a>
|
<a href=/people/k/kang-liu/>Kang Liu</a>
|
<a href=/people/j/jun-zhao/>Jun Zhao</a>
|
<a href=/people/w/wei-bi/>Wei Bi</a><br><a href=/volumes/2021.emnlp-main/ class=text-muted>Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--760><div class="card-body p-3 small">The task of <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge base population (KBP)</a> aims to discover facts about entities from texts and expand a <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge base</a> with these facts. Previous studies shape end-to-end KBP as a machine translation task, which is required to convert unordered fact into a sequence according to a pre-specified order. However, the facts stated in a sentence are unordered in essence. In this paper, we formulate end-to-end KBP as a direct set generation problem, avoiding considering the order of multiple facts. To solve the set generation problem, we propose networks featured by <a href=https://en.wikipedia.org/wiki/Transformer>transformers</a> with non-autoregressive parallel decoding. Unlike previous approaches that use an autoregressive decoder to generate facts one by one, the proposed networks can directly output the final set of facts in one shot. Furthermore, to train the <a href=https://en.wikipedia.org/wiki/Computer_network>networks</a>, we also design a <a href=https://en.wikipedia.org/wiki/Loss_function>set-based loss</a> that forces unique predictions via <a href=https://en.wikipedia.org/wiki/Bipartite_matching>bipartite matching</a>. Compared with cross-entropy loss that highly penalizes small shifts in fact order, the proposed bipartite matching loss is invariant to any permutation of predictions. Benefiting from getting rid of the burden of predicting the order of multiple facts, our proposed networks achieve state-of-the-art (SoTA) performance on two benchmark datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-demo.32.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-demo--32 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-demo.32 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-demo.32/>CroAno : A Crowd Annotation Platform for Improving Label Consistency of Chinese NER Dataset<span class=acl-fixed-case>C</span>ro<span class=acl-fixed-case>A</span>no : A Crowd Annotation Platform for Improving Label Consistency of <span class=acl-fixed-case>C</span>hinese <span class=acl-fixed-case>NER</span> Dataset</a></strong><br><a href=/people/b/baoli-zhang/>Baoli Zhang</a>
|
<a href=/people/z/zhucong-li/>Zhucong Li</a>
|
<a href=/people/z/zhen-gan/>Zhen Gan</a>
|
<a href=/people/y/yubo-chen/>Yubo Chen</a>
|
<a href=/people/j/jing-wan/>Jing Wan</a>
|
<a href=/people/k/kang-liu/>Kang Liu</a>
|
<a href=/people/j/jun-zhao/>Jun Zhao</a>
|
<a href=/people/s/shengping-liu/>Shengping Liu</a>
|
<a href=/people/y/yafei-shi/>Yafei Shi</a><br><a href=/volumes/2021.emnlp-demo/ class=text-muted>Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-demo--32><div class="card-body p-3 small">In this paper, we introduce CroAno, a web-based crowd annotation platform for the Chinese named entity recognition (NER). Besides some basic features for crowd annotation like fast tagging and <a href=https://en.wikipedia.org/wiki/Data_management>data management</a>, CroAno provides a systematic solution for improving label consistency of Chinese NER dataset. 1) Disagreement Adjudicator : CroAno uses a multi-dimensional highlight mode to visualize instance-level inconsistent entities and makes the revision process user-friendly. 2) Inconsistency Detector : CroAno employs a detector to locate corpus-level label inconsistency and provides users an interface to correct inconsistent entities in batches. 3) Prediction Error Analyzer : We deconstruct the entity prediction error of the model to six fine-grained entity error types. Users can employ this <a href=https://en.wikipedia.org/wiki/Error_detection_and_correction>error system</a> to detect corpus-level inconsistency from a model perspective. To validate the effectiveness of our <a href=https://en.wikipedia.org/wiki/Computing_platform>platform</a>, we use CroAno to revise two public datasets. In the two revised datasets, we get an improvement of +1.96 % and +2.57 % <a href=https://en.wikipedia.org/wiki/F-number>F1</a> respectively in model performance.</div></div><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.247.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--247 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.247 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.247/>Scene Restoring for Narrative Machine Reading Comprehension</a></strong><br><a href=/people/z/zhixing-tian/>Zhixing Tian</a>
|
<a href=/people/y/yuanzhe-zhang/>Yuanzhe Zhang</a>
|
<a href=/people/k/kang-liu/>Kang Liu</a>
|
<a href=/people/j/jun-zhao/>Jun Zhao</a>
|
<a href=/people/y/yantao-jia/>Yantao Jia</a>
|
<a href=/people/z/zhicheng-sheng/>Zhicheng Sheng</a><br><a href=/volumes/2020.emnlp-main/ class=text-muted>Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--247><div class="card-body p-3 small">This paper focuses on machine reading comprehension for <a href=https://en.wikipedia.org/wiki/Narrative>narrative passages</a>. Narrative passages usually describe a <a href=https://en.wikipedia.org/wiki/Chain_of_events>chain of events</a>. When reading this kind of passage, humans tend to restore a scene according to the text with their prior knowledge, which helps them understand the passage comprehensively. Inspired by this behavior of humans, we propose a method to let the machine imagine a scene during reading narrative for better comprehension. Specifically, we build a scene graph by utilizing Atomic as the external knowledge and propose a novel Graph Dimensional-Iteration Network (GDIN) to encode the <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph</a>. We conduct experiments on the ROCStories, a dataset of Story Cloze Test (SCT), and CosmosQA, a dataset of multiple choice. Our <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>method</a> achieves state-of-the-art.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.acl-main.282.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--acl-main--282 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.acl-main.282 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://slideslive.com/38928939 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.acl-main.282/>HyperCore : Hyperbolic and Co-graph Representation for Automatic ICD Coding<span class=acl-fixed-case>H</span>yper<span class=acl-fixed-case>C</span>ore: Hyperbolic and Co-graph Representation for Automatic <span class=acl-fixed-case>ICD</span> Coding</a></strong><br><a href=/people/p/pengfei-cao/>Pengfei Cao</a>
|
<a href=/people/y/yubo-chen/>Yubo Chen</a>
|
<a href=/people/k/kang-liu/>Kang Liu</a>
|
<a href=/people/j/jun-zhao/>Jun Zhao</a>
|
<a href=/people/s/shengping-liu/>Shengping Liu</a>
|
<a href=/people/w/weifeng-chong/>Weifeng Chong</a><br><a href=/volumes/2020.acl-main/ class=text-muted>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--acl-main--282><div class="card-body p-3 small">The International Classification of Diseases (ICD) provides a standardized way for <a href=https://en.wikipedia.org/wiki/Medical_classification>classifying diseases</a>, which endows each disease with a unique code. ICD coding aims to assign proper ICD codes to a <a href=https://en.wikipedia.org/wiki/Medical_record>medical record</a>. Since manual coding is very laborious and prone to errors, many methods have been proposed for the automatic ICD coding task. However, most of existing <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>methods</a> independently predict each code, ignoring two important characteristics : Code Hierarchy and <a href=https://en.wikipedia.org/wiki/Co-occurrence>Code Co-occurrence</a>. In this paper, we propose a Hyperbolic and Co-graph Representation method (HyperCore) to address the above problem. Specifically, we propose a hyperbolic representation method to leverage the code hierarchy. Moreover, we propose a graph convolutional network to utilize the code co-occurrence. Experimental results on two widely used datasets demonstrate that our proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms previous state-of-the-art methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.acl-main.576.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--acl-main--576 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.acl-main.576 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://slideslive.com/38929356 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.acl-main.576/>MIE : A Medical Information Extractor towards Medical Dialogues<span class=acl-fixed-case>MIE</span>: A Medical Information Extractor towards Medical Dialogues</a></strong><br><a href=/people/y/yuanzhe-zhang/>Yuanzhe Zhang</a>
|
<a href=/people/z/zhongtao-jiang/>Zhongtao Jiang</a>
|
<a href=/people/t/tao-zhang/>Tao Zhang</a>
|
<a href=/people/s/shiwan-liu/>Shiwan Liu</a>
|
<a href=/people/j/jiarun-cao/>Jiarun Cao</a>
|
<a href=/people/k/kang-liu/>Kang Liu</a>
|
<a href=/people/s/shengping-liu/>Shengping Liu</a>
|
<a href=/people/j/jun-zhao/>Jun Zhao</a><br><a href=/volumes/2020.acl-main/ class=text-muted>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--acl-main--576><div class="card-body p-3 small">Electronic Medical Records (EMRs) have become key components of modern <a href=https://en.wikipedia.org/wiki/Health_system>medical care systems</a>. Despite the merits of <a href=https://en.wikipedia.org/wiki/Electronic_health_record>EMRs</a>, many doctors suffer from writing them, which is time-consuming and tedious. We believe that automatically converting medical dialogues to <a href=https://en.wikipedia.org/wiki/Electronic_health_record>EMRs</a> can greatly reduce the burdens of doctors, and extracting information from medical dialogues is an essential step. To this end, we annotate online medical consultation dialogues in a window-sliding style, which is much easier than the sequential labeling annotation. We then propose a Medical Information Extractor (MIE) towards medical dialogues. MIE is able to extract mentioned symptoms, <a href=https://en.wikipedia.org/wiki/Surgery>surgeries</a>, <a href=https://en.wikipedia.org/wiki/Medical_test>tests</a>, other information and their corresponding status. To tackle the particular challenges of the task, MIE uses a deep matching architecture, taking dialogue turn-interaction into account. The experimental results demonstrate MIE is a promising solution to extract <a href=https://en.wikipedia.org/wiki/Medical_record>medical information</a> from doctor-patient dialogues.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.ccl-1.86.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--ccl-1--86 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.ccl-1.86 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.ccl-1.86/>Chinese Named Entity Recognition via Adaptive Multi-pass Memory Network with Hierarchical Tagging Mechanism<span class=acl-fixed-case>C</span>hinese Named Entity Recognition via Adaptive Multi-pass Memory Network with Hierarchical Tagging Mechanism</a></strong><br><a href=/people/p/pengfei-cao/>Pengfei Cao</a>
|
<a href=/people/y/yubo-chen/>Yubo Chen</a>
|
<a href=/people/k/kang-liu/>Kang Liu</a>
|
<a href=/people/j/jun-zhao/>Jun Zhao</a><br><a href=/volumes/2020.ccl-1/ class=text-muted>Proceedings of the 19th Chinese National Conference on Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--ccl-1--86><div class="card-body p-3 small">Named entity recognition (NER) aims to identify text spans that mention named entities and classify them into pre-defined categories. For Chinese NER task, most of the existing methods are character-based sequence labeling models and achieve great success. However, these methods usually ignore <a href=https://en.wikipedia.org/wiki/Lexical_analysis>lexical knowledge</a>, which leads to false prediction of entity boundaries. Moreover, these <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>methods</a> have difficulties in capturing tag dependencies. In this paper, we propose an Adaptive Multi-pass Memory Network with Hierarchical Tagging Mechanism (AMMNHT) to address all above problems. Specifically, to reduce the errors of predicting entity boundaries, we propose an adaptive multi-pass memory network to exploit lexical knowledge. In addition, we propose a hierarchical tagging layer to learn tag dependencies. Experimental results on three widely used Chinese NER datasets demonstrate that our proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> significantly outperforms other state-of-the-art methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.229.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--229 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.229 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.229/>How Does <a href=https://en.wikipedia.org/wiki/Context_(computing)>Context</a> Matter? On the Robustness of Event Detection with Context-Selective Mask Generalization</a></strong><br><a href=/people/j/jian-liu/>Jian Liu</a>
|
<a href=/people/y/yubo-chen/>Yubo Chen</a>
|
<a href=/people/k/kang-liu/>Kang Liu</a>
|
<a href=/people/y/yantao-jia/>Yantao Jia</a>
|
<a href=/people/z/zhicheng-sheng/>Zhicheng Sheng</a><br><a href=/volumes/2020.findings-emnlp/ class=text-muted>Findings of the Association for Computational Linguistics: EMNLP 2020</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--229><div class="card-body p-3 small">Event detection (ED) aims to identify and classify event triggers in texts, which is a crucial subtask of event extraction (EE). Despite many advances in ED, the existing studies are typically centered on improving the overall performance of an ED model, which rarely consider the robustness of an ED model. This paper aims to fill this research gap by stressing the importance of robustness modeling in ED models. We first pinpoint three stark cases demonstrating the brittleness of the existing ED models. After analyzing the underlying reason, we propose a new training mechanism, called context-selective mask generalization for ED, which can effectively mine context-specific patterns for learning and robustify an ED model. The experimental results have confirmed the effectiveness of our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> regarding defending against adversarial attacks, exploring unseen predicates, and tackling ambiguity cases. Moreover, a deeper analysis suggests that our approach can learn a complementary predictive bias with most ED models that use <a href=https://en.wikipedia.org/wiki/Context_(language_use)>full context</a> for <a href=https://en.wikipedia.org/wiki/Feature_learning>feature learning</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.130.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--130 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.130 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.130/>Pre-trained Language Model Based Active Learning for Sentence Matching</a></strong><br><a href=/people/g/guirong-bai/>Guirong Bai</a>
|
<a href=/people/s/shizhu-he/>Shizhu He</a>
|
<a href=/people/k/kang-liu/>Kang Liu</a>
|
<a href=/people/j/jun-zhao/>Jun Zhao</a>
|
<a href=/people/z/zaiqing-nie/>Zaiqing Nie</a><br><a href=/volumes/2020.coling-main/ class=text-muted>Proceedings of the 28th International Conference on Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--130><div class="card-body p-3 small">Active learning is able to significantly reduce the annotation cost for <a href=https://en.wikipedia.org/wiki/Data-driven_learning>data-driven techniques</a>. However, previous active learning approaches for <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a> mainly depend on the entropy-based uncertainty criterion, and ignore the characteristics of <a href=https://en.wikipedia.org/wiki/Natural_language>natural language</a>. In this paper, we propose a pre-trained language model based active learning approach for sentence matching. Differing from previous active learning, it can provide linguistic criteria from the pre-trained language model to measure instances and help select more effective instances for annotation. Experiments demonstrate our approach can achieve greater <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> with fewer labeled training instances.</div></div><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1035.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1035 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1035 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1035/>Learning the Extraction Order of Multiple Relational Facts in a Sentence with <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>Reinforcement Learning</a></a></strong><br><a href=/people/x/xiangrong-zeng/>Xiangrong Zeng</a>
|
<a href=/people/s/shizhu-he/>Shizhu He</a>
|
<a href=/people/d/daojian-zeng/>Daojian Zeng</a>
|
<a href=/people/k/kang-liu/>Kang Liu</a>
|
<a href=/people/s/shengping-liu/>Shengping Liu</a>
|
<a href=/people/j/jun-zhao/>Jun Zhao</a><br><a href=/volumes/D19-1/ class=text-muted>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1035><div class="card-body p-3 small">The multiple relation extraction task tries to extract all relational facts from a sentence. Existing works did n&#8217;t consider the extraction order of relational facts in a sentence. In this paper we argue that the extraction order is important in this task. To take the extraction order into consideration, we apply the <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning</a> into a sequence-to-sequence model. The proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> could generate <a href=https://en.wikipedia.org/wiki/Relational_model>relational facts</a> freely. Widely conducted experiments on two public datasets demonstrate the efficacy of the proposed <a href=https://en.wikipedia.org/wiki/Methodology>method</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1068.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1068 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1068 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1068/>Neural Cross-Lingual Event Detection with Minimal Parallel Resources</a></strong><br><a href=/people/j/jian-liu/>Jian Liu</a>
|
<a href=/people/y/yubo-chen/>Yubo Chen</a>
|
<a href=/people/k/kang-liu/>Kang Liu</a>
|
<a href=/people/j/jun-zhao/>Jun Zhao</a><br><a href=/volumes/D19-1/ class=text-muted>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1068><div class="card-body p-3 small">The scarcity in annotated data poses a great challenge for event detection (ED). Cross-lingual ED aims to tackle this challenge by transferring knowledge between different languages to boost performance. However, previous cross-lingual methods for ED demonstrated a heavy dependency on <a href=https://en.wikipedia.org/wiki/Parallel_computing>parallel resources</a>, which might limit their applicability. In this paper, we propose a new method for cross-lingual ED, demonstrating a minimal dependency on <a href=https://en.wikipedia.org/wiki/Parallel_computing>parallel resources</a>. Specifically, to construct a lexical mapping between different languages, we devise a context-dependent translation method ; to treat the word order difference problem, we propose a shared syntactic order event detector for multilingual co-training. The efficiency of our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> is studied through extensive experiments on two standard datasets. Empirical results indicate that our method is effective in 1) performing cross-lingual transfer concerning different directions and 2) tackling the extremely annotation-poor scenario.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1247.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1247 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1247 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1247/>Generating Questions for Knowledge Bases via Incorporating Diversified Contexts and Answer-Aware Loss</a></strong><br><a href=/people/c/cao-liu/>Cao Liu</a>
|
<a href=/people/k/kang-liu/>Kang Liu</a>
|
<a href=/people/s/shizhu-he/>Shizhu He</a>
|
<a href=/people/z/zaiqing-nie/>Zaiqing Nie</a>
|
<a href=/people/j/jun-zhao/>Jun Zhao</a><br><a href=/volumes/D19-1/ class=text-muted>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1247><div class="card-body p-3 small">We tackle the task of question generation over <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge bases</a>. Conventional methods for this task neglect two crucial research issues : 1) the given predicate needs to be expressed ; 2) the answer to the generated question needs to be definitive. In this paper, we strive toward the above two issues via incorporating diversified contexts and answer-aware loss. Specifically, we propose a neural encoder-decoder model with multi-level copy mechanisms to generate such questions. Furthermore, the answer aware loss is introduced to make generated questions corresponding to more definitive answers. Experiments demonstrate that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieves state-of-the-art performance. Meanwhile, such generated question is able to express the given predicate and correspond to a definitive answer.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1602.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1602 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1602 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1602/>Machine Reading Comprehension Using Structural Knowledge Graph-aware Network</a></strong><br><a href=/people/d/delai-qiu/>Delai Qiu</a>
|
<a href=/people/y/yuanzhe-zhang/>Yuanzhe Zhang</a>
|
<a href=/people/x/xinwei-feng/>Xinwei Feng</a>
|
<a href=/people/x/xiangwen-liao/>Xiangwen Liao</a>
|
<a href=/people/w/wenbin-jiang/>Wenbin Jiang</a>
|
<a href=/people/y/yajuan-lyu/>Yajuan Lyu</a>
|
<a href=/people/k/kang-liu/>Kang Liu</a>
|
<a href=/people/j/jun-zhao/>Jun Zhao</a><br><a href=/volumes/D19-1/ class=text-muted>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1602><div class="card-body p-3 small">Leveraging external knowledge is an emerging trend in machine comprehension task. Previous work usually utilizes <a href=https://en.wikipedia.org/wiki/Knowledge_graph>knowledge graphs</a> such as <a href=https://en.wikipedia.org/wiki/ConceptNet>ConceptNet</a> as external knowledge, and extracts triples from them to enhance the initial representation of the machine comprehension context. However, such <a href=https://en.wikipedia.org/wiki/Methodology>method</a> can not capture the structural information in the <a href=https://en.wikipedia.org/wiki/Knowledge_graph>knowledge graph</a>. To this end, we propose a Structural Knowledge Graph-aware Network(SKG) model, constructing sub-graphs for entities in the machine comprehension context. Our <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>method</a> dynamically updates the representation of the knowledge according to the structural information of the constructed sub-graph. Experiments show that SKG achieves state-of-the-art performance on the ReCoRD dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1367.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1367 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1367 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-1367/>Vocabulary Pyramid Network : Multi-Pass Encoding and Decoding with Multi-Level Vocabularies for Response Generation</a></strong><br><a href=/people/c/cao-liu/>Cao Liu</a>
|
<a href=/people/s/shizhu-he/>Shizhu He</a>
|
<a href=/people/k/kang-liu/>Kang Liu</a>
|
<a href=/people/j/jun-zhao/>Jun Zhao</a><br><a href=/volumes/P19-1/ class=text-muted>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1367><div class="card-body p-3 small">We study the task of response generation. Conventional methods employ a fixed vocabulary and one-pass decoding, which not only make them prone to safe and general responses but also lack further refining to the first generated raw sequence. To tackle the above two problems, we present a Vocabulary Pyramid Network (VPN) which is able to incorporate multi-pass encoding and decoding with multi-level vocabularies into response generation. Specifically, the dialogue input and output are represented by multi-level vocabularies which are obtained from hierarchical clustering of raw words. Then, multi-pass encoding and decoding are conducted on the multi-level vocabularies. Since <a href=https://en.wikipedia.org/wiki/Virtual_private_network>VPN</a> is able to leverage rich encoding and decoding information with multi-level vocabularies, it has the potential to generate better responses. Experiments on English Twitter and Chinese Weibo datasets demonstrate that <a href=https://en.wikipedia.org/wiki/Virtual_private_network>VPN</a> remarkably outperforms strong <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1418.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1418 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1418 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P19-1418.Software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1418/>AdaNSP : Uncertainty-driven Adaptive Decoding in Neural Semantic Parsing<span class=acl-fixed-case>A</span>da<span class=acl-fixed-case>NSP</span>: Uncertainty-driven Adaptive Decoding in Neural Semantic Parsing</a></strong><br><a href=/people/x/xiang-zhang/>Xiang Zhang</a>
|
<a href=/people/s/shizhu-he/>Shizhu He</a>
|
<a href=/people/k/kang-liu/>Kang Liu</a>
|
<a href=/people/j/jun-zhao/>Jun Zhao</a><br><a href=/volumes/P19-1/ class=text-muted>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1418><div class="card-body p-3 small">Neural semantic parsers utilize the encoder-decoder framework to learn an end-to-end model for semantic parsing that transduces a natural language sentence to the formal semantic representation. To keep the model aware of the underlying grammar in target sequences, many constrained decoders were devised in a multi-stage paradigm, which decode to the sketches or abstract syntax trees first, and then decode to target semantic tokens. We instead to propose an adaptive decoding method to avoid such intermediate representations. The <a href=https://en.wikipedia.org/wiki/Codec>decoder</a> is guided by model uncertainty and automatically uses deeper computations when necessary. Thus <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> can predict tokens adaptively. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms the state-of-the-art neural models and does not need any expertise like predefined grammar or <a href=https://en.wikipedia.org/wiki/Sketch_(drawing)>sketches</a> in the meantime.</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1277.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1277 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1277 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1277/>Pattern-revising Enhanced Simple Question Answering over Knowledge Bases</a></strong><br><a href=/people/y/yanchao-hao/>Yanchao Hao</a>
|
<a href=/people/h/hao-liu/>Hao Liu</a>
|
<a href=/people/s/shizhu-he/>Shizhu He</a>
|
<a href=/people/k/kang-liu/>Kang Liu</a>
|
<a href=/people/j/jun-zhao/>Jun Zhao</a><br><a href=/volumes/C18-1/ class=text-muted>Proceedings of the 27th International Conference on Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1277><div class="card-body p-3 small">Question Answering over Knowledge Bases (KB-QA), which automatically answer natural language questions based on the facts contained by a <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge base</a>, is one of the most important natural language processing (NLP) tasks. Simple questions constitute a large part of questions queried on the <a href=https://en.wikipedia.org/wiki/World_Wide_Web>web</a>, still being a challenge to <a href=https://en.wikipedia.org/wiki/Quality_assurance>QA systems</a>. In this work, we propose to conduct pattern extraction and <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entity linking</a> first, and put forward pattern revising procedure to mitigate the error propagation problem. In order to learn to rank candidate subject-predicate pairs to enable the relevant facts retrieval given a question, we propose to do joint fact selection enhanced by relation detection. Multi-level encodings and multi-dimension information are leveraged to strengthen the whole procedure. The experimental results demonstrate that our approach sets a new record in this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>, outperforming the current <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a> by an absolute large margin.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1017.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1017 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1017 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/306354811 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1017" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1017/>Adversarial Transfer Learning for Chinese Named Entity Recognition with Self-Attention Mechanism<span class=acl-fixed-case>C</span>hinese Named Entity Recognition with Self-Attention Mechanism</a></strong><br><a href=/people/p/pengfei-cao/>Pengfei Cao</a>
|
<a href=/people/y/yubo-chen/>Yubo Chen</a>
|
<a href=/people/k/kang-liu/>Kang Liu</a>
|
<a href=/people/j/jun-zhao/>Jun Zhao</a>
|
<a href=/people/s/shengping-liu/>Shengping Liu</a><br><a href=/volumes/D18-1/ class=text-muted>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1017><div class="card-body p-3 small">Named entity recognition (NER) is an important task in natural language processing area, which needs to determine entities boundaries and classify them into pre-defined categories. For Chinese NER task, there is only a very small amount of annotated data available. Chinese NER task and Chinese word segmentation (CWS) task have many similar word boundaries. There are also specificities in each <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. However, existing methods for Chinese NER either do not exploit word boundary information from CWS or can not filter the specific information of CWS. In this paper, we propose a novel adversarial transfer learning framework to make full use of task-shared boundaries information and prevent the task-specific features of CWS. Besides, since arbitrary character can provide important cues when predicting entity type, we exploit self-attention to explicitly capture long range dependencies between two tokens. Experimental results on two different widely used datasets show that our proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> significantly and consistently outperforms other state-of-the-art methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1158.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1158 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1158 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/305199664 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1158" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1158/>Collective Event Detection via a Hierarchical and Bias Tagging Networks with Gated Multi-level Attention Mechanisms</a></strong><br><a href=/people/y/yubo-chen/>Yubo Chen</a>
|
<a href=/people/h/hang-yang/>Hang Yang</a>
|
<a href=/people/k/kang-liu/>Kang Liu</a>
|
<a href=/people/j/jun-zhao/>Jun Zhao</a>
|
<a href=/people/y/yantao-jia/>Yantao Jia</a><br><a href=/volumes/D18-1/ class=text-muted>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1158><div class="card-body p-3 small">Traditional approaches to the task of ACE event detection primarily regard multiple events in one sentence as independent ones and recognize them separately by using sentence-level information. However, events in one sentence are usually interdependent and sentence-level information is often insufficient to resolve <a href=https://en.wikipedia.org/wiki/Ambiguity>ambiguities</a> for some types of events. This paper proposes a novel framework dubbed as Hierarchical and Bias Tagging Networks with Gated Multi-level Attention Mechanisms (HBTNGMA) to solve the two problems simultaneously. Firstly, we propose a hierachical and bias tagging networks to detect multiple events in one sentence collectively. Then, we devise a gated multi-level attention to automatically extract and dynamically fuse the sentence-level and document-level information. The experimental results on the widely used ACE 2005 dataset show that our approach significantly outperforms other state-of-the-art methods.</div></div><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-4005.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-4005 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-4005 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-4005/>IJCNLP-2017 Task 5 : Multi-choice Question Answering in Examinations<span class=acl-fixed-case>IJCNLP</span>-2017 Task 5: Multi-choice Question Answering in Examinations</a></strong><br><a href=/people/s/shangmin-guo/>Shangmin Guo</a>
|
<a href=/people/k/kang-liu/>Kang Liu</a>
|
<a href=/people/s/shizhu-he/>Shizhu He</a>
|
<a href=/people/c/cao-liu/>Cao Liu</a>
|
<a href=/people/j/jun-zhao/>Jun Zhao</a>
|
<a href=/people/z/zhuoyu-wei/>Zhuoyu Wei</a><br><a href=/volumes/I17-4/ class=text-muted>Proceedings of the IJCNLP 2017, Shared Tasks</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-4005><div class="card-body p-3 small">The IJCNLP-2017 Multi-choice Question Answering(MCQA) task aims at exploring the performance of current Question Answering(QA) techniques via the realworld complex questions collected from Chinese Senior High School Entrance Examination papers and CK12 website1. The questions are all 4-way multi-choice questions writing in Chinese and English respectively that cover a wide range of subjects, e.g. Biology, <a href=https://en.wikipedia.org/wiki/History>History</a>, <a href=https://en.wikipedia.org/wiki/List_of_life_sciences>Life Science</a> and etc. And, all questions are restrained within the <a href=https://en.wikipedia.org/wiki/Elementary_school_(United_States)>elementary and middle school level</a>. During the whole procedure of this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>, 7 teams submitted 323 runs in total. This paper describes the collected data, the format and size of these questions, formal run statistics and results, overview and performance statistics of different methods</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1019.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1019 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1019 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-1019/>Generating Natural Answers by Incorporating Copying and Retrieving Mechanisms in Sequence-to-Sequence Learning</a></strong><br><a href=/people/s/shizhu-he/>Shizhu He</a>
|
<a href=/people/c/cao-liu/>Cao Liu</a>
|
<a href=/people/k/kang-liu/>Kang Liu</a>
|
<a href=/people/j/jun-zhao/>Jun Zhao</a><br><a href=/volumes/P17-1/ class=text-muted>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1019><div class="card-body p-3 small">Generating answer with natural language sentence is very important in real-world question answering systems, which needs to obtain a right answer as well as a coherent natural response. In this paper, we propose an end-to-end question answering system called COREQA in sequence-to-sequence learning, which incorporates copying and retrieving mechanisms to generate natural answers within an encoder-decoder framework. Specifically, in COREQA, the semantic units (words, phrases and entities) in a natural answer are dynamically predicted from the vocabulary, copied from the given question and/or retrieved from the corresponding knowledge base jointly. Our empirical study on both synthetic and real-world datasets demonstrates the efficiency of COREQA, which is able to generate correct, coherent and natural answers for knowledge inquired questions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1021.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1021 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1021 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-1021/>An End-to-End Model for Question Answering over <a href=https://en.wikipedia.org/wiki/Knowledge_base>Knowledge Base</a> with Cross-Attention Combining Global Knowledge</a></strong><br><a href=/people/y/yanchao-hao/>Yanchao Hao</a>
|
<a href=/people/y/yuanzhe-zhang/>Yuanzhe Zhang</a>
|
<a href=/people/k/kang-liu/>Kang Liu</a>
|
<a href=/people/s/shizhu-he/>Shizhu He</a>
|
<a href=/people/z/zhanyi-liu/>Zhanyi Liu</a>
|
<a href=/people/h/hua-wu/>Hua Wu</a>
|
<a href=/people/j/jun-zhao/>Jun Zhao</a><br><a href=/volumes/P17-1/ class=text-muted>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1021><div class="card-body p-3 small">With the rapid growth of knowledge bases (KBs) on the <a href=https://en.wikipedia.org/wiki/World_Wide_Web>web</a>, how to take full advantage of them becomes increasingly important. Question answering over knowledge base (KB-QA) is one of the promising approaches to access the substantial knowledge. Meanwhile, as the neural network-based (NN-based) methods develop, NN-based KB-QA has already achieved impressive results. However, previous work did not put more emphasis on question representation, and the question is converted into a fixed vector regardless of its candidate answers. This simple representation strategy is not easy to express the proper information in the question. Hence, we present an end-to-end neural network model to represent the questions and their corresponding scores dynamically according to the various candidate answer aspects via cross-attention mechanism. In addition, we leverage the global knowledge inside the underlying KB, aiming at integrating the rich KB information into the representation of the answers. As a result, it could alleviates the out-of-vocabulary (OOV) problem, which helps the cross-attention model to represent the question more precisely. The experimental results on WebQuestions demonstrate the effectiveness of the proposed approach.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1038.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1038 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1038 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-1038/>Automatically Labeled Data Generation for Large Scale Event Extraction</a></strong><br><a href=/people/y/yubo-chen/>Yubo Chen</a>
|
<a href=/people/s/shulin-liu/>Shulin Liu</a>
|
<a href=/people/x/xiang-zhang/>Xiang Zhang</a>
|
<a href=/people/k/kang-liu/>Kang Liu</a>
|
<a href=/people/j/jun-zhao/>Jun Zhao</a><br><a href=/volumes/P17-1/ class=text-muted>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1038><div class="card-body p-3 small">Modern models of <a href=https://en.wikipedia.org/wiki/Event_extraction>event extraction</a> for tasks like ACE are based on supervised learning of events from small hand-labeled data. However, hand-labeled training data is expensive to produce, in low coverage of event types, and limited in size, which makes <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised methods</a> hard to extract large scale of events for knowledge base population. To solve the data labeling problem, we propose to automatically label training data for event extraction via <a href=https://en.wikipedia.org/wiki/World_knowledge>world knowledge</a> and linguistic knowledge, which can detect key arguments and trigger words for each event type and employ them to label events in texts automatically. The experimental results show that the <a href=https://en.wikipedia.org/wiki/Quality_(business)>quality</a> of our large scale automatically labeled data is competitive with elaborately human-labeled data. And our automatically labeled data can incorporate with human-labeled data, then improve the performance of <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> learned from these <a href=https://en.wikipedia.org/wiki/Data>data</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1164.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1164 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1164 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-1164/>Exploiting Argument Information to Improve Event Detection via Supervised Attention Mechanisms</a></strong><br><a href=/people/s/shulin-liu/>Shulin Liu</a>
|
<a href=/people/y/yubo-chen/>Yubo Chen</a>
|
<a href=/people/k/kang-liu/>Kang Liu</a>
|
<a href=/people/j/jun-zhao/>Jun Zhao</a><br><a href=/volumes/P17-1/ class=text-muted>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1164><div class="card-body p-3 small">This paper tackles the task of event detection (ED), which involves identifying and categorizing events. We argue that <a href=https://en.wikipedia.org/wiki/Argument>arguments</a> provide significant clues to this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>, but they are either completely ignored or exploited in an indirect manner in existing detection approaches. In this work, we propose to exploit <a href=https://en.wikipedia.org/wiki/Argument_(linguistics)>argument information</a> explicitly for <a href=https://en.wikipedia.org/wiki/Argument_(linguistics)>ED</a> via supervised attention mechanisms. In specific, we systematically investigate the proposed <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> under the supervision of different <a href=https://en.wikipedia.org/wiki/Attentional_control>attention strategies</a>. Experimental results show that our approach advances <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-arts</a> and achieves the best F1 score on ACE 2005 dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1011.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1011 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1011 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=E17-1011" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/E17-1011/>Which is the Effective Way for Gaokao : <a href=https://en.wikipedia.org/wiki/Information_retrieval>Information Retrieval</a> or <a href=https://en.wikipedia.org/wiki/Neural_network>Neural Networks</a>?<span class=acl-fixed-case>G</span>aokao: Information Retrieval or Neural Networks?</a></strong><br><a href=/people/s/shangmin-guo/>Shangmin Guo</a>
|
<a href=/people/x/xiangrong-zeng/>Xiangrong Zeng</a>
|
<a href=/people/s/shizhu-he/>Shizhu He</a>
|
<a href=/people/k/kang-liu/>Kang Liu</a>
|
<a href=/people/j/jun-zhao/>Jun Zhao</a><br><a href=/volumes/E17-1/ class=text-muted>Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1011><div class="card-body p-3 small">As one of the most important test of China, <a href=https://en.wikipedia.org/wiki/Gaokao>Gaokao</a> is designed to be difficult enough to distinguish the excellent high school students. In this work, we detailed the Gaokao History Multiple Choice Questions(GKHMC) and proposed two different approaches to address them using various resources. One approach is based on entity search technique (IR approach), the other is based on text entailment approach where we specifically employ deep neural networks(NN approach). The result of experiment on our collected real Gaokao questions showed that they are good at different categories of questions, that is IR approach performs much better at entity questions(EQs) while NN approach shows its advantage on sentence questions(SQs). We achieve state-of-the-art performance and show that it&#8217;s indispensable to apply hybrid method when participating in the real-world tests.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Kang+Liu+%28%E5%88%98%E5%BA%B7%29" title="Search for 'Kang Liu (刘康)' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/j/jun-zhao/ class=align-middle>Jun Zhao</a>
<span class="badge badge-secondary align-middle ml-2">28</span></li><li class=list-group-item><a href=/people/s/shizhu-he/ class=align-middle>Shizhu He (何世柱)</a>
<span class="badge badge-secondary align-middle ml-2">12</span></li><li class=list-group-item><a href=/people/y/yubo-chen/ class=align-middle>Yubo Chen</a>
<span class="badge badge-secondary align-middle ml-2">12</span></li><li class=list-group-item><a href=/people/y/yuanzhe-zhang/ class=align-middle>Yuanzhe Zhang</a>
<span class="badge badge-secondary align-middle ml-2">7</span></li><li class=list-group-item><a href=/people/s/shengping-liu/ class=align-middle>Shengping Liu</a>
<span class="badge badge-secondary align-middle ml-2">6</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/p/pengfei-cao/ class=align-middle>Pengfei Cao</a>
<span class="badge badge-secondary align-middle ml-2">5</span></li><li class=list-group-item><a href=/people/c/cao-liu/ class=align-middle>Cao Liu</a>
<span class="badge badge-secondary align-middle ml-2">5</span></li><li class=list-group-item><a href=/people/y/yantao-jia/ class=align-middle>Yantao Jia</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/h/hua-wu/ class=align-middle>Hua Wu (吴华)</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/y/yanchao-hao/ class=align-middle>Yanchao Hao</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/z/zhongtao-jiang/ class=align-middle>Zhongtao Jiang</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/s/shangmin-guo/ class=align-middle>Shangmin Guo</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/z/zhixing-tian/ class=align-middle>Zhixing Tian</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/z/zhicheng-sheng/ class=align-middle>Zhicheng Sheng</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/s/shulin-liu/ class=align-middle>Shulin Liu</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/x/xiang-zhang/ class=align-middle>Xiang Zhang</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/y/yafei-shi/ class=align-middle>Yafei Shi</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/x/xiangrong-zeng/ class=align-middle>Xiangrong Zeng</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/j/jian-liu/ class=align-middle>Jian Liu</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/z/zaiqing-nie/ class=align-middle>Zaiqing Nie</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/s/sheng-li/ class=align-middle>Sheng Li (李生)</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/maosong-sun/ class=align-middle>Maosong Sun (孙茂松)</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yang-liu-ict/ class=align-middle>Yang Liu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/w/wanxiang-che/ class=align-middle>Wanxiang Che (车万翔)</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/g/gaoqi-rao/ class=align-middle>Gaoqi Rao (饶高琦)</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/hao-liu/ class=align-middle>Hao Liu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/x/xinyu-zuo/ class=align-middle>Xinyu Zuo</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yuguang-chen/ class=align-middle>Yuguang Chen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/w/weihua-peng/ class=align-middle>Weihua Peng</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/zhao-yang/ class=align-middle>Zhao Yang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/zhuoyu-wei/ class=align-middle>Zhuoyu Wei</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/w/weifeng-chong/ class=align-middle>Weifeng Chong</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/tao-zhang/ class=align-middle>Tao Zhang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/shiwan-liu/ class=align-middle>Shiwan Liu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jiarun-cao/ class=align-middle>Jiarun Cao</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/zhanyi-liu/ class=align-middle>Zhanyi Liu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/p/pei-chen/ class=align-middle>Pei Chen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/taifeng-wang/ class=align-middle>Taifeng Wang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/hang-yang/ class=align-middle>Hang Yang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/q/qingbin-liu/ class=align-middle>Qingbin Liu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jiansong-chen/ class=align-middle>Jiansong Chen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/x/xunliang-cai/ class=align-middle>Xunliang Cai</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/f/fan-yang/ class=align-middle>Fan Yang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/cheng-yan/ class=align-middle>Cheng Yan</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yiming-ju/ class=align-middle>Yiming Ju</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/x/xiaohuan-cao/ class=align-middle>Xiaohuan Cao</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/w/wenting-zhao/ class=align-middle>Wenting Zhao</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jinlong-li/ class=align-middle>Jinlong Li</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/dianbo-sui/ class=align-middle>Dianbo Sui</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/chenhao-wang/ class=align-middle>Chenhao Wang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/w/wei-bi/ class=align-middle>Wei Bi</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/b/baoli-zhang/ class=align-middle>Baoli Zhang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/zhucong-li/ class=align-middle>Zhucong Li</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/zhen-gan/ class=align-middle>Zhen Gan</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jing-wan/ class=align-middle>Jing Wan</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/daojian-zeng/ class=align-middle>Daojian Zeng</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/delai-qiu/ class=align-middle>Delai Qiu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/x/xinwei-feng/ class=align-middle>Xinwei Feng</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/x/xiangwen-liao/ class=align-middle>Xiangwen Liao</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/w/wenbin-jiang/ class=align-middle>Wenbin Jiang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yajuan-lyu/ class=align-middle>Yajuan Lyu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/g/guirong-bai/ class=align-middle>Guirong Bai</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/emnlp/ class=align-middle>EMNLP</a><span class="badge badge-secondary align-middle ml-2">12</span></li><li class=list-group-item><a href=/venues/acl/ class=align-middle>ACL</a><span class="badge badge-secondary align-middle ml-2">10</span></li><li class=list-group-item><a href=/venues/ccl/ class=align-middle>CCL</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/coling/ class=align-middle>COLING</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/eacl/ class=align-middle>EACL</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-venues aria-expanded=false aria-controls=more-venues>show all...</li><div class="collapse border-top" id=more-venues><li class=list-group-item><a href=/venues/ijcnlp/ class=align-middle>IJCNLP</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/findings/ class=align-middle>Findings</a><span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>