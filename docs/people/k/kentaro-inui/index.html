<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Kentaro Inui - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Kentaro</span> <span class=font-weight-bold>Inui</span></h2><hr><div class=row><div class=col-lg-9><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.405.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--405 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.405 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.405" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.405/>Lower Perplexity is Not Always Human-Like</a></strong><br><a href=/people/t/tatsuki-kuribayashi/>Tatsuki Kuribayashi</a>
|
<a href=/people/y/yohei-oseki/>Yohei Oseki</a>
|
<a href=/people/t/takumi-ito/>Takumi Ito</a>
|
<a href=/people/r/ryo-yoshida/>Ryo Yoshida</a>
|
<a href=/people/m/masayuki-asahara/>Masayuki Asahara</a>
|
<a href=/people/k/kentaro-inui/>Kentaro Inui</a><br><a href=/volumes/2021.acl-long/ class=text-muted>Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--405><div class="card-body p-3 small">In computational psycholinguistics, various <a href=https://en.wikipedia.org/wiki/Language_model>language models</a> have been evaluated against human reading behavior (e.g., eye movement) to build human-like computational models. However, most previous efforts have focused almost exclusively on <a href=https://en.wikipedia.org/wiki/English_language>English</a>, despite the recent trend towards <a href=https://en.wikipedia.org/wiki/Linguistic_universal>linguistic universal</a> within the general community. In order to fill the gap, this paper investigates whether the established results in computational psycholinguistics can be generalized across languages. Specifically, we re-examine an established generalization the lower perplexity a <a href=https://en.wikipedia.org/wiki/Language_model>language model</a> has, the more human-like the <a href=https://en.wikipedia.org/wiki/Language_model>language model</a> is in <a href=https://en.wikipedia.org/wiki/Japanese_language>Japanese</a> with typologically different structures from <a href=https://en.wikipedia.org/wiki/English_language>English</a>. Our experiments demonstrate that this established <a href=https://en.wikipedia.org/wiki/Generalization>generalization</a> exhibits a surprising lack of universality ; namely, lower perplexity is not always human-like. Moreover, this discrepancy between <a href=https://en.wikipedia.org/wiki/English_language>English</a> and <a href=https://en.wikipedia.org/wiki/Japanese_language>Japanese</a> is further explored from the perspective of (non-)uniform information density. Overall, our results suggest that a cross-lingual evaluation will be necessary to construct human-like computational models.<i>the lower perplexity a language model has, the more human-like the language model is</i>&#8212; in Japanese with typologically different structures from English. Our experiments demonstrate that this established generalization exhibits a surprising lack of universality; namely, lower perplexity is not always human-like. Moreover, this discrepancy between English and Japanese is further explored from the perspective of (non-)uniform information density. Overall, our results suggest that a cross-lingual evaluation will be necessary to construct human-like computational models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.321.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--321 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.321 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.eacl-main.321" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.321/>Two Training Strategies for Improving <a href=https://en.wikipedia.org/wiki/Relation_extraction>Relation Extraction</a> over Universal Graph</a></strong><br><a href=/people/q/qin-dai/>Qin Dai</a>
|
<a href=/people/n/naoya-inoue/>Naoya Inoue</a>
|
<a href=/people/r/ryo-takahashi/>Ryo Takahashi</a>
|
<a href=/people/k/kentaro-inui/>Kentaro Inui</a><br><a href=/volumes/2021.eacl-main/ class=text-muted>Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--321><div class="card-body p-3 small">This paper explores how the Distantly Supervised Relation Extraction (DS-RE) can benefit from the use of a Universal Graph (UG), the combination of a Knowledge Graph (KG) and a large-scale text collection. A straightforward extension of a current state-of-the-art neural model for DS-RE with a UG may lead to degradation in performance. We first report that this degradation is associated with the difficulty in learning a UG and then propose two training strategies : (1) Path Type Adaptive Pretraining, which sequentially trains the model with different types of UG paths so as to prevent the reliance on a single type of UG path ; and (2) Complexity Ranking Guided Attention mechanism, which restricts the attention span according to the complexity of a UG path so as to force the model to extract features not only from simple UG paths but also from complex ones. Experimental results on both biomedical and NYT10 datasets prove the robustness of our methods and achieve a new state-of-the-art result on the NYT10 dataset. The code and datasets used in this paper are available at https://github.com/baodaiqin/UGDSRE.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.335.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--335 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.335 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.335" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.335/>Transformer-based Lexically Constrained Headline Generation</a></strong><br><a href=/people/k/kosuke-yamada/>Kosuke Yamada</a>
|
<a href=/people/y/yuta-hitomi/>Yuta Hitomi</a>
|
<a href=/people/h/hideaki-tamori/>Hideaki Tamori</a>
|
<a href=/people/r/ryohei-sasano/>Ryohei Sasano</a>
|
<a href=/people/n/naoaki-okazaki/>Naoaki Okazaki</a>
|
<a href=/people/k/kentaro-inui/>Kentaro Inui</a>
|
<a href=/people/k/koichi-takeda/>Koichi Takeda</a><br><a href=/volumes/2021.emnlp-main/ class=text-muted>Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--335><div class="card-body p-3 small">This paper explores a variant of automatic headline generation methods, where a generated headline is required to include a given phrase such as a company or a product name. Previous methods using Transformer-based models generate a <a href=https://en.wikipedia.org/wiki/Headline>headline</a> including a given phrase by providing the <a href=https://en.wikipedia.org/wiki/Encoder>encoder</a> with additional information corresponding to the given phrase. However, these methods can not always include the phrase in the generated <a href=https://en.wikipedia.org/wiki/Headline>headline</a>. Inspired by previous RNN-based methods generating token sequences in backward and forward directions from the given phrase, we propose a simple Transformer-based method that guarantees to include the given phrase in the high-quality generated headline. We also consider a new headline generation strategy that takes advantage of the controllable generation order of Transformer. Our experiments with the Japanese News Corpus demonstrate that our methods, which are guaranteed to include the phrase in the generated <a href=https://en.wikipedia.org/wiki/Headline>headline</a>, achieve ROUGE scores comparable to previous Transformer-based methods. We also show that our generation strategy performs better than previous <a href=https://en.wikipedia.org/wiki/Strategy>strategies</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.304.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--304 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.304 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.304/>Learning to Learn to be Right for the Right Reasons</a></strong><br><a href=/people/p/pride-kavumba/>Pride Kavumba</a>
|
<a href=/people/b/benjamin-heinzerling/>Benjamin Heinzerling</a>
|
<a href=/people/a/ana-brassard/>Ana Brassard</a>
|
<a href=/people/k/kentaro-inui/>Kentaro Inui</a><br><a href=/volumes/2021.naacl-main/ class=text-muted>Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--304><div class="card-body p-3 small">Improving model generalization on held-out data is one of the core objectives in common- sense reasoning. Recent work has shown that models trained on the dataset with superficial cues tend to perform well on the easy test set with superficial cues but perform poorly on the hard test set without superficial cues. Previous approaches have resorted to manual methods of encouraging models not to overfit to superficial cues. While some of the <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>methods</a> have improved performance on hard instances, they also lead to degraded performance on easy in- stances. Here, we propose to explicitly learn a <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> that does well on both the easy test set with superficial cues and the hard test set without superficial cues. Using a meta-learning objective, we learn such a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> that improves performance on both the easy test set and the hard test set. By evaluating our models on Choice of Plausible Alternatives (COPA) and Commonsense Explanation, we show that our proposed method leads to improved performance on both the easy test set and the hard test set upon which we observe up to 16.5 percentage points improvement over the baseline.</div></div><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.acl-srw.30.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--acl-srw--30 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.acl-srw.30 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://slideslive.com/38928673 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.acl-srw.30" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.acl-srw.30/>Embeddings of Label Components for Sequence Labeling : A Case Study of Fine-grained Named Entity Recognition</a></strong><br><a href=/people/t/takuma-kato/>Takuma Kato</a>
|
<a href=/people/k/kaori-abe/>Kaori Abe</a>
|
<a href=/people/h/hiroki-ouchi/>Hiroki Ouchi</a>
|
<a href=/people/s/shumpei-miyawaki/>Shumpei Miyawaki</a>
|
<a href=/people/j/jun-suzuki/>Jun Suzuki</a>
|
<a href=/people/k/kentaro-inui/>Kentaro Inui</a><br><a href=/volumes/2020.acl-srw/ class=text-muted>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--acl-srw--30><div class="card-body p-3 small">In general, the labels used in <a href=https://en.wikipedia.org/wiki/Sequence_labeling>sequence labeling</a> consist of different types of elements. For example, IOB-format entity labels, such as B-Person and I-Person, can be decomposed into span (B and I) and type information (Person). However, while most sequence labeling models do not consider such label components, the shared components across labels, such as <a href=https://en.wikipedia.org/wiki/Person_(disambiguation)>Person</a>, can be beneficial for label prediction. In this work, we propose to integrate label component information as <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a> into <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>. Through experiments on English and Japanese fine-grained named entity recognition, we demonstrate that the proposed method improves performance, especially for instances with low-frequency labels.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.sustainlp-1.6.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--sustainlp-1--6 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.sustainlp-1.6 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.sustainlp-1.6.OptionalSupplementaryMaterial.pdf data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939427 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.sustainlp-1.6/>Efficient Estimation of Influence of a Training Instance</a></strong><br><a href=/people/s/sosuke-kobayashi/>Sosuke Kobayashi</a>
|
<a href=/people/s/sho-yokoi/>Sho Yokoi</a>
|
<a href=/people/j/jun-suzuki/>Jun Suzuki</a>
|
<a href=/people/k/kentaro-inui/>Kentaro Inui</a><br><a href=/volumes/2020.sustainlp-1/ class=text-muted>Proceedings of SustaiNLP: Workshop on Simple and Efficient Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--sustainlp-1--6><div class="card-body p-3 small">Understanding the influence of a training instance on a <a href=https://en.wikipedia.org/wiki/Neural_network>neural network model</a> leads to improving <a href=https://en.wikipedia.org/wiki/Interpretability>interpretability</a>. However, it is difficult and inefficient to evaluate the influence, which shows how a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>&#8217;s prediction would be changed if a training instance were not used. In this paper, we propose an efficient <a href=https://en.wikipedia.org/wiki/Methodology>method</a> for estimating the influence. Our method is inspired by dropout, which zero-masks a <a href=https://en.wikipedia.org/wiki/Subnetwork>sub-network</a> and prevents the <a href=https://en.wikipedia.org/wiki/Subnetwork>sub-network</a> from learning each training instance. By switching between dropout masks, we can use sub-networks that learned or did not learn each training instance and estimate its influence. Through experiments with BERT and VGGNet on classification datasets, we demonstrate that the proposed method can capture training influences, enhance the interpretability of error predictions, and cleanse the training dataset for improving <a href=https://en.wikipedia.org/wiki/Generalization>generalization</a>.</div></div><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1000.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1000/>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</a></strong><br><a href=/people/k/kentaro-inui/>Kentaro Inui</a>
|
<a href=/people/j/jing-jiang/>Jing Jiang</a>
|
<a href=/people/v/vincent-ng/>Vincent Ng</a>
|
<a href=/people/x/xiaojun-wan/>Xiaojun Wan</a><br><a href=/volumes/D19-1/ class=text-muted>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1379.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1379 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1379 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1379.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D19-1379/>Transductive Learning of Neural Language Models for Syntactic and Semantic Analysis</a></strong><br><a href=/people/h/hiroki-ouchi/>Hiroki Ouchi</a>
|
<a href=/people/j/jun-suzuki/>Jun Suzuki</a>
|
<a href=/people/k/kentaro-inui/>Kentaro Inui</a><br><a href=/volumes/D19-1/ class=text-muted>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1379><div class="card-body p-3 small">In <a href=https://en.wikipedia.org/wiki/Transductive_learning>transductive learning</a>, an unlabeled test set is used for <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>model training</a>. Although this <a href=https://en.wikipedia.org/wiki/Setting_(narrative)>setting</a> deviates from the common assumption of a completely unseen test set, it is applicable in many real-world scenarios, wherein the texts to be processed are known in advance. However, despite its practical advantages, <a href=https://en.wikipedia.org/wiki/Transductive_learning>transductive learning</a> is underexplored in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a>. Here we conduct an empirical study of <a href=https://en.wikipedia.org/wiki/Transductive_learning>transductive learning</a> for neural models and demonstrate its utility in syntactic and semantic tasks. Specifically, we fine-tune language models (LMs) on an unlabeled test set to obtain test-set-specific word representations. Through extensive experiments, we demonstrate that despite its simplicity, transductive LM fine-tuning consistently improves state-of-the-art neural models in in-domain and out-of-domain settings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-3039.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-3039 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-3039 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-3039" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-3039/>TEASPN : Framework and Protocol for Integrated Writing Assistance Environments<span class=acl-fixed-case>TEASPN</span>: Framework and Protocol for Integrated Writing Assistance Environments</a></strong><br><a href=/people/m/masato-hagiwara/>Masato Hagiwara</a>
|
<a href=/people/t/takumi-ito/>Takumi Ito</a>
|
<a href=/people/t/tatsuki-kuribayashi/>Tatsuki Kuribayashi</a>
|
<a href=/people/j/jun-suzuki/>Jun Suzuki</a>
|
<a href=/people/k/kentaro-inui/>Kentaro Inui</a><br><a href=/volumes/D19-3/ class=text-muted>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP): System Demonstrations</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-3039><div class="card-body p-3 small">Language technologies play a key role in assisting people with their writing. Although there has been steady progress in e.g., grammatical error correction (GEC), human writers are yet to benefit from this progress due to the high development cost of integrating with <a href=https://en.wikipedia.org/wiki/Writing_system>writing software</a>. We propose TEASPN, a <a href=https://en.wikipedia.org/wiki/Communication_protocol>protocol</a> and an open-source framework for achieving integrated writing assistance environments. The <a href=https://en.wikipedia.org/wiki/Communication_protocol>protocol</a> standardizes the way <a href=https://en.wikipedia.org/wiki/Computer-aided_software_engineering>writing software</a> communicates with servers that implement such technologies, allowing developers and researchers to integrate the latest developments in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing (NLP)</a> with low cost. As a result, users can enjoy the integrated experience in their favorite <a href=https://en.wikipedia.org/wiki/Writing_system>writing software</a>. The results from experiments with human participants show that users use a wide range of technologies and rate their writing experience favorably, allowing them to write more fluent text.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-6119.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-6119 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-6119 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-6119/>Inject Rubrics into Short Answer Grading System</a></strong><br><a href=/people/t/tianqi-wang/>Tianqi Wang</a>
|
<a href=/people/n/naoya-inoue/>Naoya Inoue</a>
|
<a href=/people/h/hiroki-ouchi/>Hiroki Ouchi</a>
|
<a href=/people/t/tomoya-mizumoto/>Tomoya Mizumoto</a>
|
<a href=/people/k/kentaro-inui/>Kentaro Inui</a><br><a href=/volumes/D19-61/ class=text-muted>Proceedings of the 2nd Workshop on Deep Learning Approaches for Low-Resource NLP (DeepLo 2019)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-6119><div class="card-body p-3 small">Short Answer Grading (SAG) is a task of scoring students&#8217; answers in examinations. Most existing SAG systems predict scores based only on the answers, including the model used as base line in this paper, which gives the-state-of-the-art performance. But they ignore important evaluation criteria such as <a href=https://en.wikipedia.org/wiki/Rubric_(academic)>rubrics</a>, which play a crucial role for evaluating answers in real-world situations. In this paper, we present a method to inject information from rubrics into SAG systems. We implement our approach on top of word-level attention mechanism to introduce the rubric information, in order to locate information in each answer that are highly related to the score. Our experimental results demonstrate that injecting rubric information effectively contributes to the performance improvement and that our proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms the state-of-the-art SAG model on the widely used ASAP-SAS dataset under low-resource settings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-6610.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-6610 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-6610 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-6610/>Improving Evidence Detection by Leveraging Warrants</a></strong><br><a href=/people/k/keshav-singh/>Keshav Singh</a>
|
<a href=/people/p/paul-reisert/>Paul Reisert</a>
|
<a href=/people/n/naoya-inoue/>Naoya Inoue</a>
|
<a href=/people/p/pride-kavumba/>Pride Kavumba</a>
|
<a href=/people/k/kentaro-inui/>Kentaro Inui</a><br><a href=/volumes/D19-66/ class=text-muted>Proceedings of the Second Workshop on Fact Extraction and VERification (FEVER)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-6610><div class="card-body p-3 small">Recognizing the implicit link between a claim and a piece of evidence (i.e. warrant) is the key to improving the performance of evidence detection. In this work, we explore the effectiveness of automatically extracted warrants for evidence detection. Given a claim and candidate evidence, our proposed method extracts multiple warrants via similarity search from an existing, structured corpus of arguments. We then attentively aggregate the extracted warrants, considering the consistency between the given argument and the acquired <a href=https://en.wikipedia.org/wiki/Warrant_(law)>warrants</a>. Although a qualitative analysis on the warrants shows that the extraction method needs to be improved, our results indicate that our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> can still improve the performance of evidence detection.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S19-2185.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S19-2185 data-toggle=collapse aria-expanded=false aria-controls=abstract-S19-2185 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/S19-2185/>The Sally Smedley Hyperpartisan News Detector at SemEval-2019 Task 4<span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2019 Task 4</a></strong><br><a href=/people/k/kazuaki-hanawa/>Kazuaki Hanawa</a>
|
<a href=/people/s/shota-sasaki/>Shota Sasaki</a>
|
<a href=/people/h/hiroki-ouchi/>Hiroki Ouchi</a>
|
<a href=/people/j/jun-suzuki/>Jun Suzuki</a>
|
<a href=/people/k/kentaro-inui/>Kentaro Inui</a><br><a href=/volumes/S19-2/ class=text-muted>Proceedings of the 13th International Workshop on Semantic Evaluation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S19-2185><div class="card-body p-3 small">This paper describes our system submitted to the formal run of SemEval-2019 Task 4 : Hyperpartisan news detection. Our system is based on a <a href=https://en.wikipedia.org/wiki/Linear_classifier>linear classifier</a> using several <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a>, i.e., 1) embedding features based on the pre-trained BERT embeddings, 2) article length features, and 3) embedding features of informative phrases extracted from by-publisher dataset. Our <a href=https://en.wikipedia.org/wiki/System>system</a> achieved 80.9 % <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> on the test set for the formal run and got the 3rd place out of 42 teams.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2601.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-2601 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-2601 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-2601/>Distantly Supervised Biomedical Knowledge Acquisition via Knowledge Graph Based Attention</a></strong><br><a href=/people/q/qin-dai/>Qin Dai</a>
|
<a href=/people/n/naoya-inoue/>Naoya Inoue</a>
|
<a href=/people/p/paul-reisert/>Paul Reisert</a>
|
<a href=/people/r/ryo-takahashi/>Ryo Takahashi</a>
|
<a href=/people/k/kentaro-inui/>Kentaro Inui</a><br><a href=/volumes/W19-26/ class=text-muted>Proceedings of the Workshop on Extracting Structured Knowledge from Scientific Publications</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-2601><div class="card-body p-3 small">The increased demand for structured scientific knowledge has attracted considerable attention in extracting <a href=https://en.wikipedia.org/wiki/Scientific_method>scientific relation</a> from the ever growing <a href=https://en.wikipedia.org/wiki/Scientific_literature>scientific publications</a>. Distant supervision is widely applied approach to automatically generate large amounts of <a href=https://en.wikipedia.org/wiki/Data_type>labelled data</a> with low manual annotation cost. However, distant supervision inevitably accompanies the wrong labelling problem, which will negatively affect the performance of Relation Extraction (RE). To address this issue, (Han et al., 2018) proposes a novel framework for jointly training both RE model and Knowledge Graph Completion (KGC) model to extract structured knowledge from non-scientific dataset. In this work, we firstly investigate the feasibility of this <a href=https://en.wikipedia.org/wiki/Conceptual_framework>framework</a> on <a href=https://en.wikipedia.org/wiki/Scientific_literature>scientific dataset</a>, specifically on <a href=https://en.wikipedia.org/wiki/Medical_research>biomedical dataset</a>. Secondly, to achieve better performance on the biomedical dataset, we extend the <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> with other competitive KGC models. Moreover, we proposed a new end-to-end KGC model to extend the <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a>. Experimental results not only show the feasibility of the framework on the biomedical dataset, but also indicate the effectiveness of our extensions, because our extended model achieves significant and consistent improvements on distant supervised RE as compared with baselines.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2605.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-2605 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-2605 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-2605" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-2605/>Annotating with Pros and Cons of Technologies in Computer Science Papers</a></strong><br><a href=/people/h/hono-shirai/>Hono Shirai</a>
|
<a href=/people/n/naoya-inoue/>Naoya Inoue</a>
|
<a href=/people/j/jun-suzuki/>Jun Suzuki</a>
|
<a href=/people/k/kentaro-inui/>Kentaro Inui</a><br><a href=/volumes/W19-26/ class=text-muted>Proceedings of the Workshop on Extracting Structured Knowledge from Scientific Publications</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-2605><div class="card-body p-3 small">This paper explores a task for extracting a technological expression and its pros / cons from computer science papers. We report ongoing efforts on an annotated corpus of pros / cons and an analysis of the nature of the automatic extraction task. Specifically, we show how to adapt the targeted sentiment analysis task for pros / cons extraction in computer science papers and conduct an annotation study. In order to identify the challenges of the automatic extraction task, we construct a strong baseline model and conduct an error analysis. The experiments show that pros / cons can be consistently annotated by several annotators, and that the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> is challenging due to domain-specific knowledge. The annotated dataset is made publicly available for research purposes.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-8641.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-8641 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-8641 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-8641/>A Large-Scale Multi-Length Headline Corpus for Analyzing Length-Constrained Headline Generation Model Evaluation</a></strong><br><a href=/people/y/yuta-hitomi/>Yuta Hitomi</a>
|
<a href=/people/y/yuya-taguchi/>Yuya Taguchi</a>
|
<a href=/people/h/hideaki-tamori/>Hideaki Tamori</a>
|
<a href=/people/k/ko-kikuta/>Ko Kikuta</a>
|
<a href=/people/j/jiro-nishitoba/>Jiro Nishitoba</a>
|
<a href=/people/n/naoaki-okazaki/>Naoaki Okazaki</a>
|
<a href=/people/k/kentaro-inui/>Kentaro Inui</a>
|
<a href=/people/m/manabu-okumura/>Manabu Okumura</a><br><a href=/volumes/W19-86/ class=text-muted>Proceedings of the 12th International Conference on Natural Language Generation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-8641><div class="card-body p-3 small">Browsing news articles on multiple devices is now possible. The lengths of news article headlines have precise upper bounds, dictated by the size of the display of the relevant device or interface. Therefore, controlling the length of headlines is essential when applying the task of headline generation to <a href=https://en.wikipedia.org/wiki/News_media>news production</a>. However, because there is no corpus of headlines of multiple lengths for a given article, previous research on controlling output length in headline generation has not discussed whether the system outputs could be adequately evaluated without multiple references of different lengths. In this paper, we introduce two <a href=https://en.wikipedia.org/wiki/Text_corpus>corpora</a>, which are Japanese News Corpus (JNC) and JApanese MUlti-Length Headline Corpus (JAMUL), to confirm the validity of previous evaluation settings. The JNC provides common supervision data for headline generation. The JAMUL is a large-scale evaluation dataset for <a href=https://en.wikipedia.org/wiki/Headline>headlines</a> of three different lengths composed by professional editors. We report new findings on these corpora ; for example, although the longest length reference summary can appropriately evaluate the existing methods controlling output length, this evaluation setting has several problems.</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1009.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1009 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1009 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1009/>Distance-Free Modeling of Multi-Predicate Interactions in End-to-End Japanese Predicate-Argument Structure Analysis<span class=acl-fixed-case>J</span>apanese Predicate-Argument Structure Analysis</a></strong><br><a href=/people/y/yuichiroh-matsubayashi/>Yuichiroh Matsubayashi</a>
|
<a href=/people/k/kentaro-inui/>Kentaro Inui</a><br><a href=/volumes/C18-1/ class=text-muted>Proceedings of the 27th International Conference on Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1009><div class="card-body p-3 small">Capturing interactions among multiple predicate-argument structures (PASs) is a crucial issue in the task of analyzing PAS in <a href=https://en.wikipedia.org/wiki/Japanese_language>Japanese</a>. In this paper, we propose new Japanese PAS analysis models that integrate the label prediction information of arguments in multiple PASs by extending the input and last layers of a standard deep bidirectional recurrent neural network (bi-RNN) model. In these models, using the mechanisms of pooling and <a href=https://en.wikipedia.org/wiki/Attention>attention</a>, we aim to directly capture the potential interactions among multiple PASs, without being disturbed by the <a href=https://en.wikipedia.org/wiki/Word_order>word order</a> and <a href=https://en.wikipedia.org/wiki/Distance>distance</a>. Our experiments show that the proposed models improve the prediction accuracy specifically for cases where the predicate and argument are in an indirect dependency relation and achieve a new state of the art in the overall <a href=https://en.wikipedia.org/wiki/F-number>F_1</a> on a standard benchmark corpus.<tex-math>F_1</tex-math> on a standard benchmark corpus.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1286.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1286 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1286 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1286/>Predicting Stances from Social Media Posts using <a href=https://en.wikipedia.org/wiki/Factorization>Factorization Machines</a></a></strong><br><a href=/people/a/akira-sasaki/>Akira Sasaki</a>
|
<a href=/people/k/kazuaki-hanawa/>Kazuaki Hanawa</a>
|
<a href=/people/n/naoaki-okazaki/>Naoaki Okazaki</a>
|
<a href=/people/k/kentaro-inui/>Kentaro Inui</a><br><a href=/volumes/C18-1/ class=text-muted>Proceedings of the 27th International Conference on Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1286><div class="card-body p-3 small">Social media provide platforms to express, discuss, and shape opinions about events and issues in the real world. An important step to analyze the discussions on <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> and to assist in healthy decision-making is stance detection. This paper presents an approach to detect the stance of a user toward a topic based on their stances toward other topics and the social media posts of the user. We apply factorization machines, a widely used method in item recommendation, to model user preferences toward topics from the social media data. The experimental results demonstrate that users&#8217; posts are useful to model topic preferences and therefore predict stances of silent users.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1453.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1453 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1453 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/306150555 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1453" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1453/>What Makes Reading Comprehension Questions Easier?</a></strong><br><a href=/people/s/saku-sugawara/>Saku Sugawara</a>
|
<a href=/people/k/kentaro-inui/>Kentaro Inui</a>
|
<a href=/people/s/satoshi-sekine/>Satoshi Sekine</a>
|
<a href=/people/a/akiko-aizawa/>Akiko Aizawa</a><br><a href=/volumes/D18-1/ class=text-muted>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1453><div class="card-body p-3 small">A challenge in creating a <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> for machine reading comprehension (MRC) is to collect questions that require a sophisticated understanding of language to answer beyond using superficial cues. In this work, we investigate what makes questions easier across recent 12 MRC datasets with three question styles (answer extraction, description, and multiple choice). We propose to employ simple <a href=https://en.wikipedia.org/wiki/Heuristic>heuristics</a> to split each <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> into easy and hard subsets and examine the performance of two baseline models for each of the <a href=https://en.wikipedia.org/wiki/Subset>subsets</a>. We then manually annotate questions sampled from each <a href=https://en.wikipedia.org/wiki/Subset>subset</a> with both validity and requisite reasoning skills to investigate which skills explain the difference between easy and hard questions. From this study, we observed that (i) the baseline performances for the hard subsets remarkably degrade compared to those of entire datasets, (ii) hard questions require knowledge inference and multiple-sentence reasoning in comparison with easy questions, and (iii) multiple-choice questions tend to require a broader range of reasoning skills than answer extraction and description questions. These results suggest that one might overestimate recent advances in <a href=https://en.wikipedia.org/wiki/Microscopic_scale>MRC</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5210.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5210 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5210 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-5210" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-5210/>Feasible Annotation Scheme for Capturing Policy Argument Reasoning using Argument Templates</a></strong><br><a href=/people/p/paul-reisert/>Paul Reisert</a>
|
<a href=/people/n/naoya-inoue/>Naoya Inoue</a>
|
<a href=/people/t/tatsuki-kuribayashi/>Tatsuki Kuribayashi</a>
|
<a href=/people/k/kentaro-inui/>Kentaro Inui</a><br><a href=/volumes/W18-52/ class=text-muted>Proceedings of the 5th Workshop on Argument Mining</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5210><div class="card-body p-3 small">Most of the existing works on <a href=https://en.wikipedia.org/wiki/Argument_mining>argument mining</a> cast the problem of argumentative structure identification as classification tasks (e.g. attack-support relations, stance, explicit premise / claim). This paper goes a step further by addressing the task of automatically identifying reasoning patterns of arguments using predefined templates, which is called argument template (AT) instantiation. The contributions of this work are three-fold. First, we develop a simple, yet expressive set of easily annotatable ATs that can represent a majority of writer&#8217;s reasoning for texts with diverse policy topics while maintaining the computational feasibility of the task. Second, we create a small, but highly reliable annotated corpus of instantiated ATs on top of reliably annotated support and attack relations and conduct an annotation study. Third, we formulate the task of AT instantiation as <a href=https://en.wikipedia.org/wiki/Structured_prediction>structured prediction</a> constrained by a feasible set of <a href=https://en.wikipedia.org/wiki/Template_processor>templates</a>. Our evaluation demonstrates that we can annotate ATs with a reasonably high inter-annotator agreement, and the use of template-constrained inference is useful for instantiating ATs with only partial reasoning comprehension clues.<i>argument template (AT) instantiation</i>. The contributions of this work are three-fold. First, we develop a simple, yet expressive set of easily annotatable ATs that can represent a majority of writer&#8217;s reasoning for texts with diverse policy topics while maintaining the computational feasibility of the task. Second, we create a small, but highly reliable annotated corpus of instantiated ATs on top of reliably annotated support and attack relations and conduct an annotation study. Third, we formulate the task of AT instantiation as structured prediction constrained by a feasible set of templates. Our evaluation demonstrates that we can annotate ATs with a reasonably high inter-annotator agreement, and the use of template-constrained inference is useful for instantiating ATs with only partial reasoning comprehension clues.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5607.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5607 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5607 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5607/>Investigating the Challenges of Temporal Relation Extraction from Clinical Text</a></strong><br><a href=/people/d/diana-galvan/>Diana Galvan</a>
|
<a href=/people/n/naoaki-okazaki/>Naoaki Okazaki</a>
|
<a href=/people/k/koji-matsuda/>Koji Matsuda</a>
|
<a href=/people/k/kentaro-inui/>Kentaro Inui</a><br><a href=/volumes/W18-56/ class=text-muted>Proceedings of the Ninth International Workshop on Health Text Mining and Information Analysis</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5607><div class="card-body p-3 small">Temporal reasoning remains as an unsolved task for <a href=https://en.wikipedia.org/wiki/Natural_language_processing>Natural Language Processing (NLP)</a>, particularly demonstrated in the <a href=https://en.wikipedia.org/wiki/Clinical_psychology>clinical domain</a>. The complexity of temporal representation in language is evident as results of the 2016 Clinical TempEval challenge indicate : the current state-of-the-art systems perform well in solving mention-identification tasks of event and time expressions but poorly in temporal relation extraction, showing a gap of around 0.25 point below human performance. We explore to adapt the tree-based LSTM-RNN model proposed by Miwa and Bansal (2016) to temporal relation extraction from clinical text, obtaining a five point improvement over the best 2016 Clinical TempEval system and two points over the state-of-the-art. We deliver a deep analysis of the results and discuss the next step towards human-like temporal reasoning.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1200.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1200 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1200 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1200.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285805371 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-1200" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-1200/>Interpretable and Compositional Relation Learning by Joint Training with an <a href=https://en.wikipedia.org/wiki/Autoencoder>Autoencoder</a></a></strong><br><a href=/people/r/ryo-takahashi/>Ryo Takahashi</a>
|
<a href=/people/r/ran-tian/>Ran Tian</a>
|
<a href=/people/k/kentaro-inui/>Kentaro Inui</a><br><a href=/volumes/P18-1/ class=text-muted>Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1200><div class="card-body p-3 small">Embedding models for entities and relations are extremely useful for recovering missing facts in a <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge base</a>. Intuitively, a <a href=https://en.wikipedia.org/wiki/Binary_relation>relation</a> can be modeled by a matrix mapping entity vectors. However, <a href=https://en.wikipedia.org/wiki/Matrix_(mathematics)>relations</a> reside on low dimension sub-manifolds in the parameter space of arbitrary matrices for one reason, composition of two relations M1, M2 may match a third M3 (e.g. composition of relations currency_of_country and country_of_film usually matches currency_of_film_budget), which imposes compositional constraints to be satisfied by the parameters (i.e. M1*M2 = M3). In this paper we investigate a dimension reduction technique by training <a href=https://en.wikipedia.org/wiki/Binary_relation>relations</a> jointly with an <a href=https://en.wikipedia.org/wiki/Autoencoder>autoencoder</a>, which is expected to better capture compositional constraints. We achieve state-of-the-art on Knowledge Base Completion tasks with strongly improved Mean Rank, and show that joint training with an <a href=https://en.wikipedia.org/wiki/Autoencoder>autoencoder</a> leads to interpretable sparse codings of relations, helps discovering compositional constraints and benefits from compositional training. Our source code is released at.<url>github.com/tianran/glimvec</url>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2091.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2091 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2091 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2091.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-2091/>Unsupervised Learning of Style-sensitive Word Vectors</a></strong><br><a href=/people/r/reina-akama/>Reina Akama</a>
|
<a href=/people/k/kento-watanabe/>Kento Watanabe</a>
|
<a href=/people/s/sho-yokoi/>Sho Yokoi</a>
|
<a href=/people/s/sosuke-kobayashi/>Sosuke Kobayashi</a>
|
<a href=/people/k/kentaro-inui/>Kentaro Inui</a><br><a href=/volumes/P18-2/ class=text-muted>Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2091><div class="card-body p-3 small">This paper presents the first study aimed at capturing stylistic similarity between words in an unsupervised manner. We propose extending the continuous bag of words (CBOW) embedding model (Mikolov et al., 2013b) to learn style-sensitive word vectors using a wider context window under the assumption that the style of all the words in an utterance is consistent. In addition, we introduce a novel <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> to predict lexical stylistic similarity and to create a benchmark dataset for this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. Our experiment with this <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> supports our assumption and demonstrates that the proposed <a href=https://en.wikipedia.org/wiki/Plug-in_(computing)>extensions</a> contribute to the acquisition of style-sensitive word embeddings.</div></div><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1048.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1048 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1048 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=I17-1048" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/I17-1048/>A Neural Language Model for Dynamically Representing the Meanings of Unknown Words and Entities in a Discourse</a></strong><br><a href=/people/s/sosuke-kobayashi/>Sosuke Kobayashi</a>
|
<a href=/people/n/naoaki-okazaki/>Naoaki Okazaki</a>
|
<a href=/people/k/kentaro-inui/>Kentaro Inui</a><br><a href=/volumes/I17-1/ class=text-muted>Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1048><div class="card-body p-3 small">This study addresses the problem of identifying the meaning of unknown words or entities in a <a href=https://en.wikipedia.org/wiki/Discourse>discourse</a> with respect to the word embedding approaches used in neural language models. We proposed a method for on-the-fly construction and exploitation of <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> in both the input and output layers of a neural model by tracking contexts. This extends the dynamic entity representation used in Kobayashi et al. (2016) and incorporates a copy mechanism proposed independently by Gu et al. (2016) and Gulcehre et al. In addition, we construct a new task and <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> called Anonymized Language Modeling for evaluating the ability to capture <a href=https://en.wikipedia.org/wiki/Meaning_(linguistics)>word meanings</a> while reading. Experiments conducted using our novel <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> show that the proposed variant of RNN language model outperformed the <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline model</a>. Furthermore, the experiments also demonstrate that dynamic updates of an output layer help a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> predict reappearing entities, whereas those of an input layer are effective to predict words following reappearing entities.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-2022.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-2022 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-2022 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-2022/>Revisiting the Design Issues of Local Models for Japanese Predicate-Argument Structure Analysis<span class=acl-fixed-case>J</span>apanese Predicate-Argument Structure Analysis</a></strong><br><a href=/people/y/yuichiroh-matsubayashi/>Yuichiroh Matsubayashi</a>
|
<a href=/people/k/kentaro-inui/>Kentaro Inui</a><br><a href=/volumes/I17-2/ class=text-muted>Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-2022><div class="card-body p-3 small">The research trend in Japanese predicate-argument structure (PAS) analysis is shifting from pointwise prediction models with local features to global models designed to search for globally optimal solutions. However, the existing <a href=https://en.wikipedia.org/wiki/General_circulation_model>global models</a> tend to employ only relatively simple local features ; therefore, the overall performance gains are rather limited. The importance of designing a local model is demonstrated in this study by showing that the performance of a sophisticated local model can be considerably improved with recent feature embedding methods and a feature combination learning based on a <a href=https://en.wikipedia.org/wiki/Neural_network>neural network</a>, outperforming the state-of-the-art global models in F1 on a common benchmark dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-2069.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-2069 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-2069 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/I17-2069.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/I17-2069/>Generating Stylistically Consistent Dialog Responses with Transfer Learning</a></strong><br><a href=/people/r/reina-akama/>Reina Akama</a>
|
<a href=/people/k/kazuaki-inada/>Kazuaki Inada</a>
|
<a href=/people/n/naoya-inoue/>Naoya Inoue</a>
|
<a href=/people/s/sosuke-kobayashi/>Sosuke Kobayashi</a>
|
<a href=/people/k/kentaro-inui/>Kentaro Inui</a><br><a href=/volumes/I17-2/ class=text-muted>Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-2069><div class="card-body p-3 small">We propose a novel, data-driven, and stylistically consistent dialog response generation system. To create a user-friendly system, it is crucial to make generated responses not only appropriate but also stylistically consistent. For leaning both the properties effectively, our proposed <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> has two training stages inspired by <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a>. First, we train the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> to generate appropriate responses, and then we ensure that the responses have a specific style. Experimental results demonstrate that the proposed method produces stylistically consistent responses while maintaining the appropriateness of the responses learned in a general domain.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-2074.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-2074 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-2074 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-2074/>Proofread Sentence Generation as Multi-Task Learning with Editing Operation Prediction</a></strong><br><a href=/people/y/yuta-hitomi/>Yuta Hitomi</a>
|
<a href=/people/h/hideaki-tamori/>Hideaki Tamori</a>
|
<a href=/people/n/naoaki-okazaki/>Naoaki Okazaki</a>
|
<a href=/people/k/kentaro-inui/>Kentaro Inui</a><br><a href=/volumes/I17-2/ class=text-muted>Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-2074><div class="card-body p-3 small">This paper explores the idea of robot editors, automated proofreaders that enable journalists to improve the quality of their articles. We propose a novel neural model of multi-task learning that both generates proofread sentences and predicts the editing operations required to rewrite the source sentences and create the proofread ones. The <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is trained using logs of the revisions made professional editors revising draft newspaper articles written by journalists. Experiments demonstrate the effectiveness of our multi-task learning approach and the potential value of using revision logs for this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1037.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1037 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1037 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-1037/>Other Topics You May Also Agree or Disagree : Modeling Inter-Topic Preferences using Tweets and Matrix Factorization</a></strong><br><a href=/people/a/akira-sasaki/>Akira Sasaki</a>
|
<a href=/people/k/kazuaki-hanawa/>Kazuaki Hanawa</a>
|
<a href=/people/n/naoaki-okazaki/>Naoaki Okazaki</a>
|
<a href=/people/k/kentaro-inui/>Kentaro Inui</a><br><a href=/volumes/P17-1/ class=text-muted>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1037><div class="card-body p-3 small">We presents in this paper our approach for modeling inter-topic preferences of Twitter users : for example, those who agree with the <a href=https://en.wikipedia.org/wiki/Trans-Pacific_Partnership>Trans-Pacific Partnership (TPP)</a> also agree with <a href=https://en.wikipedia.org/wiki/Free_trade>free trade</a>. This kind of knowledge is useful not only for stance detection across multiple topics but also for various real-world applications including <a href=https://en.wikipedia.org/wiki/Opinion_poll>public opinion survey</a>, <a href=https://en.wikipedia.org/wiki/Prediction>electoral prediction</a>, <a href=https://en.wikipedia.org/wiki/Political_campaign>electoral campaigns</a>, and online debates. In order to extract users&#8217; preferences on <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a>, we design linguistic patterns in which people agree and disagree about specific topics (e.g., A is completely wrong). By applying these linguistic patterns to a collection of <a href=https://en.wikipedia.org/wiki/Twitter>tweets</a>, we extract statements agreeing and disagreeing with various topics. Inspired by previous work on item recommendation, we formalize the task of modeling inter-topic preferences as matrix factorization : representing users&#8217; preference as a user-topic matrix and mapping both users and topics onto a latent feature space that abstracts the preferences. Our experimental results demonstrate both that our presented approach is useful in predicting missing preferences of users and that the latent vector representations of topics successfully encode inter-topic preferences.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4208.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4208 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4208 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4208/>Analyzing the Revision Logs of a Japanese Newspaper for Article Quality Assessment<span class=acl-fixed-case>J</span>apanese Newspaper for Article Quality Assessment</a></strong><br><a href=/people/h/hideaki-tamori/>Hideaki Tamori</a>
|
<a href=/people/y/yuta-hitomi/>Yuta Hitomi</a>
|
<a href=/people/n/naoaki-okazaki/>Naoaki Okazaki</a>
|
<a href=/people/k/kentaro-inui/>Kentaro Inui</a><br><a href=/volumes/W17-42/ class=text-muted>Proceedings of the 2017 EMNLP Workshop: Natural Language Processing meets Journalism</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4208><div class="card-body p-3 small">We address the issue of the quality of journalism and analyze daily article revision logs from a <a href=https://en.wikipedia.org/wiki/List_of_newspapers_in_Japan>Japanese newspaper company</a>. The revision logs contain data that can help reveal the requirements of quality journalism such as the types and number of edit operations and aspects commonly focused in revision. This study also discusses potential applications such as <a href=https://en.wikipedia.org/wiki/Quality_assessment>quality assessment</a> and automatic article revision as our future research directions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1119.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1119 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1119 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=E17-1119" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/E17-1119/>Neural Architectures for Fine-grained Entity Type Classification</a></strong><br><a href=/people/s/sonse-shimaoka/>Sonse Shimaoka</a>
|
<a href=/people/p/pontus-stenetorp/>Pontus Stenetorp</a>
|
<a href=/people/k/kentaro-inui/>Kentaro Inui</a>
|
<a href=/people/s/sebastian-riedel/>Sebastian Riedel</a><br><a href=/volumes/E17-1/ class=text-muted>Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1119><div class="card-body p-3 small">In this work, we investigate several neural network architectures for fine-grained entity type classification and make three key contributions. Despite being a natural comparison and addition, previous work on attentive neural architectures have not considered hand-crafted features and we combine these with learnt features and establish that they complement each other. Additionally, through quantitative analysis we establish that the <a href=https://en.wikipedia.org/wiki/Attentional_control>attention mechanism</a> learns to attend over syntactic heads and the phrase containing the mention, both of which are known to be strong hand-crafted features for our task. We introduce parameter sharing between labels through a hierarchical encoding method, that in low-dimensional projections show clear clusters for each type hierarchy. Lastly, despite using the same evaluation dataset, the literature frequently compare <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> trained using different data. We demonstrate that the choice of training data has a drastic impact on performance, which decreases by as much as 9.85 % loose micro F1 score for a previously proposed method. Despite this discrepancy, our best model achieves state-of-the-art results with 75.36 % loose micro F1 score on the well-established Figer (GOLD) dataset and we report the best results for models trained using publicly available data for the OntoNotes dataset with 64.93 % loose micro F1 score.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Kentaro+Inui" title="Search for 'Kentaro Inui' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/n/naoaki-okazaki/ class=align-middle>Naoaki Okazaki</a>
<span class="badge badge-secondary align-middle ml-2">8</span></li><li class=list-group-item><a href=/people/n/naoya-inoue/ class=align-middle>Naoya Inoue</a>
<span class="badge badge-secondary align-middle ml-2">7</span></li><li class=list-group-item><a href=/people/j/jun-suzuki/ class=align-middle>Jun Suzuki</a>
<span class="badge badge-secondary align-middle ml-2">6</span></li><li class=list-group-item><a href=/people/s/sosuke-kobayashi/ class=align-middle>Sosuke Kobayashi</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/people/y/yuta-hitomi/ class=align-middle>Yuta Hitomi</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/h/hideaki-tamori/ class=align-middle>Hideaki Tamori</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/people/h/hiroki-ouchi/ class=align-middle>Hiroki Ouchi</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/people/k/kazuaki-hanawa/ class=align-middle>Kazuaki Hanawa</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/t/tatsuki-kuribayashi/ class=align-middle>Tatsuki Kuribayashi</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/r/ryo-takahashi/ class=align-middle>Ryo Takahashi</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/p/paul-reisert/ class=align-middle>Paul Reisert</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/y/yuichiroh-matsubayashi/ class=align-middle>Yuichiroh Matsubayashi</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/a/akira-sasaki/ class=align-middle>Akira Sasaki</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/t/takumi-ito/ class=align-middle>Takumi Ito</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/r/reina-akama/ class=align-middle>Reina Akama</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/q/qin-dai/ class=align-middle>Qin Dai</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/p/pride-kavumba/ class=align-middle>Pride Kavumba</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/s/sho-yokoi/ class=align-middle>Sho Yokoi</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/y/yohei-oseki/ class=align-middle>Yohei Oseki</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/ryo-yoshida/ class=align-middle>Ryo Yoshida</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/masayuki-asahara/ class=align-middle>Masayuki Asahara</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/kazuaki-inada/ class=align-middle>Kazuaki Inada</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/takuma-kato/ class=align-middle>Takuma Kato</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/kaori-abe/ class=align-middle>Kaori Abe</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/shumpei-miyawaki/ class=align-middle>Shumpei Miyawaki</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/saku-sugawara/ class=align-middle>Saku Sugawara</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/satoshi-sekine/ class=align-middle>Satoshi Sekine</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/akiko-aizawa/ class=align-middle>Akiko Aizawa</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/kosuke-yamada/ class=align-middle>Kosuke Yamada</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/ryohei-sasano/ class=align-middle>Ryohei Sasano</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/koichi-takeda/ class=align-middle>Koichi Takeda</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jing-jiang/ class=align-middle>Jing Jiang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/v/vincent-ng/ class=align-middle>Vincent Ng</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/x/xiaojun-wan/ class=align-middle>Xiaojun Wan</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/masato-hagiwara/ class=align-middle>Masato Hagiwara</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/tianqi-wang/ class=align-middle>Tianqi Wang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/tomoya-mizumoto/ class=align-middle>Tomoya Mizumoto</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/keshav-singh/ class=align-middle>Keshav Singh</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/shota-sasaki/ class=align-middle>Shota Sasaki</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/b/benjamin-heinzerling/ class=align-middle>Benjamin Heinzerling</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/ana-brassard/ class=align-middle>Ana Brassard</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/diana-galvan/ class=align-middle>Diana Galvan</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/koji-matsuda/ class=align-middle>Koji Matsuda</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/hono-shirai/ class=align-middle>Hono Shirai</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yuya-taguchi/ class=align-middle>Yuya Taguchi</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/ko-kikuta/ class=align-middle>Ko Kikuta</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jiro-nishitoba/ class=align-middle>Jiro Nishitoba</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/manabu-okumura/ class=align-middle>Manabu Okumura</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/sonse-shimaoka/ class=align-middle>Sonse Shimaoka</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/p/pontus-stenetorp/ class=align-middle>Pontus Stenetorp</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/sebastian-riedel/ class=align-middle>Sebastian Riedel</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/ran-tian/ class=align-middle>Ran Tian</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/kento-watanabe/ class=align-middle>Kento Watanabe</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/emnlp/ class=align-middle>EMNLP</a><span class="badge badge-secondary align-middle ml-2">7</span></li><li class=list-group-item><a href=/venues/ws/ class=align-middle>WS</a><span class="badge badge-secondary align-middle ml-2">6</span></li><li class=list-group-item><a href=/venues/acl/ class=align-middle>ACL</a><span class="badge badge-secondary align-middle ml-2">5</span></li><li class=list-group-item><a href=/venues/ijcnlp/ class=align-middle>IJCNLP</a><span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/venues/coling/ class=align-middle>COLING</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-venues aria-expanded=false aria-controls=more-venues>show all...</li><div class="collapse border-top" id=more-venues><li class=list-group-item><a href=/venues/eacl/ class=align-middle>EACL</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/semeval/ class=align-middle>SemEval</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/naacl/ class=align-middle>NAACL</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/sustainlp/ class=align-middle>sustainlp</a><span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>