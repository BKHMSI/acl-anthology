<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Kewei Tu - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Kewei</span> <span class=font-weight-bold>Tu</span></h2><hr><div class=row><div class=col-lg-9><h4>2022</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.acl-long.155.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--acl-long--155 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.acl-long.155 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.acl-long.155" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2022.acl-long.155/>Headed-Span-Based Projective Dependency Parsing</a></strong><br><a href=/people/s/songlin-yang/>Songlin Yang</a>
|
<a href=/people/k/kewei-tu/>Kewei Tu</a><br><a href=/volumes/2022.acl-long/ class=text-muted>Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--acl-long--155><div class="card-body p-3 small">We propose a new method for projective dependency parsing based on headed spans. In a projective dependency tree, the largest subtree rooted at each word covers a contiguous sequence (i.e., a span) in the surface order. We call such a span marked by a root word <i>headed span</i>. A projective dependency tree can be represented as a collection of headed spans. We decompose the score of a dependency tree into the scores of the headed spans and design a novel <tex-math>O(n^3)</tex-math> dynamic programming algorithm to enable global training and exact inference. Our model achieves state-of-the-art or competitive results on PTB, CTB, and UD</div></div><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.46.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--46 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.46 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.46" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.46/>Structural Knowledge Distillation : Tractably Distilling Information for Structured Predictor</a></strong><br><a href=/people/x/xinyu-wang/>Xinyu Wang</a>
|
<a href=/people/y/yong-jiang/>Yong Jiang</a>
|
<a href=/people/z/zhaohui-yan/>Zhaohui Yan</a>
|
<a href=/people/z/zixia-jia/>Zixia Jia</a>
|
<a href=/people/n/nguyen-bach/>Nguyen Bach</a>
|
<a href=/people/t/tao-wang/>Tao Wang</a>
|
<a href=/people/z/zhongqiang-huang/>Zhongqiang Huang</a>
|
<a href=/people/f/fei-huang/>Fei Huang</a>
|
<a href=/people/k/kewei-tu/>Kewei Tu</a><br><a href=/volumes/2021.acl-long/ class=text-muted>Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--46><div class="card-body p-3 small">Knowledge distillation is a critical technique to transfer knowledge between models, typically from a large model (the teacher) to a more fine-grained one (the student). The <a href=https://en.wikipedia.org/wiki/Loss_function>objective function</a> of knowledge distillation is typically the <a href=https://en.wikipedia.org/wiki/Cross-entropy>cross-entropy</a> between the teacher and the student&#8217;s output distributions. However, for structured prediction problems, the output space is exponential in size ; therefore, the cross-entropy objective becomes intractable to compute and optimize directly. In this paper, we derive a factorized form of the knowledge distillation objective for <a href=https://en.wikipedia.org/wiki/Structured_prediction>structured prediction</a>, which is tractable for many typical choices of the teacher and student models. In particular, we show the tractability and empirical effectiveness of structural knowledge distillation between sequence labeling and dependency parsing models under four different scenarios : 1) the teacher and student share the same factorization form of the output structure scoring function ; 2) the student factorization produces more fine-grained substructures than the teacher factorization ; 3) the teacher factorization produces more fine-grained substructures than the student factorization ; 4) the factorization forms from the teacher and the student are incompatible.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.142.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--142 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.142 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.142" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.142/>Improving <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>Named Entity Recognition</a> by External Context Retrieving and Cooperative Learning</a></strong><br><a href=/people/x/xinyu-wang/>Xinyu Wang</a>
|
<a href=/people/y/yong-jiang/>Yong Jiang</a>
|
<a href=/people/n/nguyen-bach/>Nguyen Bach</a>
|
<a href=/people/t/tao-wang/>Tao Wang</a>
|
<a href=/people/z/zhongqiang-huang/>Zhongqiang Huang</a>
|
<a href=/people/f/fei-huang/>Fei Huang</a>
|
<a href=/people/k/kewei-tu/>Kewei Tu</a><br><a href=/volumes/2021.acl-long/ class=text-muted>Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--142><div class="card-body p-3 small">Recent advances in Named Entity Recognition (NER) show that document-level contexts can significantly improve <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> performance. In many application scenarios, however, such contexts are not available. In this paper, we propose to find external contexts of a sentence by retrieving and selecting a set of semantically relevant texts through a <a href=https://en.wikipedia.org/wiki/Web_search_engine>search engine</a>, with the original sentence as the query. We find empirically that the contextual representations computed on the retrieval-based input view, constructed through the concatenation of a sentence and its external contexts, can achieve significantly improved performance compared to the original input view based only on the sentence. Furthermore, we can improve the model performance of both input views by <a href=https://en.wikipedia.org/wiki/Cooperative_learning>Cooperative Learning</a>, a training method that encourages the two input views to produce similar contextual representations or output label distributions. Experiments show that our approach can achieve new state-of-the-art performance on 8 NER data sets across 5 domains.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.206.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--206 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.206 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.206" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.206/>Automated Concatenation of Embeddings for <a href=https://en.wikipedia.org/wiki/Structured_prediction>Structured Prediction</a></a></strong><br><a href=/people/x/xinyu-wang/>Xinyu Wang</a>
|
<a href=/people/y/yong-jiang/>Yong Jiang</a>
|
<a href=/people/n/nguyen-bach/>Nguyen Bach</a>
|
<a href=/people/t/tao-wang/>Tao Wang</a>
|
<a href=/people/z/zhongqiang-huang/>Zhongqiang Huang</a>
|
<a href=/people/f/fei-huang/>Fei Huang</a>
|
<a href=/people/k/kewei-tu/>Kewei Tu</a><br><a href=/volumes/2021.acl-long/ class=text-muted>Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--206><div class="card-body p-3 small">Pretrained contextualized embeddings are powerful <a href=https://en.wikipedia.org/wiki/Word_processor_(electronic_device)>word representations</a> for structured prediction tasks. Recent work found that better word representations can be obtained by concatenating different types of <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a>. However, the selection of <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a> to form the best concatenated representation usually varies depending on the task and the collection of candidate embeddings, and the ever-increasing number of embedding types makes it a more difficult problem. In this paper, we propose Automated Concatenation of Embeddings (ACE) to automate the process of finding better concatenations of embeddings for structured prediction tasks, based on a formulation inspired by recent progress on neural architecture search. Specifically, a controller alternately samples a concatenation of embeddings, according to its current belief of the effectiveness of individual embedding types in consideration for a <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>, and updates the belief based on a <a href=https://en.wikipedia.org/wiki/Reward_system>reward</a>. We follow strategies in <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning</a> to optimize the parameters of the controller and compute the reward based on the accuracy of a task model, which is fed with the sampled concatenation as input and trained on a task dataset. Empirical results on 6 tasks and 21 datasets show that our approach outperforms strong baselines and achieves state-of-the-art performance with fine-tuned embeddings in all the evaluations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.207.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--207 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.207 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.207/>Multi-View Cross-Lingual Structured Prediction with Minimum Supervision</a></strong><br><a href=/people/z/zechuan-hu/>Zechuan Hu</a>
|
<a href=/people/y/yong-jiang/>Yong Jiang</a>
|
<a href=/people/n/nguyen-bach/>Nguyen Bach</a>
|
<a href=/people/t/tao-wang/>Tao Wang</a>
|
<a href=/people/z/zhongqiang-huang/>Zhongqiang Huang</a>
|
<a href=/people/f/fei-huang/>Fei Huang</a>
|
<a href=/people/k/kewei-tu/>Kewei Tu</a><br><a href=/volumes/2021.acl-long/ class=text-muted>Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--207><div class="card-body p-3 small">In structured prediction problems, cross-lingual transfer learning is an efficient way to train quality models for low-resource languages, and further improvement can be obtained by learning from multiple source languages. However, not all source models are created equal and some may hurt performance on the target language. Previous work has explored the similarity between source and target sentences as an approximate measure of strength for different source models. In this paper, we propose a multi-view framework, by leveraging a small number of labeled target sentences, to effectively combine multiple source models into an aggregated source view at different granularity levels (language, sentence, or sub-structure), and transfer it to a target view based on a task-specific model. By encouraging the two views to interact with each other, our <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> can dynamically adjust the <a href=https://en.wikipedia.org/wiki/Confidence_interval>confidence level</a> of each source model and improve the performance of both <a href=https://en.wikipedia.org/wiki/View_model>views</a> during training. Experiments for three structured prediction tasks on sixteen data sets show that our framework achieves significant improvement over all existing approaches, including these with access to additional source language data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-tutorials.1.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-tutorials--1 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-tutorials.1 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-tutorials.1/>Unsupervised Natural Language Parsing (Introductory Tutorial)</a></strong><br><a href=/people/k/kewei-tu/>Kewei Tu</a>
|
<a href=/people/y/yong-jiang/>Yong Jiang</a>
|
<a href=/people/w/wenjuan-han/>Wenjuan Han</a>
|
<a href=/people/y/yanpeng-zhao/>Yanpeng Zhao</a><br><a href=/volumes/2021.eacl-tutorials/ class=text-muted>Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Tutorial Abstracts</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-tutorials--1><div class="card-body p-3 small">Unsupervised parsing learns a syntactic parser from training sentences without parse tree annotations. Recently, there has been a resurgence of interest in unsupervised parsing, which can be attributed to the combination of two trends in the NLP community : a general trend towards <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised training</a> or pre-training, and an emerging trend towards finding or modeling linguistic structures in neural models. In this tutorial, we will introduce to the general audience what unsupervised parsing does and how it can be useful for and beyond <a href=https://en.wikipedia.org/wiki/Syntactic_parsing>syntactic parsing</a>. We will then provide a systematic overview of major classes of approaches to unsupervised parsing, namely generative and discriminative approaches, and analyze their relative strengths and weaknesses. We will cover both decade-old statistical approaches and more recent neural approaches to give the audience a sense of the historical and recent development of the field. We will also discuss emerging research topics such as BERT-based approaches and visually grounded learning.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.747.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--747 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.747 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.747/>Neuralizing Regular Expressions for Slot Filling</a></strong><br><a href=/people/c/chengyue-jiang/>Chengyue Jiang</a>
|
<a href=/people/z/zijian-jin/>Zijian Jin</a>
|
<a href=/people/k/kewei-tu/>Kewei Tu</a><br><a href=/volumes/2021.emnlp-main/ class=text-muted>Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--747><div class="card-body p-3 small">Neural models and <a href=https://en.wikipedia.org/wiki/Mathematical_logic>symbolic rules</a> such as <a href=https://en.wikipedia.org/wiki/Regular_expression>regular expressions</a> have their respective merits and weaknesses. In this paper, we study the integration of the two approaches for the slot filling task by converting <a href=https://en.wikipedia.org/wiki/Regular_expression>regular expressions</a> into <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a>. Specifically, we first convert <a href=https://en.wikipedia.org/wiki/Regular_expression>regular expressions</a> into a special form of finite-state transducers, then unfold its approximate inference algorithm as a bidirectional recurrent neural model that performs slot filling via <a href=https://en.wikipedia.org/wiki/Sequence_labeling>sequence labeling</a>. Experimental results show that our model has superior zero-shot and few-shot performance and stays competitive when there are sufficient training data.</div></div><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.182.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--182 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.182 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939221 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.182" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.182/>Adversarial Attack and Defense of Structured Prediction Models</a></strong><br><a href=/people/w/wenjuan-han/>Wenjuan Han</a>
|
<a href=/people/l/liwen-zhang/>Liwen Zhang</a>
|
<a href=/people/y/yong-jiang/>Yong Jiang</a>
|
<a href=/people/k/kewei-tu/>Kewei Tu</a><br><a href=/volumes/2020.emnlp-main/ class=text-muted>Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--182><div class="card-body p-3 small">Building an effective adversarial attacker and elaborating on countermeasures for adversarial attacks for <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing (NLP)</a> have attracted a lot of research in recent years. However, most of the existing approaches focus on <a href=https://en.wikipedia.org/wiki/Taxonomy_(biology)>classification problems</a>. In this paper, we investigate attacks and defenses for structured prediction tasks in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a>. Besides the difficulty of perturbing discrete words and the sentence fluency problem faced by attackers in any NLP tasks, there is a specific challenge to attackers of structured prediction models : the structured output of structured prediction models is sensitive to small perturbations in the input. To address these problems, we propose a novel and unified framework that learns to attack a structured prediction model using a sequence-to-sequence model with feedbacks from multiple reference models of the same structured prediction task. Based on the proposed attack, we further reinforce the victim model with <a href=https://en.wikipedia.org/wiki/Adversarial_learning>adversarial training</a>, making its prediction more robust and accurate. We evaluate the proposed <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> in <a href=https://en.wikipedia.org/wiki/Dependency_grammar>dependency parsing</a> and <a href=https://en.wikipedia.org/wiki/Part-of-speech_tagging>part-of-speech tagging</a>. Automatic and human evaluations show that our proposed framework succeeds in both attacking state-of-the-art structured prediction models and boosting them with adversarial training.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.acl-main.333.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--acl-main--333 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.acl-main.333 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://slideslive.com/38929412 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.acl-main.333" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.acl-main.333/>Towards Holistic and Automatic Evaluation of Open-Domain Dialogue Generation</a></strong><br><a href=/people/b/bo-pang/>Bo Pang</a>
|
<a href=/people/e/erik-nijkamp/>Erik Nijkamp</a>
|
<a href=/people/w/wenjuan-han/>Wenjuan Han</a>
|
<a href=/people/l/linqi-zhou/>Linqi Zhou</a>
|
<a href=/people/y/yixian-liu/>Yixian Liu</a>
|
<a href=/people/k/kewei-tu/>Kewei Tu</a><br><a href=/volumes/2020.acl-main/ class=text-muted>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--acl-main--333><div class="card-body p-3 small">Open-domain dialogue generation has gained increasing attention in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>Natural Language Processing</a>. Its evaluation requires a holistic means. Human ratings are deemed as the gold standard. As <a href=https://en.wikipedia.org/wiki/Evaluation>human evaluation</a> is inefficient and costly, an automated substitute is highly desirable. In this paper, we propose holistic evaluation metrics that capture different aspects of open-domain dialogues. Our metrics consist of (1) GPT-2 based context coherence between sentences in a dialogue, (2) GPT-2 based fluency in phrasing, (3) n-gram based diversity in responses to augmented queries, and (4) textual-entailment-inference based logical self-consistency. The empirical validity of our <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> is demonstrated by strong correlations with <a href=https://en.wikipedia.org/wiki/Judgement>human judgments</a>. We open source the code and relevant materials.<tex-math>n</tex-math>-gram based diversity in responses to augmented queries, and (4) textual-entailment-inference based logical self-consistency. The empirical validity of our metrics is demonstrated by strong correlations with human judgments. We open source the code and relevant materials.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.235.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--235 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.235 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.235" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.235/>Learning Numeral Embedding</a></strong><br><a href=/people/c/chengyue-jiang/>Chengyue Jiang</a>
|
<a href=/people/z/zhonglin-nian/>Zhonglin Nian</a>
|
<a href=/people/k/kaihao-guo/>Kaihao Guo</a>
|
<a href=/people/s/shanbo-chu/>Shanbo Chu</a>
|
<a href=/people/y/yinggong-zhao/>Yinggong Zhao</a>
|
<a href=/people/l/libin-shen/>Libin Shen</a>
|
<a href=/people/k/kewei-tu/>Kewei Tu</a><br><a href=/volumes/2020.findings-emnlp/ class=text-muted>Findings of the Association for Computational Linguistics: EMNLP 2020</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--235><div class="card-body p-3 small">Word embedding is an essential building block for <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning methods</a> for <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a>. Although <a href=https://en.wikipedia.org/wiki/Word_embedding>word embedding</a> has been extensively studied over the years, the problem of how to effectively embed <a href=https://en.wikipedia.org/wiki/Numeral_system>numerals</a>, a special subset of words, is still underexplored. Existing word embedding methods do not learn numeral embeddings well because there are an infinite number of numerals and their individual appearances in training corpora are highly scarce. In this paper, we propose two novel numeral embedding methods that can handle the out-of-vocabulary (OOV) problem for <a href=https://en.wikipedia.org/wiki/Numerical_digit>numerals</a>. We first induce a finite set of prototype numerals using either a <a href=https://en.wikipedia.org/wiki/Self-organizing_map>self-organizing map</a> or a <a href=https://en.wikipedia.org/wiki/Mixture_model>Gaussian mixture model</a>. We then represent the embedding of a <a href=https://en.wikipedia.org/wiki/Numeral_system>numeral</a> as a weighted average of the prototype number embeddings. Numeral embeddings represented in this manner can be plugged into existing word embedding learning approaches such as <a href=https://en.wikipedia.org/wiki/Skip-gram>skip-gram</a> for training. We evaluated our methods and showed its effectiveness on four intrinsic and extrinsic tasks : word similarity, embedding numeracy, numeral prediction, and <a href=https://en.wikipedia.org/wiki/Sequence_labeling>sequence labeling</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.356.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--356 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.356 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.356/>More <a href=https://en.wikipedia.org/wiki/Embedding>Embeddings</a>, Better Sequence Labelers?</a></strong><br><a href=/people/x/xinyu-wang/>Xinyu Wang</a>
|
<a href=/people/y/yong-jiang/>Yong Jiang</a>
|
<a href=/people/n/nguyen-bach/>Nguyen Bach</a>
|
<a href=/people/t/tao-wang/>Tao Wang</a>
|
<a href=/people/z/zhongqiang-huang/>Zhongqiang Huang</a>
|
<a href=/people/f/fei-huang/>Fei Huang</a>
|
<a href=/people/k/kewei-tu/>Kewei Tu</a><br><a href=/volumes/2020.findings-emnlp/ class=text-muted>Findings of the Association for Computational Linguistics: EMNLP 2020</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--356><div class="card-body p-3 small">Recent work proposes a family of contextual embeddings that significantly improves the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of sequence labelers over non-contextual embeddings. However, there is no definite conclusion on whether we can build better sequence labelers by combining different kinds of <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a> in various settings. In this paper, we conduct extensive experiments on 3 tasks over 18 datasets and 8 languages to study the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of sequence labeling with various embedding concatenations and make three observations : (1) concatenating more embedding variants leads to better <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> in rich-resource and cross-domain settings and some conditions of low-resource settings ; (2) concatenating contextual sub-word embeddings with contextual character embeddings hurts the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> in extremely low-resource settings ; (3) based on the conclusion of (1), concatenating additional similar contextual embeddings can not lead to further improvements. We hope these conclusions can help people build stronger sequence labelers in various settings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.227.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--227 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.227 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.227/>A Survey of Unsupervised Dependency Parsing</a></strong><br><a href=/people/w/wenjuan-han/>Wenjuan Han</a>
|
<a href=/people/y/yong-jiang/>Yong Jiang</a>
|
<a href=/people/h/hwee-tou-ng/>Hwee Tou Ng</a>
|
<a href=/people/k/kewei-tu/>Kewei Tu</a><br><a href=/volumes/2020.coling-main/ class=text-muted>Proceedings of the 28th International Conference on Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--227><div class="card-body p-3 small">Syntactic dependency parsing is an important task in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a>. Unsupervised dependency parsing aims to learn a <a href=https://en.wikipedia.org/wiki/Dependency_grammar>dependency parser</a> from sentences that have no annotation of their correct parse trees. Despite its difficulty, unsupervised parsing is an interesting research direction because of its capability of utilizing almost unlimited unannotated text data. It also serves as the basis for other research in low-resource parsing. In this paper, we survey existing <a href=https://en.wikipedia.org/wiki/Parsing>approaches</a> to unsupervised dependency parsing, identify two major classes of <a href=https://en.wikipedia.org/wiki/Parsing>approaches</a>, and discuss recent trends. We hope that our survey can provide insights for researchers and facilitate future research on this topic.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.322.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--322 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.322 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.322/>Deep Inside-outside Recursive Autoencoder with All-span Objective</a></strong><br><a href=/people/r/ruyue-hong/>Ruyue Hong</a>
|
<a href=/people/j/jiong-cai/>Jiong Cai</a>
|
<a href=/people/k/kewei-tu/>Kewei Tu</a><br><a href=/volumes/2020.coling-main/ class=text-muted>Proceedings of the 28th International Conference on Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--322><div class="card-body p-3 small">Deep inside-outside recursive autoencoder (DIORA) is a neural-based model designed for unsupervised constituency parsing. During its forward computation, <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> provides phrase and contextual representations for all spans in the input sentence. By utilizing the contextual representation of each leaf-level span, the span of length 1, to reconstruct the word inside the span, the model is trained without labeled data. In this work, we extend the training objective of DIORA by making use of all spans instead of only leaf-level spans. We test our new training objective on datasets of two languages : <a href=https://en.wikipedia.org/wiki/English_language>English</a> and <a href=https://en.wikipedia.org/wiki/Japanese_language>Japanese</a>, and empirically show that our method achieves improvement in parsing accuracy over the original DIORA.</div></div><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1148.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1148 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1148 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1148/>A Regularization-based Framework for Bilingual Grammar Induction</a></strong><br><a href=/people/y/yong-jiang/>Yong Jiang</a>
|
<a href=/people/w/wenjuan-han/>Wenjuan Han</a>
|
<a href=/people/k/kewei-tu/>Kewei Tu</a><br><a href=/volumes/D19-1/ class=text-muted>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1148><div class="card-body p-3 small">Grammar induction aims to discover <a href=https://en.wikipedia.org/wiki/Syntax>syntactic structures</a> from <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>unannotated sentences</a>. In this paper, we propose a framework in which the learning process of the grammar model of one language is influenced by knowledge from the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> of another language. Unlike previous work on multilingual grammar induction, our approach does not rely on any external resource, such as <a href=https://en.wikipedia.org/wiki/Parallel_text>parallel corpora</a>, <a href=https://en.wikipedia.org/wiki/Word_alignment>word alignments</a> or <a href=https://en.wikipedia.org/wiki/Phylogenetic_tree>linguistic phylogenetic trees</a>. We propose three <a href=https://en.wikipedia.org/wiki/Regularization_(mathematics)>regularization methods</a> that encourage similarity between model parameters, dependency edge scores, and parse trees respectively. We deploy our methods on a state-of-the-art unsupervised discriminative parser and evaluate it on both transfer grammar induction and bilingual grammar induction. Empirical results on multiple languages show that our <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> outperform strong baselines.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1454.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1454 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1454 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1454" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-1454/>Second-Order Semantic Dependency Parsing with End-to-End Neural Networks</a></strong><br><a href=/people/x/xinyu-wang/>Xinyu Wang</a>
|
<a href=/people/j/jingxian-huang/>Jingxian Huang</a>
|
<a href=/people/k/kewei-tu/>Kewei Tu</a><br><a href=/volumes/P19-1/ class=text-muted>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1454><div class="card-body p-3 small">Semantic dependency parsing aims to identify semantic relationships between words in a sentence that form a <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph</a>. In this paper, we propose a second-order semantic dependency parser, which takes into consideration not only individual dependency edges but also interactions between pairs of <a href=https://en.wikipedia.org/wiki/Glossary_of_graph_theory_terms>edges</a>. We show that second-order parsing can be approximated using mean field (MF) variational inference or loopy belief propagation (LBP). We can unfold both <a href=https://en.wikipedia.org/wiki/Algorithm>algorithms</a> as recurrent layers of a <a href=https://en.wikipedia.org/wiki/Neural_network>neural network</a> and therefore can train the <a href=https://en.wikipedia.org/wiki/Parsing>parser</a> in an end-to-end manner. Our experiments show that our <a href=https://en.wikipedia.org/wiki/Scientific_method>approach</a> achieves state-of-the-art performance.</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1109.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1109 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1109 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1109.Notes.zip data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285802453 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-1109" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-1109/>Gaussian Mixture Latent Vector Grammars<span class=acl-fixed-case>G</span>aussian Mixture Latent Vector Grammars</a></strong><br><a href=/people/y/yanpeng-zhao/>Yanpeng Zhao</a>
|
<a href=/people/l/liwen-zhang/>Liwen Zhang</a>
|
<a href=/people/k/kewei-tu/>Kewei Tu</a><br><a href=/volumes/P18-1/ class=text-muted>Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1109><div class="card-body p-3 small">We introduce Latent Vector Grammars (LVeGs), a new framework that extends latent variable grammars such that each nonterminal symbol is associated with a continuous vector space representing the set of (infinitely many) subtypes of the nonterminal. We show that previous models such as latent variable grammars and compositional vector grammars can be interpreted as special cases of LVeGs. We then present Gaussian Mixture LVeGs (GM-LVeGs), a new special case of LVeGs that uses Gaussian mixtures to formulate the weights of production rules over subtypes of nonterminals. A major advantage of using Gaussian mixtures is that the partition function and the expectations of subtype rules can be computed using an extension of the <a href=https://en.wikipedia.org/wiki/Inside-outside_algorithm>inside-outside algorithm</a>, which enables efficient <a href=https://en.wikipedia.org/wiki/Inference>inference</a> and <a href=https://en.wikipedia.org/wiki/Machine_learning>learning</a>. We apply GM-LVeGs to <a href=https://en.wikipedia.org/wiki/Part-of-speech_tagging>part-of-speech tagging</a> and constituency parsing and show that GM-LVeGs can achieve competitive accuracies.</div></div><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1171.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1171 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1171 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1171/>CRF Autoencoder for Unsupervised Dependency Parsing<span class=acl-fixed-case>CRF</span> Autoencoder for Unsupervised Dependency Parsing</a></strong><br><a href=/people/j/jiong-cai/>Jiong Cai</a>
|
<a href=/people/y/yong-jiang/>Yong Jiang</a>
|
<a href=/people/k/kewei-tu/>Kewei Tu</a><br><a href=/volumes/D17-1/ class=text-muted>Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1171><div class="card-body p-3 small">Unsupervised dependency parsing, which tries to discover linguistic dependency structures from unannotated data, is a very challenging task. Almost all previous work on this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> focuses on learning <a href=https://en.wikipedia.org/wiki/Generative_model>generative models</a>. In this paper, we develop an unsupervised dependency parsing model based on the CRF autoencoder. The encoder part of our model is discriminative and globally normalized which allows us to use rich features as well as universal linguistic priors. We propose an exact <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> for <a href=https://en.wikipedia.org/wiki/Parsing>parsing</a> as well as a tractable learning algorithm. We evaluated the performance of our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> on eight multilingual treebanks and found that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieved comparable performance with state-of-the-art approaches.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1176.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1176 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1176 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D17-1176.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D17-1176/>Dependency Grammar Induction with Neural Lexicalization and Big Training Data</a></strong><br><a href=/people/w/wenjuan-han/>Wenjuan Han</a>
|
<a href=/people/y/yong-jiang/>Yong Jiang</a>
|
<a href=/people/k/kewei-tu/>Kewei Tu</a><br><a href=/volumes/D17-1/ class=text-muted>Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1176><div class="card-body p-3 small">We study the impact of <a href=https://en.wikipedia.org/wiki/Big_data>big models</a> (in terms of the degree of lexicalization) and <a href=https://en.wikipedia.org/wiki/Big_data>big data</a> (in terms of the training corpus size) on dependency grammar induction. We experimented with L-DMV, a lexicalized version of Dependency Model with Valence (Klein and Manning, 2004) and L-NDMV, our lexicalized extension of the Neural Dependency Model with Valence (Jiang et al., 2016). We find that L-DMV only benefits from very small degrees of <a href=https://en.wikipedia.org/wiki/Lexicalization>lexicalization</a> and moderate sizes of training corpora. L-NDMV can benefit from <a href=https://en.wikipedia.org/wiki/Big_data>big training data</a> and <a href=https://en.wikipedia.org/wiki/Lexicalization>lexicalization</a> of greater degrees, especially when enhanced with good model initialization, and it achieves a result that is competitive with the current <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1177.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1177 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1177 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1177/>Combining Generative and Discriminative Approaches to Unsupervised Dependency Parsing via Dual Decomposition</a></strong><br><a href=/people/y/yong-jiang/>Yong Jiang</a>
|
<a href=/people/w/wenjuan-han/>Wenjuan Han</a>
|
<a href=/people/k/kewei-tu/>Kewei Tu</a><br><a href=/volumes/D17-1/ class=text-muted>Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1177><div class="card-body p-3 small">Unsupervised dependency parsing aims to learn a <a href=https://en.wikipedia.org/wiki/Dependency_grammar>dependency parser</a> from unannotated sentences. Existing work focuses on either learning <a href=https://en.wikipedia.org/wiki/Generative_model>generative models</a> using the <a href=https://en.wikipedia.org/wiki/Expectation&#8211;maximization_algorithm>expectation-maximization algorithm</a> and its variants, or learning <a href=https://en.wikipedia.org/wiki/Discriminative_model>discriminative models</a> using the discriminative clustering algorithm. In this paper, we propose a new learning strategy that learns a <a href=https://en.wikipedia.org/wiki/Generative_model>generative model</a> and a <a href=https://en.wikipedia.org/wiki/Discriminative_model>discriminative model</a> jointly based on the dual decomposition method. Our method is simple and general, yet effective to capture the advantages of both <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> and improve their learning results. We tested our <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>method</a> on the UD treebank and achieved a state-of-the-art performance on thirty languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1179.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1179 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1179 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D17-1179" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D17-1179/>Semi-supervised Structured Prediction with Neural CRF Autoencoder<span class=acl-fixed-case>CRF</span> Autoencoder</a></strong><br><a href=/people/x/xiao-zhang/>Xiao Zhang</a>
|
<a href=/people/y/yong-jiang/>Yong Jiang</a>
|
<a href=/people/h/hao-peng/>Hao Peng</a>
|
<a href=/people/k/kewei-tu/>Kewei Tu</a>
|
<a href=/people/d/dan-goldwasser/>Dan Goldwasser</a><br><a href=/volumes/D17-1/ class=text-muted>Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1179><div class="card-body p-3 small">In this paper we propose an end-to-end neural CRF autoencoder (NCRF-AE) model for semi-supervised learning of sequential structured prediction problems. Our NCRF-AE consists of two parts : an encoder which is a CRF model enhanced by <a href=https://en.wikipedia.org/wiki/Deep_learning>deep neural networks</a>, and a decoder which is a <a href=https://en.wikipedia.org/wiki/Generative_model>generative model</a> trying to reconstruct the input. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> has a unified structure with different <a href=https://en.wikipedia.org/wiki/Loss_function>loss functions</a> for labeled and unlabeled data with shared parameters. We developed a variation of the EM algorithm for optimizing both the <a href=https://en.wikipedia.org/wiki/Encoder>encoder</a> and the <a href=https://en.wikipedia.org/wiki/Codec>decoder</a> simultaneously by decoupling their parameters. Our Experimental results over the Part-of-Speech (POS) tagging task on eight different languages, show that our model can outperform competitive systems in both supervised and semi-supervised scenarios.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Kewei+Tu" title="Search for 'Kewei Tu' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/y/yong-jiang/ class=align-middle>Yong Jiang</a>
<span class="badge badge-secondary align-middle ml-2">13</span></li><li class=list-group-item><a href=/people/w/wenjuan-han/ class=align-middle>Wenjuan Han</a>
<span class="badge badge-secondary align-middle ml-2">7</span></li><li class=list-group-item><a href=/people/x/xinyu-wang/ class=align-middle>Xinyu Wang</a>
<span class="badge badge-secondary align-middle ml-2">5</span></li><li class=list-group-item><a href=/people/n/nguyen-bach/ class=align-middle>Nguyen Bach</a>
<span class="badge badge-secondary align-middle ml-2">5</span></li><li class=list-group-item><a href=/people/t/tao-wang/ class=align-middle>Tao Wang</a>
<span class="badge badge-secondary align-middle ml-2">5</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/z/zhongqiang-huang/ class=align-middle>Zhongqiang Huang</a>
<span class="badge badge-secondary align-middle ml-2">5</span></li><li class=list-group-item><a href=/people/f/fei-huang/ class=align-middle>Fei Huang</a>
<span class="badge badge-secondary align-middle ml-2">5</span></li><li class=list-group-item><a href=/people/l/liwen-zhang/ class=align-middle>Liwen Zhang</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/y/yanpeng-zhao/ class=align-middle>Yanpeng Zhao</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/c/chengyue-jiang/ class=align-middle>Chengyue Jiang</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/j/jiong-cai/ class=align-middle>Jiong Cai</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/z/zhaohui-yan/ class=align-middle>Zhaohui Yan</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/zixia-jia/ class=align-middle>Zixia Jia</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/zechuan-hu/ class=align-middle>Zechuan Hu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/b/bo-pang/ class=align-middle>Bo Pang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/e/erik-nijkamp/ class=align-middle>Erik Nijkamp</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/linqi-zhou/ class=align-middle>Linqi Zhou</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yixian-liu/ class=align-middle>Yixian Liu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/songlin-yang/ class=align-middle>Songlin Yang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/zijian-jin/ class=align-middle>Zijian Jin</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/x/xiao-zhang/ class=align-middle>Xiao Zhang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/hao-peng/ class=align-middle>Hao Peng</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/dan-goldwasser/ class=align-middle>Dan Goldwasser</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/zhonglin-nian/ class=align-middle>Zhonglin Nian</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/kaihao-guo/ class=align-middle>Kaihao Guo</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/shanbo-chu/ class=align-middle>Shanbo Chu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yinggong-zhao/ class=align-middle>Yinggong Zhao</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/libin-shen/ class=align-middle>Libin Shen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/hwee-tou-ng/ class=align-middle>Hwee Tou Ng</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/ruyue-hong/ class=align-middle>Ruyue Hong</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jingxian-huang/ class=align-middle>Jingxian Huang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/acl/ class=align-middle>ACL</a><span class="badge badge-secondary align-middle ml-2">8</span></li><li class=list-group-item><a href=/venues/emnlp/ class=align-middle>EMNLP</a><span class="badge badge-secondary align-middle ml-2">7</span></li><li class=list-group-item><a href=/venues/findings/ class=align-middle>Findings</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/coling/ class=align-middle>COLING</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/eacl/ class=align-middle>EACL</a><span class="badge badge-secondary align-middle ml-2">1</span></li></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>