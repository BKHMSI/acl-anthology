<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Katharina Kann - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Katharina</span> <span class=font-weight-bold>Kann</span></h2><hr><div class=row><div class=col-lg-9><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.mtsummit-loresmt.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.mtsummit-loresmt.0/>Proceedings of the 4th Workshop on Technologies for MT of Low Resource Languages (LoResMT2021)</a></strong><br><a href=/people/j/john-ortega/>John Ortega</a>
|
<a href=/people/a/atul-kr-ojha/>Atul Kr. Ojha</a>
|
<a href=/people/k/katharina-kann/>Katharina Kann</a>
|
<a href=/people/c/chao-hong-liu/>Chao-Hong Liu</a><br><a href=/volumes/2021.mtsummit-loresmt/ class=text-muted>Proceedings of the 4th Workshop on Technologies for MT of Low Resource Languages (LoResMT2021)</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.242.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--242 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.242 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.eacl-main.242.Dataset.zip data-toggle=tooltip data-placement=top title=Dataset><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2021.eacl-main.242/>CLiMP : A Benchmark for Chinese Language Model Evaluation<span class=acl-fixed-case>CL</span>i<span class=acl-fixed-case>MP</span>: A Benchmark for <span class=acl-fixed-case>C</span>hinese Language Model Evaluation</a></strong><br><a href=/people/b/beilei-xiang/>Beilei Xiang</a>
|
<a href=/people/c/changbing-yang/>Changbing Yang</a>
|
<a href=/people/y/yu-li/>Yu Li</a>
|
<a href=/people/a/alex-warstadt/>Alex Warstadt</a>
|
<a href=/people/k/katharina-kann/>Katharina Kann</a><br><a href=/volumes/2021.eacl-main/ class=text-muted>Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--242><div class="card-body p-3 small">Linguistically informed analyses of language models (LMs) contribute to the understanding and improvement of such <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a>. Here, we introduce the corpus of Chinese linguistic minimal pairs (CLiMP) to investigate what knowledge Chinese LMs acquire. CLiMP consists of sets of 1000 minimal pairs (MPs) for 16 syntactic contrasts in <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a>, covering 9 major <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese linguistic phenomena</a>. The MPs are semi-automatically generated, and human agreement with the labels in CLiMP is 95.8 %. We evaluate 11 different LMs on CLiMP, covering n-grams, LSTMs, and Chinese BERT. We find that classifiernoun agreement and verb complement selection are the phenomena that models generally perform best at. However, <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> struggle the most with the ba construction, binding, and filler-gap dependencies. Overall, Chinese BERT achieves an 81.8 % average accuracy, while the performances of LSTMs and 5-grams are only moderately above chance level.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.63.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--63 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.63 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.63" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.63/>The World of an Octopus : How Reporting Bias Influences a <a href=https://en.wikipedia.org/wiki/Language_model>Language Model</a>’s Perception of Color<span class=acl-fixed-case>T</span>he <span class=acl-fixed-case>W</span>orld of an <span class=acl-fixed-case>O</span>ctopus: <span class=acl-fixed-case>H</span>ow <span class=acl-fixed-case>R</span>eporting <span class=acl-fixed-case>B</span>ias <span class=acl-fixed-case>I</span>nfluences a <span class=acl-fixed-case>L</span>anguage <span class=acl-fixed-case>M</span>odel’s <span class=acl-fixed-case>P</span>erception of <span class=acl-fixed-case>C</span>olor</a></strong><br><a href=/people/c/cory-paik/>Cory Paik</a>
|
<a href=/people/s/stephane-aroca-ouellette/>Stéphane Aroca-Ouellette</a>
|
<a href=/people/a/alessandro-roncone/>Alessandro Roncone</a>
|
<a href=/people/k/katharina-kann/>Katharina Kann</a><br><a href=/volumes/2021.emnlp-main/ class=text-muted>Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--63><div class="card-body p-3 small">Recent work has raised concerns about the inherent limitations of text-only pretraining. In this paper, we first demonstrate that <a href=https://en.wikipedia.org/wiki/Reporting_bias>reporting bias</a>, the tendency of people to not state the obvious, is one of the causes of this limitation, and then investigate to what extent multimodal training can mitigate this issue. To accomplish this, we 1) generate the Color Dataset (CoDa), a dataset of human-perceived color distributions for 521 common objects ; 2) use CoDa to analyze and compare the color distribution found in text, the distribution captured by language models, and a human&#8217;s perception of color ; and 3) investigate the performance differences between text-only and multimodal models on CoDa. Our results show that the distribution of colors that a language model recovers correlates more strongly with the inaccurate distribution found in text than with the ground-truth, supporting the claim that <a href=https://en.wikipedia.org/wiki/Reporting_bias>reporting bias</a> negatively impacts and inherently limits text-only training. We then demonstrate that multimodal models can leverage their visual training to mitigate these effects, providing a promising avenue for future research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.americasnlp-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.americasnlp-1.0/>Proceedings of the First Workshop on Natural Language Processing for Indigenous Languages of the Americas</a></strong><br><a href=/people/m/manuel-mager/>Manuel Mager</a>
|
<a href=/people/a/arturo-oncevay/>Arturo Oncevay</a>
|
<a href=/people/a/annette-rios-gonzales/>Annette Rios</a>
|
<a href=/people/i/ivan-vladimir-meza-ruiz/>Ivan Vladimir Meza Ruiz</a>
|
<a href=/people/a/alexis-palmer/>Alexis Palmer</a>
|
<a href=/people/g/graham-neubig/>Graham Neubig</a>
|
<a href=/people/k/katharina-kann/>Katharina Kann</a><br><a href=/volumes/2021.americasnlp-1/ class=text-muted>Proceedings of the First Workshop on Natural Language Processing for Indigenous Languages of the Americas</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.americasnlp-1.23.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--americasnlp-1--23 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.americasnlp-1.23 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.americasnlp-1.23/>Findings of the AmericasNLP 2021 Shared Task on Open Machine Translation for Indigenous Languages of the Americas<span class=acl-fixed-case>A</span>mericas<span class=acl-fixed-case>NLP</span> 2021 Shared Task on Open Machine Translation for Indigenous Languages of the <span class=acl-fixed-case>A</span>mericas</a></strong><br><a href=/people/m/manuel-mager/>Manuel Mager</a>
|
<a href=/people/a/arturo-oncevay/>Arturo Oncevay</a>
|
<a href=/people/a/abteen-ebrahimi/>Abteen Ebrahimi</a>
|
<a href=/people/j/john-ortega/>John Ortega</a>
|
<a href=/people/a/annette-rios-gonzales/>Annette Rios</a>
|
<a href=/people/a/angela-fan/>Angela Fan</a>
|
<a href=/people/x/ximena-gutierrez-vasques/>Ximena Gutierrez-Vasques</a>
|
<a href=/people/l/luis-chiruzzo/>Luis Chiruzzo</a>
|
<a href=/people/g/gustavo-gimenez-lugo/>Gustavo Giménez-Lugo</a>
|
<a href=/people/r/ricardo-ramos/>Ricardo Ramos</a>
|
<a href=/people/i/ivan-meza-ruiz/>Ivan Vladimir Meza Ruiz</a>
|
<a href=/people/r/rolando-coto-solano/>Rolando Coto-Solano</a>
|
<a href=/people/a/alexis-palmer/>Alexis Palmer</a>
|
<a href=/people/e/elisabeth-mager-hois/>Elisabeth Mager-Hois</a>
|
<a href=/people/v/vishrav-chaudhary/>Vishrav Chaudhary</a>
|
<a href=/people/g/graham-neubig/>Graham Neubig</a>
|
<a href=/people/n/ngoc-thang-vu/>Ngoc Thang Vu</a>
|
<a href=/people/k/katharina-kann/>Katharina Kann</a><br><a href=/volumes/2021.americasnlp-1/ class=text-muted>Proceedings of the First Workshop on Natural Language Processing for Indigenous Languages of the Americas</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--americasnlp-1--23><div class="card-body p-3 small">This paper presents the results of the 2021 Shared Task on Open Machine Translation for <a href=https://en.wikipedia.org/wiki/Indigenous_languages_of_the_Americas>Indigenous Languages of the Americas</a>. The shared task featured two independent tracks, and participants submitted <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation systems</a> for up to 10 <a href=https://en.wikipedia.org/wiki/Indigenous_language>indigenous languages</a>. Overall, 8 teams participated with a total of 214 submissions. We provided <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training sets</a> consisting of data collected from various sources, as well as manually translated sentences for the <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>development and test sets</a>. An official <a href=https://en.wikipedia.org/wiki/Baseline_(medicine)>baseline</a> trained on this <a href=https://en.wikipedia.org/wiki/Data>data</a> was also provided. Team submissions featured a variety of <a href=https://en.wikipedia.org/wiki/Computer_architecture>architectures</a>, including both statistical and neural models, and for the majority of languages, many teams were able to considerably improve over the <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline</a>. The best performing <a href=https://en.wikipedia.org/wiki/System>systems</a> achieved 12.97 ChrF higher than <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline</a>, when averaged across languages.</div></div><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.iwpt-1.11.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--iwpt-1--11 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.iwpt-1.11 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://slideslive.com/38929678 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.iwpt-1.11/>Self-Training for Unsupervised Parsing with PRPN<span class=acl-fixed-case>PRPN</span></a></strong><br><a href=/people/a/anhad-mohananey/>Anhad Mohananey</a>
|
<a href=/people/k/katharina-kann/>Katharina Kann</a>
|
<a href=/people/s/samuel-bowman/>Samuel R. Bowman</a><br><a href=/volumes/2020.iwpt-1/ class=text-muted>Proceedings of the 16th International Conference on Parsing Technologies and the IWPT 2020 Shared Task on Parsing into Enhanced Universal Dependencies</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--iwpt-1--11><div class="card-body p-3 small">Neural unsupervised parsing (UP) models learn to parse without access to syntactic annotations, while being optimized for another task like <a href=https://en.wikipedia.org/wiki/Language_model>language modeling</a>. In this work, we propose self-training for neural UP models : we leverage aggregated annotations predicted by copies of our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> as supervision for future copies. To be able to use our model&#8217;s predictions during training, we extend a recent neural UP architecture, the PRPN (Shen et al., 2018a), such that it can be trained in a semi-supervised fashion. We then add examples with <a href=https://en.wikipedia.org/wiki/Parsing>parses</a> predicted by our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> to our unlabeled UP training data. Our self-trained model outperforms the PRPN by 8.1 % <a href=https://en.wikipedia.org/wiki/F-number>F1</a> and the previous state of the art by 1.6 % <a href=https://en.wikipedia.org/wiki/F-number>F1</a>. In addition, we show that our <a href=https://en.wikipedia.org/wiki/Software_architecture>architecture</a> can also be helpful for semi-supervised parsing in ultra-low-resource settings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.94.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--94 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.94 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938805 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.94/>Acrostic Poem Generation</a></strong><br><a href=/people/r/rajat-agarwal/>Rajat Agarwal</a>
|
<a href=/people/k/katharina-kann/>Katharina Kann</a><br><a href=/volumes/2020.emnlp-main/ class=text-muted>Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--94><div class="card-body p-3 small">We propose a new task in the area of <a href=https://en.wikipedia.org/wiki/Computational_creativity>computational creativity</a> : acrostic poem generation in <a href=https://en.wikipedia.org/wiki/English_language>English</a>. Acrostic poems are <a href=https://en.wikipedia.org/wiki/Poetry>poems</a> that contain a <a href=https://en.wikipedia.org/wiki/Hidden_message>hidden message</a> ; typically, the first letter of each line spells out a word or short phrase. We define the task as a generation task with multiple constraints : given an input word, 1) the initial letters of each line should spell out the provided word, 2) the <a href=https://en.wikipedia.org/wiki/Poetry>poem&#8217;s semantics</a> should also relate to it, and 3) the <a href=https://en.wikipedia.org/wiki/Poetry>poem</a> should conform to a <a href=https://en.wikipedia.org/wiki/Rhyme_scheme>rhyming scheme</a>. We further provide a baseline model for the task, which consists of a conditional neural language model in combination with a neural rhyming model. Since no dedicated datasets for acrostic poem generation exist, we create <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training data</a> for our task by first training a separate topic prediction model on a small set of topic-annotated poems and then predicting topics for additional <a href=https://en.wikipedia.org/wiki/Poetry>poems</a>. Our experiments show that the acrostic poems generated by our baseline are received well by humans and do not lose much quality due to the additional constraints. Last, we confirm that <a href=https://en.wikipedia.org/wiki/Poetry>poems</a> generated by our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> are indeed closely related to the provided prompts, and that pretraining on <a href=https://en.wikipedia.org/wiki/Wikipedia>Wikipedia</a> can boost performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.acl-main.598.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--acl-main--598 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.acl-main.598 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://slideslive.com/38929159 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.acl-main.598" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.acl-main.598/>Unsupervised Morphological Paradigm Completion</a></strong><br><a href=/people/h/huiming-jin/>Huiming Jin</a>
|
<a href=/people/l/liwei-cai/>Liwei Cai</a>
|
<a href=/people/y/yihui-peng/>Yihui Peng</a>
|
<a href=/people/c/chen-xia/>Chen Xia</a>
|
<a href=/people/a/arya-d-mccarthy/>Arya McCarthy</a>
|
<a href=/people/k/katharina-kann/>Katharina Kann</a><br><a href=/volumes/2020.acl-main/ class=text-muted>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--acl-main--598><div class="card-body p-3 small">We propose the task of unsupervised morphological paradigm completion. Given only raw text and a <a href=https://en.wikipedia.org/wiki/Lemma_(morphology)>lemma list</a>, the task consists of generating the morphological paradigms, i.e., all inflected forms, of the lemmas. From a natural language processing (NLP) perspective, this is a challenging <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised task</a>, and high-performing systems have the potential to improve tools for low-resource languages or to assist linguistic annotators. From a cognitive science perspective, this can shed light on how children acquire <a href=https://en.wikipedia.org/wiki/Morphology_(biology)>morphological knowledge</a>. We further introduce a system for the task, which generates morphological paradigms via the following steps : (i) EDIT TREE retrieval, (ii) additional lemma retrieval, (iii) paradigm size discovery, and (iv) inflection generation. We perform an evaluation on 14 typologically diverse languages. Our <a href=https://en.wikipedia.org/wiki/System>system</a> outperforms trivial baselines with ease and, for some languages, even obtains a higher <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> than minimally supervised systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.sigmorphon-1.9.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--sigmorphon-1--9 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.sigmorphon-1.9 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.sigmorphon-1.9/>The IMSCUBoulder System for the SIGMORPHON 2020 Shared Task on Unsupervised Morphological Paradigm Completion<span class=acl-fixed-case>IMS</span>–<span class=acl-fixed-case>CUB</span>oulder System for the <span class=acl-fixed-case>SIGMORPHON</span> 2020 Shared Task on Unsupervised Morphological Paradigm Completion</a></strong><br><a href=/people/m/manuel-mager/>Manuel Mager</a>
|
<a href=/people/k/katharina-kann/>Katharina Kann</a><br><a href=/volumes/2020.sigmorphon-1/ class=text-muted>Proceedings of the 17th SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--sigmorphon-1--9><div class="card-body p-3 small">In this paper, we present the systems of the University of Stuttgart IMS and the University of Colorado Boulder (IMSCUBoulder) for SIGMORPHON 2020 Task 2 on unsupervised morphological paradigm completion (Kann et al., 2020). The task consists of generating the <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphological paradigms</a> of a set of lemmas, given only the lemmas themselves and unlabeled text. Our proposed <a href=https://en.wikipedia.org/wiki/System>system</a> is a modified version of the <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline</a> introduced together with the <a href=https://en.wikipedia.org/wiki/Task_(computing)>task</a>. In particular, we experiment with substituting the inflection generation component with an LSTM sequence-to-sequence model and an LSTM pointer-generator network. Our pointer-generator system obtains the best score of all seven submitted systems on average over all languages, and outperforms the official baseline, which was best overall, on <a href=https://en.wikipedia.org/wiki/Bulgarian_language>Bulgarian</a> and <a href=https://en.wikipedia.org/wiki/Kannada>Kannada</a>.</div></div><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1329.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1329 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1329 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1329.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D19-1329/>Towards Realistic Practices In Low-Resource Natural Language Processing : The Development Set</a></strong><br><a href=/people/k/katharina-kann/>Katharina Kann</a>
|
<a href=/people/k/kyunghyun-cho/>Kyunghyun Cho</a>
|
<a href=/people/s/samuel-bowman/>Samuel R. Bowman</a><br><a href=/volumes/D19-1/ class=text-muted>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1329><div class="card-body p-3 small">Development sets are impractical to obtain for real low-resource languages, since using all available data for training is often more effective. However, development sets are widely used in research papers that purport to deal with low-resource natural language processing (NLP). Here, we aim to answer the following questions : Does using a development set for early stopping in the low-resource setting influence results as compared to a more realistic alternative, where the number of training epochs is tuned on development languages? And does <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> lead to overestimation or underestimation of performance? We repeat multiple experiments from recent work on neural models for low-resource NLP and compare results for <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> obtained by training with and without development sets. On average over languages, <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>absolute accuracy</a> differs by up to 1.4 %. However, for some languages and tasks, differences are as big as 18.0 % <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>. Our results highlight the importance of realistic experimental setups in the publication of low-resource NLP research results.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-6128.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-6128 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-6128 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-6128/>Transductive Auxiliary Task Self-Training for Neural Multi-Task Models</a></strong><br><a href=/people/j/johannes-bjerva/>Johannes Bjerva</a>
|
<a href=/people/k/katharina-kann/>Katharina Kann</a>
|
<a href=/people/i/isabelle-augenstein/>Isabelle Augenstein</a><br><a href=/volumes/D19-61/ class=text-muted>Proceedings of the 2nd Workshop on Deep Learning Approaches for Low-Resource NLP (DeepLo 2019)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-6128><div class="card-body p-3 small">Multi-task learning and self-training are two common ways to improve a <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning model</a>&#8217;s performance in settings with limited training data. Drawing heavily on ideas from those two approaches, we suggest transductive auxiliary task self-training : training a multi-task model on (i) a combination of main and auxiliary task training data, and (ii) test instances with auxiliary task labels which a single-task version of the <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> has previously generated. We perform extensive experiments on 86 combinations of languages and tasks. Our results are that, on average, transductive auxiliary task self-training improves absolute accuracy by up to 9.56 % over the pure multi-task model for <a href=https://en.wikipedia.org/wiki/Tag_(metadata)>dependency relation tagging</a> and by up to 13.03 % for <a href=https://en.wikipedia.org/wiki/Tag_(metadata)>semantic tagging</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4300.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4300/>Proceedings of the 4th Workshop on Representation Learning for NLP (RepL4NLP-2019)</a></strong><br><a href=/people/i/isabelle-augenstein/>Isabelle Augenstein</a>
|
<a href=/people/s/spandana-gella/>Spandana Gella</a>
|
<a href=/people/s/sebastian-ruder/>Sebastian Ruder</a>
|
<a href=/people/k/katharina-kann/>Katharina Kann</a>
|
<a href=/people/b/burcu-can/>Burcu Can</a>
|
<a href=/people/j/johannes-welbl/>Johannes Welbl</a>
|
<a href=/people/a/alexis-conneau/>Alexis Conneau</a>
|
<a href=/people/x/xiang-ren/>Xiang Ren</a>
|
<a href=/people/m/marek-rei/>Marek Rei</a><br><a href=/volumes/W19-43/ class=text-muted>Proceedings of the 4th Workshop on Representation Learning for NLP (RepL4NLP-2019)</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1201.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1201 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1201 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/354264673 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1201/>Subword-Level Language Identification for Intra-Word Code-Switching</a></strong><br><a href=/people/m/manuel-mager/>Manuel Mager</a>
|
<a href=/people/o/ozlem-cetinoglu/>Özlem Çetinoğlu</a>
|
<a href=/people/k/katharina-kann/>Katharina Kann</a><br><a href=/volumes/N19-1/ class=text-muted>Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1201><div class="card-body p-3 small">Language identification for code-switching (CS), the phenomenon of alternating between two or more languages in conversations, has traditionally been approached under the assumption of a single language per token. However, if at least one language is morphologically rich, a large number of words can be composed of morphemes from more than one language (intra-word CS). In this paper, we extend the language identification task to the subword-level, such that it includes splitting mixed words while tagging each part with a language ID. We further propose a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> for this task, which is based on a segmental recurrent neural network. In experiments on a new SpanishWixarika dataset and on an adapted GermanTurkish dataset, our proposed model performs slightly better than or roughly on par with our best baseline, respectively. Considering only mixed words, however, it strongly outperforms all baselines.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1574.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1574 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1574 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/385429181 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1574" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1574/>Probing for Semantic Classes : Diagnosing the Meaning Content of Word Embeddings</a></strong><br><a href=/people/y/yadollah-yaghoobzadeh/>Yadollah Yaghoobzadeh</a>
|
<a href=/people/k/katharina-kann/>Katharina Kann</a>
|
<a href=/people/t/timothy-j-hazen/>T. J. Hazen</a>
|
<a href=/people/e/eneko-agirre/>Eneko Agirre</a>
|
<a href=/people/h/hinrich-schutze/>Hinrich Schütze</a><br><a href=/volumes/P19-1/ class=text-muted>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1574><div class="card-body p-3 small">Word embeddings typically represent different meanings of a word in a single conflated vector. Empirical analysis of embeddings of ambiguous words is currently limited by the small size of manually annotated resources and by the fact that word senses are treated as unrelated individual concepts. We present a large dataset based on manual Wikipedia annotations and <a href=https://en.wikipedia.org/wiki/Word_sense>word senses</a>, where <a href=https://en.wikipedia.org/wiki/Word_sense>word senses</a> from different words are related by <a href=https://en.wikipedia.org/wiki/Semantic_class>semantic classes</a>. This is the basis for novel diagnostic tests for an embedding&#8217;s content : we probe word embeddings for <a href=https://en.wikipedia.org/wiki/Semantic_class>semantic classes</a> and analyze the embedding space by classifying embeddings into <a href=https://en.wikipedia.org/wiki/Semantic_class>semantic classes</a>. Our main findings are : (i) Information about a sense is generally represented well in a single-vector embedding if the sense is frequent. (ii) A <a href=https://en.wikipedia.org/wiki/Classifier_(linguistics)>classifier</a> can accurately predict whether a word is single-sense or multi-sense, based only on its embedding. (iii) Although rare senses are not well represented in single-vector embeddings, this does not have negative impact on an NLP application whose performance depends on frequent senses.</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3013.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3013 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3013 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-3013" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-3013/>Evaluating Word Embeddings in Multi-label Classification Using Fine-Grained Name Typing</a></strong><br><a href=/people/y/yadollah-yaghoobzadeh/>Yadollah Yaghoobzadeh</a>
|
<a href=/people/k/katharina-kann/>Katharina Kann</a>
|
<a href=/people/h/hinrich-schutze/>Hinrich Schütze</a><br><a href=/volumes/W18-30/ class=text-muted>Proceedings of The Third Workshop on Representation Learning for NLP</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3013><div class="card-body p-3 small">Embedding models typically associate each word with a single real-valued vector, representing its different properties. Evaluation methods, therefore, need to analyze the accuracy and completeness of these properties in <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a>. This requires fine-grained analysis of embedding subspaces. Multi-label classification is an appropriate way to do so. We propose a new evaluation method for <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> based on <a href=https://en.wikipedia.org/wiki/Multi-label_classification>multi-label classification</a> given a <a href=https://en.wikipedia.org/wiki/Word_embedding>word embedding</a>. The task we use is fine-grained name typing : given a large corpus, find all types that a name can refer to based on the name embedding. Given the scale of entities in <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge bases</a>, we can build <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> for this task that are complementary to the current embedding evaluation datasets in : they are very large, contain fine-grained classes, and allow the direct evaluation of embeddings without confounding factors like sentence context.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4808.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4808 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4808 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4808/>Lost in Translation : Analysis of Information Loss During <a href=https://en.wikipedia.org/wiki/Machine_translation>Machine Translation</a> Between Polysynthetic and Fusional Languages</a></strong><br><a href=/people/m/manuel-mager/>Manuel Mager</a>
|
<a href=/people/e/elisabeth-maier/>Elisabeth Mager</a>
|
<a href=/people/a/alfonso-medina-urrea/>Alfonso Medina-Urrea</a>
|
<a href=/people/i/ivan-meza-ruiz/>Ivan Vladimir Meza Ruiz</a>
|
<a href=/people/k/katharina-kann/>Katharina Kann</a><br><a href=/volumes/W18-48/ class=text-muted>Proceedings of the Workshop on Computational Modeling of Polysynthetic Languages</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4808><div class="card-body p-3 small">Machine translation from polysynthetic to fusional languages is a challenging task, which gets further complicated by the limited amount of parallel text available. Thus, <a href=https://en.wikipedia.org/wiki/Translation>translation</a> performance is far from the state of the art for high-resource and more intensively studied language pairs. To shed light on the phenomena which hamper automatic translation to and from <a href=https://en.wikipedia.org/wiki/Polysynthetic_language>polysynthetic languages</a>, we study translations from three low-resource, polysynthetic languages (Nahuatl, Wixarika and Yorem Nokki) into <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a> and vice versa. Doing so, we find that in a morpheme-to-morpheme alignment an important amount of information contained in polysynthetic morphemes has no Spanish counterpart, and its translation is often omitted. We further conduct a qualitative analysis and, thus, identify morpheme types that are commonly hard to align or ignored in the translation process.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1005.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1005 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1005 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://vimeo.com/276387788 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N18-1005/>Fortification of Neural Morphological Segmentation Models for Polysynthetic Minimal-Resource Languages</a></strong><br><a href=/people/k/katharina-kann/>Katharina Kann</a>
|
<a href=/people/j/jesus-manuel-mager-hois/>Jesus Manuel Mager Hois</a>
|
<a href=/people/i/ivan-meza-ruiz/>Ivan Vladimir Meza-Ruiz</a>
|
<a href=/people/h/hinrich-schutze/>Hinrich Schütze</a><br><a href=/volumes/N18-1/ class=text-muted>Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1005><div class="card-body p-3 small">Morphological segmentation for polysynthetic languages is challenging, because a word may consist of many individual morphemes and training data can be extremely scarce. Since neural sequence-to-sequence (seq2seq) models define the state of the art for morphological segmentation in high-resource settings and for (mostly) European languages, we first show that they also obtain competitive performance for Mexican polysynthetic languages in minimal-resource settings. We then propose two novel multi-task training approachesone with, one without need for external unlabeled resources, and two corresponding data augmentation methods, improving over the neural baseline for all languages. Finally, we explore cross-lingual transfer as a third way to fortify our neural model and show that we can train one single multi-lingual model for related languages while maintaining comparable or even improved performance, thus reducing the amount of parameters by close to 75 %. We provide our morphological segmentation datasets for <a href=https://en.wikipedia.org/wiki/Mexicanero_language>Mexicanero</a>, <a href=https://en.wikipedia.org/wiki/Nahuatl>Nahuatl</a>, Wixarika and Yorem Nokki for future research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K18-1031.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K18-1031 data-toggle=collapse aria-expanded=false aria-controls=abstract-K18-1031 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/K18-1031/>Sentence-Level Fluency Evaluation : References Help, But Can Be Spared !</a></strong><br><a href=/people/k/katharina-kann/>Katharina Kann</a>
|
<a href=/people/s/sascha-rothe/>Sascha Rothe</a>
|
<a href=/people/k/katja-filippova/>Katja Filippova</a><br><a href=/volumes/K18-1/ class=text-muted>Proceedings of the 22nd Conference on Computational Natural Language Learning</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K18-1031><div class="card-body p-3 small">Motivated by recent findings on the probabilistic modeling of acceptability judgments, we propose syntactic log-odds ratio (SLOR), a normalized language model score, as a metric for referenceless fluency evaluation of natural language generation output at the sentence level. We further introduce WPSLOR, a novel WordPiece-based version, which harnesses a more compact <a href=https://en.wikipedia.org/wiki/Language_model>language model</a>. Even though word-overlap metrics like ROUGE are computed with the help of hand-written references, our referenceless methods obtain a significantly higher correlation with human fluency scores on a benchmark dataset of compressed sentences. Finally, we present ROUGE-LM, a reference-based metric which is a natural extension of WPSLOR to the case of available references. We show that ROUGE-LM yields a significantly higher correlation with human judgments than all baseline metrics, including WPSLOR on its own.</div></div><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1182.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1182 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1182 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-1182/>One-Shot Neural Cross-Lingual Transfer for Paradigm Completion</a></strong><br><a href=/people/k/katharina-kann/>Katharina Kann</a>
|
<a href=/people/r/ryan-cotterell/>Ryan Cotterell</a>
|
<a href=/people/h/hinrich-schutze/>Hinrich Schütze</a><br><a href=/volumes/P17-1/ class=text-muted>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1182><div class="card-body p-3 small">We present a novel cross-lingual transfer method for paradigm completion, the task of mapping a lemma to its inflected forms, using a neural encoder-decoder model, the state of the art for the monolingual task. We use labeled data from a high-resource language to increase performance on a low-resource language. In experiments on 21 language pairs from four different language families, we obtain up to 58 % higher <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> than without transfer and show that even zero-shot and one-shot learning are possible. We further find that the degree of <a href=https://en.wikipedia.org/wiki/Language_family>language relatedness</a> strongly influences the ability to transfer <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphological knowledge</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4110.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4110 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4110 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4110/>Exploring Cross-Lingual Transfer of Morphological Knowledge In Sequence-to-Sequence Models</a></strong><br><a href=/people/h/huiming-jin/>Huiming Jin</a>
|
<a href=/people/k/katharina-kann/>Katharina Kann</a><br><a href=/volumes/W17-41/ class=text-muted>Proceedings of the First Workshop on Subword and Character Level Models in NLP</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4110><div class="card-body p-3 small">Multi-task training is an effective method to mitigate the data sparsity problem. It has recently been applied for cross-lingual transfer learning for paradigm completionthe task of producing inflected forms of lemmatawith sequence-to-sequence networks. However, it is still vague how the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> transfers knowledge across languages, as well as if and which information is shared. To investigate this, we propose a set of data-dependent experiments using an existing encoder-decoder recurrent neural network for the task. Our results show that indeed the performance gains surpass a pure regularization effect and that knowledge about <a href=https://en.wikipedia.org/wiki/Language>language</a> and <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphology</a> can be transferred.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4111.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4111 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4111 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4111/>Unlabeled Data for Morphological Generation With Character-Based Sequence-to-Sequence Models</a></strong><br><a href=/people/k/katharina-kann/>Katharina Kann</a>
|
<a href=/people/h/hinrich-schutze/>Hinrich Schütze</a><br><a href=/volumes/W17-41/ class=text-muted>Proceedings of the First Workshop on Subword and Character Level Models in NLP</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4111><div class="card-body p-3 small">We present a semi-supervised way of training a character-based encoder-decoder recurrent neural network for morphological reinflectionthe task of generating one inflected wordform from another. This is achieved by using unlabeled tokens or random strings as training data for an autoencoding task, adapting a network for morphological reinflection, and performing multi-task training. We thus use limited labeled data more effectively, obtaining up to 9.92 % improvement over state-of-the-art <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a> for 8 different languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1049.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1049 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1049 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1049/>Neural Multi-Source Morphological Reinflection</a></strong><br><a href=/people/k/katharina-kann/>Katharina Kann</a>
|
<a href=/people/r/ryan-cotterell/>Ryan Cotterell</a>
|
<a href=/people/h/hinrich-schutze/>Hinrich Schütze</a><br><a href=/volumes/E17-1/ class=text-muted>Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1049><div class="card-body p-3 small">We explore the task of multi-source morphological reinflection, which generalizes the standard, single-source version. The input consists of (i) a target tag and (ii) multiple pairs of source form and source tag for a lemma. The motivation is that it is beneficial to have access to more than one <a href=https://en.wikipedia.org/wiki/Form_(document)>source form</a> since different <a href=https://en.wikipedia.org/wiki/Form_(document)>source forms</a> can provide complementary information, e.g., different <a href=https://en.wikipedia.org/wiki/Word_stem>stems</a>. We further present a novel extension to the encoder-decoder recurrent neural architecture, consisting of multiple encoders, to better solve the task. We show that our new architecture outperforms single-source reinflection models and publish our dataset for multi-source morphological reinflection to facilitate future research.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Katharina+Kann" title="Search for 'Katharina Kann' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/h/hinrich-schutze/ class=align-middle>Hinrich Schütze</a>
<span class="badge badge-secondary align-middle ml-2">6</span></li><li class=list-group-item><a href=/people/m/manuel-mager/ class=align-middle>Manuel Mager</a>
<span class="badge badge-secondary align-middle ml-2">5</span></li><li class=list-group-item><a href=/people/i/ivan-meza-ruiz/ class=align-middle>Ivan Meza-Ruiz</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/j/john-ortega/ class=align-middle>John Ortega</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/s/samuel-bowman/ class=align-middle>Samuel Bowman</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/h/huiming-jin/ class=align-middle>Huiming Jin</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/r/ryan-cotterell/ class=align-middle>Ryan Cotterell</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/i/isabelle-augenstein/ class=align-middle>Isabelle Augenstein</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/a/arturo-oncevay/ class=align-middle>Arturo Oncevay</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/a/annette-rios-gonzales/ class=align-middle>Annette Rios Gonzales</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/a/alexis-palmer/ class=align-middle>Alexis Palmer</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/g/graham-neubig/ class=align-middle>Graham Neubig</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/y/yadollah-yaghoobzadeh/ class=align-middle>Yadollah Yaghoobzadeh</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/a/atul-kr-ojha/ class=align-middle>Atul Kr. Ojha</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/chao-hong-liu/ class=align-middle>Chao-Hong Liu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/anhad-mohananey/ class=align-middle>Anhad Mohananey</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/rajat-agarwal/ class=align-middle>Rajat Agarwal</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/liwei-cai/ class=align-middle>Liwei Cai</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yihui-peng/ class=align-middle>Yihui Peng</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/chen-xia/ class=align-middle>Chen Xia</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/arya-d-mccarthy/ class=align-middle>Arya D. McCarthy</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/b/beilei-xiang/ class=align-middle>Beilei Xiang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/changbing-yang/ class=align-middle>Changbing Yang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yu-li/ class=align-middle>Yu Li</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/alex-warstadt/ class=align-middle>Alex Warstadt</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/cory-paik/ class=align-middle>Cory Paik</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/stephane-aroca-ouellette/ class=align-middle>Stéphane Aroca-Ouellette</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/alessandro-roncone/ class=align-middle>Alessandro Roncone</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/kyunghyun-cho/ class=align-middle>Kyunghyun Cho</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/johannes-bjerva/ class=align-middle>Johannes Bjerva</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/i/ivan-vladimir-meza-ruiz/ class=align-middle>Ivan Vladimir Meza Ruiz</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/abteen-ebrahimi/ class=align-middle>Abteen Ebrahimi</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/angela-fan/ class=align-middle>Angela Fan</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/x/ximena-gutierrez-vasques/ class=align-middle>Ximena Gutierrez-Vasques</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/luis-chiruzzo/ class=align-middle>Luis Chiruzzo</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/g/gustavo-gimenez-lugo/ class=align-middle>Gustavo Giménez-Lugo</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/ricardo-ramos/ class=align-middle>Ricardo Ramos</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/rolando-coto-solano/ class=align-middle>Rolando Coto-Solano</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/e/elisabeth-mager-hois/ class=align-middle>Elisabeth Mager-Hois</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/v/vishrav-chaudhary/ class=align-middle>Vishrav Chaudhary</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/n/ngoc-thang-vu/ class=align-middle>Ngoc Thang Vu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/e/elisabeth-maier/ class=align-middle>Elisabeth Maier</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/alfonso-medina-urrea/ class=align-middle>Alfonso Medina-Urrea</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/spandana-gella/ class=align-middle>Spandana Gella</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/sebastian-ruder/ class=align-middle>Sebastian Ruder</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/b/burcu-can/ class=align-middle>Burcu Can</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/johannes-welbl/ class=align-middle>Johannes Welbl</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/alexis-conneau/ class=align-middle>Alexis Conneau</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/x/xiang-ren/ class=align-middle>Xiang Ren</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/marek-rei/ class=align-middle>Marek Rei</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/o/ozlem-cetinoglu/ class=align-middle>Özlem Çetinoğlu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jesus-manuel-mager-hois/ class=align-middle>Jesus Manuel Mager Hois</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/sascha-rothe/ class=align-middle>Sascha Rothe</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/katja-filippova/ class=align-middle>Katja Filippova</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/timothy-j-hazen/ class=align-middle>Timothy J. Hazen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/e/eneko-agirre/ class=align-middle>Eneko Agirre</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/ws/ class=align-middle>WS</a><span class="badge badge-secondary align-middle ml-2">5</span></li><li class=list-group-item><a href=/venues/emnlp/ class=align-middle>EMNLP</a><span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/venues/acl/ class=align-middle>ACL</a><span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/venues/eacl/ class=align-middle>EACL</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/americasnlp/ class=align-middle>AmericasNLP</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-venues aria-expanded=false aria-controls=more-venues>show all...</li><div class="collapse border-top" id=more-venues><li class=list-group-item><a href=/venues/naacl/ class=align-middle>NAACL</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/mtsummit/ class=align-middle>MTSummit</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/iwpt/ class=align-middle>IWPT</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/sigmorphon/ class=align-middle>SIGMORPHON</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/conll/ class=align-middle>CoNLL</a><span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>