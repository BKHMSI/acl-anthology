<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Kawin Ethayarajh - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Kawin</span> <span class=font-weight-bold>Ethayarajh</span></h2><hr><div class=row><div class=col-lg-9><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.393.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--393 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.393 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938914 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.393/>Utility is in the Eye of the User : A Critique of NLP Leaderboards<span class=acl-fixed-case>NLP</span> Leaderboards</a></strong><br><a href=/people/k/kawin-ethayarajh/>Kawin Ethayarajh</a>
|
<a href=/people/d/dan-jurafsky/>Dan Jurafsky</a><br><a href=/volumes/2020.emnlp-main/ class=text-muted>Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--393><div class="card-body p-3 small">Benchmarks such as GLUE have helped drive advances in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> by incentivizing the creation of more accurate models. While this leaderboard paradigm has been remarkably successful, a historical focus on performance-based evaluation has been at the expense of other qualities that the NLP community values in models, such as compactness, fairness, and <a href=https://en.wikipedia.org/wiki/Efficient_energy_use>energy efficiency</a>. In this opinion paper, we study the divergence between what is incentivized by leaderboards and what is useful in practice through the lens of <a href=https://en.wikipedia.org/wiki/Microeconomics>microeconomic theory</a>. We frame both the leaderboard and NLP practitioners as consumers and the benefit they get from a <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> as its utility to them. With this framing, we formalize how leaderboards in their current form can be poor proxies for the NLP community at large. For example, a highly inefficient model would provide less utility to practitioners but not to a <a href=https://en.wikipedia.org/wiki/Glossary_of_economics>leaderboard</a>, since it is a cost that only the former must bear. To allow practitioners to better estimate a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>&#8217;s utility to them, we advocate for more transparency on leaderboards, such as the reporting of statistics that are of practical concern (e.g., <a href=https://en.wikipedia.org/wiki/Mathematical_model>model size</a>, <a href=https://en.wikipedia.org/wiki/Efficient_energy_use>energy efficiency</a>, and inference latency).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.acl-main.262.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--acl-main--262 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.acl-main.262 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://slideslive.com/38928838 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.acl-main.262/>Is Your Classifier Actually Biased? Measuring Fairness under Uncertainty with Bernstein Bounds</a></strong><br><a href=/people/k/kawin-ethayarajh/>Kawin Ethayarajh</a><br><a href=/volumes/2020.acl-main/ class=text-muted>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--acl-main--262><div class="card-body p-3 small">Most NLP datasets are not annotated with protected attributes such as gender, making it difficult to measure classification bias using standard measures of fairness (e.g., equal opportunity). However, manually annotating a large dataset with a protected attribute is slow and expensive. Instead of annotating all the examples, can we annotate a subset of them and use that sample to estimate the <a href=https://en.wikipedia.org/wiki/Bias_(statistics)>bias</a>? While it is possible to do so, the smaller this annotated sample is, the less certain we are that the <a href=https://en.wikipedia.org/wiki/Estimation_theory>estimate</a> is close to the true bias. In this work, we propose using Bernstein bounds to represent this uncertainty about the <a href=https://en.wikipedia.org/wiki/Bias_of_an_estimator>bias estimate</a> as a <a href=https://en.wikipedia.org/wiki/Confidence_interval>confidence interval</a>. We provide empirical evidence that a 95 % confidence interval derived this way consistently bounds the true <a href=https://en.wikipedia.org/wiki/Bias_(statistics)>bias</a>. In quantifying this uncertainty, our method, which we call Bernstein-bounded unfairness, helps prevent classifiers from being deemed biased or unbiased when there is insufficient evidence to make either claim. Our findings suggest that the <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> currently used to measure specific biases are too small to conclusively identify <a href=https://en.wikipedia.org/wiki/Bias>bias</a> except in the most egregious cases. For example, consider a co-reference resolution system that is 5 % more accurate on gender-stereotypical sentences to claim it is biased with 95 % confidence, we need a bias-specific dataset that is 3.8 times larger than WinoBias, the largest available.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.eval4nlp-1.5.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--eval4nlp-1--5 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.eval4nlp-1.5 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939709 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.eval4nlp-1.5/>BLEU Neighbors : A Reference-less Approach to Automatic Evaluation<span class=acl-fixed-case>BLEU</span> Neighbors: A Reference-less Approach to Automatic Evaluation</a></strong><br><a href=/people/k/kawin-ethayarajh/>Kawin Ethayarajh</a>
|
<a href=/people/d/dorsa-sadigh/>Dorsa Sadigh</a><br><a href=/volumes/2020.eval4nlp-1/ class=text-muted>Proceedings of the First Workshop on Evaluation and Comparison of NLP Systems</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--eval4nlp-1--5><div class="card-body p-3 small">Evaluation is a bottleneck in the development of natural language generation (NLG) models. Automatic metrics such as <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a> rely on references, but for tasks such as open-ended generation, there are no references to draw upon. Although <a href=https://en.wikipedia.org/wiki/Language>language diversity</a> can be estimated using statistical measures such as <a href=https://en.wikipedia.org/wiki/Perplexity>perplexity</a>, measuring language quality requires <a href=https://en.wikipedia.org/wiki/Evaluation>human evaluation</a>. However, because human evaluation at scale is slow and expensive, <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> is used sparingly ; <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> can not be used to rapidly iterate on NLG models, in the way <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a> is used for <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>. To this end, we propose BLEU Neighbors, a nearest neighbors model for estimating language quality by using the BLEU score as a <a href=https://en.wikipedia.org/wiki/Positive-definite_kernel>kernel function</a>. On existing datasets for chitchat dialogue and open-ended sentence generation, we find that on average the quality estimation from a BLEU Neighbors model has a lower <a href=https://en.wikipedia.org/wiki/Mean_squared_error>mean squared error</a> and higher <a href=https://en.wikipedia.org/wiki/Spearman_correlation>Spearman correlation</a> with the ground truth than individual human annotators. Despite its simplicity, BLEU Neighbors even outperforms state-of-the-art <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> on automatically grading essays, including <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> that have access to a gold-standard reference essay.</div></div><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1006.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1006 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1006 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1006/>How Contextual are Contextualized Word Representations? Comparing the Geometry of BERT, ELMo, and GPT-2 Embeddings<span class=acl-fixed-case>C</span>omparing the Geometry of <span class=acl-fixed-case>BERT</span>, <span class=acl-fixed-case>ELM</span>o, and <span class=acl-fixed-case>GPT</span>-2 Embeddings</a></strong><br><a href=/people/k/kawin-ethayarajh/>Kawin Ethayarajh</a><br><a href=/volumes/D19-1/ class=text-muted>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1006><div class="card-body p-3 small">Replacing static word embeddings with contextualized word representations has yielded significant improvements on many NLP tasks. However, just how contextual are the contextualized representations produced by <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> such as ELMo and BERT? Are there infinitely many context-specific representations for each word, or are words essentially assigned one of a finite number of word-sense representations? For one, we find that the contextualized representations of all words are not isotropic in any layer of the contextualizing model. While representations of the same word in different contexts still have a greater cosine similarity than those of two different words, this <a href=https://en.wikipedia.org/wiki/Self-similarity>self-similarity</a> is much lower in upper layers. This suggests that upper layers of contextualizing models produce more context-specific representations, much like how upper layers of LSTMs produce more task-specific representations. In all layers of ELMo, BERT, and GPT-2, on average, less than 5 % of the variance in a word&#8217;s contextualized representations can be explained by a static embedding for that word, providing some justification for the success of contextualized representations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1166.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1166 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1166 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/384490216 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1166/>Understanding Undesirable Word Embedding Associations</a></strong><br><a href=/people/k/kawin-ethayarajh/>Kawin Ethayarajh</a>
|
<a href=/people/d/david-duvenaud/>David Duvenaud</a>
|
<a href=/people/g/graeme-hirst/>Graeme Hirst</a><br><a href=/volumes/P19-1/ class=text-muted>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1166><div class="card-body p-3 small">Word embeddings are often criticized for capturing undesirable word associations such as <a href=https://en.wikipedia.org/wiki/Stereotypes_of_East_Asians_in_the_United_States>gender stereotypes</a>. However, <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> for measuring and removing such <a href=https://en.wikipedia.org/wiki/Bias>biases</a> remain poorly understood. We show that for any embedding model that implicitly does matrix factorization, debiasing vectors post hoc using subspace projection (Bolukbasi et al., 2016) is, under certain conditions, equivalent to training on an unbiased corpus. We also prove that WEAT, the most common <a href=https://en.wikipedia.org/wiki/Association_test>association test</a> for word embeddings, systematically overestimates bias. Given that the subspace projection method is provably effective, we use it to derive a new measure of association called the relational inner product association (RIPA). Experiments with RIPA reveal that, on average, skipgram with negative sampling (SGNS) does not make most words any more gendered than they are in the training corpus. However, for gender-stereotyped words, SGNS actually amplifies the gender association in the corpus.</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3012.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3012 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3012 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W18-3012.Notes.pdf data-toggle=tooltip data-placement=top title=Notes><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/W18-3012/>Unsupervised Random Walk Sentence Embeddings : A Strong but Simple Baseline</a></strong><br><a href=/people/k/kawin-ethayarajh/>Kawin Ethayarajh</a><br><a href=/volumes/W18-30/ class=text-muted>Proceedings of The Third Workshop on Representation Learning for NLP</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3012><div class="card-body p-3 small">Using a <a href=https://en.wikipedia.org/wiki/Random_walk_model>random walk model</a> of <a href=https://en.wikipedia.org/wiki/Text_generator>text generation</a>, Arora et al. (2017) proposed a strong baseline for computing sentence embeddings : take a weighted average of word embeddings and modify with SVD. This simple <a href=https://en.wikipedia.org/wiki/Methodology>method</a> even outperforms far more complex approaches such as <a href=https://en.wikipedia.org/wiki/Linear_time-invariant_system>LSTMs</a> on textual similarity tasks. In this paper, we first show that word vector length has a confounding effect on the probability of a sentence being generated in Arora et al.&#8217;s model. We propose a <a href=https://en.wikipedia.org/wiki/Random_walk_model>random walk model</a> that is robust to this confound, where the probability of word generation is inversely related to the angular distance between the word and sentence embeddings. Our <a href=https://en.wikipedia.org/wiki/Stiffness>approach</a> beats Arora et al.&#8217;s by up to 44.4 % on textual similarity tasks and is competitive with state-of-the-art methods. Unlike Arora et al.&#8217;s method, ours requires no hyperparameter tuning, which means it can be used when there is no labelled data.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Kawin+Ethayarajh" title="Search for 'Kawin Ethayarajh' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/d/dan-jurafsky/ class=align-middle>Dan Jurafsky</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/dorsa-sadigh/ class=align-middle>Dorsa Sadigh</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/david-duvenaud/ class=align-middle>David Duvenaud</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/g/graeme-hirst/ class=align-middle>Graeme Hirst</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/emnlp/ class=align-middle>EMNLP</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/acl/ class=align-middle>ACL</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/eval4nlp/ class=align-middle>Eval4NLP</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/ws/ class=align-middle>WS</a><span class="badge badge-secondary align-middle ml-2">1</span></li></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>