<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Heike Adel - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Heike</span> <span class=font-weight-bold>Adel</span></h2><hr><div class=row><div class=col-lg-9><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.689.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--689 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.689 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.689" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.689/>To Share or not to Share : Predicting Sets of Sources for Model Transfer Learning<span class=acl-fixed-case>P</span>redicting Sets of Sources for Model Transfer Learning</a></strong><br><a href=/people/l/lukas-lange/>Lukas Lange</a>
|
<a href=/people/j/jannik-strotgen/>Jannik Strötgen</a>
|
<a href=/people/h/heike-adel/>Heike Adel</a>
|
<a href=/people/d/dietrich-klakow/>Dietrich Klakow</a><br><a href=/volumes/2021.emnlp-main/ class=text-muted>Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--689><div class="card-body p-3 small">In low-resource settings, model transfer can help to overcome a lack of labeled data for many tasks and domains. However, predicting useful transfer sources is a challenging problem, as even the most similar sources might lead to unexpected negative transfer results. Thus, ranking methods based on task and text similarity as suggested in prior work may not be sufficient to identify promising sources. To tackle this problem, we propose a new approach to automatically determine which and how many sources should be exploited. For this, we study the effects of model transfer on <a href=https://en.wikipedia.org/wiki/Sequence_labeling>sequence labeling</a> across various domains and tasks and show that our methods based on model similarity and support vector machines are able to predict promising sources, resulting in performance increases of up to 24 F1 points.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-demo.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-demo.0/>Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</a></strong><br><a href=/people/h/heike-adel/>Heike Adel</a>
|
<a href=/people/s/shuming-shi/>Shuming Shi</a><br><a href=/volumes/2021.emnlp-demo/ class=text-muted>Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.blackboxnlp-1.3.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--blackboxnlp-1--3 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.blackboxnlp-1.3 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.blackboxnlp-1.3" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.blackboxnlp-1.3/>Does <a href=https://en.wikipedia.org/wiki/Knowledge>External Knowledge</a> Help Explainable <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>Natural Language Inference</a>? Automatic Evaluation vs. Human Ratings</a></strong><br><a href=/people/h/hendrik-schuff/>Hendrik Schuff</a>
|
<a href=/people/h/hsiu-yu-yang/>Hsiu-Yu Yang</a>
|
<a href=/people/h/heike-adel/>Heike Adel</a>
|
<a href=/people/n/ngoc-thang-vu/>Ngoc Thang Vu</a><br><a href=/volumes/2021.blackboxnlp-1/ class=text-muted>Proceedings of the Fourth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--blackboxnlp-1--3><div class="card-body p-3 small">Natural language inference (NLI) requires models to learn and apply commonsense knowledge. These reasoning abilities are particularly important for explainable NLI systems that generate a <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language explanation</a> in addition to their label prediction. The integration of external knowledge has been shown to improve NLI systems, here we investigate whether it can also improve their explanation capabilities. For this, we investigate different sources of external knowledge and evaluate the performance of our <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> on in-domain data as well as on special transfer datasets that are designed to assess fine-grained reasoning capabilities. We find that different sources of knowledge have a different effect on reasoning abilities, for example, <a href=https://en.wikipedia.org/wiki/Implicit_knowledge>implicit knowledge</a> stored in language models can hinder reasoning on numbers and <a href=https://en.wikipedia.org/wiki/Negation>negations</a>. Finally, we conduct the largest and most fine-grained explainable NLI crowdsourcing study to date. It reveals that even large differences in automatic performance scores do neither reflect in human ratings of label, explanation, commonsense nor <a href=https://en.wikipedia.org/wiki/Grammar>grammar correctness</a>.</div></div><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.acl-main.621.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--acl-main--621 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.acl-main.621 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://slideslive.com/38928788 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.acl-main.621" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.acl-main.621/>Closing the Gap : Joint De-Identification and Concept Extraction in the Clinical Domain</a></strong><br><a href=/people/l/lukas-lange/>Lukas Lange</a>
|
<a href=/people/h/heike-adel/>Heike Adel</a>
|
<a href=/people/j/jannik-strotgen/>Jannik Strötgen</a><br><a href=/volumes/2020.acl-main/ class=text-muted>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--acl-main--621><div class="card-body p-3 small">Exploiting <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a> in the <a href=https://en.wikipedia.org/wiki/Medicine>clinical domain</a> requires de-identification, i.e., <a href=https://en.wikipedia.org/wiki/Data_anonymization>anonymization of personal information</a> in texts. However, current research considers <a href=https://en.wikipedia.org/wiki/De-identification>de-identification</a> and downstream tasks, such as concept extraction, only in isolation and does not study the effects of <a href=https://en.wikipedia.org/wiki/De-identification>de-identification</a> on other tasks. In this paper, we close this gap by reporting concept extraction performance on automatically anonymized data and investigating joint models for <a href=https://en.wikipedia.org/wiki/De-identification>de-identification</a> and concept extraction. In particular, we propose a stacked model with restricted access to <a href=https://en.wikipedia.org/wiki/Information_privacy>privacy sensitive information</a> and a multitask model. We set the new state of the art on <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmark datasets</a> in English (96.1 % F1 for <a href=https://en.wikipedia.org/wiki/De-identification>de-identification</a> and 88.9 % F1 for concept extraction) and Spanish (91.4 % F1 for concept extraction).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.eval4nlp-1.15.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--eval4nlp-1--15 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.eval4nlp-1.15 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939708 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.eval4nlp-1.15" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.eval4nlp-1.15/>ClusterDataSplit : Exploring Challenging Clustering-Based Data Splits for Model Performance Evaluation<span class=acl-fixed-case>C</span>luster<span class=acl-fixed-case>D</span>ata<span class=acl-fixed-case>S</span>plit: Exploring Challenging Clustering-Based Data Splits for Model Performance Evaluation</a></strong><br><a href=/people/h/hanna-wecker/>Hanna Wecker</a>
|
<a href=/people/a/annemarie-friedrich/>Annemarie Friedrich</a>
|
<a href=/people/h/heike-adel/>Heike Adel</a><br><a href=/volumes/2020.eval4nlp-1/ class=text-muted>Proceedings of the First Workshop on Evaluation and Comparison of NLP Systems</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--eval4nlp-1--15><div class="card-body p-3 small">This paper adds to the ongoing discussion in the natural language processing community on how to choose a good development set. Motivated by the real-life necessity of applying <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning models</a> to different <a href=https://en.wikipedia.org/wiki/Distribution_(mathematics)>data distributions</a>, we propose a clustering-based data splitting algorithm. It creates development (or test) sets which are lexically different from the training data while ensuring similar label distributions. Hence, we are able to create challenging cross-validation evaluation setups while abstracting away from performance differences resulting from label distribution shifts between training and test data. In addition, we present a <a href=https://en.wikipedia.org/wiki/Python_(programming_language)>Python-based tool</a> for analyzing and visualizing data split characteristics and model performance. We illustrate the workings and results of our approach using a <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> and a patent classification task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.343.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--343 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.343 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.343/>An Analysis of Simple Data Augmentation for Named Entity Recognition</a></strong><br><a href=/people/x/xiang-dai/>Xiang Dai</a>
|
<a href=/people/h/heike-adel/>Heike Adel</a><br><a href=/volumes/2020.coling-main/ class=text-muted>Proceedings of the 28th International Conference on Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--343><div class="card-body p-3 small">Simple yet effective data augmentation techniques have been proposed for sentence-level and sentence-pair natural language processing tasks. Inspired by these efforts, we design and compare <a href=https://en.wikipedia.org/wiki/Data_augmentation>data augmentation</a> for <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>named entity recognition</a>, which is usually modeled as a token-level sequence labeling problem. Through experiments on two data sets from the biomedical and materials science domains (i2b2-2010 and MaSciP), we show that simple augmentation can boost performance for both recurrent and transformer-based models, especially for small training sets.</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-2008.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-2008 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-2008 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-2008/>DERE : A Task and Domain-Independent Slot Filling Framework for Declarative Relation Extraction<span class=acl-fixed-case>DERE</span>: A Task and Domain-Independent Slot Filling Framework for Declarative Relation Extraction</a></strong><br><a href=/people/h/heike-adel/>Heike Adel</a>
|
<a href=/people/l/laura-ana-maria-oberlander/>Laura Ana Maria Bostan</a>
|
<a href=/people/s/sean-papay/>Sean Papay</a>
|
<a href=/people/s/sebastian-pado/>Sebastian Padó</a>
|
<a href=/people/r/roman-klinger/>Roman Klinger</a><br><a href=/volumes/D18-2/ class=text-muted>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-2008><div class="card-body p-3 small">Most <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning systems</a> for <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a> are tailored to specific tasks. As a result, comparability of <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> across tasks is missing and their applicability to new tasks is limited. This affects end users without machine learning experience as well as model developers. To address these limitations, we present DERE, a novel <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> for declarative specification and compilation of template-based information extraction. It uses a generic <a href=https://en.wikipedia.org/wiki/Specification_language>specification language</a> for the task and for data annotations in terms of spans and <a href=https://en.wikipedia.org/wiki/Frame_(artificial_intelligence)>frames</a>. This formalism enables the representation of a large variety of natural language processing challenges. The <a href=https://en.wikipedia.org/wiki/Front_and_back_ends>backend</a> can be instantiated by different <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a>, following different paradigms. The clear separation of frame specification and <a href=https://en.wikipedia.org/wiki/Modeling_language>model backend</a> will ease the implementation of new <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> and the evaluation of different <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> across different tasks. Furthermore, it simplifies <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a>, joint learning across tasks and/or domains as well as the assessment of model generalizability. DERE is available as open-source software.</div></div><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1181.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1181 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1181 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1181/>Global Normalization of <a href=https://en.wikipedia.org/wiki/Convolutional_neural_network>Convolutional Neural Networks</a> for Joint Entity and Relation Classification</a></strong><br><a href=/people/h/heike-adel/>Heike Adel</a>
|
<a href=/people/h/hinrich-schutze/>Hinrich Schütze</a><br><a href=/volumes/D17-1/ class=text-muted>Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1181><div class="card-body p-3 small">We introduce globally normalized convolutional neural networks for joint entity classification and <a href=https://en.wikipedia.org/wiki/Relation_extraction>relation extraction</a>. In particular, we propose a way to utilize a linear-chain conditional random field output layer for predicting entity types and relations between entities at the same time. Our experiments show that global normalization outperforms a locally normalized softmax layer on a benchmark dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1003.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1003 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1003 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1003/>Exploring Different Dimensions of Attention for Uncertainty Detection</a></strong><br><a href=/people/h/heike-adel/>Heike Adel</a>
|
<a href=/people/h/hinrich-schutze/>Hinrich Schütze</a><br><a href=/volumes/E17-1/ class=text-muted>Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1003><div class="card-body p-3 small">Neural networks with <a href=https://en.wikipedia.org/wiki/Attention>attention</a> have proven effective for many natural language processing tasks. In this paper, we develop <a href=https://en.wikipedia.org/wiki/Attentional_control>attention mechanisms</a> for uncertainty detection. In particular, we generalize standardly used <a href=https://en.wikipedia.org/wiki/Attentional_control>attention mechanisms</a> by introducing external attention and sequence-preserving attention. These novel architectures differ from standard approaches in that they use external resources to compute attention weights and preserve <a href=https://en.wikipedia.org/wiki/Sequence>sequence information</a>. We compare them to other <a href=https://en.wikipedia.org/wiki/Configuration_(geometry)>configurations</a> along different dimensions of attention. Our novel architectures set the new state of the art on a Wikipedia benchmark dataset and perform similar to the state-of-the-art model on a biomedical benchmark which uses a large set of linguistic features.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1111.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1111 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1111 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1111/>Noise Mitigation for Neural Entity Typing and <a href=https://en.wikipedia.org/wiki/Relation_extraction>Relation Extraction</a></a></strong><br><a href=/people/y/yadollah-yaghoobzadeh/>Yadollah Yaghoobzadeh</a>
|
<a href=/people/h/heike-adel/>Heike Adel</a>
|
<a href=/people/h/hinrich-schutze/>Hinrich Schütze</a><br><a href=/volumes/E17-1/ class=text-muted>Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1111><div class="card-body p-3 small">In this paper, we address two different types of <a href=https://en.wikipedia.org/wiki/Noise_(signal_processing)>noise</a> in information extraction models : noise from distant supervision and noise from pipeline input features. Our target tasks are <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entity typing</a> and <a href=https://en.wikipedia.org/wiki/Relation_(database)>relation extraction</a>. For the first noise type, we introduce multi-instance multi-label learning algorithms using neural network models, and apply them to fine-grained entity typing for the first time. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms the state-of-the-art supervised approach which uses global embeddings of entities. For the second noise type, we propose ways to improve the integration of noisy entity type predictions into <a href=https://en.wikipedia.org/wiki/Relation_extraction>relation extraction</a>. Our experiments show that probabilistic predictions are more robust than discrete predictions and that joint training of the two tasks performs best.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2094.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2094 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2094 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2094/>Ranking Convolutional Recurrent Neural Networks for Purchase Stage Identification on Imbalanced Twitter Data<span class=acl-fixed-case>T</span>witter Data</a></strong><br><a href=/people/h/heike-adel/>Heike Adel</a>
|
<a href=/people/f/francine-chen/>Francine Chen</a>
|
<a href=/people/y/yan-ying-chen/>Yan-Ying Chen</a><br><a href=/volumes/E17-2/ class=text-muted>Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2094><div class="card-body p-3 small">Users often use <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> to share their interest in products. We propose to identify purchase stages from Twitter data following the AIDA model (Awareness, Interest, Desire, Action). In particular, we define the task of classifying the purchase stage of each tweet in a user&#8217;s tweet sequence. We introduce RCRNN, a Ranking Convolutional Recurrent Neural Network which computes tweet representations using convolution over word embeddings and models a tweet sequence with gated recurrent units. Also, we consider various methods to cope with the imbalanced label distribution in our data and show that a ranking layer outperforms class weights.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Heike+Adel" title="Search for 'Heike Adel' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/h/hinrich-schutze/ class=align-middle>Hinrich Schütze</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/l/lukas-lange/ class=align-middle>Lukas Lange</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/j/jannik-strotgen/ class=align-middle>Jannik Strötgen</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/h/hanna-wecker/ class=align-middle>Hanna Wecker</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/annemarie-friedrich/ class=align-middle>Annemarie Friedrich</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/l/laura-ana-maria-oberlander/ class=align-middle>Laura Ana Maria Oberländer</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/sean-papay/ class=align-middle>Sean Papay</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/sebastian-pado/ class=align-middle>Sebastian Padó</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/roman-klinger/ class=align-middle>Roman Klinger</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/dietrich-klakow/ class=align-middle>Dietrich Klakow</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/shuming-shi/ class=align-middle>Shuming Shi</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/hendrik-schuff/ class=align-middle>Hendrik Schuff</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/hsiu-yu-yang/ class=align-middle>Hsiu-Yu Yang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/n/ngoc-thang-vu/ class=align-middle>Ngoc Thang Vu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/x/xiang-dai/ class=align-middle>Xiang Dai</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yadollah-yaghoobzadeh/ class=align-middle>Yadollah Yaghoobzadeh</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/f/francine-chen/ class=align-middle>Francine Chen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yan-ying-chen/ class=align-middle>Yan-Ying Chen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/emnlp/ class=align-middle>EMNLP</a><span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/venues/eacl/ class=align-middle>EACL</a><span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/venues/acl/ class=align-middle>ACL</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/eval4nlp/ class=align-middle>Eval4NLP</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/blackboxnlp/ class=align-middle>BlackboxNLP</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-venues aria-expanded=false aria-controls=more-venues>show all...</li><div class="collapse border-top" id=more-venues><li class=list-group-item><a href=/venues/coling/ class=align-middle>COLING</a><span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>