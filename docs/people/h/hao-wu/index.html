<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Hao Wu - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Hao</span> <span class=font-weight-bold>Wu</span></h2><hr><div class=row><div class=col-lg-9><h4>2022</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.acl-long.195.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--acl-long--195 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.acl-long.195 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.acl-long.195/>A Meta-framework for Spatiotemporal Quantity Extraction from Text</a></strong><br><a href=/people/q/qiang-ning/>Qiang Ning</a>
|
<a href=/people/b/ben-zhou/>Ben Zhou</a>
|
<a href=/people/h/hao-wu/>Hao Wu</a>
|
<a href=/people/h/haoruo-peng/>Haoruo Peng</a>
|
<a href=/people/c/chuchu-fan/>Chuchu Fan</a>
|
<a href=/people/m/matt-gardner/>Matt Gardner</a><br><a href=/volumes/2022.acl-long/ class=text-muted>Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--acl-long--195><div class="card-body p-3 small">News events are often associated with quantities (e.g., the number of COVID-19 patients or the number of arrests in a protest), and it is often important to extract their type, time, and location from unstructured text in order to analyze these quantity events. This paper thus formulates the NLP problem of spatiotemporal quantity extraction, and proposes the first meta-framework for solving it. This meta-framework contains a formalism that decomposes the problem into several information extraction tasks, a shareable crowdsourcing pipeline, and transformer-based baseline models. We demonstrate the meta-framework in three domains&#8212;the COVID-19 pandemic, Black Lives Matter protests, and 2020 California wildfires&#8212;to show that the formalism is general and extensible, the crowdsourcing pipeline facilitates fast and high-quality data annotation, and the baseline system can handle spatiotemporal quantity extraction well enough to be practically useful. We release all resources for future research on this topic at https://github.com/steqe.</div></div><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.238.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--238 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.238 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.emnlp-main.238.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939286 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.238" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.238/>Semi-Supervised Bilingual Lexicon Induction with Two-way Interaction</a></strong><br><a href=/people/x/xu-zhao/>Xu Zhao</a>
|
<a href=/people/z/zihao-wang/>Zihao Wang</a>
|
<a href=/people/h/hao-wu/>Hao Wu</a>
|
<a href=/people/y/yong-zhang/>Yong Zhang</a><br><a href=/volumes/2020.emnlp-main/ class=text-muted>Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--238><div class="card-body p-3 small">Semi-supervision is a promising paradigm for Bilingual Lexicon Induction (BLI) with limited annotations. However, previous semisupervised methods do not fully utilize the knowledge hidden in annotated and nonannotated data, which hinders further improvement of their performance. In this paper, we propose a new semi-supervised BLI framework to encourage the interaction between the <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised signal</a> and <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised alignment</a>. We design two message-passing mechanisms to transfer knowledge between annotated and non-annotated data, named prior optimal transport and bi-directional lexicon update respectively. Then, we perform <a href=https://en.wikipedia.org/wiki/Semi-supervised_learning>semi-supervised learning</a> based on a cyclic or a parallel parameter feeding routine to update our <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>. Our <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> is a general framework that can incorporate any supervised and unsupervised BLI methods based on <a href=https://en.wikipedia.org/wiki/Optimal_transport>optimal transport</a>. Experimental results on MUSE and VecMap datasets show significant improvement of our <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>. Ablation study also proves that the two-way interaction between the supervised signal and unsupervised alignment accounts for the gain of the overall performance. Results on distant language pairs further illustrate the advantage and robustness of our proposed <a href=https://en.wikipedia.org/wiki/Methodology>method</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.acl-main.274.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--acl-main--274 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.acl-main.274 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://slideslive.com/38929428 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.acl-main.274/>A Relaxed Matching Procedure for Unsupervised BLI<span class=acl-fixed-case>BLI</span></a></strong><br><a href=/people/x/xu-zhao/>Xu Zhao</a>
|
<a href=/people/z/zihao-wang/>Zihao Wang</a>
|
<a href=/people/y/yong-zhang/>Yong Zhang</a>
|
<a href=/people/h/hao-wu/>Hao Wu</a><br><a href=/volumes/2020.acl-main/ class=text-muted>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--acl-main--274><div class="card-body p-3 small">Recently unsupervised Bilingual Lexicon Induction(BLI) without any <a href=https://en.wikipedia.org/wiki/Parallel_corpus>parallel corpus</a> has attracted much research interest. One of the crucial parts in <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>methods</a> for the BLI task is the matching procedure. Previous works impose a too strong <a href=https://en.wikipedia.org/wiki/Constraint_(mathematics)>constraint</a> on the <a href=https://en.wikipedia.org/wiki/Impedance_matching>matching</a> and lead to many counterintuitive translation pairings. Thus We propose a relaxed matching procedure to find a more precise matching between two languages. We also find that aligning source and target language embedding space bidirectionally will bring significant improvement. We follow the previous iterative framework to conduct experiments. Results on standard <a href=https://en.wikipedia.org/wiki/Benchmarking>benchmark</a> demonstrate the effectiveness of our proposed method, which substantially outperforms previous <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised methods</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.79.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--79 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.79 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.79/>Warren at SemEval-2020 Task 4 : ALBERT and Multi-Task Learning for Commonsense Validation<span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 4: <span class=acl-fixed-case>ALBERT</span> and Multi-Task Learning for Commonsense Validation</a></strong><br><a href=/people/y/yuhang-wu/>Yuhang Wu</a>
|
<a href=/people/h/hao-wu/>Hao Wu</a><br><a href=/volumes/2020.semeval-1/ class=text-muted>Proceedings of the Fourteenth Workshop on Semantic Evaluation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--79><div class="card-body p-3 small">This paper describes our <a href=https://en.wikipedia.org/wiki/System>system</a> in subtask A of SemEval 2020 Shared Task 4. We propose a <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning model</a> based on MTL(Multi-Task Learning) to enhance the prediction ability of commonsense validation. The experimental results demonstrate that our <a href=https://en.wikipedia.org/wiki/System>system</a> outperforms the single-task text classification model. We combine MTL and ALBERT pretrain model to achieve an <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of 0.904 and our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is ranked 16th on the final leader board of the competition among the 45 teams.</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S18-1015.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S18-1015 data-toggle=collapse aria-expanded=false aria-controls=abstract-S18-1015 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/S18-1015/>NLPZZX at SemEval-2018 Task 1 : Using Ensemble Method for Emotion and Sentiment Intensity Determination<span class=acl-fixed-case>NLPZZX</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2018 Task 1: Using Ensemble Method for Emotion and Sentiment Intensity Determination</a></strong><br><a href=/people/z/zhengxin-zhang/>Zhengxin Zhang</a>
|
<a href=/people/q/qimin-zhou/>Qimin Zhou</a>
|
<a href=/people/h/hao-wu/>Hao Wu</a><br><a href=/volumes/S18-1/ class=text-muted>Proceedings of The 12th International Workshop on Semantic Evaluation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S18-1015><div class="card-body p-3 small">In this paper, we put forward a <a href=https://en.wikipedia.org/wiki/System>system</a> that competed at SemEval-2018 Task 1 : Affect in Tweets. Our <a href=https://en.wikipedia.org/wiki/System>system</a> uses a simple yet effective <a href=https://en.wikipedia.org/wiki/Ensemble_learning>ensemble method</a> which combines several <a href=https://en.wikipedia.org/wiki/Neural_circuit>neural network components</a>. We participate in two subtasks for English tweets : EI-reg and V-reg. For two subtasks, different combinations of <a href=https://en.wikipedia.org/wiki/Neural_circuit>neural components</a> are examined. For EI-reg, our system achieves an <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of 0.727 in <a href=https://en.wikipedia.org/wiki/Pearson_correlation_coefficient>Pearson Correlation Coefficient</a> (all instances) and an <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of 0.555 in <a href=https://en.wikipedia.org/wiki/Pearson_correlation_coefficient>Pearson Correlation Coefficient</a> (0.5-1). For V-reg, the achieved accuracy scores are respectively 0.835 and 0.670</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S18-1046.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S18-1046 data-toggle=collapse aria-expanded=false aria-controls=abstract-S18-1046 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/S18-1046/>Zewen at SemEval-2018 Task 1 : An Ensemble Model for Affect Prediction in Tweets<span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2018 Task 1: An Ensemble Model for Affect Prediction in Tweets</a></strong><br><a href=/people/z/zewen-chi/>Zewen Chi</a>
|
<a href=/people/h/he-yan-huang/>Heyan Huang</a>
|
<a href=/people/j/jiangui-chen/>Jiangui Chen</a>
|
<a href=/people/h/hao-wu/>Hao Wu</a>
|
<a href=/people/r/ran-wei/>Ran Wei</a><br><a href=/volumes/S18-1/ class=text-muted>Proceedings of The 12th International Workshop on Semantic Evaluation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S18-1046><div class="card-body p-3 small">This paper presents a method for Affect in Tweets, which is the task to automatically determine the intensity of emotions and intensity of sentiment of tweets. The term <a href=https://en.wikipedia.org/wiki/Affect_(psychology)>affect</a> refers to emotion-related categories such as <a href=https://en.wikipedia.org/wiki/Anger>anger</a>, <a href=https://en.wikipedia.org/wiki/Fear>fear</a>, etc. Intensity of emo-tions need to be quantified into a real valued score in [ 0, 1 ]. We propose an en-semble system including four different deep learning methods which are CNN, Bidirectional LSTM (BLSTM), LSTM-CNN and a CNN-based Attention model (CA). Our system gets an <a href=https://en.wikipedia.org/wiki/Pearson_correlation_coefficient>average Pearson correlation score</a> of 0.682 in the subtask EI-reg and an <a href=https://en.wikipedia.org/wiki/Pearson_correlation_coefficient>average Pearson correlation score</a> of 0.784 in subtask V-reg, which ranks 17th among 48 systems in EI-reg and 19th among 38 systems in V-reg.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6226.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6226 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6226 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6226/>NLP at IEST 2018 : BiLSTM-Attention and LSTM-Attention via Soft Voting in Emotion Classification<span class=acl-fixed-case>NLP</span> at <span class=acl-fixed-case>IEST</span> 2018: <span class=acl-fixed-case>B</span>i<span class=acl-fixed-case>LSTM</span>-Attention and <span class=acl-fixed-case>LSTM</span>-Attention via Soft Voting in Emotion Classification</a></strong><br><a href=/people/q/qimin-zhou/>Qimin Zhou</a>
|
<a href=/people/h/hao-wu/>Hao Wu</a><br><a href=/volumes/W18-62/ class=text-muted>Proceedings of the 9th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6226><div class="card-body p-3 small">This paper describes our <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>method</a> that competed at WASSA2018 Implicit Emotion Shared Task. The goal of this task is to classify the emotions of excluded words in tweets into six different classes : <a href=https://en.wikipedia.org/wiki/Sadness>sad</a>, <a href=https://en.wikipedia.org/wiki/Joy>joy</a>, <a href=https://en.wikipedia.org/wiki/Disgust>disgust</a>, <a href=https://en.wikipedia.org/wiki/Surprise_(emotion)>surprise</a>, <a href=https://en.wikipedia.org/wiki/Anger>anger</a> and fear. For this, we examine a BiLSTM architecture with attention mechanism (BiLSTM-Attention) and a LSTM architecture with attention mechanism (LSTM-Attention), and try different dropout rates based on these two models. We then exploit an <a href=https://en.wikipedia.org/wiki/Statistical_ensemble_(mathematical_physics)>ensemble</a> of these methods to give the final prediction which improves the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> performance significantly compared with the baseline model. The proposed <a href=https://en.wikipedia.org/wiki/Methodology>method</a> achieves 7th position out of 30 teams and outperforms the baseline method by 12.5 % in terms of macro F1.<i>Implicit Emotion Shared Task</i>. The goal of this task is to classify the emotions of excluded words in tweets into six different classes: sad, joy, disgust, surprise, anger and fear. For this, we examine a BiLSTM architecture with attention mechanism (BiLSTM-Attention) and a LSTM architecture with attention mechanism (LSTM-Attention), and try different dropout rates based on these two models. We then exploit an ensemble of these methods to give the final prediction which improves the model performance significantly compared with the baseline model. The proposed method achieves 7th position out of 30 teams and outperforms the baseline method by 12.5% in terms of <i>macro F1</i>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1077.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1077 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1077 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N18-1077.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N18-1077/>Improving Temporal Relation Extraction with a Globally Acquired Statistical Resource</a></strong><br><a href=/people/q/qiang-ning/>Qiang Ning</a>
|
<a href=/people/h/hao-wu/>Hao Wu</a>
|
<a href=/people/h/haoruo-peng/>Haoruo Peng</a>
|
<a href=/people/d/dan-roth/>Dan Roth</a><br><a href=/volumes/N18-1/ class=text-muted>Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1077><div class="card-body p-3 small">Extracting temporal relations (before, after, overlapping, etc.) is a key aspect of understanding events described in <a href=https://en.wikipedia.org/wiki/Natural_language>natural language</a>. We argue that this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> would gain from the availability of a <a href=https://en.wikipedia.org/wiki/Resource>resource</a> that provides prior knowledge in the form of the temporal order that events usually follow. This paper develops such a resource a probabilistic knowledge base acquired in the news domain by extracting temporal relations between events from the New York Times (NYT) articles over a 20-year span (19872007). We show that existing temporal extraction systems can be improved via this <a href=https://en.wikipedia.org/wiki/Resource>resource</a>. As a byproduct, we also show that interesting statistics can be retrieved from this <a href=https://en.wikipedia.org/wiki/Resource_(computer_science)>resource</a>, which can potentially benefit other time-aware tasks. The proposed <a href=https://en.wikipedia.org/wiki/System>system</a> and <a href=https://en.wikipedia.org/wiki/Resource>resource</a> are both publicly available.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1122.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1122 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1122 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1122.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1122.Notes.pdf data-toggle=tooltip data-placement=top title=Notes><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285803517 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-1122/>A Multi-Axis Annotation Scheme for Event Temporal Relations</a></strong><br><a href=/people/q/qiang-ning/>Qiang Ning</a>
|
<a href=/people/h/hao-wu/>Hao Wu</a>
|
<a href=/people/d/dan-roth/>Dan Roth</a><br><a href=/volumes/P18-1/ class=text-muted>Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1122><div class="card-body p-3 small">Existing temporal relation (TempRel) annotation schemes often have low inter-annotator agreements (IAA) even between experts, suggesting that the current annotation task needs a better definition. This paper proposes a new multi-axis modeling to better capture the temporal structure of events. In addition, we identify that event end-points are a major source of confusion in <a href=https://en.wikipedia.org/wiki/Annotation>annotation</a>, so we also propose to annotate TempRels based on start-points only. A pilot expert annotation effort using the proposed scheme shows significant improvement in IAA from the conventional 60&#8217;s to 80&#8217;s (Cohen&#8217;s Kappa). This better-defined annotation scheme further enables the use of <a href=https://en.wikipedia.org/wiki/Crowdsourcing>crowdsourcing</a> to alleviate the labor intensity for each annotator. We hope that this work can foster more interesting studies towards <a href=https://en.wikipedia.org/wiki/Event_(probability_theory)>event understanding</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1212.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1212 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1212 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1212.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1212.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285805571 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-1212/>Joint Reasoning for Temporal and Causal Relations</a></strong><br><a href=/people/q/qiang-ning/>Qiang Ning</a>
|
<a href=/people/z/zhili-feng/>Zhili Feng</a>
|
<a href=/people/h/hao-wu/>Hao Wu</a>
|
<a href=/people/d/dan-roth/>Dan Roth</a><br><a href=/volumes/P18-1/ class=text-muted>Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1212><div class="card-body p-3 small">Understanding temporal and causal relations between events is a fundamental <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language understanding task</a>. Because a cause must occur earlier than its effect, temporal and causal relations are closely related and one relation often dictates the value of the other. However, limited attention has been paid to studying these two relations jointly. This paper presents a joint inference framework for <a href=https://en.wikipedia.org/wiki/Logical_disjunction>them</a> using constrained conditional models (CCMs). Specifically, we formulate the joint problem as an integer linear programming (ILP) problem, enforcing <a href=https://en.wikipedia.org/wiki/Constraint_(mathematics)>constraints</a> that are inherent in the nature of time and <a href=https://en.wikipedia.org/wiki/Causality>causality</a>. We show that the joint inference framework results in statistically significant improvement in the extraction of both temporal and causal relations from text.</div></div><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S17-2007.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S17-2007 data-toggle=collapse aria-expanded=false aria-controls=abstract-S17-2007 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/S17-2007/>BIT at SemEval-2017 Task 1 : Using Semantic Information Space to Evaluate Semantic Textual Similarity<span class=acl-fixed-case>BIT</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2017 Task 1: Using Semantic Information Space to Evaluate Semantic Textual Similarity</a></strong><br><a href=/people/h/hao-wu/>Hao Wu</a>
|
<a href=/people/h/he-yan-huang/>Heyan Huang</a>
|
<a href=/people/p/ping-jian/>Ping Jian</a>
|
<a href=/people/y/yuhang-guo/>Yuhang Guo</a>
|
<a href=/people/c/chao-su/>Chao Su</a><br><a href=/volumes/S17-2/ class=text-muted>Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S17-2007><div class="card-body p-3 small">This paper presents three systems for semantic textual similarity (STS) evaluation at SemEval-2017 STS task. One is an <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised system</a> and the other two are <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised systems</a> which simply employ the unsupervised one. All our systems mainly depend on the (SIS), which is constructed based on the semantic hierarchical taxonomy in <a href=https://en.wikipedia.org/wiki/WordNet>WordNet</a>, to compute non-overlapping information content (IC) of sentences. Our team ranked 2nd among 31 participating teams by the primary score of Pearson correlation coefficient (PCC) mean of 7 tracks and achieved the best performance on Track 1 (AR-AR) dataset.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Hao+Wu" title="Search for 'Hao Wu' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/q/qiang-ning/ class=align-middle>Qiang Ning</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/people/d/dan-roth/ class=align-middle>Dan Roth</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/x/xu-zhao/ class=align-middle>Xu Zhao</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/z/zihao-wang/ class=align-middle>Zihao Wang</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/y/yong-zhang/ class=align-middle>Yong Zhang</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/h/haoruo-peng/ class=align-middle>Haoruo Peng</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/h/he-yan-huang/ class=align-middle>He-Yan Huang</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/q/qimin-zhou/ class=align-middle>Qimin Zhou</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/b/ben-zhou/ class=align-middle>Ben Zhou</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/chuchu-fan/ class=align-middle>Chuchu Fan</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/matt-gardner/ class=align-middle>Matt Gardner</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yuhang-wu/ class=align-middle>Yuhang Wu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/p/ping-jian/ class=align-middle>Ping Jian</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yuhang-guo/ class=align-middle>Yuhang Guo</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/chao-su/ class=align-middle>Chao Su</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/zhengxin-zhang/ class=align-middle>Zhengxin Zhang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/zewen-chi/ class=align-middle>Zewen Chi</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jiangui-chen/ class=align-middle>Jiangui Chen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/ran-wei/ class=align-middle>Ran Wei</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/zhili-feng/ class=align-middle>Zhili Feng</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/acl/ class=align-middle>ACL</a><span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/venues/semeval/ class=align-middle>SemEval</a><span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/venues/emnlp/ class=align-middle>EMNLP</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/ws/ class=align-middle>WS</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/naacl/ class=align-middle>NAACL</a><span class="badge badge-secondary align-middle ml-2">1</span></li></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>