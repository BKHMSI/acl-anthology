<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Hao Yang - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Hao</span> <span class=font-weight-bold>Yang</span></h2><hr><div class=row><div class=col-lg-9><h4>2022</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-acl.109.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-acl--109 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-acl.109 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.findings-acl.109" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2022.findings-acl.109/>Sentiment Word Aware Multimodal Refinement for Multimodal Sentiment Analysis with ASR Errors<span class=acl-fixed-case>ASR</span> Errors</a></strong><br><a href=/people/y/yang-wu/>Yang Wu</a>
|
<a href=/people/y/yanyan-zhao/>Yanyan Zhao</a>
|
<a href=/people/h/hao-yang/>Hao Yang</a>
|
<a href=/people/s/song-chen/>Song Chen</a>
|
<a href=/people/b/bing-qin/>Bing Qin</a>
|
<a href=/people/x/xiaohuan-cao/>Xiaohuan Cao</a>
|
<a href=/people/w/wenting-zhao/>Wenting Zhao</a><br><a href=/volumes/2022.findings-acl/ class=text-muted>Findings of the Association for Computational Linguistics: ACL 2022</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-acl--109><div class="card-body p-3 small">Multimodal sentiment analysis has attracted increasing attention and lots of models have been proposed However the performance of the state of the art <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> decreases sharply when they are deployed in the real world We find that the main reason is that real world applications can only access the text outputs by the automatic speech recognition ASR models which may be with errors because of the limitation of model capacity Through further analysis of the ASR outputs we find that in some cases the sentiment words the key sentiment elements in the textual modality are recognized as other words which makes the sentiment of the text change and hurts the performance of multimodal sentiment analysis models directly To address this problem we propose the sentiment word aware multimodal refinement model SWRM which can dynamically refine the erroneous sentiment words by leveraging multimodal sentiment clues Specifically we first use the sentiment word position detection module to obtain the most possible position of the sentiment word in the text and then utilize the multimodal sentiment word refinement module to dynamically refine the sentiment word embeddings The refined embeddings are taken as the textual inputs of the multimodal feature fusion module to predict the sentiment labels We conduct extensive experiments on the real world datasets including MOSI Speechbrain MOSI IBM and MOSI iFlytek and the results demonstrate the effectiveness of our model which surpasses the current state of the art models on three datasets Furthermore our approach can be adapted for other multimodal feature fusion models easily</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-acl.120.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-acl--120 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-acl.120 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.findings-acl.120/>Capture Human Disagreement Distributions by Calibrated Networks for Natural Language Inference</a></strong><br><a href=/people/y/yuxia-wang/>Yuxia Wang</a>
|
<a href=/people/m/minghan-wang/>Minghan Wang</a>
|
<a href=/people/y/yimeng-chen/>Yimeng Chen</a>
|
<a href=/people/s/shimin-tao/>Shimin Tao</a>
|
<a href=/people/j/jiaxin-guo/>Jiaxin Guo</a>
|
<a href=/people/c/chang-su/>Chang Su</a>
|
<a href=/people/m/min-zhang/>Min Zhang</a>
|
<a href=/people/h/hao-yang/>Hao Yang</a><br><a href=/volumes/2022.findings-acl/ class=text-muted>Findings of the Association for Computational Linguistics: ACL 2022</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-acl--120><div class="card-body p-3 small">Natural Language Inference NLI datasets contain examples with highly ambiguous labels due to its subjectivity Several recent efforts have been made to acknowledge and embrace the existence of <a href=https://en.wikipedia.org/wiki/Ambiguity>ambiguity</a> and explore how to capture the human disagreement distribution In contrast with directly learning from gold ambiguity labels relying on special resource we argue that the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> has naturally captured the human ambiguity distribution as long as its calibrated i.e. the predictive probability can reflect the true correctness likelihood Our experiments show that when model is well calibrated either by label smoothing or temperature scaling it can obtain competitive performance as prior work on both divergence scores between predictive probability and the true human opinion distribution and the accuracy This reveals the overhead of collecting gold ambiguity labels can be cut by broadly solving how to calibrate the NLI network</div></div><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wmt-1.21.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wmt-1--21 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wmt-1.21 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wmt-1.21/>HW-TSC’s Participation in the WMT 2021 News Translation Shared Task<span class=acl-fixed-case>HW</span>-<span class=acl-fixed-case>TSC</span>’s Participation in the <span class=acl-fixed-case>WMT</span> 2021 News Translation Shared Task</a></strong><br><a href=/people/d/daimeng-wei/>Daimeng Wei</a>
|
<a href=/people/z/zongyao-li/>Zongyao Li</a>
|
<a href=/people/z/zhanglin-wu/>Zhanglin Wu</a>
|
<a href=/people/z/zhengzhe-yu/>Zhengzhe Yu</a>
|
<a href=/people/x/xiaoyu-chen/>Xiaoyu Chen</a>
|
<a href=/people/h/hengchao-shang/>Hengchao Shang</a>
|
<a href=/people/j/jiaxin-guo/>Jiaxin Guo</a>
|
<a href=/people/m/minghan-wang/>Minghan Wang</a>
|
<a href=/people/l/lizhi-lei/>Lizhi Lei</a>
|
<a href=/people/m/min-zhang/>Min Zhang</a>
|
<a href=/people/h/hao-yang/>Hao Yang</a>
|
<a href=/people/y/ying-qin/>Ying Qin</a><br><a href=/volumes/2021.wmt-1/ class=text-muted>Proceedings of the Sixth Conference on Machine Translation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wmt-1--21><div class="card-body p-3 small">This paper presents the submission of Huawei Translate Services Center (HW-TSC) to the WMT 2021 News Translation Shared Task. We participate in 7 language pairs, including Zh / En, De / En, Ja / En, Ha / En, Is / En, Hi / Bn, and Xh / Zu in both directions under the constrained condition. We use Transformer architecture and obtain the best performance via multiple variants with larger parameter sizes. We perform detailed pre-processing and filtering on the provided large-scale bilingual and monolingual datasets. Several commonly used strategies are used to train our models, such as <a href=https://en.wikipedia.org/wiki/Back_translation>Back Translation</a>, Forward Translation, Multilingual Translation, Ensemble Knowledge Distillation, etc. Our submission obtains competitive results in the final evaluation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wmt-1.37.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wmt-1--37 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wmt-1.37 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wmt-1.37/>HW-TSC’s Participation in the WMT 2021 Triangular MT Shared Task<span class=acl-fixed-case>HW</span>-<span class=acl-fixed-case>TSC</span>’s Participation in the <span class=acl-fixed-case>WMT</span> 2021 Triangular <span class=acl-fixed-case>MT</span> Shared Task</a></strong><br><a href=/people/z/zongyao-li/>Zongyao Li</a>
|
<a href=/people/d/daimeng-wei/>Daimeng Wei</a>
|
<a href=/people/h/hengchao-shang/>Hengchao Shang</a>
|
<a href=/people/x/xiaoyu-chen/>Xiaoyu Chen</a>
|
<a href=/people/z/zhanglin-wu/>Zhanglin Wu</a>
|
<a href=/people/z/zhengzhe-yu/>Zhengzhe Yu</a>
|
<a href=/people/j/jiaxin-guo/>Jiaxin Guo</a>
|
<a href=/people/m/minghan-wang/>Minghan Wang</a>
|
<a href=/people/l/lizhi-lei/>Lizhi Lei</a>
|
<a href=/people/m/min-zhang/>Min Zhang</a>
|
<a href=/people/h/hao-yang/>Hao Yang</a>
|
<a href=/people/y/ying-qin/>Ying Qin</a><br><a href=/volumes/2021.wmt-1/ class=text-muted>Proceedings of the Sixth Conference on Machine Translation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wmt-1--37><div class="card-body p-3 small">This paper presents the submission of Huawei Translation Service Center (HW-TSC) to WMT 2021 Triangular MT Shared Task. We participate in the Russian-to-Chinese task under the constrained condition. We use Transformer architecture and obtain the best performance via a variant with larger parameter sizes. We perform detailed data pre-processing and filtering on the provided large-scale bilingual data. Several strategies are used to train our models, such as Multilingual Translation, Back Translation, Forward Translation, Data Denoising, Average Checkpoint, Ensemble, Fine-tuning, etc. Our <a href=https://en.wikipedia.org/wiki/System>system</a> obtains 32.5 <a href=https://en.wikipedia.org/wiki/British_undergraduate_degree_classification>BLEU</a> on the <a href=https://en.wikipedia.org/wiki/British_undergraduate_degree_classification>dev set</a> and 27.7 BLEU on the <a href=https://en.wikipedia.org/wiki/British_undergraduate_degree_classification>test set</a>, the highest score among all submissions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wmt-1.55.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wmt-1--55 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wmt-1.55 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wmt-1.55/>HW-TSC’s Participation in the WMT 2021 Large-Scale Multilingual Translation Task<span class=acl-fixed-case>HW</span>-<span class=acl-fixed-case>TSC</span>’s Participation in the <span class=acl-fixed-case>WMT</span> 2021 Large-Scale Multilingual Translation Task</a></strong><br><a href=/people/z/zhengzhe-yu/>Zhengzhe Yu</a>
|
<a href=/people/d/daimeng-wei/>Daimeng Wei</a>
|
<a href=/people/z/zongyao-li/>Zongyao Li</a>
|
<a href=/people/h/hengchao-shang/>Hengchao Shang</a>
|
<a href=/people/x/xiaoyu-chen/>Xiaoyu Chen</a>
|
<a href=/people/z/zhanglin-wu/>Zhanglin Wu</a>
|
<a href=/people/j/jiaxin-guo/>Jiaxin Guo</a>
|
<a href=/people/m/minghan-wang/>Minghan Wang</a>
|
<a href=/people/l/lizhi-lei/>Lizhi Lei</a>
|
<a href=/people/m/min-zhang/>Min Zhang</a>
|
<a href=/people/h/hao-yang/>Hao Yang</a>
|
<a href=/people/y/ying-qin/>Ying Qin</a><br><a href=/volumes/2021.wmt-1/ class=text-muted>Proceedings of the Sixth Conference on Machine Translation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wmt-1--55><div class="card-body p-3 small">This paper presents the submission of Huawei Translation Services Center (HW-TSC) to the WMT 2021 Large-Scale Multilingual Translation Task. We participate in Samll Track # 2, including 6 languages : <a href=https://en.wikipedia.org/wiki/Javanese_language>Javanese (Jv)</a>, <a href=https://en.wikipedia.org/wiki/Indonesian_language>Indonesian (I d)</a>, <a href=https://en.wikipedia.org/wiki/Malay_language>Malay (Ms)</a>, <a href=https://en.wikipedia.org/wiki/Tagalog_language>Tagalog (Tl)</a>, <a href=https://en.wikipedia.org/wiki/Tamil_language>Tamil (Ta)</a> and <a href=https://en.wikipedia.org/wiki/English_language>English (En)</a> with 30 directions under the constrained condition. We use Transformer architecture and obtain the best performance via multiple variants with larger parameter sizes. We train a single multilingual model to translate all the 30 directions. We perform detailed pre-processing and filtering on the provided large-scale bilingual and monolingual datasets. Several commonly used strategies are used to train our models, such as <a href=https://en.wikipedia.org/wiki/Back_translation>Back Translation</a>, Forward Translation, Ensemble Knowledge Distillation, Adapter Fine-tuning. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> obtains competitive results in the end.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wmt-1.92.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wmt-1--92 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wmt-1.92 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wmt-1.92/>HW-TSC’s Participation at WMT 2021 Quality Estimation Shared Task<span class=acl-fixed-case>HW</span>-<span class=acl-fixed-case>TSC</span>’s Participation at <span class=acl-fixed-case>WMT</span> 2021 Quality Estimation Shared Task</a></strong><br><a href=/people/y/yimeng-chen/>Yimeng Chen</a>
|
<a href=/people/c/chang-su/>Chang Su</a>
|
<a href=/people/y/yingtao-zhang/>Yingtao Zhang</a>
|
<a href=/people/y/yuxia-wang/>Yuxia Wang</a>
|
<a href=/people/x/xiang-geng/>Xiang Geng</a>
|
<a href=/people/h/hao-yang/>Hao Yang</a>
|
<a href=/people/s/shimin-tao/>Shimin Tao</a>
|
<a href=/people/g/guo-jiaxin/>Guo Jiaxin</a>
|
<a href=/people/w/wang-minghan/>Wang Minghan</a>
|
<a href=/people/m/min-zhang/>Min Zhang</a>
|
<a href=/people/y/yujia-liu/>Yujia Liu</a>
|
<a href=/people/s/shujian-huang/>Shujian Huang</a><br><a href=/volumes/2021.wmt-1/ class=text-muted>Proceedings of the Sixth Conference on Machine Translation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wmt-1--92><div class="card-body p-3 small">This paper presents our work in WMT 2021 Quality Estimation (QE) Shared Task. We participated in all of the three sub-tasks, including Sentence-Level Direct Assessment (DA) task, Word and Sentence-Level Post-editing Effort task and Critical Error Detection task, in all language pairs. Our systems employ the framework of Predictor-Estimator, concretely with a pre-trained XLM-Roberta as Predictor and task-specific classifier or regressor as Estimator. For all tasks, we improve our systems by incorporating post-edit sentence or additional high-quality translation sentence in the way of <a href=https://en.wikipedia.org/wiki/Multitask_learning>multitask learning</a> or encoding it with predictors directly. Moreover, in zero-shot setting, our data augmentation strategy based on Monte-Carlo Dropout brings up significant improvement on DA sub-task. Notably, our submissions achieve remarkable results over all <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.blackboxnlp-1.14.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--blackboxnlp-1--14 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.blackboxnlp-1.14 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.blackboxnlp-1.14/>How Length Prediction Influence the Performance of Non-Autoregressive Translation?</a></strong><br><a href=/people/m/minghan-wang/>Minghan Wang</a>
|
<a href=/people/g/guo-jiaxin/>Guo Jiaxin</a>
|
<a href=/people/y/yuxia-wang/>Yuxia Wang</a>
|
<a href=/people/y/yimeng-chen/>Yimeng Chen</a>
|
<a href=/people/s/su-chang/>Su Chang</a>
|
<a href=/people/h/hengchao-shang/>Hengchao Shang</a>
|
<a href=/people/m/min-zhang/>Min Zhang</a>
|
<a href=/people/s/shimin-tao/>Shimin Tao</a>
|
<a href=/people/h/hao-yang/>Hao Yang</a><br><a href=/volumes/2021.blackboxnlp-1/ class=text-muted>Proceedings of the Fourth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--blackboxnlp-1--14><div class="card-body p-3 small">Length prediction is a special task in a series of NAT models where target length has to be determined before generation. However, the performance of length prediction and its influence on translation quality has seldom been discussed. In this paper, we present comprehensive analyses on length prediction task of NAT, aiming to find the factors that influence performance, as well as how it associates with translation quality. We mainly perform experiments based on Conditional Masked Language Model (CMLM) (Ghazvininejad et al., 2019), a representative NAT model, and evaluate it on two language pairs, En-De and En-Ro. We draw two conclusions : 1) The performance of length prediction is mainly influenced by properties of language pairs such as alignment pattern, <a href=https://en.wikipedia.org/wiki/Word_order>word order</a> or intrinsic length ratio, and is also affected by the usage of knowledge distilled data. 2) There is a positive correlation between the performance of the length prediction and the BLEU score.</div></div><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.eamt-1.4.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--eamt-1--4 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.eamt-1.4 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.eamt-1.4/>Efficient <a href=https://en.wikipedia.org/wiki/Transfer_learning>Transfer Learning</a> for Quality Estimation with Bottleneck Adapter Layer</a></strong><br><a href=/people/h/hao-yang/>Hao Yang</a>
|
<a href=/people/m/minghan-wang/>Minghan Wang</a>
|
<a href=/people/n/ning-xie/>Ning Xie</a>
|
<a href=/people/y/ying-qin/>Ying Qin</a>
|
<a href=/people/y/yao-deng/>Yao Deng</a><br><a href=/volumes/2020.eamt-1/ class=text-muted>Proceedings of the 22nd Annual Conference of the European Association for Machine Translation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--eamt-1--4><div class="card-body p-3 small">The Predictor-Estimator framework for quality estimation (QE) is commonly used for its strong performance. Where the predictor and <a href=https://en.wikipedia.org/wiki/Estimator>estimator</a> works on <a href=https://en.wikipedia.org/wiki/Feature_extraction>feature extraction</a> and <a href=https://en.wikipedia.org/wiki/Quality_assurance>quality evaluation</a>, respectively. However, training the <a href=https://en.wikipedia.org/wiki/Prediction>predictor</a> from scratch is computationally expensive. In this paper, we propose an efficient transfer learning framework to transfer knowledge from NMT dataset into <a href=https://en.wikipedia.org/wiki/Quantum_electrodynamics>QE models</a>. A Predictor-Estimator alike model named BAL-QE is also proposed, aiming to extract high quality features with pre-trained NMT model, and make classification with a fine-tuned Bottleneck Adapter Layer (BAL). The experiment shows that BAL-QE achieves 97 % of the SOTA performance in WMT19 En-De and En-Ru QE tasks by only training 3 % of parameters within 4 hours on 4 Titan XP GPUs. Compared with the commonly used NuQE baseline, BAL-QE achieves 47 % (En-Ru) and 75 % (En-De) of performance promotions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wat-1.8.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wat-1--8 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wat-1.8 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.wat-1.8/>HW-TSC’s Participation in the WAT 2020 Indic Languages Multilingual Task<span class=acl-fixed-case>HW</span>-<span class=acl-fixed-case>TSC</span>’s Participation in the <span class=acl-fixed-case>WAT</span> 2020 Indic Languages Multilingual Task</a></strong><br><a href=/people/z/zhengzhe-yu/>Zhengzhe Yu</a>
|
<a href=/people/z/zhanglin-wu/>Zhanglin Wu</a>
|
<a href=/people/x/xiaoyu-chen/>Xiaoyu Chen</a>
|
<a href=/people/d/daimeng-wei/>Daimeng Wei</a>
|
<a href=/people/h/hengchao-shang/>Hengchao Shang</a>
|
<a href=/people/j/jiaxin-guo/>Jiaxin Guo</a>
|
<a href=/people/z/zongyao-li/>Zongyao Li</a>
|
<a href=/people/m/minghan-wang/>Minghan Wang</a>
|
<a href=/people/l/liangyou-li/>Liangyou Li</a>
|
<a href=/people/l/lizhi-lei/>Lizhi Lei</a>
|
<a href=/people/h/hao-yang/>Hao Yang</a>
|
<a href=/people/y/ying-qin/>Ying Qin</a><br><a href=/volumes/2020.wat-1/ class=text-muted>Proceedings of the 7th Workshop on Asian Translation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wat-1--8><div class="card-body p-3 small">This paper describes our work in the WAT 2020 Indic Multilingual Translation Task. We participated in all 7 language pairs (En-Bn / Hi / Gu / Ml / Mr / Ta / Te) in both directions under the constrained conditionusing only the officially provided data. Using <a href=https://en.wikipedia.org/wiki/Transformer>transformer</a> as a baseline, our Multi-En and En-Multi translation systems achieve the best performances. Detailed data filtering and data domain selection are the keys to performance enhancement in our experiment, with an average improvement of 2.6 BLEU scores for each language pair in the En-Multi system and an average improvement of 4.6 BLEU scores regarding the Multi-En. In addition, we employed language independent adapter to further improve the <a href=https://en.wikipedia.org/wiki/System>system</a> performances. Our submission obtains competitive results in the final evaluation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wmt-1.31.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wmt-1--31 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wmt-1.31 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939573 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.wmt-1.31/>HW-TSC’s Participation in the WMT 2020 News Translation Shared Task<span class=acl-fixed-case>HW</span>-<span class=acl-fixed-case>TSC</span>’s Participation in the <span class=acl-fixed-case>WMT</span> 2020 News Translation Shared Task</a></strong><br><a href=/people/d/daimeng-wei/>Daimeng Wei</a>
|
<a href=/people/h/hengchao-shang/>Hengchao Shang</a>
|
<a href=/people/z/zhanglin-wu/>Zhanglin Wu</a>
|
<a href=/people/z/zhengzhe-yu/>Zhengzhe Yu</a>
|
<a href=/people/l/liangyou-li/>Liangyou Li</a>
|
<a href=/people/j/jiaxin-guo/>Jiaxin Guo</a>
|
<a href=/people/m/minghan-wang/>Minghan Wang</a>
|
<a href=/people/h/hao-yang/>Hao Yang</a>
|
<a href=/people/l/lizhi-lei/>Lizhi Lei</a>
|
<a href=/people/y/ying-qin/>Ying Qin</a>
|
<a href=/people/s/shiliang-sun/>Shiliang Sun</a><br><a href=/volumes/2020.wmt-1/ class=text-muted>Proceedings of the Fifth Conference on Machine Translation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wmt-1--31><div class="card-body p-3 small">This paper presents our work in the WMT 2020 News Translation Shared Task. We participate in 3 language pairs including Zh / En, Km / En, and Ps / En and in both directions under the constrained condition. We use the standard Transformer-Big model as the <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline</a> and obtain the best performance via two variants with larger parameter sizes. We perform detailed pre-processing and filtering on the provided large-scale bilingual and monolingual dataset. Several commonly used strategies are used to train our <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> such as <a href=https://en.wikipedia.org/wiki/Back_translation>Back Translation</a>, Ensemble Knowledge Distillation, etc. We also conduct experiment with similar language augmentation, which lead to positive results, although not used in our submission. Our submission obtains remarkable results in the final evaluation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.iwslt-1.23.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--iwslt-1--23 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.iwslt-1.23 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://slideslive.com/38929616 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.iwslt-1.23/>The HW-TSC Video Speech Translation System at IWSLT 2020<span class=acl-fixed-case>HW</span>-<span class=acl-fixed-case>TSC</span> Video Speech Translation System at <span class=acl-fixed-case>IWSLT</span> 2020</a></strong><br><a href=/people/m/minghan-wang/>Minghan Wang</a>
|
<a href=/people/h/hao-yang/>Hao Yang</a>
|
<a href=/people/y/yao-deng/>Yao Deng</a>
|
<a href=/people/y/ying-qin/>Ying Qin</a>
|
<a href=/people/l/lizhi-lei/>Lizhi Lei</a>
|
<a href=/people/d/daimeng-wei/>Daimeng Wei</a>
|
<a href=/people/h/hengchao-shang/>Hengchao Shang</a>
|
<a href=/people/n/ning-xie/>Ning Xie</a>
|
<a href=/people/x/xiaochun-li/>Xiaochun Li</a>
|
<a href=/people/j/jiaxian-guo/>Jiaxian Guo</a><br><a href=/volumes/2020.iwslt-1/ class=text-muted>Proceedings of the 17th International Conference on Spoken Language Translation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--iwslt-1--23><div class="card-body p-3 small">The paper presents details of our <a href=https://en.wikipedia.org/wiki/System>system</a> in the IWSLT Video Speech Translation evaluation. The <a href=https://en.wikipedia.org/wiki/System>system</a> works in a cascade form, which contains three <a href=https://en.wikipedia.org/wiki/Modular_programming>modules</a> : 1) A proprietary ASR system. 2) A disfluency correction system aims to remove interregnums or other disfluent expressions with a fine-tuned BERT and a series of rule-based algorithms. 3) An NMT System based on the Transformer and trained with massive publicly available corpus.</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5523.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5523 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5523 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5523/>An End-to-End Multi-task Learning Model for <a href=https://en.wikipedia.org/wiki/Fact-checking>Fact Checking</a></a></strong><br><a href=/people/s/sizhen-li/>Sizhen Li</a>
|
<a href=/people/s/shuai-zhao/>Shuai Zhao</a>
|
<a href=/people/b/bo-cheng/>Bo Cheng</a>
|
<a href=/people/h/hao-yang/>Hao Yang</a><br><a href=/volumes/W18-55/ class=text-muted>Proceedings of the First Workshop on Fact Extraction and VERification (FEVER)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5523><div class="card-body p-3 small">With huge amount of information generated every day on the web, <a href=https://en.wikipedia.org/wiki/Fact-checking>fact checking</a> is an important and challenging task which can help people identify the authenticity of most claims as well as providing evidences selected from knowledge source like <a href=https://en.wikipedia.org/wiki/Wikipedia>Wikipedia</a>. Here we decompose this problem into two parts : an entity linking task (retrieving relative Wikipedia pages) and recognizing textual entailment between the claim and selected pages. In this paper, we present an end-to-end multi-task learning with bi-direction attention (EMBA) model to classify the claim as supports, refutes or not enough info with respect to the pages retrieved and detect sentences as evidence at the same time. We conduct experiments on the FEVER (Fact Extraction and VERification) paper test dataset and shared task test dataset, a new public dataset for verification against textual sources. Experimental results show that our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> achieves comparable performance compared with the <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline system</a>.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Hao+Yang" title="Search for 'Hao Yang' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/m/minghan-wang/ class=align-middle>Minghan Wang</a>
<span class="badge badge-secondary align-middle ml-2">9</span></li><li class=list-group-item><a href=/people/y/ying-qin/ class=align-middle>Ying Qin</a>
<span class="badge badge-secondary align-middle ml-2">7</span></li><li class=list-group-item><a href=/people/h/hengchao-shang/ class=align-middle>Hengchao Shang</a>
<span class="badge badge-secondary align-middle ml-2">7</span></li><li class=list-group-item><a href=/people/j/jiaxin-guo/ class=align-middle>Jiaxin Guo</a>
<span class="badge badge-secondary align-middle ml-2">6</span></li><li class=list-group-item><a href=/people/m/min-zhang/ class=align-middle>Min Zhang</a>
<span class="badge badge-secondary align-middle ml-2">6</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/d/daimeng-wei/ class=align-middle>Daimeng Wei</a>
<span class="badge badge-secondary align-middle ml-2">6</span></li><li class=list-group-item><a href=/people/l/lizhi-lei/ class=align-middle>Lizhi Lei</a>
<span class="badge badge-secondary align-middle ml-2">6</span></li><li class=list-group-item><a href=/people/z/zhanglin-wu/ class=align-middle>Zhanglin Wu</a>
<span class="badge badge-secondary align-middle ml-2">5</span></li><li class=list-group-item><a href=/people/z/zhengzhe-yu/ class=align-middle>Zhengzhe Yu</a>
<span class="badge badge-secondary align-middle ml-2">5</span></li><li class=list-group-item><a href=/people/z/zongyao-li/ class=align-middle>Zongyao Li</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/people/x/xiaoyu-chen/ class=align-middle>Xiaoyu Chen</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/people/y/yuxia-wang/ class=align-middle>Yuxia Wang</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/y/yimeng-chen/ class=align-middle>Yimeng Chen</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/s/shimin-tao/ class=align-middle>Shimin Tao</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/n/ning-xie/ class=align-middle>Ning Xie</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/y/yao-deng/ class=align-middle>Yao Deng</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/c/chang-su/ class=align-middle>Chang Su</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/g/guo-jiaxin/ class=align-middle>Guo Jiaxin</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/l/liangyou-li/ class=align-middle>Liangyou Li</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/y/yang-wu/ class=align-middle>Yang Wu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yanyan-zhao/ class=align-middle>Yanyan Zhao</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/song-chen/ class=align-middle>Song Chen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/b/bing-qin/ class=align-middle>Bing Qin</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/x/xiaohuan-cao/ class=align-middle>Xiaohuan Cao</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/w/wenting-zhao/ class=align-middle>Wenting Zhao</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yingtao-zhang/ class=align-middle>Yingtao Zhang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/x/xiang-geng/ class=align-middle>Xiang Geng</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/w/wang-minghan/ class=align-middle>Wang Minghan</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yujia-liu/ class=align-middle>Yujia Liu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/shujian-huang/ class=align-middle>Shujian Huang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/su-chang/ class=align-middle>Su Chang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/sizhen-li/ class=align-middle>Sizhen Li</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/shuai-zhao/ class=align-middle>Shuai Zhao</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/b/bo-cheng/ class=align-middle>Bo Cheng</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/shiliang-sun/ class=align-middle>Shiliang Sun</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/x/xiaochun-li/ class=align-middle>Xiaochun Li</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jiaxian-guo/ class=align-middle>Jiaxian Guo</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/wmt/ class=align-middle>WMT</a><span class="badge badge-secondary align-middle ml-2">5</span></li><li class=list-group-item><a href=/venues/findings/ class=align-middle>Findings</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/eamt/ class=align-middle>EAMT</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/blackboxnlp/ class=align-middle>BlackboxNLP</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/ws/ class=align-middle>WS</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-venues aria-expanded=false aria-controls=more-venues>show all...</li><div class="collapse border-top" id=more-venues><li class=list-group-item><a href=/venues/wat/ class=align-middle>WAT</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/iwslt/ class=align-middle>IWSLT</a><span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>