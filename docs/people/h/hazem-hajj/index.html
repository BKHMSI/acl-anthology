<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Hazem Hajj - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Hazem</span> <span class=font-weight-bold>Hajj</span></h2><hr><div class=row><div class=col-lg-9><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wanlp-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wanlp-1.0/>Proceedings of the Sixth Arabic Natural Language Processing Workshop</a></strong><br><a href=/people/n/nizar-habash/>Nizar Habash</a>
|
<a href=/people/h/houda-bouamor/>Houda Bouamor</a>
|
<a href=/people/h/hazem-hajj/>Hazem Hajj</a>
|
<a href=/people/w/walid-magdy/>Walid Magdy</a>
|
<a href=/people/w/wajdi-zaghouani/>Wajdi Zaghouani</a>
|
<a href=/people/f/fethi-bougares/>Fethi Bougares</a>
|
<a href=/people/n/nadi-tomeh/>Nadi Tomeh</a>
|
<a href=/people/i/ibrahim-abu-farha/>Ibrahim Abu Farha</a>
|
<a href=/people/s/samia-touileb/>Samia Touileb</a><br><a href=/volumes/2021.wanlp-1/ class=text-muted>Proceedings of the Sixth Arabic Natural Language Processing Workshop</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wanlp-1.20.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wanlp-1--20 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wanlp-1.20 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.wanlp-1.20" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.wanlp-1.20/>AraELECTRA : Pre-Training Text Discriminators for Arabic Language Understanding<span class=acl-fixed-case>A</span>ra<span class=acl-fixed-case>ELECTRA</span>: Pre-Training Text Discriminators for <span class=acl-fixed-case>A</span>rabic Language Understanding</a></strong><br><a href=/people/w/wissam-antoun/>Wissam Antoun</a>
|
<a href=/people/f/fady-baly/>Fady Baly</a>
|
<a href=/people/h/hazem-hajj/>Hazem Hajj</a><br><a href=/volumes/2021.wanlp-1/ class=text-muted>Proceedings of the Sixth Arabic Natural Language Processing Workshop</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wanlp-1--20><div class="card-body p-3 small">Advances in English language representation enabled a more sample-efficient pre-training task by Efficiently Learning an Encoder that Classifies Token Replacements Accurately (ELECTRA). Which, instead of training a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> to recover masked tokens, it trains a discriminator model to distinguish true input tokens from corrupted tokens that were replaced by a generator network. On the other hand, current Arabic language representation approaches rely only on pretraining via masked language modeling. In this paper, we develop an Arabic language representation model, which we name AraELECTRA. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is pretrained using the replaced token detection objective on large Arabic text corpora. We evaluate our model on multiple Arabic NLP tasks, including <a href=https://en.wikipedia.org/wiki/Reading_comprehension>reading comprehension</a>, <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a>, and <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>named-entity recognition</a> and we show that AraELECTRA outperforms current state-of-the-art Arabic language representation models, given the same pretraining data and with even a smaller model size.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wanlp-1.21.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wanlp-1--21 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wanlp-1.21 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.wanlp-1.21" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.wanlp-1.21/>AraGPT2 : Pre-Trained Transformer for Arabic Language Generation<span class=acl-fixed-case>A</span>ra<span class=acl-fixed-case>GPT</span>2: Pre-Trained Transformer for <span class=acl-fixed-case>A</span>rabic Language Generation</a></strong><br><a href=/people/w/wissam-antoun/>Wissam Antoun</a>
|
<a href=/people/f/fady-baly/>Fady Baly</a>
|
<a href=/people/h/hazem-hajj/>Hazem Hajj</a><br><a href=/volumes/2021.wanlp-1/ class=text-muted>Proceedings of the Sixth Arabic Natural Language Processing Workshop</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wanlp-1--21><div class="card-body p-3 small">Recently, pre-trained transformer-based architectures have proven to be very efficient at language modeling and understanding, given that they are trained on a large enough corpus. Applications in <a href=https://en.wikipedia.org/wiki/Language_generation>language generation</a> for <a href=https://en.wikipedia.org/wiki/Arabic>Arabic</a> are still lagging in comparison to other NLP advances primarily due to the lack of advanced Arabic language generation models. In this paper, we develop the first advanced Arabic language generation model, AraGPT2, trained from scratch on a large Arabic corpus of internet text and news articles. Our largest <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>, AraGPT2-mega, has 1.46 billion parameters, which makes it the largest Arabic language model available. The mega model was evaluated and showed success on different tasks including synthetic news generation, and zero-shot question answering. For text generation, our best <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieves a <a href=https://en.wikipedia.org/wiki/Perplexity>perplexity</a> of 29.8 on <a href=https://en.wikipedia.org/wiki/Wikipedia>held-out Wikipedia articles</a>. A study conducted with human evaluators showed the significant success of AraGPT2-mega in generating <a href=https://en.wikipedia.org/wiki/Article_(publishing)>news articles</a> that are difficult to distinguish from articles written by humans. We thus develop and release an automatic discriminator model with a 98 % percent <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> in detecting model-generated text. The <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> are also publicly available, hoping to encourage new research directions and applications for Arabic NLP.</div></div><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.osact-1.2.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--osact-1--2 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.osact-1.2 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.osact-1.2" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.osact-1.2/>AraBERT : Transformer-based Model for Arabic Language Understanding<span class=acl-fixed-case>A</span>ra<span class=acl-fixed-case>BERT</span>: Transformer-based Model for <span class=acl-fixed-case>A</span>rabic Language Understanding</a></strong><br><a href=/people/w/wissam-antoun/>Wissam Antoun</a>
|
<a href=/people/f/fady-baly/>Fady Baly</a>
|
<a href=/people/h/hazem-hajj/>Hazem Hajj</a><br><a href=/volumes/2020.osact-1/ class=text-muted>Proceedings of the 4th Workshop on Open-Source Arabic Corpora and Processing Tools, with a Shared Task on Offensive Language Detection</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--osact-1--2><div class="card-body p-3 small">The <a href=https://en.wikipedia.org/wiki/Arabic>Arabic language</a> is a morphologically rich language with relatively few resources and a less explored syntax compared to <a href=https://en.wikipedia.org/wiki/English_language>English</a>. Given these limitations, Arabic Natural Language Processing (NLP) tasks like Sentiment Analysis (SA), Named Entity Recognition (NER), and Question Answering (QA), have proven to be very challenging to tackle. Recently, with the surge of transformers based models, language-specific BERT based models have proven to be very efficient at <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>language understanding</a>, provided they are pre-trained on a very large corpus. Such <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> were able to set new standards and achieve state-of-the-art results for most NLP tasks. In this paper, we pre-trained BERT specifically for the <a href=https://en.wikipedia.org/wiki/Arabic>Arabic language</a> in the pursuit of achieving the same success that BERT did for the <a href=https://en.wikipedia.org/wiki/English_language>English language</a>. The performance of AraBERT is compared to multilingual BERT from <a href=https://en.wikipedia.org/wiki/Google>Google</a> and other state-of-the-art approaches. The results showed that the newly developed AraBERT achieved state-of-the-art performance on most tested Arabic NLP tasks. The pretrained araBERT models are publicly available on https://github.com/aub-mind/araBERT hoping to encourage research and applications for Arabic NLP.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.osact-1.16.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--osact-1--16 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.osact-1.16 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.osact-1.16/>Multi-Task Learning using AraBert for Offensive Language Detection<span class=acl-fixed-case>A</span>ra<span class=acl-fixed-case>B</span>ert for Offensive Language Detection</a></strong><br><a href=/people/m/marc-djandji/>Marc Djandji</a>
|
<a href=/people/f/fady-baly/>Fady Baly</a>
|
<a href=/people/w/wissam-antoun/>Wissam Antoun</a>
|
<a href=/people/h/hazem-hajj/>Hazem Hajj</a><br><a href=/volumes/2020.osact-1/ class=text-muted>Proceedings of the 4th Workshop on Open-Source Arabic Corpora and Processing Tools, with a Shared Task on Offensive Language Detection</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--osact-1--16><div class="card-body p-3 small">The use of <a href=https://en.wikipedia.org/wiki/Social_media>social media platforms</a> has become more prevalent, which has provided tremendous opportunities for people to connect but has also opened the door for misuse with the spread of <a href=https://en.wikipedia.org/wiki/Hate_speech>hate speech</a> and <a href=https://en.wikipedia.org/wiki/Profanity>offensive language</a>. This phenomenon has been driving more and more people to more extreme reactions and online aggression, sometimes causing physical harm to individuals or groups of people. There is a need to control and prevent such misuse of <a href=https://en.wikipedia.org/wiki/Social_media>online social media</a> through automatic detection of profane language. The shared task on Offensive Language Detection at the OSACT4 has aimed at achieving state of art profane language detection methods for Arabic social media. Our team BERTologists tackled this problem by leveraging state of the art pretrained Arabic language model, AraBERT, that we augment with the addition of <a href=https://en.wikipedia.org/wiki/Multi-task_learning>Multi-task learning</a> to enable our model to learn efficiently from little data. Our Multitask AraBERT approach achieved the second place in both subtasks A & B, which shows that the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> performs consistently across different tasks.</div></div><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/R19-1138.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-R19-1138 data-toggle=collapse aria-expanded=false aria-controls=abstract-R19-1138 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/R19-1138/>SenZi : A Sentiment Analysis Lexicon for the Latinised Arabic (Arabizi)<span class=acl-fixed-case>S</span>en<span class=acl-fixed-case>Z</span>i: A Sentiment Analysis Lexicon for the Latinised <span class=acl-fixed-case>A</span>rabic (<span class=acl-fixed-case>A</span>rabizi)</a></strong><br><a href=/people/t/taha-tobaili/>Taha Tobaili</a>
|
<a href=/people/m/miriam-fernandez/>Miriam Fernandez</a>
|
<a href=/people/h/harith-alani/>Harith Alani</a>
|
<a href=/people/s/sanaa-sharafeddine/>Sanaa Sharafeddine</a>
|
<a href=/people/h/hazem-hajj/>Hazem Hajj</a>
|
<a href=/people/g/goran-glavas/>Goran Glavaš</a><br><a href=/volumes/R19-1/ class=text-muted>Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2019)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-R19-1138><div class="card-body p-3 small">Arabizi is an informal written form of <a href=https://en.wikipedia.org/wiki/Varieties_of_Arabic>dialectal Arabic</a> transcribed in <a href=https://en.wikipedia.org/wiki/Latin_script>Latin alphanumeric characters</a>. It has a proven popularity on chat platforms and <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>, yet <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> suffers from a severe lack of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing (NLP) resources</a>. As such, texts written in <a href=https://en.wikipedia.org/wiki/Arabizi>Arabizi</a> are often disregarded in sentiment analysis tasks for <a href=https://en.wikipedia.org/wiki/Arabic>Arabic</a>. In this paper we describe the creation of a sentiment lexicon for <a href=https://en.wikipedia.org/wiki/Arabizi>Arabizi</a> that was enriched with <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a>. The result is a new Arabizi lexicon consisting of 11.3 K positive and 13.3 K negative words. We evaluated this <a href=https://en.wikipedia.org/wiki/Lexicon>lexicon</a> by classifying the sentiment of Arabizi tweets achieving an F1-score of 0.72. We provide a detailed error analysis to present the challenges that impact the <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> of <a href=https://en.wikipedia.org/wiki/Arabizi>Arabizi</a>.</div></div><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1308.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1308 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1308 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1308/>CAT : Credibility Analysis of Arabic Content on Twitter<span class=acl-fixed-case>CAT</span>: Credibility Analysis of <span class=acl-fixed-case>A</span>rabic Content on <span class=acl-fixed-case>T</span>witter</a></strong><br><a href=/people/r/rim-el-ballouli/>Rim El Ballouli</a>
|
<a href=/people/w/wassim-el-hajj/>Wassim El-Hajj</a>
|
<a href=/people/a/ahmad-ghandour/>Ahmad Ghandour</a>
|
<a href=/people/s/shady-elbassuoni/>Shady Elbassuoni</a>
|
<a href=/people/h/hazem-hajj/>Hazem Hajj</a>
|
<a href=/people/k/khaled-shaban/>Khaled Shaban</a><br><a href=/volumes/W17-13/ class=text-muted>Proceedings of the Third Arabic Natural Language Processing Workshop</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1308><div class="card-body p-3 small">Data generated on <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a> has become a rich source for various data mining tasks. Those data analysis tasks that are dependent on the tweet semantics, such as <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a>, emotion mining, and rumor detection among others, suffer considerably if the tweet is not credible, not real, or spam. In this paper, we perform an extensive analysis on credibility of Arabic content on <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a>. We also build a classification model (CAT) to automatically predict the <a href=https://en.wikipedia.org/wiki/Credibility>credibility</a> of a given Arabic tweet. Of particular originality is the inclusion of <a href=https://en.wikipedia.org/wiki/Software_feature>features</a> extracted directly or indirectly from the author&#8217;s profile and timeline. To train and test CAT, we annotated for <a href=https://en.wikipedia.org/wiki/Credibility>credibility</a> a <a href=https://en.wikipedia.org/wiki/Data_set>data set</a> of 9,000 <a href=https://en.wikipedia.org/wiki/Twitter>Arabic tweets</a> that are topic independent. CAT achieved consistent improvements in predicting the credibility of the tweets when compared to several baselines and when compared to the state-of-the-art approach with an improvement of 21 % in weighted average F-measure. We also conducted experiments to highlight the importance of the user-based features as opposed to the content-based features. We conclude our work with a feature reduction experiment that highlights the best indicative features of credibility.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1314.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1314 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1314 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1314/>A Characterization Study of Arabic Twitter Data with a Benchmarking for State-of-the-Art Opinion Mining Models<span class=acl-fixed-case>A</span>rabic <span class=acl-fixed-case>T</span>witter Data with a Benchmarking for State-of-the-Art Opinion Mining Models</a></strong><br><a href=/people/r/ramy-baly/>Ramy Baly</a>
|
<a href=/people/g/gilbert-badaro/>Gilbert Badaro</a>
|
<a href=/people/g/georges-el-khoury/>Georges El-Khoury</a>
|
<a href=/people/r/rawan-moukalled/>Rawan Moukalled</a>
|
<a href=/people/r/rita-aoun/>Rita Aoun</a>
|
<a href=/people/h/hazem-hajj/>Hazem Hajj</a>
|
<a href=/people/w/wassim-el-hajj/>Wassim El-Hajj</a>
|
<a href=/people/n/nizar-habash/>Nizar Habash</a>
|
<a href=/people/k/khaled-shaban/>Khaled Shaban</a><br><a href=/volumes/W17-13/ class=text-muted>Proceedings of the Third Arabic Natural Language Processing Workshop</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1314><div class="card-body p-3 small">Opinion mining in <a href=https://en.wikipedia.org/wiki/Arabic>Arabic</a> is a challenging task given the <a href=https://en.wikipedia.org/wiki/Varieties_of_Arabic>rich morphology of the language</a>. The task becomes more challenging when it is applied to Twitter data, which contains additional sources of noise, such as the use of unstandardized dialectal variations, the nonconformation to grammatical rules, the use of Arabizi and code-switching, and the use of non-text objects such as images and <a href=https://en.wikipedia.org/wiki/URL>URLs</a> to express opinion. In this paper, we perform an analytical study to observe how such linguistic phenomena vary across different <a href=https://en.wikipedia.org/wiki/Arab_world>Arab regions</a>. This study of Arabic Twitter characterization aims at providing better understanding of Arabic Tweets, and fostering advanced research on the topic. Furthermore, we explore the performance of the two schools of <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning</a> on Arabic Twitter, namely the <a href=https://en.wikipedia.org/wiki/Feature_engineering>feature engineering approach</a> and the <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning approach</a>. We consider <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> that have achieved state-of-the-art performance for opinion mining in English. Results highlight the advantages of using <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning-based models</a>, and confirm the importance of using <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphological abstractions</a> to address <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>Arabic&#8217;s complex morphology</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S17-2099.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S17-2099 data-toggle=collapse aria-expanded=false aria-controls=abstract-S17-2099 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/S17-2099/>OMAM at SemEval-2017 Task 4 : Evaluation of English State-of-the-Art Sentiment Analysis Models for Arabic and a New Topic-based Model<span class=acl-fixed-case>OMAM</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2017 Task 4: Evaluation of <span class=acl-fixed-case>E</span>nglish State-of-the-Art Sentiment Analysis Models for <span class=acl-fixed-case>A</span>rabic and a New Topic-based Model</a></strong><br><a href=/people/r/ramy-baly/>Ramy Baly</a>
|
<a href=/people/g/gilbert-badaro/>Gilbert Badaro</a>
|
<a href=/people/a/ali-hamdi/>Ali Hamdi</a>
|
<a href=/people/r/rawan-moukalled/>Rawan Moukalled</a>
|
<a href=/people/r/rita-aoun/>Rita Aoun</a>
|
<a href=/people/g/georges-el-khoury/>Georges El-Khoury</a>
|
<a href=/people/a/ahmad-al-sallab/>Ahmad Al Sallab</a>
|
<a href=/people/h/hazem-hajj/>Hazem Hajj</a>
|
<a href=/people/n/nizar-habash/>Nizar Habash</a>
|
<a href=/people/k/khaled-shaban/>Khaled Shaban</a>
|
<a href=/people/w/wassim-el-hajj/>Wassim El-Hajj</a><br><a href=/volumes/S17-2/ class=text-muted>Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S17-2099><div class="card-body p-3 small">While <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> in <a href=https://en.wikipedia.org/wiki/English_language>English</a> has achieved significant progress, it remains a challenging task in <a href=https://en.wikipedia.org/wiki/Arabic>Arabic</a> given the rich morphology of the language. It becomes more challenging when applied to Twitter data that comes with additional sources of noise including <a href=https://en.wikipedia.org/wiki/Dialect>dialects</a>, misspellings, grammatical mistakes, <a href=https://en.wikipedia.org/wiki/Code-switching>code switching</a> and the use of non-textual objects to express sentiments. This paper describes the OMAM systems that we developed as part of SemEval-2017 task 4. We evaluate English state-of-the-art methods on <a href=https://en.wikipedia.org/wiki/Twitter>Arabic tweets</a> for subtask A. As for the remaining subtasks, we introduce a topic-based approach that accounts for topic specificities by predicting topics or domains of upcoming tweets, and then using this <a href=https://en.wikipedia.org/wiki/Information>information</a> to predict their <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment</a>. Results indicate that applying the English state-of-the-art method to <a href=https://en.wikipedia.org/wiki/Arabic>Arabic</a> has achieved solid results without significant enhancements. Furthermore, the topic-based method ranked 1st in subtasks C and E, and 2nd in subtask D.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Hazem+Hajj" title="Search for 'Hazem Hajj' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/w/wissam-antoun/ class=align-middle>Wissam Antoun</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/people/f/fady-baly/ class=align-middle>Fady Baly</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/people/w/wassim-el-hajj/ class=align-middle>Wassim El-Hajj</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/k/khaled-shaban/ class=align-middle>Khaled Shaban</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/n/nizar-habash/ class=align-middle>Nizar Habash</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/r/ramy-baly/ class=align-middle>Ramy Baly</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/g/gilbert-badaro/ class=align-middle>Gilbert Badaro</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/g/georges-el-khoury/ class=align-middle>Georges El-Khoury</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/r/rawan-moukalled/ class=align-middle>Rawan Moukalled</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/r/rita-aoun/ class=align-middle>Rita Aoun</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/r/rim-el-ballouli/ class=align-middle>Rim El Ballouli</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/ahmad-ghandour/ class=align-middle>Ahmad Ghandour</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/shady-elbassuoni/ class=align-middle>Shady Elbassuoni</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/marc-djandji/ class=align-middle>Marc Djandji</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/ali-hamdi/ class=align-middle>Ali Hamdi</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/ahmad-al-sallab/ class=align-middle>Ahmad Al Sallab</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/taha-tobaili/ class=align-middle>Taha Tobaili</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/miriam-fernandez/ class=align-middle>Miriam Fernandez</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/harith-alani/ class=align-middle>Harith Alani</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/sanaa-sharafeddine/ class=align-middle>Sanaa Sharafeddine</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/g/goran-glavas/ class=align-middle>Goran Glavaš</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/houda-bouamor/ class=align-middle>Houda Bouamor</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/w/walid-magdy/ class=align-middle>Walid Magdy</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/w/wajdi-zaghouani/ class=align-middle>Wajdi Zaghouani</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/f/fethi-bougares/ class=align-middle>Fethi Bougares</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/n/nadi-tomeh/ class=align-middle>Nadi Tomeh</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/i/ibrahim-abu-farha/ class=align-middle>Ibrahim Abu Farha</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/samia-touileb/ class=align-middle>Samia Touileb</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/wanlp/ class=align-middle>WANLP</a><span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/venues/ws/ class=align-middle>WS</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/osact/ class=align-middle>OSACT</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/semeval/ class=align-middle>SemEval</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/ranlp/ class=align-middle>RANLP</a><span class="badge badge-secondary align-middle ml-2">1</span></li></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>