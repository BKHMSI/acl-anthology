<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Hinrich Schütze - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Hinrich</span> <span class=font-weight-bold>Schütze</span></h2><p class="font-weight-light text-muted"><span class=font-italic>Also published as:</span>
Hinrich <span class=font-weight-normal>Schuetze</span></p><hr><div class=row><div class=col-lg-9><h4>2022</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-acl.108.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-acl--108 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-acl.108 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.findings-acl.108/>Graph Neural Networks for Multiparallel Word Alignment</a></strong><br><a href=/people/a/ayyoob-imani/>Ayyoob Imani</a>
|
<a href=/people/l/lutfi-kerem-senel/>Lütfi Kerem Senel</a>
|
<a href=/people/m/masoud-jalili-sabet/>Masoud Jalili Sabet</a>
|
<a href=/people/f/francois-yvon/>François Yvon</a>
|
<a href=/people/h/hinrich-schutze/>Hinrich Schuetze</a><br><a href=/volumes/2022.findings-acl/ class=text-muted>Findings of the Association for Computational Linguistics: ACL 2022</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-acl--108><div class="card-body p-3 small">After a period of decrease interest in word alignments is increasing again for their usefulness in domains such as typological research cross lingual annotation projection and <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> Generally alignment algorithms only use <a href=https://en.wikipedia.org/wiki/Bitext>bitext</a> and do not make use of the fact that many parallel corpora are multiparallel Here we compute high quality <a href=https://en.wikipedia.org/wiki/Word_alignment>word alignments</a> between multiple language pairs by considering all language pairs together First we create a multiparallel word alignment graph joining all bilingual word alignment pairs in one graph Next we use graph neural networks GNNs to exploit the <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph structure</a> Our GNN approach i utilizes information about the meaning position and language of the input words ii incorporates information from multiple parallel sentences iii adds and removes edges from the initial alignments and iv yields a prediction model that can generalize beyond the training sentences We show that community detection algorithms can provide valuable information for multiparallel word alignment Our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> outperforms previous work on three word alignment datasets and on a downstream task</div></div><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.nlp4if-1.1.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--nlp4if-1--1 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.nlp4if-1.1 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.nlp4if-1.1/>Identifying Automatically Generated Headlines using Transformers</a></strong><br><a href=/people/a/antonis-maronikolakis/>Antonis Maronikolakis</a>
|
<a href=/people/h/hinrich-schutze/>Hinrich Schütze</a>
|
<a href=/people/m/mark-stevenson/>Mark Stevenson</a><br><a href=/volumes/2021.nlp4if-1/ class=text-muted>Proceedings of the Fourth Workshop on NLP for Internet Freedom: Censorship, Disinformation, and Propaganda</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--nlp4if-1--1><div class="card-body p-3 small">False information spread via the <a href=https://en.wikipedia.org/wiki/Internet>internet</a> and <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> influences public opinion and user activity, while <a href=https://en.wikipedia.org/wiki/Generative_model>generative models</a> enable fake content to be generated faster and more cheaply than had previously been possible. In the not so distant future, identifying fake content generated by <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning models</a> will play a key role in protecting users from <a href=https://en.wikipedia.org/wiki/Misinformation>misinformation</a>. To this end, a <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> containing human and computer-generated headlines was created and a user study indicated that humans were only able to identify the <a href=https://en.wikipedia.org/wiki/Fake_news>fake headlines</a> in 47.8 % of the cases. However, the most accurate automatic approach, transformers, achieved an overall accuracy of 85.7 %, indicating that content generated from <a href=https://en.wikipedia.org/wiki/Language_model>language models</a> can be filtered out accurately.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.556.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--556 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.556 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.556" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.556/>Continuous Entailment Patterns for Lexical Inference in Context</a></strong><br><a href=/people/m/martin-schmitt/>Martin Schmitt</a>
|
<a href=/people/h/hinrich-schutze/>Hinrich Schütze</a><br><a href=/volumes/2021.emnlp-main/ class=text-muted>Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--556><div class="card-body p-3 small">Combining a pretrained language model (PLM) with textual patterns has been shown to help in both zero- and few-shot settings. For zero-shot performance, it makes sense to design <a href=https://en.wikipedia.org/wiki/Pattern>patterns</a> that closely resemble the text seen during self-supervised pretraining because the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> has never seen anything else. Supervised training allows for more flexibility. If we allow for tokens outside the PLM&#8217;s vocabulary, <a href=https://en.wikipedia.org/wiki/Pattern>patterns</a> can be adapted more flexibly to a PLM&#8217;s idiosyncrasies. Contrasting patterns where a token can be any continuous vector from those where a discrete choice between vocabulary elements has to be made, we call our method CONtinous pAtterNs (CONAN). We evaluate CONAN on two established benchmarks for lexical inference in context (LIiC) a.k.a. predicate entailment, a challenging natural language understanding task with relatively <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>small training data</a>. In a direct comparison with discrete patterns, <a href=https://en.wikipedia.org/wiki/CONAN>CONAN</a> consistently leads to improved performance, setting a new state of the art. Our experiments give valuable insights on the kind of <a href=https://en.wikipedia.org/wiki/Pattern>pattern</a> that enhances a PLM&#8217;s performance on LIiC and raise important questions regarding our understanding of PLMs using text patterns.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.697.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--697 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.697 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.697/>BeliefBank : Adding Memory to a Pre-Trained Language Model for a Systematic Notion of Belief<span class=acl-fixed-case>B</span>elief<span class=acl-fixed-case>B</span>ank: Adding Memory to a Pre-Trained Language Model for a Systematic Notion of Belief</a></strong><br><a href=/people/n/nora-kassner/>Nora Kassner</a>
|
<a href=/people/o/oyvind-tafjord/>Oyvind Tafjord</a>
|
<a href=/people/h/hinrich-schutze/>Hinrich Schütze</a>
|
<a href=/people/p/peter-clark/>Peter Clark</a><br><a href=/volumes/2021.emnlp-main/ class=text-muted>Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--697><div class="card-body p-3 small">Although pretrained language models (PTLMs) contain significant amounts of <a href=https://en.wikipedia.org/wiki/World_knowledge>world knowledge</a>, they can still produce inconsistent answers to questions when probed, even after specialized training. As a result, it can be hard to identify what the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> actually believes about the world, making it susceptible to <a href=https://en.wikipedia.org/wiki/Consistency>inconsistent behavior</a> and simple errors. Our goal is to reduce these problems. Our approach is to embed a PTLM in a broader system that also includes an evolving, symbolic memory of beliefs a BeliefBank that records but then may modify the raw PTLM answers. We describe two <a href=https://en.wikipedia.org/wiki/Mechanism_(sociology)>mechanisms</a> to improve belief consistency in the overall system. First, a reasoning component a weighted MaxSAT solver revises beliefs that significantly clash with others. Second, a feedback component issues future queries to the PTLM using known beliefs as context. We show that, in a controlled experimental setting, these two mechanisms result in more consistent beliefs in the overall system, improving both the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> and consistency of its answers over time. This is significant as it is a first step towards PTLM-based architectures with a systematic notion of belief, enabling them to construct a more coherent picture of the world, and improve over time without model retraining.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.internlp-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.internlp-1.0/>Proceedings of the First Workshop on Interactive Learning for Natural Language Processing</a></strong><br><a href=/people/k/kiante-brantley/>Kianté Brantley</a>
|
<a href=/people/s/soham-dan/>Soham Dan</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a>
|
<a href=/people/j/ji-ung-lee/>Ji-Ung Lee</a>
|
<a href=/people/f/filip-radlinski/>Filip Radlinski</a>
|
<a href=/people/h/hinrich-schutze/>Hinrich Schütze</a>
|
<a href=/people/e/edwin-simpson/>Edwin Simpson</a>
|
<a href=/people/l/lili-yu/>Lili Yu</a><br><a href=/volumes/2021.internlp-1/ class=text-muted>Proceedings of the First Workshop on Interactive Learning for Natural Language Processing</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.332.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--332 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.332 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.naacl-main.332" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.332/>Multi-source Neural Topic Modeling in Multi-view Embedding Spaces</a></strong><br><a href=/people/p/pankaj-gupta/>Pankaj Gupta</a>
|
<a href=/people/y/yatin-chaudhary/>Yatin Chaudhary</a>
|
<a href=/people/h/hinrich-schutze/>Hinrich Schütze</a><br><a href=/volumes/2021.naacl-main/ class=text-muted>Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--332><div class="card-body p-3 small">Though <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> and topics are complementary representations, several past works have only used pretrained word embeddings in (neural) topic modeling to address data sparsity in short-text or small collection of documents. This work presents a novel neural topic modeling framework using multi-view embed ding spaces : (1) pretrained topic-embeddings, and (2) pretrained word-embeddings (context-insensitive from Glove and context-sensitive from BERT models) jointly from one or many sources to improve topic quality and better deal with polysemy. In doing so, we first build respective pools of pretrained topic (i.e., TopicPool) and <a href=https://en.wikipedia.org/wiki/Word_processor_(electronic_device)>word embeddings</a> (i.e., WordPool). We then identify one or more relevant source domain(s) and <a href=https://en.wikipedia.org/wiki/Knowledge_transfer>transfer knowledge</a> to guide meaningful learning in the sparse target domain. Within neural topic modeling, we quantify the quality of topics and document representations via generalization (perplexity), interpretability (topic coherence) and information retrieval (IR) using short-text, long-text, small and large document collections from news and medical domains. Introducing the multi-source multi-view embedding spaces, we have shown state-of-the-art neural topic modeling using 6 source (high-resource) and 5 target (low-resource) corpora.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.insights-1.3.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--insights-1--3 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.insights-1.3 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.insights-1.3/>BERT Can not Align Characters<span class=acl-fixed-case>BERT</span> Cannot Align Characters</a></strong><br><a href=/people/a/antonis-maronikolakis/>Antonis Maronikolakis</a>
|
<a href=/people/p/philipp-dufter/>Philipp Dufter</a>
|
<a href=/people/h/hinrich-schutze/>Hinrich Schütze</a><br><a href=/volumes/2021.insights-1/ class=text-muted>Proceedings of the Second Workshop on Insights from Negative Results in NLP</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--insights-1--3><div class="card-body p-3 small">In previous work, it has been shown that BERT can adequately align cross-lingual sentences on the <a href=https://en.wikipedia.org/wiki/Syntax>word level</a>. Here we investigate whether BERT can also operate as a char-level aligner. The languages examined are <a href=https://en.wikipedia.org/wiki/English_language>English</a>, Fake <a href=https://en.wikipedia.org/wiki/English_language>English</a>, <a href=https://en.wikipedia.org/wiki/German_language>German</a> and <a href=https://en.wikipedia.org/wiki/Greek_language>Greek</a>. We show that the closer two languages are, the better BERT can align them on the character level. BERT indeed works well in <a href=https://en.wikipedia.org/wiki/English_language>English</a> to Fake English alignment, but this does not generalize to natural languages to the same extent. Nevertheless, the proximity of two languages does seem to be a factor. English is more related to <a href=https://en.wikipedia.org/wiki/German_language>German</a> than to <a href=https://en.wikipedia.org/wiki/Greek_language>Greek</a> and this is reflected in how well BERT aligns them ; <a href=https://en.wikipedia.org/wiki/English_language>English</a> to <a href=https://en.wikipedia.org/wiki/German_language>German</a> is better than <a href=https://en.wikipedia.org/wiki/English_language>English</a> to <a href=https://en.wikipedia.org/wiki/Greek_language>Greek</a>. We examine multiple setups and show that the <a href=https://en.wikipedia.org/wiki/Similarity_matrix>similarity matrices</a> for <a href=https://en.wikipedia.org/wiki/Natural_language>natural languages</a> show weaker relations the further apart two languages are.</div></div><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.316.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--316 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.316 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939116 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.316" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.316/>DagoBERT : Generating Derivational Morphology with a Pretrained Language Model<span class=acl-fixed-case>D</span>ago<span class=acl-fixed-case>BERT</span>: <span class=acl-fixed-case>G</span>enerating Derivational Morphology with a Pretrained Language Model</a></strong><br><a href=/people/v/valentin-hofmann/>Valentin Hofmann</a>
|
<a href=/people/j/janet-pierrehumbert/>Janet Pierrehumbert</a>
|
<a href=/people/h/hinrich-schutze/>Hinrich Schütze</a><br><a href=/volumes/2020.emnlp-main/ class=text-muted>Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--316><div class="card-body p-3 small">Can pretrained language models (PLMs) generate derivationally complex words? We present the first study investigating this question, taking BERT as the example PLM. We examine BERT&#8217;s derivational capabilities in different settings, ranging from using the unmodified pretrained model to full finetuning. Our best model, DagoBERT (Derivationally and generatively optimized BERT), clearly outperforms the previous state of the art in derivation generation (DG). Furthermore, our experiments show that the input segmentation crucially impacts BERT&#8217;s derivational knowledge, suggesting that the performance of PLMs could be further improved if a morphologically informed vocabulary of units were used.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.134.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--134 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.134 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.findings-emnlp.134.OptionalSupplementaryMaterial.pdf data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38940121 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.134" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.134/>Inexpensive Domain Adaptation of Pretrained Language Models : Case Studies on Biomedical NER and Covid-19 QA<span class=acl-fixed-case>NER</span> and Covid-19 <span class=acl-fixed-case>QA</span></a></strong><br><a href=/people/n/nina-poerner/>Nina Poerner</a>
|
<a href=/people/u/ulli-waltinger/>Ulli Waltinger</a>
|
<a href=/people/h/hinrich-schutze/>Hinrich Schütze</a><br><a href=/volumes/2020.findings-emnlp/ class=text-muted>Findings of the Association for Computational Linguistics: EMNLP 2020</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--134><div class="card-body p-3 small">Domain adaptation of Pretrained Language Models (PTLMs) is typically achieved by unsupervised pretraining on target-domain text. While successful, this approach is expensive in terms of <a href=https://en.wikipedia.org/wiki/Computer_hardware>hardware</a>, <a href=https://en.wikipedia.org/wiki/Run_time_(program_lifecycle_phase)>runtime</a> and <a href=https://en.wikipedia.org/wiki/Carbon_dioxide_in_Earth&#8217;s_atmosphere>CO 2 emissions</a>. Here, we propose a cheaper alternative : We train Word2Vec on target-domain text and align the resulting word vectors with the wordpiece vectors of a general-domain PTLM. We evaluate on eight English biomedical Named Entity Recognition (NER) tasks and compare against the recently proposed BioBERT model. We cover over 60 % of the BioBERT-BERT F1 delta, at 5 % of BioBERT&#8217;s CO 2 footprint and 2 % of its cloud compute cost. We also show how to quickly adapt an existing general-domain Question Answering (QA) model to an emerging domain : the Covid-19 pandemic.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.147.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--147 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.147 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38940631 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.147" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.147/>SimAlign : High Quality Word Alignments Without Parallel Training Data Using Static and Contextualized Embeddings<span class=acl-fixed-case>S</span>im<span class=acl-fixed-case>A</span>lign: High Quality Word Alignments Without Parallel Training Data Using Static and Contextualized Embeddings</a></strong><br><a href=/people/m/masoud-jalili-sabet/>Masoud Jalili Sabet</a>
|
<a href=/people/p/philipp-dufter/>Philipp Dufter</a>
|
<a href=/people/f/francois-yvon/>François Yvon</a>
|
<a href=/people/h/hinrich-schutze/>Hinrich Schütze</a><br><a href=/volumes/2020.findings-emnlp/ class=text-muted>Findings of the Association for Computational Linguistics: EMNLP 2020</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--147><div class="card-body p-3 small">Word alignments are useful for tasks like statistical and neural machine translation (NMT) and cross-lingual annotation projection. Statistical word aligners perform well, as do methods that extract alignments jointly with translations in NMT. However, most approaches require parallel training data and <a href=https://en.wikipedia.org/wiki/Data_quality>quality</a> decreases as less <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training data</a> is available. We propose word alignment methods that require no <a href=https://en.wikipedia.org/wiki/Parallel_computing>parallel data</a>. The key idea is to leverage multilingual word embeddings both static and contextualized for <a href=https://en.wikipedia.org/wiki/Word_alignment>word alignment</a>. Our multilingual embeddings are created from <a href=https://en.wikipedia.org/wiki/Monolingualism>monolingual data</a> only without relying on any parallel data or <a href=https://en.wikipedia.org/wiki/Dictionary>dictionaries</a>. We find that alignments created from <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a> are superior for four and comparable for two language pairs compared to those produced by traditional statistical aligners even with abundant parallel data ; e.g., contextualized embeddings achieve a word alignment F1 for English-German that is 5 percentage points higher than eflomal, a high-quality statistical aligner, trained on 100k parallel sentences.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.152.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--152 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.152 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38940122 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.152" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.152/>TopicBERT for Energy Efficient Document Classification<span class=acl-fixed-case>T</span>opic<span class=acl-fixed-case>BERT</span> for Energy Efficient Document Classification</a></strong><br><a href=/people/y/yatin-chaudhary/>Yatin Chaudhary</a>
|
<a href=/people/p/pankaj-gupta/>Pankaj Gupta</a>
|
<a href=/people/k/khushbu-saxena/>Khushbu Saxena</a>
|
<a href=/people/v/vivek-kulkarni/>Vivek Kulkarni</a>
|
<a href=/people/t/thomas-runkler/>Thomas Runkler</a>
|
<a href=/people/h/hinrich-schutze/>Hinrich Schütze</a><br><a href=/volumes/2020.findings-emnlp/ class=text-muted>Findings of the Association for Computational Linguistics: EMNLP 2020</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--152><div class="card-body p-3 small">Prior research notes that BERT&#8217;s computational cost grows quadratically with sequence length thus leading to longer <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training times</a>, higher GPU memory constraints and <a href=https://en.wikipedia.org/wiki/Greenhouse_gas>carbon emissions</a>. While recent work seeks to address these scalability issues at pre-training, these issues are also prominent in <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a> especially for long sequence tasks like <a href=https://en.wikipedia.org/wiki/Document_classification>document classification</a>. Our work thus focuses on optimizing the <a href=https://en.wikipedia.org/wiki/Computational_cost>computational cost</a> of <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a> for <a href=https://en.wikipedia.org/wiki/Document_classification>document classification</a>. We achieve this by complementary learning of both topic and language models in a unified framework, named TopicBERT. This significantly reduces the number of self-attention operations a main performance bottleneck. Consequently, our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieves a 1.4x (40 %) <a href=https://en.wikipedia.org/wiki/Speedup>speedup</a> with 40 % reduction in <a href=https://en.wikipedia.org/wiki/Carbon_dioxide_in_Earth&#8217;s_atmosphere>CO2 emission</a> while retaining 99.9 % performance over 5 datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.488.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--488 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.488 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.coling-main.488" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.488/>Automatically Identifying Words That Can Serve as Labels for Few-Shot Text Classification</a></strong><br><a href=/people/t/timo-schick/>Timo Schick</a>
|
<a href=/people/h/helmut-schmid/>Helmut Schmid</a>
|
<a href=/people/h/hinrich-schutze/>Hinrich Schütze</a><br><a href=/volumes/2020.coling-main/ class=text-muted>Proceedings of the 28th International Conference on Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--488><div class="card-body p-3 small">A recent approach for few-shot text classification is to convert textual inputs to cloze questions that contain some form of task description, process them with a pretrained language model and map the predicted words to labels. Manually defining this mapping between words and labels requires both domain expertise and an understanding of the <a href=https://en.wikipedia.org/wiki/Language_model>language model</a>&#8217;s abilities. To mitigate this issue, we devise an approach that automatically finds such a <a href=https://en.wikipedia.org/wiki/Map_(mathematics)>mapping</a> given small amounts of training data. For a number of tasks, the <a href=https://en.wikipedia.org/wiki/Map_(mathematics)>mapping</a> found by our approach performs almost as well as hand-crafted label-to-word mappings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.858.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--858 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.858 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.lrec-1.858" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.858/>ThaiLMCut : Unsupervised Pretraining for Thai Word Segmentation<span class=acl-fixed-case>T</span>hai<span class=acl-fixed-case>LMC</span>ut: Unsupervised Pretraining for <span class=acl-fixed-case>T</span>hai Word Segmentation</a></strong><br><a href=/people/s/suteera-seeha/>Suteera Seeha</a>
|
<a href=/people/i/ivan-bilan/>Ivan Bilan</a>
|
<a href=/people/l/liliana-mamani-sanchez/>Liliana Mamani Sanchez</a>
|
<a href=/people/j/johannes-huber/>Johannes Huber</a>
|
<a href=/people/m/michael-matuschek/>Michael Matuschek</a>
|
<a href=/people/h/hinrich-schutze/>Hinrich Schütze</a><br><a href=/volumes/2020.lrec-1/ class=text-muted>Proceedings of the 12th Language Resources and Evaluation Conference</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--858><div class="card-body p-3 small">We propose ThaiLMCut, a semi-supervised approach for Thai word segmentation which utilizes a bi-directional character language model (LM) as a way to leverage useful linguistic knowledge from unlabeled data. After the language model is trained on substantial unlabeled corpora, the weights of its embedding and recurrent layers are transferred to a supervised word segmentation model which continues fine-tuning them on a word segmentation task. Our experimental results demonstrate that applying the LM always leads to a performance gain, especially when the amount of labeled data is small. In such cases, the <a href=https://en.wikipedia.org/wiki/F1_Score>F1 Score</a> increased by up to 2.02 %. Even on abig labeled dataset, a small improvement gain can still be obtained. The approach has also shown to be very beneficial for out-of-domain settings with a gain in <a href=https://en.wikipedia.org/wiki/F1_score>F1 Score</a> of up to 3.13 %. Finally, we show that ThaiLMCut can outperform other open source state-of-the-art <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> achieving an F1 Score of 98.78 % on the standard <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmark</a>, InterBEST2009.</div></div><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1111.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1111 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1111 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1111.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1111" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1111/>Analytical Methods for Interpretable Ultradense Word Embeddings</a></strong><br><a href=/people/p/philipp-dufter/>Philipp Dufter</a>
|
<a href=/people/h/hinrich-schutze/>Hinrich Schütze</a><br><a href=/volumes/D19-1/ class=text-muted>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1111><div class="card-body p-3 small">Word embeddings are useful for a wide variety of <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a>, but they lack <a href=https://en.wikipedia.org/wiki/Interpretability>interpretability</a>. By rotating word spaces, interpretable dimensions can be identified while preserving the information contained in the <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a> without any loss. In this work, we investigate three methods for making word spaces interpretable by rotation : Densifier (Rothe et al., 2016), linear SVMs and DensRay, a new method we propose. In contrast to Densifier, DensRay can be computed in <a href=https://en.wikipedia.org/wiki/Closed-form_expression>closed form</a>, is hyperparameter-free and thus more robust than Densifier. We evaluate the three <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> on lexicon induction and set-based word analogy. In addition we provide qualitative insights as to how interpretable word spaces can be used for removing <a href=https://en.wikipedia.org/wiki/Gender_bias>gender bias</a> from <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1173.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1173 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1173 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1173.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D19-1173/>Multi-View Domain Adapted Sentence Embeddings for Low-Resource Unsupervised Duplicate Question Detection</a></strong><br><a href=/people/n/nina-poerner/>Nina Poerner</a>
|
<a href=/people/h/hinrich-schutze/>Hinrich Schütze</a><br><a href=/volumes/D19-1/ class=text-muted>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1173><div class="card-body p-3 small">We address the problem of Duplicate Question Detection (DQD) in low-resource domain-specific Community Question Answering forums. Our multi-view framework MV-DASE combines an ensemble of sentence encoders via Generalized Canonical Correlation Analysis, using unlabeled data only. In our experiments, the <a href=https://en.wikipedia.org/wiki/Statistical_ensemble_(mathematical_physics)>ensemble</a> includes generic and domain-specific averaged word embeddings, domain-finetuned BERT and the Universal Sentence Encoder. We evaluate MV-DASE on the CQADupStack corpus and on additional low-resource Stack Exchange forums. Combining the strengths of different encoders, we significantly outperform BM25, all single-view systems as well as a recent supervised domain-adversarial DQD method.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5720.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5720 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5720 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-5720" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-5720/>Linguistically Informed Relation Extraction and Neural Architectures for Nested Named Entity Recognition in BioNLP-OST 2019<span class=acl-fixed-case>B</span>io<span class=acl-fixed-case>NLP</span>-<span class=acl-fixed-case>OST</span> 2019</a></strong><br><a href=/people/p/pankaj-gupta/>Pankaj Gupta</a>
|
<a href=/people/u/usama-yaseen/>Usama Yaseen</a>
|
<a href=/people/h/hinrich-schutze/>Hinrich Schütze</a><br><a href=/volumes/D19-57/ class=text-muted>Proceedings of The 5th Workshop on BioNLP Open Shared Tasks</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5720><div class="card-body p-3 small">Named Entity Recognition (NER) and Relation Extraction (RE) are essential tools in distilling knowledge from biomedical literature. This paper presents our findings from participating in BioNLP Shared Tasks 2019. We addressed <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>Named Entity Recognition</a> including <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>nested entities extraction</a>, Entity Normalization and <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>Relation Extraction</a>. Our proposed approach of <a href=https://en.wikipedia.org/wiki/Named_entity>Named Entities</a> can be generalized to different languages and we have shown it&#8217;s effectiveness for English and Spanish text. We investigated linguistic features, hybrid loss including ranking and Conditional Random Fields (CRF), multi-task objective and token level ensembling strategy to improve NER. We employed dictionary based fuzzy and semantic search to perform Entity Normalization. Finally, our RE system employed Support Vector Machine (SVM) with <a href=https://en.wikipedia.org/wiki/Feature_(linguistics)>linguistic features</a>. Our NER submission (team : MIC-CIS) ranked first in BB-2019 norm+NER task with standard error rate (SER) of 0.7159 and showed competitive performance on PharmaCo NER task with F1-score of 0.8662. Our RE system ranked first in the SeeDev-binary Relation Extraction Task with F1-score of 0.3738.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1048.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1048 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1048 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N19-1048.Supplementary.pdf data-toggle=tooltip data-placement=top title=Supplementary><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1048" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N19-1048/>Attentive Mimicking : Better Word Embeddings by Attending to Informative Contexts</a></strong><br><a href=/people/t/timo-schick/>Timo Schick</a>
|
<a href=/people/h/hinrich-schutze/>Hinrich Schütze</a><br><a href=/volumes/N19-1/ class=text-muted>Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1048><div class="card-body p-3 small">Learning high-quality embeddings for rare words is a hard problem because of sparse context information. Mimicking (Pinter et al., 2017) has been proposed as a solution : given <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a> learned by a standard algorithm, a model is first trained to reproduce <a href=https://en.wikipedia.org/wiki/Embedding>embeddings of frequent words</a> from their surface form and then used to compute <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a> for rare words. In this paper, we introduce attentive mimicking : the mimicking model is given access not only to a word&#8217;s surface form, but also to all available contexts and learns to attend to the most informative and reliable contexts for computing an <a href=https://en.wikipedia.org/wiki/Embedding>embedding</a>. In an evaluation on four tasks, we show that attentive mimicking outperforms previous <a href=https://en.wikipedia.org/wiki/Work_(physics)>work</a> for both rare and medium-frequency words. Thus, compared to previous work, attentive mimicking improves <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a> for a much larger part of the vocabulary, including the <a href=https://en.wikipedia.org/wiki/Medium_frequency>medium-frequency range</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1341.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1341 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1341 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/384801834 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1341/>A Multilingual BPE Embedding Space for Universal Sentiment Lexicon Induction<span class=acl-fixed-case>BPE</span> Embedding Space for Universal Sentiment Lexicon Induction</a></strong><br><a href=/people/m/mengjie-zhao/>Mengjie Zhao</a>
|
<a href=/people/h/hinrich-schutze/>Hinrich Schütze</a><br><a href=/volumes/P19-1/ class=text-muted>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1341><div class="card-body p-3 small">We present a new method for sentiment lexicon induction that is designed to be applicable to the entire range of typological diversity of the world&#8217;s languages. We evaluate our method on Parallel Bible Corpus+ (PBC+), a parallel corpus of 1593 languages. The key idea is to use Byte Pair Encodings (BPEs) as basic units for multilingual embeddings. Through zero-shot transfer from English sentiment, we learn a seed lexicon for each language in the domain of PBC+. Through <a href=https://en.wikipedia.org/wiki/Domain_adaptation>domain adaptation</a>, we then generalize the domain-specific lexicon to a general one. We show across typologically diverse languages in PBC+ good quality of seed and general-domain sentiment lexicons by intrinsic and extrinsic and by automatic and human evaluation. We make freely available our code, seed sentiment lexicons for all 1593 languages and induced general-domain sentiment lexicons for 200 languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1574.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1574 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1574 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/385429181 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1574" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1574/>Probing for Semantic Classes : Diagnosing the Meaning Content of Word Embeddings</a></strong><br><a href=/people/y/yadollah-yaghoobzadeh/>Yadollah Yaghoobzadeh</a>
|
<a href=/people/k/katharina-kann/>Katharina Kann</a>
|
<a href=/people/t/timothy-j-hazen/>T. J. Hazen</a>
|
<a href=/people/e/eneko-agirre/>Eneko Agirre</a>
|
<a href=/people/h/hinrich-schutze/>Hinrich Schütze</a><br><a href=/volumes/P19-1/ class=text-muted>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1574><div class="card-body p-3 small">Word embeddings typically represent different meanings of a word in a single conflated vector. Empirical analysis of embeddings of ambiguous words is currently limited by the small size of manually annotated resources and by the fact that word senses are treated as unrelated individual concepts. We present a large dataset based on manual Wikipedia annotations and <a href=https://en.wikipedia.org/wiki/Word_sense>word senses</a>, where <a href=https://en.wikipedia.org/wiki/Word_sense>word senses</a> from different words are related by <a href=https://en.wikipedia.org/wiki/Semantic_class>semantic classes</a>. This is the basis for novel diagnostic tests for an embedding&#8217;s content : we probe word embeddings for <a href=https://en.wikipedia.org/wiki/Semantic_class>semantic classes</a> and analyze the embedding space by classifying embeddings into <a href=https://en.wikipedia.org/wiki/Semantic_class>semantic classes</a>. Our main findings are : (i) Information about a sense is generally represented well in a single-vector embedding if the sense is frequent. (ii) A <a href=https://en.wikipedia.org/wiki/Classifier_(linguistics)>classifier</a> can accurately predict whether a word is single-sense or multi-sense, based only on its embedding. (iii) Although rare senses are not well represented in single-vector embeddings, this does not have negative impact on an NLP application whose performance depends on frequent senses.</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1343.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1343 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1343 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D18-1343.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1343" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D18-1343/>Multi-Multi-View Learning : Multilingual and Multi-Representation Entity Typing</a></strong><br><a href=/people/y/yadollah-yaghoobzadeh/>Yadollah Yaghoobzadeh</a>
|
<a href=/people/h/hinrich-schutze/>Hinrich Schütze</a><br><a href=/volumes/D18-1/ class=text-muted>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1343><div class="card-body p-3 small">Accurate and complete knowledge bases (KBs) are paramount in <a href=https://en.wikipedia.org/wiki/Neuro-linguistic_programming>NLP</a>. We employ mul-itiview learning for increasing the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> and coverage of entity type information in <a href=https://en.wikipedia.org/wiki/Knowledge_base>KBs</a>. We rely on two <a href=https://en.wikipedia.org/wiki/Metafiction>metaviews</a> : language and representation. For <a href=https://en.wikipedia.org/wiki/Language>language</a>, we consider high-resource and low-resource languages from <a href=https://en.wikipedia.org/wiki/Wikipedia>Wikipedia</a>. For <a href=https://en.wikipedia.org/wiki/Representation_(mathematics)>representation</a>, we consider representations based on the <a href=https://en.wikipedia.org/wiki/Context_(language_use)>context distribution</a> of the entity (i.e., on its embedding), on the entity&#8217;s name (i.e., on its surface form) and on its description in <a href=https://en.wikipedia.org/wiki/Wikipedia>Wikipedia</a>. The two metaviews language and representation can be freely combined : each pair of <a href=https://en.wikipedia.org/wiki/Language_(computer_science)>language</a> and representation (e.g., German embedding, English description, Spanish name) is a distinct view. Our experiments on entity typing with fine-grained classes demonstrate the effectiveness of multiview learning. We release MVET, a large multiview and, in particular, multilingual entity typing dataset we created. Mono- and multilingual fine-grained entity typing systems can be evaluated on this dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/Q18-1047.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-Q18-1047 data-toggle=collapse aria-expanded=false aria-controls=abstract-Q18-1047 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/Q18-1047/>Attentive Convolution : Equipping CNNs with RNN-style Attention Mechanisms<span class=acl-fixed-case>CNN</span>s with <span class=acl-fixed-case>RNN</span>-style Attention Mechanisms</a></strong><br><a href=/people/w/wenpeng-yin/>Wenpeng Yin</a>
|
<a href=/people/h/hinrich-schutze/>Hinrich Schütze</a><br><a href=/volumes/Q18-1/ class=text-muted>Transactions of the Association for Computational Linguistics, Volume 6</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-Q18-1047><div class="card-body p-3 small">In <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a>, convolutional neural networks (CNNs) have benefited less than recurrent neural networks (RNNs) from attention mechanisms. We hypothesize that this is because the <a href=https://en.wikipedia.org/wiki/Attention>attention</a> in CNNs has been mainly implemented as attentive pooling (i.e., it is applied to pooling) rather than as attentive convolution (i.e., it is integrated into <a href=https://en.wikipedia.org/wiki/Convolution>convolution</a>). Convolution is the differentiator of CNNs in that it can powerfully model the higher-level representation of a word by taking into account its local fixed-size context in the input text tx. In this work, we propose an attentive convolution network, ATTCONV. It extends the context scope of the convolution operation, deriving higher-level features for a word not only from local context, but also from information extracted from nonlocal context by the attention mechanism commonly used in RNNs. This nonlocal context can come (i) from parts of the input text tx that are distant or (ii) from extra (i.e., external) contexts ty. Experiments on sentence modeling with zero-context (sentiment analysis), single-context (textual entailment) and multiple-context (claim verification) demonstrate the effectiveness of ATTCONV in sentence representation learning with the incorporation of context. In particular, attentive convolution outperforms attentive pooling and is a strong competitor to popular attentive RNNs.1</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-1200.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-1200/>Proceedings of the Second Workshop on Subword/Character <span class=acl-fixed-case>LE</span>vel Models</a></strong><br><a href=/people/m/manaal-faruqui/>Manaal Faruqui</a>
|
<a href=/people/h/hinrich-schutze/>Hinrich Schütze</a>
|
<a href=/people/i/isabel-trancoso/>Isabel Trancoso</a>
|
<a href=/people/y/yulia-tsvetkov/>Yulia Tsvetkov</a>
|
<a href=/people/y/yadollah-yaghoobzadeh/>Yadollah Yaghoobzadeh</a><br><a href=/volumes/W18-12/ class=text-muted>Proceedings of the Second Workshop on Subword/Character LEvel Models</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3013.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3013 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3013 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-3013" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-3013/>Evaluating Word Embeddings in Multi-label Classification Using Fine-Grained Name Typing</a></strong><br><a href=/people/y/yadollah-yaghoobzadeh/>Yadollah Yaghoobzadeh</a>
|
<a href=/people/k/katharina-kann/>Katharina Kann</a>
|
<a href=/people/h/hinrich-schutze/>Hinrich Schütze</a><br><a href=/volumes/W18-30/ class=text-muted>Proceedings of The Third Workshop on Representation Learning for NLP</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3013><div class="card-body p-3 small">Embedding models typically associate each word with a single real-valued vector, representing its different properties. Evaluation methods, therefore, need to analyze the accuracy and completeness of these properties in <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a>. This requires fine-grained analysis of embedding subspaces. Multi-label classification is an appropriate way to do so. We propose a new evaluation method for <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> based on <a href=https://en.wikipedia.org/wiki/Multi-label_classification>multi-label classification</a> given a <a href=https://en.wikipedia.org/wiki/Word_embedding>word embedding</a>. The task we use is fine-grained name typing : given a large corpus, find all types that a name can refer to based on the name embedding. Given the scale of entities in <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge bases</a>, we can build <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> for this task that are complementary to the current embedding evaluation datasets in : they are very large, contain fine-grained classes, and allow the direct evaluation of embeddings without confounding factors like sentence context.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5418.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5418 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5418 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5418/>LISA : Explaining Recurrent Neural Network Judgments via Layer-wIse Semantic Accumulation and Example to Pattern Transformation<span class=acl-fixed-case>LISA</span>: Explaining Recurrent Neural Network Judgments via Layer-w<span class=acl-fixed-case>I</span>se Semantic Accumulation and Example to Pattern Transformation</a></strong><br><a href=/people/p/pankaj-gupta/>Pankaj Gupta</a>
|
<a href=/people/h/hinrich-schutze/>Hinrich Schütze</a><br><a href=/volumes/W18-54/ class=text-muted>Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5418><div class="card-body p-3 small">Recurrent neural networks (RNNs) are temporal networks and cumulative in nature that have shown promising results in various natural language processing tasks. Despite their success, it still remains a challenge to understand their hidden behavior. In this work, we analyze and interpret the cumulative nature of <a href=https://en.wikipedia.org/wiki/Neural_network>RNN</a> via a proposed technique named as Layer-wIse-Semantic-Accumulation (LISA) for explaining decisions and detecting the most likely (i.e., saliency) patterns that the network relies on while <a href=https://en.wikipedia.org/wiki/Decision-making>decision making</a>. We demonstrate (1) LISA : How an RNN accumulates or builds semantics during its sequential processing for a given text example and expected response (2) Example2pattern : How the saliency patterns look like for each category in the data according to the network in decision making. We analyse the sensitiveness of RNNs about different inputs to check the increase or decrease in prediction scores and further extract the saliency patterns learned by the <a href=https://en.wikipedia.org/wiki/Neural_network>network</a>. We employ two relation classification datasets : SemEval 10 Task 8 and TAC KBP Slot Filling to explain RNN predictions via the LISA and example2pattern.<i>Layer-wIse-Semantic-Accumulation</i> (LISA) for explaining decisions and detecting the most likely (i.e., saliency) patterns that the network relies on while decision making. We demonstrate (1) <i>LISA</i>: &#8220;How an RNN accumulates or builds semantics during its sequential processing for a given text example and expected response&#8221; (2) <i>Example2pattern</i>: &#8220;How the saliency patterns look like for each category in the data according to the network in decision making&#8221;. We analyse the sensitiveness of RNNs about different inputs to check the increase or decrease in prediction scores and further extract the saliency patterns learned by the network. We employ two relation classification datasets: SemEval 10 Task 8 and TAC KBP Slot Filling to explain RNN predictions via the <i>LISA</i> and <i>example2pattern</i>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6538.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6538 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6538 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6538/>Task Proposal : The TL;DR Challenge<span class=acl-fixed-case>TL</span>;<span class=acl-fixed-case>DR</span> Challenge</a></strong><br><a href=/people/s/shahbaz-syed/>Shahbaz Syed</a>
|
<a href=/people/m/michael-volske/>Michael Völske</a>
|
<a href=/people/m/martin-potthast/>Martin Potthast</a>
|
<a href=/people/n/nedim-lipka/>Nedim Lipka</a>
|
<a href=/people/b/benno-stein/>Benno Stein</a>
|
<a href=/people/h/hinrich-schutze/>Hinrich Schütze</a><br><a href=/volumes/W18-65/ class=text-muted>Proceedings of the 11th International Conference on Natural Language Generation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6538><div class="card-body p-3 small">The TL;DR challenge fosters research in abstractive summarization of informal text, the largest and fastest-growing source of textual data on the <a href=https://en.wikipedia.org/wiki/World_Wide_Web>web</a>, which has been overlooked by summarization research so far. The challenge owes its name to the frequent practice of social media users to supplement long posts with a TL;DRfor too long ; did n&#8217;t readfollowed by a short summary as a courtesy to those who would otherwise reply with the exact same abbreviation to indicate they did not care to read a post for its apparent length. Posts featuring TL;DR summaries form an excellent ground truth for summarization, and by tapping into this resource for the first time, we have mined millions of training examples from <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>, opening the door to all kinds of generative models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1005.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1005 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1005 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://vimeo.com/276387788 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N18-1005/>Fortification of Neural Morphological Segmentation Models for Polysynthetic Minimal-Resource Languages</a></strong><br><a href=/people/k/katharina-kann/>Katharina Kann</a>
|
<a href=/people/j/jesus-manuel-mager-hois/>Jesus Manuel Mager Hois</a>
|
<a href=/people/i/ivan-meza-ruiz/>Ivan Vladimir Meza-Ruiz</a>
|
<a href=/people/h/hinrich-schutze/>Hinrich Schütze</a><br><a href=/volumes/N18-1/ class=text-muted>Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1005><div class="card-body p-3 small">Morphological segmentation for polysynthetic languages is challenging, because a word may consist of many individual morphemes and training data can be extremely scarce. Since neural sequence-to-sequence (seq2seq) models define the state of the art for morphological segmentation in high-resource settings and for (mostly) European languages, we first show that they also obtain competitive performance for Mexican polysynthetic languages in minimal-resource settings. We then propose two novel multi-task training approachesone with, one without need for external unlabeled resources, and two corresponding data augmentation methods, improving over the neural baseline for all languages. Finally, we explore cross-lingual transfer as a third way to fortify our neural model and show that we can train one single multi-lingual model for related languages while maintaining comparable or even improved performance, thus reducing the amount of parameters by close to 75 %. We provide our morphological segmentation datasets for <a href=https://en.wikipedia.org/wiki/Mexicanero_language>Mexicanero</a>, <a href=https://en.wikipedia.org/wiki/Nahuatl>Nahuatl</a>, Wixarika and Yorem Nokki for future research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1098.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1098 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1098 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://vimeo.com/277669869 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N18-1098/>Deep Temporal-Recurrent-Replicated-Softmax for Topical Trends over Time</a></strong><br><a href=/people/p/pankaj-gupta/>Pankaj Gupta</a>
|
<a href=/people/s/subburam-rajaram/>Subburam Rajaram</a>
|
<a href=/people/h/hinrich-schutze/>Hinrich Schütze</a>
|
<a href=/people/b/bernt-andrassy/>Bernt Andrassy</a><br><a href=/volumes/N18-1/ class=text-muted>Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1098><div class="card-body p-3 small">Dynamic topic modeling facilitates the identification of topical trends over time in temporal collections of unstructured documents. We introduce a novel unsupervised neural dynamic topic model named as Recurrent Neural Network-Replicated Softmax Model (RNNRSM), where the discovered topics at each time influence the topic discovery in the subsequent time steps. We account for the temporal ordering of documents by explicitly modeling a joint distribution of latent topical dependencies over time, using distributional estimators with temporal recurrent connections. Applying RNN-RSM to 19 years of articles on NLP research, we demonstrate that compared to state-of-the art topic models, RNNRSM shows better generalization, topic interpretation, evolution and trends. We also introduce a <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metric</a> (named as SPAN) to quantify the capability of <a href=https://en.wikipedia.org/wiki/Dynamic_topic_model>dynamic topic model</a> to capture word evolution in topics over time.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1141.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1141 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1141 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1141.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-1141/>Embedding Learning Through Multilingual Concept Induction</a></strong><br><a href=/people/p/philipp-dufter/>Philipp Dufter</a>
|
<a href=/people/m/mengjie-zhao/>Mengjie Zhao</a>
|
<a href=/people/m/martin-schmitt/>Martin Schmitt</a>
|
<a href=/people/a/alexander-fraser/>Alexander Fraser</a>
|
<a href=/people/h/hinrich-schutze/>Hinrich Schütze</a><br><a href=/volumes/P18-1/ class=text-muted>Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1141><div class="card-body p-3 small">We present a new method for estimating vector space representations of words : embedding learning by concept induction. We test this method on a highly parallel corpus and learn semantic representations of words in 1259 different languages in a single common space. An extensive experimental evaluation on crosslingual word similarity and <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> indicates that concept-based multilingual embedding learning performs better than previous approaches.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2086.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2086 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2086 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2086.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-2086" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P18-2086/>End-Task Oriented Textual Entailment via Deep Explorations of Inter-Sentence Interactions</a></strong><br><a href=/people/w/wenpeng-yin/>Wenpeng Yin</a>
|
<a href=/people/d/dan-roth/>Dan Roth</a>
|
<a href=/people/h/hinrich-schutze/>Hinrich Schütze</a><br><a href=/volumes/P18-2/ class=text-muted>Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2086><div class="card-body p-3 small">This work deals with SciTail, a natural entailment challenge derived from a multi-choice question answering problem. The premises and hypotheses in SciTail were generated with no awareness of each other, and did not specifically aim at the entailment task. This makes it more challenging than other entailment data sets and more directly useful to the end-task question answering. We propose DEISTE (deep explorations of inter-sentence interactions for textual entailment) for this entailment task. Given word-to-word interactions between the premise-hypothesis pair (P, H), DEISTE consists of : (i) a parameter-dynamic convolution to make important words in P and H play a dominant role in learnt representations ; and (ii) a position-aware attentive convolution to encode the representation and position information of the aligned word pairs. Experiments show that DEISTE gets 5 % improvement over prior state of the art and that the pretrained DEISTE on SciTail generalizes well on RTE-5.</div></div><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1182.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1182 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1182 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-1182/>One-Shot Neural Cross-Lingual Transfer for Paradigm Completion</a></strong><br><a href=/people/k/katharina-kann/>Katharina Kann</a>
|
<a href=/people/r/ryan-cotterell/>Ryan Cotterell</a>
|
<a href=/people/h/hinrich-schutze/>Hinrich Schütze</a><br><a href=/volumes/P17-1/ class=text-muted>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1182><div class="card-body p-3 small">We present a novel cross-lingual transfer method for paradigm completion, the task of mapping a lemma to its inflected forms, using a neural encoder-decoder model, the state of the art for the monolingual task. We use labeled data from a high-resource language to increase performance on a low-resource language. In experiments on 21 language pairs from four different language families, we obtain up to 58 % higher <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> than without transfer and show that even zero-shot and one-shot learning are possible. We further find that the degree of <a href=https://en.wikipedia.org/wiki/Language_family>language relatedness</a> strongly influences the ability to transfer <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphological knowledge</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4100.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4100/>Proceedings of the First Workshop on Subword and Character Level Models in <span class=acl-fixed-case>NLP</span></a></strong><br><a href=/people/m/manaal-faruqui/>Manaal Faruqui</a>
|
<a href=/people/h/hinrich-schutze/>Hinrich Schuetze</a>
|
<a href=/people/i/isabel-trancoso/>Isabel Trancoso</a>
|
<a href=/people/y/yadollah-yaghoobzadeh/>Yadollah Yaghoobzadeh</a><br><a href=/volumes/W17-41/ class=text-muted>Proceedings of the First Workshop on Subword and Character Level Models in NLP</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4111.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4111 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4111 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4111/>Unlabeled Data for Morphological Generation With Character-Based Sequence-to-Sequence Models</a></strong><br><a href=/people/k/katharina-kann/>Katharina Kann</a>
|
<a href=/people/h/hinrich-schutze/>Hinrich Schütze</a><br><a href=/volumes/W17-41/ class=text-muted>Proceedings of the First Workshop on Subword and Character Level Models in NLP</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4111><div class="card-body p-3 small">We present a semi-supervised way of training a character-based encoder-decoder recurrent neural network for morphological reinflectionthe task of generating one inflected wordform from another. This is achieved by using unlabeled tokens or random strings as training data for an autoencoding task, adapting a network for morphological reinflection, and performing multi-task training. We thus use limited labeled data more effectively, obtaining up to 9.92 % improvement over state-of-the-art <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a> for 8 different languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/J17-2003.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-J17-2003 data-toggle=collapse aria-expanded=false aria-controls=abstract-J17-2003 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/J17-2003/>Statistical Models for Unsupervised, Semi-Supervised Supervised Transliteration Mining</a></strong><br><a href=/people/h/hassan-sajjad/>Hassan Sajjad</a>
|
<a href=/people/h/helmut-schmid/>Helmut Schmid</a>
|
<a href=/people/a/alexander-fraser/>Alexander Fraser</a>
|
<a href=/people/h/hinrich-schutze/>Hinrich Schütze</a><br><a href=/volumes/J17-2/ class=text-muted>Computational Linguistics, Volume 43, Issue 2 - June 2017</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-J17-2003><div class="card-body p-3 small">We present a <a href=https://en.wikipedia.org/wiki/Generative_model>generative model</a> that efficiently mines <a href=https://en.wikipedia.org/wiki/Transliteration>transliteration pairs</a> in a consistent fashion in three different settings : unsupervised, semi-supervised, and supervised transliteration mining. The <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> interpolates two sub-models, one for the generation of transliteration pairs and one for the generation of non-transliteration pairs (i.e., noise). The <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> is trained on <a href=https://en.wikipedia.org/wiki/Noisy_data>noisy unlabeled data</a> using the <a href=https://en.wikipedia.org/wiki/EM_algorithm>EM algorithm</a>. During training the transliteration sub-model learns to generate transliteration pairs and the fixed non-transliteration model generates the noise pairs. After training, the unlabeled data is disambiguated based on the <a href=https://en.wikipedia.org/wiki/Posterior_probability>posterior probabilities</a> of the two <a href=https://en.wikipedia.org/wiki/Statistical_model>sub-models</a>. We evaluate our transliteration mining system on data from a transliteration mining shared task and on parallel corpora. For three out of four language pairs, our <a href=https://en.wikipedia.org/wiki/System>system</a> outperforms all semi-supervised and supervised systems that participated in the NEWS 2010 shared task. On word pairs extracted from <a href=https://en.wikipedia.org/wiki/Parallel_text>parallel corpora</a> with fewer than 2 % <a href=https://en.wikipedia.org/wiki/Transliteration>transliteration pairs</a>, our <a href=https://en.wikipedia.org/wiki/System>system</a> achieves up to 86.7 % F-measure with 77.9 % <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>precision</a> and 97.8 % <a href=https://en.wikipedia.org/wiki/Recall_(memory)>recall</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/J17-3004.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-J17-3004 data-toggle=collapse aria-expanded=false aria-controls=abstract-J17-3004 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/J17-3004/>AutoExtend : Combining Word Embeddings with Semantic Resources<span class=acl-fixed-case>A</span>uto<span class=acl-fixed-case>E</span>xtend: Combining Word Embeddings with Semantic Resources</a></strong><br><a href=/people/s/sascha-rothe/>Sascha Rothe</a>
|
<a href=/people/h/hinrich-schutze/>Hinrich Schütze</a><br><a href=/volumes/J17-3/ class=text-muted>Computational Linguistics, Volume 43, Issue 3 - September 2017</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-J17-3004><div class="card-body p-3 small">We present AutoExtend, a system that combines word embeddings with semantic resources by learning embeddings for non-word objects like synsets and entities and learning word embeddings that incorporate the semantic information from the resource. The method is based on encoding and decoding the <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> and is flexible in that it can take any <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> as input and does not need an additional <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training corpus</a>. The obtained <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a> live in the same <a href=https://en.wikipedia.org/wiki/Vector_space>vector space</a> as the input word embeddings. A sparse tensor formalization guarantees efficiency and <a href=https://en.wikipedia.org/wiki/Parallelizability>parallelizability</a>. We use <a href=https://en.wikipedia.org/wiki/WordNet>WordNet</a>, <a href=https://en.wikipedia.org/wiki/GermaNet>GermaNet</a>, and <a href=https://en.wikipedia.org/wiki/Freebase>Freebase</a> as semantic resources. AutoExtend achieves state-of-the-art performance on Word-in-Context Similarity and Word Sense Disambiguation tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1181.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1181 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1181 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1181/>Global Normalization of <a href=https://en.wikipedia.org/wiki/Convolutional_neural_network>Convolutional Neural Networks</a> for Joint Entity and Relation Classification</a></strong><br><a href=/people/h/heike-adel/>Heike Adel</a>
|
<a href=/people/h/hinrich-schutze/>Hinrich Schütze</a><br><a href=/volumes/D17-1/ class=text-muted>Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1181><div class="card-body p-3 small">We introduce globally normalized convolutional neural networks for joint entity classification and <a href=https://en.wikipedia.org/wiki/Relation_extraction>relation extraction</a>. In particular, we propose a way to utilize a linear-chain conditional random field output layer for predicting entity types and relations between entities at the same time. Our experiments show that global normalization outperforms a locally normalized softmax layer on a benchmark dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1003.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1003 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1003 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1003/>Exploring Different Dimensions of Attention for Uncertainty Detection</a></strong><br><a href=/people/h/heike-adel/>Heike Adel</a>
|
<a href=/people/h/hinrich-schutze/>Hinrich Schütze</a><br><a href=/volumes/E17-1/ class=text-muted>Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1003><div class="card-body p-3 small">Neural networks with <a href=https://en.wikipedia.org/wiki/Attention>attention</a> have proven effective for many natural language processing tasks. In this paper, we develop <a href=https://en.wikipedia.org/wiki/Attentional_control>attention mechanisms</a> for uncertainty detection. In particular, we generalize standardly used <a href=https://en.wikipedia.org/wiki/Attentional_control>attention mechanisms</a> by introducing external attention and sequence-preserving attention. These novel architectures differ from standard approaches in that they use external resources to compute attention weights and preserve <a href=https://en.wikipedia.org/wiki/Sequence>sequence information</a>. We compare them to other <a href=https://en.wikipedia.org/wiki/Configuration_(geometry)>configurations</a> along different dimensions of attention. Our novel architectures set the new state of the art on a Wikipedia benchmark dataset and perform similar to the state-of-the-art model on a biomedical benchmark which uses a large set of linguistic features.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1049.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1049 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1049 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1049/>Neural Multi-Source Morphological Reinflection</a></strong><br><a href=/people/k/katharina-kann/>Katharina Kann</a>
|
<a href=/people/r/ryan-cotterell/>Ryan Cotterell</a>
|
<a href=/people/h/hinrich-schutze/>Hinrich Schütze</a><br><a href=/volumes/E17-1/ class=text-muted>Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1049><div class="card-body p-3 small">We explore the task of multi-source morphological reinflection, which generalizes the standard, single-source version. The input consists of (i) a target tag and (ii) multiple pairs of source form and source tag for a lemma. The motivation is that it is beneficial to have access to more than one <a href=https://en.wikipedia.org/wiki/Form_(document)>source form</a> since different <a href=https://en.wikipedia.org/wiki/Form_(document)>source forms</a> can provide complementary information, e.g., different <a href=https://en.wikipedia.org/wiki/Word_stem>stems</a>. We further present a novel extension to the encoder-decoder recurrent neural architecture, consisting of multiple encoders, to better solve the task. We show that our new architecture outperforms single-source reinflection models and publish our dataset for multi-source morphological reinflection to facilitate future research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1055.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1055 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1055 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1055/>Multi-level Representations for Fine-Grained Typing of Knowledge Base Entities</a></strong><br><a href=/people/y/yadollah-yaghoobzadeh/>Yadollah Yaghoobzadeh</a>
|
<a href=/people/h/hinrich-schutze/>Hinrich Schütze</a><br><a href=/volumes/E17-1/ class=text-muted>Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1055><div class="card-body p-3 small">Entities are essential elements of <a href=https://en.wikipedia.org/wiki/Natural_language>natural language</a>. In this paper, we present <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> for learning multi-level representations of entities on three complementary levels : character (character patterns in entity names extracted, e.g., by neural networks), word (embeddings of words in entity names) and entity (entity embeddings). We investigate state-of-the-art learning methods on each level and find large differences, e.g., for <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning models</a>, traditional ngram features and the subword model of fasttext (Bojanowski et al., 2016) on the character level ; for <a href=https://en.wikipedia.org/wiki/Word2vec>word2vec</a> (Mikolov et al., 2013) on the word level ; and for the order-aware model wang2vec (Ling et al., 2015a) on the entity level. We confirm experimentally that each level of <a href=https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning>representation</a> contributes complementary information and a joint representation of all three levels improves the existing embedding based baseline for fine-grained entity typing by a large margin. Additionally, we show that adding information from entity descriptions further improves multi-level representations of entities.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1066.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1066 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1066 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1066/>Task-Specific Attentive Pooling of Phrase Alignments Contributes to Sentence Matching</a></strong><br><a href=/people/w/wenpeng-yin/>Wenpeng Yin</a>
|
<a href=/people/h/hinrich-schutze/>Hinrich Schütze</a><br><a href=/volumes/E17-1/ class=text-muted>Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1066><div class="card-body p-3 small">This work studies comparatively two typical sentence matching tasks : textual entailment (TE) and answer selection (AS), observing that weaker phrase alignments are more critical in TE, while stronger phrase alignments deserve more attention in AS. The key to reach this observation lies in phrase detection, phrase representation, phrase alignment, and more importantly how to connect those aligned phrases of different matching degrees with the final <a href=https://en.wikipedia.org/wiki/Classifier_(linguistics)>classifier</a>. Prior work (i) has limitations in phrase generation and representation, or (ii) conducts <a href=https://en.wikipedia.org/wiki/Sequence_alignment>alignment</a> at word and phrase levels by handcrafted features or (iii) utilizes a single framework of <a href=https://en.wikipedia.org/wiki/Sequence_alignment>alignment</a> without considering the characteristics of specific tasks, which limits the <a href=https://en.wikipedia.org/wiki/Conceptual_framework>framework</a>&#8217;s effectiveness across tasks. We propose an architecture based on Gated Recurrent Unit that supports (i) representation learning of phrases of arbitrary granularity and (ii) task-specific attentive pooling of phrase alignments between two sentences. Experimental results on TE and AS match our observation and show the effectiveness of our approach.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1074.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1074 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1074 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1074/>Nonsymbolic Text Representation</a></strong><br><a href=/people/h/hinrich-schutze/>Hinrich Schütze</a><br><a href=/volumes/E17-1/ class=text-muted>Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1074><div class="card-body p-3 small">We introduce the first generic text representation model that is completely nonsymbolic, i.e., it does not require the availability of a segmentation or tokenization method that attempts to identify words or other symbolic units in text. This applies to training the parameters of the <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> on a training corpus as well as to applying it when computing the representation of a new text. We show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> performs better than prior work on an <a href=https://en.wikipedia.org/wiki/Information_extraction>information extraction</a> and a text denoising task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1111.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1111 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1111 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1111/>Noise Mitigation for Neural Entity Typing and <a href=https://en.wikipedia.org/wiki/Relation_extraction>Relation Extraction</a></a></strong><br><a href=/people/y/yadollah-yaghoobzadeh/>Yadollah Yaghoobzadeh</a>
|
<a href=/people/h/heike-adel/>Heike Adel</a>
|
<a href=/people/h/hinrich-schutze/>Hinrich Schütze</a><br><a href=/volumes/E17-1/ class=text-muted>Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1111><div class="card-body p-3 small">In this paper, we address two different types of <a href=https://en.wikipedia.org/wiki/Noise_(signal_processing)>noise</a> in information extraction models : noise from distant supervision and noise from pipeline input features. Our target tasks are <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entity typing</a> and <a href=https://en.wikipedia.org/wiki/Relation_(database)>relation extraction</a>. For the first noise type, we introduce multi-instance multi-label learning algorithms using neural network models, and apply them to fine-grained entity typing for the first time. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms the state-of-the-art supervised approach which uses global embeddings of entities. For the second noise type, we propose ways to improve the integration of noisy entity type predictions into <a href=https://en.wikipedia.org/wiki/Relation_extraction>relation extraction</a>. Our experiments show that probabilistic predictions are more robust than discrete predictions and that joint training of the two tasks performs best.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2119.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2119 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2119 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2119/>End-to-End Trainable Attentive Decoder for Hierarchical Entity Classification</a></strong><br><a href=/people/s/sanjeev-karn/>Sanjeev Karn</a>
|
<a href=/people/u/ulli-waltinger/>Ulli Waltinger</a>
|
<a href=/people/h/hinrich-schutze/>Hinrich Schütze</a><br><a href=/volumes/E17-2/ class=text-muted>Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2119><div class="card-body p-3 small">We address fine-grained entity classification and propose a novel attention-based recurrent neural network (RNN) encoder-decoder that generates paths in the type hierarchy and can be trained end-to-end. We show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> performs better on fine-grained entity classification than prior work that relies on flat or local classifiers that do not directly model hierarchical structure.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Hinrich+Sch%C3%BCtze" title="Search for 'Hinrich Schütze' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/y/yadollah-yaghoobzadeh/ class=align-middle>Yadollah Yaghoobzadeh</a>
<span class="badge badge-secondary align-middle ml-2">7</span></li><li class=list-group-item><a href=/people/k/katharina-kann/ class=align-middle>Katharina Kann</a>
<span class="badge badge-secondary align-middle ml-2">6</span></li><li class=list-group-item><a href=/people/p/pankaj-gupta/ class=align-middle>Pankaj Gupta</a>
<span class="badge badge-secondary align-middle ml-2">5</span></li><li class=list-group-item><a href=/people/p/philipp-dufter/ class=align-middle>Philipp Dufter</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/people/w/wenpeng-yin/ class=align-middle>Wenpeng Yin</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/h/heike-adel/ class=align-middle>Heike Adel</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/a/antonis-maronikolakis/ class=align-middle>Antonis Maronikolakis</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/r/ryan-cotterell/ class=align-middle>Ryan Cotterell</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/m/manaal-faruqui/ class=align-middle>Manaal Faruqui</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/i/isabel-trancoso/ class=align-middle>Isabel Trancoso</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/h/helmut-schmid/ class=align-middle>Helmut Schmid</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/a/alexander-fraser/ class=align-middle>Alexander Fraser</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/m/martin-schmitt/ class=align-middle>Martin Schmitt</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/n/nina-poerner/ class=align-middle>Nina Poerner</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/m/masoud-jalili-sabet/ class=align-middle>Masoud Jalili Sabet</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/f/francois-yvon/ class=align-middle>François Yvon</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/y/yatin-chaudhary/ class=align-middle>Yatin Chaudhary</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/u/ulli-waltinger/ class=align-middle>Ulli Waltinger</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/t/timo-schick/ class=align-middle>Timo Schick</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/m/mengjie-zhao/ class=align-middle>Mengjie Zhao</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/v/valentin-hofmann/ class=align-middle>Valentin Hofmann</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/janet-pierrehumbert/ class=align-middle>Janet Pierrehumbert</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/mark-stevenson/ class=align-middle>Mark Stevenson</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/hassan-sajjad/ class=align-middle>Hassan Sajjad</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/sascha-rothe/ class=align-middle>Sascha Rothe</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/n/nora-kassner/ class=align-middle>Nora Kassner</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/o/oyvind-tafjord/ class=align-middle>Oyvind Tafjord</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/p/peter-clark/ class=align-middle>Peter Clark</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/u/usama-yaseen/ class=align-middle>Usama Yaseen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/ayyoob-imani/ class=align-middle>Ayyoob Imani</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/lutfi-kerem-senel/ class=align-middle>Lütfi Kerem Senel</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/kiante-brantley/ class=align-middle>Kianté Brantley</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/soham-dan/ class=align-middle>Soham Dan</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/i/iryna-gurevych/ class=align-middle>Iryna Gurevych</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/ji-ung-lee/ class=align-middle>Ji-Ung Lee</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/f/filip-radlinski/ class=align-middle>Filip Radlinski</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/e/edwin-simpson/ class=align-middle>Edwin Simpson</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/lili-yu/ class=align-middle>Lili Yu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/khushbu-saxena/ class=align-middle>Khushbu Saxena</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/v/vivek-kulkarni/ class=align-middle>Vivek Kulkarni</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/thomas-runkler/ class=align-middle>Thomas Runkler</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yulia-tsvetkov/ class=align-middle>Yulia Tsvetkov</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/shahbaz-syed/ class=align-middle>Shahbaz Syed</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/michael-volske/ class=align-middle>Michael Völske</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/martin-potthast/ class=align-middle>Martin Potthast</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/n/nedim-lipka/ class=align-middle>Nedim Lipka</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/b/benno-stein/ class=align-middle>Benno Stein</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jesus-manuel-mager-hois/ class=align-middle>Jesus Manuel Mager Hois</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/i/ivan-meza-ruiz/ class=align-middle>Ivan Meza-Ruiz</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/subburam-rajaram/ class=align-middle>Subburam Rajaram</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/b/bernt-andrassy/ class=align-middle>Bernt Andrassy</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/suteera-seeha/ class=align-middle>Suteera Seeha</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/i/ivan-bilan/ class=align-middle>Ivan Bilan</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/liliana-mamani-sanchez/ class=align-middle>Liliana Mamani Sanchez</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/johannes-huber/ class=align-middle>Johannes Huber</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/michael-matuschek/ class=align-middle>Michael Matuschek</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/sanjeev-karn/ class=align-middle>Sanjeev Karn</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/dan-roth/ class=align-middle>Dan Roth</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/timothy-j-hazen/ class=align-middle>Timothy J. Hazen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/e/eneko-agirre/ class=align-middle>Eneko Agirre</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/emnlp/ class=align-middle>EMNLP</a><span class="badge badge-secondary align-middle ml-2">8</span></li><li class=list-group-item><a href=/venues/eacl/ class=align-middle>EACL</a><span class="badge badge-secondary align-middle ml-2">7</span></li><li class=list-group-item><a href=/venues/ws/ class=align-middle>WS</a><span class="badge badge-secondary align-middle ml-2">6</span></li><li class=list-group-item><a href=/venues/acl/ class=align-middle>ACL</a><span class="badge badge-secondary align-middle ml-2">5</span></li><li class=list-group-item><a href=/venues/findings/ class=align-middle>Findings</a><span class="badge badge-secondary align-middle ml-2">4</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-venues aria-expanded=false aria-controls=more-venues>show all...</li><div class="collapse border-top" id=more-venues><li class=list-group-item><a href=/venues/naacl/ class=align-middle>NAACL</a><span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/venues/cl/ class=align-middle>CL</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/nlp4if/ class=align-middle>NLP4IF</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/tacl/ class=align-middle>TACL</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/internlp/ class=align-middle>InterNLP</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/insights/ class=align-middle>insights</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/coling/ class=align-middle>COLING</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/lrec/ class=align-middle>LREC</a><span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>