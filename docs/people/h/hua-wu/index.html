<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Hua Wu (吴华) - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Hua</span> <span class=font-weight-bold>Wu</span></h2><hr><div class=row><div class=col-lg-9><h4>2022</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.acl-long.395.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--acl-long--395 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.acl-long.395 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.acl-long.395" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2022.acl-long.395/>Unified Structure Generation for Universal Information Extraction</a></strong><br><a href=/people/y/yaojie-lu/>Yaojie Lu</a>
|
<a href=/people/q/qing-liu/>Qing Liu</a>
|
<a href=/people/d/dai-dai/>Dai Dai</a>
|
<a href=/people/x/xinyan-xiao/>Xinyan Xiao</a>
|
<a href=/people/h/hongyu-lin/>Hongyu Lin</a>
|
<a href=/people/x/xianpei-han/>Xianpei Han</a>
|
<a href=/people/l/le-sun/>Le Sun</a>
|
<a href=/people/h/hua-wu/>Hua Wu</a><br><a href=/volumes/2022.acl-long/ class=text-muted>Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--acl-long--395><div class="card-body p-3 small">Information extraction suffers from its varying targets heterogeneous structures and demand specific schemas In this paper we propose a unified text to structure generation framework namely UIE which can universally model different IE tasks adaptively generate targeted structures and collaboratively learn general IE abilities from different knowledge sources Specifically UIE uniformly encodes different extraction structures via a structured extraction language adaptively generates target extractions via a schema based prompt mechanism structural schema instructor and captures the common IE abilities via a large scale pretrained text to structure model Experiments show that UIE achieved the state of the art performance on IE tasks datasets and on all supervised low resource and few shot settings for a wide range of entity relation event and sentiment extraction tasks and their unification These results verified the effectiveness universality and transferability of <a href=https://en.wikipedia.org/wiki/UIE>UIE</a></div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-acl.207.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-acl--207 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-acl.207 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.findings-acl.207" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2022.findings-acl.207/>Long Time No See! Open-Domain Conversation with Long-Term Persona Memory</a></strong><br><a href=/people/x/xinchao-xu/>Xinchao Xu</a>
|
<a href=/people/z/zhibin-gou/>Zhibin Gou</a>
|
<a href=/people/w/wenquan-wu/>Wenquan Wu</a>
|
<a href=/people/z/zheng-yu-niu/>Zheng-Yu Niu</a>
|
<a href=/people/h/hua-wu/>Hua Wu</a>
|
<a href=/people/h/haifeng-wang/>Haifeng Wang</a>
|
<a href=/people/s/shihang-wang/>Shihang Wang</a><br><a href=/volumes/2022.findings-acl/ class=text-muted>Findings of the Association for Computational Linguistics: ACL 2022</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-acl--207><div class="card-body p-3 small">Most of the open-domain dialogue models tend to perform poorly in the setting of long-term human-bot conversations. The possible reason is that they lack the capability of understanding and memorizing long-term dialogue history information. To address this issue, we present a novel task of Long-term Memory Conversation (LeMon) and then build a new dialogue dataset DuLeMon and a dialogue generation framework with Long-Term Memory (LTM) mechanism (called PLATO-LTM). This LTM mechanism enables our system to accurately extract and continuously update long-term persona memory without requiring multiple-session dialogue datasets for model training. To our knowledge, this is the first attempt to conduct real-time dynamic management of persona information of both parties, including the user and the bot. Results on DuLeMon indicate that PLATO-LTM can significantly outperform baselines in terms of long-term dialogue consistency, leading to better dialogue engagingness.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-acl.251.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-acl--251 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-acl.251 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.findings-acl.251/><span class=acl-fixed-case>UNIMO</span>-2: End-to-End Unified Vision-Language Grounded Learning</a></strong><br><a href=/people/w/wei-li/>Wei Li</a>
|
<a href=/people/c/can-gao/>Can Gao</a>
|
<a href=/people/g/guocheng-niu/>Guocheng Niu</a>
|
<a href=/people/x/xinyan-xiao/>Xinyan Xiao</a>
|
<a href=/people/h/hao-liu/>Hao Liu</a>
|
<a href=/people/j/jiachen-liu/>Jiachen Liu</a>
|
<a href=/people/h/hua-wu/>Hua Wu</a>
|
<a href=/people/h/haifeng-wang/>Haifeng Wang</a><br><a href=/volumes/2022.findings-acl/ class=text-muted>Findings of the Association for Computational Linguistics: ACL 2022</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-acl--251><div class="card-body p-3 small">Vision-Language Pre-training (VLP) has achieved impressive performance on various cross-modal downstream tasks. However, most existing methods can only learn from aligned image-caption data and rely heavily on expensive regional features, which greatly limits their scalability and performance. In this paper, we propose an end-to-end unified-modal pre-training framework, namely UNIMO-2, for joint learning on both aligned image-caption data and unaligned image-only and text-only corpus. We build a unified Transformer model to jointly learn visual representations, textual representations and semantic alignment between images and texts. In particular, we propose to conduct grounded learning on both images and texts via a sharing grounded space, which helps bridge unaligned images and texts, and align the visual and textual semantic spaces on different types of corpora. The experiments show that our grounded learning method can improve textual and visual semantic alignment for improving performance on various cross-modal tasks. Moreover, benefiting from effective joint modeling of different types of corpora, our model also achieves impressive performance on single-modal visual and textual tasks. Our code and models are public at the UNIMO project page https://unimo-ptm.github.io/.</div></div><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.ccl-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.ccl-1.0/>Proceedings of the 20th Chinese National Conference on Computational Linguistics</a></strong><br><a href=/people/s/sheng-li/>Sheng Li (李生)</a>
|
<a href=/people/m/maosong-sun/>Maosong Sun (孙茂松)</a>
|
<a href=/people/y/yang-liu-ict/>Yang Liu (刘洋)</a>
|
<a href=/people/h/hua-wu/>Hua Wu (吴华)</a>
|
<a href=/people/k/kang-liu/>Kang Liu (刘康)</a>
|
<a href=/people/w/wanxiang-che/>Wanxiang Che (车万翔)</a>
|
<a href=/people/s/shizhu-he/>Shizhu He (何世柱)</a>
|
<a href=/people/g/gaoqi-rao/>Gaoqi Rao (饶高琦)</a><br><a href=/volumes/2021.ccl-1/ class=text-muted>Proceedings of the 20th Chinese National Conference on Computational Linguistics</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.autosimtrans-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.autosimtrans-1.0/>Proceedings of the Second Workshop on Automatic Simultaneous Translation</a></strong><br><a href=/people/h/hua-wu/>Hua Wu</a>
|
<a href=/people/c/colin-cherry/>Colin Cherry</a>
|
<a href=/people/l/liang-huang/>Liang Huang</a>
|
<a href=/people/z/zhongjun-he/>Zhongjun He</a>
|
<a href=/people/q/qun-liu/>Qun Liu</a>
|
<a href=/people/m/maha-elbayad/>Maha Elbayad</a>
|
<a href=/people/m/mark-liberman/>Mark Liberman</a>
|
<a href=/people/h/haifeng-wang/>Haifeng Wang</a>
|
<a href=/people/m/mingbo-ma/>Mingbo Ma</a>
|
<a href=/people/r/ruiqing-zhang/>Ruiqing Zhang</a><br><a href=/volumes/2021.autosimtrans-1/ class=text-muted>Proceedings of the Second Workshop on Automatic Simultaneous Translation</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.227.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--227 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.227 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.227" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.227/>ERNIE-Doc : A Retrospective Long-Document Modeling Transformer<span class=acl-fixed-case>ERNIE</span>-<span class=acl-fixed-case>D</span>oc: A Retrospective Long-Document Modeling Transformer</a></strong><br><a href=/people/s/siyu-ding/>SiYu Ding</a>
|
<a href=/people/j/junyuan-shang/>Junyuan Shang</a>
|
<a href=/people/s/shuohuan-wang/>Shuohuan Wang</a>
|
<a href=/people/y/yu-sun/>Yu Sun</a>
|
<a href=/people/h/hao-tian/>Hao Tian</a>
|
<a href=/people/h/hua-wu/>Hua Wu</a>
|
<a href=/people/h/haifeng-wang/>Haifeng Wang</a><br><a href=/volumes/2021.acl-long/ class=text-muted>Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--227><div class="card-body p-3 small">Transformers are not suited for processing long documents, due to their quadratically increasing memory and <a href=https://en.wikipedia.org/wiki/Time_management>time consumption</a>. Simply truncating a long document or applying the sparse attention mechanism will incur the context fragmentation problem or lead to an inferior modeling capability against comparable model sizes. In this paper, we propose ERNIE-Doc, a document-level language pretraining model based on <a href=https://en.wikipedia.org/wiki/Recurrence_relation>Recurrence Transformers</a>. Two well-designed techniques, namely the retrospective feed mechanism and the enhanced recurrence mechanism, enable ERNIE-Doc, which has a much longer effective context length, to capture the contextual information of a complete document. We pretrain ERNIE-Doc to explicitly learn the relationships among segments with an additional document-aware segment-reordering objective. Various experiments were conducted on both English and Chinese document-level tasks. ERNIE-Doc improved the state-of-the-art language modeling result of <a href=https://en.wikipedia.org/wiki/Perplexity>perplexity</a> to 16.8 on WikiText-103. Moreover, it outperformed competitive pretraining models by a large margin on most language understanding tasks, such as <a href=https://en.wikipedia.org/wiki/Text_classification>text classification</a> and <a href=https://en.wikipedia.org/wiki/Question_answering>question answering</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.nlp4convai-1.8.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--nlp4convai-1--8 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.nlp4convai-1.8 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.nlp4convai-1.8" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.nlp4convai-1.8/>Amendable Generation for Dialogue State Tracking</a></strong><br><a href=/people/x/xin-tian/>Xin Tian</a>
|
<a href=/people/l/liankai-huang/>Liankai Huang</a>
|
<a href=/people/y/yingzhan-lin/>Yingzhan Lin</a>
|
<a href=/people/s/siqi-bao/>Siqi Bao</a>
|
<a href=/people/h/huang-he/>Huang He</a>
|
<a href=/people/y/yunyi-yang/>Yunyi Yang</a>
|
<a href=/people/h/hua-wu/>Hua Wu</a>
|
<a href=/people/f/fan-wang/>Fan Wang</a>
|
<a href=/people/s/shuqi-sun/>Shuqi Sun</a><br><a href=/volumes/2021.nlp4convai-1/ class=text-muted>Proceedings of the 3rd Workshop on Natural Language Processing for Conversational AI</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--nlp4convai-1--8><div class="card-body p-3 small">In task-oriented dialogue systems, recent dialogue state tracking methods tend to perform one-pass generation of the dialogue state based on the previous dialogue state. The mistakes of these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> made at the current turn are prone to be carried over to the next turn, causing <a href=https://en.wikipedia.org/wiki/Error_propagation>error propagation</a>. In this paper, we propose a novel Amendable Generation for Dialogue State Tracking (AG-DST), which contains a two-pass generation process : (1) generating a primitive dialogue state based on the dialogue of the current turn and the previous dialogue state, and (2) amending the primitive dialogue state from the first pass. With the additional amending generation pass, our model is tasked to learn more robust dialogue state tracking by amending the errors that still exist in the primitive dialogue state, which plays the role of reviser in the double-checking process and alleviates unnecessary error propagation. Experimental results show that AG-DST significantly outperforms previous works in two active DST datasets (MultiWOZ 2.2 and WOZ 2.0), achieving new state-of-the-art performances.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.nlp4convai-1.14.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--nlp4convai-1--14 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.nlp4convai-1.14 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.nlp4convai-1.14/>PLATO-KAG : Unsupervised Knowledge-Grounded Conversation via Joint Modeling<span class=acl-fixed-case>PLATO</span>-<span class=acl-fixed-case>KAG</span>: Unsupervised Knowledge-Grounded Conversation via Joint Modeling</a></strong><br><a href=/people/x/xinxian-huang/>Xinxian Huang</a>
|
<a href=/people/h/huang-he/>Huang He</a>
|
<a href=/people/s/siqi-bao/>Siqi Bao</a>
|
<a href=/people/f/fan-wang/>Fan Wang</a>
|
<a href=/people/h/hua-wu/>Hua Wu</a>
|
<a href=/people/h/haifeng-wang/>Haifeng Wang</a><br><a href=/volumes/2021.nlp4convai-1/ class=text-muted>Proceedings of the 3rd Workshop on Natural Language Processing for Conversational AI</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--nlp4convai-1--14><div class="card-body p-3 small">Large-scale conversation models are turning to leveraging <a href=https://en.wikipedia.org/wiki/Knowledge>external knowledge</a> to improve the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>factual accuracy</a> in response generation. Considering the infeasibility to annotate the external knowledge for large-scale dialogue corpora, it is desirable to learn the knowledge selection and response generation in an unsupervised manner. In this paper, we propose PLATO-KAG (Knowledge-Augmented Generation), an unsupervised learning approach for end-to-end knowledge-grounded conversation modeling. For each dialogue context, the top-k relevant knowledge elements are selected and then employed in knowledge-grounded response generation. The two components of knowledge selection and response generation are optimized jointly and effectively under a balanced objective. Experimental results on two publicly available datasets validate the superiority of PLATO-KAG.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.224.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--224 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.224 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.224" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.224/>RocketQAv2 : A Joint Training Method for Dense Passage Retrieval and Passage Re-ranking<span class=acl-fixed-case>R</span>ocket<span class=acl-fixed-case>QA</span>v2: A Joint Training Method for Dense Passage Retrieval and Passage Re-ranking</a></strong><br><a href=/people/r/ruiyang-ren/>Ruiyang Ren</a>
|
<a href=/people/y/yingqi-qu/>Yingqi Qu</a>
|
<a href=/people/j/jing-liu/>Jing Liu</a>
|
<a href=/people/w/wayne-xin-zhao/>Wayne Xin Zhao</a>
|
<a href=/people/q/qiaoqiao-she/>QiaoQiao She</a>
|
<a href=/people/h/hua-wu/>Hua Wu</a>
|
<a href=/people/h/haifeng-wang/>Haifeng Wang</a>
|
<a href=/people/j/ji-rong-wen/>Ji-Rong Wen</a><br><a href=/volumes/2021.emnlp-main/ class=text-muted>Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--224><div class="card-body p-3 small">In various <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing tasks</a>, passage retrieval and passage re-ranking are two key procedures in finding and ranking relevant information. Since both the two <a href=https://en.wikipedia.org/wiki/Procedure_(term)>procedures</a> contribute to the final performance, it is important to jointly optimize them in order to achieve mutual improvement. In this paper, we propose a novel joint training approach for dense passage retrieval and passage reranking. A major contribution is that we introduce the dynamic listwise distillation, where we design a unified listwise training approach for both the retriever and the re-ranker. During the dynamic distillation, the retriever and the re-ranker can be adaptively improved according to each other&#8217;s relevance information. We also propose a hybrid data augmentation strategy to construct diverse training instances for listwise training approach. Extensive experiments show the effectiveness of our approach on both MSMARCO and Natural Questions datasets. Our code is available at https://github.com/PaddlePaddle/RocketQA.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.356.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--356 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.356 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.356/>DuRecDial 2.0 : A Bilingual Parallel Corpus for Conversational Recommendation<span class=acl-fixed-case>D</span>u<span class=acl-fixed-case>R</span>ec<span class=acl-fixed-case>D</span>ial 2.0: A Bilingual Parallel Corpus for Conversational Recommendation</a></strong><br><a href=/people/z/zeming-liu/>Zeming Liu</a>
|
<a href=/people/h/haifeng-wang/>Haifeng Wang</a>
|
<a href=/people/z/zheng-yu-niu/>Zheng-Yu Niu</a>
|
<a href=/people/h/hua-wu/>Hua Wu</a>
|
<a href=/people/w/wanxiang-che/>Wanxiang Che</a><br><a href=/volumes/2021.emnlp-main/ class=text-muted>Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--356><div class="card-body p-3 small">In this paper, we provide a bilingual parallel human-to-human recommendation dialog dataset (DuRecDial 2.0) to enable researchers to explore a challenging task of multilingual and cross-lingual conversational recommendation. The difference between DuRecDial 2.0 and existing conversational recommendation datasets is that the data item (Profile, Goal, Knowledge, Context, Response) in DuRecDial 2.0 is annotated in two languages, both <a href=https://en.wikipedia.org/wiki/English_language>English</a> and <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a>, while other datasets are built with the setting of a single language. We collect 8.2k dialogs aligned across English and Chinese languages (16.5k dialogs and 255k utterances in total) that are annotated by crowdsourced workers with strict quality control procedure. We then build monolingual, multilingual, and cross-lingual conversational recommendation baselines on DuRecDial 2.0. Experiment results show that the use of additional English data can bring performance improvement for Chinese conversational recommendation, indicating the benefits of DuRecDial 2.0. Finally, this dataset provides a challenging testbed for future studies of monolingual, multilingual, and cross-lingual conversational recommendation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.136.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--136 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.136 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.naacl-main.136" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.136/>ERNIE-Gram : Pre-Training with Explicitly N-Gram Masked Language Modeling for <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>Natural Language Understanding</a><span class=acl-fixed-case>ERNIE</span>-Gram: Pre-Training with Explicitly N-Gram Masked Language Modeling for Natural Language Understanding</a></strong><br><a href=/people/d/dongling-xiao/>Dongling Xiao</a>
|
<a href=/people/y/yu-kun-li/>Yu-Kun Li</a>
|
<a href=/people/h/han-zhang/>Han Zhang</a>
|
<a href=/people/y/yu-sun/>Yu Sun</a>
|
<a href=/people/h/hao-tian/>Hao Tian</a>
|
<a href=/people/h/hua-wu/>Hua Wu</a>
|
<a href=/people/h/haifeng-wang/>Haifeng Wang</a><br><a href=/volumes/2021.naacl-main/ class=text-muted>Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--136><div class="card-body p-3 small">Coarse-grained linguistic information, such as named entities or phrases, facilitates adequately representation learning in pre-training. Previous works mainly focus on extending the objective of BERT&#8217;s Masked Language Modeling (MLM) from masking individual tokens to contiguous sequences of n tokens. We argue that such contiguously masking method neglects to model the intra-dependencies and inter-relation of coarse-grained linguistic information. As an alternative, we propose ERNIE-Gram, an explicitly n-gram masking method to enhance the integration of coarse-grained information into pre-training. In ERNIE-Gram, <a href=https://en.wikipedia.org/wiki/N-gram>n-grams</a> are masked and predicted directly using explicit n-gram identities rather than contiguous sequences of n tokens. Furthermore, ERNIE-Gram employs a generator model to sample plausible n-gram identities as optional n-gram masks and predict them in both coarse-grained and fine-grained manners to enable comprehensive n-gram prediction and relation modeling. We pre-train ERNIE-Gram on English and Chinese text corpora and fine-tune on 19 downstream tasks. Experimental results show that ERNIE-Gram outperforms previous pre-training models like XLNet and RoBERTa by a large margin, and achieves comparable results with state-of-the-art methods. The source codes and pre-trained models have been released at https://github.com/PaddlePaddle/ERNIE.</div></div><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.562.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--562 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.562 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938688 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.562/>DuSQL : A Large-Scale and Pragmatic Chinese Text-to-SQL Dataset<span class=acl-fixed-case>D</span>u<span class=acl-fixed-case>SQL</span>: A Large-Scale and Pragmatic <span class=acl-fixed-case>C</span>hinese Text-to-<span class=acl-fixed-case>SQL</span> Dataset</a></strong><br><a href=/people/l/lijie-wang/>Lijie Wang</a>
|
<a href=/people/a/ao-zhang/>Ao Zhang</a>
|
<a href=/people/k/kun-wu/>Kun Wu</a>
|
<a href=/people/k/ke-sun/>Ke Sun</a>
|
<a href=/people/z/zhenghua-li/>Zhenghua Li</a>
|
<a href=/people/h/hua-wu/>Hua Wu</a>
|
<a href=/people/m/min-zhang/>Min Zhang</a>
|
<a href=/people/h/haifeng-wang/>Haifeng Wang</a><br><a href=/volumes/2020.emnlp-main/ class=text-muted>Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--562><div class="card-body p-3 small">Due to the lack of <a href=https://en.wikipedia.org/wiki/Data>labeled data</a>, previous research on text-to-SQL parsing mainly focuses on <a href=https://en.wikipedia.org/wiki/English_language>English</a>. Representative English datasets include ATIS, WikiSQL, Spider, etc. This paper presents DuSQL, a larges-scale and pragmatic Chinese dataset for the cross-domain text-to-SQL task, containing 200 databases, 813 tables, and 23,797 question / SQL pairs. Our new <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> has three major characteristics. First, by manually analyzing questions from several representative applications, we try to figure out the true distribution of SQL queries in real-life needs. Second, DuSQL contains a considerable proportion of SQL queries involving row or column calculations, motivated by our analysis on the SQL query distributions. Finally, we adopt an effective data construction framework via <a href=https://en.wikipedia.org/wiki/Human&#8211;computer_interaction>human-computer collaboration</a>. The basic idea is automatically generating <a href=https://en.wikipedia.org/wiki/SQL>SQL queries</a> based on the SQL grammar and constrained by the given database. This paper describes in detail the construction process and <a href=https://en.wikipedia.org/wiki/Data_analysis>data statistics</a> of DuSQL. Moreover, we present and compare performance of several open-source text-to-SQL parsers with minor modification to accommodate <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a>, including a simple yet effective extension to IRNet for handling calculation SQL queries.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.acl-main.374.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--acl-main--374 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.acl-main.374 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://slideslive.com/38929227 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.acl-main.374" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.acl-main.374/>SKEP : Sentiment Knowledge Enhanced Pre-training for <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>Sentiment Analysis</a><span class=acl-fixed-case>SKEP</span>: Sentiment Knowledge Enhanced Pre-training for Sentiment Analysis</a></strong><br><a href=/people/h/hao-tian/>Hao Tian</a>
|
<a href=/people/c/can-gao/>Can Gao</a>
|
<a href=/people/x/xinyan-xiao/>Xinyan Xiao</a>
|
<a href=/people/h/hao-liu/>Hao Liu</a>
|
<a href=/people/b/bolei-he/>Bolei He</a>
|
<a href=/people/h/hua-wu/>Hua Wu</a>
|
<a href=/people/h/haifeng-wang/>Haifeng Wang</a>
|
<a href=/people/f/feng-wu/>Feng Wu</a><br><a href=/volumes/2020.acl-main/ class=text-muted>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--acl-main--374><div class="card-body p-3 small">Recently, <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> has seen remarkable advance with the help of pre-training approaches. However, sentiment knowledge, such as sentiment words and aspect-sentiment pairs, is ignored in the process of pre-training, despite the fact that they are widely used in traditional sentiment analysis approaches. In this paper, we introduce Sentiment Knowledge Enhanced Pre-training (SKEP) in order to learn a unified sentiment representation for multiple sentiment analysis tasks. With the help of automatically-mined knowledge, SKEP conducts sentiment masking and constructs three sentiment knowledge prediction objectives, so as to embed sentiment information at the word, polarity and aspect level into pre-trained sentiment representation. In particular, the prediction of aspect-sentiment pairs is converted into <a href=https://en.wikipedia.org/wiki/Multi-label_classification>multi-label classification</a>, aiming to capture the dependency between words in a pair. Experiments on three kinds of sentiment tasks show that SKEP significantly outperforms strong pre-training baseline, and achieves new state-of-the-art results on most of the test datasets. We release our code at https://github.com/baidu/Senta.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.autosimtrans-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.autosimtrans-1.0/>Proceedings of the First Workshop on Automatic Simultaneous Translation</a></strong><br><a href=/people/h/hua-wu/>Hua Wu</a>
|
<a href=/people/c/collin-cherry/>Collin Cherry</a>
|
<a href=/people/l/liang-huang/>Liang Huang</a>
|
<a href=/people/z/zhongjun-he/>Zhongjun He</a>
|
<a href=/people/m/mark-liberman/>Mark Liberman</a>
|
<a href=/people/j/james-cross/>James Cross</a>
|
<a href=/people/y/yang-liu-ict/>Yang Liu</a><br><a href=/volumes/2020.autosimtrans-1/ class=text-muted>Proceedings of the First Workshop on Automatic Simultaneous Translation</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.aacl-main.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.aacl-main.0/>Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing</a></strong><br><a href=/people/k/kam-fai-wong/>Kam-Fai Wong</a>
|
<a href=/people/k/kevin-knight/>Kevin Knight</a>
|
<a href=/people/h/hua-wu/>Hua Wu</a><br><a href=/volumes/2020.aacl-main/ class=text-muted>Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.69.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--69 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.69 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.findings-emnlp.69.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.69" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.69/>Syntactic and Semantic-driven Learning for Open Information Extraction</a></strong><br><a href=/people/j/jialong-tang/>Jialong Tang</a>
|
<a href=/people/y/yaojie-lu/>Yaojie Lu</a>
|
<a href=/people/h/hongyu-lin/>Hongyu Lin</a>
|
<a href=/people/x/xianpei-han/>Xianpei Han</a>
|
<a href=/people/l/le-sun/>Le Sun</a>
|
<a href=/people/x/xinyan-xiao/>Xinyan Xiao</a>
|
<a href=/people/h/hua-wu/>Hua Wu</a><br><a href=/volumes/2020.findings-emnlp/ class=text-muted>Findings of the Association for Computational Linguistics: EMNLP 2020</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--69><div class="card-body p-3 small">One of the biggest bottlenecks in building accurate, high coverage neural open IE systems is the need for large labelled corpora. The diversity of <a href=https://en.wikipedia.org/wiki/Open_domain>open domain corpora</a> and the variety of <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language expressions</a> further exacerbate this problem. In this paper, we propose a syntactic and semantic-driven learning approach, which can learn neural open IE models without any human-labelled data by leveraging syntactic and semantic knowledge as noisier, higher-level supervision. Specifically, we first employ syntactic patterns as data labelling functions and pretrain a base model using the generated labels. Then we propose a syntactic and semantic-driven reinforcement learning algorithm, which can effectively generalize the base model to open situations with high <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>. Experimental results show that our approach significantly outperforms the supervised counterparts, and can even achieve competitive performance to supervised state-of-the-art (SoA) model.</div></div><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1079.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1079 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1079 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1079/>Multi-agent Learning for Neural Machine Translation</a></strong><br><a href=/people/t/tianchi-bi/>Tianchi Bi</a>
|
<a href=/people/h/hao-xiong/>Hao Xiong</a>
|
<a href=/people/z/zhongjun-he/>Zhongjun He</a>
|
<a href=/people/h/hua-wu/>Hua Wu</a>
|
<a href=/people/h/haifeng-wang/>Haifeng Wang</a><br><a href=/volumes/D19-1/ class=text-muted>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1079><div class="card-body p-3 small">Conventional Neural Machine Translation (NMT) models benefit from the training with an additional <a href=https://en.wikipedia.org/wiki/Intelligent_agent>agent</a>, e.g., dual learning, and bidirectional decoding with one agent decod- ing from left to right and the other decoding in the opposite direction. In this paper, we extend the <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training framework</a> to the multi-agent sce- nario by introducing diverse agents in an in- teractive updating process. At training time, each agent learns advanced knowledge from others, and they work together to improve translation quality. Experimental results on NIST Chinese-English, IWSLT 2014 German- English, WMT 2014 English-German and large-scale Chinese-English translation tasks indicate that our approach achieves absolute improvements over the strong baseline sys- tems and shows competitive performance on all tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1187.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1187 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1187 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1187" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1187/>Knowledge Aware Conversation Generation with Explainable Reasoning over Augmented Graphs</a></strong><br><a href=/people/z/zhibin-liu/>Zhibin Liu</a>
|
<a href=/people/z/zheng-yu-niu/>Zheng-Yu Niu</a>
|
<a href=/people/h/hua-wu/>Hua Wu</a>
|
<a href=/people/h/haifeng-wang/>Haifeng Wang</a><br><a href=/volumes/D19-1/ class=text-muted>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1187><div class="card-body p-3 small">Two types of <a href=https://en.wikipedia.org/wiki/Knowledge>knowledge</a>, triples from <a href=https://en.wikipedia.org/wiki/Knowledge>knowledge graphs</a> and texts from documents, have been studied for <a href=https://en.wikipedia.org/wiki/Knowledge>knowledge aware open domain conversation generation</a>, in which graph paths can narrow down vertex candidates for <a href=https://en.wikipedia.org/wiki/Knowledge>knowledge selection decision</a>, and texts can provide rich information for response generation. Fusion of a <a href=https://en.wikipedia.org/wiki/Knowledge_graph>knowledge graph</a> and texts might yield mutually reinforcing advantages, but there is less study on that. To address this challenge, we propose a knowledge aware chatting machine with three components, an augmented knowledge graph with both triples and texts, knowledge selector, and knowledge aware response generator. For knowledge selection on the <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph</a>, we formulate it as a problem of multi-hop graph reasoning to effectively capture conversation flow, which is more explainable and flexible in comparison with previous works. To fully leverage long text information that differentiates our <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph</a> from others, we improve a state of the art <a href=https://en.wikipedia.org/wiki/Automated_reasoning>reasoning algorithm</a> with machine reading comprehension technology. We demonstrate the effectiveness of our <a href=https://en.wikipedia.org/wiki/System>system</a> on two <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> in comparison with state-of-the-art models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5828.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5828 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5828 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5828/>D-NET : A Pre-Training and Fine-Tuning Framework for Improving the Generalization of Machine Reading Comprehension<span class=acl-fixed-case>D</span>-<span class=acl-fixed-case>NET</span>: A Pre-Training and Fine-Tuning Framework for Improving the Generalization of Machine Reading Comprehension</a></strong><br><a href=/people/h/hongyu-li/>Hongyu Li</a>
|
<a href=/people/x/xiyuan-zhang/>Xiyuan Zhang</a>
|
<a href=/people/y/yibing-liu/>Yibing Liu</a>
|
<a href=/people/y/yiming-zhang/>Yiming Zhang</a>
|
<a href=/people/q/quan-wang/>Quan Wang</a>
|
<a href=/people/x/xiangyang-zhou/>Xiangyang Zhou</a>
|
<a href=/people/j/jing-liu/>Jing Liu</a>
|
<a href=/people/h/hua-wu/>Hua Wu</a>
|
<a href=/people/h/haifeng-wang/>Haifeng Wang</a><br><a href=/volumes/D19-58/ class=text-muted>Proceedings of the 2nd Workshop on Machine Reading for Question Answering</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5828><div class="card-body p-3 small">In this paper, we introduce a simple system Baidu submitted for MRQA (Machine Reading for Question Answering) 2019 Shared Task that focused on generalization of machine reading comprehension (MRC) models. Our <a href=https://en.wikipedia.org/wiki/System>system</a> is built on a framework of pretraining and fine-tuning, namely D-NET. The techniques of pre-trained language models and <a href=https://en.wikipedia.org/wiki/Multi-task_learning>multi-task learning</a> are explored to improve the generalization of MRC models and we conduct experiments to examine the effectiveness of these strategies. Our <a href=https://en.wikipedia.org/wiki/System>system</a> is ranked at top 1 of all the participants in terms of averaged F1 score. Our codes and models will be released at PaddleNLP.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1135.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1135 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1135 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-1135/>ARNOR : Attention Regularization based Noise Reduction for Distant Supervision Relation Classification<span class=acl-fixed-case>ARNOR</span>: Attention Regularization based Noise Reduction for Distant Supervision Relation Classification</a></strong><br><a href=/people/w/wei-jia/>Wei Jia</a>
|
<a href=/people/d/dai-dai/>Dai Dai</a>
|
<a href=/people/x/xinyan-xiao/>Xinyan Xiao</a>
|
<a href=/people/h/hua-wu/>Hua Wu</a><br><a href=/volumes/P19-1/ class=text-muted>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1135><div class="card-body p-3 small">Distant supervision is widely used in relation classification in order to create large-scale training data by aligning a <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge base</a> with an unlabeled corpus. However, it also introduces amounts of noisy labels where a contextual sentence actually does not express the labeled relation. In this paper, we propose ARNOR, a novel Attention Regularization based NOise Reduction framework for distant supervision relation classification. ARNOR assumes that a trustable relation label should be explained by the neural attention model. Specifically, our ARNOR framework iteratively learns an interpretable model and utilizes it to select trustable instances. We first introduce attention regularization to force the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> to pay attention to the patterns which explain the relation labels, so as to make the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> more interpretable. Then, if the learned model can clearly locate the relation patterns of a candidate instance in the training set, we will select it as a trustable instance for further training step. According to the experiments on NYT data, our ARNOR framework achieves significant improvements over state-of-the-art methods in both relation classification performance and noise reduction effect.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1226.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1226 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1226 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-1226/>Enhancing Pre-Trained Language Representations with Rich Knowledge for Machine Reading Comprehension</a></strong><br><a href=/people/a/an-yang/>An Yang</a>
|
<a href=/people/q/quan-wang/>Quan Wang</a>
|
<a href=/people/j/jing-liu/>Jing Liu</a>
|
<a href=/people/k/kai-liu/>Kai Liu</a>
|
<a href=/people/y/yajuan-lyu/>Yajuan Lyu</a>
|
<a href=/people/h/hua-wu/>Hua Wu</a>
|
<a href=/people/q/qiaoqiao-she/>Qiaoqiao She</a>
|
<a href=/people/s/sujian-li/>Sujian Li</a><br><a href=/volumes/P19-1/ class=text-muted>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1226><div class="card-body p-3 small">Machine reading comprehension (MRC) is a crucial and challenging task in <a href=https://en.wikipedia.org/wiki/Neuro-linguistic_programming>NLP</a>. Recently, pre-trained language models (LMs), especially BERT, have achieved remarkable success, presenting new state-of-the-art results in <a href=https://en.wikipedia.org/wiki/Machine_learning>MRC</a>. In this work, we investigate the potential of leveraging external knowledge bases (KBs) to further improve BERT for <a href=https://en.wikipedia.org/wiki/Medical_record>MRC</a>. We introduce KT-NET, which employs an attention mechanism to adaptively select desired knowledge from KBs, and then fuses selected knowledge with BERT to enable context- and knowledge-aware predictions. We believe this would combine the merits of both deep LMs and curated KBs towards better MRC. Experimental results indicate that KT-NET offers significant and consistent improvements over BERT, outperforming competitive <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>baselines</a> on ReCoRD and SQuAD1.1 benchmarks. Notably, it ranks the 1st place on the ReCoRD leaderboard, and is also the best single <a href=https://en.wikipedia.org/wiki/Computer_model>model</a> on the SQuAD1.1 leaderboard at the time of submission (March 4th, 2019).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1369.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1369 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1369 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-1369/>Proactive Human-Machine Conversation with Explicit Conversation Goal</a></strong><br><a href=/people/w/wenquan-wu/>Wenquan Wu</a>
|
<a href=/people/z/zhen-guo/>Zhen Guo</a>
|
<a href=/people/x/xiangyang-zhou/>Xiangyang Zhou</a>
|
<a href=/people/h/hua-wu/>Hua Wu</a>
|
<a href=/people/x/xiyuan-zhang/>Xiyuan Zhang</a>
|
<a href=/people/r/rongzhong-lian/>Rongzhong Lian</a>
|
<a href=/people/h/haifeng-wang/>Haifeng Wang</a><br><a href=/volumes/P19-1/ class=text-muted>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1369><div class="card-body p-3 small">Though great progress has been made for human-machine conversation, current dialogue system is still in its infancy : it usually converses passively and utters words more as a matter of response, rather than on its own initiatives. In this paper, we take a radical step towards building a human-like conversational agent : endowing it with the ability of proactively leading the conversation (introducing a new topic or maintaining the current topic). To facilitate the development of such conversation systems, we create a new <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> named Konv where one acts as a conversation leader and the other acts as the follower. The leader is provided with a <a href=https://en.wikipedia.org/wiki/Knowledge_graph>knowledge graph</a> and asked to sequentially change the discussion topics, following the given conversation goal, and meanwhile keep the dialogue as natural and engaging as possible. Konv enables a very challenging task as the model needs to both understand dialogue and plan over the given <a href=https://en.wikipedia.org/wiki/Knowledge_graph>knowledge graph</a>. We establish baseline results on this <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> (about 270 K utterances and 30k dialogues) using several state-of-the-art models. Experimental results show that dialogue models that plan over the knowledge graph can make full use of related knowledge to generate more diverse multi-turn conversations. The <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline systems</a> along with the <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> are publicly available.</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1036.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1036 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1036 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-1036/>Addressing Troublesome Words in <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a></a></strong><br><a href=/people/y/yang-zhao/>Yang Zhao</a>
|
<a href=/people/j/jiajun-zhang/>Jiajun Zhang</a>
|
<a href=/people/z/zhongjun-he/>Zhongjun He</a>
|
<a href=/people/c/chengqing-zong/>Chengqing Zong</a>
|
<a href=/people/h/hua-wu/>Hua Wu</a><br><a href=/volumes/D18-1/ class=text-muted>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1036><div class="card-body p-3 small">One of the weaknesses of Neural Machine Translation (NMT) is in handling lowfrequency and ambiguous words, which we refer as troublesome words. To address this problem, we propose a novel memoryenhanced NMT method. First, we investigate different <a href=https://en.wikipedia.org/wiki/Strategy>strategies</a> to define and detect the troublesome words. Then, a contextual memory is constructed to memorize which target words should be produced in what situations. Finally, we design a hybrid model to dynamically access the contextual memory so as to correctly translate the troublesome words. The extensive experiments on Chinese-to-English and English-to-German translation tasks demonstrate that our method significantly outperforms the strong baseline models in translation quality, especially in handling troublesome words.</div></div><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1021.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1021 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1021 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-1021/>An End-to-End Model for Question Answering over <a href=https://en.wikipedia.org/wiki/Knowledge_base>Knowledge Base</a> with Cross-Attention Combining Global Knowledge</a></strong><br><a href=/people/y/yanchao-hao/>Yanchao Hao</a>
|
<a href=/people/y/yuanzhe-zhang/>Yuanzhe Zhang</a>
|
<a href=/people/k/kang-liu/>Kang Liu</a>
|
<a href=/people/s/shizhu-he/>Shizhu He</a>
|
<a href=/people/z/zhanyi-liu/>Zhanyi Liu</a>
|
<a href=/people/h/hua-wu/>Hua Wu</a>
|
<a href=/people/j/jun-zhao/>Jun Zhao</a><br><a href=/volumes/P17-1/ class=text-muted>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1021><div class="card-body p-3 small">With the rapid growth of knowledge bases (KBs) on the <a href=https://en.wikipedia.org/wiki/World_Wide_Web>web</a>, how to take full advantage of them becomes increasingly important. Question answering over knowledge base (KB-QA) is one of the promising approaches to access the substantial knowledge. Meanwhile, as the neural network-based (NN-based) methods develop, NN-based KB-QA has already achieved impressive results. However, previous work did not put more emphasis on question representation, and the question is converted into a fixed vector regardless of its candidate answers. This simple representation strategy is not easy to express the proper information in the question. Hence, we present an end-to-end neural network model to represent the questions and their corresponding scores dynamically according to the various candidate answer aspects via cross-attention mechanism. In addition, we leverage the global knowledge inside the underlying KB, aiming at integrating the rich KB information into the representation of the answers. As a result, it could alleviates the out-of-vocabulary (OOV) problem, which helps the cross-attention model to represent the question more precisely. The experimental results on WebQuestions demonstrate the effectiveness of the proposed approach.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Hua+Wu+%28%E5%90%B4%E5%8D%8E%29" title="Search for 'Hua Wu (吴华)' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/h/haifeng-wang/ class=align-middle>Haifeng Wang</a>
<span class="badge badge-secondary align-middle ml-2">14</span></li><li class=list-group-item><a href=/people/x/xinyan-xiao/ class=align-middle>Xinyan Xiao</a>
<span class="badge badge-secondary align-middle ml-2">5</span></li><li class=list-group-item><a href=/people/z/zhongjun-he/ class=align-middle>Zhongjun He</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/people/h/hao-tian/ class=align-middle>Hao Tian</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/j/jing-liu/ class=align-middle>Jing Liu</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/z/zheng-yu-niu/ class=align-middle>Zheng-Yu Niu</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/y/yang-liu-ict/ class=align-middle>Yang Liu</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/k/kang-liu/ class=align-middle>Kang Liu (刘康)</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/w/wanxiang-che/ class=align-middle>Wanxiang Che (车万翔)</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/s/shizhu-he/ class=align-middle>Shizhu He (何世柱)</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/l/liang-huang/ class=align-middle>Liang Huang</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/m/mark-liberman/ class=align-middle>Mark Liberman</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/y/yu-sun/ class=align-middle>Yu Sun</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/c/can-gao/ class=align-middle>Can Gao</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/h/hao-liu/ class=align-middle>Hao Liu</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/y/yaojie-lu/ class=align-middle>Yaojie Lu</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/d/dai-dai/ class=align-middle>Dai Dai</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/h/hongyu-lin/ class=align-middle>Hongyu Lin</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/x/xianpei-han/ class=align-middle>Xianpei Han</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/l/le-sun/ class=align-middle>Le Sun</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/s/siqi-bao/ class=align-middle>Siqi Bao</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/h/huang-he/ class=align-middle>Huang He</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/f/fan-wang/ class=align-middle>Fan Wang</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/q/qiaoqiao-she/ class=align-middle>Qiaoqiao She</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/x/xiyuan-zhang/ class=align-middle>Xiyuan Zhang</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/q/quan-wang/ class=align-middle>Quan Wang</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/x/xiangyang-zhou/ class=align-middle>Xiangyang Zhou</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/w/wenquan-wu/ class=align-middle>Wenquan Wu</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/s/sheng-li/ class=align-middle>Sheng Li (李生)</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/maosong-sun/ class=align-middle>Maosong Sun (孙茂松)</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/g/gaoqi-rao/ class=align-middle>Gaoqi Rao (饶高琦)</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/colin-cherry/ class=align-middle>Colin Cherry</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/q/qun-liu/ class=align-middle>Qun Liu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/maha-elbayad/ class=align-middle>Maha Elbayad</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/mingbo-ma/ class=align-middle>Mingbo Ma</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/ruiqing-zhang/ class=align-middle>Ruiqing Zhang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/siyu-ding/ class=align-middle>SiYu Ding</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/junyuan-shang/ class=align-middle>Junyuan Shang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/shuohuan-wang/ class=align-middle>Shuohuan Wang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/lijie-wang/ class=align-middle>Lijie Wang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/ao-zhang/ class=align-middle>Ao Zhang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/kun-wu/ class=align-middle>Kun Wu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/ke-sun/ class=align-middle>Ke Sun</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/zhenghua-li/ class=align-middle>Zhenghua Li</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/min-zhang/ class=align-middle>Min Zhang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/b/bolei-he/ class=align-middle>Bolei He</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/f/feng-wu/ class=align-middle>Feng Wu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yanchao-hao/ class=align-middle>Yanchao Hao</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yuanzhe-zhang/ class=align-middle>Yuanzhe Zhang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/zhanyi-liu/ class=align-middle>Zhanyi Liu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jun-zhao/ class=align-middle>Jun Zhao</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/q/qing-liu/ class=align-middle>Qing Liu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/collin-cherry/ class=align-middle>Collin Cherry</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/james-cross/ class=align-middle>James Cross</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/kam-fai-wong/ class=align-middle>Kam-Fai Wong</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/kevin-knight/ class=align-middle>Kevin Knight</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/x/xin-tian/ class=align-middle>Xin Tian</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/liankai-huang/ class=align-middle>Liankai Huang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yingzhan-lin/ class=align-middle>Yingzhan Lin</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yunyi-yang/ class=align-middle>Yunyi Yang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/shuqi-sun/ class=align-middle>Shuqi Sun</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/x/xinxian-huang/ class=align-middle>Xinxian Huang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yang-zhao/ class=align-middle>Yang Zhao</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jiajun-zhang/ class=align-middle>Jiajun Zhang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/chengqing-zong/ class=align-middle>Chengqing Zong</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/ruiyang-ren/ class=align-middle>Ruiyang Ren</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yingqi-qu/ class=align-middle>Yingqi Qu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/w/wayne-xin-zhao/ class=align-middle>Wayne Xin Zhao</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/ji-rong-wen/ class=align-middle>Ji-Rong Wen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/zeming-liu/ class=align-middle>Zeming Liu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/tianchi-bi/ class=align-middle>Tianchi Bi</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/hao-xiong/ class=align-middle>Hao Xiong</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/zhibin-liu/ class=align-middle>Zhibin Liu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/hongyu-li/ class=align-middle>Hongyu Li</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yibing-liu/ class=align-middle>Yibing Liu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yiming-zhang/ class=align-middle>Yiming Zhang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/x/xinchao-xu/ class=align-middle>Xinchao Xu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/zhibin-gou/ class=align-middle>Zhibin Gou</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/shihang-wang/ class=align-middle>Shihang Wang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/w/wei-li/ class=align-middle>Wei Li</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/g/guocheng-niu/ class=align-middle>Guocheng Niu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jiachen-liu/ class=align-middle>Jiachen Liu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/dongling-xiao/ class=align-middle>Dongling Xiao</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yu-kun-li/ class=align-middle>Yu-Kun Li</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/han-zhang/ class=align-middle>Han Zhang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jialong-tang/ class=align-middle>Jialong Tang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/w/wei-jia/ class=align-middle>Wei Jia</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/an-yang/ class=align-middle>An Yang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/kai-liu/ class=align-middle>Kai Liu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yajuan-lyu/ class=align-middle>Yajuan Lyu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/sujian-li/ class=align-middle>Sujian Li</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/zhen-guo/ class=align-middle>Zhen Guo</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/rongzhong-lian/ class=align-middle>Rongzhong Lian</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/acl/ class=align-middle>ACL</a><span class="badge badge-secondary align-middle ml-2">7</span></li><li class=list-group-item><a href=/venues/emnlp/ class=align-middle>EMNLP</a><span class="badge badge-secondary align-middle ml-2">7</span></li><li class=list-group-item><a href=/venues/findings/ class=align-middle>Findings</a><span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/venues/autosimtrans/ class=align-middle>AutoSimTrans</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/nlp4convai/ class=align-middle>NLP4ConvAI</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-venues aria-expanded=false aria-controls=more-venues>show all...</li><div class="collapse border-top" id=more-venues><li class=list-group-item><a href=/venues/ccl/ class=align-middle>CCL</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/aacl/ class=align-middle>AACL</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/naacl/ class=align-middle>NAACL</a><span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>