<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Hai Zhao - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Hai</span> <span class=font-weight-bold>Zhao</span></h2><hr><div class=row><div class=col-lg-9><h4>2022</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.acl-long.91.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--acl-long--91 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.acl-long.91 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.acl-long.91" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2022.acl-long.91/>Tracing Origins: Coreference-aware Machine Reading Comprehension</a></strong><br><a href=/people/z/zhuosheng-zhang/>Zhuosheng Zhang</a>
|
<a href=/people/h/hai-zhao/>Hai Zhao</a><br><a href=/volumes/2022.acl-long/ class=text-muted>Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--acl-long--91><div class="card-body p-3 small">Machine reading comprehension is a heavily-studied research and test field for evaluating new pre-trained language models (PrLMs) and fine-tuning strategies, and recent studies have enriched the pre-trained language models with syntactic, semantic and other linguistic information to improve the performance of the models. In this paper, we imitate the human reading process in connecting the anaphoric expressions and explicitly leverage the coreference information of the entities to enhance the word embeddings from the pre-trained language model, in order to highlight the coreference mentions of the entities that must be identified for coreference-intensive question answering in QUOREF, a relatively new dataset that is specifically designed to evaluate the coreference-related performance of a model. We use two strategies to fine-tune a pre-trained language model, namely, placing an additional encoder layer after a pre-trained language model to focus on the coreference mentions or constructing a relational graph convolutional network to model the coreference relations. We demonstrate that the explicit incorporation of coreference information in the fine-tuning stage performs better than the incorporation of the coreference information in pre-training a language model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-acl.73.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-acl--73 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-acl.73 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.findings-acl.73" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2022.findings-acl.73/>Distinguishing Non-natural from Natural Adversarial Samples for More Robust Pre-trained Language Model</a></strong><br><a href=/people/j/jiayi-wang/>Jiayi Wang</a>
|
<a href=/people/r/rongzhou-bao/>Rongzhou Bao</a>
|
<a href=/people/z/zhuosheng-zhang/>Zhuosheng Zhang</a>
|
<a href=/people/h/hai-zhao/>Hai Zhao</a><br><a href=/volumes/2022.findings-acl/ class=text-muted>Findings of the Association for Computational Linguistics: ACL 2022</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-acl--73><div class="card-body p-3 small">Recently, the problem of robustness of pre-trained language models (PrLMs) has received increasing research interest. Latest studies on adversarial attacks achieve high attack success rates against PrLMs, claiming that PrLMs are not robust. However, we find that the adversarial samples that PrLMs fail are mostly non-natural and do not appear in reality. We question the validity of the current evaluation of robustness of PrLMs based on these non-natural adversarial samples and propose an anomaly detector to evaluate the robustness of PrLMs with more natural adversarial samples. We also investigate two applications of the anomaly detector: (1) In data augmentation, we employ the anomaly detector to force generating augmented data that are distinguished as non-natural, which brings larger gains to the accuracy of PrLMs. (2) We apply the anomaly detector to a defense framework to enhance the robustness of PrLMs. It can be used to defend all types of attacks and achieves higher accuracy on both adversarial samples and compliant samples than other defense frameworks.</div></div><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.398.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--398 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.398 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.398/>Pre-training Universal Language Representation</a></strong><br><a href=/people/y/yian-li/>Yian Li</a>
|
<a href=/people/h/hai-zhao/>Hai Zhao</a><br><a href=/volumes/2021.acl-long/ class=text-muted>Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--398><div class="card-body p-3 small">Despite the well-developed cut-edge representation learning for language, most language representation models usually focus on specific levels of linguistic units. This work introduces universal language representation learning, i.e., embeddings of different levels of linguistic units or text with quite diverse lengths in a <a href=https://en.wikipedia.org/wiki/Uniform_space>uniform vector space</a>. We propose the training objective MiSAD that utilizes meaningful n-grams extracted from large unlabeled corpus by a simple but effective <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> for pre-trained language models. Then we empirically verify that well designed pre-training scheme may effectively yield universal language representation, which will bring great convenience when handling multiple layers of linguistic objects in a unified way. Especially, our model achieves the highest accuracy on analogy tasks in different language levels and significantly improves the performance on downstream tasks in the GLUE benchmark and a question answering dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wat-1.4.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wat-1--4 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wat-1.4 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wat-1.4/>NICT’s Neural Machine Translation Systems for the WAT21 Restricted Translation Task<span class=acl-fixed-case>NICT</span>’s Neural Machine Translation Systems for the <span class=acl-fixed-case>WAT</span>21 Restricted Translation Task</a></strong><br><a href=/people/z/zuchao-li/>Zuchao Li</a>
|
<a href=/people/m/masao-utiyama/>Masao Utiyama</a>
|
<a href=/people/e/eiichiro-sumita/>Eiichiro Sumita</a>
|
<a href=/people/h/hai-zhao/>Hai Zhao</a><br><a href=/volumes/2021.wat-1/ class=text-muted>Proceedings of the 8th Workshop on Asian Translation (WAT2021)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wat-1--4><div class="card-body p-3 small">This paper describes our <a href=https://en.wikipedia.org/wiki/System>system</a> (Team ID : nictrb) for participating in the WAT&#8217;21 restricted machine translation task. In our submitted <a href=https://en.wikipedia.org/wiki/System>system</a>, we designed a new <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training approach</a> for restricted machine translation. By sampling from the translation target, we can solve the problem that ordinary training data does not have a restricted vocabulary. With the further help of constrained decoding in the inference phase, we achieved better results than the baseline, confirming the effectiveness of our solution. In addition, we also tried the vanilla and sparse Transformer as the backbone network of the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>, as well as <a href=https://en.wikipedia.org/wiki/Mathematical_model>model ensembling</a>, which further improved the final translation performance.</div></div><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.723.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--723 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.723 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938819 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.723/>Named Entity Recognition Only from Word Embeddings</a></strong><br><a href=/people/y/ying-luo/>Ying Luo</a>
|
<a href=/people/h/hai-zhao/>Hai Zhao</a>
|
<a href=/people/j/junlang-zhan/>Junlang Zhan</a><br><a href=/volumes/2020.emnlp-main/ class=text-muted>Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--723><div class="card-body p-3 small">Deep neural network models have helped <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>named entity recognition</a> achieve amazing performance without handcrafting features. However, existing <a href=https://en.wikipedia.org/wiki/System>systems</a> require large amounts of human annotated training data. Efforts have been made to replace <a href=https://en.wikipedia.org/wiki/Annotation>human annotations</a> with external knowledge (e.g., NE dictionary, <a href=https://en.wikipedia.org/wiki/Part-of-speech_tagging>part-of-speech tags</a>), while it is another challenge to obtain such effective resources. In this work, we propose a fully unsupervised NE recognition model which only needs to take informative clues from pre-trained word embeddings. We first apply Gaussian Hidden Markov Model and Deep Autoencoding Gaussian Mixture Model on word embeddings for entity span detection and type prediction, and then further design an instance selector based on <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning</a> to distinguish positive sentences from noisy sentences and then refine these coarse-grained annotations through neural networks. Extensive experiments on two CoNLL benchmark NER datasets (CoNLL-2003 English dataset and CoNLL-2002 Spanish dataset) demonstrate that our proposed light NE recognition model achieves remarkable performance without using any annotated lexicon or corpus.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.102.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--102 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.102 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.findings-emnlp.102.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.102" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.102/>High-order Semantic Role Labeling</a></strong><br><a href=/people/z/zuchao-li/>Zuchao Li</a>
|
<a href=/people/h/hai-zhao/>Hai Zhao</a>
|
<a href=/people/r/rui-wang/>Rui Wang</a>
|
<a href=/people/k/kevin-parnow/>Kevin Parnow</a><br><a href=/volumes/2020.findings-emnlp/ class=text-muted>Findings of the Association for Computational Linguistics: EMNLP 2020</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--102><div class="card-body p-3 small">Semantic role labeling is primarily used to identify <a href=https://en.wikipedia.org/wiki/Predicate_(grammar)>predicates</a>, <a href=https://en.wikipedia.org/wiki/Argument_(linguistics)>arguments</a>, and their <a href=https://en.wikipedia.org/wiki/Semantic_relation>semantic relationships</a>. Due to the limitations of modeling methods and the conditions of pre-identified predicates, previous work has focused on the relationships between predicates and arguments and the correlations between arguments at most, while the correlations between predicates have been neglected for a long time. High-order features and structure learning were very common in modeling such <a href=https://en.wikipedia.org/wiki/Correlation_and_dependence>correlations</a> before the neural network era. In this paper, we introduce a high-order graph structure for the neural semantic role labeling model, which enables the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> to explicitly consider not only the isolated predicate-argument pairs but also the interaction between the predicate-argument pairs. Experimental results on 7 languages of the CoNLL-2009 benchmark show that the high-order structural learning techniques are beneficial to the strong performing SRL models and further boost our baseline to achieve new state-of-the-art results.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.398.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--398 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.398 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.findings-emnlp.398.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.398" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.398/>Parsing All : Syntax and Semantics, Dependencies and Spans</a></strong><br><a href=/people/j/junru-zhou/>Junru Zhou</a>
|
<a href=/people/z/zuchao-li/>Zuchao Li</a>
|
<a href=/people/h/hai-zhao/>Hai Zhao</a><br><a href=/volumes/2020.findings-emnlp/ class=text-muted>Findings of the Association for Computational Linguistics: EMNLP 2020</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--398><div class="card-body p-3 small">Both syntactic and semantic structures are key linguistic contextual clues, in which parsing the latter has been well shown beneficial from parsing the former. However, few works ever made an attempt to let <a href=https://en.wikipedia.org/wiki/Semantic_parsing>semantic parsing</a> help syntactic parsing. As linguistic representation formalisms, both <a href=https://en.wikipedia.org/wiki/Syntax>syntax</a> and semantics may be represented in either span (constituent / phrase) or <a href=https://en.wikipedia.org/wiki/Dependency_grammar>dependency</a>, on both of which joint learning was also seldom explored. In this paper, we propose a novel joint model of syntactic and semantic parsing on both span and dependency representations, which incorporates syntactic information effectively in the encoder of <a href=https://en.wikipedia.org/wiki/Neural_network>neural network</a> and benefits from two representation formalisms in a uniform way. The experiments show that <a href=https://en.wikipedia.org/wiki/Semantics>semantics</a> and <a href=https://en.wikipedia.org/wiki/Syntax>syntax</a> can benefit each other by optimizing joint objectives. Our single model achieves new state-of-the-art or competitive results on both span and dependency semantic parsing on Propbank benchmarks and both dependency and constituent syntactic parsing on Penn Treebank.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wmt-1.22.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wmt-1--22 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wmt-1.22 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939657 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.wmt-1.22/>SJTU-NICT’s Supervised and Unsupervised Neural Machine Translation Systems for the WMT20 News Translation Task<span class=acl-fixed-case>SJTU</span>-<span class=acl-fixed-case>NICT</span>’s Supervised and Unsupervised Neural Machine Translation Systems for the <span class=acl-fixed-case>WMT</span>20 News Translation Task</a></strong><br><a href=/people/z/zuchao-li/>Zuchao Li</a>
|
<a href=/people/h/hai-zhao/>Hai Zhao</a>
|
<a href=/people/r/rui-wang/>Rui Wang</a>
|
<a href=/people/k/kehai-chen/>Kehai Chen</a>
|
<a href=/people/m/masao-utiyama/>Masao Utiyama</a>
|
<a href=/people/e/eiichiro-sumita/>Eiichiro Sumita</a><br><a href=/volumes/2020.wmt-1/ class=text-muted>Proceedings of the Fifth Conference on Machine Translation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wmt-1--22><div class="card-body p-3 small">In this paper, we introduced our joint team SJTU-NICT &#8216;s participation in the WMT 2020 machine translation shared task. In this shared task, we participated in four translation directions of three language pairs : English-Chinese, English-Polish on supervised machine translation track, German-Upper Sorbian on low-resource and unsupervised machine translation tracks. Based on different conditions of language pairs, we have experimented with diverse neural machine translation (NMT) techniques : document-enhanced NMT, XLM pre-trained language model enhanced NMT, bidirectional translation as a pre-training, reference language based UNMT, data-dependent gaussian prior objective, and BT-BLEU collaborative filtering self-training. We also used the TF-IDF algorithm to filter the training set to obtain a domain more similar set with the test set for <a href=https://en.wikipedia.org/wiki/Finetuning>finetuning</a>. In our submissions, the primary systems won the first place on <a href=https://en.wikipedia.org/wiki/English_language>English</a> to <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a>, <a href=https://en.wikipedia.org/wiki/Polish_language>Polish</a> to <a href=https://en.wikipedia.org/wiki/English_language>English</a>, and German to Upper Sorbian translation directions.</div></div><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1340.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1340 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1340 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1340" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N19-1340/>Semantic Role Labeling with Associated Memory Network</a></strong><br><a href=/people/c/chaoyu-guan/>Chaoyu Guan</a>
|
<a href=/people/y/yuhao-cheng/>Yuhao Cheng</a>
|
<a href=/people/h/hai-zhao/>Hai Zhao</a><br><a href=/volumes/N19-1/ class=text-muted>Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1340><div class="card-body p-3 small">Semantic role labeling (SRL) is a task to recognize all the predicate-argument pairs of a sentence, which has been in a performance improvement bottleneck after a series of latest works were presented. This paper proposes a novel syntax-agnostic SRL model enhanced by the proposed associated memory network (AMN), which makes use of inter-sentence attention of label-known associated sentences as a kind of <a href=https://en.wikipedia.org/wiki/Memory>memory</a> to further enhance dependency-based SRL. In detail, we use sentences and their labels from train dataset as an <a href=https://en.wikipedia.org/wiki/Association_(psychology)>associated memory cue</a> to help label the target sentence. Furthermore, we compare several associated sentences selecting strategies and label merging methods in AMN to find and utilize the label of associated sentences while attending them. By leveraging the attentive memory from known training data, Our full model reaches state-of-the-art on CoNLL-2009 benchmark datasets for syntax-agnostic setting, showing a new effective research line of SRL enhancement other than exploiting external resources such as well pre-trained language models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1230.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1230 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1230 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P19-1230.Software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1230" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-1230/>Head-Driven Phrase Structure Grammar Parsing on <a href=https://en.wikipedia.org/wiki/Penn_Treebank>Penn Treebank</a><span class=acl-fixed-case>H</span>ead-<span class=acl-fixed-case>D</span>riven <span class=acl-fixed-case>P</span>hrase <span class=acl-fixed-case>S</span>tructure <span class=acl-fixed-case>G</span>rammar Parsing on <span class=acl-fixed-case>P</span>enn <span class=acl-fixed-case>T</span>reebank</a></strong><br><a href=/people/j/junru-zhou/>Junru Zhou</a>
|
<a href=/people/h/hai-zhao/>Hai Zhao</a><br><a href=/volumes/P19-1/ class=text-muted>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1230><div class="card-body p-3 small">Head-driven phrase structure grammar (HPSG) enjoys a uniform formalism representing rich contextual syntactic and even semantic meanings. This paper makes the first attempt to formulate a simplified <a href=https://en.wikipedia.org/wiki/Head-driven_phrase_structure_grammar>HPSG</a> by integrating constituent and dependency formal representations into <a href=https://en.wikipedia.org/wiki/Head-driven_phrase_structure_grammar>head-driven phrase structure</a>. Then two parsing algorithms are respectively proposed for two converted tree representations, division span and joint span. As HPSG encodes both constituent and dependency structure information, the proposed HPSG parsers may be regarded as a sort of joint decoder for both types of structures and thus are evaluated in terms of extracted or converted constituent and dependency parsing trees. Our <a href=https://en.wikipedia.org/wiki/Parsing>parser</a> achieves new state-of-the-art performance for both parsing tasks on Penn Treebank (PTB) and Chinese Penn Treebank, verifying the effectiveness of joint learning constituent and dependency structures. In details, we report 95.84 F1 of <a href=https://en.wikipedia.org/wiki/Constituent_(linguistics)>constituent parsing</a> and 97.00 % UAS of dependency parsing on <a href=https://en.wikipedia.org/wiki/Partially_ordered_set>PTB</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K19-2004.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K19-2004 data-toggle=collapse aria-expanded=false aria-controls=abstract-K19-2004 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/K19-2004/>SJTU-NICT at MRP 2019 : Multi-Task Learning for End-to-End Uniform Semantic Graph Parsing<span class=acl-fixed-case>SJTU</span>-<span class=acl-fixed-case>NICT</span> at <span class=acl-fixed-case>MRP</span> 2019: Multi-Task Learning for End-to-End Uniform Semantic Graph Parsing</a></strong><br><a href=/people/z/zuchao-li/>Zuchao Li</a>
|
<a href=/people/h/hai-zhao/>Hai Zhao</a>
|
<a href=/people/z/zhuosheng-zhang/>Zhuosheng Zhang</a>
|
<a href=/people/r/rui-wang/>Rui Wang</a>
|
<a href=/people/m/masao-utiyama/>Masao Utiyama</a>
|
<a href=/people/e/eiichiro-sumita/>Eiichiro Sumita</a><br><a href=/volumes/K19-2/ class=text-muted>Proceedings of the Shared Task on Cross-Framework Meaning Representation Parsing at the 2019 Conference on Natural Language Learning</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K19-2004><div class="card-body p-3 small">This paper describes our SJTU-NICT&#8217;s system for participating in the shared task on Cross-Framework Meaning Representation Parsing (MRP) at the 2019 Conference for Computational Language Learning (CoNLL). Our <a href=https://en.wikipedia.org/wiki/System>system</a> uses a graph-based approach to model a variety of semantic graph parsing tasks. Our main contributions in the submitted <a href=https://en.wikipedia.org/wiki/System>system</a> are summarized as follows : 1. Our model is fully end-to-end and is capable of being trained only on the given training set which does not rely on any other extra training source including the companion data provided by the organizer ; 2. We extend our graph pruning algorithm to a variety of semantic graphs, solving the problem of excessive semantic graph search space ; 3. We introduce <a href=https://en.wikipedia.org/wiki/Multi-task_learning>multi-task learning</a> for multiple objectives within the same <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a>. The evaluation results show that our <a href=https://en.wikipedia.org/wiki/System>system</a> achieved second place in the overall <a href=https://en.wikipedia.org/wiki/Grading_in_education>F_1 score</a> and achieved the best <a href=https://en.wikipedia.org/wiki/Grading_in_education>F_1 score</a> on the DM framework.<tex-math>F_1</tex-math> score and achieved the best <tex-math>F_1</tex-math> score on the DM framework.</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1038.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1038 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1038 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=C18-1038" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/C18-1038/>One-shot Learning for <a href=https://en.wikipedia.org/wiki/Question_answering>Question-Answering</a> in Gaokao History Challenge<span class=acl-fixed-case>G</span>aokao History Challenge</a></strong><br><a href=/people/z/zhuosheng-zhang/>Zhuosheng Zhang</a>
|
<a href=/people/h/hai-zhao/>Hai Zhao</a><br><a href=/volumes/C18-1/ class=text-muted>Proceedings of the 27th International Conference on Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1038><div class="card-body p-3 small">Answering questions from university admission exams (Gaokao in Chinese) is a challenging AI task since it requires effective <a href=https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning>representation</a> to capture complicated semantic relations between questions and answers. In this work, we propose a hybrid neural model for deep question-answering task from <a href=https://en.wikipedia.org/wiki/Test_(assessment)>history examinations</a>. Our model employs a cooperative gated neural network to retrieve answers with the assistance of extra labels given by a neural turing machine labeler. Empirical study shows that the <a href=https://en.wikipedia.org/wiki/Labeler>labeler</a> works well with only a small training dataset and the gated mechanism is good at fetching the semantic representation of lengthy answers. Experiments on <a href=https://en.wikipedia.org/wiki/Question_answering>question answering</a> demonstrate the proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> obtains substantial performance gains over various neural model baselines in terms of multiple evaluation metrics.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1048.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1048 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1048 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=C18-1048" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/C18-1048/>Deep Enhanced Representation for Implicit Discourse Relation Recognition</a></strong><br><a href=/people/h/hongxiao-bai/>Hongxiao Bai</a>
|
<a href=/people/h/hai-zhao/>Hai Zhao</a><br><a href=/volumes/C18-1/ class=text-muted>Proceedings of the 27th International Conference on Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1048><div class="card-body p-3 small">Implicit discourse relation recognition is a challenging task as the relation prediction without explicit connectives in discourse parsing needs understanding of text spans and can not be easily derived from surface features from the input sentence pairs. Thus, properly representing the text is very crucial to this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. In this paper, we propose a <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> augmented with different grained text representations, including character, subword, word, sentence, and sentence pair levels. The proposed deeper model is evaluated on the benchmark treebank and achieves state-of-the-art <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> with greater than 48 % in 11-way and F1 score greater than 50 % in 4-way classifications for the first time according to our best knowledge.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1233.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1233 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1233 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1233/>A Full End-to-End Semantic Role Labeler, Syntactic-agnostic Over Syntactic-aware?</a></strong><br><a href=/people/j/jiaxun-cai/>Jiaxun Cai</a>
|
<a href=/people/s/shexia-he/>Shexia He</a>
|
<a href=/people/z/zuchao-li/>Zuchao Li</a>
|
<a href=/people/h/hai-zhao/>Hai Zhao</a><br><a href=/volumes/C18-1/ class=text-muted>Proceedings of the 27th International Conference on Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1233><div class="card-body p-3 small">Semantic role labeling (SRL) is to recognize the predicate-argument structure of a sentence, including subtasks of predicate disambiguation and argument labeling. Previous studies usually formulate the entire SRL problem into two or more subtasks. For the first time, this paper introduces an end-to-end neural model which unifiedly tackles the predicate disambiguation and the argument labeling in one shot. Using a biaffine scorer, our model directly predicts all semantic role labels for all given word pairs in the sentence without relying on any syntactic parse information. Specifically, we augment the BiLSTM encoder with a non-linear transformation to further distinguish the predicate and the argument in a given sentence, and model the semantic role labeling process as a word pair classification task by employing the biaffine attentional mechanism. Though the proposed model is syntax-agnostic with local decoder, it outperforms the state-of-the-art syntax-aware SRL systems on the CoNLL-2008, 2009 benchmarks for both <a href=https://en.wikipedia.org/wiki/English_language>English</a> and <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a>. To our best knowledge, we report the first syntax-agnostic SRL model that surpasses all known syntax-aware models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1271.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1271 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1271 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=C18-1271" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/C18-1271/>Seq2seq Dependency Parsing</a></strong><br><a href=/people/z/zuchao-li/>Zuchao Li</a>
|
<a href=/people/j/jiaxun-cai/>Jiaxun Cai</a>
|
<a href=/people/s/shexia-he/>Shexia He</a>
|
<a href=/people/h/hai-zhao/>Hai Zhao</a><br><a href=/volumes/C18-1/ class=text-muted>Proceedings of the 27th International Conference on Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1271><div class="card-body p-3 small">This paper presents a sequence to sequence (seq2seq) dependency parser by directly predicting the relative position of head for each given word, which therefore results in a truly end-to-end seq2seq dependency parser for the first time. Enjoying the advantage of seq2seq modeling, we enrich a series of embedding enhancement, including firstly introduced subword and node2vec augmentation. Meanwhile, we propose a beam search decoder with <a href=https://en.wikipedia.org/wiki/Tree_(data_structure)>tree constraint</a> and subroot decomposition over the sequence to furthermore enhance our seq2seq parser. Our <a href=https://en.wikipedia.org/wiki/Parsing>parser</a> is evaluated on benchmark treebanks, being on par with the state-of-the-art <a href=https://en.wikipedia.org/wiki/Parsing>parsers</a> by achieving 94.11 % <a href=https://en.wikipedia.org/wiki/Unit_of_analysis>UAS</a> on <a href=https://en.wikipedia.org/wiki/Tree_traversal>PTB</a> and 88.78 % <a href=https://en.wikipedia.org/wiki/Unit_of_analysis>UAS</a> on <a href=https://en.wikipedia.org/wiki/Tree_traversal>CTB</a>, respectively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-2024.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-2024 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-2024 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-2024/>Lingke : a Fine-grained Multi-turn Chatbot for Customer Service<span class=acl-fixed-case>L</span>ingke: a Fine-grained Multi-turn Chatbot for Customer Service</a></strong><br><a href=/people/p/pengfei-zhu/>Pengfei Zhu</a>
|
<a href=/people/z/zhuosheng-zhang/>Zhuosheng Zhang</a>
|
<a href=/people/j/jiangtong-li/>Jiangtong Li</a>
|
<a href=/people/y/yafang-huang/>Yafang Huang</a>
|
<a href=/people/h/hai-zhao/>Hai Zhao</a><br><a href=/volumes/C18-2/ class=text-muted>Proceedings of the 27th International Conference on Computational Linguistics: System Demonstrations</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-2024><div class="card-body p-3 small">Traditional <a href=https://en.wikipedia.org/wiki/Chatbot>chatbots</a> usually need a mass of <a href=https://en.wikipedia.org/wiki/Human&#8211;computer_interaction>human dialogue data</a>, especially when using supervised machine learning method. Though they can easily deal with single-turn question answering, for <a href=https://en.wikipedia.org/wiki/Turns,_rounds_and_time-keeping_systems_in_games>multi-turn</a> the performance is usually unsatisfactory. In this paper, we present Lingke, an information retrieval augmented chatbot which is able to answer questions based on given product introduction document and deal with multi-turn conversations. We will introduce a fine-grained pipeline processing to distill responses based on <a href=https://en.wikipedia.org/wiki/Unstructured_data>unstructured documents</a>, and attentive sequential context-response matching for multi-turn conversations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1321.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1321 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1321 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1321" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D18-1321/>Chinese Pinyin Aided IME, Input What You Have Not Keystroked Yet<span class=acl-fixed-case>C</span>hinese <span class=acl-fixed-case>P</span>inyin Aided <span class=acl-fixed-case>IME</span>, Input What You Have Not Keystroked Yet</a></strong><br><a href=/people/y/yafang-huang/>Yafang Huang</a>
|
<a href=/people/h/hai-zhao/>Hai Zhao</a><br><a href=/volumes/D18-1/ class=text-muted>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1321><div class="card-body p-3 small">Chinese pinyin input method engine (IME) converts pinyin into character so that <a href=https://en.wikipedia.org/wiki/Chinese_characters>Chinese characters</a> can be conveniently inputted into computer through common keyboard. IMEs work relying on its core component, pinyin-to-character conversion (P2C). Usually Chinese IMEs simply predict a list of character sequences for user choice only according to user pinyin input at each turn. However, Chinese inputting is a multi-turn online procedure, which can be supposed to be exploited for further user experience promoting. This paper thus for the first time introduces a sequence-to-sequence model with gated-attention mechanism for the core task in <a href=https://en.wikipedia.org/wiki/Instruction_set_architecture>IMEs</a>. The proposed neural P2C model is learned by encoding previous input utterance as extra context to enable our IME capable of predicting character sequence with incomplete pinyin input. Our model is evaluated in different benchmark datasets showing great user experience improvement compared to traditional models, which demonstrates the first engineering practice of building Chinese aided IME.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S18-1147.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S18-1147 data-toggle=collapse aria-expanded=false aria-controls=abstract-S18-1147 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/S18-1147/>SJTU-NLP at SemEval-2018 Task 9 : Neural Hypernym Discovery with Term Embeddings<span class=acl-fixed-case>SJTU</span>-<span class=acl-fixed-case>NLP</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2018 Task 9: Neural Hypernym Discovery with Term Embeddings</a></strong><br><a href=/people/z/zhuosheng-zhang/>Zhuosheng Zhang</a>
|
<a href=/people/j/jiangtong-li/>Jiangtong Li</a>
|
<a href=/people/h/hai-zhao/>Hai Zhao</a>
|
<a href=/people/b/bingjie-tang/>Bingjie Tang</a><br><a href=/volumes/S18-1/ class=text-muted>Proceedings of The 12th International Workshop on Semantic Evaluation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S18-1147><div class="card-body p-3 small">This paper describes a hypernym discovery system for our participation in the SemEval-2018 Task 9, which aims to discover the best (set of) candidate hypernyms for input concepts or entities, given the search space of a pre-defined vocabulary. We introduce a <a href=https://en.wikipedia.org/wiki/Neural_network>neural network architecture</a> for the concerned task and empirically study various neural network models to build the representations in latent space for words and phrases. The evaluated models include <a href=https://en.wikipedia.org/wiki/Convolutional_neural_network>convolutional neural network</a>, long-short term memory network, <a href=https://en.wikipedia.org/wiki/Gated_recurrent_unit>gated recurrent unit</a> and recurrent convolutional neural network. We also explore different embedding methods, including <a href=https://en.wikipedia.org/wiki/Word_embedding>word embedding</a> and sense embedding for better performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K18-2006.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K18-2006 data-toggle=collapse aria-expanded=false aria-controls=abstract-K18-2006 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=K18-2006" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/K18-2006/>Joint Learning of POS and Dependencies for Multilingual Universal Dependency Parsing<span class=acl-fixed-case>POS</span> and Dependencies for Multilingual <span class=acl-fixed-case>U</span>niversal <span class=acl-fixed-case>D</span>ependency Parsing</a></strong><br><a href=/people/z/zuchao-li/>Zuchao Li</a>
|
<a href=/people/s/shexia-he/>Shexia He</a>
|
<a href=/people/z/zhuosheng-zhang/>Zhuosheng Zhang</a>
|
<a href=/people/h/hai-zhao/>Hai Zhao</a><br><a href=/volumes/K18-2/ class=text-muted>Proceedings of the CoNLL 2018 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K18-2006><div class="card-body p-3 small">This paper describes the system of team LeisureX in the CoNLL 2018 Shared Task : Multilingual Parsing from Raw Text to Universal Dependencies. Our <a href=https://en.wikipedia.org/wiki/System>system</a> predicts the <a href=https://en.wikipedia.org/wiki/Part-of-speech_tagging>part-of-speech tag</a> and dependency tree jointly. For the basic tasks, including <a href=https://en.wikipedia.org/wiki/Lexical_analysis>tokenization</a>, <a href=https://en.wikipedia.org/wiki/Lemmatization>lemmatization</a> and <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphology prediction</a>, we employ the official baseline model (UDPipe). To train the low-resource languages, we adopt a <a href=https://en.wikipedia.org/wiki/Sampling_(statistics)>sampling method</a> based on other richresource languages. Our <a href=https://en.wikipedia.org/wiki/System>system</a> achieves a macro-average of 68.31 % LAS F1 score, with an improvement of 2.51 % compared with the UDPipe.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1192.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1192 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1192 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1192.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285805245 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-1192" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-1192/>Syntax for Semantic Role Labeling, To Be, Or Not To Be</a></strong><br><a href=/people/s/shexia-he/>Shexia He</a>
|
<a href=/people/z/zuchao-li/>Zuchao Li</a>
|
<a href=/people/h/hai-zhao/>Hai Zhao</a>
|
<a href=/people/h/hongxiao-bai/>Hongxiao Bai</a><br><a href=/volumes/P18-1/ class=text-muted>Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1192><div class="card-body p-3 small">Semantic role labeling (SRL) is dedicated to recognizing the predicate-argument structure of a sentence. Previous studies have shown <a href=https://en.wikipedia.org/wiki/Syntax>syntactic information</a> has a remarkable contribution to <a href=https://en.wikipedia.org/wiki/Speech_recognition>SRL</a> performance. However, such perception was challenged by a few recent neural SRL models which give impressive performance without a syntactic backbone. This paper intends to quantify the importance of <a href=https://en.wikipedia.org/wiki/Syntax>syntactic information</a> to dependency SRL in deep learning framework. We propose an enhanced argument labeling model companying with an extended korder argument pruning algorithm for effectively exploiting syntactic information. Our model achieves state-of-the-art results on the CoNLL-2008, 2009 benchmarks for both <a href=https://en.wikipedia.org/wiki/English_language>English</a> and <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a>, showing the quantitative significance of <a href=https://en.wikipedia.org/wiki/Syntax>syntax</a> to neural SRL together with a thorough empirical survey over existing models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-4024.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-4024 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-4024 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P18-4024/>Moon IME : Neural-based Chinese Pinyin Aided Input Method with Customizable Association<span class=acl-fixed-case>IME</span>: Neural-based <span class=acl-fixed-case>C</span>hinese <span class=acl-fixed-case>P</span>inyin Aided Input Method with Customizable Association</a></strong><br><a href=/people/y/yafang-huang/>Yafang Huang</a>
|
<a href=/people/z/zuchao-li/>Zuchao Li</a>
|
<a href=/people/z/zhuosheng-zhang/>Zhuosheng Zhang</a>
|
<a href=/people/h/hai-zhao/>Hai Zhao</a><br><a href=/volumes/P18-4/ class=text-muted>Proceedings of ACL 2018, System Demonstrations</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-4024><div class="card-body p-3 small">Chinese pinyin input method engine (IME) lets user conveniently input Chinese into a computer by typing pinyin through the common keyboard. In addition to offering high conversion quality, modern pinyin IME is supposed to aid user input with extended association function. However, existing solutions for such <a href=https://en.wikipedia.org/wiki/Function_(mathematics)>functions</a> are roughly based on oversimplified matching algorithms at word-level, whose resulting products provide limited extension associated with user inputs. This work presents the Moon IME, a pinyin IME that integrates the attention-based neural machine translation (NMT) model and Information Retrieval (IR) to offer amusive and customizable association ability. The released IME is implemented on <a href=https://en.wikipedia.org/wiki/Microsoft_Windows>Windows</a> via text services framework.</div></div><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2096.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2096 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2096 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P17-2096" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P17-2096/>Fast and Accurate Neural Word Segmentation for Chinese<span class=acl-fixed-case>C</span>hinese</a></strong><br><a href=/people/d/deng-cai/>Deng Cai</a>
|
<a href=/people/h/hai-zhao/>Hai Zhao</a>
|
<a href=/people/z/zhisong-zhang/>Zhisong Zhang</a>
|
<a href=/people/y/yuan-xin/>Yuan Xin</a>
|
<a href=/people/y/yongjian-wu/>Yongjian Wu</a>
|
<a href=/people/f/feiyue-huang/>Feiyue Huang</a><br><a href=/volumes/P17-2/ class=text-muted>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2096><div class="card-body p-3 small">Neural models with minimal feature engineering have achieved competitive performance against traditional methods for the task of <a href=https://en.wikipedia.org/wiki/Chinese_word_segmentation>Chinese word segmentation</a>. However, both training and working procedures of the current neural models are computationally inefficient. In this paper, we propose a greedy neural word segmenter with balanced word and character embedding inputs to alleviate the existing drawbacks. Our segmenter is truly end-to-end, capable of performing segmentation much faster and even more accurate than state-of-the-art neural models on Chinese benchmark datasets.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Hai+Zhao" title="Search for 'Hai Zhao' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/z/zuchao-li/ class=align-middle>Zuchao Li</a>
<span class="badge badge-secondary align-middle ml-2">10</span></li><li class=list-group-item><a href=/people/z/zhuosheng-zhang/ class=align-middle>Zhuosheng Zhang</a>
<span class="badge badge-secondary align-middle ml-2">8</span></li><li class=list-group-item><a href=/people/s/shexia-he/ class=align-middle>Shexia He</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/people/y/yafang-huang/ class=align-middle>Yafang Huang</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/r/rui-wang/ class=align-middle>Rui Wang</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/m/masao-utiyama/ class=align-middle>Masao Utiyama</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/e/eiichiro-sumita/ class=align-middle>Eiichiro Sumita</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/h/hongxiao-bai/ class=align-middle>Hongxiao Bai</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/j/jiaxun-cai/ class=align-middle>Jiaxun Cai</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/j/jiangtong-li/ class=align-middle>Jiangtong Li</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/j/junru-zhou/ class=align-middle>Junru Zhou</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/p/pengfei-zhu/ class=align-middle>Pengfei Zhu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yian-li/ class=align-middle>Yian Li</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/ying-luo/ class=align-middle>Ying Luo</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/junlang-zhan/ class=align-middle>Junlang Zhan</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/deng-cai/ class=align-middle>Deng Cai</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/zhisong-zhang/ class=align-middle>Zhisong Zhang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yuan-xin/ class=align-middle>Yuan Xin</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yongjian-wu/ class=align-middle>Yongjian Wu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/f/feiyue-huang/ class=align-middle>Feiyue Huang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jiayi-wang/ class=align-middle>Jiayi Wang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/rongzhou-bao/ class=align-middle>Rongzhou Bao</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/kevin-parnow/ class=align-middle>Kevin Parnow</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/b/bingjie-tang/ class=align-middle>Bingjie Tang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/chaoyu-guan/ class=align-middle>Chaoyu Guan</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yuhao-cheng/ class=align-middle>Yuhao Cheng</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/kehai-chen/ class=align-middle>Kehai Chen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/acl/ class=align-middle>ACL</a><span class="badge badge-secondary align-middle ml-2">6</span></li><li class=list-group-item><a href=/venues/coling/ class=align-middle>COLING</a><span class="badge badge-secondary align-middle ml-2">5</span></li><li class=list-group-item><a href=/venues/findings/ class=align-middle>Findings</a><span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/venues/emnlp/ class=align-middle>EMNLP</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/conll/ class=align-middle>CoNLL</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-venues aria-expanded=false aria-controls=more-venues>show all...</li><div class="collapse border-top" id=more-venues><li class=list-group-item><a href=/venues/wat/ class=align-middle>WAT</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/semeval/ class=align-middle>SemEval</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/naacl/ class=align-middle>NAACL</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/wmt/ class=align-middle>WMT</a><span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>