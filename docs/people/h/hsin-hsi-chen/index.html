<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Hsin-Hsi Chen - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Hsin-Hsi</span> <span class=font-weight-bold>Chen</span></h2><hr><div class=row><div class=col-lg-9><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-tutorials.2.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-tutorials--2 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-tutorials.2 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-tutorials.2/>Financial Opinion Mining</a></strong><br><a href=/people/c/chung-chi-chen/>Chung-Chi Chen</a>
|
<a href=/people/h/hen-hsen-huang/>Hen-Hsen Huang</a>
|
<a href=/people/h/hsin-hsi-chen/>Hsin-Hsi Chen</a><br><a href=/volumes/2021.emnlp-tutorials/ class=text-muted>Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: Tutorial Abstracts</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-tutorials--2><div class="card-body p-3 small">In this tutorial, we will show where we are and where we will be to those researchers interested in this topic. We divide this tutorial into three parts, including coarse-grained financial opinion mining, fine-grained financial opinion mining, and possible research directions. This tutorial starts by introducing the components in a financial opinion proposed in our research agenda and summarizes their related studies. We also highlight the task of mining customers&#8217; opinions toward <a href=https://en.wikipedia.org/wiki/Financial_services>financial services</a> in the <a href=https://en.wikipedia.org/wiki/Financial_technology>FinTech industry</a>, and compare them with usual opinions. Several potential research questions will be addressed. We hope the audiences of this tutorial will gain an overview of financial opinion mining and figure out their research directions.</div></div><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.199.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--199 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.199 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.199/>Heterogeneous Recycle Generation for Chinese Grammatical Error Correction<span class=acl-fixed-case>C</span>hinese Grammatical Error Correction</a></strong><br><a href=/people/c/charles-hinson/>Charles Hinson</a>
|
<a href=/people/h/hen-hsen-huang/>Hen-Hsen Huang</a>
|
<a href=/people/h/hsin-hsi-chen/>Hsin-Hsi Chen</a><br><a href=/volumes/2020.coling-main/ class=text-muted>Proceedings of the 28th International Conference on Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--199><div class="card-body p-3 small">Most recent works in the field of grammatical error correction (GEC) rely on neural machine translation-based models. Although these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> boast impressive performance, they require a massive amount of data to properly train. Furthermore, NMT-based systems treat GEC purely as a translation task and overlook the editing aspect of it. In this work we propose a heterogeneous approach to Chinese GEC, composed of a NMT-based model, a sequence editing model, and a <a href=https://en.wikipedia.org/wiki/Spell_checker>spell checker</a>. Our methodology not only achieves a new state-of-the-art performance for Chinese GEC, but also does so without relying on <a href=https://en.wikipedia.org/wiki/Data_augmentation>data augmentation</a> or GEC-specific architecture changes. We further experiment with all possible configurations of our <a href=https://en.wikipedia.org/wiki/System>system</a> with respect to model composition order and number of rounds of correction. A detailed analysis of each <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> and their contributions to the correction process is performed by adapting the ERRANT scorer to be able to score Chinese sentences.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.fnp-1.11.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--fnp-1--11 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.fnp-1.11 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.fnp-1.11" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.fnp-1.11/>NTUNLPL at FinCausal 2020, Task 2 : Improving Causality Detection Using Viterbi Decoder<span class=acl-fixed-case>NTUNLPL</span> at <span class=acl-fixed-case>F</span>in<span class=acl-fixed-case>C</span>ausal 2020, Task 2:Improving Causality Detection Using <span class=acl-fixed-case>V</span>iterbi Decoder</a></strong><br><a href=/people/p/pei-wei-kao/>Pei-Wei Kao</a>
|
<a href=/people/c/chung-chi-chen/>Chung-Chi Chen</a>
|
<a href=/people/h/hen-hsen-huang/>Hen-Hsen Huang</a>
|
<a href=/people/h/hsin-hsi-chen/>Hsin-Hsi Chen</a><br><a href=/volumes/2020.fnp-1/ class=text-muted>Proceedings of the 1st Joint Workshop on Financial Narrative Processing and MultiLing Financial Summarisation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--fnp-1--11><div class="card-body p-3 small">In order to provide an explanation of <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning models</a>, causality detection attracts lots of attention in the artificial intelligence research community. In this paper, we explore the cause-effect detection in financial news and propose an approach, which combines the BIO scheme with the <a href=https://en.wikipedia.org/wiki/Viterbi_decoder>Viterbi decoder</a> for addressing this challenge. Our approach is ranked the first in the official run of cause-effect detection (Task 2) of the FinCausal-2020 shared task. We not only report the implementation details and ablation analysis in this paper, but also publish our <a href=https://en.wikipedia.org/wiki/Source_code>code</a> for academic usage.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.128.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--128 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.128 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.128/>Chinese Discourse Parsing : Model and Evaluation<span class=acl-fixed-case>C</span>hinese Discourse Parsing: Model and Evaluation</a></strong><br><a href=/people/l/lin-chuan-an/>Lin Chuan-An</a>
|
<a href=/people/s/shyh-shiun-hung/>Shyh-Shiun Hung</a>
|
<a href=/people/h/hen-hsen-huang/>Hen-Hsen Huang</a>
|
<a href=/people/h/hsin-hsi-chen/>Hsin-Hsi Chen</a><br><a href=/volumes/2020.lrec-1/ class=text-muted>Proceedings of the 12th Language Resources and Evaluation Conference</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--128><div class="card-body p-3 small">Chinese discourse parsing, which aims to identify the hierarchical relationships of Chinese elementary discourse units, has not yet a consistent evaluation metric. Although <a href=https://en.wikipedia.org/wiki/Parseval>Parseval</a> is commonly used, variations of evaluation differ from three aspects : micro vs. macro F1 scores, binary vs. multiway ground truth, and left-heavy vs. right-heavy binarization. In this paper, we first propose a neural network model that unifies a pre-trained transformer and CKY-like algorithm, and then compare it with the previous models with different evaluation scenarios. The experimental results show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms the previous <a href=https://en.wikipedia.org/wiki/System>systems</a>. We conclude that (1) the pre-trained context embedding provides effective solutions to deal with implicit semantics in Chinese texts, and (2) using multiway ground truth is helpful since different binarization approaches lead to significant differences in performance.</div></div><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5500.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5500/>Proceedings of the First Workshop on Financial Technology and Natural Language Processing</a></strong><br><a href=/people/c/chung-chi-chen/>Chung-Chi Chen</a>
|
<a href=/people/h/hen-hsen-huang/>Hen-Hsen Huang</a>
|
<a href=/people/h/hiroya-takamura/>Hiroya Takamura</a>
|
<a href=/people/h/hsin-hsi-chen/>Hsin-Hsi Chen</a><br><a href=/volumes/W19-55/ class=text-muted>Proceedings of the First Workshop on Financial Technology and Natural Language Processing</a></span></p><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-2030.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-2030 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-2030 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-2030/>A <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese Writing Correction System</a> for Learning <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a> as a Foreign Language<span class=acl-fixed-case>C</span>hinese Writing Correction System for Learning <span class=acl-fixed-case>C</span>hinese as a Foreign Language</a></strong><br><a href=/people/y/yow-ting-shiue/>Yow-Ting Shiue</a>
|
<a href=/people/h/hen-hsen-huang/>Hen-Hsen Huang</a>
|
<a href=/people/h/hsin-hsi-chen/>Hsin-Hsi Chen</a><br><a href=/volumes/C18-2/ class=text-muted>Proceedings of the 27th International Conference on Computational Linguistics: System Demonstrations</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-2030><div class="card-body p-3 small">We present a <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese writing correction system</a> for learning <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a> as a foreign language. The <a href=https://en.wikipedia.org/wiki/System>system</a> takes a wrong input sentence and generates several correction suggestions. It also retrieves example <a href=https://en.wikipedia.org/wiki/Written_Chinese>Chinese sentences</a> with English translations, helping users understand the correct usages of certain <a href=https://en.wikipedia.org/wiki/Chinese_grammar>grammar patterns</a>. This is the first available Chinese writing error correction system based on the neural machine translation framework. We discuss several design choices and show empirical results to support our decisions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S18-1171.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S18-1171 data-toggle=collapse aria-expanded=false aria-controls=abstract-S18-1171 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/S18-1171/>NTU NLP Lab System at SemEval-2018 Task 10 : Verifying Semantic Differences by Integrating Distributional Information and Expert Knowledge<span class=acl-fixed-case>NTU</span> <span class=acl-fixed-case>NLP</span> Lab System at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2018 Task 10: Verifying Semantic Differences by Integrating Distributional Information and Expert Knowledge</a></strong><br><a href=/people/y/yow-ting-shiue/>Yow-Ting Shiue</a>
|
<a href=/people/h/hen-hsen-huang/>Hen-Hsen Huang</a>
|
<a href=/people/h/hsin-hsi-chen/>Hsin-Hsi Chen</a><br><a href=/volumes/S18-1/ class=text-muted>Proceedings of The 12th International Workshop on Semantic Evaluation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S18-1171><div class="card-body p-3 small">This paper presents the NTU NLP Lab system for the SemEval-2018 Capturing Discriminative Attributes task. Word embeddings, pointwise mutual information (PMI), ConceptNet edges and shortest path lengths are utilized as input features to build binary classifiers to tell whether an attribute is discriminative for a pair of concepts. Our <a href=https://en.wikipedia.org/wiki/Neural_network>neural network model</a> reaches about 73 % F1 score on the test set and ranks the 3rd in the task. Though the <a href=https://en.wikipedia.org/wiki/Attribute_(computing)>attributes</a> to deal with in this task are all visual, our <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> are not provided with any <a href=https://en.wikipedia.org/wiki/Image>image data</a>. The results indicate that <a href=https://en.wikipedia.org/wiki/Visual_system>visual information</a> can be derived from <a href=https://en.wikipedia.org/wiki/Text_(literary_theory)>textual data</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3700.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3700/>Proceedings of the 5th Workshop on Natural Language Processing Techniques for Educational Applications</a></strong><br><a href=/people/y/yuen-hsien-tseng/>Yuen-Hsien Tseng</a>
|
<a href=/people/h/hsin-hsi-chen/>Hsin-Hsi Chen</a>
|
<a href=/people/v/vincent-ng/>Vincent Ng</a>
|
<a href=/people/m/mamoru-komachi/>Mamoru Komachi</a><br><a href=/volumes/W18-37/ class=text-muted>Proceedings of the 5th Workshop on Natural Language Processing Techniques for Educational Applications</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2122.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2122 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2122 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2122.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285806165 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-2122/>Disambiguating False-Alarm Hashtag Usages in Tweets for Irony Detection</a></strong><br><a href=/people/h/hen-hsen-huang/>Hen-Hsen Huang</a>
|
<a href=/people/c/chiao-chen-chen/>Chiao-Chen Chen</a>
|
<a href=/people/h/hsin-hsi-chen/>Hsin-Hsi Chen</a><br><a href=/volumes/P18-2/ class=text-muted>Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2122><div class="card-body p-3 small">The reliability of self-labeled data is an important issue when the <a href=https://en.wikipedia.org/wiki/Data>data</a> are regarded as ground-truth for training and testing <a href=https://en.wikipedia.org/wiki/Machine_learning>learning-based models</a>. This paper addresses the issue of false-alarm hashtags in the self-labeled data for irony detection. We analyze the ambiguity of hashtag usages and propose a novel neural network-based model, which incorporates linguistic information from different aspects, to disambiguate the usage of three <a href=https://en.wikipedia.org/wiki/Hashtag>hashtags</a> that are widely used to collect the training data for irony detection. Furthermore, we apply our <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> to prune the self-labeled training data. Experimental results show that the irony detection model trained on the less but cleaner training instances outperforms the <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> trained on all data.</div></div><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1098.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1098 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1098 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-1098/>Integrating Subject, Type, and Property Identification for Simple <a href=https://en.wikipedia.org/wiki/Question_answering>Question Answering</a> over Knowledge Base</a></strong><br><a href=/people/w/wei-chuan-hsiao/>Wei-Chuan Hsiao</a>
|
<a href=/people/h/hen-hsen-huang/>Hen-Hsen Huang</a>
|
<a href=/people/h/hsin-hsi-chen/>Hsin-Hsi Chen</a><br><a href=/volumes/I17-1/ class=text-muted>Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1098><div class="card-body p-3 small">This paper presents an approach to identify subject, type and property from knowledge base (KB) for answering simple questions. We propose new <a href=https://en.wikipedia.org/wiki/Software_feature>features</a> to rank entity candidates in KB. Besides, we split a relation in <a href=https://en.wikipedia.org/wiki/Kibibyte>KB</a> into type and property. Each of <a href=https://en.wikipedia.org/wiki/Matrix_(mathematics)>them</a> is modeled by a bi-directional LSTM. Experimental results show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieves the state-of-the-art performance on the SimpleQuestions dataset. The hard questions in the experiments are also analyzed in detail.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2064.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2064 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2064 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P17-2064.Datasets.zip data-toggle=tooltip data-placement=top title=Dataset><i class="fas fa-file-archive"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-2064/>Detection of Chinese Word Usage Errors for Non-Native Chinese Learners with Bidirectional LSTM<span class=acl-fixed-case>C</span>hinese Word Usage Errors for Non-Native <span class=acl-fixed-case>C</span>hinese Learners with Bidirectional <span class=acl-fixed-case>LSTM</span></a></strong><br><a href=/people/y/yow-ting-shiue/>Yow-Ting Shiue</a>
|
<a href=/people/h/hen-hsen-huang/>Hen-Hsen Huang</a>
|
<a href=/people/h/hsin-hsi-chen/>Hsin-Hsi Chen</a><br><a href=/volumes/P17-2/ class=text-muted>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2064><div class="card-body p-3 small">Selecting appropriate words to compose a sentence is one common problem faced by <a href=https://en.wikipedia.org/wiki/Foreign_language>non-native Chinese learners</a>. In this paper, we propose (bidirectional) LSTM sequence labeling models and explore various <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> to detect word usage errors in Chinese sentences. By combining CWINDOW word embedding features and POS information, the best bidirectional LSTM model achieves <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> 0.5138 and MRR 0.6789 on the HSK dataset. For 80.79 % of the test data, the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> ranks the ground-truth within the top two at position level.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5900.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5900/>Proceedings of the 4th Workshop on Natural Language Processing Techniques for Educational Applications (<span class=acl-fixed-case>NLPTEA</span> 2017)</a></strong><br><a href=/people/y/yuen-hsien-tseng/>Yuen-Hsien Tseng</a>
|
<a href=/people/h/hsin-hsi-chen/>Hsin-Hsi Chen</a>
|
<a href=/people/l/lung-hao-lee/>Lung-Hao Lee</a>
|
<a href=/people/l/liang-chih-yu/>Liang-Chih Yu</a><br><a href=/volumes/W17-59/ class=text-muted>Proceedings of the 4th Workshop on Natural Language Processing Techniques for Educational Applications (NLPTEA 2017)</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S17-2144.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S17-2144 data-toggle=collapse aria-expanded=false aria-controls=abstract-S17-2144 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/S17-2144/>NLG301 at SemEval-2017 Task 5 : Fine-Grained Sentiment Analysis on Financial Microblogs and News<span class=acl-fixed-case>NLG</span>301 at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2017 Task 5: Fine-Grained Sentiment Analysis on Financial Microblogs and News</a></strong><br><a href=/people/c/chung-chi-chen/>Chung-Chi Chen</a>
|
<a href=/people/h/hen-hsen-huang/>Hen-Hsen Huang</a>
|
<a href=/people/h/hsin-hsi-chen/>Hsin-Hsi Chen</a><br><a href=/volumes/S17-2/ class=text-muted>Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S17-2144><div class="card-body p-3 small">Short length, multi-targets, target relation-ship, monetary expressions, and outside reference are characteristics of financial tweets. This paper proposes methods to extract target spans from a tweet and its referencing web page. Total 15 publicly available sentiment dictionaries and one sentiment dictionary constructed from training set, containing <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment scores</a> in binary or real numbers, are used to compute the <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment scores</a> of text spans. Moreover, the correlation coeffi-cients of the price return between any two stocks are learned with the price data from <a href=https://en.wikipedia.org/wiki/Bloomberg_L.P.>Bloomberg</a>. They are used to capture the relationships between the interesting tar-get and other stocks mentioned in a tweet. The best result of our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> in both <a href=https://en.wikipedia.org/wiki/Task_(project_management)>sub-task</a> are 56.68 % and 55.43 %, evaluated by evaluation method 2.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Hsin-Hsi+Chen" title="Search for 'Hsin-Hsi Chen' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/h/hen-hsen-huang/ class=align-middle>Hen-Hsen Huang</a>
<span class="badge badge-secondary align-middle ml-2">11</span></li><li class=list-group-item><a href=/people/c/chung-chi-chen/ class=align-middle>Chung-Chi Chen</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/people/y/yow-ting-shiue/ class=align-middle>Yow-Ting Shiue</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/y/yuen-hsien-tseng/ class=align-middle>Yuen-Hsien Tseng</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/w/wei-chuan-hsiao/ class=align-middle>Wei-Chuan Hsiao</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/l/lung-hao-lee/ class=align-middle>Lung-Hao Lee</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/liang-chih-yu/ class=align-middle>Liang-Chih Yu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/v/vincent-ng/ class=align-middle>Vincent Ng</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/mamoru-komachi/ class=align-middle>Mamoru Komachi</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/hiroya-takamura/ class=align-middle>Hiroya Takamura</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/charles-hinson/ class=align-middle>Charles Hinson</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/p/pei-wei-kao/ class=align-middle>Pei-Wei Kao</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/lin-chuan-an/ class=align-middle>Lin Chuan-An</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/shyh-shiun-hung/ class=align-middle>Shyh-Shiun Hung</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/chiao-chen-chen/ class=align-middle>Chiao-Chen Chen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/ws/ class=align-middle>WS</a><span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/venues/coling/ class=align-middle>COLING</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/acl/ class=align-middle>ACL</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/semeval/ class=align-middle>SemEval</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/ijcnlp/ class=align-middle>IJCNLP</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-venues aria-expanded=false aria-controls=more-venues>show all...</li><div class="collapse border-top" id=more-venues><li class=list-group-item><a href=/venues/emnlp/ class=align-middle>EMNLP</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/fnp/ class=align-middle>FNP</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/lrec/ class=align-middle>LREC</a><span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright Â©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>