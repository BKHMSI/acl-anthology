<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Hao Zhou - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Hao</span> <span class=font-weight-bold>Zhou</span></h2><hr><div class=row><div class=col-lg-9><h4>2022</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.acl-long.164.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--acl-long--164 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.acl-long.164 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2022.acl-long.164.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.acl-long.164/><span class=acl-fixed-case>CTRLE</span>val: An Unsupervised Reference-Free Metric for Evaluating Controlled Text Generation</a></strong><br><a href=/people/p/pei-ke/>Pei Ke</a>
|
<a href=/people/h/hao-zhou/>Hao Zhou</a>
|
<a href=/people/y/yankai-lin/>Yankai Lin</a>
|
<a href=/people/p/peng-li/>Peng Li</a>
|
<a href=/people/j/jie-zhou/>Jie Zhou</a>
|
<a href=/people/x/xiaoyan-zhu/>Xiaoyan Zhu</a>
|
<a href=/people/m/minlie-huang/>Minlie Huang</a><br><a href=/volumes/2022.acl-long/ class=text-muted>Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--acl-long--164><div class="card-body p-3 small">Existing reference-free metrics have obvious limitations for evaluating controlled text generation models. Unsupervised metrics can only provide a task-agnostic evaluation result which correlates weakly with human judgments, whereas supervised ones may overfit task-specific data with poor generalization ability to other datasets. In this paper, we propose an unsupervised reference-free metric called CTRLEval, which evaluates controlled text generation from different aspects by formulating each aspect into multiple text infilling tasks. On top of these tasks, the metric assembles the generation probabilities from a pre-trained language model without any model training. Experimental results show that our metric has higher correlations with human judgments than other baselines, while obtaining better generalization of evaluating generated texts from different models and with different qualities.</div></div><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.deelio-1.4.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--deelio-1--4 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.deelio-1.4 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.deelio-1.4/>Augmenting Topic Aware Knowledge-Grounded Conversations with Dynamic Built Knowledge Graphs</a></strong><br><a href=/people/j/junjie-wu/>Junjie Wu</a>
|
<a href=/people/h/hao-zhou/>Hao Zhou</a><br><a href=/volumes/2021.deelio-1/ class=text-muted>Proceedings of Deep Learning Inside Out (DeeLIO): The 2nd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--deelio-1--4><div class="card-body p-3 small">Dialog topic management and background knowledge selection are essential factors for the success of knowledge-grounded open-domain conversations. However, existing models are primarily performed with symmetric knowledge bases or stylized with pre-defined roles between conversational partners, while people usually have their own knowledge before a real chit-chat. To address this problem, we propose a dynamic knowledge graph-based topical conversation model (DKGT). Given a dialog history context, our model first builds knowledge graphs from the context as an imitation of human&#8217;s ability to form logical relationships between known and unknown topics during a conversation. This logical information will be fed into a topic predictor to promote topic management, then facilitate background knowledge selection and response generation. To the best of our knowledge, this is the first attempt to dynamically form <a href=https://en.wikipedia.org/wiki/Knowledge_graph>knowledge graphs</a> between chatting topics to assist dialog topic management during a conversation. Experimental results manifest that our model can properly schedule conversational topics and pick suitable knowledge to generate informative responses comparing to several strong baselines.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.19.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--19 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.19 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.19" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.19/>UniRE : A Unified Label Space for Entity Relation Extraction<span class=acl-fixed-case>U</span>ni<span class=acl-fixed-case>RE</span>: A Unified Label Space for Entity Relation Extraction</a></strong><br><a href=/people/y/yijun-wang/>Yijun Wang</a>
|
<a href=/people/c/changzhi-sun/>Changzhi Sun</a>
|
<a href=/people/y/yuanbin-wu/>Yuanbin Wu</a>
|
<a href=/people/h/hao-zhou/>Hao Zhou</a>
|
<a href=/people/l/lei-li/>Lei Li</a>
|
<a href=/people/j/junchi-yan/>Junchi Yan</a><br><a href=/volumes/2021.acl-long/ class=text-muted>Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--19><div class="card-body p-3 small">Many joint entity relation extraction models setup two separated label spaces for the two <a href=https://en.wikipedia.org/wiki/Task_(project_management)>sub-tasks</a> (i.e., <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entity detection</a> and relation classification). We argue that this setting may hinder the <a href=https://en.wikipedia.org/wiki/Information_exchange>information interaction</a> between entities and relations. In this work, we propose to eliminate the different treatment on the two sub-tasks&#8217; label spaces. The input of our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is a table containing all word pairs from a sentence. Entities and relations are represented by squares and rectangles in the table. We apply a unified classifier to predict each cell&#8217;s label, which unifies the learning of two sub-tasks. For testing, an effective (yet fast) approximate decoder is proposed for finding squares and rectangles from tables. Experiments on three benchmarks (ACE04, ACE05, SciERC) show that, using only half the number of parameters, our model achieves competitive accuracy with the best extractor, and is faster.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.571.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--571 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.571 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><span class="align-middle mr-1" data-toggle=tooltip data-placement=bottom title="Best Paper"><i class="fas fa-award"></i></span><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.571" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2021.acl-long.571/>Vocabulary Learning via <a href=https://en.wikipedia.org/wiki/Optimal_transport>Optimal Transport</a> for Neural Machine Translation</a></strong><br><a href=/people/j/jingjing-xu/>Jingjing Xu</a>
|
<a href=/people/h/hao-zhou/>Hao Zhou</a>
|
<a href=/people/c/chun-gan/>Chun Gan</a>
|
<a href=/people/z/zaixiang-zheng/>Zaixiang Zheng</a>
|
<a href=/people/l/lei-li/>Lei Li</a><br><a href=/volumes/2021.acl-long/ class=text-muted>Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--571><div class="card-body p-3 small">The choice of token vocabulary affects the performance of <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>. This paper aims to figure out what is a good vocabulary and whether we can find the optimal vocabulary without <a href=https://en.wikipedia.org/wiki/Trial_and_error>trial training</a>. To answer these questions, we first provide an alternative understanding of <a href=https://en.wikipedia.org/wiki/Vocabulary>vocabulary</a> from the perspective of <a href=https://en.wikipedia.org/wiki/Information_theory>information theory</a>. It motivates us to formulate the quest of vocabularization finding the best token dictionary with a proper size as an optimal transport (OT) problem. We propose VOLT, a simple and efficient solution without trial training. Empirical results show that VOLT beats widely-used vocabularies in diverse scenarios, including WMT-14 English-German translation, TED bilingual translation, and TED multilingual translation. For example, VOLT achieves 70 % vocabulary size reduction and 0.5 BLEU gain on English-German translation. Also, compared to BPE-search, VOLT reduces the search time from 384 GPU hours to 30 GPU hours on English-German translation. Codes are available at https://github.com/Jingjing-NLP/VOLT.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.251.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--251 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.251 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.eacl-main.251" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.251/>ENPAR : Enhancing Entity and Entity Pair Representations for Joint Entity Relation Extraction<span class=acl-fixed-case>ENPAR</span>:Enhancing Entity and Entity Pair Representations for Joint Entity Relation Extraction</a></strong><br><a href=/people/y/yijun-wang/>Yijun Wang</a>
|
<a href=/people/c/changzhi-sun/>Changzhi Sun</a>
|
<a href=/people/y/yuanbin-wu/>Yuanbin Wu</a>
|
<a href=/people/h/hao-zhou/>Hao Zhou</a>
|
<a href=/people/l/lei-li/>Lei Li</a>
|
<a href=/people/j/junchi-yan/>Junchi Yan</a><br><a href=/volumes/2021.eacl-main/ class=text-muted>Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--251><div class="card-body p-3 small">Current state-of-the-art systems for joint entity relation extraction (Luan et al., 2019 ; Wad-den et al., 2019) usually adopt the multi-task learning framework. However, <a href=https://en.wikipedia.org/wiki/Annotation>annotations</a> for these additional tasks such as <a href=https://en.wikipedia.org/wiki/Coreference_resolution>coreference resolution</a> and <a href=https://en.wikipedia.org/wiki/Event_extraction>event extraction</a> are always equally hard (or even harder) to obtain. In this work, we propose a pre-training method ENPAR to improve the joint extraction performance. ENPAR requires only the additional entity annotations that are much easier to collect. Unlike most existing works that only consider incorporating <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entity information</a> into the sentence encoder, we further utilize the <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entity pair information</a>. Specifically, we devise four novel <a href=https://en.wikipedia.org/wiki/Goal>objectives</a>, i.e., masked entity typing, masked entity prediction, adversarial context discrimination, and permutation prediction, to pre-train an <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entity encoder</a> and an <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entity pair encoder</a>. Comprehensive experiments show that the proposed pre-training method achieves significant improvement over BERT on ACE05, SciERC, and NYT, and outperforms current state-of-the-art on ACE05.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.95.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--95 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.95 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.95" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.95/>Learning Logic Rules for Document-Level Relation Extraction</a></strong><br><a href=/people/d/dongyu-ru/>Dongyu Ru</a>
|
<a href=/people/c/changzhi-sun/>Changzhi Sun</a>
|
<a href=/people/j/jiangtao-feng/>Jiangtao Feng</a>
|
<a href=/people/l/lin-qiu/>Lin Qiu</a>
|
<a href=/people/h/hao-zhou/>Hao Zhou</a>
|
<a href=/people/w/weinan-zhang/>Weinan Zhang</a>
|
<a href=/people/y/yong-yu/>Yong Yu</a>
|
<a href=/people/l/lei-li/>Lei Li</a><br><a href=/volumes/2021.emnlp-main/ class=text-muted>Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--95><div class="card-body p-3 small">Document-level relation extraction aims to identify relations between entities in a whole document. Prior efforts to capture long-range dependencies have relied heavily on implicitly powerful <a href=https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning>representations</a> learned through (graph) neural networks, which makes the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> less transparent. To tackle this challenge, in this paper, we propose LogiRE, a novel probabilistic model for document-level relation extraction by learning logic rules. LogiRE treats <a href=https://en.wikipedia.org/wiki/Rule_of_inference>logic rules</a> as <a href=https://en.wikipedia.org/wiki/Latent_variable>latent variables</a> and consists of two <a href=https://en.wikipedia.org/wiki/Modular_programming>modules</a> : a rule generator and a relation extractor. The rule generator is to generate <a href=https://en.wikipedia.org/wiki/Rule_of_inference>logic rules</a> potentially contributing to final predictions, and the relation extractor outputs final predictions based on the generated <a href=https://en.wikipedia.org/wiki/Rule_of_inference>logic rules</a>. Those two <a href=https://en.wikipedia.org/wiki/Modular_programming>modules</a> can be efficiently optimized with the expectation-maximization (EM) algorithm. By introducing <a href=https://en.wikipedia.org/wiki/Rule_of_inference>logic rules</a> into <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a>, LogiRE can explicitly capture long-range dependencies as well as enjoy better interpretation. Empirical results show that significantly outperforms several strong baselines in terms of relation performance and <a href=https://en.wikipedia.org/wiki/Consistency>logical consistency</a>. Our code is available at https://github.com/rudongyu/LogiRE.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.184.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--184 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.184 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.184" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.184/>EARL : Informative Knowledge-Grounded Conversation Generation with Entity-Agnostic Representation Learning<span class=acl-fixed-case>EARL</span>: Informative Knowledge-Grounded Conversation Generation with Entity-Agnostic Representation Learning</a></strong><br><a href=/people/h/hao-zhou/>Hao Zhou</a>
|
<a href=/people/m/minlie-huang/>Minlie Huang</a>
|
<a href=/people/y/yong-liu/>Yong Liu</a>
|
<a href=/people/w/wei-chen/>Wei Chen</a>
|
<a href=/people/x/xiaoyan-zhu/>Xiaoyan Zhu</a><br><a href=/volumes/2021.emnlp-main/ class=text-muted>Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--184><div class="card-body p-3 small">Generating informative and appropriate responses is challenging but important for building human-like dialogue systems. Although various knowledge-grounded conversation models have been proposed, these <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> have limitations in utilizing knowledge that infrequently occurs in the training data, not to mention integrating unseen knowledge into conversation generation. In this paper, we propose an Entity-Agnostic Representation Learning (EARL) method to introduce <a href=https://en.wikipedia.org/wiki/Knowledge_graph>knowledge graphs</a> to informative conversation generation. Unlike traditional approaches that parameterize the specific representation for each entity, EARL utilizes the context of conversations and the relational structure of knowledge graphs to learn the category representation for entities, which is generalized to incorporating unseen entities in <a href=https://en.wikipedia.org/wiki/Knowledge_graph>knowledge graphs</a> into conversation generation. Automatic and manual evaluations demonstrate that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> can generate more informative, coherent, and natural responses than baseline models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wmt-1.17.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wmt-1--17 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wmt-1.17 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wmt-1.17/>The Volctrans GLAT System : Non-autoregressive Translation Meets WMT21<span class=acl-fixed-case>GLAT</span> System: Non-autoregressive Translation Meets <span class=acl-fixed-case>WMT</span>21</a></strong><br><a href=/people/l/lihua-qian/>Lihua Qian</a>
|
<a href=/people/y/yi-zhou/>Yi Zhou</a>
|
<a href=/people/z/zaixiang-zheng/>Zaixiang Zheng</a>
|
<a href=/people/y/yaoming-zhu/>Yaoming Zhu</a>
|
<a href=/people/z/zehui-lin/>Zehui Lin</a>
|
<a href=/people/j/jiangtao-feng/>Jiangtao Feng</a>
|
<a href=/people/s/shanbo-cheng/>Shanbo Cheng</a>
|
<a href=/people/l/lei-li/>Lei Li</a>
|
<a href=/people/m/mingxuan-wang/>Mingxuan Wang</a>
|
<a href=/people/h/hao-zhou/>Hao Zhou</a><br><a href=/volumes/2021.wmt-1/ class=text-muted>Proceedings of the Sixth Conference on Machine Translation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wmt-1--17><div class="card-body p-3 small">This paper describes the Volctrans&#8217; submission to the WMT21 news translation shared task for German-English translation. We build a parallel (i.e., non-autoregressive) translation system using the Glancing Transformer, which enables fast and accurate parallel decoding in contrast to the currently prevailing <a href=https://en.wikipedia.org/wiki/Autoregressive_model>autoregressive models</a>. To the best of our knowledge, this is the first parallel translation system that can be scaled to such a practical scenario like WMT competition. More importantly, our parallel translation system achieves the best BLEU score (35.0) on German-English translation task, outperforming all strong <a href=https://en.wikipedia.org/wiki/Autoregressive_model>autoregressive counterparts</a>.</div></div><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.210.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--210 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.210 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939388 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.210" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.210/>Pre-training Multilingual Neural Machine Translation by Leveraging Alignment Information</a></strong><br><a href=/people/z/zehui-lin/>Zehui Lin</a>
|
<a href=/people/x/xiao-pan/>Xiao Pan</a>
|
<a href=/people/m/mingxuan-wang/>Mingxuan Wang</a>
|
<a href=/people/x/xipeng-qiu/>Xipeng Qiu</a>
|
<a href=/people/j/jiangtao-feng/>Jiangtao Feng</a>
|
<a href=/people/h/hao-zhou/>Hao Zhou</a>
|
<a href=/people/l/lei-li/>Lei Li</a><br><a href=/volumes/2020.emnlp-main/ class=text-muted>Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--210><div class="card-body p-3 small">We investigate the following question for machine translation (MT): can we develop a single universal MT model to serve as the common seed and obtain derivative and improved models on arbitrary language pairs? We propose mRASP, an approach to pre-train a universal multilingual neural machine translation model. Our key idea in mRASP is its novel technique of random aligned substitution, which brings words and phrases with similar meanings across multiple languages closer in the representation space. We pre-train a mRASP model on 32 language pairs jointly with only public datasets. The <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> is then fine-tuned on downstream language pairs to obtain specialized MT models. We carry out extensive experiments on 42 translation directions across a diverse settings, including low, medium, rich resource, and as well as transferring to exotic language pairs. Experimental results demonstrate that mRASP achieves significant performance improvement compared to directly training on those target pairs. It is the first time to verify that multiple lowresource language pairs can be utilized to improve rich resource MT. Surprisingly, mRASP is even able to improve the translation quality on exotic languages that never occur in the pretraining corpus. Code, data, and pre-trained models are available at https://github. com / linzehui / mRASP.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.733.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--733 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.733 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939378 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.733" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.733/>On the Sentence Embeddings from Pre-trained Language Models</a></strong><br><a href=/people/b/bohan-li/>Bohan Li</a>
|
<a href=/people/h/hao-zhou/>Hao Zhou</a>
|
<a href=/people/j/junxian-he/>Junxian He</a>
|
<a href=/people/m/mingxuan-wang/>Mingxuan Wang</a>
|
<a href=/people/y/yiming-yang/>Yiming Yang</a>
|
<a href=/people/l/lei-li/>Lei Li</a><br><a href=/volumes/2020.emnlp-main/ class=text-muted>Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--733><div class="card-body p-3 small">Pre-trained contextual representations like <a href=https://en.wikipedia.org/wiki/BERT>BERT</a> have achieved great success in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a>. However, the <a href=https://en.wikipedia.org/wiki/Sentence_embedding>sentence embeddings</a> from the pre-trained language models without <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a> have been found to poorly capture semantic meaning of sentences. In this paper, we argue that the <a href=https://en.wikipedia.org/wiki/Semantics>semantic information</a> in the BERT embeddings is not fully exploited. We first reveal the theoretical connection between the masked language model pre-training objective and the semantic similarity task theoretically, and then analyze the BERT sentence embeddings empirically. We find that BERT always induces a non-smooth anisotropic semantic space of sentences, which harms its performance of <a href=https://en.wikipedia.org/wiki/Semantic_similarity>semantic similarity</a>. To address this issue, we propose to transform the anisotropic sentence embedding distribution to a smooth and isotropic Gaussian distribution through normalizing flows that are learned with an <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised objective</a>. Experimental results show that our proposed BERT-flow method obtains significant performance gains over the state-of-the-art sentence embeddings on a variety of semantic textual similarity tasks. The code is available at.<url>https://github.com/bohanli/BERT-flow</url>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.acl-main.28.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--acl-main--28 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.acl-main.28 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://slideslive.com/38928768 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.acl-main.28/>Unsupervised Paraphrasing by Simulated Annealing</a></strong><br><a href=/people/x/xianggen-liu/>Xianggen Liu</a>
|
<a href=/people/l/lili-mou/>Lili Mou</a>
|
<a href=/people/f/fandong-meng/>Fandong Meng</a>
|
<a href=/people/h/hao-zhou/>Hao Zhou</a>
|
<a href=/people/j/jie-zhou/>Jie Zhou</a>
|
<a href=/people/s/sen-song/>Sen Song</a><br><a href=/volumes/2020.acl-main/ class=text-muted>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--acl-main--28><div class="card-body p-3 small">We propose UPSA, a novel approach that accomplishes Unsupervised Paraphrasing by Simulated Annealing. We model <a href=https://en.wikipedia.org/wiki/Paraphrase_generation>paraphrase generation</a> as an <a href=https://en.wikipedia.org/wiki/Optimization_problem>optimization problem</a> and propose a sophisticated <a href=https://en.wikipedia.org/wiki/Loss_function>objective function</a>, involving <a href=https://en.wikipedia.org/wiki/Semantic_similarity>semantic similarity</a>, expression diversity, and language fluency of paraphrases. UPSA searches the sentence space towards this objective by performing a sequence of local editing. We evaluate our approach on various datasets, namely, <a href=https://en.wikipedia.org/wiki/Quora>Quora</a>, <a href=https://en.wikipedia.org/wiki/Wikianswers>Wikianswers</a>, MSCOCO, and <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a>. Extensive results show that UPSA achieves the state-of-the-art performance compared with previous <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised methods</a> in terms of both automatic and human evaluations. Further, our approach outperforms most existing domain-adapted supervised models, showing the generalizability of UPSA.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.acl-main.635.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--acl-main--635 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.acl-main.635 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://slideslive.com/38928880 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.acl-main.635" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.acl-main.635/>KdConv : A Chinese Multi-domain Dialogue Dataset Towards Multi-turn Knowledge-driven Conversation<span class=acl-fixed-case>K</span>d<span class=acl-fixed-case>C</span>onv: A <span class=acl-fixed-case>C</span>hinese Multi-domain Dialogue Dataset Towards Multi-turn Knowledge-driven Conversation</a></strong><br><a href=/people/h/hao-zhou/>Hao Zhou</a>
|
<a href=/people/c/chujie-zheng/>Chujie Zheng</a>
|
<a href=/people/k/kaili-huang/>Kaili Huang</a>
|
<a href=/people/m/minlie-huang/>Minlie Huang</a>
|
<a href=/people/x/xiaoyan-zhu/>Xiaoyan Zhu</a><br><a href=/volumes/2020.acl-main/ class=text-muted>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--acl-main--635><div class="card-body p-3 small">The research of knowledge-driven conversational systems is largely limited due to the lack of dialog data which consists of multi-turn conversations on multiple topics and with knowledge annotations. In this paper, we propose a Chinese multi-domain knowledge-driven conversation dataset, KdConv, which grounds the topics in multi-turn conversations to <a href=https://en.wikipedia.org/wiki/Knowledge_graph>knowledge graphs</a>. Our <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> contains 4.5 K conversations from three domains (film, <a href=https://en.wikipedia.org/wiki/Music>music</a>, and travel), and 86 K utterances with an <a href=https://en.wikipedia.org/wiki/Arithmetic_mean>average turn number</a> of 19.0. These <a href=https://en.wikipedia.org/wiki/Conversation>conversations</a> contain in-depth discussions on related topics and natural transition between multiple topics. To facilitate the following research on this <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a>, we provide several benchmark models. Comparative results show that the <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> can be enhanced by introducing background knowledge, yet there is still a large space for leveraging <a href=https://en.wikipedia.org/wiki/Knowledge>knowledge</a> to model multi-turn conversations for further research. Results also show that there are obvious performance differences between different domains, indicating that it is worth further explore <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a> and domain adaptation. The <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> and <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmark models</a> are publicly available.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.acl-demos.1.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--acl-demos--1 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.acl-demos.1 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://slideslive.com/38928588 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.acl-demos.1/>Xiaomingbot : A Multilingual Robot News Reporter<span class=acl-fixed-case>X</span>iaomingbot: <span class=acl-fixed-case>A</span> <span class=acl-fixed-case>M</span>ultilingual <span class=acl-fixed-case>R</span>obot <span class=acl-fixed-case>N</span>ews <span class=acl-fixed-case>R</span>eporter</a></strong><br><a href=/people/r/runxin-xu/>Runxin Xu</a>
|
<a href=/people/j/jun-cao/>Jun Cao</a>
|
<a href=/people/m/mingxuan-wang/>Mingxuan Wang</a>
|
<a href=/people/j/jiaze-chen/>Jiaze Chen</a>
|
<a href=/people/h/hao-zhou/>Hao Zhou</a>
|
<a href=/people/y/ying-zeng/>Ying Zeng</a>
|
<a href=/people/y/yuping-wang/>Yuping Wang</a>
|
<a href=/people/l/li-chen/>Li Chen</a>
|
<a href=/people/x/xiang-yin/>Xiang Yin</a>
|
<a href=/people/x/xijin-zhang/>Xijin Zhang</a>
|
<a href=/people/s/songcheng-jiang/>Songcheng Jiang</a>
|
<a href=/people/y/yuxuan-wang/>Yuxuan Wang</a>
|
<a href=/people/l/lei-li/>Lei Li</a><br><a href=/volumes/2020.acl-demos/ class=text-muted>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--acl-demos--1><div class="card-body p-3 small">This paper proposes the building of Xiaomingbot, an intelligent, multilingual and multimodal software robot equipped with four inte- gral capabilities : news generation, news translation, news reading and avatar animation. Its <a href=https://en.wikipedia.org/wiki/System>system</a> summarizes <a href=https://en.wikipedia.org/wiki/Media_of_China>Chinese news</a> that <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> automatically generates from data tables. Next, <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> translates the summary or the full article into multiple languages, and reads the multi- lingual rendition through <a href=https://en.wikipedia.org/wiki/Speech_synthesis>synthesized speech</a>. Notably, Xiaomingbot utilizes a voice cloning technology to synthesize the speech trained from a real person&#8217;s voice data in one input language. The proposed <a href=https://en.wikipedia.org/wiki/System>system</a> enjoys several merits : it has an <a href=https://en.wikipedia.org/wiki/Avatar_(computing)>animated avatar</a>, and is able to generate and read multilingual news. Since <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> was put into practice, Xiaomingbot has written over 600,000 articles, and gained over 150,000 followers on <a href=https://en.wikipedia.org/wiki/Social_media>social media platforms</a>.</div></div><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-info align-middle mr-1" href=#abstract-D19-2005 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-2005 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-2005/>Discreteness in Neural Natural Language Processing</a></strong><br><a href=/people/l/lili-mou/>Lili Mou</a>
|
<a href=/people/h/hao-zhou/>Hao Zhou</a>
|
<a href=/people/l/lei-li/>Lei Li</a><br><a href=/volumes/D19-2/ class=text-muted>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP): Tutorial Abstracts</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-2005><div class="card-body p-3 small">This tutorial provides a comprehensive guide to the process of discreteness in neural NLP.\n\nAs a gentle start, we will briefly introduce the background of deep learning based NLP, where we point out the ubiquitous discreteness of natural language and its challenges in neural information processing. Particularly, we will focus on how such discreteness plays a role in the input space, the latent space, and the output space of a neural network. In each part, we will provide examples, discuss machine learning techniques, as well as demonstrate NLP applications.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1125.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1125 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1125 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-1125/>Imitation Learning for Non-Autoregressive Neural Machine Translation</a></strong><br><a href=/people/b/bingzhen-wei/>Bingzhen Wei</a>
|
<a href=/people/m/mingxuan-wang/>Mingxuan Wang</a>
|
<a href=/people/h/hao-zhou/>Hao Zhou</a>
|
<a href=/people/j/junyang-lin/>Junyang Lin</a>
|
<a href=/people/x/xu-sun/>Xu Sun</a><br><a href=/volumes/P19-1/ class=text-muted>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1125><div class="card-body p-3 small">Non-autoregressive translation models (NAT) have achieved impressive inference speedup. A potential issue of the existing NAT algorithms, however, is that the decoding is conducted in parallel, without directly considering previous context. In this paper, we propose an imitation learning framework for non-autoregressive machine translation, which still enjoys the fast translation speed but gives comparable translation performance compared to its auto-regressive counterpart. We conduct experiments on the IWSLT16, WMT14 and WMT16 datasets. Our proposed model achieves a significant speedup over the <a href=https://en.wikipedia.org/wiki/Autoregressive_model>autoregressive models</a>, while keeping the translation quality comparable to the <a href=https://en.wikipedia.org/wiki/Autoregressive_model>autoregressive models</a>. By sampling <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>sentence length</a> in parallel at <a href=https://en.wikipedia.org/wiki/Time_complexity>inference time</a>, we achieve the performance of 31.85 BLEU on WMT16 RoEn and 30.68 BLEU on IWSLT16 EnDe.<tex-math>\\rightarrow</tex-math>En and 30.68 BLEU on IWSLT16 En<tex-math>\\rightarrow</tex-math>De.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1559.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1559 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1559 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P19-1559.Supplementary.pdf data-toggle=tooltip data-placement=top title=Supplementary><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1559/>Generating Fluent Adversarial Examples for Natural Languages</a></strong><br><a href=/people/h/huangzhao-zhang/>Huangzhao Zhang</a>
|
<a href=/people/h/hao-zhou/>Hao Zhou</a>
|
<a href=/people/n/ning-miao/>Ning Miao</a>
|
<a href=/people/l/lei-li/>Lei Li</a><br><a href=/volumes/P19-1/ class=text-muted>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1559><div class="card-body p-3 small">Efficiently building an adversarial attacker for natural language processing (NLP) tasks is a real challenge. Firstly, as the sentence space is discrete, it is difficult to make <a href=https://en.wikipedia.org/wiki/Perturbation_theory_(quantum_mechanics)>small perturbations</a> along the direction of <a href=https://en.wikipedia.org/wiki/Gradient>gradients</a>. Secondly, the fluency of the generated examples can not be guaranteed. In this paper, we propose MHA, which addresses both problems by performing Metropolis-Hastings sampling, whose proposal is designed with the guidance of <a href=https://en.wikipedia.org/wiki/Gradient>gradients</a>. Experiments on <a href=https://en.wikipedia.org/wiki/IMDb>IMDB</a> and SNLI show that our proposed MHAoutperforms the baseline model on attacking capability. Adversarial training with MHA also leads to better robustness and performance.</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1492.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1492 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1492 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D18-1492.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1492" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D18-1492/>On Tree-Based Neural Sentence Modeling</a></strong><br><a href=/people/h/haoyue-shi/>Haoyue Shi</a>
|
<a href=/people/h/hao-zhou/>Hao Zhou</a>
|
<a href=/people/j/jiaze-chen/>Jiaze Chen</a>
|
<a href=/people/l/lei-li/>Lei Li</a><br><a href=/volumes/D18-1/ class=text-muted>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1492><div class="card-body p-3 small">Neural networks with tree-based sentence encoders have shown better results on many <a href=https://en.wikipedia.org/wiki/Downstream_(networking)>downstream tasks</a>. Most of existing tree-based encoders adopt syntactic parsing trees as the explicit structure prior. To study the effectiveness of different <a href=https://en.wikipedia.org/wiki/Tree_(data_structure)>tree structures</a>, we replace the parsing trees with <a href=https://en.wikipedia.org/wiki/Tree_(data_structure)>trivial trees</a> (i.e., binary balanced tree, left-branching tree and right-branching tree) in the <a href=https://en.wikipedia.org/wiki/Encoder>encoders</a>. Though trivial trees contain no <a href=https://en.wikipedia.org/wiki/Syntax>syntactic information</a>, those <a href=https://en.wikipedia.org/wiki/Encoder>encoders</a> get competitive or even better results on all of the ten <a href=https://en.wikipedia.org/wiki/Downstream_(networking)>downstream tasks</a> we investigated. This surprising result indicates that explicit syntax guidance may not be the main contributor to the superior performances of tree-based neural sentence modeling. Further analysis show that <a href=https://en.wikipedia.org/wiki/Tree_model>tree modeling</a> gives better results when crucial words are closer to the final representation. Additional experiments give more clues on how to design an effective tree-based encoder. Our code is open-source and available at.<url>https://github.com/ExplorerFreda/TreeEnc</url>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/Q18-1011.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-Q18-1011 data-toggle=collapse aria-expanded=false aria-controls=abstract-Q18-1011 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=Q18-1011" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/Q18-1011/>Modeling Past and Future for Neural Machine Translation</a></strong><br><a href=/people/z/zaixiang-zheng/>Zaixiang Zheng</a>
|
<a href=/people/h/hao-zhou/>Hao Zhou</a>
|
<a href=/people/s/shujian-huang/>Shujian Huang</a>
|
<a href=/people/l/lili-mou/>Lili Mou</a>
|
<a href=/people/x/xinyu-dai/>Xinyu Dai</a>
|
<a href=/people/j/jiajun-chen/>Jiajun Chen</a>
|
<a href=/people/z/zhaopeng-tu/>Zhaopeng Tu</a><br><a href=/volumes/Q18-1/ class=text-muted>Transactions of the Association for Computational Linguistics, Volume 6</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-Q18-1011><div class="card-body p-3 small">Existing neural machine translation systems do not explicitly model what has been translated and what has not during the decoding phase. To address this problem, we propose a novel mechanism that separates the source information into two parts : translated Past contents and untranslated Future contents, which are modeled by two additional recurrent layers. The Past and Future contents are fed to both the attention model and the decoder states, which provides Neural Machine Translation (NMT) systems with the knowledge of translated and untranslated contents. Experimental results show that the proposed approach significantly improves the performance in Chinese-English, German-English, and English-German translation tasks. Specifically, the proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms the conventional coverage model in terms of both the translation quality and the alignment error rate.</div></div><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2092.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2092 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2092 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P17-2092" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P17-2092/>Chunk-Based Bi-Scale Decoder for <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a></a></strong><br><a href=/people/h/hao-zhou/>Hao Zhou</a>
|
<a href=/people/z/zhaopeng-tu/>Zhaopeng Tu</a>
|
<a href=/people/s/shujian-huang/>Shujian Huang</a>
|
<a href=/people/x/xiaohua-liu/>Xiaohua Liu</a>
|
<a href=/people/h/hang-li/>Hang Li</a>
|
<a href=/people/j/jiajun-chen/>Jiajun Chen</a><br><a href=/volumes/P17-2/ class=text-muted>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2092><div class="card-body p-3 small">In typical neural machine translation (NMT), the decoder generates a sentence word by word, packing all <a href=https://en.wikipedia.org/wiki/Granularity>linguistic granularities</a> in the same time-scale of <a href=https://en.wikipedia.org/wiki/Neural_network>RNN</a>. In this paper, we propose a new type of <a href=https://en.wikipedia.org/wiki/Code>decoder</a> for NMT, which splits the decode state into two parts and updates them in two different <a href=https://en.wikipedia.org/wiki/Time_complexity>time-scales</a>. Specifically, we first predict a chunk time-scale state for phrasal modeling, on top of which multiple word time-scale states are generated. In this way, the target sentence is translated hierarchically from chunks to words, with information in different granularities being leveraged. Experiments show that our proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> significantly improves the <a href=https://en.wikipedia.org/wiki/Translation_(biology)>translation</a> performance over the state-of-the-art NMT model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1079.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1079 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1079 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1079/>Word-Context Character Embeddings for Chinese Word Segmentation<span class=acl-fixed-case>C</span>hinese Word Segmentation</a></strong><br><a href=/people/h/hao-zhou/>Hao Zhou</a>
|
<a href=/people/z/zhenting-yu/>Zhenting Yu</a>
|
<a href=/people/y/yue-zhang/>Yue Zhang</a>
|
<a href=/people/s/shujian-huang/>Shujian Huang</a>
|
<a href=/people/x/xinyu-dai/>Xinyu Dai</a>
|
<a href=/people/j/jiajun-chen/>Jiajun Chen</a><br><a href=/volumes/D17-1/ class=text-muted>Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1079><div class="card-body p-3 small">Neural parsers have benefited from automatically labeled data via dependency-context word embeddings. We investigate training character embeddings on a word-based context in a similar way, showing that the simple method improves state-of-the-art neural word segmentation models significantly, beating tri-training baselines for leveraging auto-segmented data.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Hao+Zhou" title="Search for 'Hao Zhou' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/l/lei-li/ class=align-middle>Lei Li</a>
<span class="badge badge-secondary align-middle ml-2">11</span></li><li class=list-group-item><a href=/people/m/mingxuan-wang/ class=align-middle>Mingxuan Wang</a>
<span class="badge badge-secondary align-middle ml-2">5</span></li><li class=list-group-item><a href=/people/c/changzhi-sun/ class=align-middle>Changzhi Sun</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/z/zaixiang-zheng/ class=align-middle>Zaixiang Zheng</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/j/jiangtao-feng/ class=align-middle>Jiangtao Feng</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/l/lili-mou/ class=align-middle>Lili Mou</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/m/minlie-huang/ class=align-middle>Minlie Huang</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/x/xiaoyan-zhu/ class=align-middle>Xiaoyan Zhu</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/s/shujian-huang/ class=align-middle>Shujian Huang</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/j/jiajun-chen/ class=align-middle>Jiajun Chen</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/y/yijun-wang/ class=align-middle>Yijun Wang</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/y/yuanbin-wu/ class=align-middle>Yuanbin Wu</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/j/junchi-yan/ class=align-middle>Junchi Yan</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/z/zehui-lin/ class=align-middle>Zehui Lin</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/j/jie-zhou/ class=align-middle>Jie Zhou</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/j/jiaze-chen/ class=align-middle>Jiaze Chen</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/z/zhaopeng-tu/ class=align-middle>Zhaopeng Tu</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/x/xinyu-dai/ class=align-middle>Xinyu Dai</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/j/junjie-wu/ class=align-middle>Junjie Wu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jingjing-xu/ class=align-middle>Jingjing Xu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/chun-gan/ class=align-middle>Chun Gan</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/x/xiao-pan/ class=align-middle>Xiao Pan</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/x/xipeng-qiu/ class=align-middle>Xipeng Qiu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/b/bohan-li/ class=align-middle>Bohan Li</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/junxian-he/ class=align-middle>Junxian He</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yiming-yang/ class=align-middle>Yiming Yang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/x/xianggen-liu/ class=align-middle>Xianggen Liu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/f/fandong-meng/ class=align-middle>Fandong Meng</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/sen-song/ class=align-middle>Sen Song</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/chujie-zheng/ class=align-middle>Chujie Zheng</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/kaili-huang/ class=align-middle>Kaili Huang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/runxin-xu/ class=align-middle>Runxin Xu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jun-cao/ class=align-middle>Jun Cao</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/ying-zeng/ class=align-middle>Ying Zeng</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yuping-wang/ class=align-middle>Yuping Wang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/li-chen/ class=align-middle>Li Chen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/x/xiang-yin/ class=align-middle>Xiang Yin</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/x/xijin-zhang/ class=align-middle>Xijin Zhang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/songcheng-jiang/ class=align-middle>Songcheng Jiang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yuxuan-wang/ class=align-middle>Yuxuan Wang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/x/xiaohua-liu/ class=align-middle>Xiaohua Liu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/hang-li/ class=align-middle>Hang Li</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/p/pei-ke/ class=align-middle>Pei Ke</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yankai-lin/ class=align-middle>Yankai Lin</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/p/peng-li/ class=align-middle>Peng Li</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/haoyue-shi/ class=align-middle>Haoyue Shi</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/dongyu-ru/ class=align-middle>Dongyu Ru</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/lin-qiu/ class=align-middle>Lin Qiu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/w/weinan-zhang/ class=align-middle>Weinan Zhang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yong-yu/ class=align-middle>Yong Yu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yong-liu/ class=align-middle>Yong Liu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/w/wei-chen/ class=align-middle>Wei Chen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/zhenting-yu/ class=align-middle>Zhenting Yu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yue-zhang/ class=align-middle>Yue Zhang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/lihua-qian/ class=align-middle>Lihua Qian</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yi-zhou/ class=align-middle>Yi Zhou</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yaoming-zhu/ class=align-middle>Yaoming Zhu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/shanbo-cheng/ class=align-middle>Shanbo Cheng</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/b/bingzhen-wei/ class=align-middle>Bingzhen Wei</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/junyang-lin/ class=align-middle>Junyang Lin</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/x/xu-sun/ class=align-middle>Xu Sun</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/huangzhao-zhang/ class=align-middle>Huangzhao Zhang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/n/ning-miao/ class=align-middle>Ning Miao</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/acl/ class=align-middle>ACL</a><span class="badge badge-secondary align-middle ml-2">9</span></li><li class=list-group-item><a href=/venues/emnlp/ class=align-middle>EMNLP</a><span class="badge badge-secondary align-middle ml-2">7</span></li><li class=list-group-item><a href=/venues/deelio/ class=align-middle>DeeLIO</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/eacl/ class=align-middle>EACL</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/tacl/ class=align-middle>TACL</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-venues aria-expanded=false aria-controls=more-venues>show all...</li><div class="collapse border-top" id=more-venues><li class=list-group-item><a href=/venues/wmt/ class=align-middle>WMT</a><span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>