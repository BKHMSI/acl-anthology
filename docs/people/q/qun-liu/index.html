<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Qun Liu - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Qun</span> <span class=font-weight-bold>Liu</span></h2><hr><div class=row><div class=col-lg-9><h4>2022</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.acl-long.151.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--acl-long--151 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.acl-long.151 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.acl-long.151/>bert2<span class=acl-fixed-case>BERT</span>: Towards Reusable Pretrained Language Models</a></strong><br><a href=/people/c/cheng-chen/>Cheng Chen</a>
|
<a href=/people/y/yichun-yin/>Yichun Yin</a>
|
<a href=/people/l/lifeng-shang/>Lifeng Shang</a>
|
<a href=/people/x/xin-jiang/>Xin Jiang</a>
|
<a href=/people/y/yujia-qin/>Yujia Qin</a>
|
<a href=/people/f/fengyu-wang/>Fengyu Wang</a>
|
<a href=/people/z/zhi-wang/>Zhi Wang</a>
|
<a href=/people/x/xiao-chen/>Xiao Chen</a>
|
<a href=/people/z/zhiyuan-liu/>Zhiyuan Liu</a>
|
<a href=/people/q/qun-liu/>Qun Liu</a><br><a href=/volumes/2022.acl-long/ class=text-muted>Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--acl-long--151><div class="card-body p-3 small">In recent years, researchers tend to pre-train ever-larger language models to explore the upper limit of deep models. However, large language model pre-training costs intensive computational resources, and most of the models are trained from scratch without reusing the existing pre-trained models, which is wasteful. In this paper, we propose bert2BERT, which can effectively transfer the knowledge of an existing smaller pre-trained model to a large model through parameter initialization and significantly improve the pre-training efficiency of the large model. Specifically, we extend the previous function-preserving method proposed in computer vision on the Transformer-based language model, and further improve it by proposing a novel method, advanced knowledge for large model&#8217;s initialization. In addition, a two-stage learning method is proposed to further accelerate the pre-training. We conduct extensive experiments on representative PLMs (e.g., BERT and GPT) and demonstrate that (1) our method can save a significant amount of training cost compared with baselines including learning from scratch, StackBERT and MSLT; (2) our method is generic and applicable to different types of pre-trained models. In particular, bert2BERT saves about 45% and 47% computational cost of pre-training BERT<tex-math>_{\\rm BASE}</tex-math> and GPT<tex-math>_{\\rm BASE}</tex-math> by reusing the models of almost their half sizes.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.acl-long.442.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--acl-long--442 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.acl-long.442 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.acl-long.442" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2022.acl-long.442/>Universal Conditional Masked Language Pre-training for Neural Machine Translation</a></strong><br><a href=/people/p/pengfei-li/>Pengfei Li</a>
|
<a href=/people/l/liangyou-li/>Liangyou Li</a>
|
<a href=/people/m/meng-zhang/>Meng Zhang</a>
|
<a href=/people/m/minghao-wu/>Minghao Wu</a>
|
<a href=/people/q/qun-liu/>Qun Liu</a><br><a href=/volumes/2022.acl-long/ class=text-muted>Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--acl-long--442><div class="card-body p-3 small">Pre-trained sequence-to-sequence models have significantly improved Neural Machine Translation (NMT). Different from prior works where pre-trained models usually adopt an unidirectional decoder, this paper demonstrates that pre-training a sequence-to-sequence model but with a bidirectional decoder can produce notable performance gains for both Autoregressive and Non-autoregressive NMT. Specifically, we propose CeMAT, a conditional masked language model pre-trained on large-scale bilingual and monolingual corpora in many languages. We also introduce two simple but effective methods to enhance the CeMAT, aligned code-switching & masking and dynamic dual-masking. We conduct extensive experiments and show that our CeMAT can achieve significant performance improvement for all scenarios from low- to extremely high-resource languages, i.e., up to +14.4 BLEU on low resource and +7.9 BLEU improvements on average for Autoregressive NMT. For Non-autoregressive NMT, we demonstrate it can also produce consistent performance gains, i.e., up to +5.3 BLEU. To the best of our knowledge, this is the first work to pre-train a unified model for fine-tuning on both NMT tasks. Code, data, and pre-trained models are available at https://github.com/huawei-noah/Pretrained-Language-Model/CeMAT</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.acl-long.445.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--acl-long--445 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.acl-long.445 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.acl-long.445" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2022.acl-long.445/>Achieving Reliable Human Assessment of Open-Domain Dialogue Systems</a></strong><br><a href=/people/t/tianbo-ji/>Tianbo Ji</a>
|
<a href=/people/y/yvette-graham/>Yvette Graham</a>
|
<a href=/people/g/gareth-jones/>Gareth Jones</a>
|
<a href=/people/c/chenyang-lyu/>Chenyang Lyu</a>
|
<a href=/people/q/qun-liu/>Qun Liu</a><br><a href=/volumes/2022.acl-long/ class=text-muted>Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--acl-long--445><div class="card-body p-3 small">Evaluation of open-domain dialogue systems is highly challenging and development of better techniques is highlighted time and again as desperately needed. Despite substantial efforts to carry out reliable live evaluation of systems in recent competitions, annotations have been abandoned and reported as too unreliable to yield sensible results. This is a serious problem since automatic metrics are not known to provide a good indication of what may or may not be a high-quality conversation. Answering the distress call of competitions that have emphasized the urgent need for better evaluation techniques in dialogue, we present the successful development of human evaluation that is highly reliable while still remaining feasible and low cost. Self-replication experiments reveal almost perfectly repeatable results with a correlation of <tex-math>r=0.969</tex-math>. Furthermore, due to the lack of appropriate methods of statistical significance testing, the likelihood of potential improvements to systems occurring due to chance is rarely taken into account in dialogue evaluation, and the evaluation we propose facilitates application of standard tests. Since we have developed a highly reliable evaluation method, new insights into system performance can be revealed. We therefore include a comparison of state-of-the-art models (i) with and without personas, to measure the contribution of personas to conversation quality, as well as (ii) prescribed versus freely chosen topics. Interestingly with respect to personas, results indicate that personas do not positively contribute to conversation quality as expected.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.acl-long.493.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--acl-long--493 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.acl-long.493 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2022.acl-long.493.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.acl-long.493" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2022.acl-long.493/>Hyperlink-induced Pre-training for Passage Retrieval in Open-domain Question Answering</a></strong><br><a href=/people/j/jiawei-zhou/>Jiawei Zhou</a>
|
<a href=/people/x/xiaoguang-li/>Xiaoguang Li</a>
|
<a href=/people/l/lifeng-shang/>Lifeng Shang</a>
|
<a href=/people/l/lan-luo/>Lan Luo</a>
|
<a href=/people/k/ke-zhan/>Ke Zhan</a>
|
<a href=/people/e/enrui-hu/>Enrui Hu</a>
|
<a href=/people/x/xinyu-zhang/>Xinyu Zhang</a>
|
<a href=/people/h/hao-jiang/>Hao Jiang</a>
|
<a href=/people/z/zhao-cao/>Zhao Cao</a>
|
<a href=/people/f/fan-yu/>Fan Yu</a>
|
<a href=/people/x/xin-jiang/>Xin Jiang</a>
|
<a href=/people/q/qun-liu/>Qun Liu</a>
|
<a href=/people/l/lei-chen/>Lei Chen</a><br><a href=/volumes/2022.acl-long/ class=text-muted>Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--acl-long--493><div class="card-body p-3 small">To alleviate the data scarcity problem in training question answering systems, recent works propose additional intermediate pre-training for dense passage retrieval (DPR). However, there still remains a large discrepancy between the provided upstream signals and the downstream question-passage relevance, which leads to less improvement. To bridge this gap, we propose the HyperLink-induced Pre-training (HLP), a method to pre-train the dense retriever with the text relevance induced by hyperlink-based topology within Web documents. We demonstrate that the hyperlink-based structures of dual-link and co-mention can provide effective relevance signals for large-scale pre-training that better facilitate downstream passage retrieval. We investigate the effectiveness of our approach across a wide range of open-domain QA datasets under zero-shot, few-shot, multi-hop, and out-of-domain scenarios. The experiments show our HLP outperforms the BM25 by up to 7 points as well as other pre-training methods by more than 10 points in terms of top-20 retrieval accuracy under the zero-shot scenario. Furthermore, HLP significantly outperforms other pre-training methods under the other scenarios.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-acl.136.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-acl--136 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-acl.136 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.findings-acl.136/>How Pre-trained Language Models Capture Factual Knowledge? A Causal-Inspired Analysis</a></strong><br><a href=/people/s/shaobo-li/>Shaobo Li</a>
|
<a href=/people/x/xiaoguang-li/>Xiaoguang Li</a>
|
<a href=/people/l/lifeng-shang/>Lifeng Shang</a>
|
<a href=/people/z/zhenhua-dong/>Zhenhua Dong</a>
|
<a href=/people/c/cheng-jie-sun/>Chengjie Sun</a>
|
<a href=/people/b/bingquan-liu/>Bingquan Liu</a>
|
<a href=/people/z/zhenzhou-ji/>Zhenzhou Ji</a>
|
<a href=/people/x/xin-jiang/>Xin Jiang</a>
|
<a href=/people/q/qun-liu/>Qun Liu</a><br><a href=/volumes/2022.findings-acl/ class=text-muted>Findings of the Association for Computational Linguistics: ACL 2022</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-acl--136><div class="card-body p-3 small">Recently, there has been a trend to investigate the factual knowledge captured by Pre-trained Language Models (PLMs). Many works show the PLMs&#8217; ability to fill in the missing factual words in cloze-style prompts such as &#8221;Dante was born in [MASK].&#8221; However, it is still a mystery how PLMs generate the results correctly: relying on effective clues or shortcut patterns? We try to answer this question by a causal-inspired analysis that quantitatively measures and evaluates the word-level patterns that PLMs depend on to generate the missing words. We check the words that have three typical associations with the missing words: knowledge-dependent, positionally close, and highly co-occurred. Our analysis shows: (1) PLMs generate the missing factual words more by the positionally close and highly co-occurred words than the knowledge-dependent words; (2) the dependence on the knowledge-dependent words is more effective than the positionally close and highly co-occurred words. Accordingly, we conclude that the PLMs capture the factual knowledge ineffectively because of depending on the inadequate associations.</div></div><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.autosimtrans-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.autosimtrans-1.0/>Proceedings of the Second Workshop on Automatic Simultaneous Translation</a></strong><br><a href=/people/h/hua-wu/>Hua Wu</a>
|
<a href=/people/c/colin-cherry/>Colin Cherry</a>
|
<a href=/people/l/liang-huang/>Liang Huang</a>
|
<a href=/people/z/zhongjun-he/>Zhongjun He</a>
|
<a href=/people/q/qun-liu/>Qun Liu</a>
|
<a href=/people/m/maha-elbayad/>Maha Elbayad</a>
|
<a href=/people/m/mark-liberman/>Mark Liberman</a>
|
<a href=/people/h/haifeng-wang/>Haifeng Wang</a>
|
<a href=/people/m/mingbo-ma/>Mingbo Ma</a>
|
<a href=/people/r/ruiqing-zhang/>Ruiqing Zhang</a><br><a href=/volumes/2021.autosimtrans-1/ class=text-muted>Proceedings of the Second Workshop on Automatic Simultaneous Translation</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.256.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--256 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.256 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.256" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.256/>Neural Machine Translation with Heterogeneous Topic Knowledge Embeddings</a></strong><br><a href=/people/w/weixuan-wang/>Weixuan Wang</a>
|
<a href=/people/w/wei-peng/>Wei Peng</a>
|
<a href=/people/m/meng-zhang/>Meng Zhang</a>
|
<a href=/people/q/qun-liu/>Qun Liu</a><br><a href=/volumes/2021.emnlp-main/ class=text-muted>Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--256><div class="card-body p-3 small">Neural Machine Translation (NMT) has shown a strong ability to utilize <a href=https://en.wikipedia.org/wiki/Context_(language_use)>local context</a> to disambiguate the meaning of words. However, it remains a challenge for <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NMT</a> to leverage <a href=https://en.wikipedia.org/wiki/Context_(language_use)>broader context information</a> like topics. In this paper, we propose heterogeneous ways of embedding topic information at the sentence level into an NMT model to improve <a href=https://en.wikipedia.org/wiki/Translation_(biology)>translation</a> performance. Specifically, the topic information can be incorporated as pre-encoder topic embedding, post-encoder topic embedding, and decoder topic embedding to increase the likelihood of selecting target words from the same topic of the source sentence. Experimental results show that NMT models with the proposed topic knowledge embedding outperform the baselines on the English-German and English-French translation tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.267.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--267 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.267 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.267/>Self-Supervised Quality Estimation for <a href=https://en.wikipedia.org/wiki/Machine_translation>Machine Translation</a></a></strong><br><a href=/people/y/yuanhang-zheng/>Yuanhang Zheng</a>
|
<a href=/people/z/zhixing-tan/>Zhixing Tan</a>
|
<a href=/people/m/meng-zhang/>Meng Zhang</a>
|
<a href=/people/m/mieradilijiang-maimaiti/>Mieradilijiang Maimaiti</a>
|
<a href=/people/h/huanbo-luan/>Huanbo Luan</a>
|
<a href=/people/m/maosong-sun/>Maosong Sun</a>
|
<a href=/people/q/qun-liu/>Qun Liu</a>
|
<a href=/people/y/yang-liu-ict/>Yang Liu</a><br><a href=/volumes/2021.emnlp-main/ class=text-muted>Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--267><div class="card-body p-3 small">Quality estimation (QE) of machine translation (MT) aims to evaluate the quality of machine-translated sentences without references and is important in practical applications of MT. Training QE models require massive parallel data with hand-crafted quality annotations, which are time-consuming and labor-intensive to obtain. To address the issue of the absence of annotated training data, previous studies attempt to develop unsupervised QE methods. However, very few of them can be applied to both sentence- and word-level QE tasks, and they may suffer from noises in the synthetic data. To reduce the negative impact of noises, we propose a self-supervised method for both sentence- and word-level QE, which performs quality estimation by recovering the masked target words. Experimental results show that our method outperforms previous <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised methods</a> on several QE tasks in different language pairs and domains.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.580.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--580 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.580 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.580/>Uncertainty-Aware Balancing for Multilingual and Multi-Domain Neural Machine Translation Training</a></strong><br><a href=/people/m/minghao-wu/>Minghao Wu</a>
|
<a href=/people/y/yitong-li/>Yitong Li</a>
|
<a href=/people/m/meng-zhang/>Meng Zhang</a>
|
<a href=/people/l/liangyou-li/>Liangyou Li</a>
|
<a href=/people/g/gholamreza-haffari/>Gholamreza Haffari</a>
|
<a href=/people/q/qun-liu/>Qun Liu</a><br><a href=/volumes/2021.emnlp-main/ class=text-muted>Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--580><div class="card-body p-3 small">Learning multilingual and multi-domain translation model is challenging as the heterogeneous and imbalanced data make the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> converge inconsistently over different corpora in real world. One common practice is to adjust the share of each corpus in the training, so that the learning process is balanced and low-resource cases can benefit from the high resource ones. However, automatic balancing methods usually depend on the intra- and inter-dataset characteristics, which is usually agnostic or requires human priors. In this work, we propose an approach, MultiUAT, that dynamically adjusts the training data usage based on the model&#8217;s uncertainty on a small set of trusted clean data for multi-corpus machine translation. We experiments with two classes of uncertainty measures on multilingual (16 languages with 4 settings) and multi-domain settings (4 for in-domain and 2 for out-of-domain on English-German translation) and demonstrate our approach MultiUAT substantially outperforms its baselines, including both static and dynamic strategies. We analyze the cross-domain transfer and show the deficiency of static and similarity based methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wmt-1.88.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wmt-1--88 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wmt-1.88 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wmt-1.88/>Huawei AARC’s Submissions to the WMT21 Biomedical Translation Task : Domain Adaption from a Practical Perspective<span class=acl-fixed-case>AARC</span>’s Submissions to the <span class=acl-fixed-case>WMT</span>21 Biomedical Translation Task: Domain Adaption from a Practical Perspective</a></strong><br><a href=/people/w/weixuan-wang/>Weixuan Wang</a>
|
<a href=/people/w/wei-peng/>Wei Peng</a>
|
<a href=/people/x/xupeng-meng/>Xupeng Meng</a>
|
<a href=/people/q/qun-liu/>Qun Liu</a><br><a href=/volumes/2021.wmt-1/ class=text-muted>Proceedings of the Sixth Conference on Machine Translation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wmt-1--88><div class="card-body p-3 small">This paper describes Huawei Artificial Intelligence Application Research Center&#8217;s neural machine translation systems and submissions to the WMT21 biomedical translation shared task. Four of the submissions achieve state-of-the-art BLEU scores based on the official-released automatic evaluation results (EN-FR, EN-IT and ZH-EN). We perform experiments to unveil the practical insights of the involved domain adaptation techniques, including finetuning order, terminology dictionaries, and ensemble decoding. Issues associated with <a href=https://en.wikipedia.org/wiki/Overfitting>overfitting</a> and under-translation are also discussed.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.iwslt-1.17.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--iwslt-1--17 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.iwslt-1.17 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.iwslt-1.17/>Multilingual Speech Translation with Unified Transformer : Huawei Noah’s Ark Lab at IWSLT 2021<span class=acl-fixed-case>N</span>oah’s Ark Lab at <span class=acl-fixed-case>IWSLT</span> 2021</a></strong><br><a href=/people/x/xingshan-zeng/>Xingshan Zeng</a>
|
<a href=/people/l/liangyou-li/>Liangyou Li</a>
|
<a href=/people/q/qun-liu/>Qun Liu</a><br><a href=/volumes/2021.iwslt-1/ class=text-muted>Proceedings of the 18th International Conference on Spoken Language Translation (IWSLT 2021)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--iwslt-1--17><div class="card-body p-3 small">This paper describes the system submitted to the IWSLT 2021 Multilingual Speech Translation (MultiST) task from Huawei Noah&#8217;s Ark Lab. We use a unified transformer architecture for our MultiST model, so that the data from different modalities (i.e., speech and text) and different <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> (i.e., <a href=https://en.wikipedia.org/wiki/Speech_recognition>Speech Recognition</a>, <a href=https://en.wikipedia.org/wiki/Machine_translation>Machine Translation</a>, and Speech Translation) can be exploited to enhance the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a>&#8217;s ability. Specifically, speech and text inputs are firstly fed to different feature extractors to extract acoustic and textual features, respectively. Then, these <a href=https://en.wikipedia.org/wiki/Software_feature>features</a> are processed by a shared encoderdecoder architecture. We apply several training techniques to improve the performance, including <a href=https://en.wikipedia.org/wiki/Multi-task_learning>multi-task learning</a>, task-level curriculum learning, <a href=https://en.wikipedia.org/wiki/Data_augmentation>data augmentation</a>, etc. Our final system achieves significantly better results than bilingual baselines on supervised language pairs and yields reasonable results on zero-shot language pairs.</div></div><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.iwdp-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.iwdp-1.0/>Proceedings of the Second International Workshop of Discourse Processing</a></strong><br><a href=/people/q/qun-liu/>Qun Liu</a>
|
<a href=/people/d/deyi-xiong/>Deyi Xiong</a>
|
<a href=/people/s/shili-ge/>Shili Ge</a>
|
<a href=/people/x/xiaojun-zhang/>Xiaojun Zhang</a><br><a href=/volumes/2020.iwdp-1/ class=text-muted>Proceedings of the Second International Workshop of Discourse Processing</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.37.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--37 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.37 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939194 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.37" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.37/>TernaryBERT : Distillation-aware Ultra-low Bit BERT<span class=acl-fixed-case>T</span>ernary<span class=acl-fixed-case>BERT</span>: Distillation-aware Ultra-low Bit <span class=acl-fixed-case>BERT</span></a></strong><br><a href=/people/w/wei-zhang/>Wei Zhang</a>
|
<a href=/people/l/lu-hou/>Lu Hou</a>
|
<a href=/people/y/yichun-yin/>Yichun Yin</a>
|
<a href=/people/l/lifeng-shang/>Lifeng Shang</a>
|
<a href=/people/x/xiao-chen/>Xiao Chen</a>
|
<a href=/people/x/xin-jiang/>Xin Jiang</a>
|
<a href=/people/q/qun-liu/>Qun Liu</a><br><a href=/volumes/2020.emnlp-main/ class=text-muted>Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--37><div class="card-body p-3 small">Transformer-based pre-training models like <a href=https://en.wikipedia.org/wiki/BERT>BERT</a> have achieved remarkable performance in many natural language processing tasks. However, these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> are both computation and memory expensive, hindering their deployment to resource-constrained devices. In this work, we propose TernaryBERT, which ternarizes the weights in a fine-tuned BERT model. Specifically, we use both approximation-based and loss-aware ternarization methods and empirically investigate the ternarization granularity of different parts of BERT. Moreover, to reduce the accuracy degradation caused by lower capacity of low bits, we leverage the knowledge distillation technique in the training process. Experiments on the GLUE benchmark and SQuAD show that our proposed TernaryBERT outperforms the other BERT quantization methods, and even achieves comparable performance as the full-precision model while being 14.9x smaller.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-demos.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.emnlp-demos.0/>Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</a></strong><br><a href=/people/q/qun-liu/>Qun Liu</a>
|
<a href=/people/d/david-schlangen/>David Schlangen</a><br><a href=/volumes/2020.emnlp-demos/ class=text-muted>Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.acl-main.540.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--acl-main--540 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.acl-main.540 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://slideslive.com/38928950 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.acl-main.540" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.acl-main.540/>Word-level Textual Adversarial Attacking as <a href=https://en.wikipedia.org/wiki/Combinatorial_optimization>Combinatorial Optimization</a></a></strong><br><a href=/people/y/yuan-zang/>Yuan Zang</a>
|
<a href=/people/f/fanchao-qi/>Fanchao Qi</a>
|
<a href=/people/c/chenghao-yang/>Chenghao Yang</a>
|
<a href=/people/z/zhiyuan-liu/>Zhiyuan Liu</a>
|
<a href=/people/m/meng-zhang/>Meng Zhang</a>
|
<a href=/people/q/qun-liu/>Qun Liu</a>
|
<a href=/people/m/maosong-sun/>Maosong Sun</a><br><a href=/volumes/2020.acl-main/ class=text-muted>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--acl-main--540><div class="card-body p-3 small">Adversarial attacks are carried out to reveal the vulnerability of <a href=https://en.wikipedia.org/wiki/Deep_learning>deep neural networks</a>. Textual adversarial attacking is challenging because text is discrete and a small perturbation can bring significant change to the original input. Word-level attacking, which can be regarded as a <a href=https://en.wikipedia.org/wiki/Combinatorial_optimization>combinatorial optimization problem</a>, is a well-studied class of textual attack methods. However, existing word-level attack models are far from perfect, largely because unsuitable search space reduction methods and inefficient <a href=https://en.wikipedia.org/wiki/Mathematical_optimization>optimization algorithms</a> are employed. In this paper, we propose a novel attack model, which incorporates the sememe-based word substitution method and particle swarm optimization-based search algorithm to solve the two problems separately. We conduct exhaustive experiments to evaluate our attack model by attacking BiLSTM and <a href=https://en.wikipedia.org/wiki/BERT>BERT</a> on three benchmark datasets. Experimental results demonstrate that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> consistently achieves much higher attack success rates and crafts more high-quality adversarial examples as compared to baseline methods. Also, further experiments show our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> has higher transferability and can bring more robustness enhancement to victim models by adversarial training. All the code and data of this paper can be obtained on https://github.com/thunlp/SememePSO-Attack.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.207.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--207 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.207 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.207/>BERT-MK : Integrating Graph Contextualized Knowledge into Pre-trained Language Models<span class=acl-fixed-case>BERT</span>-<span class=acl-fixed-case>MK</span>: Integrating Graph Contextualized Knowledge into Pre-trained Language Models</a></strong><br><a href=/people/b/bin-he/>Bin He</a>
|
<a href=/people/d/di-zhou/>Di Zhou</a>
|
<a href=/people/j/jinghui-xiao/>Jinghui Xiao</a>
|
<a href=/people/x/xin-jiang/>Xin Jiang</a>
|
<a href=/people/q/qun-liu/>Qun Liu</a>
|
<a href=/people/n/nicholas-jing-yuan/>Nicholas Jing Yuan</a>
|
<a href=/people/t/tong-xu/>Tong Xu</a><br><a href=/volumes/2020.findings-emnlp/ class=text-muted>Findings of the Association for Computational Linguistics: EMNLP 2020</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--207><div class="card-body p-3 small">Complex node interactions are common in knowledge graphs (KGs), and these interactions can be considered as contextualized knowledge exists in the topological structure of KGs. Traditional knowledge representation learning (KRL) methods usually treat a single triple as a training unit, neglecting the usage of graph contextualized knowledge. To utilize these unexploited graph-level knowledge, we propose an approach to model <a href=https://en.wikipedia.org/wiki/Glossary_of_graph_theory_terms>subgraphs</a> in a medical KG. Then, the learned <a href=https://en.wikipedia.org/wiki/Knowledge>knowledge</a> is integrated with a pre-trained language model to do the <a href=https://en.wikipedia.org/wiki/Knowledge>knowledge generalization</a>. Experimental results demonstrate that our model achieves the state-of-the-art performance on several medical NLP tasks, and the improvement above MedERNIE indicates that graph contextualized knowledge is beneficial.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.372.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--372 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.372 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.findings-emnlp.372.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.372" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.372/>TinyBERT : Distilling BERT for <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>Natural Language Understanding</a><span class=acl-fixed-case>T</span>iny<span class=acl-fixed-case>BERT</span>: Distilling <span class=acl-fixed-case>BERT</span> for Natural Language Understanding</a></strong><br><a href=/people/x/xiaoqi-jiao/>Xiaoqi Jiao</a>
|
<a href=/people/y/yichun-yin/>Yichun Yin</a>
|
<a href=/people/l/lifeng-shang/>Lifeng Shang</a>
|
<a href=/people/x/xin-jiang/>Xin Jiang</a>
|
<a href=/people/x/xiao-chen/>Xiao Chen</a>
|
<a href=/people/l/linlin-li/>Linlin Li</a>
|
<a href=/people/f/fang-wang/>Fang Wang</a>
|
<a href=/people/q/qun-liu/>Qun Liu</a><br><a href=/volumes/2020.findings-emnlp/ class=text-muted>Findings of the Association for Computational Linguistics: EMNLP 2020</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--372><div class="card-body p-3 small">Language model pre-training, such as <a href=https://en.wikipedia.org/wiki/BERT>BERT</a>, has significantly improved the performances of many natural language processing tasks. However, pre-trained language models are usually computationally expensive, so it is difficult to efficiently execute them on resource-restricted devices. To accelerate <a href=https://en.wikipedia.org/wiki/Inference>inference</a> and reduce model size while maintaining accuracy, we first propose a novel Transformer distillation method that is specially designed for knowledge distillation (KD) of the Transformer-based models. By leveraging this new KD method, the plenty of knowledge encoded in a large teacher BERT can be effectively transferred to a small student TinyBERT. Then, we introduce a new two-stage learning framework for TinyBERT, which performs Transformer distillation at both the pre-training and task-specific learning stages. This <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> ensures that TinyBERT can capture the general-domain as well as the task-specific knowledge in BERT. TinyBERT4 with 4 layers is empirically effective and achieves more than 96.8 % the performance of its teacher BERT-Base on GLUE benchmark, while being 7.5x smaller and 9.4x faster on inference. TinyBERT4 is also significantly better than 4-layer state-of-the-art baselines on BERT distillation, with only ~28 % parameters and ~31 % inference time of them. Moreover, TinyBERT6 with 6 layers performs on-par with its teacher BERT-Base.</div></div><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2307.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-2307 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-2307 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-2307/>Bilingual-GAN : A Step Towards Parallel Text Generation<span class=acl-fixed-case>GAN</span>: A Step Towards Parallel Text Generation</a></strong><br><a href=/people/a/ahmad-rashid/>Ahmad Rashid</a>
|
<a href=/people/a/alan-do-omri/>Alan Do-Omri</a>
|
<a href=/people/m/md-akmal-haidar/>Md. Akmal Haidar</a>
|
<a href=/people/q/qun-liu/>Qun Liu</a>
|
<a href=/people/m/mehdi-rezagholizadeh/>Mehdi Rezagholizadeh</a><br><a href=/volumes/W19-23/ class=text-muted>Proceedings of the Workshop on Methods for Optimizing and Evaluating Neural Language Generation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-2307><div class="card-body p-3 small">Latent space based GAN methods and attention based sequence to sequence models have achieved impressive results in text generation and unsupervised machine translation respectively. Leveraging the two domains, we propose an adversarial latent space based model capable of generating parallel sentences in two languages concurrently and translating bidirectionally. The bilingual generation goal is achieved by sampling from the latent space that is shared between both languages. First two denoising autoencoders are trained, with shared encoders and back-translation to enforce a shared latent state between the two languages. The <a href=https://en.wikipedia.org/wiki/Code>decoder</a> is shared for the two translation directions. Next, a GAN is trained to generate synthetic &#8216;code&#8217; mimicking the languages&#8217; shared latent space. This <a href=https://en.wikipedia.org/wiki/Code>code</a> is then fed into the <a href=https://en.wikipedia.org/wiki/Code_generation_(compiler)>decoder</a> to generate text in either language. We perform our experiments on Europarl and Multi30k datasets, on the English-French language pair, and document our performance using both supervised and unsupervised machine translation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5420.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5420 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5420 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5420/>Huawei’s NMT Systems for the WMT 2019 Biomedical Translation Task<span class=acl-fixed-case>NMT</span> Systems for the <span class=acl-fixed-case>WMT</span> 2019 Biomedical Translation Task</a></strong><br><a href=/people/w/wei-peng/>Wei Peng</a>
|
<a href=/people/j/jianfeng-liu/>Jianfeng Liu</a>
|
<a href=/people/l/liangyou-li/>Liangyou Li</a>
|
<a href=/people/q/qun-liu/>Qun Liu</a><br><a href=/volumes/W19-54/ class=text-muted>Proceedings of the Fourth Conference on Machine Translation (Volume 3: Shared Task Papers, Day 2)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5420><div class="card-body p-3 small">This paper describes Huawei&#8217;s neural machine translation systems for the WMT 2019 biomedical translation shared task. We trained and fine-tuned our systems on a combination of out-of-domain and in-domain parallel corpora for six translation directions covering EnglishChinese, EnglishFrench and EnglishGerman language pairs. Our submitted systems achieve the best BLEU scores on EnglishFrench and EnglishGerman language pairs according to the official evaluation results. In the EnglishChinese translation task, our <a href=https://en.wikipedia.org/wiki/System>systems</a> are in the second place. The enhanced performance is attributed to more in-domain training and more sophisticated <a href=https://en.wikipedia.org/wiki/Computer_simulation>models</a> developed. Development of translation models and transfer learning (or domain adaptation) methods has significantly contributed to the progress of the task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1139.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1139 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1139 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1139" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-1139/>ERNIE : Enhanced Language Representation with Informative Entities<span class=acl-fixed-case>ERNIE</span>: Enhanced Language Representation with Informative Entities</a></strong><br><a href=/people/z/zhengyan-zhang/>Zhengyan Zhang</a>
|
<a href=/people/x/xu-han/>Xu Han</a>
|
<a href=/people/z/zhiyuan-liu/>Zhiyuan Liu</a>
|
<a href=/people/x/xin-jiang/>Xin Jiang</a>
|
<a href=/people/m/maosong-sun/>Maosong Sun</a>
|
<a href=/people/q/qun-liu/>Qun Liu</a><br><a href=/volumes/P19-1/ class=text-muted>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1139><div class="card-body p-3 small">Neural language representation models such as BERT pre-trained on large-scale corpora can well capture rich semantic patterns from plain text, and be fine-tuned to consistently improve the performance of various NLP tasks. However, the existing pre-trained language models rarely consider incorporating knowledge graphs (KGs), which can provide rich structured knowledge facts for better <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>language understanding</a>. We argue that informative entities in KGs can enhance <a href=https://en.wikipedia.org/wiki/Representation_(mathematics)>language representation</a> with <a href=https://en.wikipedia.org/wiki/Knowledge>external knowledge</a>. In this paper, we utilize both large-scale textual corpora and KGs to train an enhanced language representation model (ERNIE), which can take full advantage of lexical, syntactic, and knowledge information simultaneously. The experimental results have demonstrated that ERNIE achieves significant improvements on various knowledge-driven tasks, and meanwhile is comparable with the state-of-the-art model BERT on other common NLP tasks. The code and datasets will be available in the future.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1332.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1332 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1332 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/384795570 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1332/>Decomposable Neural Paraphrase Generation</a></strong><br><a href=/people/z/zichao-li/>Zichao Li</a>
|
<a href=/people/x/xin-jiang/>Xin Jiang</a>
|
<a href=/people/l/lifeng-shang/>Lifeng Shang</a>
|
<a href=/people/q/qun-liu/>Qun Liu</a><br><a href=/volumes/P19-1/ class=text-muted>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1332><div class="card-body p-3 small">Paraphrasing exists at different granularity levels, such as <a href=https://en.wikipedia.org/wiki/Lexicon>lexical level</a>, <a href=https://en.wikipedia.org/wiki/Phrase_structure>phrasal level</a> and <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>sentential level</a>. This paper presents Decomposable Neural Paraphrase Generator (DNPG), a Transformer-based model that can learn and generate paraphrases of a sentence at different levels of granularity in a disentangled way. Specifically, the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is composed of multiple encoders and decoders with different structures, each of which corresponds to a specific granularity. The empirical study shows that the decomposition mechanism of DNPG makes <a href=https://en.wikipedia.org/wiki/Paraphrase_generation>paraphrase generation</a> more interpretable and controllable. Based on DNPG, we further develop an unsupervised domain adaptation method for <a href=https://en.wikipedia.org/wiki/Paraphrase_generation>paraphrase generation</a>. Experimental results show that the proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieves competitive in-domain performance compared to state-of-the-art neural models, and significantly better performance when adapting to a new domain.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1571.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1571 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1571 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/385428643 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1571" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1571/>Modeling Semantic Compositionality with Sememe Knowledge</a></strong><br><a href=/people/f/fanchao-qi/>Fanchao Qi</a>
|
<a href=/people/j/junjie-huang/>Junjie Huang</a>
|
<a href=/people/c/chenghao-yang/>Chenghao Yang</a>
|
<a href=/people/z/zhiyuan-liu/>Zhiyuan Liu</a>
|
<a href=/people/x/xiao-chen/>Xiao Chen</a>
|
<a href=/people/q/qun-liu/>Qun Liu</a>
|
<a href=/people/m/maosong-sun/>Maosong Sun</a><br><a href=/volumes/P19-1/ class=text-muted>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1571><div class="card-body p-3 small">Semantic compositionality (SC) refers to the phenomenon that the meaning of a complex linguistic unit can be composed of the meanings of its constituents. Most related works focus on using complicated compositionality functions to model SC while few works consider external knowledge in <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>. In this paper, we verify the effectiveness of <a href=https://en.wikipedia.org/wiki/Sememe>sememes</a>, the minimum semantic units of human languages, in modeling <a href=https://en.wikipedia.org/wiki/Semantic_space>SC</a> by a confirmatory experiment. Furthermore, we make the first attempt to incorporate sememe knowledge into SC models, and employ the sememe-incorporated models in learning representations of multiword expressions, a typical task of SC. In experiments, we implement our models by incorporating knowledge from a famous sememe knowledge base HowNet and perform both intrinsic and extrinsic evaluations. Experimental results show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> achieve significant performance boost as compared to the baseline methods without considering sememe knowledge. We further conduct quantitative analysis and case studies to demonstrate the effectiveness of applying <a href=https://en.wikipedia.org/wiki/Sememe>sememe knowledge</a> in modeling SC.All the code and data of this paper can be obtained on https://github.com/thunlp/Sememe-SC.</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1265.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1265 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1265 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1265/>Tailoring Neural Architectures for Translating from Morphologically Rich Languages</a></strong><br><a href=/people/p/peyman-passban/>Peyman Passban</a>
|
<a href=/people/a/andy-way/>Andy Way</a>
|
<a href=/people/q/qun-liu/>Qun Liu</a><br><a href=/volumes/C18-1/ class=text-muted>Proceedings of the 27th International Conference on Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1265><div class="card-body p-3 small">A morphologically complex word (MCW) is a hierarchical constituent with meaning-preserving subunits, so word-based models which rely on surface forms might not be powerful enough to translate such structures. When translating from morphologically rich languages (MRLs), a source word could be mapped to several words or even a full sentence on the target side, which means an MCW should not be treated as an atomic unit. In order to provide better translations for <a href=https://en.wikipedia.org/wiki/Machine_translation>MRLs</a>, we boost the existing neural machine translation (NMT) architecture with a double- channel encoder and a double-attentive decoder. The main goal targeted in this research is to provide richer information on the encoder side and redesign the decoder accordingly to benefit from such information. Our experimental results demonstrate that we could achieve our goal as the proposed model outperforms existing subword- and character-based architectures and showed significant improvements on translating from <a href=https://en.wikipedia.org/wiki/German_language>German</a>, <a href=https://en.wikipedia.org/wiki/Russian_language>Russian</a>, and <a href=https://en.wikipedia.org/wiki/Turkish_language>Turkish</a> into <a href=https://en.wikipedia.org/wiki/English_language>English</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1460.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1460 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1460 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/306134160 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1460/>Speeding Up Neural Machine Translation Decoding by Cube Pruning</a></strong><br><a href=/people/w/wen-zhang/>Wen Zhang</a>
|
<a href=/people/l/liang-huang/>Liang Huang</a>
|
<a href=/people/y/yang-feng/>Yang Feng</a>
|
<a href=/people/l/lei-shen/>Lei Shen</a>
|
<a href=/people/q/qun-liu/>Qun Liu</a><br><a href=/volumes/D18-1/ class=text-muted>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1460><div class="card-body p-3 small">Although <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a> has achieved promising results, <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> suffers from slow translation speed. The direct consequence is that a trade-off has to be made between translation quality and speed, thus its performance can not come into full play. We apply cube pruning, a popular technique to speed up <a href=https://en.wikipedia.org/wiki/Dynamic_programming>dynamic programming</a>, into <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a> to speed up the <a href=https://en.wikipedia.org/wiki/Translation>translation</a>. To construct the <a href=https://en.wikipedia.org/wiki/Equivalence_class>equivalence class</a>, similar target hidden states are combined, leading to less RNN expansion operations on the target side and less softmax operations over the large target vocabulary. The experiments show that, at the same or even better translation quality, our method can translate faster compared with naive beam search by 3.3x on <a href=https://en.wikipedia.org/wiki/Graphics_processing_unit>GPUs</a> and 3.5x on <a href=https://en.wikipedia.org/wiki/Central_processing_unit>CPUs</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3405.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3405 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3405 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3405/>Multimodal Neural Machine Translation for Low-resource Language Pairs using Synthetic Data</a></strong><br><a href=/people/k/koel-dutta-chowdhury/>Koel Dutta Chowdhury</a>
|
<a href=/people/m/mohammed-hasanuzzaman/>Mohammed Hasanuzzaman</a>
|
<a href=/people/q/qun-liu/>Qun Liu</a><br><a href=/volumes/W18-34/ class=text-muted>Proceedings of the Workshop on Deep Learning Approaches for Low-Resource NLP</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3405><div class="card-body p-3 small">In this paper, we investigate the effectiveness of training a multimodal neural machine translation (MNMT) system with image features for a low-resource language pair, <a href=https://en.wikipedia.org/wiki/Hindi>Hindi</a> and <a href=https://en.wikipedia.org/wiki/English_language>English</a>, using synthetic data. A three-way parallel corpus which contains <a href=https://en.wikipedia.org/wiki/Multilingualism>bilingual texts</a> and corresponding images is required to train a MNMT system with <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>image features</a>. However, such a <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> is not available for low resource language pairs. To address this, we developed both a synthetic training dataset and a manually curated development / test dataset for <a href=https://en.wikipedia.org/wiki/Hindi>Hindi</a> based on an existing English-image parallel corpus. We used these <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> to build our image description translation system by adopting state-of-the-art MNMT models. Our results show that it is possible to train a MNMT system for low-resource language pairs through the use of synthetic data and that such a <a href=https://en.wikipedia.org/wiki/System>system</a> can benefit from image features.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1006.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1006 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1006 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://vimeo.com/276415442 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N18-1006/>Improving Character-Based Decoding Using Target-Side Morphological Information for Neural Machine Translation</a></strong><br><a href=/people/p/peyman-passban/>Peyman Passban</a>
|
<a href=/people/q/qun-liu/>Qun Liu</a>
|
<a href=/people/a/andy-way/>Andy Way</a><br><a href=/volumes/N18-1/ class=text-muted>Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1006><div class="card-body p-3 small">Recently, neural machine translation (NMT) has emerged as a powerful alternative to conventional statistical approaches. However, its performance drops considerably in the presence of morphologically rich languages (MRLs). Neural engines usually fail to tackle the large vocabulary and high out-of-vocabulary (OOV) word rate of MRLs. Therefore, it is not suitable to exploit existing word-based models to translate this set of languages. In this paper, we propose an extension to the state-of-the-art model of Chung et al. (2016), which works at the character level and boosts the decoder with target-side morphological information. In our <a href=https://en.wikipedia.org/wiki/Architecture>architecture</a>, an additional morphology table is plugged into the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a>. Each time the <a href=https://en.wikipedia.org/wiki/Encoder>decoder</a> samples from a target vocabulary, the table sends auxiliary signals from the most relevant <a href=https://en.wikipedia.org/wiki/Affix>affixes</a> in order to enrich the <a href=https://en.wikipedia.org/wiki/Encoder>decoder</a>&#8217;s current state and constrain it to provide better predictions. We evaluated our model to translate <a href=https://en.wikipedia.org/wiki/English_language>English</a> into <a href=https://en.wikipedia.org/wiki/German_language>German</a>, <a href=https://en.wikipedia.org/wiki/Russian_language>Russian</a>, and <a href=https://en.wikipedia.org/wiki/Turkish_language>Turkish</a> as three MRLs and observed significant improvements.</div></div><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-3009.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-3009 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-3009 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-3009/>Semantics-Enhanced Task-Oriented Dialogue Translation : A Case Study on Hotel Booking</a></strong><br><a href=/people/l/longyue-wang/>Longyue Wang</a>
|
<a href=/people/j/jinhua-du/>Jinhua Du</a>
|
<a href=/people/l/liangyou-li/>Liangyou Li</a>
|
<a href=/people/z/zhaopeng-tu/>Zhaopeng Tu</a>
|
<a href=/people/a/andy-way/>Andy Way</a>
|
<a href=/people/q/qun-liu/>Qun Liu</a><br><a href=/volumes/I17-3/ class=text-muted>Proceedings of the IJCNLP 2017, System Demonstrations</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-3009><div class="card-body p-3 small">We showcase TODAY, a semantics-enhanced task-oriented dialogue translation system, whose novelties are : (i) task-oriented named entity (NE) definition and a hybrid strategy for NE recognition and <a href=https://en.wikipedia.org/wiki/Translation>translation</a> ; and (ii) a novel grounded semantic method for dialogue understanding and task-order management. TODAY is a case-study demo which can efficiently and accurately assist customers and agents in different languages to reach an agreement in a dialogue for the hotel booking.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-4010.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-4010 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-4010 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-4010/>ADAPT Centre Cone Team at IJCNLP-2017 Task 5 : A Similarity-Based Logistic Regression Approach to Multi-choice Question Answering in an Examinations Shared Task<span class=acl-fixed-case>ADAPT</span> Centre Cone Team at <span class=acl-fixed-case>IJCNLP</span>-2017 Task 5: A Similarity-Based Logistic Regression Approach to Multi-choice Question Answering in an Examinations Shared Task</a></strong><br><a href=/people/d/daria-dzendzik/>Daria Dzendzik</a>
|
<a href=/people/a/alberto-poncelas/>Alberto Poncelas</a>
|
<a href=/people/c/carl-vogel/>Carl Vogel</a>
|
<a href=/people/q/qun-liu/>Qun Liu</a><br><a href=/volumes/I17-4/ class=text-muted>Proceedings of the IJCNLP 2017, Shared Tasks</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-4010><div class="card-body p-3 small">We describe the work of a team from the ADAPT Centre in Ireland in addressing automatic answer selection for the Multi-choice Question Answering in Examinations shared task. The system is based on a <a href=https://en.wikipedia.org/wiki/Logistic_regression>logistic regression</a> over the string similarities between question, answer, and additional text. We obtain the highest grade out of six <a href=https://en.wikipedia.org/wiki/System>systems</a> : 48.7 % <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> on a validation set (vs. a baseline of 29.45 %) and 45.6 % on a test set.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1013.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1013 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1013 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234951392 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-1013/>Deep Neural Machine Translation with <a href=https://en.wikipedia.org/wiki/Linear_algebra>Linear Associative Unit</a></a></strong><br><a href=/people/m/mingxuan-wang/>Mingxuan Wang</a>
|
<a href=/people/z/zhengdong-lu/>Zhengdong Lu</a>
|
<a href=/people/j/jie-zhou/>Jie Zhou</a>
|
<a href=/people/q/qun-liu/>Qun Liu</a><br><a href=/volumes/P17-1/ class=text-muted>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1013><div class="card-body p-3 small">Deep Neural Networks (DNNs) have provably enhanced the state-of-the-art Neural Machine Translation (NMT) with its capability in modeling complex functions and capturing complex linguistic structures. However NMT with deep architecture in its encoder or decoder RNNs often suffer from severe gradient diffusion due to the non-linear recurrent activations, which often makes the optimization much more difficult. To address this problem we propose a novel linear associative units (LAU) to reduce the gradient propagation path inside the recurrent unit. Different from conventional approaches (LSTM unit and GRU), LAUs uses linear associative connections between input and output of the recurrent unit, which allows unimpeded information flow through both space and time The model is quite simple, but it is surprisingly effective. Our empirical study on Chinese-English translation shows that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> with proper configuration can improve by 11.7 BLEU upon Groundhog and the best reported on results in the same setting. On WMT14 English-German task and a larger WMT14 English-French task, our model achieves comparable results with the <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1140.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1140 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1140 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-1140/>Incorporating Word Reordering Knowledge into Attention-based Neural Machine Translation</a></strong><br><a href=/people/j/jinchao-zhang/>Jinchao Zhang</a>
|
<a href=/people/m/mingxuan-wang/>Mingxuan Wang</a>
|
<a href=/people/q/qun-liu/>Qun Liu</a>
|
<a href=/people/j/jie-zhou/>Jie Zhou</a><br><a href=/volumes/P17-1/ class=text-muted>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1140><div class="card-body p-3 small">This paper proposes three distortion models to explicitly incorporate the word reordering knowledge into attention-based Neural Machine Translation (NMT) for further improving translation performance. Our proposed models enable <a href=https://en.wikipedia.org/wiki/Attentional_control>attention mechanism</a> to attend to source words regarding both the semantic requirement and the word reordering penalty. Experiments on Chinese-English translation show that the approaches can improve word alignment quality and achieve significant translation improvements over a basic attention-based NMT by large margins. Compared with previous works on identical corpora, our <a href=https://en.wikipedia.org/wiki/System>system</a> achieves the state-of-the-art performance on translation quality.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1175.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1175 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1175 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-1175/>Doubly-Attentive Decoder for Multi-modal Neural Machine Translation</a></strong><br><a href=/people/i/iacer-calixto/>Iacer Calixto</a>
|
<a href=/people/q/qun-liu/>Qun Liu</a>
|
<a href=/people/n/nick-campbell/>Nick Campbell</a><br><a href=/volumes/P17-1/ class=text-muted>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1175><div class="card-body p-3 small">We introduce a Multi-modal Neural Machine Translation model in which a doubly-attentive decoder naturally incorporates spatial visual features obtained using pre-trained convolutional neural networks, bridging the gap between image description and translation. Our decoder learns to attend to source-language words and parts of an image independently by means of two separate attention mechanisms as it generates words in the target language. We find that our model can efficiently exploit not just back-translated in-domain multi-modal data but also large general-domain text-only MT corpora. We also report state-of-the-art results on the Multi30k data set.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1105.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1105 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1105 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1105/>Incorporating Global Visual Features into Attention-based Neural Machine Translation.</a></strong><br><a href=/people/i/iacer-calixto/>Iacer Calixto</a>
|
<a href=/people/q/qun-liu/>Qun Liu</a><br><a href=/volumes/D17-1/ class=text-muted>Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1105><div class="card-body p-3 small">We introduce multi-modal, attention-based neural machine translation (NMT) models which incorporate visual features into different parts of both the <a href=https://en.wikipedia.org/wiki/Encoder>encoder</a> and the <a href=https://en.wikipedia.org/wiki/Codec>decoder</a>. Global image features are extracted using a pre-trained convolutional neural network and are incorporated (i) as words in the source sentence, (ii) to initialise the encoder hidden state, and (iii) as additional data to initialise the decoder hidden state. In our experiments, we evaluate translations into <a href=https://en.wikipedia.org/wiki/English_language>English</a> and <a href=https://en.wikipedia.org/wiki/German_language>German</a>, how different strategies to incorporate global image features compare and which ones perform best. We also study the impact that adding synthetic multi-modal, multilingual data brings and find that the additional <a href=https://en.wikipedia.org/wiki/Data>data</a> have a positive impact on multi-modal NMT models. We report new state-of-the-art results and our best models also significantly improve on a comparable phrase-based Statistical MT (PBSMT) model trained on the Multi30k data set according to all metrics evaluated. To the best of our knowledge, it is the first time a purely neural model significantly improves over a PBSMT model on all metrics evaluated on this <a href=https://en.wikipedia.org/wiki/Data_set>data set</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1262.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1262 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1262 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D17-1262" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D17-1262/>Further Investigation into Reference Bias in Monolingual Evaluation of <a href=https://en.wikipedia.org/wiki/Machine_translation>Machine Translation</a></a></strong><br><a href=/people/q/qingsong-ma/>Qingsong Ma</a>
|
<a href=/people/y/yvette-graham/>Yvette Graham</a>
|
<a href=/people/t/timothy-baldwin/>Timothy Baldwin</a>
|
<a href=/people/q/qun-liu/>Qun Liu</a><br><a href=/volumes/D17-1/ class=text-muted>Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1262><div class="card-body p-3 small">Monolingual evaluation of Machine Translation (MT) aims to simplify human assessment by requiring assessors to compare the meaning of the MT output with a reference translation, opening up the task to a much larger pool of genuinely qualified evaluators. Monolingual evaluation runs the risk, however, of bias in favour of MT systems that happen to produce translations superficially similar to the reference and, consistent with this intuition, previous investigations have concluded monolingual assessment to be strongly biased in this respect. On re-examination of past analyses, we identify a series of potential analytical errors that force some important questions to be raised about the reliability of past conclusions, however. We subsequently carry out further investigation into reference bias via direct human assessment of MT adequacy via quality controlled crowd-sourcing. Contrary to both intuition and past conclusions, results for show no significant evidence of reference bias in monolingual evaluation of MT.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1012.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1012 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1012 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1012/>If You Ca n’t Beat Them Join Them : Handcrafted Features Complement Neural Nets for Non-Factoid Answer Reranking</a></strong><br><a href=/people/d/dasha-bogdanova/>Dasha Bogdanova</a>
|
<a href=/people/j/jennifer-foster/>Jennifer Foster</a>
|
<a href=/people/d/daria-dzendzik/>Daria Dzendzik</a>
|
<a href=/people/q/qun-liu/>Qun Liu</a><br><a href=/volumes/E17-1/ class=text-muted>Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1012><div class="card-body p-3 small">We show that a neural approach to the task of non-factoid answer reranking can benefit from the inclusion of tried-and-tested handcrafted features. We present a neural network architecture based on a combination of <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent neural networks</a> that are used to encode questions and answers, and a <a href=https://en.wikipedia.org/wiki/Multilayer_perceptron>multilayer perceptron</a>. We show how this approach can be combined with additional <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a>, in particular, the discourse features used by previous research. Our neural approach achieves state-of-the-art performance on a public dataset from Yahoo ! Answers and its performance is further improved by incorporating the discourse features. Additionally, we present a new dataset of Ask Ubuntu questions where the hybrid approach also achieves good results.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2057.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2057 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2057 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2057/>Improving Evaluation of Document-level Machine Translation Quality Estimation</a></strong><br><a href=/people/y/yvette-graham/>Yvette Graham</a>
|
<a href=/people/q/qingsong-ma/>Qingsong Ma</a>
|
<a href=/people/t/timothy-baldwin/>Timothy Baldwin</a>
|
<a href=/people/q/qun-liu/>Qun Liu</a>
|
<a href=/people/c/carla-parra-escartin/>Carla Parra</a>
|
<a href=/people/c/carolina-scarton/>Carolina Scarton</a><br><a href=/volumes/E17-2/ class=text-muted>Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2057><div class="card-body p-3 small">Meaningful conclusions about the relative performance of NLP systems are only possible if the <a href=https://en.wikipedia.org/wiki/Gold_standard_(test)>gold standard</a> employed in a given <a href=https://en.wikipedia.org/wiki/Evaluation>evaluation</a> is both valid and reliable. In this paper, we explore the validity of human annotations currently employed in the evaluation of document-level quality estimation for <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation (MT)</a>. We demonstrate the degree to which MT system rankings are dependent on weights employed in the construction of the <a href=https://en.wikipedia.org/wiki/Gold_standard>gold standard</a>, before proposing direct human assessment as a valid alternative. Experiments show direct assessment (DA) scores for documents to be highly reliable, achieving a correlation of above 0.9 in a self-replication experiment, in addition to a substantial estimated cost reduction through quality controlled crowd-sourcing. The original <a href=https://en.wikipedia.org/wiki/Gold_standard>gold standard</a> based on <a href=https://en.wikipedia.org/wiki/Post-editing>post-edits</a> incurs a 1020 times greater cost than DA.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Qun+Liu" title="Search for 'Qun Liu' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/x/xin-jiang/ class=align-middle>Xin Jiang</a>
<span class="badge badge-secondary align-middle ml-2">8</span></li><li class=list-group-item><a href=/people/l/lifeng-shang/ class=align-middle>Lifeng Shang</a>
<span class="badge badge-secondary align-middle ml-2">6</span></li><li class=list-group-item><a href=/people/l/liangyou-li/ class=align-middle>Liangyou Li</a>
<span class="badge badge-secondary align-middle ml-2">5</span></li><li class=list-group-item><a href=/people/m/meng-zhang/ class=align-middle>Meng Zhang</a>
<span class="badge badge-secondary align-middle ml-2">5</span></li><li class=list-group-item><a href=/people/x/xiao-chen/ class=align-middle>Xiao Chen</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/z/zhiyuan-liu/ class=align-middle>Zhiyuan Liu</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/people/m/maosong-sun/ class=align-middle>Maosong Sun (孙茂松)</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/people/a/andy-way/ class=align-middle>Andy Way</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/y/yichun-yin/ class=align-middle>Yichun Yin</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/y/yvette-graham/ class=align-middle>Yvette Graham</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/w/wei-peng/ class=align-middle>Wei Peng</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/l/liang-huang/ class=align-middle>Liang Huang</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/p/peyman-passban/ class=align-middle>Peyman Passban</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/d/daria-dzendzik/ class=align-middle>Daria Dzendzik</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/f/fanchao-qi/ class=align-middle>Fanchao Qi</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/c/chenghao-yang/ class=align-middle>Chenghao Yang</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/m/mingxuan-wang/ class=align-middle>Mingxuan Wang</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/j/jie-zhou/ class=align-middle>Jie Zhou</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/i/iacer-calixto/ class=align-middle>Iacer Calixto</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/m/minghao-wu/ class=align-middle>Minghao Wu</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/x/xiaoguang-li/ class=align-middle>Xiaoguang Li</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/w/weixuan-wang/ class=align-middle>Weixuan Wang</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/q/qingsong-ma/ class=align-middle>Qingsong Ma</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/t/timothy-baldwin/ class=align-middle>Timothy Baldwin</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/h/hua-wu/ class=align-middle>Hua Wu (吴华)</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/colin-cherry/ class=align-middle>Colin Cherry</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/zhongjun-he/ class=align-middle>Zhongjun He</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/maha-elbayad/ class=align-middle>Maha Elbayad</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/mark-liberman/ class=align-middle>Mark Liberman</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/haifeng-wang/ class=align-middle>Haifeng Wang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/mingbo-ma/ class=align-middle>Mingbo Ma</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/ruiqing-zhang/ class=align-middle>Ruiqing Zhang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/deyi-xiong/ class=align-middle>Deyi Xiong</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/shili-ge/ class=align-middle>Shili Ge</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/x/xiaojun-zhang/ class=align-middle>Xiaojun Zhang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/longyue-wang/ class=align-middle>Longyue Wang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jinhua-du/ class=align-middle>Jinhua Du</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/zhaopeng-tu/ class=align-middle>Zhaopeng Tu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/alberto-poncelas/ class=align-middle>Alberto Poncelas</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/carl-vogel/ class=align-middle>Carl Vogel</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/w/wei-zhang/ class=align-middle>Wei Zhang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/lu-hou/ class=align-middle>Lu Hou</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/david-schlangen/ class=align-middle>David Schlangen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yuan-zang/ class=align-middle>Yuan Zang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/zhengdong-lu/ class=align-middle>Zhengdong Lu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jinchao-zhang/ class=align-middle>Jinchao Zhang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/n/nick-campbell/ class=align-middle>Nick Campbell</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/cheng-chen/ class=align-middle>Cheng Chen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yujia-qin/ class=align-middle>Yujia Qin</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/f/fengyu-wang/ class=align-middle>Fengyu Wang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/zhi-wang/ class=align-middle>Zhi Wang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/p/pengfei-li/ class=align-middle>Pengfei Li</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/tianbo-ji/ class=align-middle>Tianbo Ji</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/g/gareth-jones/ class=align-middle>Gareth Jones</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/chenyang-lyu/ class=align-middle>Chenyang Lyu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jiawei-zhou/ class=align-middle>Jiawei Zhou</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/lan-luo/ class=align-middle>Lan Luo</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/ke-zhan/ class=align-middle>Ke Zhan</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/e/enrui-hu/ class=align-middle>Enrui Hu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/x/xinyu-zhang/ class=align-middle>Xinyu Zhang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/hao-jiang/ class=align-middle>Hao Jiang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/zhao-cao/ class=align-middle>Zhao Cao</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/f/fan-yu/ class=align-middle>Fan Yu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/lei-chen/ class=align-middle>Lei Chen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/w/wen-zhang/ class=align-middle>Wen Zhang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yang-feng/ class=align-middle>Yang Feng</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/lei-shen/ class=align-middle>Lei Shen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yuanhang-zheng/ class=align-middle>Yuanhang Zheng</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/zhixing-tan/ class=align-middle>Zhixing Tan</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/mieradilijiang-maimaiti/ class=align-middle>Mieradilijiang Maimaiti</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/huanbo-luan/ class=align-middle>Huanbo Luan</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yang-liu-ict/ class=align-middle>Yang Liu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yitong-li/ class=align-middle>Yitong Li</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/g/gholamreza-haffari/ class=align-middle>Gholamreza Haffari</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/shaobo-li/ class=align-middle>Shaobo Li</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/zhenhua-dong/ class=align-middle>Zhenhua Dong</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/cheng-jie-sun/ class=align-middle>Cheng-Jie Sun</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/b/bingquan-liu/ class=align-middle>Bingquan Liu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/zhenzhou-ji/ class=align-middle>Zhenzhou Ji</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/x/xupeng-meng/ class=align-middle>Xupeng Meng</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/b/bin-he/ class=align-middle>Bin He</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/di-zhou/ class=align-middle>Di Zhou</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jinghui-xiao/ class=align-middle>Jinghui Xiao</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/n/nicholas-jing-yuan/ class=align-middle>Nicholas Jing Yuan</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/tong-xu/ class=align-middle>Tong Xu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/x/xiaoqi-jiao/ class=align-middle>Xiaoqi Jiao</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/linlin-li/ class=align-middle>Linlin Li</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/f/fang-wang/ class=align-middle>Fang Wang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/x/xingshan-zeng/ class=align-middle>Xingshan Zeng</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/koel-dutta-chowdhury/ class=align-middle>Koel Dutta Chowdhury</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/mohammed-hasanuzzaman/ class=align-middle>Mohammed Hasanuzzaman</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/ahmad-rashid/ class=align-middle>Ahmad Rashid</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/alan-do-omri/ class=align-middle>Alan Do-Omri</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/md-akmal-haidar/ class=align-middle>Md. Akmal Haidar</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/mehdi-rezagholizadeh/ class=align-middle>Mehdi Rezagholizadeh</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jianfeng-liu/ class=align-middle>Jianfeng Liu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/dasha-bogdanova/ class=align-middle>Dasha Bogdanova</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jennifer-foster/ class=align-middle>Jennifer Foster</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/carla-parra-escartin/ class=align-middle>Carla Parra Escartín</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/carolina-scarton/ class=align-middle>Carolina Scarton</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/zhengyan-zhang/ class=align-middle>Zhengyan Zhang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/x/xu-han/ class=align-middle>Xu Han</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/zichao-li/ class=align-middle>Zichao Li</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/junjie-huang/ class=align-middle>Junjie Huang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/acl/ class=align-middle>ACL</a><span class="badge badge-secondary align-middle ml-2">11</span></li><li class=list-group-item><a href=/venues/emnlp/ class=align-middle>EMNLP</a><span class="badge badge-secondary align-middle ml-2">8</span></li><li class=list-group-item><a href=/venues/findings/ class=align-middle>Findings</a><span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/venues/ws/ class=align-middle>WS</a><span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/venues/ijcnlp/ class=align-middle>IJCNLP</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-venues aria-expanded=false aria-controls=more-venues>show all...</li><div class="collapse border-top" id=more-venues><li class=list-group-item><a href=/venues/eacl/ class=align-middle>EACL</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/autosimtrans/ class=align-middle>AutoSimTrans</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/coling/ class=align-middle>COLING</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/iwdp/ class=align-middle>iwdp</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/wmt/ class=align-middle>WMT</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/iwslt/ class=align-middle>IWSLT</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/naacl/ class=align-middle>NAACL</a><span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>