<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Qin Lu - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Qin</span> <span class=font-weight-bold>Lu</span></h2><hr><div class=row><div class=col-lg-9><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.14.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--14 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.14 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.14/>Affection Driven Neural Networks for Sentiment Analysis</a></strong><br><a href=/people/r/rong-xiang/>Rong Xiang</a>
|
<a href=/people/y/yunfei-long/>Yunfei Long</a>
|
<a href=/people/m/mingyu-wan/>Mingyu Wan</a>
|
<a href=/people/j/jinghang-gu/>Jinghang Gu</a>
|
<a href=/people/q/qin-lu/>Qin Lu</a>
|
<a href=/people/c/chu-ren-huang/>Chu-Ren Huang</a><br><a href=/volumes/2020.lrec-1/ class=text-muted>Proceedings of the 12th Language Resources and Evaluation Conference</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--14><div class="card-body p-3 small">Deep neural network models have played a critical role in <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> with promising results in the recent decade. One of the essential challenges, however, is how external sentiment knowledge can be effectively utilized. In this work, we propose a novel affection-driven approach to incorporating affective knowledge into <a href=https://en.wikipedia.org/wiki/Neural_circuit>neural network models</a>. The affective knowledge is obtained in the form of a lexicon under the Affect Control Theory (ACT), which is represented by vectors of three-dimensional attributes in Evaluation, Potency, and Activity (EPA). The EPA vectors are mapped to an affective influence value and then integrated into Long Short-term Memory (LSTM) models to highlight affective terms. Experimental results show a consistent improvement of our approach over conventional LSTM models by 1.0 % to 1.5 % in <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> on three large benchmark datasets. Evaluations across a variety of <a href=https://en.wikipedia.org/wiki/Algorithm>algorithms</a> have also proven the effectiveness of leveraging affective terms for deep model enhancement.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.701.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--701 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.701 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.701/>Ciron : a New Benchmark Dataset for Chinese Irony Detection<span class=acl-fixed-case>C</span>iron: a New Benchmark Dataset for <span class=acl-fixed-case>C</span>hinese Irony Detection</a></strong><br><a href=/people/r/rong-xiang/>Rong Xiang</a>
|
<a href=/people/x/xuefeng-gao/>Xuefeng Gao</a>
|
<a href=/people/y/yunfei-long/>Yunfei Long</a>
|
<a href=/people/a/anran-li/>Anran Li</a>
|
<a href=/people/e/emmanuele-chersoni/>Emmanuele Chersoni</a>
|
<a href=/people/q/qin-lu/>Qin Lu</a>
|
<a href=/people/c/chu-ren-huang/>Chu-Ren Huang</a><br><a href=/volumes/2020.lrec-1/ class=text-muted>Proceedings of the 12th Language Resources and Evaluation Conference</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--701><div class="card-body p-3 small">Automatic Chinese irony detection is a challenging task, and it has a strong impact on <a href=https://en.wikipedia.org/wiki/Linguistics>linguistic research</a>. However, Chinese irony detection often lacks labeled benchmark datasets. In this paper, we introduce <a href=https://en.wikipedia.org/wiki/Iron>Ciron</a>, the first Chinese benchmark dataset available for irony detection for <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning models</a>. Ciron includes more than 8.7 K posts, collected from <a href=https://en.wikipedia.org/wiki/Sina_Weibo>Weibo</a>, a <a href=https://en.wikipedia.org/wiki/Microblogging_in_China>micro blogging platform</a>. Most importantly, <a href=https://en.wikipedia.org/wiki/Ciron>Ciron</a> is collected with no pre-conditions to ensure a much wider coverage. Evaluation on seven different <a href=https://en.wikipedia.org/wiki/Statistical_classification>machine learning classifiers</a> proves the usefulness of <a href=https://en.wikipedia.org/wiki/Ciron>Ciron</a> as an important resource for Chinese irony detection.</div></div><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5541.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5541 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5541 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5541/>Improving Multi-label Emotion Classification by Integrating both General and Domain-specific Knowledge</a></strong><br><a href=/people/w/wenhao-ying/>Wenhao Ying</a>
|
<a href=/people/r/rong-xiang/>Rong Xiang</a>
|
<a href=/people/q/qin-lu/>Qin Lu</a><br><a href=/volumes/D19-55/ class=text-muted>Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT 2019)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5541><div class="card-body p-3 small">Deep learning based general language models have achieved state-of-the-art results in many popular <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> such as <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> and QA tasks. Text in domains like <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> has its own salient characteristics. Domain knowledge should be helpful in domain relevant tasks. In this work, we devise a simple method to obtain <a href=https://en.wikipedia.org/wiki/Domain_knowledge>domain knowledge</a> and further propose a method to integrate <a href=https://en.wikipedia.org/wiki/Domain_knowledge>domain knowledge</a> with <a href=https://en.wikipedia.org/wiki/General_knowledge>general knowledge</a> based on deep language models to improve performance of <a href=https://en.wikipedia.org/wiki/Emotion_classification>emotion classification</a>. Experiments on Twitter data show that even though a deep language model fine-tuned by a target domain data has attained comparable results to that of previous state-of-the-art models, this fine-tuned model can still benefit from our extracted <a href=https://en.wikipedia.org/wiki/Domain_knowledge>domain knowledge</a> to obtain more improvement. This highlights the importance of making use of <a href=https://en.wikipedia.org/wiki/Domain_knowledge>domain knowledge</a> in <a href=https://en.wikipedia.org/wiki/Domain-specific_language>domain-specific applications</a>.</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6214.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6214 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6214 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6214/>Leveraging Writing Systems Change for Deep Learning Based Chinese Emotion Analysis<span class=acl-fixed-case>C</span>hinese Emotion Analysis</a></strong><br><a href=/people/r/rong-xiang/>Rong Xiang</a>
|
<a href=/people/y/yunfei-long/>Yunfei Long</a>
|
<a href=/people/q/qin-lu/>Qin Lu</a>
|
<a href=/people/d/dan-xiong/>Dan Xiong</a>
|
<a href=/people/i/i-hsuan-chen/>I-Hsuan Chen</a><br><a href=/volumes/W18-62/ class=text-muted>Proceedings of the 9th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6214><div class="card-body p-3 small">Social media text written in <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese communities</a> contains mixed scripts including major text written in <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a>, an ideograph-based writing system, and some minor text using <a href=https://en.wikipedia.org/wiki/Latin_script>Latin letters</a>, an alphabet-based writing system. This phenomenon is called writing systems changes (WSCs). Past studies have shown that WSCs can be used to express emotions, particularly where the social and political environment is more conservative. However, because WSCs can break the <a href=https://en.wikipedia.org/wiki/Syntax>syntax</a> of the major text, it poses more challenges in Natural Language Processing (NLP) tasks like <a href=https://en.wikipedia.org/wiki/Emotion_classification>emotion classification</a>. In this work, we present a novel deep learning based method to include WSCs as an effective <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>feature</a> for emotion analysis. The method first identifies all WSCs points. Then representation of the major text is learned through an LSTM model whereas the minor text is learned by a separate CNN model. Emotions in the minor text are further highlighted through an <a href=https://en.wikipedia.org/wiki/Attentional_control>attention mechanism</a> before <a href=https://en.wikipedia.org/wiki/Emotion_classification>emotion classification</a>. Performance evaluation shows that incorporating WSCs features using deep learning models can improve performance measured by F1-scores compared to the state-of-the-art model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6220.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6220 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6220 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6220/>Dual Memory Network Model for Biased Product Review Classification</a></strong><br><a href=/people/y/yunfei-long/>Yunfei Long</a>
|
<a href=/people/m/mingyu-ma/>Mingyu Ma</a>
|
<a href=/people/q/qin-lu/>Qin Lu</a>
|
<a href=/people/r/rong-xiang/>Rong Xiang</a>
|
<a href=/people/c/chu-ren-huang/>Chu-Ren Huang</a><br><a href=/volumes/W18-62/ class=text-muted>Proceedings of the 9th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6220><div class="card-body p-3 small">In sentiment analysis (SA) of product reviews, both user and product information are proven to be useful. Current tasks handle <a href=https://en.wikipedia.org/wiki/User_profile>user profile</a> and product information in a unified model which may not be able to learn salient features of users and products effectively. In this work, we propose a dual user and product memory network (DUPMN) model to learn user profiles and product reviews using separate memory networks. Then, the two <a href=https://en.wikipedia.org/wiki/Representation_(mathematics)>representations</a> are used jointly for <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment prediction</a>. The use of separate <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> aims to capture <a href=https://en.wikipedia.org/wiki/User_profile>user profiles</a> and <a href=https://en.wikipedia.org/wiki/Product_information>product information</a> more effectively. Compared to state-of-the-art unified prediction models, the evaluations on three benchmark datasets, <a href=https://en.wikipedia.org/wiki/Internet_Movie_Database>IMDB</a>, Yelp13, and Yelp14, show that our dual learning model gives performance gain of 0.6 %, 1.2 %, and 0.9 %, respectively. The improvements are also deemed very significant measured by <a href=https://en.wikipedia.org/wiki/P-value>p-values</a>.<i>p-values</i>.</div></div><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-2043.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-2043 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-2043 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-2043/>Fake News Detection Through Multi-Perspective Speaker Profiles</a></strong><br><a href=/people/y/yunfei-long/>Yunfei Long</a>
|
<a href=/people/q/qin-lu/>Qin Lu</a>
|
<a href=/people/r/rong-xiang/>Rong Xiang</a>
|
<a href=/people/m/minglei-li/>Minglei Li</a>
|
<a href=/people/c/chu-ren-huang/>Chu-Ren Huang</a><br><a href=/volumes/I17-2/ class=text-muted>Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-2043><div class="card-body p-3 small">Automatic fake news detection is an important, yet very challenging topic. Traditional <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> using lexical features have only very limited success. This paper proposes a novel method to incorporate <a href=https://en.wikipedia.org/wiki/Public_speaking>speaker profiles</a> into an attention based LSTM model for <a href=https://en.wikipedia.org/wiki/Fake_news>fake news detection</a>. Speaker profiles contribute to the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> in two ways. One is to include <a href=https://en.wikipedia.org/wiki/Them>them</a> in the attention model. The other includes <a href=https://en.wikipedia.org/wiki/List_of_Latin_phrases_(M)>them</a> as additional input data. By adding speaker profiles such as <a href=https://en.wikipedia.org/wiki/Political_party>party affiliation</a>, speaker title, location and <a href=https://en.wikipedia.org/wiki/Credit_history>credit history</a>, our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms the <a href=https://en.wikipedia.org/wiki/State-of-the-art>state-of-the-art method</a> by 14.5 % in <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> using a benchmark fake news detection dataset. This proves that speaker profiles provide valuable information to validate the credibility of news articles.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Qin+Lu" title="Search for 'Qin Lu' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/r/rong-xiang/ class=align-middle>Rong Xiang</a>
<span class="badge badge-secondary align-middle ml-2">6</span></li><li class=list-group-item><a href=/people/y/yunfei-long/ class=align-middle>Yunfei Long</a>
<span class="badge badge-secondary align-middle ml-2">5</span></li><li class=list-group-item><a href=/people/c/chu-ren-huang/ class=align-middle>Chu-Ren Huang</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/people/m/minglei-li/ class=align-middle>Minglei Li</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/w/wenhao-ying/ class=align-middle>Wenhao Ying</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/d/dan-xiong/ class=align-middle>Dan Xiong</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/i/i-hsuan-chen/ class=align-middle>I-Hsuan Chen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/mingyu-ma/ class=align-middle>Mingyu Ma</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/mingyu-wan/ class=align-middle>Mingyu Wan</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jinghang-gu/ class=align-middle>Jinghang Gu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/x/xuefeng-gao/ class=align-middle>Xuefeng Gao</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/anran-li/ class=align-middle>Anran Li</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/e/emmanuele-chersoni/ class=align-middle>Emmanuele Chersoni</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/ws/ class=align-middle>WS</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/lrec/ class=align-middle>LREC</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/ijcnlp/ class=align-middle>IJCNLP</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/emnlp/ class=align-middle>EMNLP</a><span class="badge badge-secondary align-middle ml-2">1</span></li></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright Â©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>