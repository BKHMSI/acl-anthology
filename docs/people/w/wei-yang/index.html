<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Wei Yang - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Wei</span> <span class=font-weight-bold>Yang</span></h2><hr><div class=row><div class=col-lg-9><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1352.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1352 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1352 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1352/>Cross-Domain Modeling of Sentence-Level Evidence for Document Retrieval</a></strong><br><a href=/people/z/zeynep-akkalyoncu-yilmaz/>Zeynep Akkalyoncu Yilmaz</a>
|
<a href=/people/w/wei-yang/>Wei Yang</a>
|
<a href=/people/h/haotian-zhang/>Haotian Zhang</a>
|
<a href=/people/j/jimmy-lin/>Jimmy Lin</a><br><a href=/volumes/D19-1/ class=text-muted>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1352><div class="card-body p-3 small">This paper applies BERT to ad hoc document retrieval on news articles, which requires addressing two challenges : relevance judgments in existing test collections are typically provided only at the document level, and documents often exceed the length that BERT was designed to handle. Our solution is to aggregate <a href=https://en.wikipedia.org/wiki/Sentence_(law)>sentence-level evidence</a> to rank documents. Furthermore, we are able to leverage passage-level relevance judgments fortuitously available in other domains to fine-tune BERT models that are able to capture cross-domain notions of relevance, and can be directly used for ranking news articles. Our simple neural ranking models achieve state-of-the-art effectiveness on three standard test collections.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1540.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1540 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1540 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1540/>Bridging the Gap between Relevance Matching and <a href=https://en.wikipedia.org/wiki/Semantic_matching>Semantic Matching</a> for Short Text Similarity Modeling</a></strong><br><a href=/people/j/jinfeng-rao/>Jinfeng Rao</a>
|
<a href=/people/l/linqing-liu/>Linqing Liu</a>
|
<a href=/people/y/yi-tay/>Yi Tay</a>
|
<a href=/people/w/wei-yang/>Wei Yang</a>
|
<a href=/people/p/peng-shi/>Peng Shi</a>
|
<a href=/people/j/jimmy-lin/>Jimmy Lin</a><br><a href=/volumes/D19-1/ class=text-muted>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1540><div class="card-body p-3 small">A core problem of information retrieval (IR) is relevance matching, which is to rank documents by relevance to a user&#8217;s query. On the other hand, many NLP problems, such as <a href=https://en.wikipedia.org/wiki/Question_answering>question answering</a> and paraphrase identification, can be considered variants of <a href=https://en.wikipedia.org/wiki/Semantic_matching>semantic matching</a>, which is to measure the <a href=https://en.wikipedia.org/wiki/Semantic_distance>semantic distance</a> between two pieces of short texts. While at a high level both <a href=https://en.wikipedia.org/wiki/Relevance_(information_retrieval)>relevance</a> and <a href=https://en.wikipedia.org/wiki/Semantic_matching>semantic matching</a> require modeling textual similarity, many existing techniques for one can not be easily adapted to the other. To bridge this gap, we propose a novel model, HCAN (Hybrid Co-Attention Network), that comprises (1) a hybrid encoder module that includes ConvNet-based and LSTM-based encoders, (2) a relevance matching module that measures soft term matches with importance weighting at multiple granularities, and (3) a semantic matching module with co-attention mechanisms that capture context-aware semantic relatedness. Evaluations on multiple IR and NLP benchmarks demonstrate state-of-the-art effectiveness compared to approaches that do not exploit pretraining on external data. Extensive ablation studies suggest that relevance and semantic matching signals are complementary across many problem settings, regardless of the choice of underlying <a href=https://en.wikipedia.org/wiki/Encoder>encoders</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4108.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4108 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4108 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4108/>End-to-End Neural Context Reconstruction in Chinese Dialogue<span class=acl-fixed-case>C</span>hinese Dialogue</a></strong><br><a href=/people/w/wei-yang/>Wei Yang</a>
|
<a href=/people/r/rui-qiao/>Rui Qiao</a>
|
<a href=/people/h/haocheng-qin/>Haocheng Qin</a>
|
<a href=/people/a/amy-sun/>Amy Sun</a>
|
<a href=/people/l/luchen-tan/>Luchen Tan</a>
|
<a href=/people/k/kun-xiong/>Kun Xiong</a>
|
<a href=/people/m/ming-li/>Ming Li</a><br><a href=/volumes/W19-41/ class=text-muted>Proceedings of the First Workshop on NLP for Conversational AI</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4108><div class="card-body p-3 small">We tackle the problem of context reconstruction in Chinese dialogue, where the task is to replace <a href=https://en.wikipedia.org/wiki/Pronoun>pronouns</a>, zero pronouns, and other referring expressions with their referent nouns so that sentences can be processed in isolation without context. Following a standard decomposition of the context reconstruction task into referring expression detection and <a href=https://en.wikipedia.org/wiki/Coreference_resolution>coreference resolution</a>, we propose a novel end-to-end architecture for separately and jointly accomplishing this <a href=https://en.wikipedia.org/wiki/Task_(computing)>task</a>. Key features of this model include POS and position encoding using CNNs and a novel pronoun masking mechanism. One perennial problem in building such models is the paucity of training data, which we address by augmenting previously-proposed methods to generate a large amount of realistic training data. The combination of more data and better <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> yields <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> higher than the state-of-the-art method in <a href=https://en.wikipedia.org/wiki/Coreference_resolution>coreference resolution</a> and end-to-end context reconstruction.</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1189.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1189 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1189 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-1189/>SemRegex : A Semantics-Based Approach for Generating <a href=https://en.wikipedia.org/wiki/Regular_expression>Regular Expressions</a> from Natural Language Specifications<span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>R</span>egex: A Semantics-Based Approach for Generating Regular Expressions from Natural Language Specifications</a></strong><br><a href=/people/z/zexuan-zhong/>Zexuan Zhong</a>
|
<a href=/people/j/jiaqi-guo/>Jiaqi Guo</a>
|
<a href=/people/w/wei-yang/>Wei Yang</a>
|
<a href=/people/j/jian-peng/>Jian Peng</a>
|
<a href=/people/t/tao-xie/>Tao Xie</a>
|
<a href=/people/j/jian-guang-lou/>Jian-Guang Lou</a>
|
<a href=/people/t/ting-liu/>Ting Liu</a>
|
<a href=/people/d/dongmei-zhang/>Dongmei Zhang</a><br><a href=/volumes/D18-1/ class=text-muted>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1189><div class="card-body p-3 small">Recent research proposes syntax-based approaches to address the problem of <a href=https://en.wikipedia.org/wiki/Computer_programming>generating programs</a> from <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language specifications</a>. These approaches typically train a sequence-to-sequence learning model using a syntax-based objective : <a href=https://en.wikipedia.org/wiki/Maximum_likelihood_estimation>maximum likelihood estimation (MLE)</a>. Such syntax-based approaches do not effectively address the goal of generating semantically correct programs, because these approaches fail to handle Program Aliasing, i.e., semantically equivalent programs may have many syntactically different forms. To address this issue, in this paper, we propose a semantics-based approach named SemRegex. SemRegex provides solutions for a subtask of the program-synthesis problem : generating <a href=https://en.wikipedia.org/wiki/Regular_expression>regular expressions</a> from <a href=https://en.wikipedia.org/wiki/Natural_language>natural language</a>. Different from the existing syntax-based approaches, SemRegex trains the model by maximizing the expected semantic correctness of the generated <a href=https://en.wikipedia.org/wiki/Regular_expression>regular expressions</a>. The semantic correctness is measured using the DFA-equivalence oracle, random test cases, and distinguishing test cases. The experiments on three public datasets demonstrate the superiority of SemRegex over the existing state-of-the-art approaches.</div></div><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1047.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1047 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1047 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D17-1047.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D17-1047" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D17-1047/>Recurrent Attention Network on Memory for Aspect Sentiment Analysis</a></strong><br><a href=/people/p/peng-chen/>Peng Chen</a>
|
<a href=/people/z/zhongqian-sun/>Zhongqian Sun</a>
|
<a href=/people/l/lidong-bing/>Lidong Bing</a>
|
<a href=/people/w/wei-yang/>Wei Yang</a><br><a href=/volumes/D17-1/ class=text-muted>Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1047><div class="card-body p-3 small">We propose a novel framework based on <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a> to identify the sentiment of opinion targets in a comment / review. Our <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> adopts multiple-attention mechanism to capture <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment features</a> separated by a long distance, so that it is more robust against irrelevant information. The results of multiple attentions are non-linearly combined with a <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent neural network</a>, which strengthens the expressive power of our model for handling more complications. The weighted-memory mechanism not only helps us avoid the labor-intensive feature engineering work, but also provides a tailor-made memory for different opinion targets of a sentence. We examine the merit of our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> on four <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> : two are from SemEval2014, i.e. reviews of restaurants and laptops ; a twitter dataset, for testing its performance on social media data ; and a Chinese news comment dataset, for testing its language sensitivity. The experimental results show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> consistently outperforms the state-of-the-art methods on different types of data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1312.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1312 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1312 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/238231213 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D17-1312/>A Simple <a href=https://en.wikipedia.org/wiki/Regularization_(mathematics)>Regularization-based Algorithm</a> for Learning Cross-Domain Word Embeddings</a></strong><br><a href=/people/w/wei-yang/>Wei Yang</a>
|
<a href=/people/w/wei-lu/>Wei Lu</a>
|
<a href=/people/v/vincent-zheng/>Vincent Zheng</a><br><a href=/volumes/D17-1/ class=text-muted>Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1312><div class="card-body p-3 small">Learning word embeddings has received a significant amount of attention recently. Often, <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> are learned in an <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised manner</a> from a large collection of text. The genre of the text typically plays an important role in the effectiveness of the resulting <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a>. How to effectively train word embedding models using data from different domains remains a problem that is less explored. In this paper, we present a simple yet effective method for learning <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> based on text from different domains. We demonstrate the effectiveness of our approach through extensive experiments on various down-stream NLP tasks.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Wei+Yang" title="Search for 'Wei Yang' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/j/jimmy-lin/ class=align-middle>Jimmy Lin</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/z/zexuan-zhong/ class=align-middle>Zexuan Zhong</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jiaqi-guo/ class=align-middle>Jiaqi Guo</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jian-peng/ class=align-middle>Jian Peng</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/tao-xie/ class=align-middle>Tao Xie</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/j/jian-guang-lou/ class=align-middle>Jian-Guang Lou</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/ting-liu/ class=align-middle>Ting Liu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/dongmei-zhang/ class=align-middle>Dongmei Zhang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/zeynep-akkalyoncu-yilmaz/ class=align-middle>Zeynep Akkalyoncu Yilmaz</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/haotian-zhang/ class=align-middle>Haotian Zhang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jinfeng-rao/ class=align-middle>Jinfeng Rao</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/linqing-liu/ class=align-middle>Linqing Liu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yi-tay/ class=align-middle>Yi Tay</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/p/peng-shi/ class=align-middle>Peng Shi</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/p/peng-chen/ class=align-middle>Peng Chen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/zhongqian-sun/ class=align-middle>Zhongqian Sun</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/lidong-bing/ class=align-middle>Lidong Bing</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/w/wei-lu/ class=align-middle>Wei Lu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/v/vincent-zheng/ class=align-middle>Vincent Zheng</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/rui-qiao/ class=align-middle>Rui Qiao</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/haocheng-qin/ class=align-middle>Haocheng Qin</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/amy-sun/ class=align-middle>Amy Sun</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/luchen-tan/ class=align-middle>Luchen Tan</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/kun-xiong/ class=align-middle>Kun Xiong</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/ming-li/ class=align-middle>Ming Li</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/emnlp/ class=align-middle>EMNLP</a><span class="badge badge-secondary align-middle ml-2">5</span></li><li class=list-group-item><a href=/venues/ws/ class=align-middle>WS</a><span class="badge badge-secondary align-middle ml-2">1</span></li></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>