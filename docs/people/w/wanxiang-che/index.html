<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Wanxiang Che (车万翔) - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Wanxiang</span> <span class=font-weight-bold>Che</span></h2><hr><div class=row><div class=col-lg-9><h4>2022</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.acl-long.191.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--acl-long--191 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.acl-long.191 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2022.acl-long.191.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.acl-long.191" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2022.acl-long.191/><span class=acl-fixed-case>GL</span>-<span class=acl-fixed-case>CL</span>e<span class=acl-fixed-case>F</span>: A Global–Local Contrastive Learning Framework for Cross-lingual Spoken Language Understanding</a></strong><br><a href=/people/l/libo-qin/>Libo Qin</a>
|
<a href=/people/q/qiguang-chen/>Qiguang Chen</a>
|
<a href=/people/t/tianbao-xie/>Tianbao Xie</a>
|
<a href=/people/q/qixin-li/>Qixin Li</a>
|
<a href=/people/j/jian-guang-lou/>Jian-Guang Lou</a>
|
<a href=/people/w/wanxiang-che/>Wanxiang Che</a>
|
<a href=/people/m/min-yen-kan/>Min-Yen Kan</a><br><a href=/volumes/2022.acl-long/ class=text-muted>Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--acl-long--191><div class="card-body p-3 small">Due to high data demands of current methods, attention to zero-shot cross-lingual spoken language understanding (SLU) has grown, as such approaches greatly reduce human annotation effort. However, existing models solely rely on shared parameters, which can only perform implicit alignment across languages. We present Global-Local Contrastive Learning Framework (GL-CLeF) to address this shortcoming. Specifically, we employ contrastive learning, leveraging bilingual dictionaries to construct multilingual views of the same utterance, then encourage their representations to be more similar than negative example pairs, which achieves to explicitly align representations of similar sentences across languages. In addition, a key step in GL-CLeF is a proposed Local and Global component, which achieves a fine-grained cross-lingual transfer (i.e., sentence-level Local intent transfer, token-level Local slot transfer, and semantic-level Global transfer across intent and slot). Experiments on MultiATIS++ show that GL-CLeF achieves the best performance and successfully pulls representations of similar sentences across languages closer.</div></div><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.ccl-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.ccl-1.0/>Proceedings of the 20th Chinese National Conference on Computational Linguistics</a></strong><br><a href=/people/s/sheng-li/>Sheng Li (李生)</a>
|
<a href=/people/m/maosong-sun/>Maosong Sun (孙茂松)</a>
|
<a href=/people/y/yang-liu-ict/>Yang Liu (刘洋)</a>
|
<a href=/people/h/hua-wu/>Hua Wu (吴华)</a>
|
<a href=/people/k/kang-liu/>Kang Liu (刘康)</a>
|
<a href=/people/w/wanxiang-che/>Wanxiang Che (车万翔)</a>
|
<a href=/people/s/shizhu-he/>Shizhu He (何世柱)</a>
|
<a href=/people/g/gaoqi-rao/>Gaoqi Rao (饶高琦)</a><br><a href=/volumes/2021.ccl-1/ class=text-muted>Proceedings of the 20th Chinese National Conference on Computational Linguistics</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.mrqa-1.10.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--mrqa-1--10 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.mrqa-1.10 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.mrqa-1.10/>Bilingual Alignment Pre-Training for Zero-Shot Cross-Lingual Transfer</a></strong><br><a href=/people/z/ziqing-yang/>Ziqing Yang</a>
|
<a href=/people/w/wentao-ma/>Wentao Ma</a>
|
<a href=/people/y/yiming-cui/>Yiming Cui</a>
|
<a href=/people/j/jiani-ye/>Jiani Ye</a>
|
<a href=/people/w/wanxiang-che/>Wanxiang Che</a>
|
<a href=/people/s/shijin-wang/>Shijin Wang</a><br><a href=/volumes/2021.mrqa-1/ class=text-muted>Proceedings of the 3rd Workshop on Machine Reading for Question Answering</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--mrqa-1--10><div class="card-body p-3 small">Multilingual pre-trained models have achieved remarkable performance on cross-lingual transfer learning. Some multilingual models such as mBERT, have been pre-trained on unlabeled corpora, therefore the embeddings of different languages in the <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> may not be aligned very well. In this paper, we aim to improve the zero-shot cross-lingual transfer performance by proposing a pre-training task named Word-Exchange Aligning Model (WEAM), which uses the statistical alignment information as the prior knowledge to guide cross-lingual word prediction. We evaluate our model on multilingual machine reading comprehension task MLQA and natural language interface task XNLI. The results show that <a href=https://en.wikipedia.org/wiki/WEAM>WEAM</a> can significantly improve the zero-shot performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.15.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--15 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.15 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.15" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.15/>GL-GIN : Fast and Accurate Non-Autoregressive Model for Joint Multiple Intent Detection and Slot Filling<span class=acl-fixed-case>GL</span>-<span class=acl-fixed-case>GIN</span>: Fast and Accurate Non-Autoregressive Model for Joint Multiple Intent Detection and Slot Filling</a></strong><br><a href=/people/l/libo-qin/>Libo Qin</a>
|
<a href=/people/f/fuxuan-wei/>Fuxuan Wei</a>
|
<a href=/people/t/tianbao-xie/>Tianbao Xie</a>
|
<a href=/people/x/xiao-xu/>Xiao Xu</a>
|
<a href=/people/w/wanxiang-che/>Wanxiang Che</a>
|
<a href=/people/t/ting-liu/>Ting Liu</a><br><a href=/volumes/2021.acl-long/ class=text-muted>Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--15><div class="card-body p-3 small">Multi-intent SLU can handle multiple intents in an utterance, which has attracted increasing attention. However, the state-of-the-art joint models heavily rely on <a href=https://en.wikipedia.org/wiki/Autoregressive_model>autoregressive approaches</a>, resulting in two issues : slow inference speed and <a href=https://en.wikipedia.org/wiki/Information_leakage>information leakage</a>. In this paper, we explore a non-autoregressive model for joint multiple intent detection and slot filling, achieving more fast and accurate. Specifically, we propose a Global-Locally Graph Interaction Network (GL-GIN) where a local slot-aware graph interaction layer is proposed to model slot dependency for alleviating uncoordinated slots problem while a global intent-slot graph interaction layer is introduced to model the interaction between multiple intents and all slots in the utterance. Experimental results on two public datasets show that our <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> achieves state-of-the-art performance while being 11.5 times faster.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.264.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--264 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.264 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.264" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.264/>Consistency Regularization for Cross-Lingual Fine-Tuning</a></strong><br><a href=/people/b/bo-zheng/>Bo Zheng</a>
|
<a href=/people/l/li-dong/>Li Dong</a>
|
<a href=/people/s/shaohan-huang/>Shaohan Huang</a>
|
<a href=/people/w/wenhui-wang/>Wenhui Wang</a>
|
<a href=/people/z/zewen-chi/>Zewen Chi</a>
|
<a href=/people/s/saksham-singhal/>Saksham Singhal</a>
|
<a href=/people/w/wanxiang-che/>Wanxiang Che</a>
|
<a href=/people/t/ting-liu/>Ting Liu</a>
|
<a href=/people/x/xia-song/>Xia Song</a>
|
<a href=/people/f/furu-wei/>Furu Wei</a><br><a href=/volumes/2021.acl-long/ class=text-muted>Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--264><div class="card-body p-3 small">Fine-tuning pre-trained cross-lingual language models can transfer task-specific supervision from one language to the others. In this work, we propose to improve cross-lingual fine-tuning with consistency regularization. Specifically, we use example consistency regularization to penalize the prediction sensitivity to four types of data augmentations, i.e., subword sampling, <a href=https://en.wikipedia.org/wiki/Gaussian_noise>Gaussian noise</a>, code-switch substitution, and <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>. In addition, we employ <a href=https://en.wikipedia.org/wiki/Consistency_(statistics)>model consistency</a> to regularize the <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> trained with two augmented versions of the same training set. Experimental results on the XTREME benchmark show that our method significantly improves cross-lingual fine-tuning across various tasks, including text classification, <a href=https://en.wikipedia.org/wiki/Question_answering>question answering</a>, and <a href=https://en.wikipedia.org/wiki/Sequence_labeling>sequence labeling</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.356.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--356 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.356 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.356/>DuRecDial 2.0 : A Bilingual Parallel Corpus for Conversational Recommendation<span class=acl-fixed-case>D</span>u<span class=acl-fixed-case>R</span>ec<span class=acl-fixed-case>D</span>ial 2.0: A Bilingual Parallel Corpus for Conversational Recommendation</a></strong><br><a href=/people/z/zeming-liu/>Zeming Liu</a>
|
<a href=/people/h/haifeng-wang/>Haifeng Wang</a>
|
<a href=/people/z/zheng-yu-niu/>Zheng-Yu Niu</a>
|
<a href=/people/h/hua-wu/>Hua Wu</a>
|
<a href=/people/w/wanxiang-che/>Wanxiang Che</a><br><a href=/volumes/2021.emnlp-main/ class=text-muted>Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--356><div class="card-body p-3 small">In this paper, we provide a bilingual parallel human-to-human recommendation dialog dataset (DuRecDial 2.0) to enable researchers to explore a challenging task of multilingual and cross-lingual conversational recommendation. The difference between DuRecDial 2.0 and existing conversational recommendation datasets is that the data item (Profile, Goal, Knowledge, Context, Response) in DuRecDial 2.0 is annotated in two languages, both <a href=https://en.wikipedia.org/wiki/English_language>English</a> and <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a>, while other datasets are built with the setting of a single language. We collect 8.2k dialogs aligned across English and Chinese languages (16.5k dialogs and 255k utterances in total) that are annotated by crowdsourced workers with strict quality control procedure. We then build monolingual, multilingual, and cross-lingual conversational recommendation baselines on DuRecDial 2.0. Experiment results show that the use of additional English data can bring performance improvement for Chinese conversational recommendation, indicating the benefits of DuRecDial 2.0. Finally, this dataset provides a challenging testbed for future studies of monolingual, multilingual, and cross-lingual conversational recommendation.</div></div><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.acl-main.10.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--acl-main--10 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.acl-main.10 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://slideslive.com/38928822 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.acl-main.10/>Slot-consistent NLG for Task-oriented Dialogue Systems with Iterative Rectification Network<span class=acl-fixed-case>NLG</span> for Task-oriented Dialogue Systems with Iterative Rectification Network</a></strong><br><a href=/people/y/yangming-li/>Yangming Li</a>
|
<a href=/people/k/kaisheng-yao/>Kaisheng Yao</a>
|
<a href=/people/l/libo-qin/>Libo Qin</a>
|
<a href=/people/w/wanxiang-che/>Wanxiang Che</a>
|
<a href=/people/x/xiaolong-li/>Xiaolong Li</a>
|
<a href=/people/t/ting-liu/>Ting Liu</a><br><a href=/volumes/2020.acl-main/ class=text-muted>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--acl-main--10><div class="card-body p-3 small">Data-driven approaches using <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a> have achieved promising performances in natural language generation (NLG). However, neural generators are prone to make mistakes, e.g., neglecting an input slot value and generating a redundant slot value. Prior works refer this to <a href=https://en.wikipedia.org/wiki/Hallucination>hallucination phenomenon</a>. In this paper, we study slot consistency for building reliable NLG systems with all slot values of input dialogue act (DA) properly generated in output sentences. We propose Iterative Rectification Network (IRN) for improving general NLG systems to produce both correct and fluent responses. It applies a <a href=https://en.wikipedia.org/wiki/Bootstrapping_(statistics)>bootstrapping algorithm</a> to sample training candidates and uses <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning</a> to incorporate discrete reward related to slot inconsistency into training. Comprehensive studies have been conducted on multiple benchmark datasets, showing that the proposed methods have significantly reduced the slot error rate (ERR) for all strong baselines. Human evaluations also have confirmed its effectiveness.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.acl-main.599.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--acl-main--599 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.acl-main.599 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://slideslive.com/38928861 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.acl-main.599" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.acl-main.599/>Document Modeling with Graph Attention Networks for Multi-grained Machine Reading Comprehension</a></strong><br><a href=/people/b/bo-zheng/>Bo Zheng</a>
|
<a href=/people/h/haoyang-wen/>Haoyang Wen</a>
|
<a href=/people/y/yaobo-liang/>Yaobo Liang</a>
|
<a href=/people/n/nan-duan/>Nan Duan</a>
|
<a href=/people/w/wanxiang-che/>Wanxiang Che</a>
|
<a href=/people/d/daxin-jiang/>Daxin Jiang</a>
|
<a href=/people/m/ming-zhou/>Ming Zhou</a>
|
<a href=/people/t/ting-liu/>Ting Liu</a><br><a href=/volumes/2020.acl-main/ class=text-muted>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--acl-main--599><div class="card-body p-3 small">Natural Questions is a new challenging machine reading comprehension benchmark with two-grained answers, which are a long answer (typically a paragraph) and a short answer (one or more entities inside the long answer). Despite the effectiveness of existing methods on this <a href=https://en.wikipedia.org/wiki/Benchmarking>benchmark</a>, they treat these two sub-tasks individually during training while ignoring their dependencies. To address this issue, we present a novel multi-grained machine reading comprehension framework that focuses on modeling documents at their hierarchical nature, which are different levels of granularity : documents, paragraphs, sentences, and tokens. We utilize graph attention networks to obtain different levels of <a href=https://en.wikipedia.org/wiki/Representation_(mathematics)>representations</a> so that they can be learned simultaneously. The long and short answers can be extracted from paragraph-level representation and token-level representation, respectively. In this way, we can model the dependencies between the two-grained answers to provide evidence for each other. We jointly train the two sub-tasks, and our experiments show that our approach significantly outperforms previous systems at both long and short answer criteria.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.acl-demos.2.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--acl-demos--2 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.acl-demos.2 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://slideslive.com/38928594 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.acl-demos.2" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.acl-demos.2/>TextBrewer : An Open-Source Knowledge Distillation Toolkit for <a href=https://en.wikipedia.org/wiki/Natural_language_processing>Natural Language Processing</a><span class=acl-fixed-case>T</span>ext<span class=acl-fixed-case>B</span>rewer: <span class=acl-fixed-case>A</span>n <span class=acl-fixed-case>O</span>pen-<span class=acl-fixed-case>S</span>ource <span class=acl-fixed-case>K</span>nowledge <span class=acl-fixed-case>D</span>istillation <span class=acl-fixed-case>T</span>oolkit for <span class=acl-fixed-case>N</span>atural <span class=acl-fixed-case>L</span>anguage <span class=acl-fixed-case>P</span>rocessing</a></strong><br><a href=/people/z/ziqing-yang/>Ziqing Yang</a>
|
<a href=/people/y/yiming-cui/>Yiming Cui</a>
|
<a href=/people/z/zhipeng-chen/>Zhipeng Chen</a>
|
<a href=/people/w/wanxiang-che/>Wanxiang Che</a>
|
<a href=/people/t/ting-liu/>Ting Liu</a>
|
<a href=/people/s/shijin-wang/>Shijin Wang</a>
|
<a href=/people/g/guoping-hu/>Guoping Hu</a><br><a href=/volumes/2020.acl-demos/ class=text-muted>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--acl-demos--2><div class="card-body p-3 small">In this paper, we introduce TextBrewer, an open-source knowledge distillation toolkit designed for <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a>. It works with different neural network models and supports various kinds of supervised learning tasks, such as <a href=https://en.wikipedia.org/wiki/Text_classification>text classification</a>, <a href=https://en.wikipedia.org/wiki/Reading_comprehension>reading comprehension</a>, <a href=https://en.wikipedia.org/wiki/Sequence_labeling>sequence labeling</a>. TextBrewer provides a simple and uniform workflow that enables quick setting up of <a href=https://en.wikipedia.org/wiki/Distillation>distillation</a> experiments with highly flexible configurations. It offers a set of predefined distillation methods and can be extended with custom code. As a case study, we use TextBrewer to distill BERT on several typical NLP tasks. With simple configurations, we achieve results that are comparable with or even higher than the public distilled BERT models with similar numbers of parameters.</div></div><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1013.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1013 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1013 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1013" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1013/>Entity-Consistent End-to-end Task-Oriented Dialogue System with KB Retriever<span class=acl-fixed-case>KB</span> Retriever</a></strong><br><a href=/people/l/libo-qin/>Libo Qin</a>
|
<a href=/people/y/yijia-liu/>Yijia Liu</a>
|
<a href=/people/w/wanxiang-che/>Wanxiang Che</a>
|
<a href=/people/h/haoyang-wen/>Haoyang Wen</a>
|
<a href=/people/y/yangming-li/>Yangming Li</a>
|
<a href=/people/t/ting-liu/>Ting Liu</a><br><a href=/volumes/D19-1/ class=text-muted>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1013><div class="card-body p-3 small">Querying the knowledge base (KB) has long been a challenge in the end-to-end task-oriented dialogue system. Previous sequence-to-sequence (Seq2Seq) dialogue generation work treats the <a href=https://en.wikipedia.org/wiki/Kibibyte>KB query</a> as an attention over the entire <a href=https://en.wikipedia.org/wiki/Kibibyte>KB</a>, without the guarantee that the generated entities are consistent with each other. In this paper, we propose a novel <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> which queries the <a href=https://en.wikipedia.org/wiki/Kibibyte>KB</a> in two steps to improve the consistency of generated entities. In the first step, inspired by the observation that a response can usually be supported by a single <a href=https://en.wikipedia.org/wiki/Row_(database)>KB row</a>, we introduce a KB retrieval component which explicitly returns the most relevant <a href=https://en.wikipedia.org/wiki/Row_(database)>KB row</a> given a dialogue history. The retrieval result is further used to filter the irrelevant entities in a Seq2Seq response generation model to improve the consistency among the output entities. In the second step, we further perform the <a href=https://en.wikipedia.org/wiki/Attentional_control>attention mechanism</a> to address the most correlated KB column. Two methods are proposed to make the training feasible without labeled retrieval data, which include distant supervision and Gumbel-Softmax technique. Experiments on two publicly available task oriented dialog datasets show the effectiveness of our model by outperforming the baseline systems and producing entity-consistent responses.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1169.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1169 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1169 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1169" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1169/>Cross-Lingual Machine Reading Comprehension</a></strong><br><a href=/people/y/yiming-cui/>Yiming Cui</a>
|
<a href=/people/w/wanxiang-che/>Wanxiang Che</a>
|
<a href=/people/t/ting-liu/>Ting Liu</a>
|
<a href=/people/b/bing-qin/>Bing Qin</a>
|
<a href=/people/s/shijin-wang/>Shijin Wang</a>
|
<a href=/people/g/guoping-hu/>Guoping Hu</a><br><a href=/volumes/D19-1/ class=text-muted>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1169><div class="card-body p-3 small">Though the community has made great progress on Machine Reading Comprehension (MRC) task, most of the previous works are solving English-based MRC problems, and there are few efforts on other languages mainly due to the lack of large-scale training data. In this paper, we propose Cross-Lingual Machine Reading Comprehension (CLMRC) task for the languages other than English. Firstly, we present several back-translation approaches for CLMRC task which is straightforward to adopt. However, to exactly align the answer into source language is difficult and could introduce additional noise. In this context, we propose a novel model called Dual BERT, which takes advantage of the large-scale training data provided by rich-resource language (such as English) and learn the semantic relations between the passage and question in bilingual context, and then utilize the learned knowledge to improve <a href=https://en.wikipedia.org/wiki/Reading_comprehension>reading comprehension</a> performance of low-resource language. We conduct experiments on two Chinese machine reading comprehension datasets CMRC 2018 and DRCD. The results show consistent and significant improvements over various state-of-the-art systems by a large margin, which demonstrate the potentials in CLMRC task. Resources available : https://github.com/ymcui/Cross-Lingual-MRC</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1214.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1214 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1214 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1214" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1214/>A Stack-Propagation Framework with Token-Level Intent Detection for Spoken Language Understanding</a></strong><br><a href=/people/l/libo-qin/>Libo Qin</a>
|
<a href=/people/w/wanxiang-che/>Wanxiang Che</a>
|
<a href=/people/y/yangming-li/>Yangming Li</a>
|
<a href=/people/h/haoyang-wen/>Haoyang Wen</a>
|
<a href=/people/t/ting-liu/>Ting Liu</a><br><a href=/volumes/D19-1/ class=text-muted>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1214><div class="card-body p-3 small">Intent detection and slot filling are two main tasks for building a spoken language understanding (SLU) system. The two <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> are closely tied and the slots often highly depend on the intent. In this paper, we propose a novel <a href=https://en.wikipedia.org/wiki/Conceptual_framework>framework</a> for SLU to better incorporate the intent information, which further guiding the slot filling. In our framework, we adopt a joint model with Stack-Propagation which can directly use the intent information as input for slot filling, thus to capture the intent semantic knowledge. In addition, to further alleviate the <a href=https://en.wikipedia.org/wiki/Error_propagation>error propagation</a>, we perform the token-level intent detection for the Stack-Propagation framework. Experiments on two publicly datasets show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieves the state-of-the-art performance and outperforms other previous methods by a large margin. Finally, we use the Bidirectional Encoder Representation from Transformer (BERT) model in our framework, which further boost our performance in SLU task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1600.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1600 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1600 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1600" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1600/>A Span-Extraction Dataset for Chinese Machine Reading Comprehension<span class=acl-fixed-case>C</span>hinese Machine Reading Comprehension</a></strong><br><a href=/people/y/yiming-cui/>Yiming Cui</a>
|
<a href=/people/t/ting-liu/>Ting Liu</a>
|
<a href=/people/w/wanxiang-che/>Wanxiang Che</a>
|
<a href=/people/l/li-xiao/>Li Xiao</a>
|
<a href=/people/z/zhipeng-chen/>Zhipeng Chen</a>
|
<a href=/people/w/wentao-ma/>Wentao Ma</a>
|
<a href=/people/s/shijin-wang/>Shijin Wang</a>
|
<a href=/people/g/guoping-hu/>Guoping Hu</a><br><a href=/volumes/D19-1/ class=text-muted>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1600><div class="card-body p-3 small">Machine Reading Comprehension (MRC) has become enormously popular recently and has attracted a lot of attention. However, the existing reading comprehension datasets are mostly in <a href=https://en.wikipedia.org/wiki/English_language>English</a>. In this paper, we introduce a Span-Extraction dataset for Chinese machine reading comprehension to add <a href=https://en.wikipedia.org/wiki/Language_diversity>language diversities</a> in this area. The <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> is composed by near 20,000 real questions annotated on Wikipedia paragraphs by human experts. We also annotated a challenge set which contains the questions that need comprehensive understanding and multi-sentence inference throughout the context. We present several <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline systems</a> as well as anonymous submissions for demonstrating the difficulties in this <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>. With the release of the <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>, we hosted the Second Evaluation Workshop on Chinese Machine Reading Comprehension (CMRC 2018). We hope the release of the <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> could further accelerate the Chinese machine reading comprehension research. Resources are available : https://github.com/ymcui/cmrc2018</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1103.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1103 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1103 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/384478489 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1103" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1103/>Generating Natural Language Adversarial Examples through Probability Weighted Word Saliency</a></strong><br><a href=/people/s/shuhuai-ren/>Shuhuai Ren</a>
|
<a href=/people/y/yihe-deng/>Yihe Deng</a>
|
<a href=/people/k/kun-he/>Kun He</a>
|
<a href=/people/w/wanxiang-che/>Wanxiang Che</a><br><a href=/volumes/P19-1/ class=text-muted>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1103><div class="card-body p-3 small">We address the problem of adversarial attacks on <a href=https://en.wikipedia.org/wiki/Text_classification>text classification</a>, which is rarely studied comparing to attacks on <a href=https://en.wikipedia.org/wiki/Image_classification>image classification</a>. The challenge of this task is to generate adversarial examples that maintain <a href=https://en.wikipedia.org/wiki/Lexical_analysis>lexical correctness</a>, <a href=https://en.wikipedia.org/wiki/Grammaticality>grammatical correctness</a> and <a href=https://en.wikipedia.org/wiki/Semantic_similarity>semantic similarity</a>. Based on the synonyms substitution strategy, we introduce a new word replacement order determined by both the word saliency and the classification probability, and propose a <a href=https://en.wikipedia.org/wiki/Greedy_algorithm>greedy algorithm</a> called probability weighted word saliency (PWWS) for text adversarial attack. Experiments on three popular datasets using <a href=https://en.wikipedia.org/wiki/Convolution>convolutional</a> as well as LSTM models show that PWWS reduces the classification accuracy to the most extent, and keeps a very low word substitution rate. A human evaluation study shows that our generated adversarial examples maintain the <a href=https://en.wikipedia.org/wiki/Semantic_similarity>semantic similarity</a> well and are hard for humans to perceive. Performing adversarial training using our perturbed datasets improves the robustness of the <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a>. At last, our <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>method</a> also exhibits a good transferability on the generated adversarial examples.</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1105.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1105 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1105 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=C18-1105" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/C18-1105/>Sequence-to-Sequence Data Augmentation for Dialogue Language Understanding</a></strong><br><a href=/people/y/yutai-hou/>Yutai Hou</a>
|
<a href=/people/y/yijia-liu/>Yijia Liu</a>
|
<a href=/people/w/wanxiang-che/>Wanxiang Che</a>
|
<a href=/people/t/ting-liu/>Ting Liu</a><br><a href=/volumes/C18-1/ class=text-muted>Proceedings of the 27th International Conference on Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1105><div class="card-body p-3 small">In this paper, we study the problem of <a href=https://en.wikipedia.org/wiki/Data_augmentation>data augmentation</a> for <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>language understanding</a> in task-oriented dialogue system. In contrast to previous work which augments an utterance without considering its relation with other utterances, we propose a sequence-to-sequence generation based data augmentation framework that leverages one utterance&#8217;s same semantic alternatives in the training data. A novel diversity rank is incorporated into the utterance representation to make the model produce diverse utterances and these diversely augmented utterances help to improve the language understanding module. Experimental results on the Airline Travel Information System dataset and a newly created semantic frame annotation on Stanford Multi-turn, Multi-domain Dialogue Dataset show that our framework achieves significant improvements of 6.38 and 10.04 F-scores respectively when only a training set of hundreds utterances is represented. Case studies also confirm that our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> generates diverse utterances.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1320.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1320 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1320 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1320/>Sequence-to-Sequence Learning for Task-oriented Dialogue with Dialogue State Representation</a></strong><br><a href=/people/h/haoyang-wen/>Haoyang Wen</a>
|
<a href=/people/y/yijia-liu/>Yijia Liu</a>
|
<a href=/people/w/wanxiang-che/>Wanxiang Che</a>
|
<a href=/people/l/libo-qin/>Libo Qin</a>
|
<a href=/people/t/ting-liu/>Ting Liu</a><br><a href=/volumes/C18-1/ class=text-muted>Proceedings of the 27th International Conference on Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1320><div class="card-body p-3 small">Classic <a href=https://en.wikipedia.org/wiki/Pipeline_(software)>pipeline models</a> for task-oriented dialogue system require explicit modeling the dialogue states and hand-crafted action spaces to query a domain-specific knowledge base. Conversely, sequence-to-sequence models learn to map dialogue history to the response in current turn without explicit knowledge base querying. In this work, we propose a novel <a href=https://en.wikipedia.org/wiki/Conceptual_framework>framework</a> that leverages the advantages of classic pipeline and sequence-to-sequence models. Our framework models a dialogue state as a fixed-size distributed representation and use this <a href=https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning>representation</a> to query a <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge base</a> via an attention mechanism. Experiment on Stanford Multi-turn Multi-domain Task-oriented Dialogue Dataset shows that our framework significantly outperforms other sequence-to-sequence based baseline models on both automatic and human evaluation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1264.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1264 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1264 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/306049123 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1264" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1264/>An AMR Aligner Tuned by Transition-based Parser<span class=acl-fixed-case>AMR</span> Aligner Tuned by Transition-based Parser</a></strong><br><a href=/people/y/yijia-liu/>Yijia Liu</a>
|
<a href=/people/w/wanxiang-che/>Wanxiang Che</a>
|
<a href=/people/b/bo-zheng/>Bo Zheng</a>
|
<a href=/people/b/bing-qin/>Bing Qin</a>
|
<a href=/people/t/ting-liu/>Ting Liu</a><br><a href=/volumes/D18-1/ class=text-muted>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1264><div class="card-body p-3 small">In this paper, we propose a new rich resource enhanced AMR aligner which produces multiple alignments and a new transition system for AMR parsing along with its oracle parser. Our aligner is further tuned by our <a href=https://en.wikipedia.org/wiki/Oracle_machine>oracle parser</a> via picking the alignment that leads to the highest-scored achievable AMR graph. Experimental results show that our aligner outperforms the rule-based aligner in previous work by achieving higher alignment F1 score and consistently improving two open-sourced AMR parsers. Based on our aligner and transition system, we develop a transition-based AMR parser that parses a sentence into its AMR graph directly. An ensemble of our <a href=https://en.wikipedia.org/wiki/Parsing>parsers</a> with only words and POS tags as input leads to 68.4 Smatch F1 score, which outperforms the current state-of-the-art parser.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1088.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1088 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1088 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N18-1088" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N18-1088/>Parsing Tweets into Universal Dependencies<span class=acl-fixed-case>U</span>niversal <span class=acl-fixed-case>D</span>ependencies</a></strong><br><a href=/people/y/yijia-liu/>Yijia Liu</a>
|
<a href=/people/y/yi-zhu/>Yi Zhu</a>
|
<a href=/people/w/wanxiang-che/>Wanxiang Che</a>
|
<a href=/people/b/bing-qin/>Bing Qin</a>
|
<a href=/people/n/nathan-schneider/>Nathan Schneider</a>
|
<a href=/people/n/noah-a-smith/>Noah A. Smith</a><br><a href=/volumes/N18-1/ class=text-muted>Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1088><div class="card-body p-3 small">We study the problem of analyzing <a href=https://en.wikipedia.org/wiki/Twitter>tweets</a> with universal dependencies (UD). We extend the UD guidelines to cover special constructions in tweets that affect <a href=https://en.wikipedia.org/wiki/Lexical_analysis>tokenization</a>, <a href=https://en.wikipedia.org/wiki/Part-of-speech_tagging>part-of-speech tagging</a>, and labeled dependencies. Using the extended guidelines, we create a new tweet treebank for English (Tweebank v2) that is four times larger than the (unlabeled) Tweebank v1 introduced by Kong et al. We characterize the disagreements between our annotators and show that it is challenging to deliver consistent annotation due to ambiguity in understanding and explaining tweets. Nonetheless, using the new <a href=https://en.wikipedia.org/wiki/Treebank>treebank</a>, we build a <a href=https://en.wikipedia.org/wiki/Pipeline_(software)>pipeline system</a> to parse raw tweets into UD. To overcome the annotation noise without sacrificing computational efficiency, we propose a new method to distill an ensemble of 20 transition-based parsers into a single one. Our <a href=https://en.wikipedia.org/wiki/Parsing>parser</a> achieves an improvement of 2.2 in <a href=https://en.wikipedia.org/wiki/Lisp_(programming_language)>LAS</a> over the un-ensembled baseline and outperforms parsers that are state-of-the-art on other treebanks in both <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> and <a href=https://en.wikipedia.org/wiki/Speed>speed</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K18-2005.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K18-2005 data-toggle=collapse aria-expanded=false aria-controls=abstract-K18-2005 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=K18-2005" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/K18-2005/>Towards Better UD Parsing : Deep Contextualized Word Embeddings, Ensemble, and Treebank Concatenation<span class=acl-fixed-case>UD</span> Parsing: Deep Contextualized Word Embeddings, Ensemble, and Treebank Concatenation</a></strong><br><a href=/people/w/wanxiang-che/>Wanxiang Che</a>
|
<a href=/people/y/yijia-liu/>Yijia Liu</a>
|
<a href=/people/y/yuxuan-wang/>Yuxuan Wang</a>
|
<a href=/people/b/bo-zheng/>Bo Zheng</a>
|
<a href=/people/t/ting-liu/>Ting Liu</a><br><a href=/volumes/K18-2/ class=text-muted>Proceedings of the CoNLL 2018 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K18-2005><div class="card-body p-3 small">This paper describes our system (HIT-SCIR) submitted to the CoNLL 2018 shared task on Multilingual Parsing from Raw Text to Universal Dependencies. We base our submission on Stanford&#8217;s winning system for the CoNLL 2017 shared task and make two effective extensions : 1) incorporating deep contextualized word embeddings into both the part of speech tagger and <a href=https://en.wikipedia.org/wiki/Parsing>parser</a> ; 2) ensembling parsers trained with different initialization. We also explore different ways of concatenating <a href=https://en.wikipedia.org/wiki/Treebank>treebanks</a> for further improvements. Experimental results on the development data show the effectiveness of our <a href=https://en.wikipedia.org/wiki/Methodology>methods</a>. In the final evaluation, our <a href=https://en.wikipedia.org/wiki/System>system</a> was ranked first according to <a href=https://en.wikipedia.org/wiki/Level_of_service>LAS</a> (75.84 %) and outperformed the other <a href=https://en.wikipedia.org/wiki/System>systems</a> by a large margin.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-3000.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P18-3000/>Proceedings of <span class=acl-fixed-case>ACL</span> 2018, Student Research Workshop</a></strong><br><a href=/people/v/vered-shwartz/>Vered Shwartz</a>
|
<a href=/people/j/jeniya-tabassum/>Jeniya Tabassum</a>
|
<a href=/people/r/rob-voigt/>Rob Voigt</a>
|
<a href=/people/w/wanxiang-che/>Wanxiang Che</a>
|
<a href=/people/m/marie-catherine-de-marneffe/>Marie-Catherine de Marneffe</a>
|
<a href=/people/m/malvina-nissim/>Malvina Nissim</a><br><a href=/volumes/P18-3/ class=text-muted>Proceedings of ACL 2018, Student Research Workshop</a></span></p><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-5001.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-5001 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-5001 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-5001/>Deep Learning in Lexical Analysis and Parsing</a></strong><br><a href=/people/w/wanxiang-che/>Wanxiang Che</a>
|
<a href=/people/y/yue-zhang/>Yue Zhang</a><br><a href=/volumes/I17-5/ class=text-muted>Proceedings of the IJCNLP 2017, Tutorial Abstracts</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-5001><div class="card-body p-3 small">Neural networks, also with a fancy name <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning</a>, just right can overcome the above feature engineering problem. In theory, they can use non-linear activation functions and multiple layers to automatically find useful <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a>. The novel <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>network structures</a>, such as convolutional or recurrent, help to reduce the difficulty further. These deep learning models have been successfully used for <a href=https://en.wikipedia.org/wiki/Lexical_analysis>lexical analysis</a> and <a href=https://en.wikipedia.org/wiki/Parsing>parsing</a>. In this tutorial, we will give a review of each line of work, by contrasting them with traditional <a href=https://en.wikipedia.org/wiki/Statistics>statistical methods</a>, and organizing them in consistent orders.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K17-3005.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K17-3005 data-toggle=collapse aria-expanded=false aria-controls=abstract-K17-3005 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/K17-3005/>The HIT-SCIR System for End-to-End Parsing of Universal Dependencies<span class=acl-fixed-case>HIT</span>-<span class=acl-fixed-case>SCIR</span> System for End-to-End Parsing of <span class=acl-fixed-case>U</span>niversal <span class=acl-fixed-case>D</span>ependencies</a></strong><br><a href=/people/w/wanxiang-che/>Wanxiang Che</a>
|
<a href=/people/j/jiang-guo/>Jiang Guo</a>
|
<a href=/people/y/yuxuan-wang/>Yuxuan Wang</a>
|
<a href=/people/b/bo-zheng/>Bo Zheng</a>
|
<a href=/people/h/huaipeng-zhao/>Huaipeng Zhao</a>
|
<a href=/people/y/yang-liu/>Yang Liu</a>
|
<a href=/people/d/dechuan-teng/>Dechuan Teng</a>
|
<a href=/people/t/ting-liu/>Ting Liu</a><br><a href=/volumes/K17-3/ class=text-muted>Proceedings of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K17-3005><div class="card-body p-3 small">This paper describes our system (HIT-SCIR) for the CoNLL 2017 shared task : Multilingual Parsing from Raw Text to Universal Dependencies. Our system includes three pipelined components : <a href=https://en.wikipedia.org/wiki/Lexical_analysis>tokenization</a>, Part-of-Speech (POS) tagging and dependency parsing. We use character-based bidirectional long short-term memory (LSTM) networks for both tokenization and POS tagging. Afterwards, we employ a list-based transition-based algorithm for general non-projective parsing and present an improved Stack-LSTM-based architecture for representing each transition state and making predictions. Furthermore, to parse low / zero-resource languages and cross-domain data, we use a model transfer approach to make effective use of existing resources. We demonstrate substantial gains against the UDPipe baseline, with an average improvement of 3.76 % in LAS of all languages. And finally, we rank the 4th place on the official test sets.<i>tokenization</i>,\n <i>Part-of-Speech</i> (POS) <i>tagging</i> and <i>dependency parsing</i>.\n We use character-based bidirectional long short-term memory (LSTM) networks for\n both tokenization and POS tagging.\n Afterwards, we employ a list-based transition-based algorithm for general\n non-projective parsing and present an improved Stack-LSTM-based architecture\n for representing each transition state and making predictions.\n Furthermore, to parse low/zero-resource languages and cross-domain data, we use\n a model transfer approach to make effective use of existing resources.\n We demonstrate substantial gains against the UDPipe baseline, with an average\n improvement of 3.76% in LAS of all languages. And finally, we rank the 4th\n place on the official test sets.\n</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1296.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1296 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1296 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D17-1296" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D17-1296/>Transition-Based Disfluency Detection using <a href=https://en.wikipedia.org/wiki/Linear_time-invariant_system>LSTMs</a><span class=acl-fixed-case>LSTM</span>s</a></strong><br><a href=/people/s/shaolei-wang/>Shaolei Wang</a>
|
<a href=/people/w/wanxiang-che/>Wanxiang Che</a>
|
<a href=/people/y/yue-zhang/>Yue Zhang</a>
|
<a href=/people/m/meishan-zhang/>Meishan Zhang</a>
|
<a href=/people/t/ting-liu/>Ting Liu</a><br><a href=/volumes/D17-1/ class=text-muted>Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1296><div class="card-body p-3 small">In this paper, we model the problem of disfluency detection using a transition-based framework, which incrementally constructs and labels the disfluency chunk of input sentences using a new transition system without syntax information. Compared with sequence labeling methods, it can capture non-local chunk-level features ; compared with joint parsing and disfluency detection methods, it is free for noise in syntax. Experiments show that our <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> achieves state-of-the-art <a href=https://en.wikipedia.org/wiki/F-score>f-score</a> of 87.5 % on the commonly used English Switchboard test set, and a set of in-house annotated Chinese data.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Wanxiang+Che+%28%E8%BD%A6%E4%B8%87%E7%BF%94%29" title="Search for 'Wanxiang Che (车万翔)' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/t/ting-liu/ class=align-middle>Ting Liu</a>
<span class="badge badge-secondary align-middle ml-2">15</span></li><li class=list-group-item><a href=/people/y/yijia-liu/ class=align-middle>Yijia Liu</a>
<span class="badge badge-secondary align-middle ml-2">6</span></li><li class=list-group-item><a href=/people/l/libo-qin/ class=align-middle>Libo Qin</a>
<span class="badge badge-secondary align-middle ml-2">6</span></li><li class=list-group-item><a href=/people/b/bo-zheng/ class=align-middle>Bo Zheng</a>
<span class="badge badge-secondary align-middle ml-2">5</span></li><li class=list-group-item><a href=/people/y/yiming-cui/ class=align-middle>Yiming Cui</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/s/shijin-wang/ class=align-middle>Shijin Wang</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/people/h/haoyang-wen/ class=align-middle>Haoyang Wen</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/people/y/yangming-li/ class=align-middle>Yangming Li</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/g/guoping-hu/ class=align-middle>Guoping Hu</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/b/bing-qin/ class=align-middle>Bing Qin</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/h/hua-wu/ class=align-middle>Hua Wu (吴华)</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/z/ziqing-yang/ class=align-middle>Ziqing Yang</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/w/wentao-ma/ class=align-middle>Wentao Ma</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/t/tianbao-xie/ class=align-middle>Tianbao Xie</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/y/yue-zhang/ class=align-middle>Yue Zhang</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/z/zhipeng-chen/ class=align-middle>Zhipeng Chen</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/y/yuxuan-wang/ class=align-middle>Yuxuan Wang</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/s/sheng-li/ class=align-middle>Sheng Li (李生)</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/maosong-sun/ class=align-middle>Maosong Sun (孙茂松)</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yang-liu-ict/ class=align-middle>Yang Liu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/kang-liu/ class=align-middle>Kang Liu (刘康)</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/shizhu-he/ class=align-middle>Shizhu He (何世柱)</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/g/gaoqi-rao/ class=align-middle>Gaoqi Rao (饶高琦)</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jiani-ye/ class=align-middle>Jiani Ye</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yutai-hou/ class=align-middle>Yutai Hou</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/f/fuxuan-wei/ class=align-middle>Fuxuan Wei</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/x/xiao-xu/ class=align-middle>Xiao Xu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/li-dong/ class=align-middle>Li Dong</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/shaohan-huang/ class=align-middle>Shaohan Huang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/w/wenhui-wang/ class=align-middle>Wenhui Wang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/zewen-chi/ class=align-middle>Zewen Chi</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/saksham-singhal/ class=align-middle>Saksham Singhal</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/x/xia-song/ class=align-middle>Xia Song</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/f/furu-wei/ class=align-middle>Furu Wei</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/kaisheng-yao/ class=align-middle>Kaisheng Yao</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/x/xiaolong-li/ class=align-middle>Xiaolong Li</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yaobo-liang/ class=align-middle>Yaobo Liang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/n/nan-duan/ class=align-middle>Nan Duan</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/daxin-jiang/ class=align-middle>Daxin Jiang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/ming-zhou/ class=align-middle>Ming Zhou</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jiang-guo/ class=align-middle>Jiang Guo</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/huaipeng-zhao/ class=align-middle>Huaipeng Zhao</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yang-liu/ class=align-middle>Yang Liu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/dechuan-teng/ class=align-middle>Dechuan Teng</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/q/qiguang-chen/ class=align-middle>Qiguang Chen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/q/qixin-li/ class=align-middle>Qixin Li</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jian-guang-lou/ class=align-middle>Jian-Guang Lou</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/min-yen-kan/ class=align-middle>Min-Yen Kan</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/zeming-liu/ class=align-middle>Zeming Liu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/haifeng-wang/ class=align-middle>Haifeng Wang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/zheng-yu-niu/ class=align-middle>Zheng-Yu Niu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/li-xiao/ class=align-middle>Li Xiao</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/shaolei-wang/ class=align-middle>Shaolei Wang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/meishan-zhang/ class=align-middle>Meishan Zhang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yi-zhu/ class=align-middle>Yi Zhu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/n/nathan-schneider/ class=align-middle>Nathan Schneider</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/n/noah-a-smith/ class=align-middle>Noah A. Smith</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/v/vered-shwartz/ class=align-middle>Vered Shwartz</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jeniya-tabassum/ class=align-middle>Jeniya Tabassum</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/rob-voigt/ class=align-middle>Rob Voigt</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/marie-catherine-de-marneffe/ class=align-middle>Marie-Catherine de Marneffe</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/malvina-nissim/ class=align-middle>Malvina Nissim</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/shuhuai-ren/ class=align-middle>Shuhuai Ren</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yihe-deng/ class=align-middle>Yihe Deng</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/kun-he/ class=align-middle>Kun He</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/acl/ class=align-middle>ACL</a><span class="badge badge-secondary align-middle ml-2">8</span></li><li class=list-group-item><a href=/venues/emnlp/ class=align-middle>EMNLP</a><span class="badge badge-secondary align-middle ml-2">7</span></li><li class=list-group-item><a href=/venues/coling/ class=align-middle>COLING</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/conll/ class=align-middle>CoNLL</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/ccl/ class=align-middle>CCL</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-venues aria-expanded=false aria-controls=more-venues>show all...</li><div class="collapse border-top" id=more-venues><li class=list-group-item><a href=/venues/mrqa/ class=align-middle>MRQA</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/ijcnlp/ class=align-middle>IJCNLP</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/naacl/ class=align-middle>NAACL</a><span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>