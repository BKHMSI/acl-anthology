<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Wolfgang Minker - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Wolfgang</span> <span class=font-weight-bold>Minker</span></h2><hr><div class=row><div class=col-lg-9><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.ranlp-1.96.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--ranlp-1--96 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.ranlp-1.96 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.ranlp-1.96/>Naturalness Evaluation of Natural Language Generation in Task-oriented Dialogues Using BERT<span class=acl-fixed-case>BERT</span></a></strong><br><a href=/people/y/ye-liu/>Ye Liu</a>
|
<a href=/people/w/wolfgang-maier/>Wolfgang Maier</a>
|
<a href=/people/w/wolfgang-minker/>Wolfgang Minker</a>
|
<a href=/people/s/stefan-ultes/>Stefan Ultes</a><br><a href=/volumes/2021.ranlp-1/ class=text-muted>Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2021)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--ranlp-1--96><div class="card-body p-3 small">This paper presents an automatic method to evaluate the naturalness of <a href=https://en.wikipedia.org/wiki/Natural-language_generation>natural language generation</a> in <a href=https://en.wikipedia.org/wiki/Dialogue>dialogue systems</a>. While this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> was previously rendered through expensive and time-consuming human labor, we present this novel task of automatic naturalness evaluation of generated language. By fine-tuning the BERT model, our proposed naturalness evaluation method shows robust results and outperforms the baselines : support vector machines, bi-directional LSTMs, and BLEURT. In addition, the training speed and evaluation performance of naturalness model are improved by <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a> from quality and informativeness linguistic knowledge.</div></div><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.54.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--54 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.54 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.54/>A Comparison of Explicit and Implicit Proactive Dialogue Strategies for Conversational Recommendation</a></strong><br><a href=/people/m/matthias-kraus/>Matthias Kraus</a>
|
<a href=/people/f/fabian-fischbach/>Fabian Fischbach</a>
|
<a href=/people/p/pascal-jansen/>Pascal Jansen</a>
|
<a href=/people/w/wolfgang-minker/>Wolfgang Minker</a><br><a href=/volumes/2020.lrec-1/ class=text-muted>Proceedings of the 12th Language Resources and Evaluation Conference</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--54><div class="card-body p-3 small">Recommendation systems aim at facilitating <a href=https://en.wikipedia.org/wiki/Information_retrieval>information retrieval</a> for users by taking into account their preferences. Based on previous user behaviour, such a <a href=https://en.wikipedia.org/wiki/System>system</a> suggests items or provides information that a user might like or find useful. Nonetheless, how to provide suggestions is still an open question. Depending on the way a recommendation is communicated influences the user&#8217;s perception of the system. This paper presents an empirical study on the effects of proactive dialogue strategies on <a href=https://en.wikipedia.org/wiki/Acceptance>user acceptance</a>. Therefore, an explicit <a href=https://en.wikipedia.org/wiki/Strategy>strategy</a> based on user preferences provided directly by the user, and an implicit proactive strategy, using autonomously gathered information, are compared. The results show that proactive dialogue systems significantly affect the perception of human-computer interaction. Although no significant differences are found between implicit and explicit strategies, <a href=https://en.wikipedia.org/wiki/Proactivity>proactivity</a> significantly influences the <a href=https://en.wikipedia.org/wiki/User_experience>user experience</a> compared to reactive system behaviour. The study contributes new insights to the human-agent interaction and the voice user interface design. Furthermore, we discover interesting tendencies that motivate futurework.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.61.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--61 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.61 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.61/>How Users React to Proactive Voice Assistant Behavior While Driving</a></strong><br><a href=/people/m/maria-schmidt/>Maria Schmidt</a>
|
<a href=/people/w/wolfgang-minker/>Wolfgang Minker</a>
|
<a href=/people/s/steffen-werner/>Steffen Werner</a><br><a href=/volumes/2020.lrec-1/ class=text-muted>Proceedings of the 12th Language Resources and Evaluation Conference</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--61><div class="card-body p-3 small">Nowadays Personal Assistants (PAs) are available in multiple environments and become increasingly popular to use via <a href=https://en.wikipedia.org/wiki/Human_voice>voice</a>. Therefore, we aim to provide proactive PA suggestions to car drivers via <a href=https://en.wikipedia.org/wiki/Speech>speech</a>. These suggestions should be neither obtrusive nor increase the drivers&#8217; cognitive load, while enhancing <a href=https://en.wikipedia.org/wiki/User_experience>user experience</a>. To assess these factors, we conducted a <a href=https://en.wikipedia.org/wiki/Usability_study>usability study</a> in which 42 participants perceive proactive voice output in a Wizard-of-Oz study in a <a href=https://en.wikipedia.org/wiki/Driving_simulator>driving simulator</a>. Traffic density was varied during a <a href=https://en.wikipedia.org/wiki/Driving>highway drive</a> and it included six in-car-specific use cases. The latter were presented by a proactive voice assistant and in a non-proactive control condition. We assessed the users&#8217; subjective cognitive load and their satisfaction in different questionnaires during the interaction with both PA variants. Furthermore, we analyze the user reactions : both regarding their content and the elapsed response times to PA actions. The results show that proactive assistant behavior is rated similarly positive as non-proactive behavior. Furthermore, the participants agreed to 73.8 % of proactive suggestions. In line with previous research, driving-relevant use cases receive the best ratings, here we reach 82.5 % acceptance. Finally, the users reacted significantly faster to proactive PA actions, which we interpret as less <a href=https://en.wikipedia.org/wiki/Cognitive_load>cognitive load</a> compared to non-proactive behavior.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.65.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--65 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.65 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.65/>Evaluation of Argument Search Approaches in the Context of Argumentative Dialogue Systems</a></strong><br><a href=/people/n/niklas-rach/>Niklas Rach</a>
|
<a href=/people/y/yuki-matsuda/>Yuki Matsuda</a>
|
<a href=/people/j/johannes-daxenberger/>Johannes Daxenberger</a>
|
<a href=/people/s/stefan-ultes/>Stefan Ultes</a>
|
<a href=/people/k/keiichi-yasumoto/>Keiichi Yasumoto</a>
|
<a href=/people/w/wolfgang-minker/>Wolfgang Minker</a><br><a href=/volumes/2020.lrec-1/ class=text-muted>Proceedings of the 12th Language Resources and Evaluation Conference</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--65><div class="card-body p-3 small">We present an approach to evaluate argument search techniques in view of their use in argumentative dialogue systems by assessing quality aspects of the retrieved arguments. To this end, we introduce a dialogue system that presents arguments by means of a virtual avatar and synthetic speech to users and allows them to rate the presented content in four different categories (Interesting, Convincing, Comprehensible, Relation). The approach is applied in a <a href=https://en.wikipedia.org/wiki/User_study>user study</a> in order to compare two state of the art argument search engines to each other and with a system based on traditional <a href=https://en.wikipedia.org/wiki/Web_search_engine>web search</a>. The results show a significant advantage of the two <a href=https://en.wikipedia.org/wiki/Web_search_engine>search engines</a> over the <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline</a>. Moreover, the two <a href=https://en.wikipedia.org/wiki/Web_search_engine>search engines</a> show significant advantages over each other in different categories, thereby reflecting strengths and weaknesses of the different underlying techniques.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.68.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--68 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.68 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.68/>Estimating User Communication Styles for Spoken Dialogue Systems</a></strong><br><a href=/people/j/juliana-miehle/>Juliana Miehle</a>
|
<a href=/people/i/isabel-feustel/>Isabel Feustel</a>
|
<a href=/people/j/julia-hornauer/>Julia Hornauer</a>
|
<a href=/people/w/wolfgang-minker/>Wolfgang Minker</a>
|
<a href=/people/s/stefan-ultes/>Stefan Ultes</a><br><a href=/volumes/2020.lrec-1/ class=text-muted>Proceedings of the 12th Language Resources and Evaluation Conference</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--68><div class="card-body p-3 small">We present a neural network approach to estimate the communication style of spoken interaction, namely the stylistic variations elaborateness and directness, and investigate which type of input <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> to the <a href=https://en.wikipedia.org/wiki/Estimator>estimator</a> are necessary to achive good performance. First, we describe our annotated corpus of recordings in the health care domain and analyse the corpus statistics in terms of <a href=https://en.wikipedia.org/wiki/Consensus_decision-making>agreement</a>, <a href=https://en.wikipedia.org/wiki/Correlation_and_dependence>correlation</a> and <a href=https://en.wikipedia.org/wiki/Reliability_(statistics)>reliability</a> of the ratings. We use this <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> to estimate the <a href=https://en.wikipedia.org/wiki/Elaboration>elaborateness</a> and the directness of each utterance. We test different feature sets consisting of dialogue act features, grammatical features and linguistic features as input for our <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifier</a> and perform classification in two and three classes. Our <a href=https://en.wikipedia.org/wiki/Classifier_(linguistics)>classifiers</a> use only <a href=https://en.wikipedia.org/wiki/Feature_(linguistics)>features</a> that can be automatically derived during an ongoing interaction in any spoken dialogue system without any prior annotation. Our results show that the elaborateness can be classified by only using the dialogue act and the amount of words contained in the corresponding utterance. The directness is a more difficult <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification task</a> and additional linguistic features in form of <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> improve the <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a> results. Afterwards, we run a comparison with a <a href=https://en.wikipedia.org/wiki/Support_vector_machine>support vector machine</a> and a <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent neural network classifier</a>.</div></div><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5933.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5933 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5933 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5933/>Cross-Corpus Data Augmentation for Acoustic Addressee Detection</a></strong><br><a href=/people/o/oleg-akhtiamov/>Oleg Akhtiamov</a>
|
<a href=/people/i/ingo-siegert/>Ingo Siegert</a>
|
<a href=/people/a/alexey-karpov/>Alexey Karpov</a>
|
<a href=/people/w/wolfgang-minker/>Wolfgang Minker</a><br><a href=/volumes/W19-59/ class=text-muted>Proceedings of the 20th Annual SIGdial Meeting on Discourse and Dialogue</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5933><div class="card-body p-3 small">Acoustic addressee detection (AD) is a modern paralinguistic and dialogue challenge that especially arises in <a href=https://en.wikipedia.org/wiki/Voice_assistant>voice assistants</a>. In the present study, we distinguish addressees in two settings (a conversation between several people and a spoken dialogue system, and a conversation between several adults and a child) and introduce the first competitive baseline (unweighted average recall equals 0.891) for the Voice Assistant Conversation Corpus that models the first setting. We jointly solve both classification problems, using three models : a linear support vector machine dealing with acoustic functionals and two <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a> utilising raw waveforms alongside with acoustic low-level descriptors. We investigate how different corpora influence each other, applying the mixup approach to <a href=https://en.wikipedia.org/wiki/Data_augmentation>data augmentation</a>. We also study the influence of various acoustic context lengths on <a href=https://en.wikipedia.org/wiki/Anno_Domini>AD</a>. Two-second speech fragments turn out to be sufficient for reliable <a href=https://en.wikipedia.org/wiki/Anno_Domini>AD</a>. Mixup is shown to be beneficial for merging acoustic data (extracted features but not raw waveforms) from different domains that allows us to reach a higher classification performance on human-machine AD and also for training a multipurpose neural network that is capable of solving both human-machine and adult-child AD problems.</div></div><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5520.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5520 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5520 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5520/>Interaction Quality Estimation Using Long Short-Term Memories</a></strong><br><a href=/people/n/niklas-rach/>Niklas Rach</a>
|
<a href=/people/w/wolfgang-minker/>Wolfgang Minker</a>
|
<a href=/people/s/stefan-ultes/>Stefan Ultes</a><br><a href=/volumes/W17-55/ class=text-muted>Proceedings of the 18th Annual SIGdial Meeting on Discourse and Dialogue</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5520><div class="card-body p-3 small">For estimating the Interaction Quality (IQ) in Spoken Dialogue Systems (SDS), the dialogue history is of significant importance. Previous works included this information manually in the form of precomputed temporal features into the <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification process</a>. Here, we employ a deep learning architecture based on Long Short-Term Memories (LSTM) to extract this information automatically from the data, thus estimating <a href=https://en.wikipedia.org/wiki/Intelligence_quotient>IQ</a> solely by using current exchange features. We show that it is thereby possible to achieve competitive results as in a scenario where manually optimized temporal features have been included.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Wolfgang+Minker" title="Search for 'Wolfgang Minker' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/s/stefan-ultes/ class=align-middle>Stefan Ultes</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/people/n/niklas-rach/ class=align-middle>Niklas Rach</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/y/ye-liu/ class=align-middle>Ye Liu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/w/wolfgang-maier/ class=align-middle>Wolfgang Maier</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/o/oleg-akhtiamov/ class=align-middle>Oleg Akhtiamov</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/i/ingo-siegert/ class=align-middle>Ingo Siegert</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/alexey-karpov/ class=align-middle>Alexey Karpov</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/matthias-kraus/ class=align-middle>Matthias Kraus</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/f/fabian-fischbach/ class=align-middle>Fabian Fischbach</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/p/pascal-jansen/ class=align-middle>Pascal Jansen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/maria-schmidt/ class=align-middle>Maria Schmidt</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/steffen-werner/ class=align-middle>Steffen Werner</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yuki-matsuda/ class=align-middle>Yuki Matsuda</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/johannes-daxenberger/ class=align-middle>Johannes Daxenberger</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/keiichi-yasumoto/ class=align-middle>Keiichi Yasumoto</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/juliana-miehle/ class=align-middle>Juliana Miehle</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/i/isabel-feustel/ class=align-middle>Isabel Feustel</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/julia-hornauer/ class=align-middle>Julia Hornauer</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/lrec/ class=align-middle>LREC</a><span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/venues/ws/ class=align-middle>WS</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/ranlp/ class=align-middle>RANLP</a><span class="badge badge-secondary align-middle ml-2">1</span></li></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>