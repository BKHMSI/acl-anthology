<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Wilker Aziz - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Wilker</span> <span class=font-weight-bold>Aziz</span></h2><hr><div class=row><div class=col-lg-9><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.262.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--262 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.262 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.emnlp-main.262.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938648 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.262" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.262/>How do Decisions Emerge across Layers in Neural Models? Interpretation with Differentiable Masking</a></strong><br><a href=/people/n/nicola-de-cao/>Nicola De Cao</a>
|
<a href=/people/m/michael-sejr-schlichtkrull/>Michael Sejr Schlichtkrull</a>
|
<a href=/people/w/wilker-aziz/>Wilker Aziz</a>
|
<a href=/people/i/ivan-titov/>Ivan Titov</a><br><a href=/volumes/2020.emnlp-main/ class=text-muted>Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--262><div class="card-body p-3 small">Attribution methods assess the contribution of inputs to the <a href=https://en.wikipedia.org/wiki/Prediction>model prediction</a>. One way to do so is erasure : a subset of inputs is considered irrelevant if it can be removed without affecting the prediction. Though conceptually simple, erasure&#8217;s objective is intractable and approximate search remains expensive with modern deep NLP models. Erasure is also susceptible to the <a href=https://en.wikipedia.org/wiki/Hindsight_bias>hindsight bias</a> : the fact that an input can be dropped does not mean that the model &#8216;knows&#8217; it can be dropped. The resulting <a href=https://en.wikipedia.org/wiki/Pruning>pruning</a> is over-aggressive and does not reflect how the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> arrives at the prediction. To deal with these challenges, we introduce Differentiable Masking. DiffMask learns to mask-out subsets of the input while maintaining differentiability. The decision to include or disregard an input token is made with a simple <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> based on intermediate hidden layers of the analyzed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>. First, this makes the approach efficient because we predict rather than <a href=https://en.wikipedia.org/wiki/Search_algorithm>search</a>. Second, as with probing classifiers, this reveals what the network &#8216;knows&#8217; at the corresponding layers. This lets us not only plot attribution heatmaps but also analyze how decisions are formed across <a href=https://en.wikipedia.org/wiki/Network_layer>network layers</a>. We use DiffMask to study BERT models on sentiment classification and <a href=https://en.wikipedia.org/wiki/Question_answering>question answering</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.398.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--398 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.398 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.398/>Is MAP Decoding All You Need? The Inadequacy of the Mode in Neural Machine Translation<span class=acl-fixed-case>MAP</span> Decoding All You Need? The Inadequacy of the Mode in Neural Machine Translation</a></strong><br><a href=/people/b/bryan-eikema/>Bryan Eikema</a>
|
<a href=/people/w/wilker-aziz/>Wilker Aziz</a><br><a href=/volumes/2020.coling-main/ class=text-muted>Proceedings of the 28th International Conference on Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--398><div class="card-body p-3 small">Recent studies have revealed a number of pathologies of neural machine translation (NMT) systems. Hypotheses explaining these mostly suggest there is something fundamentally wrong with NMT as a <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> or its training algorithm, <a href=https://en.wikipedia.org/wiki/Maximum_likelihood_estimation>maximum likelihood estimation (MLE)</a>. Most of this evidence was gathered using maximum a posteriori (MAP) decoding, a <a href=https://en.wikipedia.org/wiki/Decision_rule>decision rule</a> aimed at identifying the highest-scoring translation, i.e. the <a href=https://en.wikipedia.org/wiki/Mode_(user_interface)>mode</a>. We argue that the evidence corroborates the inadequacy of MAP decoding more than casts doubt on the model and its training algorithm. In this work, we show that translation distributions do reproduce various statistics of the data well, but that <a href=https://en.wikipedia.org/wiki/Beam_search>beam search</a> strays from such statistics. We show that some of the known pathologies and biases of NMT are due to MAP decoding and not to NMT&#8217;s statistical assumptions nor MLE. In particular, we show that the most likely <a href=https://en.wikipedia.org/wiki/Translation_(geometry)>translations</a> under the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> accumulate so little <a href=https://en.wikipedia.org/wiki/Probability_mass_function>probability mass</a> that the mode can be considered essentially arbitrary. We therefore advocate for the use of <a href=https://en.wikipedia.org/wiki/Decision_rule>decision rules</a> that take into account the translation distribution holistically. We show that an approximation to minimum Bayes risk decoding gives competitive results confirming that NMT models do capture important aspects of translation well in expectation.</div></div><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1240.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1240 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1240 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1240" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N19-1240/>Question Answering by Reasoning Across Documents with Graph Convolutional Networks</a></strong><br><a href=/people/n/nicola-de-cao/>Nicola De Cao</a>
|
<a href=/people/w/wilker-aziz/>Wilker Aziz</a>
|
<a href=/people/i/ivan-titov/>Ivan Titov</a><br><a href=/volumes/N19-1/ class=text-muted>Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1240><div class="card-body p-3 small">Most research in <a href=https://en.wikipedia.org/wiki/Reading_comprehension>reading comprehension</a> has focused on answering questions based on individual documents or even single paragraphs. We introduce a neural model which integrates and reasons relying on information spread within documents and across multiple documents. We frame it as an inference problem on a <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph</a>. Mentions of entities are <a href=https://en.wikipedia.org/wiki/Vertex_(graph_theory)>nodes</a> of this <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph</a> while <a href=https://en.wikipedia.org/wiki/Glossary_of_graph_theory_terms>edges</a> encode relations between different <a href=https://en.wikipedia.org/wiki/Note_(typography)>mentions</a> (e.g., within- and cross-document co-reference). Graph convolutional networks (GCNs) are applied to these <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graphs</a> and trained to perform multi-step reasoning. Our Entity-GCN method is scalable and compact, and it achieves state-of-the-art results on a multi-document question answering dataset, WikiHop (Welbl et al., 2018).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1642.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1642 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1642 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1642" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-1642/>Latent Variable Model for Multi-modal Translation</a></strong><br><a href=/people/i/iacer-calixto/>Iacer Calixto</a>
|
<a href=/people/m/miguel-rios/>Miguel Rios</a>
|
<a href=/people/w/wilker-aziz/>Wilker Aziz</a><br><a href=/volumes/P19-1/ class=text-muted>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1642><div class="card-body p-3 small">In this work, we propose to model the interaction between visual and textual features for multi-modal neural machine translation (MMT) through a <a href=https://en.wikipedia.org/wiki/Latent_variable_model>latent variable model</a>. This <a href=https://en.wikipedia.org/wiki/Latent_variable>latent variable</a> can be seen as a multi-modal stochastic embedding of an image and its description in a <a href=https://en.wikipedia.org/wiki/Foreign_language>foreign language</a>. It is used in a target-language decoder and also to predict <a href=https://en.wikipedia.org/wiki/Feature_(computer_vision)>image features</a>. Importantly, our model formulation utilises visual and textual inputs during training but does not require that images be available at test time. We show that our latent variable MMT formulation improves considerably over strong baselines, including a multi-task learning approach (Elliott and Kadar, 2017) and a conditional variational auto-encoder approach (Toyama et al., 2016). Finally, we show improvements due to (i) predicting image features in addition to only conditioning on them, (ii) imposing a <a href=https://en.wikipedia.org/wiki/Constraint_(mathematics)>constraint</a> on the KL term to promote models with non-negligible mutual information between inputs and <a href=https://en.wikipedia.org/wiki/Latent_variable>latent variable</a>, and (iii) by training on additional target-language image descriptions (i.e. synthetic data).</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1092.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1092 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1092 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://vimeo.com/277669962 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N18-1092" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/N18-1092/>Deep Generative Model for Joint Alignment and Word Representation</a></strong><br><a href=/people/m/miguel-rios/>Miguel Rios</a>
|
<a href=/people/w/wilker-aziz/>Wilker Aziz</a>
|
<a href=/people/k/khalil-simaan/>Khalil Sima’an</a><br><a href=/volumes/N18-1/ class=text-muted>Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1092><div class="card-body p-3 small">This work exploits translation data as a source of semantically relevant learning signal for models of word representation. In particular, we exploit equivalence through translation as a form of distributional context and jointly learn how to embed and align with a deep generative model. Our EmbedAlign model embeds words in their complete observed context and learns by marginalisation of latent lexical alignments. Besides, it embeds <a href=https://en.wikipedia.org/wiki/Word_(group_theory)>words</a> as posterior probability densities, rather than <a href=https://en.wikipedia.org/wiki/Point_estimation>point estimates</a>, which allows us to compare words in context using a measure of overlap between distributions (e.g. KL divergence). We investigate our model&#8217;s performance on a range of lexical semantics tasks achieving competitive results on several standard benchmarks including natural language inference, <a href=https://en.wikipedia.org/wiki/Paraphrase>paraphrasing</a>, and text similarity.</div></div><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1209.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1209 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1209 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1209/>Graph Convolutional Encoders for Syntax-aware Neural Machine Translation</a></strong><br><a href=/people/j/jasmijn-bastings/>Jasmijn Bastings</a>
|
<a href=/people/i/ivan-titov/>Ivan Titov</a>
|
<a href=/people/w/wilker-aziz/>Wilker Aziz</a>
|
<a href=/people/d/diego-marcheggiani/>Diego Marcheggiani</a>
|
<a href=/people/k/khalil-simaan/>Khalil Sima’an</a><br><a href=/volumes/D17-1/ class=text-muted>Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1209><div class="card-body p-3 small">We present a simple and effective approach to incorporating syntactic structure into neural attention-based encoder-decoder models for <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>. We rely on graph-convolutional networks (GCNs), a recent class of <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a> developed for modeling <a href=https://en.wikipedia.org/wiki/Graph_(abstract_data_type)>graph-structured data</a>. Our GCNs use predicted syntactic dependency trees of source sentences to produce representations of words (i.e. hidden states of the encoder) that are sensitive to their syntactic neighborhoods. GCNs take <a href=https://en.wikipedia.org/wiki/Word_processor_(electronic_device)>word representations</a> as input and produce <a href=https://en.wikipedia.org/wiki/Word_processor_(electronic_device)>word representations</a> as output, so they can easily be incorporated as layers into standard <a href=https://en.wikipedia.org/wiki/Encoder>encoders</a> (e.g., on top of bidirectional RNNs or convolutional neural networks). We evaluate their effectiveness with English-German and English-Czech translation experiments for different types of encoders and observe substantial improvements over their syntax-agnostic versions in all the considered setups.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Wilker+Aziz" title="Search for 'Wilker Aziz' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/i/ivan-titov/ class=align-middle>Ivan Titov</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/n/nicola-de-cao/ class=align-middle>Nicola De Cao</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/k/khalil-simaan/ class=align-middle>Khalil Sima’an</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/m/miguel-rios/ class=align-middle>Miguel Rios</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/m/michael-sejr-schlichtkrull/ class=align-middle>Michael Sejr Schlichtkrull</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/j/jasmijn-bastings/ class=align-middle>Jasmijn Bastings</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/diego-marcheggiani/ class=align-middle>Diego Marcheggiani</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/b/bryan-eikema/ class=align-middle>Bryan Eikema</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/i/iacer-calixto/ class=align-middle>Iacer Calixto</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/emnlp/ class=align-middle>EMNLP</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/naacl/ class=align-middle>NAACL</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/coling/ class=align-middle>COLING</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/acl/ class=align-middle>ACL</a><span class="badge badge-secondary align-middle ml-2">1</span></li></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>