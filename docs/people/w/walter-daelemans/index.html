<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Walter Daelemans - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Walter</span> <span class=font-weight-bold>Daelemans</span></h2><hr><div class=row><div class=col-lg-9><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.mrqa-1.1.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--mrqa-1--1 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.mrqa-1.1 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.mrqa-1.1" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.mrqa-1.1/>MFAQ : a Multilingual FAQ Dataset<span class=acl-fixed-case>MFAQ</span>: a Multilingual <span class=acl-fixed-case>FAQ</span> Dataset</a></strong><br><a href=/people/m/maxime-de-bruyn/>Maxime De Bruyn</a>
|
<a href=/people/e/ehsan-lotfi/>Ehsan Lotfi</a>
|
<a href=/people/j/jeska-buhmann/>Jeska Buhmann</a>
|
<a href=/people/w/walter-daelemans/>Walter Daelemans</a><br><a href=/volumes/2021.mrqa-1/ class=text-muted>Proceedings of the 3rd Workshop on Machine Reading for Question Answering</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--mrqa-1--1><div class="card-body p-3 small">In this paper, we present the first multilingual FAQ dataset publicly available. We collected around 6 M FAQ pairs from the <a href=https://en.wikipedia.org/wiki/World_Wide_Web>web</a>, in 21 different languages. Although this is significantly larger than existing FAQ retrieval datasets, it comes with its own challenges : duplication of content and uneven distribution of topics. We adopt a similar setup as Dense Passage Retrieval (DPR) and test various bi-encoders on this dataset. Our experiments reveal that a multilingual model based on XLM-RoBERTa achieves the best results, except for <a href=https://en.wikipedia.org/wiki/English_language>English</a>. Lower resources languages seem to learn from one another as a multilingual model achieves a higher MRR than language-specific ones. Our qualitative analysis reveals the brittleness of the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> on simple word changes. We publicly release our <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>, <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a>, and training script.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.nlp4if-1.3.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--nlp4if-1--3 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.nlp4if-1.3 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.nlp4if-1.3/>Improving Cross-Domain Hate Speech Detection by Reducing the False Positive Rate</a></strong><br><a href=/people/i/ilia-markov/>Ilia Markov</a>
|
<a href=/people/w/walter-daelemans/>Walter Daelemans</a><br><a href=/volumes/2021.nlp4if-1/ class=text-muted>Proceedings of the Fourth Workshop on NLP for Internet Freedom: Censorship, Disinformation, and Propaganda</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--nlp4if-1--3><div class="card-body p-3 small">Hate speech detection is an actively growing field of research with a variety of recently proposed approaches that allowed to push the state-of-the-art results. One of the challenges of such automated approaches namely recent <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning models</a> is a risk of false positives (i.e., false accusations), which may lead to over-blocking or removal of harmless social media content in applications with little moderator intervention. We evaluate <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning models</a> both under in-domain and cross-domain hate speech detection conditions, and introduce an SVM approach that allows to significantly improve the state-of-the-art results when combined with the <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning models</a> through a simple majority-voting ensemble. The improvement is mainly due to a reduction of the <a href=https://en.wikipedia.org/wiki/False_positives_and_false_negatives>false positive rate</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.louhi-1.6.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--louhi-1--6 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.louhi-1.6 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.louhi-1.6" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.louhi-1.6/>Integrating Higher-Level Semantics into Robust Biomedical Name Representations</a></strong><br><a href=/people/p/pieter-fivez/>Pieter Fivez</a>
|
<a href=/people/s/simon-suster/>Simon Suster</a>
|
<a href=/people/w/walter-daelemans/>Walter Daelemans</a><br><a href=/volumes/2021.louhi-1/ class=text-muted>Proceedings of the 12th International Workshop on Health Text Mining and Information Analysis</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--louhi-1--6><div class="card-body p-3 small">Neural encoders of biomedical names are typically considered robust if representations can be effectively exploited for various downstream NLP tasks. To achieve this, encoders need to model domain-specific biomedical semantics while rivaling the universal applicability of pretrained self-supervised representations. Previous work on robust representations has focused on learning low-level distinctions between names of fine-grained biomedical concepts. These fine-grained concepts can also be clustered together to reflect higher-level, more general semantic distinctions, such as grouping the names nettle sting and tick-borne fever together under the description puncture wound of skin. It has not yet been empirically confirmed that training biomedical name encoders on fine-grained distinctions automatically leads to <a href=https://en.wikipedia.org/wiki/Top-down_and_bottom-up_design>bottom-up encoding</a> of such higher-level semantics. In this paper, we show that this bottom-up effect exists, but that it is still relatively limited. As a solution, we propose a scalable multi-task training regime for biomedical name encoders which can also learn robust representations using only higher-level semantic classes. These representations can generalise both bottom-up as well as top-down among various semantic hierarchies. Moreover, we show how they can be used out-of-the-box for improved unsupervised detection of hypernyms, while retaining robust performance on various semantic relatedness benchmarks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.nlp4convai-1.24.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--nlp4convai-1--24 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.nlp4convai-1.24 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.nlp4convai-1.24/>Teach Me What to Say and I Will Learn What to Pick : Unsupervised Knowledge Selection Through Response Generation with Pretrained Generative Models<span class=acl-fixed-case>I</span> Will Learn What to Pick: Unsupervised Knowledge Selection Through Response Generation with Pretrained Generative Models</a></strong><br><a href=/people/e/ehsan-lotfi/>Ehsan Lotfi</a>
|
<a href=/people/m/maxime-de-bruyn/>Maxime De Bruyn</a>
|
<a href=/people/j/jeska-buhmann/>Jeska Buhmann</a>
|
<a href=/people/w/walter-daelemans/>Walter Daelemans</a><br><a href=/volumes/2021.nlp4convai-1/ class=text-muted>Proceedings of the 3rd Workshop on Natural Language Processing for Conversational AI</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--nlp4convai-1--24><div class="card-body p-3 small">Knowledge Grounded Conversation Models are usually based on a selection / retrieval module and a generation module, trained separately or simultaneously, with or without having access to a &#8216;gold&#8217; knowledge option. With the introduction of large pre-trained generative models, the selection and generation part have become more and more entangled, shifting the focus towards enhancing knowledge incorporation (from multiple sources) instead of trying to pick the best knowledge option. These approaches however depend on knowledge labels and/or a separate dense retriever for their best performance. In this work we study the unsupervised selection abilities of pre-trained generative models (e.g. BART) and show that by adding a score-and-aggregate module between <a href=https://en.wikipedia.org/wiki/Encoder>encoder</a> and <a href=https://en.wikipedia.org/wiki/Codec>decoder</a>, they are capable of learning to pick the proper knowledge through minimising the language modelling loss (i.e. without having access to knowledge labels). Trained as such, our model-K-Mine-shows competitive selection and generation performance against models that benefit from knowledge labels and/or separate dense retriever.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.bionlp-1.3.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--bionlp-1--3 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.bionlp-1.3 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.bionlp-1.3" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.bionlp-1.3/>Scalable Few-Shot Learning of Robust Biomedical Name Representations</a></strong><br><a href=/people/p/pieter-fivez/>Pieter Fivez</a>
|
<a href=/people/s/simon-suster/>Simon Suster</a>
|
<a href=/people/w/walter-daelemans/>Walter Daelemans</a><br><a href=/volumes/2021.bionlp-1/ class=text-muted>Proceedings of the 20th Workshop on Biomedical Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--bionlp-1--3><div class="card-body p-3 small">Recent research on robust representations of biomedical names has focused on modeling large amounts of fine-grained conceptual distinctions using complex neural encoders. In this paper, we explore the opposite <a href=https://en.wikipedia.org/wiki/Paradigm>paradigm</a> : training a simple encoder architecture using only small sets of <a href=https://en.wikipedia.org/wiki/Name>names</a> sampled from high-level biomedical concepts. Our encoder post-processes pretrained representations of biomedical names, and is effective for various types of input representations, both domain-specific or unsupervised. We validate our proposed few-shot learning approach on multiple biomedical relatedness benchmarks, and show that it allows for continual learning, where we accumulate information from various conceptual hierarchies to consistently improve encoder performance. Given these findings, we propose our approach as a low-cost alternative for exploring the impact of conceptual distinctions on robust biomedical name representations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wassa-1.16.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wassa-1--16 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wassa-1.16 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wassa-1.16/>Exploring Stylometric and Emotion-Based Features for Multilingual Cross-Domain Hate Speech Detection</a></strong><br><a href=/people/i/ilia-markov/>Ilia Markov</a>
|
<a href=/people/n/nikola-ljubesic/>Nikola Ljubešić</a>
|
<a href=/people/d/darja-fiser/>Darja Fišer</a>
|
<a href=/people/w/walter-daelemans/>Walter Daelemans</a><br><a href=/volumes/2021.wassa-1/ class=text-muted>Proceedings of the Eleventh Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wassa-1--16><div class="card-body p-3 small">In this paper, we describe experiments designed to evaluate the impact of stylometric and emotion-based features on hate speech detection : the task of classifying textual content into hate or non-hate speech classes. Our experiments are conducted for three languages English, <a href=https://en.wikipedia.org/wiki/Slovene_language>Slovene</a>, and <a href=https://en.wikipedia.org/wiki/Dutch_language>Dutch</a> both in in-domain and cross-domain setups, and aim to investigate <a href=https://en.wikipedia.org/wiki/Hate_speech>hate speech</a> using features that model two linguistic phenomena : the writing style of hateful social media content operationalized as function word usage on the one hand, and emotion expression in hateful messages on the other hand. The results of experiments with <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> that model different combinations of these phenomena support our hypothesis that stylometric and emotion-based features are robust indicators of <a href=https://en.wikipedia.org/wiki/Hate_speech>hate speech</a>. Their contribution remains persistent with respect to <a href=https://en.wikipedia.org/wiki/Variation_(linguistics)>domain and language variation</a>. We show that the combination of <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> that model the targeted phenomena outperforms words and character n-gram features under cross-domain conditions, and provides a significant boost to <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning models</a>, which currently obtain the best results, when combined with them in an <a href=https://en.wikipedia.org/wiki/Statistical_ensemble_(mathematical_physics)>ensemble</a>.</div></div><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.latechclfl-1.5.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--latechclfl-1--5 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.latechclfl-1.5 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.latechclfl-1.5/>Neural Machine Translation of Artwork Titles Using Iconclass Codes</a></strong><br><a href=/people/n/nikolay-banar/>Nikolay Banar</a>
|
<a href=/people/w/walter-daelemans/>Walter Daelemans</a>
|
<a href=/people/m/mike-kestemont/>Mike Kestemont</a><br><a href=/volumes/2020.latechclfl-1/ class=text-muted>Proceedings of the The 4th Joint SIGHUM Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--latechclfl-1--5><div class="card-body p-3 small">We investigate the use of <a href=https://en.wikipedia.org/wiki/Iconclass>Iconclass</a> in the context of <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a> for NL-EN artwork titles. Iconclass is a widely used <a href=https://en.wikipedia.org/wiki/Iconography>iconographic classification system</a> used in the <a href=https://en.wikipedia.org/wiki/Cultural_heritage>cultural heritage domain</a> to describe and retrieve subjects represented in the <a href=https://en.wikipedia.org/wiki/Visual_arts>visual arts</a>. The resource contains keywords and definitions to encode the presence of objects, people, events and ideas depicted in <a href=https://en.wikipedia.org/wiki/Work_of_art>artworks</a>, such as <a href=https://en.wikipedia.org/wiki/Painting>paintings</a>. We propose a simple concatenation approach that improves the quality of automatically generated title translations for <a href=https://en.wikipedia.org/wiki/Work_of_art>artworks</a>, by leveraging textual information extracted from <a href=https://en.wikipedia.org/wiki/Iconclass>Iconclass</a>. Our results demonstrate that a neural machine translation system is able to exploit this <a href=https://en.wikipedia.org/wiki/Metadata>metadata</a> to boost the translation performance of artwork titles. This <a href=https://en.wikipedia.org/wiki/Technology>technology</a> enables interesting applications of <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning</a> in resource-scarce domains in the <a href=https://en.wikipedia.org/wiki/Culture>cultural sector</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.407.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--407 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.407 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.407/>The European Language Technology Landscape in 2020 : Language-Centric and Human-Centric AI for Cross-Cultural Communication in Multilingual Europe<span class=acl-fixed-case>E</span>uropean Language Technology Landscape in 2020: Language-Centric and Human-Centric <span class=acl-fixed-case>AI</span> for Cross-Cultural Communication in Multilingual <span class=acl-fixed-case>E</span>urope</a></strong><br><a href=/people/g/georg-rehm/>Georg Rehm</a>
|
<a href=/people/k/katrin-marheinecke/>Katrin Marheinecke</a>
|
<a href=/people/s/stefanie-hegele/>Stefanie Hegele</a>
|
<a href=/people/s/stelios-piperidis/>Stelios Piperidis</a>
|
<a href=/people/k/kalina-bontcheva/>Kalina Bontcheva</a>
|
<a href=/people/j/jan-hajic/>Jan Hajič</a>
|
<a href=/people/k/khalid-choukri/>Khalid Choukri</a>
|
<a href=/people/a/andrejs-vasiljevs/>Andrejs Vasiļjevs</a>
|
<a href=/people/g/gerhard-backfried/>Gerhard Backfried</a>
|
<a href=/people/c/christoph-prinz/>Christoph Prinz</a>
|
<a href=/people/j/jose-manuel-gomez-perez/>José Manuel Gómez-Pérez</a>
|
<a href=/people/l/luc-meertens/>Luc Meertens</a>
|
<a href=/people/p/paul-lukowicz/>Paul Lukowicz</a>
|
<a href=/people/j/josef-van-genabith/>Josef van Genabith</a>
|
<a href=/people/a/andrea-losch/>Andrea Lösch</a>
|
<a href=/people/p/philipp-slusallek/>Philipp Slusallek</a>
|
<a href=/people/m/morten-irgens/>Morten Irgens</a>
|
<a href=/people/p/patrick-gatellier/>Patrick Gatellier</a>
|
<a href=/people/j/joachim-kohler/>Joachim Köhler</a>
|
<a href=/people/l/laure-le-bars/>Laure Le Bars</a>
|
<a href=/people/d/dimitra-anastasiou/>Dimitra Anastasiou</a>
|
<a href=/people/a/albina-auksoriute/>Albina Auksoriūtė</a>
|
<a href=/people/n/nuria-bel/>Núria Bel</a>
|
<a href=/people/a/antonio-branco/>António Branco</a>
|
<a href=/people/g/gerhard-budin/>Gerhard Budin</a>
|
<a href=/people/w/walter-daelemans/>Walter Daelemans</a>
|
<a href=/people/k/koenraad-de-smedt/>Koenraad De Smedt</a>
|
<a href=/people/r/radovan-garabik/>Radovan Garabík</a>
|
<a href=/people/m/maria-gavriilidou/>Maria Gavriilidou</a>
|
<a href=/people/d/dagmar-gromann/>Dagmar Gromann</a>
|
<a href=/people/s/svetla-koeva/>Svetla Koeva</a>
|
<a href=/people/s/simon-krek/>Simon Krek</a>
|
<a href=/people/c/cvetana-krstev/>Cvetana Krstev</a>
|
<a href=/people/k/krister-linden/>Krister Lindén</a>
|
<a href=/people/b/bernardo-magnini/>Bernardo Magnini</a>
|
<a href=/people/j/jan-odijk/>Jan Odijk</a>
|
<a href=/people/m/maciej-ogrodniczuk/>Maciej Ogrodniczuk</a>
|
<a href=/people/e/eirikur-rognvaldsson/>Eiríkur Rögnvaldsson</a>
|
<a href=/people/m/michael-rosner/>Mike Rosner</a>
|
<a href=/people/b/bolette-sandford-pedersen/>Bolette Pedersen</a>
|
<a href=/people/i/inguna-skadina/>Inguna Skadiņa</a>
|
<a href=/people/m/marko-tadic/>Marko Tadić</a>
|
<a href=/people/d/dan-tufis/>Dan Tufiș</a>
|
<a href=/people/t/tamas-varadi/>Tamás Váradi</a>
|
<a href=/people/k/kadri-vider/>Kadri Vider</a>
|
<a href=/people/a/andy-way/>Andy Way</a>
|
<a href=/people/f/francois-yvon/>François Yvon</a><br><a href=/volumes/2020.lrec-1/ class=text-muted>Proceedings of the 12th Language Resources and Evaluation Conference</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--407><div class="card-body p-3 small">Multilingualism is a cultural cornerstone of Europe and firmly anchored in the <a href=https://en.wikipedia.org/wiki/Treaties_of_the_European_Union>European treaties</a> including <a href=https://en.wikipedia.org/wiki/Linguistic_rights>full language equality</a>. However, <a href=https://en.wikipedia.org/wiki/Language_barrier>language barriers</a> impacting business, <a href=https://en.wikipedia.org/wiki/Cross-cultural_communication>cross-lingual and cross-cultural communication</a> are still omnipresent. Language Technologies (LTs) are a powerful means to break down these barriers. While the last decade has seen various initiatives that created a multitude of <a href=https://en.wikipedia.org/wiki/Software_development_process>approaches</a> and technologies tailored to Europe&#8217;s specific needs, there is still an immense level of fragmentation. At the same time, AI has become an increasingly important concept in the European Information and Communication Technology area. For a few years now, <a href=https://en.wikipedia.org/wiki/Artificial_intelligence>AI</a> including many opportunities, synergies but also misconceptions has been overshadowing every other topic. We present an overview of the European LT landscape, describing funding programmes, activities, actions and challenges in the different countries with regard to LT, including the current state of play in industry and the LT market. We present a brief overview of the main LT-related activities on the EU level in the last ten years and develop strategic guidance with regard to four key dimensions.</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3922.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3922 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3922 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3922/>Exploring Classifier Combinations for Language Variety Identification</a></strong><br><a href=/people/t/tim-kreutz/>Tim Kreutz</a>
|
<a href=/people/w/walter-daelemans/>Walter Daelemans</a><br><a href=/volumes/W18-39/ class=text-muted>Proceedings of the Fifth Workshop on NLP for Similar Languages, Varieties and Dialects (VarDial 2018)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3922><div class="card-body p-3 small">This paper describes CLiPS&#8217;s submissions for the Discriminating between Dutch and Flemish in Subtitles (DFS) shared task at VarDial 2018. We explore different ways to combine <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifiers</a> trained on different <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>feature groups</a>. Our best system uses two Linear SVM classifiers ; one trained on lexical features (word n-grams) and one trained on syntactic features (PoS n-grams). The final prediction for a document to be in Flemish Dutch or Netherlandic Dutch is made by the <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifier</a> that outputs the highest probability for one of the two labels. This confidence vote approach outperforms a meta-classifier on the <a href=https://en.wikipedia.org/wiki/Software_development_process>development data</a> and on the <a href=https://en.wikipedia.org/wiki/Test_data>test data</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6248.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6248 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6248 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6248/>Predicting Adolescents’ Educational Track from Chat Messages on Dutch Social Media<span class=acl-fixed-case>D</span>utch Social Media</a></strong><br><a href=/people/l/lisa-hilte/>Lisa Hilte</a>
|
<a href=/people/w/walter-daelemans/>Walter Daelemans</a>
|
<a href=/people/r/reinhild-vandekerckhove/>Reinhild Vandekerckhove</a><br><a href=/volumes/W18-62/ class=text-muted>Proceedings of the 9th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6248><div class="card-body p-3 small">We aim to predict Flemish adolescents&#8217; educational track based on their Dutch social media writing. We distinguish between the three main types of Belgian secondary education : General (theory-oriented), Vocational (practice-oriented), and Technical Secondary Education (hybrid). The best results are obtained with a Naive Bayes model, i.e. an F-score of 0.68 (std. dev. 0.05) in <a href=https://en.wikipedia.org/wiki/Cross-validation_(statistics)>10-fold cross-validation</a> experiments on the training data and an <a href=https://en.wikipedia.org/wiki/F-score>F-score</a> of 0.60 on unseen data. Many of the most informative features are character n-grams containing specific occurrences of chatspeak phenomena such as <a href=https://en.wikipedia.org/wiki/Emoticon>emoticons</a>. While the detection of the most theory- and practice-oriented educational tracks seems to be a relatively easy task, the hybrid Technical level appears to be much harder to capture based on online writing style, as expected.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1140.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1140 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1140 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N18-1140" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N18-1140/>CliCR : a Dataset of Clinical Case Reports for Machine Reading Comprehension<span class=acl-fixed-case>C</span>li<span class=acl-fixed-case>CR</span>: a Dataset of Clinical Case Reports for Machine Reading Comprehension</a></strong><br><a href=/people/s/simon-suster/>Simon Šuster</a>
|
<a href=/people/w/walter-daelemans/>Walter Daelemans</a><br><a href=/volumes/N18-1/ class=text-muted>Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1140><div class="card-body p-3 small">We present a new <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> for <a href=https://en.wikipedia.org/wiki/Machine_learning>machine comprehension</a> in the <a href=https://en.wikipedia.org/wiki/Medicine>medical domain</a>. Our <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> uses clinical case reports with around 100,000 gap-filling queries about these cases. We apply several baselines and state-of-the-art neural readers to the dataset, and observe a considerable gap in performance (20 % F1) between the best human and machine readers. We analyze the skills required for successful answering and show how <a href=https://en.wikipedia.org/wiki/Reading>reader</a> performance varies depending on the applicable skills. We find that inferences using <a href=https://en.wikipedia.org/wiki/Domain_knowledge>domain knowledge</a> and object tracking are the most frequently required skills, and that recognizing omitted information and spatio-temporal reasoning are the most difficult for the machines.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K18-1008.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K18-1008 data-toggle=collapse aria-expanded=false aria-controls=abstract-K18-1008 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=K18-1008" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/K18-1008/>From Strings to Other Things : Linking the Neighborhood and Transposition Effects in Word Reading</a></strong><br><a href=/people/s/stephan-tulkens/>Stéphan Tulkens</a>
|
<a href=/people/d/dominiek-sandra/>Dominiek Sandra</a>
|
<a href=/people/w/walter-daelemans/>Walter Daelemans</a><br><a href=/volumes/K18-1/ class=text-muted>Proceedings of the 22nd Conference on Computational Natural Language Learning</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K18-1008><div class="card-body p-3 small">We investigate the relation between the transposition and deletion effects in word reading, i.e., the finding that readers can successfully read SLAT as SALT, or WRK as WORK, and the <a href=https://en.wikipedia.org/wiki/Neighbourhood_effect>neighborhood effect</a>. In particular, we investigate whether lexical orthographic neighborhoods take into account transposition and <a href=https://en.wikipedia.org/wiki/Deletion_(linguistics)>deletion</a> in determining neighbors. If this is the case, it is more likely that the <a href=https://en.wikipedia.org/wiki/Neighbourhood_effect>neighborhood effect</a> takes place early during processing, and does not solely rely on similarity of internal representations. We introduce a new neighborhood measure, rd20, which can be used to quantify <a href=https://en.wikipedia.org/wiki/Neighbourhood_effect>neighborhood effects</a> over arbitrary feature spaces. We calculate the rd20 over large sets of words in three languages using various feature sets and show that feature sets that do not allow for transposition or <a href=https://en.wikipedia.org/wiki/Deletion_(linguistics)>deletion</a> explain more variance in Reaction Time (RT) measurements. We also show that the rd20 can be calculated using the hidden state representations of an Multi-Layer Perceptron, and show that these explain less variance than the raw features. We conclude that the <a href=https://en.wikipedia.org/wiki/Neighbourhood_effect>neighborhood effect</a> is unlikely to have a perceptual basis, but is more likely to be the result of items co-activating after recognition. All code is available at :<url>www.github.com/clips/conll2018</url>\n</div></div><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1610.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1610 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1610 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1610/>A Short Review of Ethical Challenges in Clinical Natural Language Processing</a></strong><br><a href=/people/s/simon-suster/>Simon Šuster</a>
|
<a href=/people/s/stephan-tulkens/>Stéphan Tulkens</a>
|
<a href=/people/w/walter-daelemans/>Walter Daelemans</a><br><a href=/volumes/W17-16/ class=text-muted>Proceedings of the First ACL Workshop on Ethics in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1610><div class="card-body p-3 small">Clinical NLP has an immense potential in contributing to how clinical practice will be revolutionized by the advent of large scale processing of clinical records. However, this potential has remained largely untapped due to slow progress primarily caused by strict data access policies for researchers. In this paper, we discuss the concern for <a href=https://en.wikipedia.org/wiki/Privacy>privacy</a> and the measures it entails. We also suggest sources of less sensitive data. Finally, we draw attention to biases that can compromise the validity of empirical research and lead to socially harmful applications.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2317.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2317 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2317 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-2317/>Unsupervised Context-Sensitive Spelling Correction of Clinical Free-Text with Word and Character N-Gram Embeddings</a></strong><br><a href=/people/p/pieter-fivez/>Pieter Fivez</a>
|
<a href=/people/s/simon-suster/>Simon Šuster</a>
|
<a href=/people/w/walter-daelemans/>Walter Daelemans</a><br><a href=/volumes/W17-23/ class=text-muted>BioNLP 2017</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2317><div class="card-body p-3 small">We present an unsupervised context-sensitive spelling correction method for clinical free-text that uses word and character n-gram embeddings. Our method generates misspelling replacement candidates and ranks them according to their semantic fit, by calculating a weighted cosine similarity between the vectorized representation of a candidate and the misspelling context. We greatly outperform two baseline off-the-shelf spelling correction tools on a manually annotated MIMIC-III test set, and counter the frequency bias of an optimized noisy channel model, showing that neural embeddings can be successfully exploited to include context-awareness in a spelling correction model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4407.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4407 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4407 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4407/>Simple Queries as Distant Labels for Predicting Gender on Twitter<span class=acl-fixed-case>T</span>witter</a></strong><br><a href=/people/c/chris-emmery/>Chris Emmery</a>
|
<a href=/people/g/grzegorz-chrupala/>Grzegorz Chrupała</a>
|
<a href=/people/w/walter-daelemans/>Walter Daelemans</a><br><a href=/volumes/W17-44/ class=text-muted>Proceedings of the 3rd Workshop on Noisy User-generated Text</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4407><div class="card-body p-3 small">The majority of research on extracting missing user attributes from <a href=https://en.wikipedia.org/wiki/User_profile>social media profiles</a> use costly hand-annotated labels for <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised learning</a>. Distantly supervised methods exist, although these generally rely on knowledge gathered using external sources. This paper demonstrates the effectiveness of gathering distant labels for self-reported gender on <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a> using simple queries. We confirm the reliability of this query heuristic by comparing with <a href=https://en.wikipedia.org/wiki/Annotation>manual annotation</a>. Moreover, using these labels for distant supervision, we demonstrate competitive model performance on the same data as <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> trained on manual annotations. As such, we offer a cheap, extensible, and fast alternative that can be employed beyond the task of <a href=https://en.wikipedia.org/wiki/Gender>gender classification</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4914.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4914 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4914 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4914/>Assessing the Stylistic Properties of Neurally Generated Text in Authorship Attribution</a></strong><br><a href=/people/e/enrique-manjavacas/>Enrique Manjavacas</a>
|
<a href=/people/j/jeroen-de-gussem/>Jeroen De Gussem</a>
|
<a href=/people/w/walter-daelemans/>Walter Daelemans</a>
|
<a href=/people/m/mike-kestemont/>Mike Kestemont</a><br><a href=/volumes/W17-49/ class=text-muted>Proceedings of the Workshop on Stylistic Variation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4914><div class="card-body p-3 small">Recent applications of neural language models have led to an increased interest in the <a href=https://en.wikipedia.org/wiki/Natural-language_generation>automatic generation of natural language</a>. However impressive, the evaluation of neurally generated text has so far remained rather informal and anecdotal. Here, we present an attempt at the systematic assessment of one aspect of the quality of neurally generated text. We focus on a specific aspect of neural language generation : its ability to reproduce authorial writing styles. Using established <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> for <a href=https://en.wikipedia.org/wiki/Attribution_(psychology)>authorship attribution</a>, we empirically assess the <a href=https://en.wikipedia.org/wiki/Style_(visual_arts)>stylistic qualities</a> of neurally generated text. In comparison to conventional <a href=https://en.wikipedia.org/wiki/Language_model>language models</a>, neural models generate fuzzier text, that is relatively harder to attribute correctly. Nevertheless, our results also suggest that neurally generated text offers more valuable perspectives for the augmentation of training data.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Walter+Daelemans" title="Search for 'Walter Daelemans' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/s/simon-suster/ class=align-middle>Simon Suster</a>
<span class="badge badge-secondary align-middle ml-2">5</span></li><li class=list-group-item><a href=/people/p/pieter-fivez/ class=align-middle>Pieter Fivez</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/m/maxime-de-bruyn/ class=align-middle>Maxime De Bruyn</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/e/ehsan-lotfi/ class=align-middle>Ehsan Lotfi</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/j/jeska-buhmann/ class=align-middle>Jeska Buhmann</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/i/ilia-markov/ class=align-middle>Ilia Markov</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/s/stephan-tulkens/ class=align-middle>Stéphan Tulkens</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/m/mike-kestemont/ class=align-middle>Mike Kestemont</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/c/chris-emmery/ class=align-middle>Chris Emmery</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/g/grzegorz-chrupala/ class=align-middle>Grzegorz Chrupała</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/e/enrique-manjavacas/ class=align-middle>Enrique Manjavacas</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jeroen-de-gussem/ class=align-middle>Jeroen De Gussem</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/n/nikolay-banar/ class=align-middle>Nikolay Banar</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/tim-kreutz/ class=align-middle>Tim Kreutz</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/lisa-hilte/ class=align-middle>Lisa Hilte</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/reinhild-vandekerckhove/ class=align-middle>Reinhild Vandekerckhove</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/g/georg-rehm/ class=align-middle>Georg Rehm</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/katrin-marheinecke/ class=align-middle>Katrin Marheinecke</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/stefanie-hegele/ class=align-middle>Stefanie Hegele</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/stelios-piperidis/ class=align-middle>Stelios Piperidis</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/kalina-bontcheva/ class=align-middle>Kalina Bontcheva</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jan-hajic/ class=align-middle>Jan Hajic</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/khalid-choukri/ class=align-middle>Khalid Choukri</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/andrejs-vasiljevs/ class=align-middle>Andrejs Vasiļjevs</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/g/gerhard-backfried/ class=align-middle>Gerhard Backfried</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/christoph-prinz/ class=align-middle>Christoph Prinz</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jose-manuel-gomez-perez/ class=align-middle>José Manuel Gómez-Pérez</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/luc-meertens/ class=align-middle>Luc Meertens</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/p/paul-lukowicz/ class=align-middle>Paul Lukowicz</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/josef-van-genabith/ class=align-middle>Josef van Genabith</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/andrea-losch/ class=align-middle>Andrea Lösch</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/p/philipp-slusallek/ class=align-middle>Philipp Slusallek</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/morten-irgens/ class=align-middle>Morten Irgens</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/p/patrick-gatellier/ class=align-middle>Patrick Gatellier</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/joachim-kohler/ class=align-middle>Joachim Köhler</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/laure-le-bars/ class=align-middle>Laure Le Bars</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/dimitra-anastasiou/ class=align-middle>Dimitra Anastasiou</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/albina-auksoriute/ class=align-middle>Albina Auksoriūtė</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/n/nuria-bel/ class=align-middle>Núria Bel</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/antonio-branco/ class=align-middle>António Branco</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/g/gerhard-budin/ class=align-middle>Gerhard Budin</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/koenraad-de-smedt/ class=align-middle>Koenraad De Smedt</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/radovan-garabik/ class=align-middle>Radovan Garabík</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/maria-gavriilidou/ class=align-middle>Maria Gavriilidou</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/dagmar-gromann/ class=align-middle>Dagmar Gromann</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/svetla-koeva/ class=align-middle>Svetla Koeva</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/simon-krek/ class=align-middle>Simon Krek</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/cvetana-krstev/ class=align-middle>Cvetana Krstev</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/krister-linden/ class=align-middle>Krister Lindén</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/b/bernardo-magnini/ class=align-middle>Bernardo Magnini</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jan-odijk/ class=align-middle>Jan Odijk</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/maciej-ogrodniczuk/ class=align-middle>Maciej Ogrodniczuk</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/e/eirikur-rognvaldsson/ class=align-middle>Eirikur Rögnvaldsson</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/michael-rosner/ class=align-middle>Michael Rosner</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/b/bolette-sandford-pedersen/ class=align-middle>Bolette Sandford Pedersen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/i/inguna-skadina/ class=align-middle>Inguna Skadiņa</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/marko-tadic/ class=align-middle>Marko Tadić</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/dan-tufis/ class=align-middle>Dan Tufiş</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/tamas-varadi/ class=align-middle>Tamás Váradi</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/kadri-vider/ class=align-middle>Kadri Vider</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/andy-way/ class=align-middle>Andy Way</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/f/francois-yvon/ class=align-middle>François Yvon</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/n/nikola-ljubesic/ class=align-middle>Nikola Ljubešić</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/darja-fiser/ class=align-middle>Darja Fišer</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/dominiek-sandra/ class=align-middle>Dominiek Sandra</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/ws/ class=align-middle>WS</a><span class="badge badge-secondary align-middle ml-2">6</span></li><li class=list-group-item><a href=/venues/mrqa/ class=align-middle>MRQA</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/nlp4if/ class=align-middle>NLP4IF</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/louhi/ class=align-middle>Louhi</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/nlp4convai/ class=align-middle>NLP4ConvAI</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-venues aria-expanded=false aria-controls=more-venues>show all...</li><div class="collapse border-top" id=more-venues><li class=list-group-item><a href=/venues/latechclfl/ class=align-middle>LaTeCHCLfL</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/bionlp/ class=align-middle>BioNLP</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/naacl/ class=align-middle>NAACL</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/lrec/ class=align-middle>LREC</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/wassa/ class=align-middle>WASSA</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/conll/ class=align-middle>CoNLL</a><span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>