<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Jie Hao - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Jie</span> <span class=font-weight-bold>Hao</span></h2><hr><div class=row><div class=col-lg-9><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.nlp4convai-1.17.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--nlp4convai-1--17 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.nlp4convai-1.17 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.nlp4convai-1.17/>Personalized Search-based Query Rewrite System for Conversational AI<span class=acl-fixed-case>AI</span></a></strong><br><a href=/people/e/eunah-cho/>Eunah Cho</a>
|
<a href=/people/z/ziyan-jiang/>Ziyan Jiang</a>
|
<a href=/people/j/jie-hao/>Jie Hao</a>
|
<a href=/people/z/zheng-chen/>Zheng Chen</a>
|
<a href=/people/s/saurabh-gupta/>Saurabh Gupta</a>
|
<a href=/people/x/xing-fan/>Xing Fan</a>
|
<a href=/people/c/chenlei-guo/>Chenlei Guo</a><br><a href=/volumes/2021.nlp4convai-1/ class=text-muted>Proceedings of the 3rd Workshop on Natural Language Processing for Conversational AI</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--nlp4convai-1--17><div class="card-body p-3 small">Query rewrite (QR) is an emerging component in conversational AI systems, reducing user defect. User defect is caused by various reasons, such as errors in the spoken dialogue system, users&#8217; slips of the tongue or their abridged language. Many of the user defects stem from personalized factors, such as user&#8217;s speech pattern, <a href=https://en.wikipedia.org/wiki/Dialect>dialect</a>, or preferences. In this work, we propose a personalized search-based QR framework, which focuses on automatic reduction of user defect. We build a personalized index for each user, which encompasses diverse affinity layers to reflect personal preferences for each user in the conversational AI. Our personalized QR system contains retrieval and ranking layers. Supported by user feedback based learning, training our <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> does not require hand-annotated data. Experiments on personalized test set showed that our personalized QR system is able to correct systematic and user errors by utilizing phonetic and semantic inputs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.143.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--143 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.143 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.143/>Contextual Rephrase Detection for Reducing Friction in Dialogue Systems</a></strong><br><a href=/people/z/zhuoyi-wang/>Zhuoyi Wang</a>
|
<a href=/people/s/saurabh-gupta/>Saurabh Gupta</a>
|
<a href=/people/j/jie-hao/>Jie Hao</a>
|
<a href=/people/x/xing-fan/>Xing Fan</a>
|
<a href=/people/d/dingcheng-li/>Dingcheng Li</a>
|
<a href=/people/a/alexander-hanbo-li/>Alexander Hanbo Li</a>
|
<a href=/people/c/chenlei-guo/>Chenlei Guo</a><br><a href=/volumes/2021.emnlp-main/ class=text-muted>Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--143><div class="card-body p-3 small">For voice assistants like <a href=https://en.wikipedia.org/wiki/Amazon_Alexa>Alexa</a>, <a href=https://en.wikipedia.org/wiki/Google_Assistant>Google Assistant</a>, and <a href=https://en.wikipedia.org/wiki/Siri>Siri</a>, correctly interpreting users&#8217; intentions is of utmost importance. However, users sometimes experience friction with these assistants, caused by errors from different system components or <a href=https://en.wikipedia.org/wiki/User_error>user errors</a> such as slips of the tongue. Users tend to rephrase their queries until they get a satisfactory response. Rephrase detection is used to identify the rephrases and has long been treated as a task with pairwise input, which does not fully utilize the contextual information (e.g. users&#8217; implicit feedback). To this end, we propose a contextual rephrase detection model ContReph to automatically identify rephrases from multi-turn dialogues. We showcase how to leverage the dialogue context and user-agent interaction signals, including the user&#8217;s implicit feedback and the time gap between different turns, which can help significantly outperform the pairwise rephrase detection models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.402.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--402 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.402 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.402/>RAST : Domain-Robust Dialogue Rewriting as Sequence Tagging<span class=acl-fixed-case>RAST</span>: Domain-Robust Dialogue Rewriting as Sequence Tagging</a></strong><br><a href=/people/j/jie-hao/>Jie Hao</a>
|
<a href=/people/l/linfeng-song/>Linfeng Song</a>
|
<a href=/people/l/liwei-wang/>Liwei Wang</a>
|
<a href=/people/k/kun-xu/>Kun Xu</a>
|
<a href=/people/z/zhaopeng-tu/>Zhaopeng Tu</a>
|
<a href=/people/d/dong-yu/>Dong Yu</a><br><a href=/volumes/2021.emnlp-main/ class=text-muted>Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--402><div class="card-body p-3 small">The task of dialogue rewriting aims to reconstruct the latest dialogue utterance by copying the missing content from the dialogue context. Until now, the existing <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> for this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> suffer from the robustness issue, i.e., performances drop dramatically when testing on a different dataset. We address this robustness issue by proposing a novel sequence-tagging-based model so that the <a href=https://en.wikipedia.org/wiki/Feasible_region>search space</a> is significantly reduced, yet the core of this task is still well covered. As a common issue of most tagging models for text generation, the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a>&#8217;s outputs may lack fluency. To alleviate this issue, we inject the loss signal from <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a> or GPT-2 under a REINFORCE framework. Experiments show huge improvements of our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> over the current state-of-the-art <a href=https://en.wikipedia.org/wiki/System>systems</a> when transferring to another <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wmt-1.25.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wmt-1--25 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wmt-1.25 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wmt-1.25/>The Mininglamp Machine Translation System for WMT21<span class=acl-fixed-case>WMT</span>21</a></strong><br><a href=/people/s/shiyu-zhao/>Shiyu Zhao</a>
|
<a href=/people/x/xiaopu-li/>Xiaopu Li</a>
|
<a href=/people/m/minghui-wu/>Minghui Wu</a>
|
<a href=/people/j/jie-hao/>Jie Hao</a><br><a href=/volumes/2021.wmt-1/ class=text-muted>Proceedings of the Sixth Conference on Machine Translation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wmt-1--25><div class="card-body p-3 small">This paper describes Mininglamp neural machine translation systems of the WMT2021 news translation tasks. We have participated in eight directions translation tasks for news text including <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a> to / from English, Hausa to / from English, <a href=https://en.wikipedia.org/wiki/German_language>German</a> to / from English and <a href=https://en.wikipedia.org/wiki/French_language>French</a> to / from German. Our fundamental system was based on Transformer architecture, with wider or smaller construction for different news translation tasks. We mainly utilized the method of <a href=https://en.wikipedia.org/wiki/Back-translation>back-translation</a>, knowledge distillation and <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a> to boost single model, while the <a href=https://en.wikipedia.org/wiki/Statistical_ensemble_(mathematical_physics)>ensemble</a> was used to combine single models. Our final submission has ranked first for the English to / from Hausa task.</div></div><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wmt-1.30.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wmt-1--30 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wmt-1.30 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939558 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.wmt-1.30/>OPPO’s Machine Translation Systems for WMT20<span class=acl-fixed-case>OPPO</span>’s Machine Translation Systems for <span class=acl-fixed-case>WMT</span>20</a></strong><br><a href=/people/t/tingxun-shi/>Tingxun Shi</a>
|
<a href=/people/s/shiyu-zhao/>Shiyu Zhao</a>
|
<a href=/people/x/xiaopu-li/>Xiaopu Li</a>
|
<a href=/people/x/xiaoxue-wang/>Xiaoxue Wang</a>
|
<a href=/people/q/qian-zhang/>Qian Zhang</a>
|
<a href=/people/d/di-ai/>Di Ai</a>
|
<a href=/people/d/dawei-dang/>Dawei Dang</a>
|
<a href=/people/x/xue-zhengshan/>Xue Zhengshan</a>
|
<a href=/people/j/jie-hao/>Jie Hao</a><br><a href=/volumes/2020.wmt-1/ class=text-muted>Proceedings of the Fifth Conference on Machine Translation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wmt-1--30><div class="card-body p-3 small">In this paper we demonstrate our (OPPO&#8217;s) machine translation systems for the WMT20 Shared Task on News Translation for all the 22 language pairs. We will give an overview of the common aspects across all the systems firstly, including two parts : the data preprocessing part will show how the data are preprocessed and filtered, and the system part will show our models architecture and the techniques we followed. Detailed information, such as training hyperparameters and the results generated by each technique will be depicted in the corresponding subsections. Our final submissions ranked top in 6 directions (English Czech, English Russian, French German and Tamil English), third in 2 directions (English German, English Japanese), and fourth in 2 directions (English Pashto and and English Tamil).<tex-math>\\leftrightarrow</tex-math> Czech, English <tex-math>\\leftrightarrow</tex-math> Russian, French <tex-math>\\rightarrow</tex-math> German and Tamil <tex-math>\\rightarrow</tex-math> English), third in 2 directions (English <tex-math>\\rightarrow</tex-math> German, English <tex-math>\\rightarrow</tex-math> Japanese), and fourth in 2 directions (English <tex-math>\\rightarrow</tex-math> Pashto and and English <tex-math>\\rightarrow</tex-math> Tamil).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.iwslt-1.13.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--iwslt-1--13 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.iwslt-1.13 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://slideslive.com/38929611 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.iwslt-1.13/>OPPO’s Machine Translation System for the IWSLT 2020 Open Domain Translation Task<span class=acl-fixed-case>OPPO</span>’s Machine Translation System for the <span class=acl-fixed-case>IWSLT</span> 2020 Open Domain Translation Task</a></strong><br><a href=/people/q/qian-zhang/>Qian Zhang</a>
|
<a href=/people/x/xiaopu-li/>Xiaopu Li</a>
|
<a href=/people/d/dawei-dang/>Dawei Dang</a>
|
<a href=/people/t/tingxun-shi/>Tingxun Shi</a>
|
<a href=/people/d/di-ai/>Di Ai</a>
|
<a href=/people/z/zhengshan-xue/>Zhengshan Xue</a>
|
<a href=/people/j/jie-hao/>Jie Hao</a><br><a href=/volumes/2020.iwslt-1/ class=text-muted>Proceedings of the 17th International Conference on Spoken Language Translation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--iwslt-1--13><div class="card-body p-3 small">In this paper, we demonstrate our <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation system</a> applied for the Chinese-Japanese bidirectional translation task (aka. open domain translation task) for the IWSLT 2020. Our model is based on Transformer (Vaswani et al., 2017), with the help of many popular, widely proved effective data preprocessing and augmentation methods. Experiments show that these <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> can improve the <a href=https://en.wikipedia.org/wiki/Baseline_(medicine)>baseline model</a> steadily and significantly.</div></div><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1135.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1135 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1135 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1135.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D19-1135/>Towards Better Modeling <a href=https://en.wikipedia.org/wiki/Hierarchical_structure>Hierarchical Structure</a> for Self-Attention with Ordered Neurons</a></strong><br><a href=/people/j/jie-hao/>Jie Hao</a>
|
<a href=/people/x/xing-wang/>Xing Wang</a>
|
<a href=/people/s/shuming-shi/>Shuming Shi</a>
|
<a href=/people/j/jinfeng-zhang/>Jinfeng Zhang</a>
|
<a href=/people/z/zhaopeng-tu/>Zhaopeng Tu</a><br><a href=/volumes/D19-1/ class=text-muted>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1135><div class="card-body p-3 small">Recent studies have shown that a hybrid of self-attention networks (SANs) and recurrent neural networks RNNs outperforms both individual architectures, while not much is known about why the hybrid models work. With the belief that modeling hierarchical structure is an essential complementary between SANs and RNNs, we propose to further enhance the strength of hybrid models with an advanced variant of RNNs Ordered Neurons LSTM (ON-LSTM), which introduces a syntax-oriented inductive bias to perform tree-like composition. Experimental results on the benchmark machine translation task show that the proposed approach outperforms both individual <a href=https://en.wikipedia.org/wiki/Computer_architecture>architectures</a> and a standard hybrid model. Further analyses on targeted linguistic evaluation and logical inference tasks demonstrate that the proposed approach indeed benefits from a better modeling of hierarchical structure.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Jie+Hao" title="Search for 'Jie Hao' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/x/xiaopu-li/ class=align-middle>Xiaopu Li</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/s/saurabh-gupta/ class=align-middle>Saurabh Gupta</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/x/xing-fan/ class=align-middle>Xing Fan</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/c/chenlei-guo/ class=align-middle>Chenlei Guo</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/z/zhaopeng-tu/ class=align-middle>Zhaopeng Tu</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/s/shiyu-zhao/ class=align-middle>Shiyu Zhao</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/t/tingxun-shi/ class=align-middle>Tingxun Shi</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/q/qian-zhang/ class=align-middle>Qian Zhang</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/d/di-ai/ class=align-middle>Di Ai</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/d/dawei-dang/ class=align-middle>Dawei Dang</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/e/eunah-cho/ class=align-middle>Eunah Cho</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/ziyan-jiang/ class=align-middle>Ziyan Jiang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/zheng-chen/ class=align-middle>Zheng Chen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/zhuoyi-wang/ class=align-middle>Zhuoyi Wang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/dingcheng-li/ class=align-middle>Dingcheng Li</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/alexander-hanbo-li/ class=align-middle>Alexander Hanbo Li</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/linfeng-song/ class=align-middle>Linfeng Song</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/liwei-wang/ class=align-middle>Liwei Wang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/kun-xu/ class=align-middle>Kun Xu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/dong-yu/ class=align-middle>Dong Yu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/x/xing-wang/ class=align-middle>Xing Wang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/shuming-shi/ class=align-middle>Shuming Shi</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jinfeng-zhang/ class=align-middle>Jinfeng Zhang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/minghui-wu/ class=align-middle>Minghui Wu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/x/xiaoxue-wang/ class=align-middle>Xiaoxue Wang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/x/xue-zhengshan/ class=align-middle>Xue Zhengshan</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/zhengshan-xue/ class=align-middle>Zhengshan Xue</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/emnlp/ class=align-middle>EMNLP</a><span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/venues/wmt/ class=align-middle>WMT</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/nlp4convai/ class=align-middle>NLP4ConvAI</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/iwslt/ class=align-middle>IWSLT</a><span class="badge badge-secondary align-middle ml-2">1</span></li></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>