<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Jimmy Lin - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Jimmy</span> <span class=font-weight-bold>Lin</span></h2><hr><div class=row><div class=col-lg-9><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.sustainlp-1.8.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--sustainlp-1--8 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.sustainlp-1.8 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.sustainlp-1.8/>Learning to Rank in the Age of <a href=https://en.wikipedia.org/wiki/The_Muppets>Muppets</a> : EffectivenessEfficiency Tradeoffs in Multi-Stage Ranking</a></strong><br><a href=/people/y/yue-zhang/>Yue Zhang</a>
|
<a href=/people/c/chengcheng-hu/>ChengCheng Hu</a>
|
<a href=/people/y/yuqi-liu/>Yuqi Liu</a>
|
<a href=/people/h/hui-fang/>Hui Fang</a>
|
<a href=/people/j/jimmy-lin/>Jimmy Lin</a><br><a href=/volumes/2021.sustainlp-1/ class=text-muted>Proceedings of the Second Workshop on Simple and Efficient Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--sustainlp-1--8><div class="card-body p-3 small">It is well known that rerankers built on pretrained transformer models such as <a href=https://en.wikipedia.org/wiki/BERT>BERT</a> have dramatically improved retrieval effectiveness in many tasks. However, these gains have come at substantial costs in terms of <a href=https://en.wikipedia.org/wiki/Economic_efficiency>efficiency</a>, as noted by many researchers. In this work, we show that it is possible to retain the benefits of transformer-based rerankers in a multi-stage reranking pipeline by first using feature-based learning-to-rank techniques to reduce the number of candidate documents under consideration without adversely affecting their quality in terms of recall. Applied to the MS MARCO passage and document ranking tasks, we are able to achieve the same level of <a href=https://en.wikipedia.org/wiki/Effectiveness>effectiveness</a>, but with up to 18 increase in <a href=https://en.wikipedia.org/wiki/Efficiency>efficiency</a>. Furthermore, our techniques are orthogonal to other methods focused on accelerating transformer inference, and thus can be combined for even greater efficiency gains. A higher-level message from our work is that, even though pretrained transformers dominate the modern IR landscape, there are still important roles for traditional LTR techniques, and that we should not forget history.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.louhi-1.11.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--louhi-1--11 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.louhi-1.11 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.louhi-1.11/>Scientific Claim Verification with VerT5erini<span class=acl-fixed-case>V</span>er<span class=acl-fixed-case>T</span>5erini</a></strong><br><a href=/people/r/ronak-pradeep/>Ronak Pradeep</a>
|
<a href=/people/x/xueguang-ma/>Xueguang Ma</a>
|
<a href=/people/r/rodrigo-nogueira/>Rodrigo Nogueira</a>
|
<a href=/people/j/jimmy-lin/>Jimmy Lin</a><br><a href=/volumes/2021.louhi-1/ class=text-muted>Proceedings of the 12th International Workshop on Health Text Mining and Information Analysis</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--louhi-1--11><div class="card-body p-3 small">This work describes the adaptation of a pretrained sequence-to-sequence model to the task of scientific claim verification in the biomedical domain. We propose a system called VerT5erini that exploits T5 for abstract retrieval, sentence selection, and label prediction, which are three critical sub-tasks of <a href=https://en.wikipedia.org/wiki/Verification_and_validation>claim verification</a>. We evaluate our pipeline on SciFACT, a newly curated dataset that requires models to not just predict the veracity of claims but also provide relevant sentences from a corpus of scientific literature that support the prediction. Empirically, our <a href=https://en.wikipedia.org/wiki/System>system</a> outperforms a strong <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline</a> in each of the three sub-tasks. We further show VerT5erini&#8217;s ability to generalize to two new datasets of COVID-19 claims using evidence from the CORD-19 corpus.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.nlp4prog-1.10.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--nlp4prog-1--10 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.nlp4prog-1.10 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.nlp4prog-1.10/>Bag-of-Words Baselines for Semantic Code Search</a></strong><br><a href=/people/x/xinyu-zhang/>Xinyu Zhang</a>
|
<a href=/people/j/ji-xin/>Ji Xin</a>
|
<a href=/people/a/andrew-yates/>Andrew Yates</a>
|
<a href=/people/j/jimmy-lin/>Jimmy Lin</a><br><a href=/volumes/2021.nlp4prog-1/ class=text-muted>Proceedings of the 1st Workshop on Natural Language Processing for Programming (NLP4Prog 2021)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--nlp4prog-1--10><div class="card-body p-3 small">The task of semantic code search is to retrieve <a href=https://en.wikipedia.org/wiki/Snippet_(programming)>code snippets</a> from a <a href=https://en.wikipedia.org/wiki/Text_corpus>source code corpus</a> based on an information need expressed in <a href=https://en.wikipedia.org/wiki/Natural_language>natural language</a>. The semantic gap between natural language and programming languages has for long been regarded as one of the most significant obstacles to the effectiveness of keyword-based information retrieval (IR) methods. It is a common assumption that traditional bag-of-words IR methods are poorly suited for semantic code search : our work empirically investigates this assumption. Specifically, we examine the effectiveness of two traditional IR methods, namely <a href=https://en.wikipedia.org/wiki/BM25>BM25</a> and <a href=https://en.wikipedia.org/wiki/RM3>RM3</a>, on the CodeSearchNet Corpus, which consists of natural language queries paired with relevant code snippets. We find that the two keyword-based methods outperform several pre-BERT neural models. We also compare several code-specific data pre-processing strategies and find that specialized tokenization improves effectiveness.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.68.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--68 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.68 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.68/>Voice Query Auto Completion</a></strong><br><a href=/people/r/raphael-tang/>Raphael Tang</a>
|
<a href=/people/k/karun-kumar/>Karun Kumar</a>
|
<a href=/people/k/kendra-chalkley/>Kendra Chalkley</a>
|
<a href=/people/j/ji-xin/>Ji Xin</a>
|
<a href=/people/l/liming-zhang/>Liming Zhang</a>
|
<a href=/people/w/wenyan-li/>Wenyan Li</a>
|
<a href=/people/g/gefei-yang/>Gefei Yang</a>
|
<a href=/people/y/yajie-mao/>Yajie Mao</a>
|
<a href=/people/j/junho-shin/>Junho Shin</a>
|
<a href=/people/g/geoffrey-craig-murray/>Geoffrey Craig Murray</a>
|
<a href=/people/j/jimmy-lin/>Jimmy Lin</a><br><a href=/volumes/2021.emnlp-main/ class=text-muted>Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--68><div class="card-body p-3 small">Query auto completion (QAC) is the task of predicting a search engine user&#8217;s final query from their intermediate, incomplete query. In this paper, we extend <a href=https://en.wikipedia.org/wiki/Speech_recognition>QAC</a> to the streaming voice search setting, where automatic speech recognition systems produce intermediate transcriptions as users speak. Naively applying existing methods fails because the intermediate transcriptions often do n&#8217;t form prefixes or even substrings of the final transcription. To address this issue, we propose to condition QAC approaches on intermediate transcriptions to complete voice queries. We evaluate our models on a speech-enabled smart television with real-life voice search traffic, finding that this ASR-aware conditioning improves the completion quality. Our best <a href=https://en.wikipedia.org/wiki/Methodology>method</a> obtains an 18 % relative improvement in mean reciprocal rank over previous <a href=https://en.wikipedia.org/wiki/Methodology>methods</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.227.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--227 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.227 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.227/>Simple and Effective Unsupervised Redundancy Elimination to Compress Dense Vectors for Passage Retrieval</a></strong><br><a href=/people/x/xueguang-ma/>Xueguang Ma</a>
|
<a href=/people/m/minghan-li/>Minghan Li</a>
|
<a href=/people/k/kai-sun/>Kai Sun</a>
|
<a href=/people/j/ji-xin/>Ji Xin</a>
|
<a href=/people/j/jimmy-lin/>Jimmy Lin</a><br><a href=/volumes/2021.emnlp-main/ class=text-muted>Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--227><div class="card-body p-3 small">Recent work has shown that dense passage retrieval techniques achieve better ranking accuracy in open-domain question answering compared to sparse retrieval techniques such as <a href=https://en.wikipedia.org/wiki/BM25>BM25</a>, but at the cost of large space and memory requirements. In this paper, we analyze the <a href=https://en.wikipedia.org/wiki/Redundancy_(information_theory)>redundancy</a> present in encoded dense vectors and show that the default dimension of 768 is unnecessarily large. To improve space efficiency, we propose a simple unsupervised compression pipeline that consists of <a href=https://en.wikipedia.org/wiki/Principal_component_analysis>principal component analysis (PCA)</a>, product quantization, and hybrid search. We further investigate other supervised baselines and find surprisingly that unsupervised PCA outperforms them in some settings. We perform extensive experiments on five question answering datasets and demonstrate that our best pipeline achieves good accuracyspace trade-offs, for example, 48 compression with less than 3 % drop in top-100 retrieval accuracy on average or 96 compression with less than 4 % drop. Code and data are available at.<tex-math>48\\times</tex-math> compression with less than 3% drop in top-100 retrieval accuracy on average or <tex-math>96\\times</tex-math> compression with less than 4% drop. Code and data are available at <url>http://pyserini.io/</url>.</div></div><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.repl4nlp-1.10.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--repl4nlp-1--10 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.repl4nlp-1.10 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://slideslive.com/38929776 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.repl4nlp-1.10/>Exploring the Limits of Simple Learners in Knowledge Distillation for <a href=https://en.wikipedia.org/wiki/Document_classification>Document Classification</a> with DocBERT<span class=acl-fixed-case>D</span>oc<span class=acl-fixed-case>BERT</span></a></strong><br><a href=/people/a/ashutosh-adhikari/>Ashutosh Adhikari</a>
|
<a href=/people/a/achyudh-ram/>Achyudh Ram</a>
|
<a href=/people/r/raphael-tang/>Raphael Tang</a>
|
<a href=/people/w/william-l-hamilton/>William L. Hamilton</a>
|
<a href=/people/j/jimmy-lin/>Jimmy Lin</a><br><a href=/volumes/2020.repl4nlp-1/ class=text-muted>Proceedings of the 5th Workshop on Representation Learning for NLP</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--repl4nlp-1--10><div class="card-body p-3 small">Fine-tuned variants of BERT are able to achieve state-of-the-art <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> on many natural language processing tasks, although at significant computational costs. In this paper, we verify BERT&#8217;s effectiveness for document classification and investigate the extent to which BERT-level effectiveness can be obtained by different baselines, combined with knowledge distillationa popular model compression method. The results show that BERT-level effectiveness can be achieved by a single-layer LSTM with at least 40 fewer FLOPS and only 3 % parameters. More importantly, this study analyzes the limits of knowledge distillation as we distill BERT&#8217;s knowledge all the way down to linear modelsa relevant baseline for the task. We report substantial improvement in <a href=https://en.wikipedia.org/wiki/Effectiveness>effectiveness</a> for even the simplest <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>, as they capture the knowledge learnt by BERT.<tex-math>40\\times</tex-math> fewer FLOPS and only <tex-math>{\\sim}3\\%</tex-math> parameters. More importantly, this study analyzes the limits of knowledge distillation as we distill BERT&#8217;s knowledge all the way down to linear models&#8212;a relevant baseline for the task. We report substantial improvement in effectiveness for even the simplest models, as they capture the knowledge learnt by BERT.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.sustainlp-1.11.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--sustainlp-1--11 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.sustainlp-1.11 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939433 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.sustainlp-1.11" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.sustainlp-1.11/>Early Exiting BERT for Efficient Document Ranking<span class=acl-fixed-case>BERT</span> for Efficient Document Ranking</a></strong><br><a href=/people/j/ji-xin/>Ji Xin</a>
|
<a href=/people/r/rodrigo-nogueira/>Rodrigo Nogueira</a>
|
<a href=/people/y/yaoliang-yu/>Yaoliang Yu</a>
|
<a href=/people/j/jimmy-lin/>Jimmy Lin</a><br><a href=/volumes/2020.sustainlp-1/ class=text-muted>Proceedings of SustaiNLP: Workshop on Simple and Efficient Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--sustainlp-1--11><div class="card-body p-3 small">Pre-trained language models such as BERT have shown their effectiveness in various tasks. Despite their power, they are known to be computationally intensive, which hinders real-world applications. In this paper, we introduce early exiting BERT for <a href=https://en.wikipedia.org/wiki/Document_ranking>document ranking</a>. With a slight modification, BERT becomes a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> with multiple output paths, and each inference sample can exit early from these <a href=https://en.wikipedia.org/wiki/Path_(graph_theory)>paths</a>. In this way, <a href=https://en.wikipedia.org/wiki/Computation>computation</a> can be effectively allocated among samples, and overall <a href=https://en.wikipedia.org/wiki/Latency_(engineering)>system latency</a> is significantly reduced while the original <a href=https://en.wikipedia.org/wiki/Quality_(business)>quality</a> is maintained. Our experiments on two document ranking datasets demonstrate up to 2.5x <a href=https://en.wikipedia.org/wiki/Time_complexity>inference speedup</a> with minimal quality degradation. The source code of our implementation can be found at https://github.com/castorini/earlyexiting-monobert.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.sustainlp-1.14.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--sustainlp-1--14 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.sustainlp-1.14 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939436 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.sustainlp-1.14/>A Little Bit Is Worse Than None : Ranking with Limited Training Data</a></strong><br><a href=/people/x/xinyu-zhang/>Xinyu Zhang</a>
|
<a href=/people/a/andrew-yates/>Andrew Yates</a>
|
<a href=/people/j/jimmy-lin/>Jimmy Lin</a><br><a href=/volumes/2020.sustainlp-1/ class=text-muted>Proceedings of SustaiNLP: Workshop on Simple and Efficient Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--sustainlp-1--14><div class="card-body p-3 small">Researchers have proposed simple yet effective techniques for the retrieval problem based on using BERT as a relevance classifier to rerank initial candidates from <a href=https://en.wikipedia.org/wiki/Keyword_search>keyword search</a>. In this work, we tackle the challenge of fine-tuning these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> for specific domains in a data and computationally efficient manner. Typically, researchers fine-tune <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> using corpus-specific labeled data from sources such as TREC. We first answer the question : How much data of this type do we need? Recognizing that the most computationally efficient training is no training, we explore zero-shot ranking using BERT models that have already been fine-tuned with the large MS MARCO passage retrieval dataset. We arrive at the surprising and novel finding that some labeled in-domain data can be worse than none at all.</div></div><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1352.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1352 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1352 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1352/>Cross-Domain Modeling of Sentence-Level Evidence for Document Retrieval</a></strong><br><a href=/people/z/zeynep-akkalyoncu-yilmaz/>Zeynep Akkalyoncu Yilmaz</a>
|
<a href=/people/w/wei-yang/>Wei Yang</a>
|
<a href=/people/h/haotian-zhang/>Haotian Zhang</a>
|
<a href=/people/j/jimmy-lin/>Jimmy Lin</a><br><a href=/volumes/D19-1/ class=text-muted>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1352><div class="card-body p-3 small">This paper applies BERT to ad hoc document retrieval on news articles, which requires addressing two challenges : relevance judgments in existing test collections are typically provided only at the document level, and documents often exceed the length that BERT was designed to handle. Our solution is to aggregate <a href=https://en.wikipedia.org/wiki/Sentence_(law)>sentence-level evidence</a> to rank documents. Furthermore, we are able to leverage passage-level relevance judgments fortuitously available in other domains to fine-tune BERT models that are able to capture cross-domain notions of relevance, and can be directly used for ranking news articles. Our simple neural ranking models achieve state-of-the-art effectiveness on three standard test collections.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1451.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1451 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1451 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1451" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1451/>Aligning Cross-Lingual Entities with Multi-Aspect Information</a></strong><br><a href=/people/h/hsiu-wei-yang/>Hsiu-Wei Yang</a>
|
<a href=/people/y/yanyan-zou/>Yanyan Zou</a>
|
<a href=/people/p/peng-shi/>Peng Shi</a>
|
<a href=/people/w/wei-lu/>Wei Lu</a>
|
<a href=/people/j/jimmy-lin/>Jimmy Lin</a>
|
<a href=/people/x/xu-sun/>Xu Sun</a><br><a href=/volumes/D19-1/ class=text-muted>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1451><div class="card-body p-3 small">Multilingual knowledge graphs (KGs), such as YAGO and DBpedia, represent entities in different languages. The task of cross-lingual entity alignment is to match entities in a source language with their counterparts in target languages. In this work, we investigate embedding-based approaches to encode <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entities</a> from multilingual KGs into the same <a href=https://en.wikipedia.org/wiki/Vector_space>vector space</a>, where equivalent entities are close to each other. Specifically, we apply graph convolutional networks (GCNs) to combine multi-aspect information of entities, including topological connections, relations, and attributes of entities, to learn entity embeddings. To exploit the literal descriptions of entities expressed in different languages, we propose two uses of a pretrained multilingual BERT model to bridge cross-lingual gaps. We further propose two strategies to integrate GCN-based and BERT-based modules to boost performance. Extensive experiments on two benchmark datasets demonstrate that our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> significantly outperforms existing <a href=https://en.wikipedia.org/wiki/System>systems</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1540.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1540 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1540 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1540/>Bridging the Gap between Relevance Matching and <a href=https://en.wikipedia.org/wiki/Semantic_matching>Semantic Matching</a> for Short Text Similarity Modeling</a></strong><br><a href=/people/j/jinfeng-rao/>Jinfeng Rao</a>
|
<a href=/people/l/linqing-liu/>Linqing Liu</a>
|
<a href=/people/y/yi-tay/>Yi Tay</a>
|
<a href=/people/w/wei-yang/>Wei Yang</a>
|
<a href=/people/p/peng-shi/>Peng Shi</a>
|
<a href=/people/j/jimmy-lin/>Jimmy Lin</a><br><a href=/volumes/D19-1/ class=text-muted>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1540><div class="card-body p-3 small">A core problem of information retrieval (IR) is relevance matching, which is to rank documents by relevance to a user&#8217;s query. On the other hand, many NLP problems, such as <a href=https://en.wikipedia.org/wiki/Question_answering>question answering</a> and paraphrase identification, can be considered variants of <a href=https://en.wikipedia.org/wiki/Semantic_matching>semantic matching</a>, which is to measure the <a href=https://en.wikipedia.org/wiki/Semantic_distance>semantic distance</a> between two pieces of short texts. While at a high level both <a href=https://en.wikipedia.org/wiki/Relevance_(information_retrieval)>relevance</a> and <a href=https://en.wikipedia.org/wiki/Semantic_matching>semantic matching</a> require modeling textual similarity, many existing techniques for one can not be easily adapted to the other. To bridge this gap, we propose a novel model, HCAN (Hybrid Co-Attention Network), that comprises (1) a hybrid encoder module that includes ConvNet-based and LSTM-based encoders, (2) a relevance matching module that measures soft term matches with importance weighting at multiple granularities, and (3) a semantic matching module with co-attention mechanisms that capture context-aware semantic relatedness. Evaluations on multiple IR and NLP benchmarks demonstrate state-of-the-art effectiveness compared to approaches that do not exploit pretraining on external data. Extensive ablation studies suggest that relevance and semantic matching signals are complementary across many problem settings, regardless of the choice of underlying <a href=https://en.wikipedia.org/wiki/Encoder>encoders</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1591.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1591 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1591 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1591/>What Part of the <a href=https://en.wikipedia.org/wiki/Neural_network>Neural Network</a> Does This? Understanding LSTMs by Measuring and Dissecting Neurons<span class=acl-fixed-case>LSTM</span>s by Measuring and Dissecting Neurons</a></strong><br><a href=/people/j/ji-xin/>Ji Xin</a>
|
<a href=/people/j/jimmy-lin/>Jimmy Lin</a>
|
<a href=/people/y/yaoliang-yu/>Yaoliang Yu</a><br><a href=/volumes/D19-1/ class=text-muted>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1591><div class="card-body p-3 small">Memory neurons of long short-term memory (LSTM) networks encode and process information in powerful yet mysterious ways. While there has been work to analyze their behavior in carrying low-level information such as linguistic properties, how they directly contribute to label prediction remains unclear. We find inspiration from biologists and study the affinity between individual neurons and labels, propose a novel <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metric</a> to quantify the sensitivity of neurons to each label, and conduct experiments to show the validity of our proposed <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metric</a>. We discover that some neurons are trained to specialize on a subset of labels, and while dropping an arbitrary neuron has little effect on the overall <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of the model, dropping label-specialized neurons predictably and significantly degrades prediction accuracy on the associated label. We further examine the consistency of neuron-label affinity across different models. These observations provide insight into the inner mechanisms of LSTMs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1229.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1229 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1229 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-1229/>Simple Attention-Based Representation Learning for Ranking Short Social Media Posts</a></strong><br><a href=/people/p/peng-shi/>Peng Shi</a>
|
<a href=/people/j/jinfeng-rao/>Jinfeng Rao</a>
|
<a href=/people/j/jimmy-lin/>Jimmy Lin</a><br><a href=/volumes/N19-1/ class=text-muted>Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1229><div class="card-body p-3 small">This paper explores the problem of ranking short social media posts with respect to user queries using <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a>. Instead of starting with a complex <a href=https://en.wikipedia.org/wiki/Software_architecture>architecture</a>, we proceed from the bottom up and examine the effectiveness of a simple, word-level Siamese architecture augmented with attention-based mechanisms for capturing semantic soft matches between query and post tokens. Extensive experiments on datasets from the TREC Microblog Tracks show that our simple models not only achieve better effectiveness than existing approaches that are far more complex or exploit a more diverse set of relevance signals, but are also much faster.</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1178.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1178 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1178 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=C18-1178" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/C18-1178/>Farewell Freebase : Migrating the SimpleQuestions Dataset to <a href=https://en.wikipedia.org/wiki/DBpedia>DBpedia</a><span class=acl-fixed-case>F</span>reebase: Migrating the <span class=acl-fixed-case>S</span>imple<span class=acl-fixed-case>Q</span>uestions Dataset to <span class=acl-fixed-case>DB</span>pedia</a></strong><br><a href=/people/m/michael-azmy/>Michael Azmy</a>
|
<a href=/people/p/peng-shi/>Peng Shi</a>
|
<a href=/people/j/jimmy-lin/>Jimmy Lin</a>
|
<a href=/people/i/ihab-ilyas/>Ihab Ilyas</a><br><a href=/volumes/C18-1/ class=text-muted>Proceedings of the 27th International Conference on Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1178><div class="card-body p-3 small">Question answering over <a href=https://en.wikipedia.org/wiki/Knowledge_graph>knowledge graphs</a> is an important problem of interest both commercially and academically. There is substantial interest in the class of natural language questions that can be answered via the lookup of a single fact, driven by the availability of the popular SimpleQuestions dataset. The problem with this <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>, however, is that answer triples are provided from <a href=https://en.wikipedia.org/wiki/Freebase>Freebase</a>, which has been defunct for several years. As a result, it is difficult to build real-world question answering systems that are operationally deployable. Furthermore, a defunct knowledge graph means that much of the infrastructure for querying, browsing, and manipulating triples no longer exists. To address this problem, we present SimpleDBpediaQA, a new benchmark dataset for simple question answering over knowledge graphs that was created by mapping SimpleQuestions entities and predicates from <a href=https://en.wikipedia.org/wiki/Freebase>Freebase</a> to <a href=https://en.wikipedia.org/wiki/DBpedia>DBpedia</a>. Although this mapping is conceptually straightforward, there are a number of nuances that make the task non-trivial, owing to the different conceptual organizations of the two <a href=https://en.wikipedia.org/wiki/Knowledge_graph>knowledge graphs</a>. To lay the foundation for future research using this <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>, we leverage recent work to provide simple yet strong baselines with and without <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-5002.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-5002 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-5002 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-5002/>Pay-Per-Request Deployment of Neural Network Models Using Serverless Architectures</a></strong><br><a href=/people/z/zhucheng-tu/>Zhucheng Tu</a>
|
<a href=/people/m/mengping-li/>Mengping Li</a>
|
<a href=/people/j/jimmy-lin/>Jimmy Lin</a><br><a href=/volumes/N18-5/ class=text-muted>Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Demonstrations</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-5002><div class="card-body p-3 small">We demonstrate the serverless deployment of neural networks for model inferencing in NLP applications using Amazon&#8217;s Lambda service for feedforward evaluation and <a href=https://en.wikipedia.org/wiki/DynamoDB>DynamoDB</a> for storing word embeddings. Our architecture realizes a pay-per-request pricing model, requiring zero ongoing costs for maintaining server instances. All virtual machine management is handled behind the scenes by the cloud provider without any direct developer intervention. We describe a number of techniques that allow efficient use of serverless resources, and evaluations confirm that our design is both scalable and inexpensive.</div></div><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1285.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1285 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1285 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1285/>An Insight Extraction System on BioMedical Literature with Deep Neural Networks<span class=acl-fixed-case>B</span>io<span class=acl-fixed-case>M</span>edical Literature with Deep Neural Networks</a></strong><br><a href=/people/h/hua-he/>Hua He</a>
|
<a href=/people/k/kris-ganjam/>Kris Ganjam</a>
|
<a href=/people/n/navendu-jain/>Navendu Jain</a>
|
<a href=/people/j/jessica-lundin/>Jessica Lundin</a>
|
<a href=/people/r/ryen-white/>Ryen White</a>
|
<a href=/people/j/jimmy-lin/>Jimmy Lin</a><br><a href=/volumes/D17-1/ class=text-muted>Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1285><div class="card-body p-3 small">Mining biomedical text offers an opportunity to automatically discover important facts and infer associations among them. As new scientific findings appear across a large collection of biomedical publications, our aim is to tap into this literature to automate biomedical knowledge extraction and identify important insights from them. Towards that goal, we develop a <a href=https://en.wikipedia.org/wiki/System>system</a> with novel <a href=https://en.wikipedia.org/wiki/Deep_learning>deep neural networks</a> to extract insights on biomedical literature. Evaluation shows our <a href=https://en.wikipedia.org/wiki/System>system</a> is able to provide insights with competitive accuracy of human acceptance and its relation extraction component outperforms previous work.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Jimmy+Lin" title="Search for 'Jimmy Lin' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/j/ji-xin/ class=align-middle>Ji Xin</a>
<span class="badge badge-secondary align-middle ml-2">5</span></li><li class=list-group-item><a href=/people/p/peng-shi/ class=align-middle>Peng Shi</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/people/x/xueguang-ma/ class=align-middle>Xueguang Ma</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/r/rodrigo-nogueira/ class=align-middle>Rodrigo Nogueira</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/x/xinyu-zhang/ class=align-middle>Xinyu Zhang</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/a/andrew-yates/ class=align-middle>Andrew Yates</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/r/raphael-tang/ class=align-middle>Raphael Tang</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/w/wei-yang/ class=align-middle>Wei Yang</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/j/jinfeng-rao/ class=align-middle>Jinfeng Rao</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/y/yaoliang-yu/ class=align-middle>Yaoliang Yu</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/m/michael-azmy/ class=align-middle>Michael Azmy</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/i/ihab-ilyas/ class=align-middle>Ihab Ilyas</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yue-zhang/ class=align-middle>Yue Zhang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/chengcheng-hu/ class=align-middle>ChengCheng Hu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yuqi-liu/ class=align-middle>Yuqi Liu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/hui-fang/ class=align-middle>Hui Fang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/ronak-pradeep/ class=align-middle>Ronak Pradeep</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/karun-kumar/ class=align-middle>Karun Kumar</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/kendra-chalkley/ class=align-middle>Kendra Chalkley</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/liming-zhang/ class=align-middle>Liming Zhang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/w/wenyan-li/ class=align-middle>Wenyan Li</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/g/gefei-yang/ class=align-middle>Gefei Yang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yajie-mao/ class=align-middle>Yajie Mao</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/junho-shin/ class=align-middle>Junho Shin</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/g/geoffrey-craig-murray/ class=align-middle>Geoffrey Craig Murray</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/minghan-li/ class=align-middle>Minghan Li</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/kai-sun/ class=align-middle>Kai Sun</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/ashutosh-adhikari/ class=align-middle>Ashutosh Adhikari</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/achyudh-ram/ class=align-middle>Achyudh Ram</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/w/william-l-hamilton/ class=align-middle>William L. Hamilton</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/zeynep-akkalyoncu-yilmaz/ class=align-middle>Zeynep Akkalyoncu Yilmaz</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/haotian-zhang/ class=align-middle>Haotian Zhang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/hsiu-wei-yang/ class=align-middle>Hsiu-Wei Yang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yanyan-zou/ class=align-middle>Yanyan Zou</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/w/wei-lu/ class=align-middle>Wei Lu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/x/xu-sun/ class=align-middle>Xu Sun</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/linqing-liu/ class=align-middle>Linqing Liu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yi-tay/ class=align-middle>Yi Tay</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/hua-he/ class=align-middle>Hua He</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/kris-ganjam/ class=align-middle>Kris Ganjam</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/n/navendu-jain/ class=align-middle>Navendu Jain</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jessica-lundin/ class=align-middle>Jessica Lundin</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/ryen-white/ class=align-middle>Ryen White</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/zhucheng-tu/ class=align-middle>Zhucheng Tu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/mengping-li/ class=align-middle>Mengping Li</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/emnlp/ class=align-middle>EMNLP</a><span class="badge badge-secondary align-middle ml-2">7</span></li><li class=list-group-item><a href=/venues/sustainlp/ class=align-middle>sustainlp</a><span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/venues/naacl/ class=align-middle>NAACL</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/coling/ class=align-middle>COLING</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/louhi/ class=align-middle>Louhi</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-venues aria-expanded=false aria-controls=more-venues>show all...</li><div class="collapse border-top" id=more-venues><li class=list-group-item><a href=/venues/nlp4prog/ class=align-middle>NLP4Prog</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/repl4nlp/ class=align-middle>RepL4NLP</a><span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright &nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>