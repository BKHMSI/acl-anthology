<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>James Pustejovsky - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>James</span> <span class=font-weight-bold>Pustejovsky</span></h2><hr><div class=row><div class=col-lg-9><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.louhi-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.louhi-1.0/>Proceedings of the 12th International Workshop on Health Text Mining and Information Analysis</a></strong><br><a href=/people/e/eben-holderness/>Eben Holderness</a>
|
<a href=/people/a/antonio-jimeno-yepes/>Antonio Jimeno Yepes</a>
|
<a href=/people/a/alberto-lavelli/>Alberto Lavelli</a>
|
<a href=/people/a/anne-lyse-minard/>Anne-Lyse Minard</a>
|
<a href=/people/j/james-pustejovsky/>James Pustejovsky</a>
|
<a href=/people/f/fabio-rinaldi/>Fabio Rinaldi</a><br><a href=/volumes/2021.louhi-1/ class=text-muted>Proceedings of the 12th International Workshop on Health Text Mining and Information Analysis</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.starsem-1.21.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--starsem-1--21 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.starsem-1.21 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.starsem-1.21/>Neural Metaphor Detection with Visibility Embeddings</a></strong><br><a href=/people/g/gitit-kehat/>Gitit Kehat</a>
|
<a href=/people/j/james-pustejovsky/>James Pustejovsky</a><br><a href=/volumes/2021.starsem-1/ class=text-muted>Proceedings of *SEM 2021: The Tenth Joint Conference on Lexical and Computational Semantics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--starsem-1--21><div class="card-body p-3 small">We present new results for the problem of sequence metaphor labeling, using the recently developed Visibility Embeddings. We show that concatenating such <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a> to the input of a BiLSTM obtains consistent and significant improvements at almost no cost, and we present further improved results when visibility embeddings are combined with BERT.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.mmsr-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.mmsr-1.0/>Proceedings of the 1st Workshop on Multimodal Semantic Representations (MMSR)</a></strong><br><a href=/people/l/lucia-donatelli/>Lucia Donatelli</a>
|
<a href=/people/n/nikhil-krishnaswamy/>Nikhil Krishnaswamy</a>
|
<a href=/people/k/kenneth-lai/>Kenneth Lai</a>
|
<a href=/people/j/james-pustejovsky/>James Pustejovsky</a><br><a href=/volumes/2021.mmsr-1/ class=text-muted>Proceedings of the 1st Workshop on Multimodal Semantic Representations (MMSR)</a></span></p><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.dmr-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.dmr-1.0/>Proceedings of the Second International Workshop on Designing Meaning Representations</a></strong><br><a href=/people/n/nianwen-xue/>Nianwen Xue</a>
|
<a href=/people/j/johan-bos/>Johan Bos</a>
|
<a href=/people/w/william-croft/>William Croft</a>
|
<a href=/people/j/jan-hajic/>Jan Hajič</a>
|
<a href=/people/c/chu-ren-huang/>Chu-Ren Huang</a>
|
<a href=/people/s/stephan-oepen/>Stephan Oepen</a>
|
<a href=/people/m/martha-palmer/>Martha Palmer</a>
|
<a href=/people/j/james-pustejovsky/>James Pustejovsky</a><br><a href=/volumes/2020.dmr-1/ class=text-muted>Proceedings of the Second International Workshop on Designing Meaning Representations</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.louhi-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.louhi-1.0/>Proceedings of the 11th International Workshop on Health Text Mining and Information Analysis</a></strong><br><a href=/people/e/eben-holderness/>Eben Holderness</a>
|
<a href=/people/a/antonio-jimeno-yepes/>Antonio Jimeno Yepes</a>
|
<a href=/people/a/alberto-lavelli/>Alberto Lavelli</a>
|
<a href=/people/a/anne-lyse-minard/>Anne-Lyse Minard</a>
|
<a href=/people/j/james-pustejovsky/>James Pustejovsky</a>
|
<a href=/people/f/fabio-rinaldi/>Fabio Rinaldi</a><br><a href=/volumes/2020.louhi-1/ class=text-muted>Proceedings of the 11th International Workshop on Health Text Mining and Information Analysis</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.893.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--893 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.893 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.893/>Interchange Formats for Visualization : LIF and MMIF<span class=acl-fixed-case>LIF</span> and <span class=acl-fixed-case>MMIF</span></a></strong><br><a href=/people/k/kyeongmin-rim/>Kyeongmin Rim</a>
|
<a href=/people/k/kelley-lynch/>Kelley Lynch</a>
|
<a href=/people/m/marc-verhagen/>Marc Verhagen</a>
|
<a href=/people/n/nancy-ide/>Nancy Ide</a>
|
<a href=/people/j/james-pustejovsky/>James Pustejovsky</a><br><a href=/volumes/2020.lrec-1/ class=text-muted>Proceedings of the 12th Language Resources and Evaluation Conference</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--893><div class="card-body p-3 small">Promoting interoperrable computational linguistics (CL) and natural language processing (NLP) application platforms and interchange-able data formats have contributed improving discoverabilty and accessbility of the openly available NLP software. In this paper, wediscuss the enhanced data visualization capabilities that are also enabled by inter-operating NLP pipelines and interchange formats. For adding openly available visualization tools and graphical annotation tools to the Language Applications Grid (LAPPS Grid) andComputational Linguistics Applications for Multimedia Services (CLAMS) toolboxes, we have developed interchange formats that cancarry annotations and metadata for text and audiovisual source data. We descibe those data formats and present case studies where wesuccessfully adopt open-source visualization tools and combine them with CL tools.</div></div><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-6200.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-6200/>Proceedings of the Tenth International Workshop on Health Text Mining and Information Analysis (LOUHI 2019)</a></strong><br><a href=/people/e/eben-holderness/>Eben Holderness</a>
|
<a href=/people/a/antonio-jimeno-yepes/>Antonio Jimeno Yepes</a>
|
<a href=/people/a/alberto-lavelli/>Alberto Lavelli</a>
|
<a href=/people/a/anne-lyse-minard/>Anne-Lyse Minard</a>
|
<a href=/people/j/james-pustejovsky/>James Pustejovsky</a>
|
<a href=/people/f/fabio-rinaldi/>Fabio Rinaldi</a><br><a href=/volumes/D19-62/ class=text-muted>Proceedings of the Tenth International Workshop on Health Text Mining and Information Analysis (LOUHI 2019)</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-0601.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-0601 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-0601 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-0601/>A <a href=https://en.wikipedia.org/wiki/Dynamic_semantics>Dynamic Semantics</a> for Causal Counterfactuals</a></strong><br><a href=/people/k/kenneth-lai/>Kenneth Lai</a>
|
<a href=/people/j/james-pustejovsky/>James Pustejovsky</a><br><a href=/volumes/W19-06/ class=text-muted>Proceedings of the 13th International Conference on Computational Semantics - Student Papers</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-0601><div class="card-body p-3 small">Under the standard approach to counterfactuals, to determine the meaning of a counterfactual sentence, we consider the closest possible world(s) where the antecedent is true, and evaluate the consequent. Building on the standard approach, some researchers have found that the set of worlds to be considered is dependent on context ; it evolves with the <a href=https://en.wikipedia.org/wiki/Discourse>discourse</a>. Others have focused on how to define the distance between possible worlds, using ideas from <a href=https://en.wikipedia.org/wiki/Causal_model>causal modeling</a>. This paper integrates the two ideas. We present a <a href=https://en.wikipedia.org/wiki/Semantics>semantics</a> for <a href=https://en.wikipedia.org/wiki/Counterfactual_conditional>counterfactuals</a> that uses a distance measure based on <a href=https://en.wikipedia.org/wiki/Causality>causal laws</a>, that can also change over time. We show how our <a href=https://en.wikipedia.org/wiki/Semantics_(computer_science)>semantics</a> can be implemented in the <a href=https://en.wikipedia.org/wiki/Haskell_(programming_language)>Haskell programming language</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-1915.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-1915 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-1915 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-1915/>Distinguishing Clinical Sentiment : The Importance of Domain Adaptation in Psychiatric Patient Health Records</a></strong><br><a href=/people/e/eben-holderness/>Eben Holderness</a>
|
<a href=/people/p/philip-cawkwell/>Philip Cawkwell</a>
|
<a href=/people/k/kirsten-bolton/>Kirsten Bolton</a>
|
<a href=/people/j/james-pustejovsky/>James Pustejovsky</a>
|
<a href=/people/m/mei-hua-hall/>Mei-Hua Hall</a><br><a href=/volumes/W19-19/ class=text-muted>Proceedings of the 2nd Clinical Natural Language Processing Workshop</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-1915><div class="card-body p-3 small">Recently <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing (NLP) tools</a> have been developed to identify and extract salient risk indicators in <a href=https://en.wikipedia.org/wiki/Electronic_health_record>electronic health records (EHRs)</a>. Sentiment analysis, although widely used in non-medical areas for improving <a href=https://en.wikipedia.org/wiki/Decision-making>decision making</a>, has been studied minimally in the clinical setting. In this study, we undertook, to our knowledge, the first domain adaptation of <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> to psychiatric EHRs by defining psychiatric clinical sentiment, performing an annotation project, and evaluating multiple sentence-level sentiment machine learning (ML) models. Results indicate that off-the-shelf sentiment analysis tools fail in identifying clinically positive or negative polarity, and that the definition of clinical sentiment that we provide is learnable with relatively small amounts of training data. This project is an initial step towards further refining <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis methods</a> for clinical use. Our long-term objective is to incorporate the results of this project as part of a <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning model</a> that predicts inpatient readmission risk. We hope that this work will initiate a discussion concerning domain adaptation of <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> to the clinical setting.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2512.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-2512 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-2512 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-2512/>Computational Linguistics Applications for Multimedia Services</a></strong><br><a href=/people/k/kyeongmin-rim/>Kyeongmin Rim</a>
|
<a href=/people/k/kelley-lynch/>Kelley Lynch</a>
|
<a href=/people/j/james-pustejovsky/>James Pustejovsky</a><br><a href=/volumes/W19-25/ class=text-muted>Proceedings of the 3rd Joint SIGHUM Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-2512><div class="card-body p-3 small">We present Computational Linguistics Applications for Multimedia Services (CLAMS), a platform that provides access to computational content analysis tools for archival multimedia material that appear in different media, such as <a href=https://en.wikipedia.org/wiki/Text_(literary_theory)>text</a>, <a href=https://en.wikipedia.org/wiki/Sound>audio</a>, <a href=https://en.wikipedia.org/wiki/Image>image</a>, and <a href=https://en.wikipedia.org/wiki/Video>video</a>. The primary goal of CLAMS is : (1) to develop an interchange format between multimodal metadata generation tools to ensure interoperability between tools ; (2) to provide users with a portable, user-friendly workflow engine to chain selected tools to extract meaningful analyses ; and (3) to create a public software development kit (SDK) for developers that eases deployment of analysis tools within the CLAMS platform. CLAMS is designed to help archives and libraries enrich the <a href=https://en.wikipedia.org/wiki/Metadata>metadata</a> associated with their mass-digitized multimedia collections, that would otherwise be largely unsearchable.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3303.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3303 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3303 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3303/>Modeling Quantification and Scope in Abstract Meaning Representations<span class=acl-fixed-case>A</span>bstract <span class=acl-fixed-case>M</span>eaning <span class=acl-fixed-case>R</span>epresentations</a></strong><br><a href=/people/j/james-pustejovsky/>James Pustejovsky</a>
|
<a href=/people/k/ken-lai/>Ken Lai</a>
|
<a href=/people/n/nianwen-xue/>Nianwen Xue</a><br><a href=/volumes/W19-33/ class=text-muted>Proceedings of the First International Workshop on Designing Meaning Representations</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3303><div class="card-body p-3 small">In this paper, we propose an extension to Abstract Meaning Representations (AMRs) to encode scope information of quantifiers and negation, in a way that overcomes the semantic gaps of the schema while maintaining its cognitive simplicity. Specifically, we address three phenomena not previously part of the AMR specification : <a href=https://en.wikipedia.org/wiki/Quantification_(science)>quantification</a>, <a href=https://en.wikipedia.org/wiki/Negation>negation</a> (generally), and <a href=https://en.wikipedia.org/wiki/Modal_logic>modality</a>. The resulting representation, which we call Uniform Meaning Representation (UMR), adopts the predicative core of AMR and embeds it under a scope graph when appropriate. UMR representations differ from other treatments of quantification and modal scope phenomena in two ways : (a) they are more transparent ; and (b) they specify default scope when possible. &#8216;</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3318.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3318 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3318 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3318/>VerbNet Representations : Subevent Semantics for Transfer Verbs<span class=acl-fixed-case>V</span>erb<span class=acl-fixed-case>N</span>et Representations: Subevent Semantics for Transfer Verbs</a></strong><br><a href=/people/s/susan-windisch-brown/>Susan Windisch Brown</a>
|
<a href=/people/j/julia-bonn/>Julia Bonn</a>
|
<a href=/people/j/james-gung/>James Gung</a>
|
<a href=/people/a/annie-zaenen/>Annie Zaenen</a>
|
<a href=/people/j/james-pustejovsky/>James Pustejovsky</a>
|
<a href=/people/m/martha-palmer/>Martha Palmer</a><br><a href=/volumes/W19-33/ class=text-muted>Proceedings of the First International Workshop on Designing Meaning Representations</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3318><div class="card-body p-3 small">This paper announces the release of a new version of the English lexical resource VerbNet with substantially revised semantic representations designed to facilitate computer planning and reasoning based on human language. We use the transfer of possession and transfer of information event representations to illustrate both the general framework of the <a href=https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning>representations</a> and the types of nuances the new <a href=https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning>representations</a> can capture. These representations use a Generative Lexicon-inspired subevent structure to track attributes of event participants across time, highlighting oppositions and temporal and causal relations among the subevents.</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-1400.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-1400/>Proceedings of the First International Workshop on Spatial Language Understanding</a></strong><br><a href=/people/p/parisa-kordjamshidi/>Parisa Kordjamshidi</a>
|
<a href=/people/a/archna-bhatia/>Archna Bhatia</a>
|
<a href=/people/j/james-pustejovsky/>James Pustejovsky</a>
|
<a href=/people/m/marie-francine-moens/>Marie-Francine Moens</a><br><a href=/volumes/W18-14/ class=text-muted>Proceedings of the First International Workshop on Spatial Language Understanding</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4301.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4301 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4301 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4301/>Every Object Tells a Story</a></strong><br><a href=/people/j/james-pustejovsky/>James Pustejovsky</a>
|
<a href=/people/n/nikhil-krishnaswamy/>Nikhil Krishnaswamy</a><br><a href=/volumes/W18-43/ class=text-muted>Proceedings of the Workshop Events and Stories in the News 2018</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4301><div class="card-body p-3 small">Most work within the computational event modeling community has tended to focus on the interpretation and ordering of events that are associated with <a href=https://en.wikipedia.org/wiki/Verb>verbs</a> and event nominals in linguistic expressions. What is often overlooked in the construction of a global interpretation of a narrative is the role contributed by the objects participating in these structures, and the latent events and activities conventionally associated with them. Recently, the analysis of visual images has also enriched the scope of how events can be identified, by anchoring both linguistic expressions and ontological labels to segments, subregions, and properties of <a href=https://en.wikipedia.org/wiki/Image>images</a>. By semantically grounding <a href=https://en.wikipedia.org/wiki/Event_(philosophy)>event descriptions</a> in their visualization, the importance of <a href=https://en.wikipedia.org/wiki/Object-oriented_programming>object-based attributes</a> becomes more apparent. In this position paper, we look at the narrative structure of objects : that is, how objects reference events through their intrinsic attributes, such as <a href=https://en.wikipedia.org/wiki/Affordance>affordances</a>, purposes, and <a href=https://en.wikipedia.org/wiki/Function_(mathematics)>functions</a>. We argue that, not only do objects encode conventionalized events, but that when they are composed within specific habitats, the ensemble can be viewed as modeling coherent event sequences, thereby enriching the global interpretation of the evolving narrative being constructed.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5615.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5615 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5615 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5615/>Analysis of Risk Factor Domains in Psychosis Patient Health Records</a></strong><br><a href=/people/e/eben-holderness/>Eben Holderness</a>
|
<a href=/people/n/nicholas-miller/>Nicholas Miller</a>
|
<a href=/people/k/kirsten-bolton/>Kirsten Bolton</a>
|
<a href=/people/p/philip-cawkwell/>Philip Cawkwell</a>
|
<a href=/people/m/marie-meteer/>Marie Meteer</a>
|
<a href=/people/j/james-pustejovsky/>James Pustejovsky</a>
|
<a href=/people/m/mei-hua-hall/>Mei Hua-Hall</a><br><a href=/volumes/W18-56/ class=text-muted>Proceedings of the Ninth International Workshop on Health Text Mining and Information Analysis</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5615><div class="card-body p-3 small">Readmission after discharge from a hospital is disruptive and costly, regardless of the reason. However, it can be particularly problematic for psychiatric patients, so predicting which patients may be readmitted is critically important but also very difficult. Clinical narratives in psychiatric electronic health records (EHRs) span a wide range of topics and vocabulary ; therefore, a psychiatric readmission prediction model must begin with a robust and interpretable topic extraction component. We created a data pipeline for using document vector similarity metrics to perform topic extraction on psychiatric EHR data in service of our long-term goal of creating a readmission risk classifier. We show initial results for our topic extraction model and identify additional <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> we will be incorporating in the future.</div></div><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-2018.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-2018 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-2018 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-2018/>Integrating Vision and Language Datasets to Measure Word Concreteness</a></strong><br><a href=/people/g/gitit-kehat/>Gitit Kehat</a>
|
<a href=/people/j/james-pustejovsky/>James Pustejovsky</a><br><a href=/volumes/I17-2/ class=text-muted>Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-2018><div class="card-body p-3 small">We present and take advantage of the inherent visualizability properties of words in visual corpora (the textual components of vision-language datasets) to compute concreteness scores for <a href=https://en.wikipedia.org/wiki/Word>words</a>. Our simple method does not require hand-annotated concreteness score lists for training, and yields state-of-the-art results when evaluated against concreteness scores lists and previously derived scores, as well as when used for metaphor detection.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2017.lilt-15.1.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2017--lilt-15--1 data-toggle=collapse aria-expanded=false aria-controls=abstract-2017.lilt-15.1 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2017.lilt-15.1/>Lexical Factorization and Syntactic Behavior</a></strong><br><a href=/people/j/james-pustejovsky/>James Pustejovsky</a>
|
<a href=/people/a/aravind-joshi/>Aravind Joshi</a><br><a href=/volumes/2017.lilt-15/ class=text-muted>Linguistic Issues in Language Technology, Volume 15, 2017</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2017--lilt-15--1><div class="card-body p-3 small">In this paper, we examine the correlation between <a href=https://en.wikipedia.org/wiki/Lexical_semantics>lexical semantics</a> and the syntactic realization of the different components of a word&#8217;s meaning in <a href=https://en.wikipedia.org/wiki/Natural_language>natural language</a>. More specifically, we will explore the effect that lexical factorization in verb semantics has on the suppression or expression of semantic features within the sentence. Factorization was a common analytic tool employed in early generative linguistic approaches to lexical decomposition, and continues to play a role in contemporary <a href=https://en.wikipedia.org/wiki/Semantics>semantics</a>, in various guises and modified forms. Building on the unpublished analysis of verbs of seeing in Joshi (1972), we argue here that the significance of lexical factorization is twofold : first, current models of verb meaning owe much of their insight to factor-based theories of meaning ; secondly, the factorization properties of a lexical item appear to influence, both directly and indirectly, the possible syntactic expressibility of arguments and adjuncts in sentence composition. We argue that this information can be used to compute what we call the factor expression likelihood (FEL) associated with a verb in a sentence. This is the likelihood that the overt syntactic expression of a factor will cooccur with the verb. This has consequences for the compositional mechanisms responsible for computing the meaning of the sentence, as well as significance in the creation of <a href=https://en.wikipedia.org/wiki/Computational_linguistics>computational models</a> attempting to capture linguistic behavior over large corpora.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-5006.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-5006 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-5006 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-5006/>Building Multimodal Simulations for Natural Language</a></strong><br><a href=/people/j/james-pustejovsky/>James Pustejovsky</a>
|
<a href=/people/n/nikhil-krishnaswamy/>Nikhil Krishnaswamy</a><br><a href=/volumes/E17-5/ class=text-muted>Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Tutorial Abstracts</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-5006><div class="card-body p-3 small">In this tutorial, we introduce a computational framework and modeling language (VoxML) for composing multimodal simulations of natural language expressions within a 3D simulation environment (VoxSim). We demonstrate how to construct voxemes, which are visual object representations of linguistic entities. We also show how to compose events and actions over these <a href=https://en.wikipedia.org/wiki/Object_(computer_science)>objects</a>, within a restricted domain of dynamics. This gives us the building blocks to simulate narratives of multiple events or participate in a multimodal dialogue with synthetic agents in the simulation environment. To our knowledge, this is the first time such material has been presented as a tutorial within the CL community. This will be of relevance to students and researchers interested in modeling actionable language, natural language communication with agents and robots, spatial and temporal constraint solving through language, referring expression generation, embodied cognition, as well as minimal model creation. Multimodal simulation of language, particularly motion expressions, brings together a number of existing lines of research from the computational linguistic, semantics, robotics, and formal logic communities, including action and event representation (Di Eugenio, 1991), modeling gestural correlates to NL expressions (Kipp et al., 2007 ; Neff et al., 2008), and action event modeling (Kipper and Palmer, 2000 ; Yang et al., 2015). We combine an approach to event modeling with a scene generation approach akin to those found in work by (Coyne and Sproat, 2001 ; Siskind, 2011 ; Chang et al., 2015). Mapping natural language expressions through a <a href=https://en.wikipedia.org/wiki/Formal_system>formal model</a> and a dynamic logic interpretation into a visualization of the event described provides an environment for grounding concepts and referring expressions that is interpretable by both a computer and a human user. This opens a variety of avenues for humans to communicate with computerized agents and robots, as in (Matuszek et al., 2013 ; Lauria et al., 2001), (Forbes et al., 2015), and (Deits et al., 2013 ; Walter et al., 2013 ; Tellex et al., 2014). Simulation and automatic visualization of events from natural language descriptions and supplementary modalities, such as gestures, allows humans to use their native capabilities as linguistic and visual interpreters to collaborate on tasks with an artificial agent or to put semantic intuitions to the test in an environment where user and agent share a common context. In previous work (Pustejovsky and Krishnaswamy, 2014 ; Pustejovsky, 2013a), we introduced a method for modeling natural language expressions within a 3D simulation environment built on top of the game development platform Unity (Goldstone, 2009). The goal of that work was to evaluate, through explicit visualizations of linguistic input, the semantic presuppositions inherent in the different lexical choices of an utterance. This work led to two additional lines of research : an explicit encoding for how an object is itself situated relative to its environment ; and an operational characterization of how an object changes its location or how an agent acts on an object over time, e.g., its affordance structure. The former has developed into a semantic notion of situational context, called a habitat (Pustejovsky, 2013a ; McDonald and Pustejovsky, 2014), while the latter is addressed by dynamic interpretations of event structure (Pustejovsky and Moszkowicz, 2011 ; Pustejovsky and Krishnaswamy, 2016b ; Pustejovsky, 2013b).The requirements on building a visual simulation from language include several components. We require a rich type system for lexical items and their composition, as well as a language for modeling the dynamics of events, based on Generative Lexicon (GL). Further, a minimal embedding space (MES) for the <a href=https://en.wikipedia.org/wiki/Simulation>simulation</a> must be determined. This is the <a href=https://en.wikipedia.org/wiki/Three-dimensional_space>3D region</a> within which the state is configured or the event unfolds. Object-based attributes for participants in a situation or event also need to be specified ; e.g., <a href=https://en.wikipedia.org/wiki/Orientation_(geometry)>orientation</a>, relative size, default position or pose, etc. The <a href=https://en.wikipedia.org/wiki/Simulation>simulation</a> establishes an epistemic condition on the object and event rendering, imposing an implicit point of view (POV). Finally, there must be some sort of agent-dependent embodiment ; this determines the relative scaling of an agent and its event participants and their surroundings, as it engages in the environment. In order to construct a robust simulation from linguistic input, an event and its participants must be embedded within an appropriate minimal embedding space. This must sufficiently enclose the <a href=https://en.wikipedia.org/wiki/Event_(computing)>event localization</a>, while optionally including space enough for a frame of reference for the event (the viewers perspective).We first describe the formal multimodal foundations for the <a href=https://en.wikipedia.org/wiki/Modeling_language>modeling language</a>, VoxML, which creates a minimal simulation from the linguistic input interpreted by the multimodal language, DITL. We then describe VoxSim, the compositional modeling and simulation environment, which maps the minimal VoxML model of the linguistic utterance to a simulation in <a href=https://en.wikipedia.org/wiki/Unity_(game_engine)>Unity</a>. This knowledge includes specification of object affordances, e.g., what actions are possible or enabled by use an object. VoxML (Pustejovsky and Krishnaswamy, 2016b ; Pustejovsky and Krishnaswamy, 2016a) encodes semantic knowledge of real-world objects represented as <a href=https://en.wikipedia.org/wiki/3D_modeling>3D models</a>, and of events and attributes related to and enacted over these <a href=https://en.wikipedia.org/wiki/Object_(computer_science)>objects</a>. VoxML goes beyond the limitations of existing 3D visual markup languages by allowing for the encoding of a broad range of semantic knowledge that can be exploited by a simulation platform such as VoxSim. VoxSim (Krishnaswamy and Pustejovsky, 2016a ; Krishnaswamy and Pustejovsky, 2016b) uses object and event semantic knowledge to generate animated scenes in real time without a complex animation interface. It uses the <a href=https://en.wikipedia.org/wiki/Unity_(game_engine)>Unity game engine</a> for graphics and I / O processing and takes as input a simple <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language utterance</a>. The parsed utterance is semantically interpreted and transformed into a hybrid dynamic logic representation (DITL), and used to generate a minimal simulation of the event when composed with VoxML knowledge.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=James+Pustejovsky" title="Search for 'James Pustejovsky' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/e/eben-holderness/ class=align-middle>Eben Holderness</a>
<span class="badge badge-secondary align-middle ml-2">5</span></li><li class=list-group-item><a href=/people/a/antonio-jimeno-yepes/ class=align-middle>Antonio Jimeno Yepes</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/a/alberto-lavelli/ class=align-middle>Alberto Lavelli</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/a/anne-lyse-minard/ class=align-middle>Anne-Lyse Minard</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/f/fabio-rinaldi/ class=align-middle>Fabio Rinaldi</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/n/nikhil-krishnaswamy/ class=align-middle>Nikhil Krishnaswamy</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/g/gitit-kehat/ class=align-middle>Gitit Kehat</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/n/nianwen-xue/ class=align-middle>Nianwen Xue</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/m/martha-palmer/ class=align-middle>Martha Palmer</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/k/kirsten-bolton/ class=align-middle>Kirsten Bolton</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/p/philip-cawkwell/ class=align-middle>Philip Cawkwell</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/m/mei-hua-hall/ class=align-middle>Mei-Hua Hall</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/k/kenneth-lai/ class=align-middle>Kenneth Lai</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/k/kyeongmin-rim/ class=align-middle>Kyeongmin Rim</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/k/kelley-lynch/ class=align-middle>Kelley Lynch</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/j/johan-bos/ class=align-middle>Johan Bos</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/w/william-croft/ class=align-middle>William Croft</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jan-hajic/ class=align-middle>Jan Hajic</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/chu-ren-huang/ class=align-middle>Chu-Ren Huang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/stephan-oepen/ class=align-middle>Stephan Oepen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/aravind-joshi/ class=align-middle>Aravind Joshi</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/p/parisa-kordjamshidi/ class=align-middle>Parisa Kordjamshidi</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/archna-bhatia/ class=align-middle>Archna Bhatia</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/marie-francine-moens/ class=align-middle>Marie Francine Moens</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/n/nicholas-miller/ class=align-middle>Nicholas Miller</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/marie-meteer/ class=align-middle>Marie Meteer</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/ken-lai/ class=align-middle>Ken Lai</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/susan-windisch-brown/ class=align-middle>Susan Windisch Brown</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/julia-bonn/ class=align-middle>Julia Bonn</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/james-gung/ class=align-middle>James Gung</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/annie-zaenen/ class=align-middle>Annie Zaenen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/marc-verhagen/ class=align-middle>Marc Verhagen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/n/nancy-ide/ class=align-middle>Nancy Ide</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/lucia-donatelli/ class=align-middle>Lucia Donatelli</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/ws/ class=align-middle>WS</a><span class="badge badge-secondary align-middle ml-2">8</span></li><li class=list-group-item><a href=/venues/louhi/ class=align-middle>Louhi</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/ijcnlp/ class=align-middle>IJCNLP</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/dmr/ class=align-middle>DMR</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/emnlp/ class=align-middle>EMNLP</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-venues aria-expanded=false aria-controls=more-venues>show all...</li><div class="collapse border-top" id=more-venues><li class=list-group-item><a href=/venues/starsem/ class=align-middle>*SEM</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/lilt/ class=align-middle>LILT</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/lrec/ class=align-middle>LREC</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/mmsr/ class=align-middle>MMSR</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/eacl/ class=align-middle>EACL</a><span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>