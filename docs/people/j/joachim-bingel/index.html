<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Joachim Bingel - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Joachim</span> <span class=font-weight-bold>Bingel</span></h2><hr><div class=row><div class=col-lg-9><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S19-2026.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S19-2026 data-toggle=collapse aria-expanded=false aria-controls=abstract-S19-2026 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/S19-2026/>CoAStaL at SemEval-2019 Task 3 : Affect Classification in Dialogue using Attentive BiLSTMs<span class=acl-fixed-case>C</span>o<span class=acl-fixed-case>AS</span>ta<span class=acl-fixed-case>L</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2019 Task 3: Affect Classification in Dialogue using Attentive <span class=acl-fixed-case>B</span>i<span class=acl-fixed-case>LSTM</span>s</a></strong><br><a href=/people/a/ana-valeria-gonzalez/>Ana Valeria González</a>
|
<a href=/people/v/victor-petren-bach-hansen/>Victor Petrén Bach Hansen</a>
|
<a href=/people/j/joachim-bingel/>Joachim Bingel</a>
|
<a href=/people/a/anders-sogaard/>Anders Søgaard</a><br><a href=/volumes/S19-2/ class=text-muted>Proceedings of the 13th International Workshop on Semantic Evaluation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S19-2026><div class="card-body p-3 small">This work describes the system presented by the CoAStaL Natural Language Processing group at University of Copenhagen. The main <a href=https://en.wikipedia.org/wiki/System>system</a> we present uses the same <a href=https://en.wikipedia.org/wiki/Attentional_control>attention mechanism</a> presented in (Yang et al., 2016). Our overall model architecture is also inspired by their hierarchical classification model and adapted to deal with classification in dialogue by encoding information at the turn level. We use different encodings for each turn to create a more expressive representation of dialogue context which is then fed into our classifier. We also define a custom preprocessing step in order to deal with language commonly used in interactions across many social media outlets. Our proposed system achieves a micro F1 score of 0.7340 on the test set and shows significant gains in performance compared to a system using dialogue level encoding.</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1021.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1021 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1021 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1021/>Lexi : A tool for adaptive, personalized text simplification<span class=acl-fixed-case>L</span>exi: A tool for adaptive, personalized text simplification</a></strong><br><a href=/people/j/joachim-bingel/>Joachim Bingel</a>
|
<a href=/people/g/gustavo-paetzold/>Gustavo Paetzold</a>
|
<a href=/people/a/anders-sogaard/>Anders Søgaard</a><br><a href=/volumes/C18-1/ class=text-muted>Proceedings of the 27th International Conference on Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1021><div class="card-body p-3 small">Most previous research in <a href=https://en.wikipedia.org/wiki/Text_simplification>text simplification</a> has aimed to develop generic solutions, assuming very homogeneous target audiences with consistent intra-group simplification needs. We argue that this assumption does not hold, and that instead we need to develop simplification systems that adapt to the individual needs of specific users. As a first step towards personalized simplification, we propose a framework for adaptive lexical simplification and introduce Lexi, a free open-source and easily extensible tool for adaptive, personalized text simplification. Lexi is easily installed as a <a href=https://en.wikipedia.org/wiki/Browser_extension>browser extension</a>, enabling easy access to the service for its users.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0503.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0503 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0503 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0503/>Predicting misreadings from gaze in children with reading difficulties</a></strong><br><a href=/people/j/joachim-bingel/>Joachim Bingel</a>
|
<a href=/people/m/maria-barrett/>Maria Barrett</a>
|
<a href=/people/s/sigrid-klerke/>Sigrid Klerke</a><br><a href=/volumes/W18-05/ class=text-muted>Proceedings of the Thirteenth Workshop on Innovative Use of NLP for Building Educational Applications</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0503><div class="card-body p-3 small">We present the first work on predicting reading mistakes in children with <a href=https://en.wikipedia.org/wiki/Reading_disability>reading difficulties</a> based on eye-tracking data from real-world reading teaching. Our approach employs several linguistic and gaze-based features to inform an ensemble of different <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifiers</a>, including multi-task learning models that let us transfer knowledge about individual readers to attain better predictions. Notably, the <a href=https://en.wikipedia.org/wiki/Data>data</a> we use in this work stems from noisy readings in the wild, outside of controlled lab conditions. Our experiments show that despite the <a href=https://en.wikipedia.org/wiki/Noise_(signal_processing)>noise</a> and despite the small fraction of misreadings, gaze data improves the performance more than any other feature group and our <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> achieve good performance. We further show that gaze patterns for misread words do not fully generalize across readers, but that we can transfer some knowledge between readers using <a href=https://en.wikipedia.org/wiki/Multitask_learning>multitask learning</a> at least in some cases. Applications of our <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> include partial automation of reading assessment as well as personalized text simplification.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3403.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3403 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3403 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W18-3403.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/W18-3403/>Multi-task learning for historical text normalization : Size matters</a></strong><br><a href=/people/m/marcel-bollmann/>Marcel Bollmann</a>
|
<a href=/people/a/anders-sogaard/>Anders Søgaard</a>
|
<a href=/people/j/joachim-bingel/>Joachim Bingel</a><br><a href=/volumes/W18-34/ class=text-muted>Proceedings of the Workshop on Deep Learning Approaches for Low-Resource NLP</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3403><div class="card-body p-3 small">Historical text normalization suffers from small datasets that exhibit high variance, and previous work has shown that <a href=https://en.wikipedia.org/wiki/Multi-task_learning>multi-task learning</a> can be used to leverage data from related problems in order to obtain more robust models. Previous work has been limited to datasets from a specific language and a specific historical period, and it is not clear whether results generalize. It therefore remains an open problem, when historical text normalization benefits from <a href=https://en.wikipedia.org/wiki/Multi-task_learning>multi-task learning</a>. We explore the benefits of <a href=https://en.wikipedia.org/wiki/Multi-task_learning>multi-task learning</a> across 10 different <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>, representing different languages and periods. Our main findingcontrary to what has been observed for other NLP tasksis that <a href=https://en.wikipedia.org/wiki/Multi-task_learning>multi-task learning</a> mainly works when target task data is very scarce.</div></div><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1030.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1030 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1030 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/I17-1030.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=I17-1030" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/I17-1030/>Learning How to Simplify From Explicit Labeling of Complex-Simplified Text Pairs</a></strong><br><a href=/people/f/fernando-alva-manchego/>Fernando Alva-Manchego</a>
|
<a href=/people/j/joachim-bingel/>Joachim Bingel</a>
|
<a href=/people/g/gustavo-paetzold/>Gustavo Paetzold</a>
|
<a href=/people/c/carolina-scarton/>Carolina Scarton</a>
|
<a href=/people/l/lucia-specia/>Lucia Specia</a><br><a href=/volumes/I17-1/ class=text-muted>Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1030><div class="card-body p-3 small">Current research in <a href=https://en.wikipedia.org/wiki/Text_simplification>text simplification</a> has been hampered by two central problems : (i) the small amount of high-quality parallel simplification data available, and (ii) the lack of explicit annotations of simplification operations, such as deletions or substitutions, on existing data. While the recently introduced Newsela corpus has alleviated the first problem, simplifications still need to be learned directly from parallel text using black-box, end-to-end approaches rather than from explicit annotations. These complex-simple parallel sentence pairs often differ to such a high degree that <a href=https://en.wikipedia.org/wiki/Generalization>generalization</a> becomes difficult. End-to-end models also make it hard to interpret what is actually learned from data. We propose a <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>method</a> that decomposes the task of TS into its sub-problems. We devise a way to automatically identify operations in a parallel corpus and introduce a sequence-labeling approach based on these annotations. Finally, we provide insights on the types of <a href=https://en.wikipedia.org/wiki/Transformation_(function)>transformations</a> that different <a href=https://en.wikipedia.org/wiki/Interpretation_(logic)>approaches</a> can model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2026.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2026 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2026 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=E17-2026" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/E17-2026/>Identifying beneficial task relations for <a href=https://en.wikipedia.org/wiki/Multi-task_learning>multi-task learning</a> in deep neural networks</a></strong><br><a href=/people/j/joachim-bingel/>Joachim Bingel</a>
|
<a href=/people/a/anders-sogaard/>Anders Søgaard</a><br><a href=/volumes/E17-2/ class=text-muted>Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2026><div class="card-body p-3 small">Multi-task learning (MTL) in <a href=https://en.wikipedia.org/wiki/Deep_learning>deep neural networks</a> for <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> has recently received increasing interest due to some compelling benefits, including its potential to efficiently regularize models and to reduce the need for <a href=https://en.wikipedia.org/wiki/Labeled_data>labeled data</a>. While it has brought significant improvements in a number of <a href=https://en.wikipedia.org/wiki/Neuro-linguistic_programming>NLP tasks</a>, mixed results have been reported, and little is known about the conditions under which MTL leads to gains in <a href=https://en.wikipedia.org/wiki/Neuro-linguistic_programming>NLP</a>. This paper sheds light on the specific task relations that can lead to gains from MTL models over single-task setups.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Joachim+Bingel" title="Search for 'Joachim Bingel' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/a/anders-sogaard/ class=align-middle>Anders Søgaard</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/people/g/gustavo-paetzold/ class=align-middle>Gustavo Paetzold</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/f/fernando-alva-manchego/ class=align-middle>Fernando Alva-Manchego</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/carolina-scarton/ class=align-middle>Carolina Scarton</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/lucia-specia/ class=align-middle>Lucia Specia</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/a/ana-valeria-gonzalez/ class=align-middle>Ana Valeria González</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/v/victor-petren-bach-hansen/ class=align-middle>Victor Petrén Bach Hansen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/maria-barrett/ class=align-middle>Maria Barrett</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/sigrid-klerke/ class=align-middle>Sigrid Klerke</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/marcel-bollmann/ class=align-middle>Marcel Bollmann</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/ws/ class=align-middle>WS</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/coling/ class=align-middle>COLING</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/ijcnlp/ class=align-middle>IJCNLP</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/semeval/ class=align-middle>SemEval</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/eacl/ class=align-middle>EACL</a><span class="badge badge-secondary align-middle ml-2">1</span></li></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>